sep=|
date|title|note|url|keywords
2024-01-29|Memory-Inspired Temporal Prompt Interaction for Text-Image Classification|Prompt engineering for multi-modal interation|https://arxiv.org/abs/2401.14856|[]
2024-01-30|Synchformer: Efficient Synchronization from Sparse Cues|Sparse-clued audio visual synchronization through pre-training|https://arxiv.org/abs/2401.16423|[]
2024-01-30|InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model|"VLM, add LoRA to visual encoder, QA data"|https://arxiv.org/abs/2401.16420|[]
2024-01-30|MoE-LLaVA: Mixture of Experts for Large Vision-Language Models|Mixture of expert LLaVA (sparesly activated VLM)|https://arxiv.org/abs/2401.15947|[]
2024-01-30|Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization|VLM to deal with OOD data by synthesized augmentation features|https://arxiv.org/abs/2401.15914|[]
2024-01-30|M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining|multi-lingual (en-ch) CLIP-like VLM with 6B dataset|https://arxiv.org/abs/2401.15896|[]
2024-01-30|Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA|multi-pannel VQA (using off-the-shelf VLM)|https://arxiv.org/abs/2401.15847|[]
2024-01-30|Data-Free Generalized Zero-Shot Learning|zero-shot learning without access to pretraining dataset|https://arxiv.org/abs/2401.15657|[]
2024-01-30|SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection|as title|https://arxiv.org/abs/2401.15293|[]
2024-01-30|Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks|Continual learning of VL tasks using knowledge distillation|https://arxiv.org/abs/2401.15275|[]
2024-01-31|MouSi: Poly-Visual-Expert Vision-Language Models|VLM with multiple visual encoder (for different tasks)|https://arxiv.org/abs/2401.17221|[]
2024-01-31|Category-wise Fine-Tuning: Resisting Incorrect Pseudo-Labels in Multi-Label Image Classification with Partial Labels|Improved pseudo labeling strategy by finetuning each class individually|https://arxiv.org/abs/2401.16991|[]
2024-01-31|Reviving Undersampling for Long-Tailed Learning|as title|https://arxiv.org/abs/2401.16811|[]
2024-01-31|MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images|zero-shot anomaly detection by occurance frequency difference|https://arxiv.org/abs/2401.16753|[]
2024-01-31|Multi-granularity Correspondence Learning from Long-term Noisy Videos|long video by captioning clips|https://arxiv.org/abs/2401.16702|[]
2024-01-31|SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design|efficient vit with fewer patches|https://arxiv.org/abs/2401.16456|[]
2024-01-31|Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking|new probing method that explore VLM has ability to understand verbs|https://arxiv.org/abs/2401.16575|[]
2024-02-09|SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models|"VLM that can do detection, grounding, .... visual tasks"|https://arxiv.org/abs/2402.05935|[]
2024-02-09|Point-VOS: Pointing Up Video Object Segmentation|video segmentation with sparse annotation|https://arxiv.org/abs/2402.05917|[]
2024-02-09|Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data|mamba for multi-dimensional data|https://arxiv.org/abs/2402.05892|[]
2024-02-09|CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion|multi-modal video LLM|https://arxiv.org/abs/2402.05889|[]
2024-02-09|Memory Consolidation Enables Long-Context Video Understanding|vit with explicit memory for long videos|https://arxiv.org/abs/2402.05861|[]
2024-02-09|Question Aware Vision Transformer for Multimodal Reasoning|vqa using llm with attention-enhenced vision encoder|https://arxiv.org/abs/2402.05472|[]
2024-02-09|Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts|MoE during MAE pretraining|https://arxiv.org/abs/2402.05382|[]
2024-02-08|EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss|efficient SAM by distillation and continual training|https://arxiv.org/abs/2402.05008|[]
2024-02-08|ConvLoRA and AdaBN based Domain Adaptation via Self-Training|DA using PEFT and self-training|https://arxiv.org/abs/2402.04964|[]
2024-02-08|Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation|test time adaptation|https://arxiv.org/abs/2402.04958|[]
2024-02-08|Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation|as title|https://arxiv.org/abs/2402.04929|[]
2024-02-08|Data-efficient Large Vision Models through Sequential Autoregression|autoregressive vision model|https://arxiv.org/abs/2402.04841|[]
2024-02-08|LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors|improve open vocabulary object detection by attribute description|https://arxiv.org/abs/2402.04630|[]
2024-02-13|Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models|propose new VLM|https://arxiv.org/abs/2402.07865|[]
2024-02-13|Towards Meta-Pruning via Optimal Transport|new pruning methods using model fusion and optimal transport|https://arxiv.org/abs/2402.07839|[]
2024-02-13|PBADet: A One-Stage Anchor-Free Approach for Part-Body Association|as title|https://arxiv.org/abs/2402.07814|[]
2024-02-13|Complete Instances Mining for Weakly Supervised Instance Segmentation|instance segmentation using image-level labels|https://arxiv.org/abs/2402.07633|[]
2024-02-13|A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)|as title|https://arxiv.org/abs/2402.07410|[]
2024-02-13|Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy|new VQA benchmark|https://arxiv.org/abs/2402.07270|[]
2024-02-13|PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs|prompt VLMs with iteratively refined QAa|https://arxiv.org/abs/2402.07872|[]
2024-02-12|Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy|model compression by low rank approximation|https://arxiv.org/abs/2402.06004|[]
2024-02-14|PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs|enable detection ability of VLM|https://arxiv.org/abs/2402.08657|[]
2024-02-14|Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks|adapt VLM to different tasks by transforming data in QA pairs|https://arxiv.org/abs/2402.08360|[]
2024-02-14|Pix2Code: Learning to Compose Neural Visual Concepts as Programs|solve visual problems by composing programs|https://arxiv.org/abs/2402.08280|[]
2024-02-15|Gradient Alignment with Prototype Feature for Fully Test-time Adaptation|test time adaptation|https://arxiv.org/abs/2402.09004|[]
2024-02-15|Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision|open vocabulary segmentation using independent image-mask and image-text pairs|https://arxiv.org/abs/2402.08960|[]
2024-02-15|DoRA: Weight-Decomposed Low-Rank Adaptation|Improved LoRA by decomposing weights into direction and magnitude|https://arxiv.org/abs/2402.09353|[]
2024-02-15|Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models|as title|https://arxiv.org/abs/2402.08756|[]
2024-02-15|BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation|test time adaptation using LoRA and MoE|https://arxiv.org/abs/2402.08712|[]
2024-02-16|MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations|adding contrastive learning to MAE to boost linear probing or 1-shot learning|https://arxiv.org/abs/2402.10093|[]
2024-02-16|Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP|domain generalization of PEFT|https://arxiv.org/abs/2402.09613|[]
2024-02-16|BitDelta: Your Fine-Tune May Only Be Worth One Bit|decompose fine-tune into direction (1-bit) and a scalar|https://arxiv.org/abs/2402.10193|[]
2024-02-16|Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection|pruning neurons that leads to overfitting|https://arxiv.org/abs/2402.10062|[]
2024-02-16|Fast Vocabulary Transfer for Language Model Compression|improve efficiency by fine-tune tokenizer on downstream domain so that the tokenized sequence is shorter|https://arxiv.org/abs/2402.09977|[]
2024-02-16|Multi-Word Tokenization for Sequence Compression|improve efficiency by view multiple words as one token|https://arxiv.org/abs/2402.09949|[]
2024-01-30|SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing|freezing layers to accelerate training|https://arxiv.org/abs/2401.16720|[]
2024-02-19|PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter|tiny LM between LLaMA adapters and LLM|https://arxiv.org/abs/2402.10896|[]
2024-02-19|Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering|Video QA by captioning frames|https://arxiv.org/abs/2402.10698|[]
2024-02-19|Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond|make MLLM to do retrieval (generation) tasks|https://arxiv.org/abs/2402.10805|[]
2024-02-21|CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples|improve VLM's ability of counting and position understanding by data augmentation|https://arxiv.org/abs/2402.13254|[]
2024-02-21|Video ReCap: Recursive Captioning of Hour-Long Videos|long video captioning by hierarchical decomposing videos|https://arxiv.org/abs/2402.13250|[]
2024-02-21|VideoPrism: A Foundational Visual Encoder for Video Understanding|large video understanding model by google team|https://arxiv.org/abs/2402.13217|[]
2024-02-21|OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog|video QA requires object state understanding (?)|https://arxiv.org/abs/2402.13146|[]
2024-02-21|Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model|DA with black box API using pseudo labeling|https://arxiv.org/abs/2402.13122|[]
2024-02-21|Slot-VLM: SlowFast Slots for Video-Language Modeling|"Use slow-fast, object-centric, event-centric idea to patchify videos"|https://arxiv.org/abs/2402.13088|[]
2024-02-21|ConVQG: Contrastive Visual Question Generation with Multimodal Guidance|Use contrastive loss to generate VQA that is related to both image and text|https://arxiv.org/abs/2402.12846|[]
2024-02-21|Model Composition for Multimodal Large Language Models|combining foundation model of each domain to build a MLLM|https://arxiv.org/abs/2402.12750|[]
2024-02-21|Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering|VQA with multi-modal external knowledge source|https://arxiv.org/abs/2402.12728|[]
2024-02-21|Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition|try to learn domain-invariant physics to improve few-shot action recognition|https://arxiv.org/abs/2402.12706|[]
2024-02-21|Efficient Parameter Mining and Freezing for Continual Object Detection|continual learning by selecting parameters to freeze|https://arxiv.org/abs/2402.12624|[]
2024-02-21|Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization|MoE that is better at scaling the number of experts|https://arxiv.org/abs/2402.12550|[]
2024-02-21|Towards Cross-Domain Continual Learning|as title|https://arxiv.org/abs/2402.12490|[]
2024-02-21|Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation|teacher-agnostic knowledge distillation without original data|https://arxiv.org/abs/2402.12406|[]
2024-02-20|UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking|deal with ID switch during multi object tracking|https://arxiv.org/abs/2402.12303|[]
2024-02-20|Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers|processing long sequence by cross attention with with learnable tokens|https://arxiv.org/abs/2402.12138|[]
2024-02-20|LVCHAT: Facilitating Long Video Comprehension|enable long video by hierarchical token merging|https://arxiv.org/abs/2402.12079|[]
2024-02-22|Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation|segmentation using vision query for better generalizability|https://arxiv.org/abs/2402.13697|[]
2024-02-22|A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models|evaluate gender bias by observing output (text or image) preference|https://arxiv.org/abs/2402.13636|[]
2024-02-22|Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement|improve moment retrieval by detecting relevance in text and vision separately|https://arxiv.org/abs/2402.13576|[]
2024-02-22|Event-aware Video Corpus Moment Retrieval|improve moment retrieval by merging semantically similar scene into event|https://arxiv.org/abs/2402.13566|[]
2024-02-22|Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization|improve low precision training by considering data distribution|https://arxiv.org/abs/2402.13497|[]
2024-02-22|Unsupervised learning based object detection using Contrastive Learning|as title|https://arxiv.org/abs/2402.13465|[]
2024-02-22|Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning|use CLIP and discriminator as reward to train image captioning model|https://arxiv.org/abs/2402.13936|[]
2024-02-22|Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment|better modality alignment of VLM in addition to Q-Former|https://arxiv.org/abs/2402.13561|[]
2024-02-22|Unsupervised Concept Discovery Mitigates Spurious Correlations|improve robustness of classification by unsupervisedly clustering concepts and adjusting sample strategy|https://arxiv.org/abs/2402.13368|[]
2024-02-22|SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning|"dealing with imbalanced data withtout predefining distribution (e.g., whether long tail or not)"|https://arxiv.org/abs/2402.13505|[]
2024-02-22|LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs|make LLM deal with long video by token merging|https://arxiv.org/abs/2402.13546|[]
2024-02-20|Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before|multi-stage contrastive learning|https://arxiv.org/abs/2402.11816|[]
2024-02-20|Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models|self-consistency of VLM|https://arxiv.org/abs/2402.11622|[]
2024-02-20|CPN: Complementary Proposal Network for Unconstrained Text Detection|text detection that has irregular layout|https://arxiv.org/abs/2402.11540|[]
2024-02-20|Key Patch Proposer: Key Patches Contain Rich Information|select (without training) most important patches that minimize reconstruction loss|https://arxiv.org/abs/2402.11458|[]
2024-02-20|Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning|large video LLM using new automatic annotated large dataset|https://arxiv.org/abs/2402.11435|[]
2024-02-20|Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition|zero-shot learning by generating unseen data but use OOD detector to do regularization|https://arxiv.org/abs/2402.11424|[]
2024-02-20|Learning by Reconstruction Produces Uninformative Features For Perception|Yann LeCun argues the power of learning by reconstruction|https://arxiv.org/abs/2402.11337|[]
2024-02-20|ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition|intention driven dataset and VLM for better user experience|https://arxiv.org/abs/2402.11301|[]
2024-02-20|CoLLaVO: Crayon Large Language and Vision mOdel|using panoptic segmentation as additional clues for better zero-shot performance|https://arxiv.org/abs/2402.11248|[]
2024-02-20|A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation|UNet-like decoder for fewer computational cost segmentation|https://arxiv.org/abs/2402.11201|[]
2024-02-20|GIM: Learning Generalizable Image Matcher From Internet Videos|as title|https://arxiv.org/abs/2402.11095|[]
2024-02-20|The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test|as title|https://arxiv.org/abs/2402.11089|[]
2024-02-20|AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling|MLLM that understands and generates multimodal data|https://arxiv.org/abs/2402.12226|[]
2024-02-23|WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition|use SAM as pseudo ground truth to solve weakly supervised object detection|https://arxiv.org/abs/2402.14812|[]
2024-02-23|DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models|do detection fisrt and then zoom in to solve fine-grained VQA|https://arxiv.org/abs/2402.14767|[]
2024-02-23|Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition|adapt foundation model to downstream tasks with large domain shift (visual place recognition)|https://arxiv.org/abs/2402.14505|[]
2024-02-23|Reading Relevant Feature from Global Representation Memory for Visual Object Tracking|object tracking with memory and a module to select memory to use|https://arxiv.org/abs/2402.14392|[]
2024-02-20|The Effectiveness of Random Forgetting for Robust Generalization|reinitialize weights (forget) to mitigate attach|https://arxiv.org/abs/2402.11733|[]
2024-02-20|Aligning Modalities in Vision Large Language Models via Preference Fine-tuning|solve hallucination problem of VLM by adding noise to cause hallucinationg and guide model with ground truth|https://arxiv.org/abs/2402.11411|[]
2024-02-20|Knowledge Distillation Based on Transformed Teacher Matching|study on function of temperature used in knowledge distillation|https://arxiv.org/abs/2402.11148|[]
2024-02-23|Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning|"as title (evaluate on protein biology, chemical property prediction, and particle physics)"|https://arxiv.org/abs/2402.14789|[]
2024-02-23|OmniPred: Language Models as Universal Regressors|as title|https://arxiv.org/abs/2402.14547|[]
2024-02-23|TinyLLaVA: A Framework of Small-scale Large Multimodal Models|better data with smaller MLLM|https://arxiv.org/abs/2402.14289|[]
2024-02-23|Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model|use a intermediate teacher model between foundatioin model and student|https://arxiv.org/abs/2402.14035|[]
2024-02-26|Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding|as title|https://arxiv.org/abs/2402.15300|[]
2024-02-26|Attention-Guided Masked Autoencoders For Learning Image Representations|MAE but mask patches with high attention |https://arxiv.org/abs/2402.15172|[]
2024-02-26|Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?|combining multiple sets of LoRA|https://arxiv.org/abs/2402.15414|[]
2024-02-26|CommVQA: Situating Visual Question Answering in Communicative Contexts|VQA with communicative context|https://arxiv.org/abs/2402.15002|[]
2024-02-27|Gradient-Guided Modality Decoupling for Missing-Modality Robustness|"Missing modality when using multi-modal model, remove the conflict in gradients of different modalities"|https://arxiv.org/abs/2402.16318|[]
2024-02-27|One-stage Prompt-based Continual Learning|as title|https://arxiv.org/abs/2402.16189|[]
2024-02-27|LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding|deal with long video by making model sample relevant frames|https://arxiv.org/abs/2402.16050|[]
2024-02-27|Semi-supervised Open-World Object Detection|detection with unknown class and unlabel data|https://arxiv.org/abs/2402.16013|[]
2024-02-27|Multimodal Instruction Tuning with Conditional Mixture of LoRA|like pathway or mixture of expert using LoRA|https://arxiv.org/abs/2402.15896|[]
2024-02-27|"TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages"|as title|https://arxiv.org/abs/2402.16021|[]
2024-02-28|VRP-SAM: SAM with Visual Reference Prompt|make SAM segment according to reference (object from other pictures)|https://arxiv.org/abs/2402.17726|[]
2024-02-28|Interactive Multi-Head Self-Attention with Linear Complexity|efficient MHSA|https://arxiv.org/abs/2402.17507|[]
2024-02-28|LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning|prompt tuning while consider early block features having low level information|https://arxiv.org/abs/2402.17406|[]
2024-02-28|Scaling Supervised Local Learning with Augmented Auxiliary Networks|training with local gradient-isolated network rather than backpropagation of global loss|https://arxiv.org/abs/2402.17318|[]
2024-02-28|m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers|distillation of modular network|https://arxiv.org/abs/2402.16918|[]
2024-02-29|Gradient Reweighting: Towards Imbalanced Class-Incremental Learning|as title|https://arxiv.org/abs/2402.18528|[]
2024-02-29|Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation|weakly supervised segmentation using patch pseudo lable and contrastive learning|https://arxiv.org/abs/2402.18467|[]
2024-02-29|Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization|use text prompt to capture object feature and help generalization|https://arxiv.org/abs/2402.18447|[]
2024-02-29|Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport|try to learn a prototype of each class to do retrieval (?)|https://arxiv.org/abs/2402.18411|[]
2024-02-29|Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization|use additional masking model that take downstream validation loss into consideration|https://arxiv.org/abs/2402.18128|[]
2024-02-29|UniVS: Unified and Universal Video Segmentation with Prompts as Queries|as title|https://arxiv.org/abs/2402.18115|[]
2024-02-29|Polos: Multimodal Metric Learning from Human Feedback for Image Captioning|"new dataset with human feed back, learned based metric for captioning"|https://arxiv.org/abs/2402.18091|[]
2024-02-29|Generalizable Two-Branch Framework for Image Class-Incremental Learning|side network for continual learning|https://arxiv.org/abs/2402.18086|[]
2024-02-29|Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation|weakly supervised segmentation with refined patch-level pseudo label|https://arxiv.org/abs/2402.17891|[]
2024-02-29|REPrune: Channel Pruning via Kernel Representative Selection|"pruning for CNN according to ""maximum cluster coverage problem"""|https://arxiv.org/abs/2402.17862|[]
2024-02-29|Classes Are Not Equal: An Empirical Study on Image Recognition Fairness|as title|https://arxiv.org/abs/2402.18133|[]
2024-03-01|Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models|DeepMind's new attempt for efficient LM|https://arxiv.org/abs/2402.19427|[]
2024-03-01|Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning|about continual learning and catastrophic forgetting in PEFT|https://arxiv.org/abs/2402.18865|[]
2024-03-01|Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains|fixed classifier according to Equiangular Tight Frame simplex|https://arxiv.org/abs/2402.18614|[]
2024-03-01|FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning|System that can do inference and parameter-efficient finetuning requests in the same iteration|https://arxiv.org/abs/2402.18789|[]
2024-03-01|Learning to Compress Prompt in Natural Language Formats|as title|https://arxiv.org/abs/2402.18700|[]
2024-03-01|Simple linear attention language models balance the recall-throughput tradeoff|linear and sliding-window attention for efficient LLM|https://arxiv.org/abs/2402.18668|[]
2024-02-29|SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization|theoretically understand pruning|https://arxiv.org/abs/2402.17902|[]
2024-02-29|DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation|as title|https://arxiv.org/abs/2402.17812|[]
2024-02-29|HOP to the Next Tasks and Domains for Continual Learning in NLP|"specialized MLP for each problem, use statistical moments to capture information"|https://arxiv.org/abs/2402.18449|[]
2024-02-29|Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation|generate NLP dataset using unannotated text and task attributes|https://arxiv.org/abs/2402.18334|[]
2024-03-01|Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers|as title|https://arxiv.org/abs/2402.19479|[]
2024-03-01|PEM: Prototype-based Efficient MaskFormer for Image Segmentation|"restrict attention region for efficiency, using feature pyramid"|https://arxiv.org/abs/2402.19422|[]
2024-03-01|VIXEN: Visual Text Comparison Network for Image Difference Captioning|"summarize difference between 2 images, use synthesized data and GPT4 to do augmentation"|https://arxiv.org/abs/2402.19119|[]
2024-03-01|VideoMAC: Video Masked Autoencoders Meet ConvNets|video MAE using ConvNet which reduces required data size|https://arxiv.org/abs/2402.19082|[]
2024-03-01|Debiased Novel Category Discovering and Localization|detection taking unknown objects into account|https://arxiv.org/abs/2402.18821|[]
2024-03-01|Motion Guided Token Compression for Efficient Masked Video Modeling|use variance between patches to detect motion and mask still patches|https://arxiv.org/abs/2402.18577|[]
2024-03-01|"CAMixerSR: Only Details Need More ""Attention"""|predict which patch need to be processed by attention|https://arxiv.org/abs/2402.19289|[]
2024-03-01|Rethinking Multi-domain Generalization with A General Learning Objective|design objective that learn domain-independent feature|https://arxiv.org/abs/2402.18853|[]
2024-03-04|Can Transformers Capture Spatial Relations between Objects?|long range attention to improve spatial relation capability|https://arxiv.org/abs/2403.00729|[]
2024-03-04|Tri-Modal Motion Retrieval by Learning a Joint Embedding Space|"text, video, motion retrieval"|https://arxiv.org/abs/2403.00691|[]
2024-03-04|VisionLLaMA: A Unified LLaMA Interface for Vision Tasks|"propse ViT architecture similar to LLaMA (e.g., with 2DRoPE)"|https://arxiv.org/abs/2403.00522|[]
2024-03-04|Learning and Leveraging World Models in Visual Representation Learning|"using world model, a techinique of RL to do representation learning"|https://arxiv.org/abs/2403.00504|[]
2024-03-04|TempCompass: Do Video LLMs Really Understand Videos?|video LLM benchmark|https://arxiv.org/abs/2403.00476|[]
2024-03-04|Invariant Test-Time Adaptation for Vision-Language Model Generalization|"test time adaptation, using SAM to find foreground and background, and using mask to prevent shortcut"|https://arxiv.org/abs/2403.00376|[]
2024-03-04|Task Indicating Transformer for Task-conditional Dense Predictions|segmentation for different purpose using Task Gate Decoder|https://arxiv.org/abs/2403.00327|[]
2024-03-04|Multi-modal Attribute Prompting for Vision-Language Models|increase CLIP generalizability by alignment visual and text attribute|https://arxiv.org/abs/2403.00219|[]
2024-03-04|Few-Shot Relation Extraction with Hybrid Visual Evidence|detect objects apearing in caption and find their relation|https://arxiv.org/abs/2403.00724|[]
2024-03-04|Rethinking The Uniformity Metric in Self-Supervised Learning|objectives that improve SSL and prevent dimension collapse theoretically|https://arxiv.org/abs/2403.00642|[]
2024-03-05|RegionGPT: Towards Region Understanding Vision Language Model|using patch merging and spatial region embedding|https://arxiv.org/abs/2403.02330|[]
2024-03-05|Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training|observing result when mask RoI to reduce hallucination|https://arxiv.org/abs/2403.02325|[]
2024-03-05|Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures|as title|https://arxiv.org/abs/2403.02308|[]
2024-03-05|Non-autoregressive Sequence-to-Sequence Vision-Language Models|predetermined output lengths of learnable tokens; input info as KV in cross attention|https://arxiv.org/abs/2403.02249|[]
2024-03-05|Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations|data and benchmark for multi-person interaction understanding|https://arxiv.org/abs/2403.02090|[]
2024-03-05|VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT|VTG using image captioning and finding similarity between query and caption|https://arxiv.org/abs/2403.02076|[]
2024-03-05|A Generative Approach for Wikipedia-Scale Visual Entity Recognition|"classification with 6M class, assign ""code"" to each class and use generative model to predict code"|https://arxiv.org/abs/2403.02041|[]
2024-03-05|Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification|general image augmentation aiming for robustness|https://arxiv.org/abs/2403.01944|[]
2024-03-05|xT: Nested Tokenization for Larger Context in Large Images|"divide image into sub-region before patchify, better local & global feature"|https://arxiv.org/abs/2403.01915|[]
2024-03-05|AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation|regularized by reconstruction labeled feature from pseudo labeled feature|https://arxiv.org/abs/2403.01818|[]
2024-03-05|Training-Free Pretrained Model Merging|mergine according activatioin and weight similarity|https://arxiv.org/abs/2403.01753|[]
2024-03-05|Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection|new task configuration|https://arxiv.org/abs/2403.01680|[]
2024-03-05|Logit Standardization in Knowledge Distillation|design on temperature to adjust target of student networks|https://arxiv.org/abs/2403.01427|[]
2024-03-05|Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning|use text encoder of CLIP and prompting LLM to replace image data|https://arxiv.org/abs/2403.01209|[]
2024-03-06|Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization|use two teachers to validate the pseudo label is reliable|https://arxiv.org/abs/2403.03145|[]
2024-03-06|Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization|use two model to validate the pseudo label is reliable|https://arxiv.org/abs/2403.03095|[]
2024-03-06|Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models|two pathway for different resolution of images|https://arxiv.org/abs/2403.03003|[]
2024-03-06|MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer|use cross-modal similarity as clue to pruning tokens in each layer|https://arxiv.org/abs/2403.02991|[]
2024-03-06|Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception|concatenating MLLM and a segmentation decoder|https://arxiv.org/abs/2403.02969|[]
2024-03-06|Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples|"hard negative mining by swapping predefine keywords (e.g., color, size...)"|https://arxiv.org/abs/2403.02875|[]
2024-03-06|PromptKD: Unsupervised Prompt Distillation for Vision-Language Models|distillation into learnable prompt token (like distillation + prompt tuning)|https://arxiv.org/abs/2403.02781|[]
2024-03-06|DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization|as title|https://arxiv.org/abs/2403.02714|[]
2024-03-07|MeaCap: Memory-Augmented Zero-shot Image Captioning|using CLIP to retrieve related text and refine captions|https://arxiv.org/abs/2403.03715|[]
2024-03-07|Enhancing Vision-Language Pre-training with Rich Supervisions|seeking supervision website source code|https://arxiv.org/abs/2403.03346|[]
2024-03-08|Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation|try to save distribution of previous task by saving hard negative using contrastive loss|https://arxiv.org/abs/2403.04599|[]
2024-03-08|CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?|"explore (mainly gender) bias in CLIP, define several metrics for bias"|https://arxiv.org/abs/2403.04547|[]
2024-03-08|Aligners: Decoupling LLMs and Alignment|"avoid training whole LLM, train additional module for multiple LLM"|https://arxiv.org/abs/2403.04224|[]
2024-03-07|GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection|memory efficient whole model training by decompose gradient|https://arxiv.org/abs/2403.03507|[]
2024-03-07|The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models|study on sub-network with same ID performance but better OOD performance|https://arxiv.org/abs/2403.03942|[]
2024-03-07|Learning to Decode Collaboratively with Multiple Language Models|ensemble of LLMs on token likelihodd level|https://arxiv.org/abs/2403.03870|[]
2024-02-01|Repeat After Me: Transformers are Better than State Space Models at Copying|argue that Transformer may be better than SSM on copying and retrieving|https://arxiv.org/abs/2402.01032|[]
2024-03-08|SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM|VQA dataset with fine-grain objects and prposed baseline (using augmentation)|https://arxiv.org/abs/2403.04735|[]
2024-03-08|Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level|kernel fusion of neighborhood attention|https://arxiv.org/abs/2403.04690|[]
2024-03-08|CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios|"audio visual LM on audio visual QA, audio visual instruction dataset"|https://arxiv.org/abs/2403.04640|[]
2024-03-08|LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking|PEFT combining LoRA and parameter sharing|https://arxiv.org/abs/2403.04303|[]
2024-03-08|LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition|improve contrastive learning by masking|https://arxiv.org/abs/2403.04066|[]
2024-03-08|A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition|"propose Modality Bias Hypothesis, using knowledge distillation to make model robust to missing frame and modality"|https://arxiv.org/abs/2403.04245|[]
2024-03-11|"Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos"|use image caption to obtain pseudo label for DA|https://arxiv.org/abs/2403.05535|[]
2024-03-11|Attention-guided Feature Distillation for Semantic Segmentation|as title|https://arxiv.org/abs/2403.05451|[]
2024-03-11|VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model|use VLM to refine or valide pseudo label|https://arxiv.org/abs/2403.05346|[]
2024-03-11|Debiasing Large Visual Language Models|use meaningless image to calibrate bias from LLM|https://arxiv.org/abs/2403.05262|[]
2024-03-11|Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval|use intra-modality similarity as supervision of inter-modality similarity|https://arxiv.org/abs/2403.05261|[]
2024-03-11|Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval|rematch mismatched pairs using optimal transport to calibrate data|https://arxiv.org/abs/2403.05105|[]
2024-03-11|Agile Multi-Source-Free Domain Adaptation|parameter and throughput efficient|https://arxiv.org/abs/2403.05062|[]
2024-03-11|Poly-View Contrastive Learning|"contrastive learning with more than paired view, reduce required batch size"|https://arxiv.org/abs/2403.05490|[]
2024-03-11|Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization|"learn domain-invariant prototype to do pseudo labeling, utilize labeled paired to make model robust to pseudo label"|https://arxiv.org/abs/2403.05209|[]
2024-03-11|Denoising Autoregressive Representation Learning|Causal transformer for images to do next patch prediction (denoising)|https://arxiv.org/abs/2403.05196|[]
2024-03-12|Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling|"Prompt tuning for video tasks, insert tokens in KV rather than input sequence"|https://arxiv.org/abs/2403.06978|[]
2024-03-12|VideoMamba: State Space Model for Efficient Video Understanding|"Mamba for video, MAE with CLIP as target"|https://arxiv.org/abs/2403.06977|[]
2024-03-12|Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation|further disentangle CLIP feature using orthogonal loss|https://arxiv.org/abs/2403.06946|[]
2024-03-12|LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations|use original image as anchor to prevent images after augmentation become negative pair|https://arxiv.org/abs/2403.06813|[]
2024-03-12|An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models|token pruning for VLM acceleration|https://arxiv.org/abs/2403.06764|[]
2024-03-12|Answering Diverse Questions via Text Attached with Key Audio-Visual Clues|"audio visual QA, utilize knowledge distiall for alignment"|https://arxiv.org/abs/2403.06679|[]
2024-03-12|OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation|"unsupervised segmentation using cluster and optimal transport, no pseudo label is used"|https://arxiv.org/abs/2403.06546|[]
2024-03-12|VkD:  Improving Knowledge Distillation using Orthogonal Projections|as title|https://arxiv.org/abs/2403.06213|[]
2024-03-12|RESTORE: Towards Feature Shift for Vision-Language Prompt Learning|text and image alignment of CLIP for better novel class generalizability|https://arxiv.org/abs/2403.06136|[]
2024-03-12|ClickVOS: Click Video Object Segmentation|segmentation mask translation starting with one point per object (during inference)|https://arxiv.org/abs/2403.06130|[]
2024-03-12|In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model|test time adaptation using prompt tuning with CLIP|https://arxiv.org/abs/2403.06126|[]
2024-03-12|Multisize Dataset Condensation|reduce dataset size for efficient training|https://arxiv.org/abs/2403.06075|[]
2024-03-12|Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning|test time adaptation by prompt tuning and estimation of distribution|https://arxiv.org/abs/2403.06059|[]
2024-03-12|CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot Learning|attribute and object disentangle using cycle consistency|https://arxiv.org/abs/2403.05924|[]
2024-03-12|LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content|prompt GPT4V as supervision for long-tail data|https://arxiv.org/abs/2403.05854|[]
2024-03-12|Augmentations vs Algorithms: What Works in Self-Supervised Learning|"argue that augmentation is much important than algorthm in contrastive learning, propose a unified framework to experiment"|https://arxiv.org/abs/2403.05726|[]
2024-03-13|Beyond Text: Frozen Large Language Models in Visual Signal Comprehension|translate images to natural language|https://arxiv.org/abs/2403.07874|[]
2024-03-13|Distilling the Knowledge in Data Pruning|data condensation using knowledge distillation|https://arxiv.org/abs/2403.07854|[]
2024-03-13|MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric|pruning for VLM|https://arxiv.org/abs/2403.07839|[]
2024-03-13|Multi-modal Auto-regressive Modeling via Visual Words|map image patches to distribution over dictionary and treat it as text|https://arxiv.org/abs/2403.07720|[]
2024-03-13|Masked AutoDecoder is Effective Multi-Task Vision Generalist|use causal decoder to make vision model to do multiple tasks|https://arxiv.org/abs/2403.07692|[]
2024-03-13|Unified Source-Free Domain Adaptation|problem framework of domain adaptation|https://arxiv.org/abs/2403.07601|[]
2024-03-13|Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors|test time adaptation by observing results before and after augmentation|https://arxiv.org/abs/2403.07366|[]
2024-03-13|Open-World Semantic Segmentation Including Class Similarity|use post-process to cluster unknow class|https://arxiv.org/abs/2403.07532|[]
2024-03-13|MoAI: Mixture of All Intelligence for Large Language and Vision Models|make use of multiple visual feature|https://arxiv.org/abs/2403.07508|[]
2024-03-13|Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models|decouple visual capability into task-agnostic and task-specific|https://arxiv.org/abs/2403.07304|[]
2024-03-13|IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers|quantization can be used for training and inference|https://arxiv.org/abs/2403.07339|[]
2024-03-14|DAM: Dynamic Adapter Merging for Continual Video QA Learning|Merge domain specific adapter to do continual VQA learning|https://arxiv.org/abs/2403.08755|[]
2024-03-14|Consistent Prompting for Rehearsal-Free Continual Learning|"multiple learable prompt and classifier for different domain, use cross domain prediction smoothness as regularization and auxiliary loss"|https://arxiv.org/abs/2403.08568|[]
2024-03-14|Cross-modality debiasing: using language to mitigate sub-population shifts in imaging|as title|https://arxiv.org/abs/2403.07888|[]
2024-03-14|Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection|increase pretraining speed by removing patches based on similarity to text|https://arxiv.org/abs/2403.07883|[]
2022-05-24|Large Language Models are Zero-Shot Reasoners||https://arxiv.org/abs/2205.11916|[]
2022-05-21|Least-to-Most Prompting Enables Complex Reasoning in Large Language Models||https://arxiv.org/abs/2205.10625|[]
2021-06-30|Attention Bottlenecks for Multimodal Fusion||https://arxiv.org/abs/2107.00135|[]
2019-04-16|Co-Separating Sounds of Visual Objects||https://arxiv.org/abs/1904.07750|[]
2020-11-03|Learning Representations from Audio-Visual Spatial Alignment||https://arxiv.org/abs/2011.01819|[]
2023-03-30|Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment||https://arxiv.org/abs/2303.17490|[]
2022-03-30|MAE-AST: Masked Autoencoding Audio Spectrogram Transformer||https://arxiv.org/abs/2203.16691|[]
2022-07-13|Masked Autoencoders that Listen||https://arxiv.org/abs/2207.06405|[]
2017-11-30|A Closer Look at Spatiotemporal Convolutions for Action Recognition||https://arxiv.org/abs/1711.11248|[]
2021-03-10|VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples||https://arxiv.org/abs/2103.05905|[]
2020-10-19|Self-supervised Co-training for Video Representation Learning||https://arxiv.org/abs/2010.09709|[]
2021-04-22|Multiscale Vision Transformers||https://arxiv.org/abs/2104.11227|[]
2022-03-03|BEVT: BERT Pretraining of Video Transformers||https://arxiv.org/abs/2112.01529|[]
2022-04-06|ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound||https://arxiv.org/abs/2204.02874|[]
2023-04-10|Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition||https://arxiv.org/abs/2304.04704|[]
2022-04-14|Masked Siamese Networks for Label-Efficient Learning||https://arxiv.org/abs/2204.07141|[]
2022-10-17|Token Merging: Your ViT But Faster||https://arxiv.org/abs/2210.09461|[]
2021-11-03|VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts||https://arxiv.org/abs/2111.02358|[]
2022-05-04|CoCa: Contrastive Captioners are Image-Text Foundation Models||https://arxiv.org/abs/2205.01917|[]
2023-06-01|StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners||https://arxiv.org/abs/2306.00984|[]
2022-06-16|MixGen: A New Multi-Modal Data Augmentation||https://arxiv.org/abs/2206.08358|[]
2023-04-24|A Cookbook of Self-Supervised Learning||https://arxiv.org/abs/2304.12210|[]
2022-05-21|Self-Supervised Speech Representation Learning: A Review||https://arxiv.org/abs/2205.10643|[]
2022-05-18|Masked Autoencoders As Spatiotemporal Learners||https://arxiv.org/abs/2205.09113|[]
2021-06-15|BEiT: BERT Pre-Training of Image Transformers||https://arxiv.org/abs/2106.08254|[]
2020-02-13|Self-supervised learning for audio-visual speaker diarization||https://arxiv.org/abs/2002.05314|[]
2019-01-27|Augment your batch: better training with larger batches||https://arxiv.org/abs/1901.09335|[]
2019-06-19|XLNet: Generalized Autoregressive Pretraining for Language Understanding||https://arxiv.org/abs/1906.08237|[]
2017-11-14|Decoupled Weight Decay Regularization||https://arxiv.org/abs/1711.05101|[]
2021-12-16|Masked Feature Prediction for Self-Supervised Visual Pre-Training||https://arxiv.org/abs/2112.09133|[]
2024-03-15|OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning|use prompt tuning to tackle various additional information|https://arxiv.org/abs/2403.09634|[]
2024-03-15|Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding|evaluation of video Mamba|https://arxiv.org/abs/2403.09626|[]
2024-03-15|"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"|30B LM from Apple|https://arxiv.org/abs/2403.09611|[]
2024-03-15|GiT: Towards Generalist Vision Transformer through Universal Language Interface|Multimodal LM without concatenating encoder and LLM|https://arxiv.org/abs/2403.09394|[]
2024-03-15|LocalMamba: Visual State Space Model with Windowed Selective Scan|as title|https://arxiv.org/abs/2403.09338|[]
2024-03-15|Are Vision Language Models Texture or Shape Biased and Can We Steer Them?|compare bias in vision encoder and corresponding VLM|https://arxiv.org/abs/2403.09193|[]
2024-03-15|PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation|PEFT + token pruning to achieve inference and training efficiency|https://arxiv.org/abs/2403.09192|[]
2024-03-18|Frozen Feature Augmentation for Few-Shot Image Classification|emperical study on augmentation of few shot linear probing|https://arxiv.org/abs/2403.10519|[]
2024-03-18|VideoAgent: Long-form Video Understanding with Large Language Model as Agent|"use CLIP to retrieve, use VLM to validate the candidate"|https://arxiv.org/abs/2403.10517|[]
2024-03-18|CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning|remove bias from imbalanced data making prediction on no-pattern images neutral|https://arxiv.org/abs/2403.10391|[]
2024-03-18|Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models|concatenate YOLO GPT4V SAM to do few shot dense prediction|https://arxiv.org/abs/2403.10287|[]
2024-03-18|HawkEye: Training Video-Text LLMs for Grounding Text in Videos|large scale video text dataset|https://arxiv.org/abs/2403.10228|[]
2024-03-18|Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt|disentangle learable prompts into task invariant (initialized from base classes) and task specific|https://arxiv.org/abs/2403.09857|[]
2024-03-18|Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks|"when fine-tune downsstream tasks, adaptive update class distribution, minimize intra and maximize inter distancce"|https://arxiv.org/abs/2403.10097|[]
2024-03-18|Uni-SMART: Universal Science Multimodal Analysis and Research Transformer|"Multimodal LM that can process table, chart, chemical reactions, ..."|https://arxiv.org/abs/2403.10301|[]
2024-03-18|EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba|as title|https://arxiv.org/abs/2403.09977|[]
2024-03-18|Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers|as title|https://arxiv.org/abs/2403.10030|[]
2024-03-18|Knowledge Condensation and Reasoning for Knowledge-based VQA|"VQA with external knowledge passage, summary knowledge before feed to models"|https://arxiv.org/abs/2403.10037|[]
2024-03-19|Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning|"New adapters for new class, and construct prototype to prevent forgetting"|https://arxiv.org/abs/2403.12030|[]
2024-03-19|Align and Distill: Unifying and Improving Domain Adaptive Object Detection|new benchmark and baseline for DA object detection|https://arxiv.org/abs/2403.12029|[]
2024-03-19|"FlexCap: Generating Rich, Localized, and Flexible Captions in Images"|"deepmind's length, bbox conditioned captioning model using new dataset"|https://arxiv.org/abs/2403.12026|[]
2024-03-19|SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules|try to summarize and unify mechanisms of LoRA variants|https://arxiv.org/abs/2403.11887|[]
2024-03-19|Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation|PEFT + token pruning to achieve inference and training efficiency|https://arxiv.org/abs/2403.11808|[]
2024-03-19|Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs|APE to enhance visual recognition|https://arxiv.org/abs/2403.11755|[]
2024-03-19|Towards Generalizing to Unseen Domains with Few Labels|Semi-supervise domain adaptation|https://arxiv.org/abs/2403.11674|[]
2024-03-19|Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters|use MoE and routing mechanism to deal with ID and OOD data|https://arxiv.org/abs/2403.11549|[]
2024-03-19|Semantic Prompting with Image-Token for Continual Learning|extract task agnostic feature to improve generalizibility|https://arxiv.org/abs/2403.11537|[]
2024-03-19|Do CLIPs Always Generalize Better than ImageNet Models?|argue the robustness of CLIP|https://arxiv.org/abs/2403.11497|[]
2024-03-19|Towards Generalizing to Unseen Domains with Few Labels|semi-supervise domain adaptation|https://arxiv.org/abs/2403.11674|[]
2024-03-19|VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding|as title|https://arxiv.org/abs/2403.11481|[]
2024-03-19|Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding|video grounding without timestamp label|https://arxiv.org/abs/2403.11463|[]
2024-03-19|Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration|"continual learning with missing modality, solving with PEFT"|https://arxiv.org/abs/2403.11373|[]
2024-03-19|SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant|Try to generate high quality QA to do instruction tuning|https://arxiv.org/abs/2403.11299|[]
2024-03-19|MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts|"besides static soft prompt, learn a module to generate dynamic soft prompt based on other modality"|https://arxiv.org/abs/2403.10568|[]
2024-03-19|Self-Supervised Quantization-Aware Knowledge Distillation|full precision teacher distill to low precision students|https://arxiv.org/abs/2403.11106|[]
2024-03-19|Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches|Comparing mapping images to LLM's embedding space and to natural language captions|https://arxiv.org/abs/2403.11317|[]
2024-03-19|PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation|use gradient to determine which layer to train during test time adaptation|https://arxiv.org/abs/2403.10650|[]
2024-03-19|: Source-free Domain Adaptation Through the Lens of Data Augmentation|"do cluster in embedding space, regularization is inspired by augmentation"|https://arxiv.org/abs/2403.10834|[]
2024-03-19|Rethinking Multi-view Representation Learning via Distilled Disentangling|"learn view-independent feature by cross-view MAE, then learn view-specific feature by disentangle with learnt view-independent feature"|https://arxiv.org/abs/2403.10897|[]
2024-03-19|Task-Aware Low-Rank Adaptation of Segment Anything Model|decompose LoRA into three component (one additional for different tasks)|https://arxiv.org/abs/2403.10971|[]
2024-03-19|Audio-Visual Segmentation via Unlabeled Frame Exploitation|"Self-training for AV segmentation, use optical flow for frames near labeled frames, teacher model for distant unlabeled frames"|https://arxiv.org/abs/2403.11074|[]
2024-03-19|Self-supervised co-salient object detection via feature correspondence at multiple scales|"segmentation co-occurance objects in a set of images in SSL manner, use attention weight to get object masks, and find co-occurance object using feature similarity"|https://arxiv.org/abs/2403.11107|[]
2024-03-19|Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence|modified self- and cross attention to do dense matching|https://arxiv.org/abs/2403.11120|[]
2024-03-19|DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation|two networks that generate pseudo labels for each other|https://arxiv.org/abs/2403.11184|[]
2024-03-19|TAG: Guidance-free Open-Vocabulary Semantic Segmentation|"use DINO to do segmentation, us CLIP to retrieve text and mergin the segmentation according to similarity to text"|https://arxiv.org/abs/2403.11197|[]
2024-03-19|Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias|"DA setting where target domain may not cover all class of source domain, and may contain new classes"|https://arxiv.org/abs/2403.11234|[]
2024-03-20|Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models|"let VLM output RoI first, and then ask the question again with the RoI image"|https://arxiv.org/abs/2403.12966|[]
2024-03-20|Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models|"PEFT, try to predict what the image is and is not simultaneously"|https://arxiv.org/abs/2403.12964|[]
2024-03-20|Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments|"observing top-2 prediction to identify confiusing pair, then do correction"|https://arxiv.org/abs/2403.12883|[]
2024-03-20|Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models|Test time adaptation by estimating shift between training and testing data prototype generated by text encoder|https://arxiv.org/abs/2403.12952|[]
2024-03-20|Confidence Self-Calibration for Multi-Label Class-Incremental Learning|prevent overfitting on new class by entropy regularization|https://arxiv.org/abs/2403.12559|[]
2024-03-20|UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All|Use text embedding center as anchor rather than image as in ImageBind|https://arxiv.org/abs/2403.12532|[]
2024-03-20|Towards Multimodal In-Context Learning for Vision & Language Models|"Verify off-the-shelf VLM can't do in context learning, using fine-tuning to enable them"|https://arxiv.org/abs/2403.12736|[]
2024-03-20|DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM|"overlapping information (e.g., ruler) on image can improve detection ability of GPT 4V"|https://arxiv.org/abs/2403.12488|[]
2024-03-20|CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation|as title|https://arxiv.org/abs/2403.12455|[]
2024-03-20|Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity|"using less data to train CLIP, condensation according to covariance"|https://arxiv.org/abs/2403.12267|[]
2024-03-20|Non-negative Contrastive Learning|non-negative constraint on feature space make feature sparse and interpretable|https://arxiv.org/abs/2403.12459|[]
2024-03-20|Do Generated Data Always Help Contrastive Learning?|study on relation between synthesized data and data augmentation|https://arxiv.org/abs/2403.12448|[]
2024-03-21|On Pretraining Data Diversity for Self-Supervised Learning|"although diversity may improve performance, problem of domain shift is more crucial"|https://arxiv.org/abs/2403.13808|[]
2024-03-21|SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning|"clustering unlabled data, refine the representation by contrastive loss"|https://arxiv.org/abs/2403.13684|[]
2024-03-21|VL-Mamba: Exploring State Space Models for Multimodal Learning|"replace LLaMA with Mamba, explore scan mechanism"|https://arxiv.org/abs/2403.13600|[]
2024-03-21|What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models|"use GPT4V to generate counterfactual keywords, to preven hallucination"|https://arxiv.org/abs/2403.13513|[]
2024-03-21|Scale Decoupled Distillation|use multi-scale pooling to distill fine-grained logits rather than one global logit|https://arxiv.org/abs/2403.13512|[]
2024-03-21|Improved Baselines for Data-efficient Perceptual Augmentation of LLMs|"evaluate interfaces of MLM, propose data efficient interfance"|https://arxiv.org/abs/2403.13499|[]
2024-03-21|Counting Network for Learning from Majority Label|label is given at bag-level rather than instance level|https://arxiv.org/abs/2403.13370|[]
2024-03-21|vid-TLDR: Training Free Token merging for Light-weight Video Transformer|Merge tokens according to attention map in video transformer|https://arxiv.org/abs/2403.13347|[]
2024-03-21|PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns|benchmark of abstract VQA|https://arxiv.org/abs/2403.13315|[]
2024-03-21|Rotary Position Embedding for Vision Transformer|RoPE can benefit ViT especially on high resolution images|https://arxiv.org/abs/2403.13298|[]
2024-03-21|SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models|Use self-consistency (location and description) and reinforcement learning to improve detection|https://arxiv.org/abs/2403.13263|[]
2024-03-21|When Do We Not Need Larger Vision Models?|argue that scaling resolution may surpass scaling model size|https://arxiv.org/abs/2403.13043|[]
2024-03-21|BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning|balance feature distribution using contrastive learning|https://arxiv.org/abs/2403.12986|[]
2024-03-21|Bridge the Modality and Capacity Gaps in Vision-Language Model Selection|try to select best VLM base on the label set (text) of target dataset|https://arxiv.org/abs/2403.13797|[]
2024-03-21|HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models|use module generated by HyperNetwork to project visual feature to text-like token|https://arxiv.org/abs/2403.13447|[]
2024-03-21|A Unified and General Framework for Continual Learning|"unified framework of regularization-, Bayesian-, memory-based methods, propose refresh learning (unlearn then relearn)"|https://arxiv.org/abs/2403.13249|[]
2024-03-21|RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition|"use CLIP to retrieve, use VLM to ranking, improve zero-shot ability"|https://arxiv.org/abs/2403.13805|[]
2024-03-21|Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments|"study on relation between bbox stability and accuracy, evaluate detector without ground truth"|https://arxiv.org/abs/2403.13803|[]
2024-03-22|Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification|"two new measure to quantify bias, use augmentation to reduce bias"|https://arxiv.org/abs/2403.13925|[]
2024-03-22|How to be fair? A study of label and selection bias|"theoretically understan bias, debias methods and their relation"|https://arxiv.org/abs/2403.14282|[]
2024-03-22|AI and Memory Wall|"show that in the modern architecture, DRAM rather than FLOPs is the bottleneck"|https://arxiv.org/abs/2403.14123|[]
2024-03-22|Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey|survey of PEFT|https://arxiv.org/abs/2403.14608|[]
2024-03-26|Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models|use optional memory to enhance (training free) few-shot adaptation|https://arxiv.org/abs/2403.17589|[]
2024-03-26|The Unreasonable Ineffectiveness of the Deeper Layers|emperical study on standard layer pruning + recovering strategy|https://arxiv.org/abs/2403.17887|[]
2024-03-26|Multi-Task Dense Prediction via Mixture of Low-Rank Experts|MoE (PEFT) for dense prediction|https://arxiv.org/abs/2403.17749|[]
2024-03-26|DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning|"continual learning but can not store previous data, use underfitting network with compensation to prevent forgettng"|https://arxiv.org/abs/2403.17503|[]
2024-03-25|If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions|"align LLM to CLIP, so that LLM can output high similarity description, and then analyze these description"|https://arxiv.org/abs/2403.16442|[]
2024-03-25|LLMs Are Few-Shot In-Context Low-Resource Language Learners|study on shortcoming of in-context learning on low-resourse domain|https://arxiv.org/abs/2403.16512|[]
2024-03-25|DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization|use text prompts to simulate unseen domain without access to images|https://arxiv.org/abs/2403.16697|[]
2024-03-25|Understanding Long Videos in One Multimodal Language Model Pass|use frame selection to help VLM process long video|https://arxiv.org/abs/2403.16998|[]
2024-03-25|DreamLIP: Language-Image Pre-training with Long Captions|train CLIP with longer and multiple captions|https://arxiv.org/abs/2403.17007|[]
2024-03-25|Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models|make LLM able to retrieve multimodal data|https://arxiv.org/abs/2403.17359|[]
2024-03-25|One-Shot Domain Incremental Learning|as title|https://arxiv.org/abs/2403.16707|[]
2024-03-25|LLMs Are Few-Shot In-Context Low-Resource Language Learners|evaluate different in context learning format for low resourse language|https://arxiv.org/abs/2403.16512|[]
2024-03-25|Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models|visual chain of thought dataset and benchmark|https://arxiv.org/abs/2403.16999|[]
2024-03-27|Efficient Test-Time Adaptation of Vision-Language Models|"training free test time adaptation, maintain two queue that will affect prediction distribution"|https://arxiv.org/abs/2403.18293|[]
2024-03-27|Toward Interactive Regional Understanding in Vision-Large Language Models|user indicated region aware VLM|https://arxiv.org/abs/2403.18260|[]
2024-03-27|An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM|put frames in grid and treat it as 1 image|https://arxiv.org/abs/2403.18406|[]
2024-03-27|Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation|knowledge distillation between adapter modules|https://arxiv.org/abs/2403.18804|[]
2024-03-27|Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models|"multi-resolution, patch info mining visual encoder, concatenate LLM and text to image model for any to any VLM"|https://arxiv.org/abs/2403.18814|[]
2024-03-27|CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models|"use probablistic model (mean, std of normal distribution) for robust continual learning"|https://arxiv.org/abs/2403.19137|[]
2024-03-27|Dense Vision Transformer Compression with Few Samples|compression of ViT by dropping attention and shrinking MLP in few shot scenario|https://arxiv.org/abs/2403.18708|[]
2024-03-27|Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation|use Mamba for audio. divided audio in to chucks for local and global processing|https://arxiv.org/abs/2403.18257|[]
2024-03-27|Few-Shot Recalibration of Language Models|train recalibration model to evaluate confidence and precision curve of LLM|https://arxiv.org/abs/2403.18286|[]
2024-03-27|Towards Non-Exemplar Semi-Supervised Class-Incremental Learning|semi supervised prototype and contrastive learning to avoid forgetting|https://arxiv.org/abs/2403.18291|[]
2024-03-27|ViTAR: Vision Transformer with Any Resolution|token mergin and modified positional encoding for dynamic resolution|https://arxiv.org/abs/2403.18361|[]
2024-03-27|Debiasing Sentence Embedders through Contrastive Word Pairs|as title|https://arxiv.org/abs/2403.18555|[]
2024-03-27|SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens|generate draft token and verify to speed up LLM|https://arxiv.org/abs/2403.18647|[]
2024-03-27|Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding|use prompt to exacerbate hallucinated concept for easier detection|https://arxiv.org/abs/2403.18715|[]
2024-03-27|Projective Methods for Mitigating Gender Bias in Pre-trained Language Models|apply tranditional debiasing method to BERT|https://arxiv.org/abs/2403.18803|[]
2024-03-27|Measuring Political Bias in Large Language Models: What Is Said and How It Is Said|as title|https://arxiv.org/abs/2403.18932|[]
2024-03-27|LITA: Language Instructed Temporal-Localization Assistant|temporal reasoning localization dataset|https://arxiv.org/abs/2403.19046|[]
2024-03-27|Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach|"similar to LoRA, using SVD"|https://arxiv.org/abs/2403.19067|[]
2024-03-27|FACTOID: FACtual enTailment fOr hallucInation Detection|hallucination benchmark for LLM|https://arxiv.org/abs/2403.19113|[]
2024-03-27|Compressing Large Language Models by Streamlining the Unimportant Layer|layer pruning and layer replacement for LLM|https://arxiv.org/abs/2403.19135|[]
2024-03-28|Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment|only has ordered list of action when training, improve pseudo label efficiency|https://arxiv.org/abs/2403.19225|[['cs.CV'], ['CVPR']]
2024-03-28|CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection|tackle class imbalance problem in DA, learning class relation and do augmentation|https://arxiv.org/abs/2403.19278|[['cs.CV'], ['CVPR']]
2024-03-28|Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality|multimodal narratives caption for long video, augmentation to handl missing modality|https://arxiv.org/abs/2403.19221|[['cs.CV']]
2024-03-28|Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models|benchmark and baseline (decide whether to zoom in) for VLM to handle high resolution, text rich images|https://arxiv.org/abs/2403.19322|[['cs.CV']]
2024-03-28|Checkpoint Merging via Bayesian Optimization in LLM Pretraining|study on merging LLM|https://arxiv.org/abs/2403.19390|[['cs.CL']]
2024-03-28|Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition|dynamically use cross attention to prevent effect between low valence modality|https://arxiv.org/abs/2403.19554|[['cs.CV']]
2024-03-28|LocCa: Visual Pretraining with Location-aware Captioners|as title|https://arxiv.org/abs/2403.19596|[['cs.CV']]
2024-03-28|Siamese Vision Transformers are Scalable Audio-visual Learners|same ViT to process audio and visual, pretrained with contrastive learning and MAE|https://arxiv.org/abs/2403.19638|[['cs.CV']]
2024-03-28|MedBN: Robust Test-Time Adaptation against Malicious Test Samples|use statistics (batch norm) to handle manipulated data in test time adaptation|https://arxiv.org/abs/2403.19326|[['attack'], ['cs.LG'], ['CVPR']]
2024-03-29|Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer|use adapter in class incremental learning, learn prototypes for old classes|https://arxiv.org/abs/2403.19979|[['parameter-efficient'], ['cs.CV'], ['CVPR']]
2024-03-29|MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning|task specific LoRA, and task agnostic LoRA for multi task learning|https://arxiv.org/abs/2403.20320|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CV'], ['CVPR']]
2024-03-29|LayerNorm: A key component in parameter-efficient fine-tuning|argue that tuning layer norm is enough for downstream tasks|https://arxiv.org/abs/2403.20284|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL']]
2024-03-29|Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations|text to image model based on can't read negated description, try to make CLIP understand negation|https://arxiv.org/abs/2403.20312|[['Vision-Language'], ['cs.CV']]
2024-03-29|Are We on the Right Way for Evaluating Large Vision-Language Models?|benchmark for VLM, avoid data leakage and make sure necessity of visual input|https://arxiv.org/abs/2403.20330|[['Vision-Language'], ['cs.CV']]
2024-03-29|Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models|make VLM refuse to answer when the question is not solvable|https://arxiv.org/abs/2403.20331|[['Vision Language', 'VLM'], ['cs.CV']]
2024-03-29|Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning|increase complexity of data augmentation as training process goes|https://arxiv.org/abs/2403.20012|[['cs.CV'], ['ICLR']]
2024-03-29|Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion|as title|https://arxiv.org/abs/2403.20015|[['cs.CL'], ['ICLR']]
2024-03-29|ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning|continual learn for segmentation using prompt tuning |https://arxiv.org/abs/2403.20126|[['cs.CV'], ['CVPR']]
2024-03-29|Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions|benchmark and simple baseline for temporal corrupted|https://arxiv.org/abs/2403.20254|[['cs.CV'], ['CVPR']]
2024-03-29|Convolutional Prompting meets Language Models for Continual Learning|convolutional hyper network to achieve layer-wise input-aware prompt tuning for continual learning|https://arxiv.org/abs/2403.20317|[['cs.CV'], ['CVPR']]
2024-03-29|On Large Language Models' Hallucination with Regard to Known Facts|detect hallucination in LLM by tracking token probabilities|https://arxiv.org/abs/2403.20009|[['cs.CL']]
2024-03-29|FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues|swap features of same concept from different modalities and train on matching loss to enhance performance|https://arxiv.org/abs/2403.20026|[['cs.CV']]
2024-03-29|Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning|prompt and finetune to make LLM learn from error during CoT|https://arxiv.org/abs/2403.20046|[['cs.CL']]
2024-04-02|T-VSL: Text-Guided Visual Sound Source Localization in Mixtures|use text as guide to disentangle audio and visual for multi source sounding localization|https://arxiv.org/abs/2404.01751|[['audio-visual'], ['cs.CV'], ['CVPR']]
2024-04-02|BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition|audio visual SSL pretraining, dual encoders MAE|https://arxiv.org/abs/2404.02098|[['audio-visual'], ['cs.CV'], ['ICASSP']]
2024-04-02|ViTamin: Designing Scalable Vision Models in the Vision-Language Era|propose evaluation protocol for visual model (using CLIP method), propose CNN Transformer hybrid model|https://arxiv.org/abs/2404.02132|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|Iterated Learning Improves Compositionality in Large Vision-Language Models|finite code book and re-initialize model during CLIP training for better compositionality ("girl in white facing man in black" vs "girl in black facing man in white")|https://arxiv.org/abs/2404.02145|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|VLRM: Vision-Language Models act as Reward Models for Image Captioning|reinforcement learning for better captioning|https://arxiv.org/abs/2404.01911|[['Vision-Language'], ['cs.CV']]
2024-04-02|Weakly-supervised Audio Separation via Bi-modal Semantic Similarity|audio separation without access to single source audio, mix multi source audio and use contrastive loss|https://arxiv.org/abs/2404.01740|[['cs.SD'], ['ICLR']]
2024-04-02|Joint-Task Regularization for Partially Labeled Multi-Task Learning|as title|https://arxiv.org/abs/2404.01976|[['cs.CV'], ['CVPR']]
2024-04-02|Using Interpretation Methods for Model Enhancement|learn to make interpretation supervisedly, different ways to generate interpretation|https://arxiv.org/abs/2404.02068|[['cs.CL'], ['EMNLP']]
2024-04-02|Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners|as title|https://arxiv.org/abs/2404.02117|[['cs.CV'], ['CVPR']]
2024-04-04|ReFT: Representation Finetuning for Language Models|can't understand but it is PEFT + LLM|https://arxiv.org/abs/2404.03592|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-04-04|Would Deep Generative Models Amplify Bias in Future Models?|find that using sythesized data may not increase bias|https://arxiv.org/abs/2404.03242|[['social biases'], ['Diffusion'], ['cs.CV'], ['CVPR']]
2024-04-04|Scaling Up Video Summarization Pretraining with Large Language Models|new dataset for long video, using automated pipeline|https://arxiv.org/abs/2404.03398|[['cs.CV'], ['CVPR']]
2024-04-04|MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens|VLM, concatenate frame feature and subtitle feature for video understanding|https://arxiv.org/abs/2404.03413|[['cs.CV']]
2024-04-04|Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought|RL, knowledge distillation for small LM to deal with multi-hop QA|https://arxiv.org/abs/2404.03414|[['cs.CL']]
2024-04-16|Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models|align visual feature with text feature GPT4 prompt; leaarable prompt for CLIP classification|https://arxiv.org/abs/2404.10357|[['Vision-Language'], ['cs.CV']]
2024-04-16|Self-Supervised Visual Preference Alignment|use data augmentation to produce data for DPO|https://arxiv.org/abs/2404.10501|[['Vision-Language', 'VLM'], ['cs.CV']]
2024-04-16|Future Language Modeling from Temporal Document History|new task of future language modeling|https://arxiv.org/abs/2404.10297|[['cs.CL'], ['ICLR']]
2024-04-16|Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation|perturbating on feature space to simulate domain shift and use adapter to rectify the perturbation and deal with domain shift|https://arxiv.org/abs/2404.10322|[['cs.CV'], ['CVPR']]
2024-04-16|Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model|locate modules that is adept for specific tasks to prevent catastrophoic forgetting|https://arxiv.org/abs/2404.10306|[['cs.CL']]
2024-04-16|Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards|LLM self-correction to do DPO|https://arxiv.org/abs/2404.10346|[['cs.CL']]
2024-04-16|Dual Modalities of Text: Visual and Textual Generative Pre-training|render text as image and do next patch, token prediction simultaneously|https://arxiv.org/abs/2404.10710|[['cs.CL']]
2024-04-16|Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study|as title|https://arxiv.org/abs/2404.10719|[['cs.CL']]
2024-04-15|Bridging Vision and Language Spaces with Assignment Prediction|align visual feature with word embedding, don't need to modify LLM's weights|https://arxiv.org/abs/2404.09632|[['vision-language'], ['cs.CV'], ['ICLR']]
2024-04-15|Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering|use proxy model to sample neighborhood question, and use consistency on neighborhood questions to measure reliability|https://arxiv.org/abs/2404.10193|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-15|Leveraging Temporal Contextualization for Video Action Recognition|extract context tokens of videos as KV for better aggregation of information|https://arxiv.org/abs/2404.09490|[['vision-language'], ['cs.CV']]
2024-04-15|Conditional Prototype Rectification Prompt Learning|learn textual prototype in semi-supervised manners to mitigate overfitting|https://arxiv.org/abs/2404.09872|[['vision-language'], ['cs.CV']]
2024-04-15|Evolving Interpretable Visual Classifiers with Large Language Models|use elvolutionary search with LLM and CLIP to build interpretable image classifier|https://arxiv.org/abs/2404.09941|[['vision-language'], ['cs.CV']]
2024-04-15|LoRA Dropout as a Sparsity Regularizer for Overfitting Control|add noise to LoRA weights to increase parameter sparisty, prevent overfitting theoretically|https://arxiv.org/abs/2404.09610|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.LG']]
2024-04-15|The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning|semi supervised method for CLIP to tackle narrow distribution during few shots learning|https://arxiv.org/abs/2404.09778|[['cs.CV'], ['ECCV']]
2024-04-15|Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model|study on quality of instruction tuning data|https://arxiv.org/abs/2404.09717|[['cs.CL']]
2024-04-15|Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations|as title|https://arxiv.org/abs/2404.09785|[['cs.CL']]
2024-04-15|TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding|use captioning, detection, and cropping to enhance VQA performance|https://arxiv.org/abs/2404.09797|[['cs.CV']]
2024-04-15|Impact of Preference Noise on the Alignment Performance of Generative Language Models|study on quality of data to do alignment|https://arxiv.org/abs/2404.09824|[['cs.CL']]
2024-04-15|CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting|study on behaviors of LLM on different languages and cultures|https://arxiv.org/abs/2404.10199|[['cs.CL']]
2024-04-15|Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models|as title|https://arxiv.org/abs/2404.09529|[['cs.LG']]
2024-04-15|LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models|low rank pruning of LLM by observation of low rank structure of MHA|https://arxiv.org/abs/2404.09695|[['cs.LG']]
2024-04-25|AAPL: Adding Attributes to Prompt Learning for Vision-Language Models|reduce bias introduced by data augmentation in previous zero shot classification method|https://arxiv.org/abs/2404.16804|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-25|Training-Free Unsupervised Prompt for Vision-Language Models|training and label free method to boost zero-shot classification, generate high confident prototype and compute similarity to adjust logits|https://arxiv.org/abs/2404.16339|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-04-25|Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class|use multiple vectors to represent a class so that different attribute in a class can be taken into account|https://arxiv.org/abs/2404.16717|[['Vision-language', 'VLM'], ['cs.CV']]
2024-04-25|Multi-Scale Representations by Varying Window Attention for Semantic Segmentation|constrain attention region for better multi-scale feature extraction in segmentation|https://arxiv.org/abs/2404.16573|[['cs.CV'], ['ICLR']]
2024-04-25|EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning|instruction tuning for visual emotional recognition, new data and model|https://arxiv.org/abs/2404.16670|[['cs.CV'], ['CVPR']]
2024-04-25|List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs|data set for set-of-mark (use visual tags on image for MLM to refer to), a trick enhance model performance and interpretability|https://arxiv.org/abs/2404.16375|[['cs.CV']]
2024-04-25|Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities|feature disentangle during distillation to handle missing modality|https://arxiv.org/abs/2404.16456|[['cs.CV']]
2024-04-25|Exploring Internal Numeracy in Language Models: A Case Study on ALBERT|study on how LM handle embeddings of numbers (increase in one direction after PCA)|https://arxiv.org/abs/2404.16574|[['cs.CL']]
2024-04-25|Understanding Privacy Risks of Embeddings Induced by Large Language Models|study on potential of LLM reconstruction pre-training data|https://arxiv.org/abs/2404.16587|[['cs.CL']]
2024-04-25|TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning|mtrain model to generate code for calculation, token merging for high resolution images|https://arxiv.org/abs/2404.16635|[['cs.CV']]
2024-04-25|Zero-Shot Distillation for Image Encoders: How to Make Effective Use of Synthetic Data|L2 loss is better than contrastive loss for distillation|https://arxiv.org/abs/2404.16637|[['cs.CV']]
2024-04-25|Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4|GPT-4 are more biased to add information when asked to improve its answers when subtraction was more efficient|https://arxiv.org/abs/2404.16692|[['cs.CL']]
2024-04-25|Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding|layer dropout in LLM during training, early exit during inference and self-speculative decoding for speed up inference|https://arxiv.org/abs/2404.16710|[['cs.CL']]
2024-04-25|SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension|as title|https://arxiv.org/abs/2404.16790|[['cs.CV']]
2024-04-25|Make Your LLM Fully Utilize the Context|fine tune LLM on long context|https://arxiv.org/abs/2404.16811|[['cs.CL']]
2024-04-25|How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites|VLM with 6B ViT, dynamic resolution, English and Chinese training data|https://arxiv.org/abs/2404.16821|[['cs.CV']]
2024-04-25|T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients|as title|https://arxiv.org/abs/2404.16495|[['cs.LG']]
2024-04-25|VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations|argue that LMs have difficulties in distinguishing between text semantic variations|https://arxiv.org/abs/2404.16365|[['vision-language', 'VLMs'], ['face'], ['cs.CL']]
2024-04-24|FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication|study about effect of deduplication on social bias|https://arxiv.org/abs/2404.16123|[['Vision-Language'], ['social biases'], ['cs.CV'], ['CVPR']]
2024-04-24|What Makes Multimodal In-Context Learning Work?|argue the lack of exploration in multimodal in context learning|https://arxiv.org/abs/2404.15736|[['cs.CV'], ['CVPR']]
2024-04-24|MoDE: CLIP Data Experts via Clustering|train multiple CLIP on some clusters of data and ensemble them at inference time|https://arxiv.org/abs/2404.16030|[['cs.CV'], ['CVPR']]
2024-04-24|CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data|train CLIP by prediction synset (BCE loss), speed up 2.7x|https://arxiv.org/abs/2404.15653|[['cs.CV']]
2024-05-01|CLIPArTT: Light-weight Adaptation of CLIP to New Domains at Test Time|test time adaptation by aggregate top k prediction into a new prompt to generate pseudo label,  standardize benchmarks|https://arxiv.org/abs/2405.00754|[['vision-language', 'VLMs'], ['cs.CV']]
2024-05-01|AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts|adaptive select LoRA experts rather than select fixed top k|https://arxiv.org/abs/2405.00361|[['cs.CL']]
2024-05-01|Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval|interpolate between query image and text to obtain feature to do retrieval|https://arxiv.org/abs/2405.00571|[['cs.CV']]
2024-05-01|Are Models Biased on Text without Gender-related Language?|show that LLM is biased even if correlation between gender related words in the sentences are calibrated by pretrain data, ICLR 2024|https://arxiv.org/abs/2405.00588|[['cs.CL']]
2024-05-01|Learning to Compose: Improving Object Centric Learning by Injecting Compositionality|objectives to enable decode composite images via composite representation, improve object-centricc learning, ICLR 2024|https://arxiv.org/abs/2405.00646|[['cs.CV']]
2024-05-01|DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling|tune the model to predict multiple tokens at a time to speed up inference time|https://arxiv.org/abs/2405.00888|[['cs.CL']]
2024-05-01|LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets|model and data pruning at the same time|https://arxiv.org/abs/2405.00906|[['cs.CV']]
2024-05-01|LLaVA Finds Free Lunch: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs|tune LLaVA by predicting receiver behavior|https://arxiv.org/abs/2405.00942|[['cs.CV']]
2024-05-02|Learning Object States from Actions via Large Language Models|extract objects states from action in the caption using LLM|https://arxiv.org/abs/2405.01090|[['vision-language'], ['cs.CV']]
2024-05-02|MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors|resources efficient way to tune LLM for 3D|https://arxiv.org/abs/2405.01413|[['parameter-efficient', 'efficient fine-tuning'], ['vision-language'], ['3D', 'point cloud'], ['cs.CV']]
2024-05-02|MANTIS: Interleaved Multi-Image Instruction Tuning|VLM with multiple image inputs, train on 16 A100|https://arxiv.org/abs/2405.01483|[['vision language'], ['cs.CV']]
2024-05-02|Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models|theoreticaly understand retrieval using CLIP, show that I2I is better than T2I|https://arxiv.org/abs/2405.01468|[['Vision-Language'], ['cs.LG']]
2024-05-02|WildChat: 1M ChatGPT Interaction Logs in the Wild|dataset of conversation with GPT-4|https://arxiv.org/abs/2405.01470|[['cs.CL'], ['ICLR']]
2024-05-02|Why Tabular Foundation Models Should Be a Research Priority|call for more  study on tubular modality|https://arxiv.org/abs/2405.01147|[['cs.LG']]
2024-05-02|ATOM: Attention Mixer for Efficient Dataset Distillation|different attention mechanism for feature matching in dataset distillation|https://arxiv.org/abs/2405.01373|[['architecture search'], ['cs.CV'], ['CVPR']]
2024-04-30|MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation|domain generalization with some attribute correlation with base that may confuse models|https://arxiv.org/abs/2404.19644|[['vision-language'], ['cs.CV'], ['ICLR']]
2024-04-30|MoPEFT: A Mixture-of-PEFTs for the Segment Anything Model|use different PEFT techniques and dynamically learns to activate some of them|https://arxiv.org/abs/2405.00293|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CV'], ['CVPR']]
2024-04-30|Soft Prompt Generation for Domain Generalization|learn soft prompt for different domain as target, and train generative model to produce soft prompt for different domain|https://arxiv.org/abs/2404.19286|[['vision language', 'VLMs'], ['cs.CV']]
2024-04-30|CLIP-Mamba: CLIP Pretrained Mamba Models with OOD and Hessian Evaluation|67M Mamba on par with 307M ViT on zero shot, much better on OOD|https://arxiv.org/abs/2404.19394|[['parameter efficiency'], ['cs.CV']]
2024-04-30|SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models|progressively tune adapters at different layers|https://arxiv.org/abs/2405.00201|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-04-30|On Improving the Algorithm-, Model-, and Data- Efficiency of Self-Supervised Learning|new SSL method to improve performance, memory bank, square regularization|https://arxiv.org/abs/2404.19289|[['cs.CV']]
2024-04-30|StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation|highlight variance of initialization of PEFT, separate hard and soft prompt|https://arxiv.org/abs/2404.19335|[['cs.CL']]
2024-04-30|One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features|one stage open vocabulary temporal action detection rather than making proposal then identifying|https://arxiv.org/abs/2404.19542|[['cs.CV']]
2024-04-30|SemiPL: A Semi-supervised Method for Event Sound Source Localization|as title|https://arxiv.org/abs/2404.19615|[['cs.CV']]
2024-04-30|Better & Faster Large Language Models via Multi-token Prediction|pre train LLM to predict multiple tokens from scratch|https://arxiv.org/abs/2404.19737|[['cs.CL']]
2024-04-30|DOCCI: Descriptions of Connected and Contrasting Images|fine grained long text image pair dataset|https://arxiv.org/abs/2404.19753|[['Vision-language'], ['text-to-image'], ['cs.CV']]
2024-04-30|ASAM: Boosting Segment Anything Model with Adversarial Tuning|generate adversarial examples using diffusion model to boost SAM|https://arxiv.org/abs/2405.00256|[['diffusion'], ['cs.CV'], ['CVPR']]
2024-04-30|Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey|as title|https://arxiv.org/abs/2405.00314|[['cs.LG']]
2024-05-02|Early Transformers: A study on Efficient Training of Transformer Models through Early-Bird Lottery Tickets|iterative pruning, selective retraining to improve training efficiency|https://arxiv.org/abs/2405.02353|[['training efficiency'], ['cs.CL']]
2024-05-02|Enhancing User Experience in On-Device Machine Learning with Gated Compression Layers|dynamically filter non-essential inputs to improve power consumption|https://arxiv.org/abs/2405.01739|[['cs.LG']]
2024-05-02|COPAL: Continual Pruning in Large Language Generative Models|continual pruning weights with high sensitivity to new data in continual learning setting|https://arxiv.org/abs/2405.02347|[['cs.LG']]
2024-05-03|What matters when building vision-language models?|experiment with different design decision of VLM, propose a 8B VLM|https://arxiv.org/abs/2405.02246|[['vision-language', 'VLMs'], ['cs.CV']]
2024-05-03|Auto-Encoding Morph-Tokens for Multimodal LLM|VLM use quantized visual token, can generate both image and text|https://arxiv.org/abs/2405.01926|[['cs.CV']]
2024-05-03|MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot Action Recognition|few-shot action recognition, take into account different velocity|https://arxiv.org/abs/2405.02077|[['cs.CV']]
2024-05-08|THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models|hallucination benchmark for free form QA rather than multiple choice question|https://arxiv.org/abs/2405.05256|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-05-08|Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion|utilize different multimodality data (text-image, text-audio, audio-image, etc) to improve unified model (text-image-audio)|https://arxiv.org/abs/2405.04883|[['cs.CV'], ['ICML']]
2024-05-08|Estimating Noisy Class Posterior with Part-level Labels for Noisy Label Learning|partition images into distinct parts and make pseudo labels on them, and optimize model using multi-label loss to improve robustness|https://arxiv.org/abs/2405.05714|[['cs.CV'], ['CVPR']]
2024-05-08|You Only Cache Once: Decoder-Decoder Architectures for Language Models|new decoder-decoder architecture for LLM that reduces key value caches usage to improve throughput, and GPU memory|https://arxiv.org/abs/2405.05254|[['cs.CL']]
