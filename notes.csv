sep=|
date|title|note|url|keywords
2024-01-29|Memory-Inspired Temporal Prompt Interaction for Text-Image Classification|Prompt engineering for multi-modal interation|https://arxiv.org/abs/2401.14856|[]
2024-01-30|Synchformer: Efficient Synchronization from Sparse Cues|Sparse-clued audio visual synchronization through pre-training|https://arxiv.org/abs/2401.16423|[]
2024-01-30|InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model|"VLM, add LoRA to visual encoder, QA data"|https://arxiv.org/abs/2401.16420|[]
2024-01-30|MoE-LLaVA: Mixture of Experts for Large Vision-Language Models|Mixture of expert LLaVA (sparesly activated VLM)|https://arxiv.org/abs/2401.15947|[]
2024-01-30|Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization|VLM to deal with OOD data by synthesized augmentation features|https://arxiv.org/abs/2401.15914|[]
2024-01-30|M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining|multi-lingual (en-ch) CLIP-like VLM with 6B dataset|https://arxiv.org/abs/2401.15896|[]
2024-01-30|Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA|multi-pannel VQA (using off-the-shelf VLM)|https://arxiv.org/abs/2401.15847|[]
2024-01-30|Data-Free Generalized Zero-Shot Learning|zero-shot learning without access to pretraining dataset|https://arxiv.org/abs/2401.15657|[]
2024-01-30|SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection|as title|https://arxiv.org/abs/2401.15293|[]
2024-01-30|Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks|Continual learning of VL tasks using knowledge distillation|https://arxiv.org/abs/2401.15275|[]
2024-01-31|MouSi: Poly-Visual-Expert Vision-Language Models|VLM with multiple visual encoder (for different tasks)|https://arxiv.org/abs/2401.17221|[]
2024-01-31|Category-wise Fine-Tuning: Resisting Incorrect Pseudo-Labels in Multi-Label Image Classification with Partial Labels|Improved pseudo labeling strategy by finetuning each class individually|https://arxiv.org/abs/2401.16991|[]
2024-01-31|Reviving Undersampling for Long-Tailed Learning|as title|https://arxiv.org/abs/2401.16811|[]
2024-01-31|MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images|zero-shot anomaly detection by occurance frequency difference|https://arxiv.org/abs/2401.16753|[]
2024-01-31|Multi-granularity Correspondence Learning from Long-term Noisy Videos|long video by captioning clips|https://arxiv.org/abs/2401.16702|[]
2024-01-31|SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design|efficient vit with fewer patches|https://arxiv.org/abs/2401.16456|[]
2024-01-31|Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking|new probing method that explore VLM has ability to understand verbs|https://arxiv.org/abs/2401.16575|[]
2024-02-09|SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models|"VLM that can do detection, grounding, .... visual tasks"|https://arxiv.org/abs/2402.05935|[]
2024-02-09|Point-VOS: Pointing Up Video Object Segmentation|video segmentation with sparse annotation|https://arxiv.org/abs/2402.05917|[]
2024-02-09|Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data|mamba for multi-dimensional data|https://arxiv.org/abs/2402.05892|[]
2024-02-09|CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion|multi-modal video LLM|https://arxiv.org/abs/2402.05889|[]
2024-02-09|Memory Consolidation Enables Long-Context Video Understanding|vit with explicit memory for long videos|https://arxiv.org/abs/2402.05861|[]
2024-02-09|Question Aware Vision Transformer for Multimodal Reasoning|vqa using llm with attention-enhenced vision encoder|https://arxiv.org/abs/2402.05472|[]
2024-02-09|Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts|MoE during MAE pretraining|https://arxiv.org/abs/2402.05382|[]
2024-02-08|EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss|efficient SAM by distillation and continual training|https://arxiv.org/abs/2402.05008|[]
2024-02-08|ConvLoRA and AdaBN based Domain Adaptation via Self-Training|DA using PEFT and self-training|https://arxiv.org/abs/2402.04964|[]
2024-02-08|Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation|test time adaptation|https://arxiv.org/abs/2402.04958|[]
2024-02-08|Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation|as title|https://arxiv.org/abs/2402.04929|[]
2024-02-08|Data-efficient Large Vision Models through Sequential Autoregression|autoregressive vision model|https://arxiv.org/abs/2402.04841|[]
2024-02-08|LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors|improve open vocabulary object detection by attribute description|https://arxiv.org/abs/2402.04630|[]
2024-02-13|Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models|propose new VLM|https://arxiv.org/abs/2402.07865|[]
2024-02-13|Towards Meta-Pruning via Optimal Transport|new pruning methods using model fusion and optimal transport|https://arxiv.org/abs/2402.07839|[]
2024-02-13|PBADet: A One-Stage Anchor-Free Approach for Part-Body Association|as title|https://arxiv.org/abs/2402.07814|[]
2024-02-13|Complete Instances Mining for Weakly Supervised Instance Segmentation|instance segmentation using image-level labels|https://arxiv.org/abs/2402.07633|[]
2024-02-13|A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)|as title|https://arxiv.org/abs/2402.07410|[]
2024-02-13|Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy|new VQA benchmark|https://arxiv.org/abs/2402.07270|[]
2024-02-13|PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs|prompt VLMs with iteratively refined QAa|https://arxiv.org/abs/2402.07872|[]
2024-02-12|Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy|model compression by low rank approximation|https://arxiv.org/abs/2402.06004|[]
2024-02-14|PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs|enable detection ability of VLM|https://arxiv.org/abs/2402.08657|[]
2024-02-14|Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks|adapt VLM to different tasks by transforming data in QA pairs|https://arxiv.org/abs/2402.08360|[]
2024-02-14|Pix2Code: Learning to Compose Neural Visual Concepts as Programs|solve visual problems by composing programs|https://arxiv.org/abs/2402.08280|[]
2024-02-15|Gradient Alignment with Prototype Feature for Fully Test-time Adaptation|test time adaptation|https://arxiv.org/abs/2402.09004|[]
2024-02-15|Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision|open vocabulary segmentation using independent image-mask and image-text pairs|https://arxiv.org/abs/2402.08960|[]
2024-02-15|DoRA: Weight-Decomposed Low-Rank Adaptation|Improved LoRA by decomposing weights into direction and magnitude|https://arxiv.org/abs/2402.09353|[]
2024-02-15|Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models|as title|https://arxiv.org/abs/2402.08756|[]
2024-02-15|BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation|test time adaptation using LoRA and MoE|https://arxiv.org/abs/2402.08712|[]
2024-02-16|MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations|adding contrastive learning to MAE to boost linear probing or 1-shot learning|https://arxiv.org/abs/2402.10093|[]
2024-02-16|Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP|domain generalization of PEFT|https://arxiv.org/abs/2402.09613|[]
2024-02-16|BitDelta: Your Fine-Tune May Only Be Worth One Bit|decompose fine-tune into direction (1-bit) and a scalar|https://arxiv.org/abs/2402.10193|[]
2024-02-16|Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection|pruning neurons that leads to overfitting|https://arxiv.org/abs/2402.10062|[]
2024-02-16|Fast Vocabulary Transfer for Language Model Compression|improve efficiency by fine-tune tokenizer on downstream domain so that the tokenized sequence is shorter|https://arxiv.org/abs/2402.09977|[]
2024-02-16|Multi-Word Tokenization for Sequence Compression|improve efficiency by view multiple words as one token|https://arxiv.org/abs/2402.09949|[]
2024-01-30|SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing|freezing layers to accelerate training|https://arxiv.org/abs/2401.16720|[]
2024-02-19|PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter|tiny LM between LLaMA adapters and LLM|https://arxiv.org/abs/2402.10896|[]
2024-02-19|Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering|Video QA by captioning frames|https://arxiv.org/abs/2402.10698|[]
2024-02-19|Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond|make MLLM to do retrieval (generation) tasks|https://arxiv.org/abs/2402.10805|[]
2024-02-21|CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples|improve VLM's ability of counting and position understanding by data augmentation|https://arxiv.org/abs/2402.13254|[]
2024-02-21|Video ReCap: Recursive Captioning of Hour-Long Videos|long video captioning by hierarchical decomposing videos|https://arxiv.org/abs/2402.13250|[]
2024-02-21|VideoPrism: A Foundational Visual Encoder for Video Understanding|large video understanding model by google team|https://arxiv.org/abs/2402.13217|[]
2024-02-21|OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog|video QA requires object state understanding (?)|https://arxiv.org/abs/2402.13146|[]
2024-02-21|Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model|DA with black box API using pseudo labeling|https://arxiv.org/abs/2402.13122|[]
2024-02-21|Slot-VLM: SlowFast Slots for Video-Language Modeling|"Use slow-fast, object-centric, event-centric idea to patchify videos"|https://arxiv.org/abs/2402.13088|[]
2024-02-21|ConVQG: Contrastive Visual Question Generation with Multimodal Guidance|Use contrastive loss to generate VQA that is related to both image and text|https://arxiv.org/abs/2402.12846|[]
2024-02-21|Model Composition for Multimodal Large Language Models|combining foundation model of each domain to build a MLLM|https://arxiv.org/abs/2402.12750|[]
2024-02-21|Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering|VQA with multi-modal external knowledge source|https://arxiv.org/abs/2402.12728|[]
2024-02-21|Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition|try to learn domain-invariant physics to improve few-shot action recognition|https://arxiv.org/abs/2402.12706|[]
2024-02-21|Efficient Parameter Mining and Freezing for Continual Object Detection|continual learning by selecting parameters to freeze|https://arxiv.org/abs/2402.12624|[]
2024-02-21|Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization|MoE that is better at scaling the number of experts|https://arxiv.org/abs/2402.12550|[]
2024-02-21|Towards Cross-Domain Continual Learning|as title|https://arxiv.org/abs/2402.12490|[]
2024-02-21|Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation|teacher-agnostic knowledge distillation without original data|https://arxiv.org/abs/2402.12406|[]
2024-02-20|UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking|deal with ID switch during multi object tracking|https://arxiv.org/abs/2402.12303|[]
2024-02-20|Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers|processing long sequence by cross attention with with learnable tokens|https://arxiv.org/abs/2402.12138|[]
2024-02-20|LVCHAT: Facilitating Long Video Comprehension|enable long video by hierarchical token merging|https://arxiv.org/abs/2402.12079|[]
2024-02-22|Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation|segmentation using vision query for better generalizability|https://arxiv.org/abs/2402.13697|[]
2024-02-22|A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models|evaluate gender bias by observing output (text or image) preference|https://arxiv.org/abs/2402.13636|[]
2024-02-22|Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement|improve moment retrieval by detecting relevance in text and vision separately|https://arxiv.org/abs/2402.13576|[]
2024-02-22|Event-aware Video Corpus Moment Retrieval|improve moment retrieval by merging semantically similar scene into event|https://arxiv.org/abs/2402.13566|[]
2024-02-22|Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization|improve low precision training by considering data distribution|https://arxiv.org/abs/2402.13497|[]
2024-02-22|Unsupervised learning based object detection using Contrastive Learning|as title|https://arxiv.org/abs/2402.13465|[]
2024-02-22|Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning|use CLIP and discriminator as reward to train image captioning model|https://arxiv.org/abs/2402.13936|[]
2024-02-22|Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment|better modality alignment of VLM in addition to Q-Former|https://arxiv.org/abs/2402.13561|[]
2024-02-22|Unsupervised Concept Discovery Mitigates Spurious Correlations|improve robustness of classification by unsupervisedly clustering concepts and adjusting sample strategy|https://arxiv.org/abs/2402.13368|[]
2024-02-22|SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning|"dealing with imbalanced data withtout predefining distribution (e.g., whether long tail or not)"|https://arxiv.org/abs/2402.13505|[]
2024-02-22|LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs|make LLM deal with long video by token merging|https://arxiv.org/abs/2402.13546|[]
2024-02-20|Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before|multi-stage contrastive learning|https://arxiv.org/abs/2402.11816|[]
2024-02-20|Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models|self-consistency of VLM|https://arxiv.org/abs/2402.11622|[]
2024-02-20|CPN: Complementary Proposal Network for Unconstrained Text Detection|text detection that has irregular layout|https://arxiv.org/abs/2402.11540|[]
2024-02-20|Key Patch Proposer: Key Patches Contain Rich Information|select (without training) most important patches that minimize reconstruction loss|https://arxiv.org/abs/2402.11458|[]
2024-02-20|Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning|large video LLM using new automatic annotated large dataset|https://arxiv.org/abs/2402.11435|[]
2024-02-20|Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition|zero-shot learning by generating unseen data but use OOD detector to do regularization|https://arxiv.org/abs/2402.11424|[]
2024-02-20|Learning by Reconstruction Produces Uninformative Features For Perception|Yann LeCun argues the power of learning by reconstruction|https://arxiv.org/abs/2402.11337|[]
2024-02-20|ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition|intention driven dataset and VLM for better user experience|https://arxiv.org/abs/2402.11301|[]
2024-02-20|CoLLaVO: Crayon Large Language and Vision mOdel|using panoptic segmentation as additional clues for better zero-shot performance|https://arxiv.org/abs/2402.11248|[]
2024-02-20|A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation|UNet-like decoder for fewer computational cost segmentation|https://arxiv.org/abs/2402.11201|[]
2024-02-20|GIM: Learning Generalizable Image Matcher From Internet Videos|as title|https://arxiv.org/abs/2402.11095|[]
2024-02-20|The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test|as title|https://arxiv.org/abs/2402.11089|[]
2024-02-20|AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling|MLLM that understands and generates multimodal data|https://arxiv.org/abs/2402.12226|[]
2024-02-23|WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition|use SAM as pseudo ground truth to solve weakly supervised object detection|https://arxiv.org/abs/2402.14812|[]
2024-02-23|DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models|do detection fisrt and then zoom in to solve fine-grained VQA|https://arxiv.org/abs/2402.14767|[]
2024-02-23|Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition|adapt foundation model to downstream tasks with large domain shift (visual place recognition)|https://arxiv.org/abs/2402.14505|[]
2024-02-23|Reading Relevant Feature from Global Representation Memory for Visual Object Tracking|object tracking with memory and a module to select memory to use|https://arxiv.org/abs/2402.14392|[]
2024-02-20|The Effectiveness of Random Forgetting for Robust Generalization|reinitialize weights (forget) to mitigate attach|https://arxiv.org/abs/2402.11733|[]
2024-02-20|Aligning Modalities in Vision Large Language Models via Preference Fine-tuning|solve hallucination problem of VLM by adding noise to cause hallucinationg and guide model with ground truth|https://arxiv.org/abs/2402.11411|[]
2024-02-20|Knowledge Distillation Based on Transformed Teacher Matching|study on function of temperature used in knowledge distillation|https://arxiv.org/abs/2402.11148|[]
2024-02-23|Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning|"as title (evaluate on protein biology, chemical property prediction, and particle physics)"|https://arxiv.org/abs/2402.14789|[]
2024-02-23|OmniPred: Language Models as Universal Regressors|as title|https://arxiv.org/abs/2402.14547|[]
2024-02-23|TinyLLaVA: A Framework of Small-scale Large Multimodal Models|better data with smaller MLLM|https://arxiv.org/abs/2402.14289|[]
2024-02-23|Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model|use a intermediate teacher model between foundatioin model and student|https://arxiv.org/abs/2402.14035|[]
2024-02-26|Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding|as title|https://arxiv.org/abs/2402.15300|[]
2024-02-26|Attention-Guided Masked Autoencoders For Learning Image Representations|MAE but mask patches with high attention |https://arxiv.org/abs/2402.15172|[]
2024-02-26|Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?|combining multiple sets of LoRA|https://arxiv.org/abs/2402.15414|[]
2024-02-26|CommVQA: Situating Visual Question Answering in Communicative Contexts|VQA with communicative context|https://arxiv.org/abs/2402.15002|[]
2024-02-27|Gradient-Guided Modality Decoupling for Missing-Modality Robustness|"Missing modality when using multi-modal model, remove the conflict in gradients of different modalities"|https://arxiv.org/abs/2402.16318|[]
2024-02-27|One-stage Prompt-based Continual Learning|as title|https://arxiv.org/abs/2402.16189|[]
2024-02-27|LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding|deal with long video by making model sample relevant frames|https://arxiv.org/abs/2402.16050|[]
2024-02-27|Semi-supervised Open-World Object Detection|detection with unknown class and unlabel data|https://arxiv.org/abs/2402.16013|[]
2024-02-27|Multimodal Instruction Tuning with Conditional Mixture of LoRA|like pathway or mixture of expert using LoRA|https://arxiv.org/abs/2402.15896|[]
2024-02-27|"TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages"|as title|https://arxiv.org/abs/2402.16021|[]
2024-02-28|VRP-SAM: SAM with Visual Reference Prompt|make SAM segment according to reference (object from other pictures)|https://arxiv.org/abs/2402.17726|[]
2024-02-28|Interactive Multi-Head Self-Attention with Linear Complexity|efficient MHSA|https://arxiv.org/abs/2402.17507|[]
2024-02-28|LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning|prompt tuning while consider early block features having low level information|https://arxiv.org/abs/2402.17406|[]
2024-02-28|Scaling Supervised Local Learning with Augmented Auxiliary Networks|training with local gradient-isolated network rather than backpropagation of global loss|https://arxiv.org/abs/2402.17318|[]
2024-02-28|m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers|distillation of modular network|https://arxiv.org/abs/2402.16918|[]
2024-02-29|Gradient Reweighting: Towards Imbalanced Class-Incremental Learning|as title|https://arxiv.org/abs/2402.18528|[]
2024-02-29|Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation|weakly supervised segmentation using patch pseudo lable and contrastive learning|https://arxiv.org/abs/2402.18467|[]
2024-02-29|Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization|use text prompt to capture object feature and help generalization|https://arxiv.org/abs/2402.18447|[]
2024-02-29|Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport|try to learn a prototype of each class to do retrieval (?)|https://arxiv.org/abs/2402.18411|[]
2024-02-29|Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization|use additional masking model that take downstream validation loss into consideration|https://arxiv.org/abs/2402.18128|[]
2024-02-29|UniVS: Unified and Universal Video Segmentation with Prompts as Queries|as title|https://arxiv.org/abs/2402.18115|[]
2024-02-29|Polos: Multimodal Metric Learning from Human Feedback for Image Captioning|"new dataset with human feed back, learned based metric for captioning"|https://arxiv.org/abs/2402.18091|[]
2024-02-29|Generalizable Two-Branch Framework for Image Class-Incremental Learning|side network for continual learning|https://arxiv.org/abs/2402.18086|[]
2024-02-29|Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation|weakly supervised segmentation with refined patch-level pseudo label|https://arxiv.org/abs/2402.17891|[]
2024-02-29|REPrune: Channel Pruning via Kernel Representative Selection|"pruning for CNN according to ""maximum cluster coverage problem"""|https://arxiv.org/abs/2402.17862|[]
2024-02-29|Classes Are Not Equal: An Empirical Study on Image Recognition Fairness|as title|https://arxiv.org/abs/2402.18133|[]
2024-03-01|Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models|DeepMind's new attempt for efficient LM|https://arxiv.org/abs/2402.19427|[]
2024-03-01|Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning|about continual learning and catastrophic forgetting in PEFT|https://arxiv.org/abs/2402.18865|[]
2024-03-01|Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains|fixed classifier according to Equiangular Tight Frame simplex|https://arxiv.org/abs/2402.18614|[]
2024-03-01|FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning|System that can do inference and parameter-efficient finetuning requests in the same iteration|https://arxiv.org/abs/2402.18789|[]
2024-03-01|Learning to Compress Prompt in Natural Language Formats|as title|https://arxiv.org/abs/2402.18700|[]
2024-03-01|Simple linear attention language models balance the recall-throughput tradeoff|linear and sliding-window attention for efficient LLM|https://arxiv.org/abs/2402.18668|[]
2024-02-29|SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization|theoretically understand pruning|https://arxiv.org/abs/2402.17902|[]
2024-02-29|DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation|as title|https://arxiv.org/abs/2402.17812|[]
2024-02-29|HOP to the Next Tasks and Domains for Continual Learning in NLP|"specialized MLP for each problem, use statistical moments to capture information"|https://arxiv.org/abs/2402.18449|[]
2024-02-29|Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation|generate NLP dataset using unannotated text and task attributes|https://arxiv.org/abs/2402.18334|[]
2024-03-01|Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers|as title|https://arxiv.org/abs/2402.19479|[]
2024-03-01|PEM: Prototype-based Efficient MaskFormer for Image Segmentation|"restrict attention region for efficiency, using feature pyramid"|https://arxiv.org/abs/2402.19422|[]
2024-03-01|VIXEN: Visual Text Comparison Network for Image Difference Captioning|"summarize difference between 2 images, use synthesized data and GPT4 to do augmentation"|https://arxiv.org/abs/2402.19119|[]
2024-03-01|VideoMAC: Video Masked Autoencoders Meet ConvNets|video MAE using ConvNet which reduces required data size|https://arxiv.org/abs/2402.19082|[]
2024-03-01|Debiased Novel Category Discovering and Localization|detection taking unknown objects into account|https://arxiv.org/abs/2402.18821|[]
2024-03-01|Motion Guided Token Compression for Efficient Masked Video Modeling|use variance between patches to detect motion and mask still patches|https://arxiv.org/abs/2402.18577|[]
2024-03-01|"CAMixerSR: Only Details Need More ""Attention"""|predict which patch need to be processed by attention|https://arxiv.org/abs/2402.19289|[]
2024-03-01|Rethinking Multi-domain Generalization with A General Learning Objective|design objective that learn domain-independent feature|https://arxiv.org/abs/2402.18853|[]
2024-03-04|Can Transformers Capture Spatial Relations between Objects?|long range attention to improve spatial relation capability|https://arxiv.org/abs/2403.00729|[]
2024-03-04|Tri-Modal Motion Retrieval by Learning a Joint Embedding Space|"text, video, motion retrieval"|https://arxiv.org/abs/2403.00691|[]
2024-03-04|VisionLLaMA: A Unified LLaMA Interface for Vision Tasks|"propse ViT architecture similar to LLaMA (e.g., with 2DRoPE)"|https://arxiv.org/abs/2403.00522|[]
2024-03-04|Learning and Leveraging World Models in Visual Representation Learning|"using world model, a techinique of RL to do representation learning"|https://arxiv.org/abs/2403.00504|[]
2024-03-04|TempCompass: Do Video LLMs Really Understand Videos?|video LLM benchmark|https://arxiv.org/abs/2403.00476|[]
2024-03-04|Invariant Test-Time Adaptation for Vision-Language Model Generalization|"test time adaptation, using SAM to find foreground and background, and using mask to prevent shortcut"|https://arxiv.org/abs/2403.00376|[]
2024-03-04|Task Indicating Transformer for Task-conditional Dense Predictions|segmentation for different purpose using Task Gate Decoder|https://arxiv.org/abs/2403.00327|[]
2024-03-04|Multi-modal Attribute Prompting for Vision-Language Models|increase CLIP generalizability by alignment visual and text attribute|https://arxiv.org/abs/2403.00219|[]
2024-03-04|Few-Shot Relation Extraction with Hybrid Visual Evidence|detect objects apearing in caption and find their relation|https://arxiv.org/abs/2403.00724|[]
2024-03-04|Rethinking The Uniformity Metric in Self-Supervised Learning|objectives that improve SSL and prevent dimension collapse theoretically|https://arxiv.org/abs/2403.00642|[]
2024-03-05|RegionGPT: Towards Region Understanding Vision Language Model|using patch merging and spatial region embedding|https://arxiv.org/abs/2403.02330|[]
2024-03-05|Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training|observing result when mask RoI to reduce hallucination|https://arxiv.org/abs/2403.02325|[]
2024-03-05|Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures|as title|https://arxiv.org/abs/2403.02308|[]
2024-03-05|Non-autoregressive Sequence-to-Sequence Vision-Language Models|predetermined output lengths of learnable tokens; input info as KV in cross attention|https://arxiv.org/abs/2403.02249|[]
2024-03-05|Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations|data and benchmark for multi-person interaction understanding|https://arxiv.org/abs/2403.02090|[]
2024-03-05|VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT|VTG using image captioning and finding similarity between query and caption|https://arxiv.org/abs/2403.02076|[]
2024-03-05|A Generative Approach for Wikipedia-Scale Visual Entity Recognition|"classification with 6M class, assign ""code"" to each class and use generative model to predict code"|https://arxiv.org/abs/2403.02041|[]
2024-03-05|Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification|general image augmentation aiming for robustness|https://arxiv.org/abs/2403.01944|[]
2024-03-05|xT: Nested Tokenization for Larger Context in Large Images|"divide image into sub-region before patchify, better local & global feature"|https://arxiv.org/abs/2403.01915|[]
2024-03-05|AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation|regularized by reconstruction labeled feature from pseudo labeled feature|https://arxiv.org/abs/2403.01818|[]
2024-03-05|Training-Free Pretrained Model Merging|mergine according activatioin and weight similarity|https://arxiv.org/abs/2403.01753|[]
2024-03-05|Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection|new task configuration|https://arxiv.org/abs/2403.01680|[]
2024-03-05|Logit Standardization in Knowledge Distillation|design on temperature to adjust target of student networks|https://arxiv.org/abs/2403.01427|[]
2024-03-05|Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning|use text encoder of CLIP and prompting LLM to replace image data|https://arxiv.org/abs/2403.01209|[]
2024-03-06|Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization|use two teachers to validate the pseudo label is reliable|https://arxiv.org/abs/2403.03145|[]
2024-03-06|Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization|use two model to validate the pseudo label is reliable|https://arxiv.org/abs/2403.03095|[]
2024-03-06|Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models|two pathway for different resolution of images|https://arxiv.org/abs/2403.03003|[]
2024-03-06|MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer|use cross-modal similarity as clue to pruning tokens in each layer|https://arxiv.org/abs/2403.02991|[]
2024-03-06|Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception|concatenating MLLM and a segmentation decoder|https://arxiv.org/abs/2403.02969|[]
2024-03-06|Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples|"hard negative mining by swapping predefine keywords (e.g., color, size...)"|https://arxiv.org/abs/2403.02875|[]
2024-03-06|PromptKD: Unsupervised Prompt Distillation for Vision-Language Models|distillation into learnable prompt token (like distillation + prompt tuning)|https://arxiv.org/abs/2403.02781|[]
2024-03-06|DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization|as title|https://arxiv.org/abs/2403.02714|[]
2024-03-07|MeaCap: Memory-Augmented Zero-shot Image Captioning|using CLIP to retrieve related text and refine captions|https://arxiv.org/abs/2403.03715|[]
2024-03-07|Enhancing Vision-Language Pre-training with Rich Supervisions|seeking supervision website source code|https://arxiv.org/abs/2403.03346|[]
2024-03-08|Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation|try to save distribution of previous task by saving hard negative using contrastive loss|https://arxiv.org/abs/2403.04599|[]
2024-03-08|CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?|"explore (mainly gender) bias in CLIP, define several metrics for bias"|https://arxiv.org/abs/2403.04547|[]
2024-03-08|Aligners: Decoupling LLMs and Alignment|"avoid training whole LLM, train additional module for multiple LLM"|https://arxiv.org/abs/2403.04224|[]
2024-03-07|GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection|memory efficient whole model training by decompose gradient|https://arxiv.org/abs/2403.03507|[]
2024-03-07|The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models|study on sub-network with same ID performance but better OOD performance|https://arxiv.org/abs/2403.03942|[]
2024-03-07|Learning to Decode Collaboratively with Multiple Language Models|ensemble of LLMs on token likelihodd level|https://arxiv.org/abs/2403.03870|[]
2024-02-01|Repeat After Me: Transformers are Better than State Space Models at Copying|argue that Transformer may be better than SSM on copying and retrieving|https://arxiv.org/abs/2402.01032|[]
2024-03-08|SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM|VQA dataset with fine-grain objects and prposed baseline (using augmentation)|https://arxiv.org/abs/2403.04735|[]
2024-03-08|Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level|kernel fusion of neighborhood attention|https://arxiv.org/abs/2403.04690|[]
2024-03-08|CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios|"audio visual LM on audio visual QA, audio visual instruction dataset"|https://arxiv.org/abs/2403.04640|[]
2024-03-08|LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking|PEFT combining LoRA and parameter sharing|https://arxiv.org/abs/2403.04303|[]
2024-03-08|LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition|improve contrastive learning by masking|https://arxiv.org/abs/2403.04066|[]
2024-03-08|A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition|"propose Modality Bias Hypothesis, using knowledge distillation to make model robust to missing frame and modality"|https://arxiv.org/abs/2403.04245|[]
2024-03-11|"Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos"|use image caption to obtain pseudo label for DA|https://arxiv.org/abs/2403.05535|[]
2024-03-11|Attention-guided Feature Distillation for Semantic Segmentation|as title|https://arxiv.org/abs/2403.05451|[]
2024-03-11|VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model|use VLM to refine or valide pseudo label|https://arxiv.org/abs/2403.05346|[]
2024-03-11|Debiasing Large Visual Language Models|use meaningless image to calibrate bias from LLM|https://arxiv.org/abs/2403.05262|[]
2024-03-11|Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval|use intra-modality similarity as supervision of inter-modality similarity|https://arxiv.org/abs/2403.05261|[]
2024-03-11|Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval|rematch mismatched pairs using optimal transport to calibrate data|https://arxiv.org/abs/2403.05105|[]
2024-03-11|Agile Multi-Source-Free Domain Adaptation|parameter and throughput efficient|https://arxiv.org/abs/2403.05062|[]
2024-03-11|Poly-View Contrastive Learning|"contrastive learning with more than paired view, reduce required batch size"|https://arxiv.org/abs/2403.05490|[]
2024-03-11|Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization|"learn domain-invariant prototype to do pseudo labeling, utilize labeled paired to make model robust to pseudo label"|https://arxiv.org/abs/2403.05209|[]
2024-03-11|Denoising Autoregressive Representation Learning|Causal transformer for images to do next patch prediction (denoising)|https://arxiv.org/abs/2403.05196|[]
2024-03-12|Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling|"Prompt tuning for video tasks, insert tokens in KV rather than input sequence"|https://arxiv.org/abs/2403.06978|[]
2024-03-12|VideoMamba: State Space Model for Efficient Video Understanding|"Mamba for video, MAE with CLIP as target"|https://arxiv.org/abs/2403.06977|[]
2024-03-12|Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation|further disentangle CLIP feature using orthogonal loss|https://arxiv.org/abs/2403.06946|[]
2024-03-12|LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations|use original image as anchor to prevent images after augmentation become negative pair|https://arxiv.org/abs/2403.06813|[]
2024-03-12|An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models|token pruning for VLM acceleration|https://arxiv.org/abs/2403.06764|[]
2024-03-12|Answering Diverse Questions via Text Attached with Key Audio-Visual Clues|"audio visual QA, utilize knowledge distiall for alignment"|https://arxiv.org/abs/2403.06679|[]
2024-03-12|OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation|"unsupervised segmentation using cluster and optimal transport, no pseudo label is used"|https://arxiv.org/abs/2403.06546|[]
2024-03-12|VkD:  Improving Knowledge Distillation using Orthogonal Projections|as title|https://arxiv.org/abs/2403.06213|[]
2024-03-12|RESTORE: Towards Feature Shift for Vision-Language Prompt Learning|text and image alignment of CLIP for better novel class generalizability|https://arxiv.org/abs/2403.06136|[]
2024-03-12|ClickVOS: Click Video Object Segmentation|segmentation mask translation starting with one point per object (during inference)|https://arxiv.org/abs/2403.06130|[]
2024-03-12|In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model|test time adaptation using prompt tuning with CLIP|https://arxiv.org/abs/2403.06126|[]
2024-03-12|Multisize Dataset Condensation|reduce dataset size for efficient training|https://arxiv.org/abs/2403.06075|[]
2024-03-12|Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning|test time adaptation by prompt tuning and estimation of distribution|https://arxiv.org/abs/2403.06059|[]
2024-03-12|CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot Learning|attribute and object disentangle using cycle consistency|https://arxiv.org/abs/2403.05924|[]
2024-03-12|LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content|prompt GPT4V as supervision for long-tail data|https://arxiv.org/abs/2403.05854|[]
2024-03-12|Augmentations vs Algorithms: What Works in Self-Supervised Learning|"argue that augmentation is much important than algorthm in contrastive learning, propose a unified framework to experiment"|https://arxiv.org/abs/2403.05726|[]
2024-03-13|Beyond Text: Frozen Large Language Models in Visual Signal Comprehension|translate images to natural language|https://arxiv.org/abs/2403.07874|[]
2024-03-13|Distilling the Knowledge in Data Pruning|data condensation using knowledge distillation|https://arxiv.org/abs/2403.07854|[]
2024-03-13|MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric|pruning for VLM|https://arxiv.org/abs/2403.07839|[]
2024-03-13|Multi-modal Auto-regressive Modeling via Visual Words|map image patches to distribution over dictionary and treat it as text|https://arxiv.org/abs/2403.07720|[]
2024-03-13|Masked AutoDecoder is Effective Multi-Task Vision Generalist|use causal decoder to make vision model to do multiple tasks|https://arxiv.org/abs/2403.07692|[]
2024-03-13|Unified Source-Free Domain Adaptation|problem framework of domain adaptation|https://arxiv.org/abs/2403.07601|[]
2024-03-13|Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors|test time adaptation by observing results before and after augmentation|https://arxiv.org/abs/2403.07366|[]
2024-03-13|Open-World Semantic Segmentation Including Class Similarity|use post-process to cluster unknow class|https://arxiv.org/abs/2403.07532|[]
2024-03-13|MoAI: Mixture of All Intelligence for Large Language and Vision Models|make use of multiple visual feature|https://arxiv.org/abs/2403.07508|[]
2024-03-13|Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models|decouple visual capability into task-agnostic and task-specific|https://arxiv.org/abs/2403.07304|[]
2024-03-13|IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers|quantization can be used for training and inference|https://arxiv.org/abs/2403.07339|[]
2024-03-14|DAM: Dynamic Adapter Merging for Continual Video QA Learning|Merge domain specific adapter to do continual VQA learning|https://arxiv.org/abs/2403.08755|[]
2024-03-14|Consistent Prompting for Rehearsal-Free Continual Learning|"multiple learable prompt and classifier for different domain, use cross domain prediction smoothness as regularization and auxiliary loss"|https://arxiv.org/abs/2403.08568|[]
2024-03-14|Cross-modality debiasing: using language to mitigate sub-population shifts in imaging|as title|https://arxiv.org/abs/2403.07888|[]
2024-03-14|Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection|increase pretraining speed by removing patches based on similarity to text|https://arxiv.org/abs/2403.07883|[]
2022-05-24|Large Language Models are Zero-Shot Reasoners||https://arxiv.org/abs/2205.11916|[]
2022-05-21|Least-to-Most Prompting Enables Complex Reasoning in Large Language Models||https://arxiv.org/abs/2205.10625|[]
2021-06-30|Attention Bottlenecks for Multimodal Fusion||https://arxiv.org/abs/2107.00135|[]
2019-04-16|Co-Separating Sounds of Visual Objects||https://arxiv.org/abs/1904.07750|[]
2020-11-03|Learning Representations from Audio-Visual Spatial Alignment||https://arxiv.org/abs/2011.01819|[]
2023-03-30|Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment||https://arxiv.org/abs/2303.17490|[]
2022-03-30|MAE-AST: Masked Autoencoding Audio Spectrogram Transformer||https://arxiv.org/abs/2203.16691|[]
2022-07-13|Masked Autoencoders that Listen||https://arxiv.org/abs/2207.06405|[]
2017-11-30|A Closer Look at Spatiotemporal Convolutions for Action Recognition||https://arxiv.org/abs/1711.11248|[]
2021-03-10|VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples||https://arxiv.org/abs/2103.05905|[]
2020-10-19|Self-supervised Co-training for Video Representation Learning||https://arxiv.org/abs/2010.09709|[]
2021-04-22|Multiscale Vision Transformers||https://arxiv.org/abs/2104.11227|[]
2022-03-03|BEVT: BERT Pretraining of Video Transformers||https://arxiv.org/abs/2112.01529|[]
2022-04-06|ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound||https://arxiv.org/abs/2204.02874|[]
2023-04-10|Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition||https://arxiv.org/abs/2304.04704|[]
2022-04-14|Masked Siamese Networks for Label-Efficient Learning||https://arxiv.org/abs/2204.07141|[]
2022-10-17|Token Merging: Your ViT But Faster||https://arxiv.org/abs/2210.09461|[]
2021-11-03|VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts||https://arxiv.org/abs/2111.02358|[]
2022-05-04|CoCa: Contrastive Captioners are Image-Text Foundation Models||https://arxiv.org/abs/2205.01917|[]
2023-06-01|StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners||https://arxiv.org/abs/2306.00984|[]
2022-06-16|MixGen: A New Multi-Modal Data Augmentation||https://arxiv.org/abs/2206.08358|[]
2023-04-24|A Cookbook of Self-Supervised Learning||https://arxiv.org/abs/2304.12210|[]
2022-05-21|Self-Supervised Speech Representation Learning: A Review||https://arxiv.org/abs/2205.10643|[]
2022-05-18|Masked Autoencoders As Spatiotemporal Learners||https://arxiv.org/abs/2205.09113|[]
2021-06-15|BEiT: BERT Pre-Training of Image Transformers||https://arxiv.org/abs/2106.08254|[]
2020-02-13|Self-supervised learning for audio-visual speaker diarization||https://arxiv.org/abs/2002.05314|[]
2019-01-27|Augment your batch: better training with larger batches||https://arxiv.org/abs/1901.09335|[]
2019-06-19|XLNet: Generalized Autoregressive Pretraining for Language Understanding||https://arxiv.org/abs/1906.08237|[]
2017-11-14|Decoupled Weight Decay Regularization||https://arxiv.org/abs/1711.05101|[]
2021-12-16|Masked Feature Prediction for Self-Supervised Visual Pre-Training||https://arxiv.org/abs/2112.09133|[]
2024-03-15|OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning|use prompt tuning to tackle various additional information|https://arxiv.org/abs/2403.09634|[]
2024-03-15|Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding|evaluation of video Mamba|https://arxiv.org/abs/2403.09626|[]
2024-03-15|"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"|30B LM from Apple|https://arxiv.org/abs/2403.09611|[]
2024-03-15|GiT: Towards Generalist Vision Transformer through Universal Language Interface|Multimodal LM without concatenating encoder and LLM|https://arxiv.org/abs/2403.09394|[]
2024-03-15|LocalMamba: Visual State Space Model with Windowed Selective Scan|as title|https://arxiv.org/abs/2403.09338|[]
2024-03-15|Are Vision Language Models Texture or Shape Biased and Can We Steer Them?|compare bias in vision encoder and corresponding VLM|https://arxiv.org/abs/2403.09193|[]
2024-03-15|PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation|PEFT + token pruning to achieve inference and training efficiency|https://arxiv.org/abs/2403.09192|[]
2024-03-18|Frozen Feature Augmentation for Few-Shot Image Classification|emperical study on augmentation of few shot linear probing|https://arxiv.org/abs/2403.10519|[]
2024-03-18|VideoAgent: Long-form Video Understanding with Large Language Model as Agent|"use CLIP to retrieve, use VLM to validate the candidate"|https://arxiv.org/abs/2403.10517|[]
2024-03-18|CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning|remove bias from imbalanced data making prediction on no-pattern images neutral|https://arxiv.org/abs/2403.10391|[]
2024-03-18|Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models|concatenate YOLO GPT4V SAM to do few shot dense prediction|https://arxiv.org/abs/2403.10287|[]
2024-03-18|HawkEye: Training Video-Text LLMs for Grounding Text in Videos|large scale video text dataset|https://arxiv.org/abs/2403.10228|[]
2024-03-18|Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt|disentangle learable prompts into task invariant (initialized from base classes) and task specific|https://arxiv.org/abs/2403.09857|[]
2024-03-18|Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks|"when fine-tune downsstream tasks, adaptive update class distribution, minimize intra and maximize inter distancce"|https://arxiv.org/abs/2403.10097|[]
2024-03-18|Uni-SMART: Universal Science Multimodal Analysis and Research Transformer|"Multimodal LM that can process table, chart, chemical reactions, ..."|https://arxiv.org/abs/2403.10301|[]
2024-03-18|EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba|as title|https://arxiv.org/abs/2403.09977|[]
2024-03-18|Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers|as title|https://arxiv.org/abs/2403.10030|[]
2024-03-18|Knowledge Condensation and Reasoning for Knowledge-based VQA|"VQA with external knowledge passage, summary knowledge before feed to models"|https://arxiv.org/abs/2403.10037|[]
2024-03-19|Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning|"New adapters for new class, and construct prototype to prevent forgetting"|https://arxiv.org/abs/2403.12030|[]
2024-03-19|Align and Distill: Unifying and Improving Domain Adaptive Object Detection|new benchmark and baseline for DA object detection|https://arxiv.org/abs/2403.12029|[]
2024-03-19|"FlexCap: Generating Rich, Localized, and Flexible Captions in Images"|"deepmind's length, bbox conditioned captioning model using new dataset"|https://arxiv.org/abs/2403.12026|[]
2024-03-19|SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules|try to summarize and unify mechanisms of LoRA variants|https://arxiv.org/abs/2403.11887|[]
2024-03-19|Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation|PEFT + token pruning to achieve inference and training efficiency|https://arxiv.org/abs/2403.11808|[]
2024-03-19|Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs|APE to enhance visual recognition|https://arxiv.org/abs/2403.11755|[]
2024-03-19|Towards Generalizing to Unseen Domains with Few Labels|Semi-supervise domain adaptation|https://arxiv.org/abs/2403.11674|[]
2024-03-19|Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters|use MoE and routing mechanism to deal with ID and OOD data|https://arxiv.org/abs/2403.11549|[]
2024-03-19|Semantic Prompting with Image-Token for Continual Learning|extract task agnostic feature to improve generalizibility|https://arxiv.org/abs/2403.11537|[]
2024-03-19|Do CLIPs Always Generalize Better than ImageNet Models?|argue the robustness of CLIP|https://arxiv.org/abs/2403.11497|[]
2024-03-19|Towards Generalizing to Unseen Domains with Few Labels|semi-supervise domain adaptation|https://arxiv.org/abs/2403.11674|[]
2024-03-19|VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding|as title|https://arxiv.org/abs/2403.11481|[]
2024-03-19|Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding|video grounding without timestamp label|https://arxiv.org/abs/2403.11463|[]
2024-03-19|Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration|"continual learning with missing modality, solving with PEFT"|https://arxiv.org/abs/2403.11373|[]
2024-03-19|SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant|Try to generate high quality QA to do instruction tuning|https://arxiv.org/abs/2403.11299|[]
2024-03-19|MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts|"besides static soft prompt, learn a module to generate dynamic soft prompt based on other modality"|https://arxiv.org/abs/2403.10568|[]
2024-03-19|Self-Supervised Quantization-Aware Knowledge Distillation|full precision teacher distill to low precision students|https://arxiv.org/abs/2403.11106|[]
2024-03-19|Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches|Comparing mapping images to LLM's embedding space and to natural language captions|https://arxiv.org/abs/2403.11317|[]
2024-03-19|PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation|use gradient to determine which layer to train during test time adaptation|https://arxiv.org/abs/2403.10650|[]
2024-03-19|: Source-free Domain Adaptation Through the Lens of Data Augmentation|"do cluster in embedding space, regularization is inspired by augmentation"|https://arxiv.org/abs/2403.10834|[]
2024-03-19|Rethinking Multi-view Representation Learning via Distilled Disentangling|"learn view-independent feature by cross-view MAE, then learn view-specific feature by disentangle with learnt view-independent feature"|https://arxiv.org/abs/2403.10897|[]
2024-03-19|Task-Aware Low-Rank Adaptation of Segment Anything Model|decompose LoRA into three component (one additional for different tasks)|https://arxiv.org/abs/2403.10971|[]
2024-03-19|Audio-Visual Segmentation via Unlabeled Frame Exploitation|"Self-training for AV segmentation, use optical flow for frames near labeled frames, teacher model for distant unlabeled frames"|https://arxiv.org/abs/2403.11074|[]
2024-03-19|Self-supervised co-salient object detection via feature correspondence at multiple scales|"segmentation co-occurance objects in a set of images in SSL manner, use attention weight to get object masks, and find co-occurance object using feature similarity"|https://arxiv.org/abs/2403.11107|[]
2024-03-19|Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence|modified self- and cross attention to do dense matching|https://arxiv.org/abs/2403.11120|[]
2024-03-19|DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation|two networks that generate pseudo labels for each other|https://arxiv.org/abs/2403.11184|[]
2024-03-19|TAG: Guidance-free Open-Vocabulary Semantic Segmentation|"use DINO to do segmentation, us CLIP to retrieve text and mergin the segmentation according to similarity to text"|https://arxiv.org/abs/2403.11197|[]
2024-03-19|Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias|"DA setting where target domain may not cover all class of source domain, and may contain new classes"|https://arxiv.org/abs/2403.11234|[]
2024-03-20|Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models|"let VLM output RoI first, and then ask the question again with the RoI image"|https://arxiv.org/abs/2403.12966|[]
2024-03-20|Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models|"PEFT, try to predict what the image is and is not simultaneously"|https://arxiv.org/abs/2403.12964|[]
2024-03-20|Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments|"observing top-2 prediction to identify confiusing pair, then do correction"|https://arxiv.org/abs/2403.12883|[]
2024-03-20|Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models|Test time adaptation by estimating shift between training and testing data prototype generated by text encoder|https://arxiv.org/abs/2403.12952|[]
2024-03-20|Confidence Self-Calibration for Multi-Label Class-Incremental Learning|prevent overfitting on new class by entropy regularization|https://arxiv.org/abs/2403.12559|[]
2024-03-20|UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All|Use text embedding center as anchor rather than image as in ImageBind|https://arxiv.org/abs/2403.12532|[]
2024-03-20|Towards Multimodal In-Context Learning for Vision & Language Models|"Verify off-the-shelf VLM can't do in context learning, using fine-tuning to enable them"|https://arxiv.org/abs/2403.12736|[]
2024-03-20|DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM|"overlapping information (e.g., ruler) on image can improve detection ability of GPT 4V"|https://arxiv.org/abs/2403.12488|[]
2024-03-20|CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation|as title|https://arxiv.org/abs/2403.12455|[]
2024-03-20|Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity|"using less data to train CLIP, condensation according to covariance"|https://arxiv.org/abs/2403.12267|[]
2024-03-20|Non-negative Contrastive Learning|non-negative constraint on feature space make feature sparse and interpretable|https://arxiv.org/abs/2403.12459|[]
2024-03-20|Do Generated Data Always Help Contrastive Learning?|study on relation between synthesized data and data augmentation|https://arxiv.org/abs/2403.12448|[]
2024-03-21|On Pretraining Data Diversity for Self-Supervised Learning|"although diversity may improve performance, problem of domain shift is more crucial"|https://arxiv.org/abs/2403.13808|[]
2024-03-21|SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning|"clustering unlabled data, refine the representation by contrastive loss"|https://arxiv.org/abs/2403.13684|[]
2024-03-21|VL-Mamba: Exploring State Space Models for Multimodal Learning|"replace LLaMA with Mamba, explore scan mechanism"|https://arxiv.org/abs/2403.13600|[]
2024-03-21|What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models|"use GPT4V to generate counterfactual keywords, to preven hallucination"|https://arxiv.org/abs/2403.13513|[]
2024-03-21|Scale Decoupled Distillation|use multi-scale pooling to distill fine-grained logits rather than one global logit|https://arxiv.org/abs/2403.13512|[]
2024-03-21|Improved Baselines for Data-efficient Perceptual Augmentation of LLMs|"evaluate interfaces of MLM, propose data efficient interfance"|https://arxiv.org/abs/2403.13499|[]
2024-03-21|Counting Network for Learning from Majority Label|label is given at bag-level rather than instance level|https://arxiv.org/abs/2403.13370|[]
2024-03-21|vid-TLDR: Training Free Token merging for Light-weight Video Transformer|Merge tokens according to attention map in video transformer|https://arxiv.org/abs/2403.13347|[]
2024-03-21|PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns|benchmark of abstract VQA|https://arxiv.org/abs/2403.13315|[]
2024-03-21|Rotary Position Embedding for Vision Transformer|RoPE can benefit ViT especially on high resolution images|https://arxiv.org/abs/2403.13298|[]
2024-03-21|SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models|Use self-consistency (location and description) and reinforcement learning to improve detection|https://arxiv.org/abs/2403.13263|[]
2024-03-21|When Do We Not Need Larger Vision Models?|argue that scaling resolution may surpass scaling model size|https://arxiv.org/abs/2403.13043|[]
2024-03-21|BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning|balance feature distribution using contrastive learning|https://arxiv.org/abs/2403.12986|[]
2024-03-21|Bridge the Modality and Capacity Gaps in Vision-Language Model Selection|try to select best VLM base on the label set (text) of target dataset|https://arxiv.org/abs/2403.13797|[]
2024-03-21|HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models|use module generated by HyperNetwork to project visual feature to text-like token|https://arxiv.org/abs/2403.13447|[]
2024-03-21|A Unified and General Framework for Continual Learning|"unified framework of regularization-, Bayesian-, memory-based methods, propose refresh learning (unlearn then relearn)"|https://arxiv.org/abs/2403.13249|[]
2024-03-21|RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition|"use CLIP to retrieve, use VLM to ranking, improve zero-shot ability"|https://arxiv.org/abs/2403.13805|[]
2024-03-21|Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments|"study on relation between bbox stability and accuracy, evaluate detector without ground truth"|https://arxiv.org/abs/2403.13803|[]
2024-03-22|Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification|"two new measure to quantify bias, use augmentation to reduce bias"|https://arxiv.org/abs/2403.13925|[]
2024-03-22|How to be fair? A study of label and selection bias|"theoretically understan bias, debias methods and their relation"|https://arxiv.org/abs/2403.14282|[]
2024-03-22|AI and Memory Wall|"show that in the modern architecture, DRAM rather than FLOPs is the bottleneck"|https://arxiv.org/abs/2403.14123|[]
2024-03-22|Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey|survey of PEFT|https://arxiv.org/abs/2403.14608|[]
2024-03-26|Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models|use optional memory to enhance (training free) few-shot adaptation|https://arxiv.org/abs/2403.17589|[]
2024-03-26|The Unreasonable Ineffectiveness of the Deeper Layers|emperical study on standard layer pruning + recovering strategy|https://arxiv.org/abs/2403.17887|[]
2024-03-26|Multi-Task Dense Prediction via Mixture of Low-Rank Experts|MoE (PEFT) for dense prediction|https://arxiv.org/abs/2403.17749|[]
2024-03-26|DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning|"continual learning but can not store previous data, use underfitting network with compensation to prevent forgettng"|https://arxiv.org/abs/2403.17503|[]
2024-03-25|If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions|"align LLM to CLIP, so that LLM can output high similarity description, and then analyze these description"|https://arxiv.org/abs/2403.16442|[]
2024-03-25|LLMs Are Few-Shot In-Context Low-Resource Language Learners|study on shortcoming of in-context learning on low-resourse domain|https://arxiv.org/abs/2403.16512|[]
2024-03-25|DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization|use text prompts to simulate unseen domain without access to images|https://arxiv.org/abs/2403.16697|[]
2024-03-25|Understanding Long Videos in One Multimodal Language Model Pass|use frame selection to help VLM process long video|https://arxiv.org/abs/2403.16998|[]
2024-03-25|DreamLIP: Language-Image Pre-training with Long Captions|train CLIP with longer and multiple captions|https://arxiv.org/abs/2403.17007|[]
2024-03-25|Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models|make LLM able to retrieve multimodal data|https://arxiv.org/abs/2403.17359|[]
2024-03-25|One-Shot Domain Incremental Learning|as title|https://arxiv.org/abs/2403.16707|[]
2024-03-25|LLMs Are Few-Shot In-Context Low-Resource Language Learners|evaluate different in context learning format for low resourse language|https://arxiv.org/abs/2403.16512|[]
2024-03-25|Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models|visual chain of thought dataset and benchmark|https://arxiv.org/abs/2403.16999|[]
2024-03-27|Efficient Test-Time Adaptation of Vision-Language Models|"training free test time adaptation, maintain two queue that will affect prediction distribution"|https://arxiv.org/abs/2403.18293|[]
2024-03-27|Toward Interactive Regional Understanding in Vision-Large Language Models|user indicated region aware VLM|https://arxiv.org/abs/2403.18260|[]
2024-03-27|An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM|put frames in grid and treat it as 1 image|https://arxiv.org/abs/2403.18406|[]
2024-03-27|Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation|knowledge distillation between adapter modules|https://arxiv.org/abs/2403.18804|[]
2024-03-27|Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models|"multi-resolution, patch info mining visual encoder, concatenate LLM and text to image model for any to any VLM"|https://arxiv.org/abs/2403.18814|[]
2024-03-27|CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models|"use probablistic model (mean, std of normal distribution) for robust continual learning"|https://arxiv.org/abs/2403.19137|[]
2024-03-27|Dense Vision Transformer Compression with Few Samples|compression of ViT by dropping attention and shrinking MLP in few shot scenario|https://arxiv.org/abs/2403.18708|[]
2024-03-27|Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation|use Mamba for audio. divided audio in to chucks for local and global processing|https://arxiv.org/abs/2403.18257|[]
2024-03-27|Few-Shot Recalibration of Language Models|train recalibration model to evaluate confidence and precision curve of LLM|https://arxiv.org/abs/2403.18286|[]
2024-03-27|Towards Non-Exemplar Semi-Supervised Class-Incremental Learning|semi supervised prototype and contrastive learning to avoid forgetting|https://arxiv.org/abs/2403.18291|[]
2024-03-27|ViTAR: Vision Transformer with Any Resolution|token mergin and modified positional encoding for dynamic resolution|https://arxiv.org/abs/2403.18361|[]
2024-03-27|Debiasing Sentence Embedders through Contrastive Word Pairs|as title|https://arxiv.org/abs/2403.18555|[]
2024-03-27|SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens|generate draft token and verify to speed up LLM|https://arxiv.org/abs/2403.18647|[]
2024-03-27|Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding|use prompt to exacerbate hallucinated concept for easier detection|https://arxiv.org/abs/2403.18715|[]
2024-03-27|Projective Methods for Mitigating Gender Bias in Pre-trained Language Models|apply tranditional debiasing method to BERT|https://arxiv.org/abs/2403.18803|[]
2024-03-27|Measuring Political Bias in Large Language Models: What Is Said and How It Is Said|as title|https://arxiv.org/abs/2403.18932|[]
2024-03-27|LITA: Language Instructed Temporal-Localization Assistant|temporal reasoning localization dataset|https://arxiv.org/abs/2403.19046|[]
2024-03-27|Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach|"similar to LoRA, using SVD"|https://arxiv.org/abs/2403.19067|[]
2024-03-27|FACTOID: FACtual enTailment fOr hallucInation Detection|hallucination benchmark for LLM|https://arxiv.org/abs/2403.19113|[]
2024-03-27|Compressing Large Language Models by Streamlining the Unimportant Layer|layer pruning and layer replacement for LLM|https://arxiv.org/abs/2403.19135|[]
2024-03-28|Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment|only has ordered list of action when training, improve pseudo label efficiency|https://arxiv.org/abs/2403.19225|[['cs.CV'], ['CVPR']]
2024-03-28|CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection|tackle class imbalance problem in DA, learning class relation and do augmentation|https://arxiv.org/abs/2403.19278|[['cs.CV'], ['CVPR']]
2024-03-28|Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality|multimodal narratives caption for long video, augmentation to handl missing modality|https://arxiv.org/abs/2403.19221|[['cs.CV']]
2024-03-28|Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models|benchmark and baseline (decide whether to zoom in) for VLM to handle high resolution, text rich images|https://arxiv.org/abs/2403.19322|[['cs.CV']]
2024-03-28|Checkpoint Merging via Bayesian Optimization in LLM Pretraining|study on merging LLM|https://arxiv.org/abs/2403.19390|[['cs.CL']]
2024-03-28|Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition|dynamically use cross attention to prevent effect between low valence modality|https://arxiv.org/abs/2403.19554|[['cs.CV']]
2024-03-28|LocCa: Visual Pretraining with Location-aware Captioners|as title|https://arxiv.org/abs/2403.19596|[['cs.CV']]
2024-03-28|Siamese Vision Transformers are Scalable Audio-visual Learners|same ViT to process audio and visual, pretrained with contrastive learning and MAE|https://arxiv.org/abs/2403.19638|[['cs.CV']]
2024-03-28|MedBN: Robust Test-Time Adaptation against Malicious Test Samples|use statistics (batch norm) to handle manipulated data in test time adaptation|https://arxiv.org/abs/2403.19326|[['attack'], ['cs.LG'], ['CVPR']]
2024-03-29|Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer|use adapter in class incremental learning, learn prototypes for old classes|https://arxiv.org/abs/2403.19979|[['parameter-efficient'], ['cs.CV'], ['CVPR']]
2024-03-29|MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning|task specific LoRA, and task agnostic LoRA for multi task learning|https://arxiv.org/abs/2403.20320|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CV'], ['CVPR']]
2024-03-29|LayerNorm: A key component in parameter-efficient fine-tuning|argue that tuning layer norm is enough for downstream tasks|https://arxiv.org/abs/2403.20284|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL']]
2024-03-29|Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations|text to image model based on can't read negated description, try to make CLIP understand negation|https://arxiv.org/abs/2403.20312|[['Vision-Language'], ['cs.CV']]
2024-03-29|Are We on the Right Way for Evaluating Large Vision-Language Models?|benchmark for VLM, avoid data leakage and make sure necessity of visual input|https://arxiv.org/abs/2403.20330|[['Vision-Language'], ['cs.CV']]
2024-03-29|Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models|make VLM refuse to answer when the question is not solvable|https://arxiv.org/abs/2403.20331|[['Vision Language', 'VLM'], ['cs.CV']]
2024-03-29|Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning|increase complexity of data augmentation as training process goes|https://arxiv.org/abs/2403.20012|[['cs.CV'], ['ICLR']]
2024-03-29|Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion|as title|https://arxiv.org/abs/2403.20015|[['cs.CL'], ['ICLR']]
2024-03-29|ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning|continual learn for segmentation using prompt tuning |https://arxiv.org/abs/2403.20126|[['cs.CV'], ['CVPR']]
2024-03-29|Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions|benchmark and simple baseline for temporal corrupted|https://arxiv.org/abs/2403.20254|[['cs.CV'], ['CVPR']]
2024-03-29|Convolutional Prompting meets Language Models for Continual Learning|convolutional hyper network to achieve layer-wise input-aware prompt tuning for continual learning|https://arxiv.org/abs/2403.20317|[['cs.CV'], ['CVPR']]
2024-03-29|On Large Language Models' Hallucination with Regard to Known Facts|detect hallucination in LLM by tracking token probabilities|https://arxiv.org/abs/2403.20009|[['cs.CL']]
2024-03-29|FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues|swap features of same concept from different modalities and train on matching loss to enhance performance|https://arxiv.org/abs/2403.20026|[['cs.CV']]
2024-03-29|Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning|prompt and finetune to make LLM learn from error during CoT|https://arxiv.org/abs/2403.20046|[['cs.CL']]
2024-04-02|T-VSL: Text-Guided Visual Sound Source Localization in Mixtures|use text as guide to disentangle audio and visual for multi source sounding localization|https://arxiv.org/abs/2404.01751|[['audio-visual'], ['cs.CV'], ['CVPR']]
2024-04-02|BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition|audio visual SSL pretraining, dual encoders MAE|https://arxiv.org/abs/2404.02098|[['audio-visual'], ['cs.CV'], ['ICASSP']]
2024-04-02|ViTamin: Designing Scalable Vision Models in the Vision-Language Era|propose evaluation protocol for visual model (using CLIP method), propose CNN Transformer hybrid model|https://arxiv.org/abs/2404.02132|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|Iterated Learning Improves Compositionality in Large Vision-Language Models|finite code book and re-initialize model during CLIP training for better compositionality ("girl in white facing man in black" vs "girl in black facing man in white")|https://arxiv.org/abs/2404.02145|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|VLRM: Vision-Language Models act as Reward Models for Image Captioning|reinforcement learning for better captioning|https://arxiv.org/abs/2404.01911|[['Vision-Language'], ['cs.CV']]
2024-04-02|Weakly-supervised Audio Separation via Bi-modal Semantic Similarity|audio separation without access to single source audio, mix multi source audio and use contrastive loss|https://arxiv.org/abs/2404.01740|[['cs.SD'], ['ICLR']]
2024-04-02|Joint-Task Regularization for Partially Labeled Multi-Task Learning|as title|https://arxiv.org/abs/2404.01976|[['cs.CV'], ['CVPR']]
2024-04-02|Using Interpretation Methods for Model Enhancement|learn to make interpretation supervisedly, different ways to generate interpretation|https://arxiv.org/abs/2404.02068|[['cs.CL'], ['EMNLP']]
2024-04-02|Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners|as title|https://arxiv.org/abs/2404.02117|[['cs.CV'], ['CVPR']]
2024-04-04|ReFT: Representation Finetuning for Language Models|can't understand but it is PEFT + LLM|https://arxiv.org/abs/2404.03592|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-04-04|Would Deep Generative Models Amplify Bias in Future Models?|find that using sythesized data may not increase bias|https://arxiv.org/abs/2404.03242|[['social biases'], ['Diffusion'], ['cs.CV'], ['CVPR']]
2024-04-04|Scaling Up Video Summarization Pretraining with Large Language Models|new dataset for long video, using automated pipeline|https://arxiv.org/abs/2404.03398|[['cs.CV'], ['CVPR']]
2024-04-04|MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens|VLM, concatenate frame feature and subtitle feature for video understanding|https://arxiv.org/abs/2404.03413|[['cs.CV']]
2024-04-04|Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought|RL, knowledge distillation for small LM to deal with multi-hop QA|https://arxiv.org/abs/2404.03414|[['cs.CL']]
2024-04-16|Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models|align visual feature with text feature GPT4 prompt; leaarable prompt for CLIP classification|https://arxiv.org/abs/2404.10357|[['Vision-Language'], ['cs.CV']]
2024-04-16|Self-Supervised Visual Preference Alignment|use data augmentation to produce data for DPO|https://arxiv.org/abs/2404.10501|[['Vision-Language', 'VLM'], ['cs.CV']]
2024-04-16|Future Language Modeling from Temporal Document History|new task of future language modeling|https://arxiv.org/abs/2404.10297|[['cs.CL'], ['ICLR']]
2024-04-16|Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation|perturbating on feature space to simulate domain shift and use adapter to rectify the perturbation and deal with domain shift|https://arxiv.org/abs/2404.10322|[['cs.CV'], ['CVPR']]
2024-04-16|Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model|locate modules that is adept for specific tasks to prevent catastrophoic forgetting|https://arxiv.org/abs/2404.10306|[['cs.CL']]
2024-04-16|Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards|LLM self-correction to do DPO|https://arxiv.org/abs/2404.10346|[['cs.CL']]
2024-04-16|Dual Modalities of Text: Visual and Textual Generative Pre-training|render text as image and do next patch, token prediction simultaneously|https://arxiv.org/abs/2404.10710|[['cs.CL']]
2024-04-16|Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study|as title|https://arxiv.org/abs/2404.10719|[['cs.CL']]
2024-04-15|Bridging Vision and Language Spaces with Assignment Prediction|align visual feature with word embedding, don't need to modify LLM's weights|https://arxiv.org/abs/2404.09632|[['vision-language'], ['cs.CV'], ['ICLR']]
2024-04-15|Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering|use proxy model to sample neighborhood question, and use consistency on neighborhood questions to measure reliability|https://arxiv.org/abs/2404.10193|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-15|Leveraging Temporal Contextualization for Video Action Recognition|extract context tokens of videos as KV for better aggregation of information|https://arxiv.org/abs/2404.09490|[['vision-language'], ['cs.CV']]
2024-04-15|Conditional Prototype Rectification Prompt Learning|learn textual prototype in semi-supervised manners to mitigate overfitting|https://arxiv.org/abs/2404.09872|[['vision-language'], ['cs.CV']]
2024-04-15|Evolving Interpretable Visual Classifiers with Large Language Models|use elvolutionary search with LLM and CLIP to build interpretable image classifier|https://arxiv.org/abs/2404.09941|[['vision-language'], ['cs.CV']]
2024-04-15|LoRA Dropout as a Sparsity Regularizer for Overfitting Control|add noise to LoRA weights to increase parameter sparisty, prevent overfitting theoretically|https://arxiv.org/abs/2404.09610|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.LG']]
2024-04-15|The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning|semi supervised method for CLIP to tackle narrow distribution during few shots learning|https://arxiv.org/abs/2404.09778|[['cs.CV'], ['ECCV']]
2024-04-15|Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model|study on quality of instruction tuning data|https://arxiv.org/abs/2404.09717|[['cs.CL']]
2024-04-15|Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations|as title|https://arxiv.org/abs/2404.09785|[['cs.CL']]
2024-04-15|TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding|use captioning, detection, and cropping to enhance VQA performance|https://arxiv.org/abs/2404.09797|[['cs.CV']]
2024-04-15|Impact of Preference Noise on the Alignment Performance of Generative Language Models|study on quality of data to do alignment|https://arxiv.org/abs/2404.09824|[['cs.CL']]
2024-04-15|CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting|study on behaviors of LLM on different languages and cultures|https://arxiv.org/abs/2404.10199|[['cs.CL']]
2024-04-15|Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models|as title|https://arxiv.org/abs/2404.09529|[['cs.LG']]
2024-04-15|LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models|low rank pruning of LLM by observation of low rank structure of MHA|https://arxiv.org/abs/2404.09695|[['cs.LG']]
2024-04-25|AAPL: Adding Attributes to Prompt Learning for Vision-Language Models|reduce bias introduced by data augmentation in previous zero shot classification method|https://arxiv.org/abs/2404.16804|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-25|Training-Free Unsupervised Prompt for Vision-Language Models|training and label free method to boost zero-shot classification, generate high confident prototype and compute similarity to adjust logits|https://arxiv.org/abs/2404.16339|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-04-25|Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class|use multiple vectors to represent a class so that different attribute in a class can be taken into account|https://arxiv.org/abs/2404.16717|[['Vision-language', 'VLM'], ['cs.CV']]
2024-04-25|Multi-Scale Representations by Varying Window Attention for Semantic Segmentation|constrain attention region for better multi-scale feature extraction in segmentation|https://arxiv.org/abs/2404.16573|[['cs.CV'], ['ICLR']]
2024-04-25|EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning|instruction tuning for visual emotional recognition, new data and model|https://arxiv.org/abs/2404.16670|[['cs.CV'], ['CVPR']]
2024-04-25|List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs|data set for set-of-mark (use visual tags on image for MLM to refer to), a trick enhance model performance and interpretability|https://arxiv.org/abs/2404.16375|[['cs.CV']]
2024-04-25|Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities|feature disentangle during distillation to handle missing modality|https://arxiv.org/abs/2404.16456|[['cs.CV']]
2024-04-25|Exploring Internal Numeracy in Language Models: A Case Study on ALBERT|study on how LM handle embeddings of numbers (increase in one direction after PCA)|https://arxiv.org/abs/2404.16574|[['cs.CL']]
2024-04-25|Understanding Privacy Risks of Embeddings Induced by Large Language Models|study on potential of LLM reconstruction pre-training data|https://arxiv.org/abs/2404.16587|[['cs.CL']]
2024-04-25|TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning|mtrain model to generate code for calculation, token merging for high resolution images|https://arxiv.org/abs/2404.16635|[['cs.CV']]
2024-04-25|Zero-Shot Distillation for Image Encoders: How to Make Effective Use of Synthetic Data|L2 loss is better than contrastive loss for distillation|https://arxiv.org/abs/2404.16637|[['cs.CV']]
2024-04-25|Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4|GPT-4 are more biased to add information when asked to improve its answers when subtraction was more efficient|https://arxiv.org/abs/2404.16692|[['cs.CL']]
2024-04-25|Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding|layer dropout in LLM during training, early exit during inference and self-speculative decoding for speed up inference|https://arxiv.org/abs/2404.16710|[['cs.CL']]
2024-04-25|SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension|as title|https://arxiv.org/abs/2404.16790|[['cs.CV']]
2024-04-25|Make Your LLM Fully Utilize the Context|fine tune LLM on long context|https://arxiv.org/abs/2404.16811|[['cs.CL']]
2024-04-25|How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites|VLM with 6B ViT, dynamic resolution, English and Chinese training data|https://arxiv.org/abs/2404.16821|[['cs.CV']]
2024-04-25|T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients|as title|https://arxiv.org/abs/2404.16495|[['cs.LG']]
2024-04-25|VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations|argue that LMs have difficulties in distinguishing between text semantic variations|https://arxiv.org/abs/2404.16365|[['vision-language', 'VLMs'], ['face'], ['cs.CL']]
2024-04-24|FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication|study about effect of deduplication on social bias|https://arxiv.org/abs/2404.16123|[['Vision-Language'], ['social biases'], ['cs.CV'], ['CVPR']]
2024-04-24|What Makes Multimodal In-Context Learning Work?|argue the lack of exploration in multimodal in context learning|https://arxiv.org/abs/2404.15736|[['cs.CV'], ['CVPR']]
2024-04-24|MoDE: CLIP Data Experts via Clustering|train multiple CLIP on some clusters of data and ensemble them at inference time|https://arxiv.org/abs/2404.16030|[['cs.CV'], ['CVPR']]
2024-04-24|CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data|train CLIP by prediction synset (BCE loss), speed up 2.7x|https://arxiv.org/abs/2404.15653|[['cs.CV']]
2024-05-01|CLIPArTT: Light-weight Adaptation of CLIP to New Domains at Test Time|test time adaptation by aggregate top k prediction into a new prompt to generate pseudo label,  standardize benchmarks|https://arxiv.org/abs/2405.00754|[['vision-language', 'VLMs'], ['cs.CV']]
2024-05-01|AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts|adaptive select LoRA experts rather than select fixed top k|https://arxiv.org/abs/2405.00361|[['cs.CL']]
2024-05-01|Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval|interpolate between query image and text to obtain feature to do retrieval|https://arxiv.org/abs/2405.00571|[['cs.CV']]
2024-05-01|Are Models Biased on Text without Gender-related Language?|show that LLM is biased even if correlation between gender related words in the sentences are calibrated by pretrain data, ICLR 2024|https://arxiv.org/abs/2405.00588|[['cs.CL']]
2024-05-01|Learning to Compose: Improving Object Centric Learning by Injecting Compositionality|objectives to enable decode composite images via composite representation, improve object-centricc learning, ICLR 2024|https://arxiv.org/abs/2405.00646|[['cs.CV']]
2024-05-01|DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling|tune the model to predict multiple tokens at a time to speed up inference time|https://arxiv.org/abs/2405.00888|[['cs.CL']]
2024-05-01|LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets|model and data pruning at the same time|https://arxiv.org/abs/2405.00906|[['cs.CV']]
2024-05-01|LLaVA Finds Free Lunch: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs|tune LLaVA by predicting receiver behavior|https://arxiv.org/abs/2405.00942|[['cs.CV']]
2024-05-02|Learning Object States from Actions via Large Language Models|extract objects states from action in the caption using LLM|https://arxiv.org/abs/2405.01090|[['vision-language'], ['cs.CV']]
2024-05-02|MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors|resources efficient way to tune LLM for 3D|https://arxiv.org/abs/2405.01413|[['parameter-efficient', 'efficient fine-tuning'], ['vision-language'], ['3D', 'point cloud'], ['cs.CV']]
2024-05-02|MANTIS: Interleaved Multi-Image Instruction Tuning|VLM with multiple image inputs, train on 16 A100|https://arxiv.org/abs/2405.01483|[['vision language'], ['cs.CV']]
2024-05-02|Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models|theoreticaly understand retrieval using CLIP, show that I2I is better than T2I|https://arxiv.org/abs/2405.01468|[['Vision-Language'], ['cs.LG']]
2024-05-02|WildChat: 1M ChatGPT Interaction Logs in the Wild|dataset of conversation with GPT-4|https://arxiv.org/abs/2405.01470|[['cs.CL'], ['ICLR']]
2024-05-02|Why Tabular Foundation Models Should Be a Research Priority|call for more  study on tubular modality|https://arxiv.org/abs/2405.01147|[['cs.LG']]
2024-05-02|ATOM: Attention Mixer for Efficient Dataset Distillation|different attention mechanism for feature matching in dataset distillation|https://arxiv.org/abs/2405.01373|[['architecture search'], ['cs.CV'], ['CVPR']]
2024-04-30|MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation|domain generalization with some attribute correlation with base that may confuse models|https://arxiv.org/abs/2404.19644|[['vision-language'], ['cs.CV'], ['ICLR']]
2024-04-30|MoPEFT: A Mixture-of-PEFTs for the Segment Anything Model|use different PEFT techniques and dynamically learns to activate some of them|https://arxiv.org/abs/2405.00293|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CV'], ['CVPR']]
2024-04-30|Soft Prompt Generation for Domain Generalization|learn soft prompt for different domain as target, and train generative model to produce soft prompt for different domain|https://arxiv.org/abs/2404.19286|[['vision language', 'VLMs'], ['cs.CV']]
2024-04-30|CLIP-Mamba: CLIP Pretrained Mamba Models with OOD and Hessian Evaluation|67M Mamba on par with 307M ViT on zero shot, much better on OOD|https://arxiv.org/abs/2404.19394|[['parameter efficiency'], ['cs.CV']]
2024-04-30|SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models|progressively tune adapters at different layers|https://arxiv.org/abs/2405.00201|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-04-30|On Improving the Algorithm-, Model-, and Data- Efficiency of Self-Supervised Learning|new SSL method to improve performance, memory bank, square regularization|https://arxiv.org/abs/2404.19289|[['cs.CV']]
2024-04-30|StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation|highlight variance of initialization of PEFT, separate hard and soft prompt|https://arxiv.org/abs/2404.19335|[['cs.CL']]
2024-04-30|One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features|one stage open vocabulary temporal action detection rather than making proposal then identifying|https://arxiv.org/abs/2404.19542|[['cs.CV']]
2024-04-30|SemiPL: A Semi-supervised Method for Event Sound Source Localization|as title|https://arxiv.org/abs/2404.19615|[['cs.CV']]
2024-04-30|Better & Faster Large Language Models via Multi-token Prediction|pre train LLM to predict multiple tokens from scratch|https://arxiv.org/abs/2404.19737|[['cs.CL']]
2024-04-30|DOCCI: Descriptions of Connected and Contrasting Images|fine grained long text image pair dataset|https://arxiv.org/abs/2404.19753|[['Vision-language'], ['text-to-image'], ['cs.CV']]
2024-04-30|ASAM: Boosting Segment Anything Model with Adversarial Tuning|generate adversarial examples using diffusion model to boost SAM|https://arxiv.org/abs/2405.00256|[['diffusion'], ['cs.CV'], ['CVPR']]
2024-04-30|Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey|as title|https://arxiv.org/abs/2405.00314|[['cs.LG']]
2024-05-02|Early Transformers: A study on Efficient Training of Transformer Models through Early-Bird Lottery Tickets|iterative pruning, selective retraining to improve training efficiency|https://arxiv.org/abs/2405.02353|[['training efficiency'], ['cs.CL']]
2024-05-02|Enhancing User Experience in On-Device Machine Learning with Gated Compression Layers|dynamically filter non-essential inputs to improve power consumption|https://arxiv.org/abs/2405.01739|[['cs.LG']]
2024-05-02|COPAL: Continual Pruning in Large Language Generative Models|continual pruning weights with high sensitivity to new data in continual learning setting|https://arxiv.org/abs/2405.02347|[['cs.LG']]
2024-05-03|What matters when building vision-language models?|experiment with different design decision of VLM, propose a 8B VLM|https://arxiv.org/abs/2405.02246|[['vision-language', 'VLMs'], ['cs.CV']]
2024-05-03|Auto-Encoding Morph-Tokens for Multimodal LLM|VLM use quantized visual token, can generate both image and text|https://arxiv.org/abs/2405.01926|[['cs.CV']]
2024-05-03|MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot Action Recognition|few-shot action recognition, take into account different velocity|https://arxiv.org/abs/2405.02077|[['cs.CV']]
2024-05-08|THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models|hallucination benchmark for free form QA rather than multiple choice question|https://arxiv.org/abs/2405.05256|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-05-08|Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion|utilize different multimodality data (text-image, text-audio, audio-image, etc) to improve unified model (text-image-audio)|https://arxiv.org/abs/2405.04883|[['cs.CV'], ['ICML']]
2024-05-08|Estimating Noisy Class Posterior with Part-level Labels for Noisy Label Learning|partition images into distinct parts and make pseudo labels on them, and optimize model using multi-label loss to improve robustness|https://arxiv.org/abs/2405.05714|[['cs.CV'], ['CVPR']]
2024-05-08|You Only Cache Once: Decoder-Decoder Architectures for Language Models|new decoder-decoder architecture for LLM that reduces key value caches usage to improve throughput, and GPU memory|https://arxiv.org/abs/2405.05254|[['cs.CL']]
2024-05-08|Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers|use FFT to speed up computation of attention|https://arxiv.org/abs/2405.05219|[['cs.LG']]
2024-05-08|KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation|improve the speed for prefill phase (first token) by parallelizing|https://arxiv.org/abs/2405.05329|[['ICML']]
2024-05-08|Vidur: A Large-Scale Simulation Framework For LLM Inference|simulate LLM inference using experimental profiling and predictive modeling to improve inference speed|https://arxiv.org/abs/2405.05465|[['cs.LG']]
2024-05-07|The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models|study on bias of CLIP when scaling up data|https://arxiv.org/abs/2405.04623|[['VLMs'], ['cs.CY']]
2024-05-07|Differentially Private Post-Processing for Fair Regression|general method to improve fairness of regressor|https://arxiv.org/abs/2405.04034|[['cs.LG', 'cs.CY'], ['ICML']]
2024-05-07|Granite Code Models: A Family of Open Foundation Models for Code Intelligence|IBM LLM for code generation|https://arxiv.org/abs/2405.04324|[['cs.AI', 'cs.CL']]
2024-05-07|Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks|as title|https://arxiv.org/abs/2405.04403|[['Vision-Language', 'VLMs'], ['Attacks'], ['cs.CV', 'cs.CL']]
2024-05-07|Structured Click Control in Transformer-based Interactive Segmentation|use GNN to handle user clicks for segmentation|https://arxiv.org/abs/2405.04009|[['graph'], ['cs.AI', 'cs.CV'], ['NeurIPS']]
2024-05-07|Towards Stability of Parameter-free Optimization|optimizer that estimate step size from AdaGrad-Norm to avoid hyperparameter tuning|https://arxiv.org/abs/2405.04376|[['cs.LG']]
2024-05-09|Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning|merge visual feature into feed forward network to improve efficiency of VLM|https://arxiv.org/abs/2405.05615|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['Vision-Language'], ['cs.LG', 'cs.CV', 'cs.CL'], ['ICML']]
2024-05-09|DARA: Domain- and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding|domain aware and relation aware adapters for temporal grounding|https://arxiv.org/abs/2405.06217|[['Parameter-efficient'], ['vision-language'], ['cs.CV']]
2024-05-09|Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse|utilize relation between good feature and symmetric weights (neural collapse) to improve training on biased data|https://arxiv.org/abs/2405.05587|[['cs.LG', 'cs.CV'], ['CVPR']]
2024-05-09|Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference|ignore some visual tokens to improve efficiency|https://arxiv.org/abs/2405.05803|[['cs.AI', 'cs.CV']]
2024-05-09|CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts|MoE for MLM|https://arxiv.org/abs/2405.05949|[['cs.CV']]
2024-05-09|OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning|several pruning techniques to achieve high compression of LLM|https://arxiv.org/abs/2405.05957|[['cs.CL']]
2024-05-09|LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models|toolkit and benchmark for quantized LLM|https://arxiv.org/abs/2405.06001|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-09|SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models|as title|https://arxiv.org/abs/2405.06219|[['cs.LG', 'cs.CL']]
2024-05-09|MaskMatch: Boosting Semi-Supervised Learning Through Mask Autoencoder-Driven Feature Learning|semi supervised learning try to utilize all unlabeled data (no threshold for pseudo label confidence)|https://arxiv.org/abs/2405.06227|[['cs.CV']]
2024-05-13|Localizing Task Information for Improved Model Merging and Compression|find that the model after merging retaining ability of each task but may conflict or interfere each other, use compression to avoid conflict|https://arxiv.org/abs/2405.07813|[['cs.LG', 'cs.CV'], ['ICML']]
2024-05-13|Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models|adaptively train sub modules to improve efficiency|https://arxiv.org/abs/2405.07527|[['cs.AI', 'cs.LG'], ['NeurIPS']]
2024-05-13|MambaOut: Do We Really Need Mamba for Vision?|show that mamba may not be suitable for specific tasks in CV|https://arxiv.org/abs/2405.07992|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-05-13|SpeechVerse: A Large-scale Generalizable Audio Language Model|aws audio text model|https://arxiv.org/abs/2405.08295|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-05-12|CLIP-Powered TASS: Target-Aware Single-Stream Network for Audio-Visual Question Answering|AVQA, focus on audio visual fusion and region aware visual feature|https://arxiv.org/abs/2405.07451|[['vision-language', 'VLMs'], ['Audio-Visual'], ['cs.CV']]
2024-05-12|Enhanced Online Test-time Adaptation with Feature-Weight Cosine Alignment|claim that minimizing cosine similarty is better than minimizing entropy for test time adaptation|https://arxiv.org/abs/2405.07171|[['cs.CV']]
2024-05-12|Unified Video-Language Pre-training with Synchronized Audio|MAE and contrastive pretraining for audio visual and text|https://arxiv.org/abs/2405.07202|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.SD', 'eess.AS']]
2024-05-12|Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains|pretraining task for tabular data|https://arxiv.org/abs/2405.07414|[['cs.AI', 'cs.LG'], ['ICML']]
2024-05-14|Efficient Vision-Language Pre-training by Cluster Masking|masking durin CLIP pretraining to improve performance and training speed|https://arxiv.org/abs/2405.08815|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-05-14|Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis|analyze the cost of long input for transformers|https://arxiv.org/abs/2405.08944|[['GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-14|Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation|improve segmentation by refine the attention maps by text visual or visual visual alignment|https://arxiv.org/abs/2405.08458|[['cs.CV'], ['CVPR']]
2024-05-14|Improving Transformers with Dynamically Composable Multi-Head Attention|new design for multi head self attention, refine attention in a query key dependent way|https://arxiv.org/abs/2405.08553|[['cs.LG', 'cs.CL'], ['ICML']]
2024-05-14|EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training|curriculum learning (design which data to feed during training) according to frequency|https://arxiv.org/abs/2405.08768|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICCV']]
2024-05-14|Cross-Domain Feature Augmentation for Domain Generalization|augmentation on feature space to improve domain generalization|https://arxiv.org/abs/2405.08586|[['cs.CV']]
2024-05-14|CinePile: A Long Video Question Answering Dataset and Benchmark| as title|https://arxiv.org/abs/2405.08813|[['cs.LG', 'cs.CV']]
2024-05-14|Spatial Semantic Recurrent Mining for Referring Image Segmentation|better modality features fusion for referring image segmentation (segmentation given description)|https://arxiv.org/abs/2405.09006|[['cs.CV', 'cs.CL']]
2024-05-14|AD-Aligning: Emulating Human-like Generalization for Cognitive Domain Adaptation in Deep Learning|adversarial learning and CORAL loss for domain adaptation|https://arxiv.org/abs/2405.09582|[['cs.CV', 'eess.IV']]
2024-05-14|Improving Transformers using Faithful Positional Encoding|different positional embeddings with mathematical guarantee|https://arxiv.org/abs/2405.09061|[['cs.LG']]
2024-05-15|SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge|commonsense machine generated VQA |https://arxiv.org/abs/2405.09713|[['vision-language'], ['cs.AI', 'cs.CV', 'cs.CL'], ['CVPR']]
2024-05-15|LoRA Learns Less and Forgets Less|LoRA underperform on target tasks but maintain generalizibility, including some pratices of using LoRA|https://arxiv.org/abs/2405.09673|[['parameter-efficient', 'efficient finetuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-15|ReconBoost: Boosting Can Achieve Modality Reconcilement|use idea similar to boosting, update one modality each time to prevent dominant modality|https://arxiv.org/abs/2405.09321|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-05-15|Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection|as title|https://arxiv.org/abs/2405.09782|[['cs.CV'], ['ICML']]
2024-05-15|Curriculum Dataset Distillation|progressively generate adverserial data to achieve dataset distillation|https://arxiv.org/abs/2405.09150|[['cs.CV']]
2024-05-15|HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants|evaluate LLM by calculating correlation between LLM preference (log likelihood) and human ranked response|https://arxiv.org/abs/2405.09186|[['cs.CL']]
2024-05-15|Prompting-based Synthetic Data Generation for Few-Shot Question Answering|use LLM to generate or refine QA data|https://arxiv.org/abs/2405.09335|[['cs.CL']]
2024-05-15|Spectral Editing of Activations for Large Language Model Alignment|test time editing to improve truthfulness and reduce hallucination|https://arxiv.org/abs/2405.09719|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-15|SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data|task where users can access to specific portion of data to study security and privacy|https://arxiv.org/abs/2405.09805|[['cs.CL']]
2024-05-15|Overcoming Domain Drift in Online Continual Learning|use contrastive loss and distillation in continual domain adaptation|https://arxiv.org/abs/2405.09133|[['cs.LG']]
2024-05-15|Hierarchical Emotion Prediction and Control in Text-to-Speech Synthesis|predict hierarchical emotion to help tts|https://arxiv.org/abs/2405.09171|[['Synthesis'], ['cs.SD', 'eess.AS'], ['ICASSP']]
2024-05-15|Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models Promptly|new measures for fairness using distance in feature space and its approximation algorithm|https://arxiv.org/abs/2405.09251|[['cs.LG', 'cs.CY']]
2024-05-15|Dynamic Activation Pitfalls in LLaMA Models: An Empirical Study|study on activation function and KV cache skipping in LLaMA|https://arxiv.org/abs/2405.09274|[['cs.LG']]
2024-05-15|SA-FedLora: Adaptive Parameter Allocation for Efficient Federated Learning with LoRA Tuning|as title|https://arxiv.org/abs/2405.09394|[['parameter-efficient'], ['Federated Learning'], ['cs.LG']]
2024-05-15|STAR: A Benchmark for Situated Reasoning in Real-World Videos|video QA, reasoning benchmark (NeurIPS'21)|https://arxiv.org/abs/2405.09711|[['graph'], ['cs.AI', 'cs.CV', 'cs.CL'], ['NeurIPS']]
2024-05-16|FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models|affect of negative label and low caption quality to VLM, use sigmoid loss to improve|https://arxiv.org/abs/2405.10286|[['Vision-Language'], ['cs.AI', 'cs.CV'], ['CVPR']]
2024-05-16|Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning|use VLM to interact with environment and improve it using RL|https://arxiv.org/abs/2405.10292|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-05-16|SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection|construct different level of object description to deal with various granularities in open vocabulary object detection|https://arxiv.org/abs/2405.10053|[['cs.CV'], ['CVPR']]
2024-05-16|Libra: Building Decoupled Vision System on Large Language Models|VLM decouples inner and inter modality modeling, use auto regressive modeling on both vision and language|https://arxiv.org/abs/2405.10140|[['cs.CV'], ['ICML']]
2024-05-16|DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data|synthetic data for data augmentation for instance segmentation|https://arxiv.org/abs/2405.10185|[['cs.CV'], ['CVPR']]
2024-05-16|Chameleon: Mixed-Modal Early-Fusion Foundation Models|VLM with early modality fusion (not concatenation of visual encoder and LLM), handle arbitrary modality order, can generate image, made by META|https://arxiv.org/abs/2405.09818|[['cs.CL']]
2024-05-16|Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling|generate soft negative sample (high textual quality but illogical sematics) to do contrastive to ease hallucination|https://arxiv.org/abs/2405.09848|[['cs.AI', 'cs.CL']]
2024-05-16|Towards Realistic Incremental Scenario in Class Incremental Semantic Segmentation|argue the unrealistic setting in previous class incremental segmentation (same image with different labels)|https://arxiv.org/abs/2405.09858|[['cs.LG', 'cs.CV']]
2024-05-16|Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models|let LLM do cloze to correct ASR results|https://arxiv.org/abs/2405.10025|[['cs.AI', 'cs.LG', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-05-16|Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation|as title, optimal transport to deal with misaligned pairs, ICLR'24|https://arxiv.org/abs/2405.10084|[['cs.AI', 'cs.SD', 'eess.AS']]
2024-05-16|Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection|grounding DINO 1.5|https://arxiv.org/abs/2405.10300|[['cs.CV']]
2024-05-16|Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models|prompting for fair and unbias LLM|https://arxiv.org/abs/2405.10431|[['cs.CL']]
2024-05-16|Rethinking ChatGPT's Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs' Prompting|try to explain the reason behind the power of auto regressive LLM|https://arxiv.org/abs/2405.10474|[['cs.CL']]
2024-05-16|Learnable Privacy Neurons Localization in Language Models|localize privacy information learnt by LLM through adversairal training|https://arxiv.org/abs/2405.10989|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-16|In-context Contrastive Learning for Event Causality Identification|contrastive learning to enable ability to learn from positive and negative demonstration|https://arxiv.org/abs/2405.10512|[['cs.LG']]
2024-05-17|Driving Referring Video Object Segmentation with Vision-Language Pre-trained Models|prompt tuning to make CLIP better at pixel label tasks (segmentation)|https://arxiv.org/abs/2405.10610|[['Vision-Language'], ['cs.CV']]
2024-05-17|CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly Supervised Audio-Visual Video Parsing|detect audible or visible event with video level labels, use unimodal and cross modal encoders to do contrastive learning|https://arxiv.org/abs/2405.10690|[['Audio-Visual'], ['cs.CV']]
2024-05-17|HARIS: Human-Like Attention for Reference Image Segmentation|reuse transformer layers with feed back connection|https://arxiv.org/abs/2405.10707|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CV']]
2024-05-17|Open-Vocabulary Spatio-Temporal Action Detection|benchmark for open vocabulary spatio temporal action detection and its baseline|https://arxiv.org/abs/2405.10832|[['VLM'], ['cs.CV']]
2024-05-17|Detecting Multimodal Situations with Insufficient Context and Abstaining from Baseless Predictions|argue that VQA data's answer may rely on unsupported assumption causing hallucination, collect context data for them to reduce hallucination|https://arxiv.org/abs/2405.11145|[['Vision-Language'], ['cs.AI', 'cs.CV']]
2024-05-17|Towards Modular LLMs by Building and Reusing a Library of LoRAs|learn a collection of LoRA from multi-task data and dynamically select them during inference|https://arxiv.org/abs/2405.11157|[['parameter-efficient'], ['cs.LG', 'cs.CL']]
2024-05-17|Dynamic data sampler for cross-language transfer learning in large language models|gradually transfer from continual pretrain to supervised finetune to transfer LLM to other language|https://arxiv.org/abs/2405.10626|[['cs.CL'], ['ICASSP']]
2024-05-17|DINO as a von Mises-Fisher mixture model|theoretical insight of DINO pretraining method, small change base on the insight improve performance|https://arxiv.org/abs/2405.10939|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICLR']]
2024-05-17|Acoustic modeling for Overlapping Speech Recognition: JHU Chime-5 Challenge System|as title, ICASSP'19|https://arxiv.org/abs/2405.11078|[['eess.AS'], ['ICASSP']]
2024-05-17|Improving Point-based Crowd Counting and Localization Based on Auxiliary Point Guidance|matching method to increase stability when optimizing model for crowd counting|https://arxiv.org/abs/2405.10589|[['cs.AI', 'cs.CV', 'eess.IV']]
2024-05-17|Learning Object-Centric Representation via Reverse Hierarchy Guidance|predict object mask to obtain top level feature to guide the original model|https://arxiv.org/abs/2405.10598|[['cs.CV']]
2024-05-17|Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization|as title|https://arxiv.org/abs/2405.10616|[['cs.LG', 'cs.CL']]
2024-05-17|Layer-Condensed KV Cache for Efficient Inference of Large Language Models|only compute and caches the key and values to save memory and improve throughput|https://arxiv.org/abs/2405.10637|[['cs.CL']]
2024-05-17|Feature-Adaptive and Data-Scalable In-Context Learning|feature space in context learning to avoid length limit|https://arxiv.org/abs/2405.10738|[['cs.CL']]
2024-05-17|Efficient Multimodal Large Language Models: A Survey|as title|https://arxiv.org/abs/2405.10739|[['cs.AI', 'cs.CV']]
2024-05-17|SBAAM! Eliminating Transcript Dependency in Automatic Subtitling|end to end model for translating dialogue, segmenting, estimating timestamps without intermediate ASR model|https://arxiv.org/abs/2405.10741|[['cs.CL']]
2024-05-17|Observational Scaling Laws and the Predictability of Language Model Performance|scaling laws without training process for different model family to predict model's performance|https://arxiv.org/abs/2405.10938|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-17|Prompt Exploration with Prompt Regression|predict the effect of a prompt and select a optimal combination of prompts|https://arxiv.org/abs/2405.11083|[['cs.LG', 'cs.CL']]
2024-05-17|Revisiting the Robust Generalization of Adversarial Prompt Tuning|prompt tuning to tackle adversarial sample, use KL divergence as regularization|https://arxiv.org/abs/2405.11154|[['vision-language'], ['attacks'], ['cs.AI', 'cs.CV']]
2024-05-18|Fuse & Calibrate: A bi-directional Vision-Language Guided Framework for Referring Image Segmentation|text visual feature fusion for eeter text guided segmentation|https://arxiv.org/abs/2405.11205|[['Vision-Language'], ['cs.CV']]
2024-05-18|Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts|MoE on MLM (speech visual text)|https://arxiv.org/abs/2405.11273|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-18|MBIAS: Mitigating Bias in Large Language Models While Retaining Context|instruction tuning using new data to debias LLM|https://arxiv.org/abs/2405.11290|[['cs.CL']]
2024-05-18|A Unified Approach Towards Active Learning and Out-of-Distribution Detection|Solve active learning (determine label candidates) from the view of out of distribution detection|https://arxiv.org/abs/2405.11337|[['cs.CV']]
2024-05-18|Exploring speech style spaces with language models: Emotional TTS without emotion labels|emotional TTS based on text it self rather than other emotion label (use pseudo label to solve)|https://arxiv.org/abs/2405.11413|[['cs.LG', 'eess.AS']]
2024-05-18|MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning|as title|https://arxiv.org/abs/2405.11446|[['cs.LG', 'cs.CL']]
2024-05-19|MICap: A Unified Model for Identity-aware Movie Descriptions|end to end movie caption rather than predicting characters afterward|https://arxiv.org/abs/2405.11483|[['cs.CV'], ['CVPR']]
2024-05-19|SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization|progressively replace layer norm with batch norm in training, attention without sofmax to improve the efficacy|https://arxiv.org/abs/2405.11582|[['cs.CV', 'cs.CL'], ['ICML']]
2024-05-19|Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks|study the efficiency bottleneck of transformers during training and inference|https://arxiv.org/abs/2405.11704|[['training efficiency'], ['cs.AI', 'cs.LG']]
2024-05-19|Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion|reduce length of soft prompt to improve efficiency, use subspace projection to increase performance, similar to hyper network|https://arxiv.org/abs/2405.11464|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-19|A Multi-Perspective Analysis of Memorization in Large Language Models|study on how and why LLM memorize training data|https://arxiv.org/abs/2405.11577|[['cs.AI', 'cs.CL']]
2024-05-19|Token-wise Influential Training Data Retrieval for Large Language Models|training data influence estimation|https://arxiv.org/abs/2405.11724|[['cs.AI', 'cs.CL']]
2024-05-19|Configurable Mirror Descent: Towards a Unification of Decision Making|unified framework for reinforcement learning (number of agent, how they interect...)|https://arxiv.org/abs/2405.11746|[['cs.AI', 'cs.LG'], ['ICML']]
2024-05-19|Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning|foundation model is biased toward certain classes that effect semi supervised learning, solve by modified softmax and label smoothing|https://arxiv.org/abs/2405.11756|[['cs.LG'], ['ICML']]
2024-05-19|DATR: Unsupervised Domain Adaptive Detection Transformer with Dataset-Level Adaptation and Prototypical Alignment|unsupervisedly learn domain prototype for cross domain detection|https://arxiv.org/abs/2405.11765|[['cs.CV']]
2024-05-19|Your Transformer is Secretly Linear|some block in transformer is actually linear function use linear approximation of them won't effect performance|https://arxiv.org/abs/2405.12250|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-19|Learning More Generalized Experts by Merging Experts in Mixture-of-Experts|merge MoE module according usage frequency|https://arxiv.org/abs/2405.11530|[['cs.LG']]
2024-05-20|FeTT: Continual Class Incremental Learning via Feature Transformation Tuning|Fix backbone and use non-learning based transform can do class incremental learning|https://arxiv.org/abs/2405.11822|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CV']]
2024-05-20|SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model|as title|https://arxiv.org/abs/2405.11831|[['GPU memory'], ['cs.LG', 'eess.AS']]
2024-05-20|Rethinking Overlooked Aspects in Vision-Language Models|pipeline to select data for better sample efficiency|https://arxiv.org/abs/2405.11850|[['Vision-Language'], ['cs.CV']]
2024-05-20|Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process|combine the last two steps when training LLM|https://arxiv.org/abs/2405.11870|[['training efficiency'], ['cs.AI', 'cs.CL']]
2024-05-20|MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning|challenge the low rank assumption in LoRA|https://arxiv.org/abs/2405.12130|[['Parameter-Efficient', 'Efficient Fine-Tuning'], ['cs.LG', 'cs.CL']]
2024-05-20|Unveiling and Manipulating Prompt Influence in Large Language Models|estimate token significance based on embedding distribution to do prompt manipulation|https://arxiv.org/abs/2405.11891|[['cs.AI', 'cs.CL'], ['ICLR']]
2024-05-20|CSTA: CNN-based Spatiotemporal Attention for Video Summarization|CNN sliding windows attention for more efficiency spatiotemporal understanding|https://arxiv.org/abs/2405.11905|[['cs.CV'], ['CVPR']]
2024-05-20|xFinder: Robust and Pinpoint Answer Extraction for Large Language Models|a model for extracting answer from natural language response to robust evaluation of LLM|https://arxiv.org/abs/2405.11874|[['cs.CL']]
2024-05-20|Multiple-Choice Questions are Efficient and Robust LLM Evaluators|correlation of multiple choice prompt and natural response problem is high, maybe multiple choice is enough|https://arxiv.org/abs/2405.11966|[['cs.CL']]
2024-05-20|MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering|multilingual VQA benchmark|https://arxiv.org/abs/2405.11985|[['cs.CV']]
2024-05-20|WorldAfford: Affordance Grounding based on Natural Language Instructions|grounding according to complex human instruction|https://arxiv.org/abs/2405.12461|[['cs.AI', 'cs.CV']]
2024-05-20|TinyLLaVA Factory: A Modularized Codebase for Small-scale Large Multimodal Models|codebase for multimodal model|https://arxiv.org/abs/2405.11788|[['cs.LG']]
2024-05-20|Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices|temporal consistency of video editing base on the observation of slices on temporal dimension of video can look like natural images|https://arxiv.org/abs/2405.12211|[['Diffusion', 'synthesis', 'Video Editing', 'Text-to-Image'], ['cs.CV'], ['ICML']]
2024-05-21|PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference|adjust KV cache based on the number of important key and value in each layer|https://arxiv.org/abs/2405.12532|[['GPU memory'], ['cs.CL']]
2024-05-21|C3L: Content Correlated Vision-Language Instruction Tuning Data Generation via Contrastive Learning|note the problem of generating visual instruction tuning data from LLM and use contrastive learning to do calibrate|https://arxiv.org/abs/2405.12752|[['Vision-Language'], ['cs.CV']]
2024-05-21|Inconsistency-Aware Cross-Attention for Audio-Visual Fusion in Dimensional Emotion Recognition|different modalities may show low complementary relationship, add gating mechanism to deal with this|https://arxiv.org/abs/2405.12853|[['Audio-Visual'], ['cs.CV']]
2024-05-21|OmniGlue: Generalizable Feature Matching with Foundation Model Guidance|improve image matching on unseen domain using DINO|https://arxiv.org/abs/2405.12979|[['cs.CV'], ['CVPR']]
2024-05-21|Context-Enhanced Video Moment Retrieval with Large Language Models|use LLM to enhance query context to improve moment retrieval|https://arxiv.org/abs/2405.12540|[['cs.CV']]
2024-05-21|Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression|control outlier of KV cache to impreve KV cache quantization and decomposition|https://arxiv.org/abs/2405.12591|[['cs.CL']]
2024-05-21|Mamba in Speech: Towards an Alternative to Self-Attention|as titile|https://arxiv.org/abs/2405.12609|[['cs.SD', 'eess.AS']]
2024-05-21|Tagengo: A Multilingual Chat Dataset|dataset of prompt-response pairs in multiple languages|https://arxiv.org/abs/2405.12612|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-21|Text-Video Retrieval with Global-Local Semantic Consistent Learning|text video retriveal by considering both global and local semantics|https://arxiv.org/abs/2405.12710|[['cs.CV']]
2024-05-21|BIMM: Brain Inspired Masked Modeling for Video Representation Learning|MAE for image, video, hand-crafted features simultaneously|https://arxiv.org/abs/2405.12757|[['cs.CV']]
2024-05-21|Reducing Transformer Key-Value Cache Size with Cross-Layer Attention|reduce KV cache by sharing key value head between adjacent heads|https://arxiv.org/abs/2405.12981|[['cs.LG', 'cs.CL']]
2024-05-21|Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting|investigate different fine-tune strategy with few shot setting|https://arxiv.org/abs/2405.13181|[['cs.LG', 'cs.CL']]
2024-05-21|Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum|batching strategy for efficient pretraining of LLM, prevent naive chunk and caoncatenate|https://arxiv.org/abs/2405.13226|[['cs.LG', 'cs.CL']]
2024-05-21|MELD-ST: An Emotion-aware Speech Translation Dataset|speech translation with emotion label (english-japanese and english-german)|https://arxiv.org/abs/2405.13233|[['cs.CL']]
2024-05-21|Vision Transformer with Sparse Scan Prior|local, sparse, hierarchical attention for ViT with lower FLOPs|https://arxiv.org/abs/2405.13335|[['cs.CV']]
2024-05-21|Semantic Equitable Clustering: A Simple, Fast and Effective Strategy for Vision Transformer|semantic aware region partion for attention|https://arxiv.org/abs/2405.13337|[['cs.CV']]
2024-05-21|Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks|defense attack based on quantization by not rounding simply to nearest value|https://arxiv.org/abs/2405.12725|[['Attacks'], ['cs.CV'], ['CVPR']]
2024-05-21|ReALLM: A general framework for LLM compression and fine-tuning|unified frame work for quantization and finetuning by decompose weights and updating some of them|https://arxiv.org/abs/2405.13155|[['cs.LG']]
2024-05-22|No Filter: Cultural and Socioeconomic Diversityin Contrastive Vision-Language Models|explore performance gap of CLIP about different region, note that data filter make CLIP more bias|https://arxiv.org/abs/2405.13777|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-05-22|Dense Connector for MLLMs|use visual features from different layers other than the last one|https://arxiv.org/abs/2405.13800|[['vision-language'], ['cs.AI', 'cs.CV']]
2024-05-22|What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions|efficient gradient-based method to evaluate impoortance of data|https://arxiv.org/abs/2405.13954|[['GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-22|Unveiling the Tapestry of Consistency in Large Vision-Language Models|benchmark consistency of VLM when facing different prompt (open ended question, multiple choice question)|https://arxiv.org/abs/2405.14156|[['Vision-Language'], ['cs.CV']]
2024-05-22|Gradient Projection For Parameter-Efficient Continual Learning|reformulate PEFT methods to preserve old tasks prediction by orthogonal gradient projection|https://arxiv.org/abs/2405.13383|[['Parameter-Efficient'], ['cs.LG']]
2024-05-22|Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation|evaluate the performance of RAG by automatically generated exam|https://arxiv.org/abs/2405.13622|[['cs.CL'], ['ICML']]
2024-05-22|Spectral Adapter: Fine-Tuning in Spectral Space|fine tune the top spectral space of SVD of pretrained weights, experiments on GLUE and difussion model|https://arxiv.org/abs/2405.13952|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.AI', 'cs.LG']]
2024-05-22|Efficient Multitask Dense Predictor via Binarization|use distiallation to obtain a binary backbone for multiple dense prediction task (depth, edge, segmention)|https://arxiv.org/abs/2405.14136|[['cs.CV'], ['CVPR']]
2024-05-22|AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs|separate outlier weights to preserve original distribution after quantization|https://arxiv.org/abs/2405.13358|[['cs.CL']]
2024-05-22|VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding|add timestamp information to visual feature, similar to timechat but apply in feature space rather than natural language, instruction dataset for temporal tasks|https://arxiv.org/abs/2405.13382|[['cs.CV']]
2024-05-22|Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning|dynamically change instruction tuning data distribution based on difficulty measured by the other LLM|https://arxiv.org/abs/2405.13448|[['cs.CL']]
2024-05-22|ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation|transplantation alignment results from one LLM to another|https://arxiv.org/abs/2405.13578|[['cs.CL']]
2024-05-22|Safety Alignment for Vision Language Models|note that visual branch of VLM is vulnerable to attach, add modules before LLM to solve|https://arxiv.org/abs/2405.13581|[['Vision Language', 'VLMs'], ['attacks'], ['cs.AI', 'cs.CV']]
2024-05-22|Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations|use corrective feedback and demonstration to align LLM|https://arxiv.org/abs/2405.13828|[['cs.AI', 'cs.CL']]
2024-05-22|TOPA: Extend Large Language Models for Video Understanding via Text-Only Pre-Alignment|use video frame captions and CLIP text encoder to mimic video data enable video alignment without video|https://arxiv.org/abs/2405.13911|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-22|One-shot Training for Video Object Segmentation|general methods by manipulating time order of pseudo labels for video segmentation model to fine-tune on single video|https://arxiv.org/abs/2405.14010|[['cs.CV']]
2024-05-22|AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability|different samples need different effort to align, learn alignment capability tokens during pretraining, and combine these token according to instruction data during alignment|https://arxiv.org/abs/2405.14129|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-22|There is HOPE to Avoid HiPPOs for Long-memory State Space Models|replace HiPPO theory so that specific initialization and exponential decaying memory is not required, and training is more stable|https://arxiv.org/abs/2405.13975|[['cs.LG']]
2024-05-23|Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models|in retrieval system using CLIP, make new embeddings compatible with old embeddings to prevent re-computing embeddings|https://arxiv.org/abs/2405.14715|[['parameter-efficient'], ['Vision-Language'], ['cs.AI', 'cs.CV']]
2024-05-23|Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations|combining MAE and CLIP pretraining method|https://arxiv.org/abs/2405.14239|[['Vision-language'], ['cs.LG', 'cs.CV']]
2024-05-23|Segformer++: Efficient Token-Merging Strategies for High-Resolution Semantic Segmentation|experiment with different token merging strategies for segmentation|https://arxiv.org/abs/2405.14467|[['training efficiency'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-05-23|Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference|layer residual connection of adapters and token merging|https://arxiv.org/abs/2405.14700|[['Parameter-efficient', 'PEFT', 'Efficient Fine-tuning', 'GPU memory'], ['cs.CV']]
2024-05-23|FLoRA: Low-Rank Core Space for N-dimension|LoRA that preserves structural locality of activation in high dimension operation like convolution|https://arxiv.org/abs/2405.14739|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CV']]
2024-05-23|Bitune: Bidirectional Instruction-Tuning|use bidirectional attention rather than causal masking during prefilling|https://arxiv.org/abs/2405.14862|[['parameter-efficient', 'PEFT', 'efficient finetuning'], ['cs.CL']]
2024-05-23|Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining|scaling law of LLM and find that data mixing as some benefit to pretrainng|https://arxiv.org/abs/2405.14908|[['training efficiency'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-23|ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning|spatiotemporal culstering, random permutation and autoregressivly prediction as pretext task|https://arxiv.org/abs/2405.15160|[['training efficiency', 'GPU memory'], ['cs.CV']]
2024-05-23|VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks|reduce parameter counts of LoRA by sharing weights in codebook (similar idea as hyper network)|https://arxiv.org/abs/2405.15179|[['Parameter Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CL']]
2024-05-23|EMR-Merging: Tuning-Free High-Performance Model Merging|elect a unified model, generate mask of weights and rescale factors for model merging|https://arxiv.org/abs/2405.17461|[['PEFT'], ['cs.LG', 'cs.CV']]
2024-05-23|ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification|use attention score to identify salient tokens to help KV cache quantization|https://arxiv.org/abs/2405.14256|[['GPU memory'], ['cs.AI', 'cs.LG']]
2024-05-23|Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models|adaptive adjust number of expert to activate (to train)|https://arxiv.org/abs/2405.14297|[['Vision-Language'], ['cs.AI', 'cs.LG']]
2024-05-23|Multi-Scale VMamba: Hierarchy in Hierarchy Visual State Space Model|use depth wise convolution to apply SSM on features of different scales|https://arxiv.org/abs/2405.14174|[['cs.CV']]
2024-05-23|From Text to Pixel: Advancing Long-Context Understanding in MLLMs|MLLM for long text and multiple images|https://arxiv.org/abs/2405.14213|[['cs.CV', 'cs.CL']]
2024-05-23|RAQ-VAE: Rate-Adaptive Vector-Quantized Variational Autoencoder|VAE whose codebook comes from clustering of other codebook or sequence to sequence model to improve scalability of VQ-VAE|https://arxiv.org/abs/2405.14222|[['cs.LG', 'cs.CV', 'eess.IV']]
2024-05-23|Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast|substract inactivated expert's output from that of activated ones to improve performance|https://arxiv.org/abs/2405.14507|[['cs.LG', 'cs.CL']]
2024-05-23|RE-Adapt: Reverse Engineered Adaptation of Large Language Models|reverse engineering to obtain LLM without instruction following ability, update domain specific knowledge on this model and add instruction tuning vector back|https://arxiv.org/abs/2405.15007|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-23|RAEE: A Training-Free Retrieval-Augmented Early Exiting Framework for Efficient Inference|early exiting by retrieving based on distribution matching|https://arxiv.org/abs/2405.15198|[['cs.CL']]
2024-05-23|Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for LLMs|parallel decoding based on pretrained LLM's original ability for faster inference|https://arxiv.org/abs/2405.15208|[['cs.AI', 'cs.CL']]
2024-05-23|Not All Language Model Features Are Linear|try to explore the property of latent representation in LLM, find that they can be circular (months of year)|https://arxiv.org/abs/2405.14860|[['cs.LG']]
2024-05-24|Learning from True-False Labels via Multi-modal Prompt Retrieving|strategy of generating pseudo label by asking VLM true or false rather than open questions|https://arxiv.org/abs/2405.15228|[['vision-language', 'VLMs'], ['cs.LG', 'cs.CV']]
2024-05-24|Sparse Matrix in Large Language Model Fine-tuning|select sparse protion of weights to fine-tune, won't plateau as trainable parameters increases like LoRA|https://arxiv.org/abs/2405.15525|[['parameter-efficient', 'PEFT', 'efficient fine-tuning', 'GPU memory'], ['cs.CL']]
2024-05-24|Streaming Long Video Understanding with Large Language Models|memory base method (similar to RNN) to handle lone videos|https://arxiv.org/abs/2405.16009|[['vision-language'], ['cs.CV']]
2024-05-24|SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models|add weights after pruning to retain performance|https://arxiv.org/abs/2405.16057|[['Parameter-Efficient', 'Efficient Fine-Tuning'], ['cs.LG', 'cs.CL']]
2024-05-24|Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation|hyper network to generate soft prompts|https://arxiv.org/abs/2405.15282|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.AI', 'cs.LG']]
2024-05-24|BiSup: Bidirectional Quantization Error Suppression for Large Language Models|weight activation qantization that takes attention mechanism into account|https://arxiv.org/abs/2405.15346|[['parameter-efficient', 'efficient fine-tuning'], ['diffusion'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-24|Less is more: Summarizing Patch Tokens for efficient Multi-Label Class-Incremental Learning|task specific sub-network for eacch task, train a token summarizer to reduce computation cost|https://arxiv.org/abs/2405.15633|[['cs.AI', 'cs.CV']]
2024-05-24|What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models|project outputs of MLM (prediction, class description, image feature) to a common feature space as the query feature (just like ensemble feature for CLIP)|https://arxiv.org/abs/2405.15668|[['cs.CV']]
2024-05-24|Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models|make the visual encoder of VLM aware of text prompt by feeding global textual feature to visual encoder and apply additional attention conditioned on text after visual encoder|https://arxiv.org/abs/2405.15684|[['cs.AI', 'cs.CV']]
2024-05-24|LM4LV: A Frozen Large Language Model for Low-level Vision Tasks|let LLM to generate images by using embedding extracted by encoder as target|https://arxiv.org/abs/2405.15734|[['vision-language'], ['text-to-image'], ['cs.CV']]
2024-05-24|ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models|hierarchical visual encoder that reduce output token length to enable high resolution input|https://arxiv.org/abs/2405.15738|[['cs.CV']]
2024-05-24|Sparse Expansion and Neuronal Disentanglement|copy weights to MoE modules and apply pruning, speed up inference while maintaining performance due to feature disentangle|https://arxiv.org/abs/2405.15756|[['cs.AI', 'cs.LG'], ['NeurIPS']]
2024-05-24|Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications|as title|https://arxiv.org/abs/2405.15877|[['cs.LG', 'cs.CL']]
2024-05-24|HyperInterval: Hypernetwork approach to training weight interval regions in continual learning|use hyper network to select interval of weights for each task to prevent forgetting|https://arxiv.org/abs/2405.15444|[['cs.AI', 'cs.LG']]
2024-05-24|MoEUT: Mixture-of-Experts Universal Transformers|combining unversal transformer (layer shared transformer) and MoE|https://arxiv.org/abs/2405.16039|[['cs.AI', 'cs.LG']]
2024-05-25|CRoFT: Robust Fine-Tuning with Concurrent Optimization for OOD Generalization and Open-Set OOD Detection|regularization of minimizing gradient magnitude of energy scores to improve OOD detection|https://arxiv.org/abs/2405.16417|[['vision-language'], ['cs.CV'], ['ICML']]
2024-05-25|DynRefer: Delving into Region-level Multi-modality Tasks via Dynamic Resolution|incorporate image of different region level for fine-grained tasks|https://arxiv.org/abs/2405.16071|[['vision-language'], ['cs.CV']]
2024-05-25|Keypoint-based Progressive Chain-of-Thought Distillation for LLMs|tricks for distilling rationale ability to small model, predict token importance and rationale difficulties|https://arxiv.org/abs/2405.16064|[['cs.CL'], ['ICML']]
2024-05-25|Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection|use LLM prefilling to assess relevance of retrieved document parallelly to accelerate RAG|https://arxiv.org/abs/2405.16178|[['cs.CL']]
2024-05-25|SpinQuant: LLM quantization with learned rotations|rotate weights make quantization easier|https://arxiv.org/abs/2405.16406|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-05-26|LoQT: Low Rank Adapters for Quantized Training|SVD gradient of the weights to initialize LoRA|https://arxiv.org/abs/2405.16528|[['cs.LG', 'cs.CL']]
2024-05-26|Compressing Lengthy Context With UltraGist|segment input text and insert special tokens to gather information by cross attention thus decreasing input length|https://arxiv.org/abs/2405.16635|[['cs.CL']]
2024-05-27|Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR|pinpoint some shortcoming of VLM to improve|https://arxiv.org/abs/2405.16934|[['Vision-Language'], ['cs.CV']]
2024-05-27|Unifying Demonstration Selection and Compression for In-Context Learning|as title|https://arxiv.org/abs/2405.17062|[['parameter-efficient'], ['cs.CL']]
2024-05-27|DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution|replace LoRA with multiple 1 rank sub-modules and then apply pruning|https://arxiv.org/abs/2405.17357|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CL']]
2024-05-27|LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters|use SVD to reduce trainable parameters in LoRA|https://arxiv.org/abs/2405.17604|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-27|Visual Anchors Are Strong Information Aggregators For Multimodal Large Language Model|some tokens called visual anchors though in background region are informative, identify and use them for VLM inputs|https://arxiv.org/abs/2405.17815|[['vision-language'], ['cs.CV']]
2024-05-27|Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models|prevent VLM from focusing on few image tokens to reduce hallucination|https://arxiv.org/abs/2405.17820|[['Vision Language'], ['cs.AI', 'cs.CV']]
2024-05-27|RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs|data augmentation to reduce hallucination|https://arxiv.org/abs/2405.17821|[['Vision Language'], ['cs.AI', 'cs.CV']]
2024-05-27|$\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning|use synthetic data to transfer LoRA of a LLM to another one|https://arxiv.org/abs/2405.17258|[['Parameter Efficient', 'PEFT', 'Efficient Finetuning'], ['cs.AI', 'cs.LG']]
2024-05-27|Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention|new linear attention design for efficient training and inference|https://arxiv.org/abs/2405.17381|[['cs.CL'], ['ICML']]
2024-05-27|MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance|prevent gradient conflict when using multimodal objectives|https://arxiv.org/abs/2405.17730|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-05-27|From Obstacle to Opportunity: Enhancing Semi-supervised Learning with Synthetic Data|taking synthetic data on Web into account when performing semi supervised learning|https://arxiv.org/abs/2405.16930|[['cs.CV']]
2024-05-27|VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models|VLM that do reasoning by object grounding boxes|https://arxiv.org/abs/2405.16919|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-27|Compositional Few-Shot Class-Incremental Learning|decompose a class into sub concepts and construct class concept by these prototypes|https://arxiv.org/abs/2405.17022|[['cs.AI', 'cs.CV']]
2024-05-27|Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization|dataset that challenges tokenization process of LLM|https://arxiv.org/abs/2405.17067|[['cs.AI', 'cs.CL']]
2024-05-27|Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective|unified analysis for linear complexity sequence model|https://arxiv.org/abs/2405.17383|[['cs.CL']]
2024-05-28|Low-Rank Few-Shot Adaptation of Vision-Language Models|use LoRA in CLIP for few shot adaptation|https://arxiv.org/abs/2405.18541|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['Vision-Language', 'VLMs'], ['cs.CV']]
2024-05-28|Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment|use prediction results of VLM with or without image input which idicates the text image correlation as weights for loss|https://arxiv.org/abs/2405.17871|[['Vision Language', 'VLMs'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-28|VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections|compress intermediate activateion to reduce memory usage during fine tuning|https://arxiv.org/abs/2405.17991|[['PEFT'], ['cs.AI', 'cs.CV']]
2024-05-28|IAPT: Instruction-Aware Prompt Tuning for Large Language Models|prompt generator to generate soft prompts for each layer based on input|https://arxiv.org/abs/2405.18203|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL']]
2024-05-28|Frustratingly Easy Test-Time Adaptation of Vision-Language Models|test time adaptation by augmentation, confidence filtering, marginalize after setting temperature to zero|https://arxiv.org/abs/2405.18330|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-05-28|OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning|dynamically select layers to train and project gradient to low rank space to reduce memory usage|https://arxiv.org/abs/2405.18380|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-28|WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization|use embedding for class name and image to find domain specific feature|https://arxiv.org/abs/2405.18405|[['vision-language'], ['cs.AI', 'cs.CV']]
2024-05-28|Why are Visually-Grounded Language Models Bad at Image Classification?|LLM with visual input do worse than CLIP on classification|https://arxiv.org/abs/2405.18415|[['VLMs'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-05-28|ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention|linear attention model designed for vision|https://arxiv.org/abs/2405.18425|[['GPU memory'], ['cs.AI', 'cs.CV']]
2024-05-28|Mitigating Object Hallucination via Data Augmented Contrastive Tuning|as title|https://arxiv.org/abs/2405.18654|[['vision-language'], ['cs.CV']]
2024-05-28|LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification|generate hierarchical visual concept by GPT4V and use text features of different level to do classification|https://arxiv.org/abs/2405.18672|[['vision-language'], ['cs.CV', 'cs.CL']]
2024-05-28|Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model|new disign of architecture for multimodal mamba|https://arxiv.org/abs/2405.18014|[['GPU memory'], ['cs.AI']]
2024-05-28|Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities|fuse two LLM (discrete speech token and text) by cross attention|https://arxiv.org/abs/2405.18669|[['cs.AI', 'cs.LG', 'cs.CL', 'eess.AS'], ['NeurIPS']]
2024-05-28|Provable Contrastive Continual Learning|as title|https://arxiv.org/abs/2405.18756|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-05-28|FocSAM: Delving Deeply into Focused Objects in Segmenting Anything|dynamically focus on query region to improve SAM in interative scenario|https://arxiv.org/abs/2405.18706|[['cs.CV'], ['CVPR']]
2024-05-28|OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision|pseudo label for unknown object in traning data to prevent detector bias on seen categories|https://arxiv.org/abs/2405.17913|[['cs.AI', 'cs.CV']]
2024-05-28|The Evolution of Multimodal Model Architectures|surevy paper for model architectures of MLLM|https://arxiv.org/abs/2405.17927|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'eess.AS']]
2024-05-28|DMT-JEPA: Discriminative Masked Targets for Joint-Embedding Predictive Architecture|region level (clustered by feature similarity) MAE|https://arxiv.org/abs/2405.17995|[['cs.AI', 'cs.LG', 'cs.CV', 'eess.IV']]
2024-05-28|MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution|different size of patch embedding kernel for images of different resolution|https://arxiv.org/abs/2405.18240|[['cs.CV']]
2024-05-28|Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass|use superposition of candidate embeddings to obtain multiple predictions from a single pass|https://arxiv.org/abs/2405.18400|[['cs.LG', 'cs.CL']]
2024-05-28|Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference|parallel decoding that accelerate inference without memory overhead|https://arxiv.org/abs/2405.18628|[['cs.LG', 'cs.CL']]
2024-06-13|PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation|LoRA finetune and distillation at the same time|https://arxiv.org/abs/2406.09117|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.CV'], ['CVPR']]
2024-06-13|Towards Multilingual Audio-Visual Question Answering|multilingual audio visual QA and its multi branch baseline|https://arxiv.org/abs/2406.09156|[['Audio-Visual'], ['cs.LG', 'cs.CV', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-13|MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning|decompose the pretrained weights, only tune the component with smaller singular values|https://arxiv.org/abs/2406.09044|[['Parameter-Efficient', 'Efficient finetuning'], ['cs.CL']]
2024-06-13|Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA|use clustering and key frame selection for efficient long video processing|https://arxiv.org/abs/2406.09396|[['vision language', 'VLMs'], ['cs.CV']]
2024-06-13|AdaPTwin: Low-Cost Adaptive Compression of Product Twins in Transformers|weight compression with initialization from SVD|https://arxiv.org/abs/2406.08904|[['cs.LG', 'cs.SD', 'eess.AS'], ['NeurIPS']]
2024-06-13|LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks|continual pretraining for speech SSL model with alignment loss|https://arxiv.org/abs/2406.09153|[['cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-13|Adaptive Slot Attention: Object Discovery with Dynamic Slot Number|object number aware designed attention for object centric learning|https://arxiv.org/abs/2406.09196|[['cs.LG', 'cs.CV'], ['CVPR']]
2024-06-13|Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations|measurement for speaker and phone representation in speech SSL models|https://arxiv.org/abs/2406.09200|[['cs.CL'], ['Interspeech']]
2024-06-13|Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection|use modality specific LoRA to adapt LLM to multi modalities inputs (text, video, audio) while being robust to missing modality|https://arxiv.org/abs/2406.09617|[['cs.CL', 'eess.AS'], ['Interspeech']]
2024-06-13|Exploring Training on Heterogeneous Data with Mixture of Low-rank Adapters|MoE design for multiple input domain, output task, modality|https://arxiv.org/abs/2406.09679|[['cs.CV'], ['ICML']]
2024-06-13|Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning|architecture design for multilingual unseen speaker speech emotion recogonition|https://arxiv.org/abs/2406.08931|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-06-13|ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models|selective quantization and routing for memory efficient MoE|https://arxiv.org/abs/2406.09041|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-13|Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs|fine tune with retionale generated by tree of thought to improve chain of thought ability|https://arxiv.org/abs/2406.09136|[['cs.LG', 'cs.CL']]
2024-06-13|Reducing Task Discrepancy of Text Encoders for Zero-Shot Composed Image Retrieval|use text feature with noise to mimic image feature in CLIP for composed image retrieval (retrieval based on both image and text)|https://arxiv.org/abs/2406.09188|[['cs.CV']]
2024-06-13|MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction|refine quantization model by reconstructing fulle precision model|https://arxiv.org/abs/2406.09229|[['cs.CV']]
2024-06-13|You Don't Need Data-Augmentation in Self-Supervised Learning|argue that SSL in vision (DINO) don't need data augmentation as long as dataset is large enough|https://arxiv.org/abs/2406.09294|[['cs.LG', 'cs.CV']]
2024-06-13|DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding|use discrete unit for speech lanugae model|https://arxiv.org/abs/2406.09345|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-06-13|Improving Autoregressive Training with Dynamic Oracles|training technique for discrepancy generated from teacher forcing and target variation in autoregressive model|https://arxiv.org/abs/2406.09393|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-13|4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities|encoder decoder model for a lot of task and modalities (text, depth, rgb, edge...), training on feature map of SOTA models, pseudo labels...|https://arxiv.org/abs/2406.09406|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-13|Explore the Limits of Omni-modal Pretraining at Scale|pretraining with paired text, audio video data|https://arxiv.org/abs/2406.09412|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-13|VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding|use both image and video as visual input for VLM|https://arxiv.org/abs/2406.09418|[['cs.CV']]
2024-06-12|KernelWarehouse: Rethinking the Design of Dynamic Convolution|dynamic (input dependent) convolution kernel with shared component to reduce number of parameters|https://arxiv.org/abs/2406.07879|[['parameter efficient'], ['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-06-12|Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models|token sampler to reducen token length of videos, highlight importance of video instruction data (temporal understanding)|https://arxiv.org/abs/2406.08024|[['Vision-Language'], ['cs.AI', 'cs.CV']]
2024-06-12|VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks|VLM that is able to use other visual task expert by predicting special routing token to handle different task|https://arxiv.org/abs/2406.08394|[['Vision-Language'], ['cs.CV']]
2024-06-12|The Impact of Initialization on LoRA Finetuning Dynamics|theoretical analysis for initialization of LoRA, we should initialize the first weights to random and the second one to 0|https://arxiv.org/abs/2406.08447|[['PEFT'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-12|VLind-Bench: Measuring Language Priors in Large Vision-Language Models|measuring whether VLM response based solely on textual and disregarding image|https://arxiv.org/abs/2406.08702|[['Vision-Language'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-12|Updating CLIP to Prefer Descriptions Over Captions|pinoint the difference between description (same information as image) and caption (complement information in image), fine-tune CLIP to prefer description|https://arxiv.org/abs/2406.09458|[['parameter efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-12|Small Scale Data-Free Knowledge Distillation|improve process of sampling synthetic data for data free knowledge distillation|https://arxiv.org/abs/2406.07876|[['training efficiency'], ['synthesize'], ['cs.AI', 'cs.LG', 'cs.CV'], ['CVPR']]
2024-06-12|Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations|train several encoder for different features by contrastive learning to better ensemble all features|https://arxiv.org/abs/2406.07900|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-12|Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation|reuse layers in teacher model as student model to prevent alignment disagreement of CTC model|https://arxiv.org/abs/2406.07909|[['cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-12|Self-Supervised Speech Representations are More Phonetic than Semantic|semantically similar audio show higher similarity than phonetically similar audio under speech SSL model, suggest that intent classification dataset is no adequate to measure semantic feature|https://arxiv.org/abs/2406.08619|[['cs.LG', 'cs.CL', 'eess.AS'], ['Interspeech']]
2024-06-12|An Empirical Study of Mamba-based Language Models|benchmark transformer, mamba, and hybrid model with same amount of training data|https://arxiv.org/abs/2406.07887|[['cs.LG', 'cs.CL']]
2024-06-12|LVBench: An Extreme Long Video Understanding Benchmark|as title|https://arxiv.org/abs/2406.08035|[['cs.AI', 'cs.CV']]
2024-06-12|Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking|adaptively predict whether to bypass a transformer block to speedup inference|https://arxiv.org/abs/2406.08037|[['cs.CV']]
2024-06-12|Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams|highlight the online scenario of video understanding (causality during watching video), memory based method and a benchmark|https://arxiv.org/abs/2406.08085|[['cs.CV']]
2024-06-12|What If We Recaption Billions of Web Images with LLaMA-3?|recaption image text pair to improve data quality for VLM|https://arxiv.org/abs/2406.08478|[['vision-language'], ['Diffusion', 'text-to-image'], ['cs.CV', 'cs.CL']]
2024-06-12|OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text|large scale document dataset with interleaved image and text|https://arxiv.org/abs/2406.08418|[['cs.AI', 'cs.CV']]
2024-06-12|Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models|text input dependent patch selector to deal with high resolution images|https://arxiv.org/abs/2406.08487|[['cs.CV']]
2024-06-12|Adaptive Teaching with Shared Classifier for Knowledge Distillation|knowledge distillation where teacher model adjust to better align with learning needs of student, can be applied to multiple teacher models scenario|https://arxiv.org/abs/2406.08528|[['cs.LG', 'cs.CV']]
2024-06-17|BaFTA: Backprop-Free Test-Time Adaptation For Zero-Shot Vision-Language Models|test time adaptation that utilize augmentation and clustering|https://arxiv.org/abs/2406.11309|[['Vision-Language'], ['cs.CV'], ['ICLR']]
2024-06-17|Mining Open Semantics from CLIP: A Relation Transition Perspective for Few-Shot Learning|learn a set of anchor concept (class), when encountering other class, consider its relation (similarity) with anchor to do prediction|https://arxiv.org/abs/2406.11252|[['Vision-Language'], ['cs.CV']]
2024-06-17|ClawMachine: Fetching Visual Tokens as An Entity for Referring and Grounding|make MLM directly output visual feature and retrieve from the input patches to handle grounding and referring problems|https://arxiv.org/abs/2406.11327|[['vision-language'], ['cs.CV']]
2024-06-17|Unveiling Encoder-Free Vision-Language Models|VLM without visual encoder to deal with flexible resolution and aspect ration input, start from LLM and align with visual encoder|https://arxiv.org/abs/2406.11832|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-06-17|MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs|as title|https://arxiv.org/abs/2406.11833|[['Vision-Language'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-17|MCSD: An Efficient Language Model with Diverse Fusion|architecture design for linear time complexity and constant space complexity sequence modeling|https://arxiv.org/abs/2406.12230|[['GPU memory'], ['cs.AI', 'cs.CL']]
2024-06-17|TroL: Traversal of Layers for Large Language and Vision Models|apply a transformer block multiple times and aggregate all the results|https://arxiv.org/abs/2406.12246|[['vision language'], ['cs.LG', 'cs.CV', 'cs.CL']]
2024-06-17|Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts|weight the data sampling rate based on MoE's gate load|https://arxiv.org/abs/2406.11256|[['cs.CL']]
2024-06-17|ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking|compression training checkpoints by comparing adjacent checkpoints and evaluate weight importance by optimizer|https://arxiv.org/abs/2406.11257|[['cs.LG'], ['ICML']]
2024-06-17|MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens|as title|https://arxiv.org/abs/2406.11271|[['cs.LG', 'cs.CV']]
2024-06-17|Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers|queries in a layer can interact with keys and values from a preceding layer, improve the ability to capture dependencies bewteen high and low level feature|https://arxiv.org/abs/2406.11274|[['cs.CL']]
2024-06-17|Meta Reasoning for Large Language Models|make LLM planning how to reason before reasoning|https://arxiv.org/abs/2406.11698|[['cs.CL']]
2024-06-17|GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities|audio language model uses q-former, MLP, multi level cross attention as a connector between and audio encoder LLM|https://arxiv.org/abs/2406.11768|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-06-16|Concept-skill Transferability-based Data Selection for Large Vision-Language Models|use a small VLM to do clustering, consider similarity between concept clusters, scheduled sampling from each cluster|https://arxiv.org/abs/2406.10995|[['efficient finetuning'], ['Vision-Language'], ['cs.LG', 'cs.CV']]
2024-06-16|VELOCITI: Can Video-Language Models Bind Semantic Concepts through Time?|benchmark for VLM to asscociate entities through appropriate relationships|https://arxiv.org/abs/2406.10889|[['vision-language'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-16|Few-Shot Recognition via Stage-Wise Augmented Finetuning|retrieve pretraining data based on few shot examples and finetune on examples and retrieved data|https://arxiv.org/abs/2406.11148|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-16|On the Effectiveness of Supervision in Asymmetric Non-Contrastive Learning|add supervision to BYOL, theoreticaal insight suggest it will reduce intra-class variance|https://arxiv.org/abs/2406.10815|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-06-16|Pick-or-Mix: Dynamic Channel Sampling for ConvNets|dynamically choose channel to use to speedup forward pass|https://arxiv.org/abs/2406.10935|[['cs.CV'], ['CVPR']]
2024-06-16|NAST: Noise Aware Speech Tokenization for Speech Language Models|as title|https://arxiv.org/abs/2406.11037|[['cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-16|Fine-grained Classes and How to Find Them|finding fine grained classes from coarse labels|https://arxiv.org/abs/2406.11070|[['cs.LG', 'cs.CV'], ['ICML']]
2024-06-16|Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation|besides using CLIP for pseudo label, use it as backbone, pseudo label from activation map|https://arxiv.org/abs/2406.11189|[['cs.CV'], ['CVPR']]
2024-06-16|Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved Tags|suggest that image connector discard fine grained detailes in images, retrieve tags by images to improve performance|https://arxiv.org/abs/2406.10839|[['cs.CV', 'cs.CL']]
2024-06-15|ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation|share LoRA weights across different layers|https://arxiv.org/abs/2406.10785|[['Parameter Efficient', 'PEFT'], ['cs.AI', 'cs.CL']]
2024-06-15|Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference|selectively load KV cache based on key values to speedup inference|https://arxiv.org/abs/2406.10774|[['cs.LG', 'cs.CL'], ['ICML']]
2024-06-15|Optimization-based Structural Pruning for Large Language Models without Back-Propagation|use policy gradient to learn a weight mask for pruning with respect to LLM loss|https://arxiv.org/abs/2406.10576|[['cs.LG', 'cs.CL']]
2024-06-15|Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models|observe that prompt gains more attention and more stable attention distribution are more generalizable, design objective toward this goal during prompt tuning|https://arxiv.org/abs/2406.10584|[['cs.CL']]
2024-06-15|BlockPruner: Fine-grained Pruning for Large Language Models|LLM pruning at MHA and MLP level, training free, based on perplexity|https://arxiv.org/abs/2406.10594|[['cs.CL']]
2024-06-15|CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training|data selection by Basyian optimization problem|https://arxiv.org/abs/2406.10670|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-15|SemanticMIM: Marring Masked Image Modeling with Semantics Compression for General Visual Representation|in masked image modeling, a set of learnable tokens (information bottleneck) are passed to decoder rather than encoded patch features|https://arxiv.org/abs/2406.10673|[['cs.CV']]
2024-06-15|RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning|sparse LoRA that is suitable for knowledge editting (only few weights will be modified)|https://arxiv.org/abs/2406.10777|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['Knowledge Editing'], ['cs.CL']]
2024-06-14|Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation|integrate visual features into Whisper for audio visual speech recognition|https://arxiv.org/abs/2406.10082|[['Audio-Visual'], ['cs.CV', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-14|Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data|intra and inter class confidence thresholding for pseudolabeling to alleviate impact of incorrect hard pseudolabel|https://arxiv.org/abs/2406.10502|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-06-14|HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning|only preserve entries with higher estimated score in the attention matrix to reduce complexity, the score is estimated by a tree search like algorithm|https://arxiv.org/abs/2406.09827|[['GPU memory'], ['cs.LG', 'cs.CV', 'cs.CL']]
2024-06-14|VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models|dataset to challenge VLM's ability to understand documents with interleaved images and text|https://arxiv.org/abs/2406.10228|[['Vision-Language'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-14|Open-Vocabulary Semantic Segmentation with Image Embedding Balancing|weighted sum of frozen CLIP feature and finetuned feature for better generalizability|https://arxiv.org/abs/2406.09829|[['cs.CV'], ['CVPR']]
2024-06-14|ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers|merge tokens with high enough similarity|https://arxiv.org/abs/2406.09936|[['cs.CV'], ['CVPR']]
2024-06-14|Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations|panoptic segmentation aware of hierarchical relation between object and parts, use learnable query to represent each object and do part segmentation within each object|https://arxiv.org/abs/2406.10114|[['cs.CV'], ['CVPR']]
2024-06-14|One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model|nested network to avoid training models of different sizes for multiple times|https://arxiv.org/abs/2406.10160|[['cs.AI', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-14|When Will Gradient Regularization Be Harmful?|gradient regularization may be harmful during warmup|https://arxiv.org/abs/2406.09723|[['cs.AI', 'cs.LG'], ['ICML']]
2024-06-14|Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition|use history utterarnce to generate soft prompts for specific user to improve ASR performance|https://arxiv.org/abs/2406.09873|[['cs.AI', 'cs.SD', 'eess.AS']]
2024-06-14|What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark|benchmark speech emotion recognition performance when training on combined dataset (with different sampling methods) or only in distributon dataset|https://arxiv.org/abs/2406.09933|[['cs.AI', 'cs.LG', 'cs.SD']]
2024-06-14|Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection|training free method enable streaming ASR (predict before finish input) using Whisper, use attention weights to detect truncated words|https://arxiv.org/abs/2406.10052|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-06-14|UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner|neural codec (built on LLM codebook) projecting audio to textual space enable in context learning of speech language model|https://arxiv.org/abs/2406.10056|[['cs.SD', 'eess.AS']]
2024-06-14|Localizing Events in Videos with Multimodal Queries|benchmark that use image (of different style) and text to do localization|https://arxiv.org/abs/2406.10079|[['cs.AI', 'cs.CV']]
2024-06-14|CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation|as title|https://arxiv.org/abs/2406.10462|[['cs.CV']]
2024-06-14|A Label is Worth a Thousand Images in Dataset Distillation|highlight the importance of soft labels assigned to synthetic data is more important (the way to synthesize data) during data distillation|https://arxiv.org/abs/2406.10485|[['cs.LG', 'cs.CV']]
2024-06-19|AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding|VLM and dataset for egocentric videos|https://arxiv.org/abs/2406.13807|[['parameter-efficient'], ['Vision-Language', 'VLMs'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-19|BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation|logit is long tail and noisy for knowledge distillation, align top k logits of both teacher and student and their corresponding parts to handle this problem|https://arxiv.org/abs/2406.13555|[['cs.AI', 'cs.CL'], ['EMNLP']]
2024-06-19|Controlling Forgetting with Test-Time Data in Continual Learning|in continual learning, use test time data to unsupervisedly refresh distribution in previous tasks|https://arxiv.org/abs/2406.13653|[['vision-language'], ['cs.LG']]
2024-06-19|A Primal-Dual Framework for Transformers and Neural Networks|theoretical analysis of self attention helps designing 2 new attention layers|https://arxiv.org/abs/2406.13781|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL'], ['ICLR']]
2024-06-19|SDQ: Sparse Decomposed Quantization for LLM Inference|apply sparsification and quantization at the same time|https://arxiv.org/abs/2406.13868|[['memory efficiency'], ['cs.AI', 'cs.LG']]
2024-06-19|Transferable speech-to-text large language model alignment module|concatenation style of speech language model, find that alignment connector can transfer from normal version to chat version of LLM|https://arxiv.org/abs/2406.13357|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-06-19|Improving Zero-Shot Cross-Lingual Transfer via Progressive Code-Switching|as title|https://arxiv.org/abs/2406.13361|[['cs.LG', 'cs.CL']]
2024-06-19|Strengthening Layer Interaction via Dynamic Layer Attention|use bindirectional RNN structure to maintain the feature for layer level attention|https://arxiv.org/abs/2406.13392|[['cs.CV']]
2024-06-19|In-Context Former: Lightning-fast Compressing Context for Large Language Model|learn a Q former for text to reduce context length|https://arxiv.org/abs/2406.13618|[['cs.CL']]
2024-06-19|Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations|generate few shot demonstration based on the query context to avoid long context for each demonstration|https://arxiv.org/abs/2406.13632|[['cs.CL']]
2024-06-19|SpatialBot: Precise Spatial Understanding with Vision Language Models|VLM benchmark, dataset, baseline for RGBD visual tasks|https://arxiv.org/abs/2406.13642|[['Vision Language', 'VLMs'], ['depth'], ['cs.CV']]
2024-06-18|VoCo-LLaMA: Towards Vision Compression with Large Language Models|distill visual feature into a learnable token by concatenating and processing via LLM, use attention mask to prevent text to attend visual token and increase context length|https://arxiv.org/abs/2406.12275|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-06-18|Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning|as title|https://arxiv.org/abs/2406.12742|[['visual language', 'VLMs'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-18|LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation|initialize LoRA with SVD, add a small square matrix in middle, freeze the first matrix from beginning, gradually freezes the last one, do compare GPU memory|https://arxiv.org/abs/2406.12832|[['parameter efficiency', 'GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-18|ViLCo-Bench: VIdeo Language COntinual learning Benchmark|query incremental for video retrieval (text or visual query, bounding box or temporal span targets)|https://arxiv.org/abs/2406.13123|[['memory-efficient'], ['cs.AI', 'cs.CV']]
2024-06-18|Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models|MoE for binaarized LLM, each expert is represented by a scaling factor|https://arxiv.org/abs/2406.12311|[['Memory-Efficient'], ['cs.LG']]
2024-06-18|Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model|prune a multi task model for each task to prevent catastrophic forgetting|https://arxiv.org/abs/2406.12317|[['cs.CL', 'eess.AS'], ['Interspeech']]
2024-06-18|Bayesian-LoRA: LoRA based Parameter Efficient Fine-Tuning using Optimal Quantization levels and Rank Values trough Differentiable Bayesian Gates|select rank and quantization level of LoRA from a Bayesian perspective|https://arxiv.org/abs/2406.13046|[['Parameter Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.AI']]
2024-06-18|Sparse High Rank Adapters|directly fine tune sparse set of pretrain weights, which prevent merging latency required by LoRA while using similar resources during training|https://arxiv.org/abs/2406.13175|[['memory-efficient', 'PEFT', 'Efficient Finetuning', 'GPU memory'], ['cs.AI', 'cs.LG']]
2024-06-18|Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters|as title|https://arxiv.org/abs/2406.12335|[['cs.LG', 'cs.CL']]
2024-06-18|From Instance Training to Instruction Learning: Task Adapters Generation from Instructions|generate LoRA weights by hyper network|https://arxiv.org/abs/2406.12382|[['cs.CL']]
2024-06-18|PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems|speech language model that input and output speech and its text counterparts in parallel|https://arxiv.org/abs/2406.12428|[['cs.AI', 'cs.LG', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-06-18|Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages|distil multilingual ability from machine translation expert to LLM|https://arxiv.org/abs/2406.12739|[['cs.CL']]
2024-06-18|DrVideo: Document Retrieval Based Long Video Understanding|transform long video into text to do relevant segment retrieval and iteratively refine and explore missing information|https://arxiv.org/abs/2406.12846|[['cs.CV']]
2024-06-18|D2O:Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models|observe varying pattern of attention weights density in different layer to determine different amount of KV cache, mergine similar token|https://arxiv.org/abs/2406.13035|[['cs.CL']]
2024-06-18|When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models|find that some subnetwork of LLM perform better than other or the full model, improve performance by reweighting these subnetworks|https://arxiv.org/abs/2406.13131|[['cs.CL']]
2024-06-18|Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style|additional transformer decoder, encoder layers for speculative decoding|https://arxiv.org/abs/2406.13170|[['cs.AI', 'cs.CL']]
2024-06-20|Live Video Captioning|dense video captioning in online scenario|https://arxiv.org/abs/2406.14206|[['cs.CV']]
2024-06-20|LiveMind: Low-latency Large Language Models with Simultaneous Inference|try to make response before all input is given to reduce latency|https://arxiv.org/abs/2406.14319|[['cs.AI', 'cs.CL']]
2024-06-20|Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities|let LLM write code to visualize visual related question and its reasoning for better performance|https://arxiv.org/abs/2406.14562|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-11|AutoTVG: A New Vision-language Pre-training Paradigm for Temporal Video Grounding|use asr, captioning model, CLIP and clustering to generate pseudo labels for temporal grounding|https://arxiv.org/abs/2406.07091|[['Vision-language'], ['cs.CV']]
2024-06-11|Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement|positional embedding to ensure model focuses more on the information in the middle of long context|https://arxiv.org/abs/2406.07138|[['training-efficient'], ['cs.CL']]
2024-06-11|Let Go of Your Labels with Unsupervised Transfer|theoretically analysis of unsupervised transfer (only has the number of class, no class name and label), I think it is some kind of clustering|https://arxiv.org/abs/2406.07236|[['vision-language'], ['cs.LG'], ['ICML']]
2024-06-11|AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning|extract text feature in demonstration and feed into LLM for incontext learning (usable for any MLM)|https://arxiv.org/abs/2406.07588|[['parameter-efficient'], ['cs.CL']]
2024-06-11|Evolving Subnetwork Training for Large Language Models|start from a subnetwork in LLM, gradually increase the size of subnetwork, save computational source for pre training|https://arxiv.org/abs/2406.06962|[['cs.AI', 'cs.CL'], ['ICML']]
2024-06-11|Scaling up masked audio encoder learning for general audio classification|1.2B MAE styled audio model train on 272k hours of audio|https://arxiv.org/abs/2406.06992|[['cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-11|Translating speech with just images|generate text supervision from image captioning, especially useful for low resource language|https://arxiv.org/abs/2406.07133|[['cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-11|When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models|directly apply linear attention to current LLM and speculative decoding strategy|https://arxiv.org/abs/2406.07368|[['cs.AI', 'cs.LG', 'cs.CL'], ['ICML']]
2024-06-11|FastAST: Accelerating Audio Spectrogram Transformer via Token Merging and Cross-Model Knowledge Distillation|as title|https://arxiv.org/abs/2406.07676|[['cs.AI', 'cs.LG', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-11|RWKV-CLIP: A Robust Vision-Language Representation Learner|use RWKV to train a CLIP on VLM refined dataset|https://arxiv.org/abs/2406.06973|[['Vision-Language'], ['synthesize'], ['cs.CV']]
2024-06-11|Teaching with Uncertainty: Unleashing the Potential of Knowledge Distillation in Object Detection|knowledge distillation takes uncertainty (estimated by Monte Carlo droupout) in teacher model into account|https://arxiv.org/abs/2406.06999|[['cs.CV']]
2024-06-11|Bridging Language Gaps in Audio-Text Retrieval|improve multilinugal audio text retrieval by machine translated text and multilingual text encoder|https://arxiv.org/abs/2406.07012|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-06-11|MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations|weight importancce may vary a lot when small weight perturbation occurs (data type change), they propose a estimation method with provable robustness|https://arxiv.org/abs/2406.07017|[['cs.LG', 'cs.CL']]
2024-06-11|Effectively Compress KV Heads for LLM|observe that KV cache is low rank, use low rank approximation for compression|https://arxiv.org/abs/2406.07056|[['cs.CL']]
2024-06-11|EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark|as title|https://arxiv.org/abs/2406.07162|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-06-11|Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?|concatenate asr and tts for speech translation, aligning feature space for better alignment|https://arxiv.org/abs/2406.07289|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-06-11|Transferring Knowledge from Large Foundation Models to Small Downstream Models|knowledge distillation with data selection|https://arxiv.org/abs/2406.07337|[['cs.LG'], ['ICML']]
2024-06-11|VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs|concatenation style audio visual language model|https://arxiv.org/abs/2406.07476|[['cs.CV', 'cs.CL']]
2024-06-11|ReduceFormer: Attention with Tensor Reduction by Summation|attention with only element wise operation (only experiment on some simple task and dataset)|https://arxiv.org/abs/2406.07488|[['cs.CV']]
2024-06-11|Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling|3.8B hybrid (sliding window attention and state space model) model for long sequence modeling|https://arxiv.org/abs/2406.07522|[['cs.LG', 'cs.CL']]
2024-06-11|Autoregressive Pretraining with Mamba in Vision|700M Mamba model pretrain using audioregressive predicting|https://arxiv.org/abs/2406.07537|[['cs.CV']]
2024-06-11|Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning|pretraining with interleaved visual text data, contrastive learning between image and preceding text, next token prediction on the subsequent text|https://arxiv.org/abs/2406.07543|[['cs.CV']]
2024-06-11|Dual-Pipeline with Low-Rank Adaptation for New Language Integration in Multilingual ASR|deal with a new language in multilingual ASR using LoRA|https://arxiv.org/abs/2406.07842|[['cs.CL', 'eess.AS']]
2024-06-11|GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model|distil by autoregressively predicting hidden representation of teacher model layer by layer|https://arxiv.org/abs/2406.09444|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-06-21|video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models|audio visual language model, using Q former with slow and fast branches, propose a benchmark|https://arxiv.org/abs/2406.15704|[['Audio-Visual'], ['cs.CV'], ['ICML']]
2024-06-21|MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression|use different sliding window attention pattern for each heads, reduce gpu usage and increase throughput|https://arxiv.org/abs/2406.14909|[['GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-21|RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs|after pruning model becomes heterogenous, learning base method to determine rank of LoRA for recovering|https://arxiv.org/abs/2406.15734|[['efficient fine-tuning'], ['cs.AI', 'cs.CL']]
2024-06-21|GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech|535 hours speech dataset with 23k speaker with 164 accents|https://arxiv.org/abs/2406.14875|[['cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-21|Prompting Whisper for QA-driven Zero-shot End-to-end Spoken Language Understanding|prompt whisper decoder to enable zero shot speech to speech task|https://arxiv.org/abs/2406.15209|[['eess.AS'], ['Interspeech']]
2024-06-21|Optimised Grouped-Query Attention Mechanism for Transformers|group key and values to improve efficiency|https://arxiv.org/abs/2406.14963|[['cs.LG'], ['ICML']]
2024-06-21|HLQ: Fast and Efficient Backpropagation via Hadamard Low-rank Quantization|low rank approximation and quantization of gradient of convolutional and linear layers|https://arxiv.org/abs/2406.15102|[['cs.LG', 'cs.CV']]
2024-06-21|Hybrid Alignment Training for Large Language Models|alternating between instruction following and human preference to alleviate conflict between two objectives|https://arxiv.org/abs/2406.15178|[['cs.CL']]
2024-06-21|Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance|mask gradient with relative small magnitudes|https://arxiv.org/abs/2406.15330|[['cs.AI', 'cs.CL']]
2024-06-21|Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning|learnable prompt to represent multimodal demonstration (the prompt seems to be static)|https://arxiv.org/abs/2406.15334|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-06-21|Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization|argue that doing reconstruction for recover after pruning is suboptimal|https://arxiv.org/abs/2406.15524|[['cs.LG', 'cs.CL']]
2024-06-22|What Matters in Transformers? Not All Attention is Needed|find that attention layers in LLM have high similarity and can be pruned safely, propose to drop attention and MLP layers|https://arxiv.org/abs/2406.15786|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-22|Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization|note that LLM usually exhibit  U shaped attention bias, calibrate this bias to improve long context performance|https://arxiv.org/abs/2406.16008|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-23|A Simple Framework for Open-Vocabulary Zero-Shot Segmentation|utilize vision only model that aware of spatial information and align a text encoder with it using text image pair data to achieve zeroshot generation|https://arxiv.org/abs/2406.16085|[['vision-language'], ['cs.CV']]
2024-06-23|Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation|memory efficient backpropagation of activation function, share activation memory in adjecent layer|https://arxiv.org/abs/2406.16282|[['memory-efficient'], ['cs.AI', 'cs.LG'], ['ICML']]
2024-06-23|Decoder-only Architecture for Streaming End-to-end Speech Recognition|as title|https://arxiv.org/abs/2406.16107|[['cs.CL', 'eess.AS'], ['Interspeech']]
2024-06-23|Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method|domain incremental learning by tighten decision bound, adapt new sample distribution, and then refine|https://arxiv.org/abs/2406.16231|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-06-23|UBiSS: A Unified Framework for Bimodal Semantic Summarization of Videos|new dataset and baseline for visual and textual summarization of long video|https://arxiv.org/abs/2406.16301|[['cs.AI', 'cs.CV']]
2024-06-24|Unlocking Continual Learning Abilities in Language Models|observe that activation with large magnitude behave differently for different task, fine tune corresponding weights for continual learning|https://arxiv.org/abs/2406.17245|[['parameter-efficient', 'efficient finetuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-24|Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers|efficient approximation focusing on linear layer rather than attention, low rank approximation with shuffle to reduce computation, technique for stable training|https://arxiv.org/abs/2406.16450|[['cs.CL']]
2024-06-24|LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training|transform an LLM to LLM with MoE modules by partition the linear layers and continual pretraining|https://arxiv.org/abs/2406.16554|[['cs.CL']]
2024-06-24|ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models|predict LLM activation and do dynamical pruning to speedup inference|https://arxiv.org/abs/2406.16635|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-24|Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers|select key value in a way that can be trained by gradient decent enable linear attention|https://arxiv.org/abs/2406.16747|[['cs.LG', 'cs.CL']]
2024-06-24|Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs|update sparse weights of LLM to improve continual learning ability|https://arxiv.org/abs/2406.16797|[['cs.AI', 'cs.CL']]
2024-06-24|Long Context Transfer from Language to Vision|long context abililty in LLM (NLP) can transfer to visual, training on image can process long video|https://arxiv.org/abs/2406.16852|[['cs.CV']]
2024-06-24|Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs|MLM and benchmark that focus more on visual tasks rather than language tasks|https://arxiv.org/abs/2406.16860|[['cs.CV']]
2024-06-25|MSRS: Training Multimodal Speech Recognition Models from Scratch with Sparse Mask Optimization|regularization to learn sparse weights that increase training speed for audio visual speech recognition from scratch|https://arxiv.org/abs/2406.17614|[['audio-visual'], ['cs.CV'], ['Interspeech']]
2024-06-25|ScanFormer: Referring Expression Comprehension by Iteratively Scanning|process images from coarse to fine and discard linguistic irrelevant patches when using higher location|https://arxiv.org/abs/2406.18048|[['vision-language'], ['cs.CV'], ['CVPR']]
2024-06-25|Minimal Interaction Edge Tuning: A New Paradigm for Visual Adaptation|highlight the ability of side tuning to finetune on edge device with backbone on service, propose to use sum of intermediate feature maps|https://arxiv.org/abs/2406.17559|[['parameter efficiency'], ['cs.CV']]
2024-06-25|Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP|share parameters and increase intra-modality separation of CLIP to achieve better alignment|https://arxiv.org/abs/2406.17639|[['vision-language'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-06-25|Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning|propose to use other structured matrix to parameterize weights which offer more flexibility than LoRA|https://arxiv.org/abs/2406.17740|[['Parameter Efficient', 'PEFT', 'Efficient Fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-25|BlockLLM: Memory-Efficient Adaptation of LLMs by Selecting and Optimizing the Right Coordinate Blocks|select weights with small magnitude to optimize to reduce training resources|https://arxiv.org/abs/2406.17296|[['Memory-Efficient', 'GPU memory'], ['cs.LG']]
2024-06-25|Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels|quantize different layer at different bit level, importance measured by difference of output feature from input and magnnitude of weights|https://arxiv.org/abs/2406.17415|[['cs.AI', 'cs.LG', 'cs.CL'], ['EMNLP']]
2024-06-25|Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients|transform gradients into structured sparse updates to reduce GPU memory|https://arxiv.org/abs/2406.17660|[['GPU memory'], ['cs.LG']]
2024-06-25|Dual-Space Knowledge Distillation for Large Language Models|distillation method applicable when output representation and distribution of statudent and teacher is different (different vocabulary)|https://arxiv.org/abs/2406.17328|[['cs.AI', 'cs.CL']]
2024-06-25|Forget but Recall: Incremental Latent Rectification in Continual Learning|before train on new task, obtain representation of old task from current weights|https://arxiv.org/abs/2406.17381|[['cs.LG', 'cs.CV']]
2024-06-25|Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training|noisy training to improve speculative decoding|https://arxiv.org/abs/2406.17404|[['cs.LG', 'cs.CL']]
2024-06-25|MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning|MLM with different resolution, object centric visual input|https://arxiv.org/abs/2406.17770|[['cs.CV']]
2024-06-25|MLLM as Video Narrator: Mitigating Modality Imbalance in Video Moment Retrieval|use MLM to do dense caption to help moment retrieval|https://arxiv.org/abs/2406.17880|[['cs.CV']]
2024-06-25|AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning|improve MeZO which try to update model with only forward pass to reduce memory comsumption|https://arxiv.org/abs/2406.18060|[['Memory-Efficient'], ['graph'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-27|OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding|use segmentation as visual encoder in VLM for pixel level reasoning|https://arxiv.org/abs/2406.19389|[['vision-language'], ['cs.CV']]
2024-06-26|LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference|KV cache pruning and merging for MLM|https://arxiv.org/abs/2406.18139|[['time efficiency'], ['cs.CV', 'cs.CL']]
2024-06-26|MammothModa: Multi-Modal Large Language Model|Chinese MLM and data|https://arxiv.org/abs/2406.18193|[['visual language'], ['cs.AI', 'cs.CV']]
2024-06-26|Learn it or Leave it: Module Composition and Pruning for Continual Learning|learn a module for new task initialized by modules of other tasks based on similarity, apply pruning maintain size|https://arxiv.org/abs/2406.18708|[['parameter efficiency'], ['cs.LG', 'cs.CL']]
2024-06-26|Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study|interleaved text speech discrete token sequence and proposed boundary token to enable decoder only streaming ASR|https://arxiv.org/abs/2406.18862|[['cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-26|The Surprising Effectiveness of Multimodal Large Language Models for Video Moment Retrieval|straightforward way to transform BLIP to a moment retrieval model, (feed frames, timestampls, and query. output interval)|https://arxiv.org/abs/2406.18113|[['cs.CV']]
2024-06-27|LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models|reparameterize prompt tuning by low rank matrix multiplication|https://arxiv.org/abs/2406.19486|[['Parameter Efficient'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-27|SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs|large VQA dataset requires external knowledge|https://arxiv.org/abs/2406.19593|[['VLMs'], ['cs.CV', 'cs.CL']]
2024-06-27|Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding|benchmark VLM's ability to understand implicit meaning of an image by grounding the relevent parts and reasoning|https://arxiv.org/abs/2406.18925|[['cs.CV', 'cs.CL']]
2024-06-27|VideoMambaPro: A Leap Forward for Mamba in Video Understanding|theoretically identify limitation of Mamba for video understanding, modification based on these|https://arxiv.org/abs/2406.19006|[['cs.CV']]
2024-06-27|Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model|fast, high recolution SAM using RWKV|https://arxiv.org/abs/2406.19369|[['cs.CV']]
2024-06-27|Mixture of In-Context Experts Enhance LLMs' Long Context Awareness|use router to directing attention of a head to specific position to handle long input|https://arxiv.org/abs/2406.19598|[['cs.CL']]
2024-06-28|Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model|loss function to improve MoE training process|https://arxiv.org/abs/2406.19905|[['Vision-Language'], ['cs.CV']]
2024-06-28|LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context Compression|reduce number of visual token by average pooling, training strategy for robust token reduction|https://arxiv.org/abs/2406.20092|[['training efficiency'], ['cs.CV']]
2024-06-28|Less is More: Accurate Speech Recognition & Translation without Web-Scale Data|SOTA ASR model using relative less (synthetic) data, and other training tricks|https://arxiv.org/abs/2406.19674|[['cs.LG', 'cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-28|SAML: Speaker Adaptive Mixture of LoRA Experts for End-to-End ASR|quantization and MoE to build edge device ASR model, also utilize user specific test time adaptation |https://arxiv.org/abs/2406.19706|[['cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-28|Single Parent Family: A Spectrum of Family Members from a Single Pre-Trained Foundation Model|incremental compression to generate LLM of different size|https://arxiv.org/abs/2406.19995|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-28|Segment Anything without Supervision|training SAM using only SSL model (DINO) pseudo label which generated by iterative and multi level clustering|https://arxiv.org/abs/2406.20081|[['cs.LG', 'cs.CV']]
2024-06-29|It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization|automatically select model for specific task as object to merge|https://arxiv.org/abs/2407.00487|[['cs.CL']]
2024-06-29|LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement|use contrastive learning to fine tune LLM from errors|https://arxiv.org/abs/2407.00497|[['cs.CL']]
2024-06-30|Hierarchical Memory for Long Video QA|as title|https://arxiv.org/abs/2407.00603|[['GPU memory'], ['cs.CV']]
2024-06-30|Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs|evolutionary method to prune MoE modules with forward pass only|https://arxiv.org/abs/2407.00945|[['GPU memory'], ['cs.LG']]
2024-06-30|Tarsier: Recipes for Training and Evaluating Large Video Description Models|video description with naive but effective CLIP and LLM concatenation model|https://arxiv.org/abs/2407.00634|[['cs.LG', 'cs.CV']]
2024-06-30|Less Forgetting for Better Generalization: Exploring Continual-learning Fine-tuning Methods for Speech Self-supervised Representations|using continual learning technique to preserve pretraining ability during downstream to improve generalizability|https://arxiv.org/abs/2407.00756|[['cs.SD', 'eess.AS']]
2024-06-30|Towards Robust Speech Representation Learning for Thousands of Languages|SSL model train on 1M hours data of 4000 languages|https://arxiv.org/abs/2407.00837|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-07-01|M$^2$IST: Multi-Modal Interactive Side-Tuning for Memory-efficient Referring Expression Comprehension|side tuning for text conditioned detection|https://arxiv.org/abs/2407.01131|[['Memory-efficient', 'GPU memory'], ['vision-language'], ['cs.CV']]
2024-07-01|Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning|construct multiple modules by different masking for a single module|https://arxiv.org/abs/2407.01320|[['Parameter Efficient', 'Efficient Fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL'], ['ICLR']]
2024-07-01|GalLoP: Learning Global and Local Prompts for Vision-Language Models|learn soft prompt of different level for CLIP|https://arxiv.org/abs/2407.01400|[['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-07-01|Eliminating Position Bias of Language Models: A Mechanistic Approach|argue that causal attention and relative positional encoding causes position bias, use segment level bidirectional attention to solve|https://arxiv.org/abs/2407.01100|[['vision-language', 'VLMs'], ['cs.LG', 'cs.CL']]
2024-07-01|Semantic Compositions Enhance Vision-Language Contrastive Learning|make CLIP aware of semantically composite rather than global concept by augmentation of mixing two image and caption|https://arxiv.org/abs/2407.01408|[['Vision-Language'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-07-01|FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training with Limited Resources|training strategy for CLIP|https://arxiv.org/abs/2407.01445|[['training efficiency'], ['cs.LG', 'cs.CV']]
2024-07-01|Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning|training noisy LoRA for each epoch and use EMA to obtain final LoRA, increase robustness|https://arxiv.org/abs/2407.01491|[['training efficiency', 'Efficient fine-tuning'], ['cs.CV', 'cs.CL']]
2024-07-01|Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time|design two module to align audio and visual input before feeding into LLM, 3M audio visual instruction tuning|https://arxiv.org/abs/2407.01851|[['Audio-Visual'], ['cs.AI', 'cs.LG', 'cs.CV', 'eess.AS'], ['ECCV']]
2024-07-01|HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling|task conditioned hyper network to generate adapter weights|https://arxiv.org/abs/2407.01411|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL']]
2024-07-01|Needle in the Haystack for Memory Based Large Language Models|LLM with external memory modules that can generalize to longer context|https://arxiv.org/abs/2407.01437|[['GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-01|Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models|finetuning scheme desgined for MoE on multi tasks scenario|https://arxiv.org/abs/2407.01906|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-01|CPT: Consistent Proxy Tuning for Black-box Optimization|try to optimize a black box model using an additive module|https://arxiv.org/abs/2407.01155|[['Vision-Language', 'VLMs'], ['cs.LG']]
2024-07-01|Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision|cross modality mask modeling and Jigsaw Puzzles pretraining for domain generalization|https://arxiv.org/abs/2407.01518|[['cs.AI', 'cs.LG', 'cs.CV'], ['ECCV']]
2024-07-01|Referring Atomic Video Action Recognition|text conditioned action localization|https://arxiv.org/abs/2407.01872|[['cs.CV', 'eess.IV'], ['ECCV']]
2024-07-01|Self-Cooperation Knowledge Distillation for Novel Class Discovery|novel class discovery with more unknown class then known ones, use two sets of representation to reconstruct each other|https://arxiv.org/abs/2407.01930|[['cs.CV'], ['ECCV']]
2024-07-01|$\text{Memory}^3$: Language Modeling with Explicit Memory|train a 2.4B LLM from scratch with explicit memory bank|https://arxiv.org/abs/2407.01178|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-01|Cross-Architecture Auxiliary Feature Space Translation for Efficient Few-Shot Personalized Object Detection|few shot detection using SSL model knowledge distillation as regularization|https://arxiv.org/abs/2407.01193|[['cs.CV']]
2024-07-01|Gradient-based Class Weighting for Unsupervised Domain Adaptation in Dense Prediction Visual Tasks|use gradient information to handle imbalanced class in unsupervised domain adaptation|https://arxiv.org/abs/2407.01327|[['cs.LG', 'cs.CV']]
2024-07-02|SAVE: Segment Audio-Visual Easy way using Segment Anything Model|fuse audio encoder feature into SAM to do audio visual segmentation|https://arxiv.org/abs/2407.02004|[['Audio-Visual'], ['cs.AI', 'cs.CV', 'cs.SD', 'eess.AS']]
2024-07-02|Conceptual Codebook Learning for Vision-Language Models|learn a codebook of visual conception (color, shape, ...) to improve few shot ability|https://arxiv.org/abs/2407.02350|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-07-02|SOT Triggered Neural Clustering for Speaker Attributed ASR|using clustering to handle diarization and ASR simultaneously|https://arxiv.org/abs/2407.02007|[['eess.AS'], ['Interspeech']]
2024-07-02|Occlusion-Aware Seamless Segmentation|benchmark and solution to panoramic segmentation|https://arxiv.org/abs/2407.02182|[['cs.CV', 'eess.IV'], ['ECCV']]
2024-07-02|SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation|text conditioned segmentation using box or mask annotion|https://arxiv.org/abs/2407.02389|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL'], ['ECCV']]
2024-07-02|SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot Object Detection|objective to improve few shot object detection|https://arxiv.org/abs/2407.02665|[['cs.CV'], ['ECCV']]
2024-07-02|Open Panoramic Segmentation|open vocabulary panoramic segmentation|https://arxiv.org/abs/2407.02685|[['cs.CV'], ['ECCV']]
2024-07-02|Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation|refine the method where we construct pseudo label using erased images in weakly supervised segmentation|https://arxiv.org/abs/2407.02768|[['cs.CV'], ['ECCV']]
2024-07-02|Foster Adaptivity and Balance in Learning with Noisy Labels|reweight and refine noisy label for classification|https://arxiv.org/abs/2407.02778|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-02|S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models|predict output based on each layer of smal LM for LLM speculative decoding|https://arxiv.org/abs/2407.01955|[['cs.CL']]
2024-07-02|An End-to-End Speech Summarization Using Large Language Model|concatenation based method to build speech language model for summarization|https://arxiv.org/abs/2407.02005|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-07-02|Multi-Grained Contrast for Data-Efficient Unsupervised Representation Learning|contrastive learning for different level of view (part, object, or scene)|https://arxiv.org/abs/2407.02014|[['cs.CV']]
2024-07-02|Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts|architecture designed for few shot segmentation|https://arxiv.org/abs/2407.02075|[['cs.CV']]
2024-07-02|HRSAM: Efficiently Segment Anything in High-Resolution Images|use attention window to enable high resolution SAM|https://arxiv.org/abs/2407.02109|[['cs.AI', 'cs.CV']]
2024-07-02|Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale|study on continual pretraining to improve multilingual ability and its scaling law|https://arxiv.org/abs/2407.02118|[['cs.CL']]
2024-07-02|PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning|progressively reduce template and demonstration during training to enhance LLM performance without demonstration|https://arxiv.org/abs/2407.02211|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-02|MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based Decoders|Mamba that can do multiple dense prediction at the same time|https://arxiv.org/abs/2407.02228|[['cs.AI', 'cs.CV']]
2024-07-02|Parameter-Selective Continual Test-Time Adaptation|as title|https://arxiv.org/abs/2407.02253|[['cs.LG', 'cs.CV']]
2024-07-02|Efficient Sparse Attention needs Adaptive Token Release|dynamically release KV if attention score is low cache and rebuild when necessary|https://arxiv.org/abs/2407.02328|[['cs.CL']]
2024-07-02|Revisiting Cascaded Ensembles for Efficient Inference|strategy for efficient inference using different level of cascaded models|https://arxiv.org/abs/2407.02348|[['cs.LG'], ['ICML']]
2024-06-30|Universal Approximation Theory: The basic theory for large language models|use universal approximation theory to explain in context learning, LoRA and other ability of LLM|https://arxiv.org/abs/2407.00958|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-02|OpenSlot: Mixed Open-set Recognition with Object-centric Learning|known and unknown object appear joinly in object centric learning|https://arxiv.org/abs/2407.02386|[['cs.CV']]
2024-07-02|TokenPacker: Efficient Visual Projector for Multimodal LLM|use coarse feature as query and fine grained feature as key and value to merge visual tokens|https://arxiv.org/abs/2407.02392|[['cs.CV']]
2024-07-02|Neurocache: Efficient Vector Retrieval for Long-range Language Modeling|cache hidden states to extend context size and use KNN to retrieve|https://arxiv.org/abs/2407.02486|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-02|MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention|sparse attention calculation to improve lattency during pre filling|https://arxiv.org/abs/2407.02490|[['cs.LG', 'cs.CL']]
2024-07-02|Towards the Next Frontier in Speech Representation Learning Using Disentanglement|speech self supervised learning for different level (frame, utterance)|https://arxiv.org/abs/2407.02543|[['cs.AI', 'cs.LG', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-07-02|ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers|quantization method to handle activation outliear in ViT|https://arxiv.org/abs/2407.02763|[['cs.CV']]
2024-07-02|Automatic gradient descent with generalized Newton's method|second order optimization without backpropagation apply on GPT and ResNet|https://arxiv.org/abs/2407.02772|[['cs.LG', 'cs.CV', 'cs.CL']]
2024-07-02|MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models|distillation on different layers of transformer|https://arxiv.org/abs/2407.02775|[['cs.LG', 'cs.CL']]
2024-07-02|52B to 1T: Lessons Learned via Tele-FLM Series|technical report for finetuning a 52B model and scale it to 1T model|https://arxiv.org/abs/2407.02783|[['cs.AI', 'cs.CL']]
2024-06-10|A Parameter-efficient Language Extension Framework for Multilingual ASR|extend ASR model to different language, decompose problem into LID and ASR, experiment with different PEFT modules|https://arxiv.org/abs/2406.06329|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL', 'eess.AS'], ['Interspeech']]
2024-06-10|mHuBERT-147: A Compact Multilingual HuBERT Model|scale up HuBERT training for multilingual representation|https://arxiv.org/abs/2406.06371|[['parameter efficiency'], ['cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-10|UVIS: Unsupervised Video Instance Segmentation|use CLIP and DINO to obtain pseudo label of video instance segmentation|https://arxiv.org/abs/2406.06908|[['vision-language'], ['cs.CV'], ['CVPR']]
2024-06-10|Extending Segment Anything Model into Auditory and Temporal Dimensions for Audio-Visual Segmentation|additional module to integrate spatiotemporal and audio information into SAM feature before decoder to achieve audio visual segmentation|https://arxiv.org/abs/2406.06163|[['Audio-Visual'], ['cs.CV']]
2024-06-10|Low-Rank Quantization-Aware Training for LLMs|quantization aware continual pretraining using LoRA|https://arxiv.org/abs/2406.06385|[['memory efficient', 'PEFT', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-10|Parallelizing Linear Transformers with the Delta Rule over Sequence Length|training algorithm to parallelize computation for training extended 1.3B linear transformer model|https://arxiv.org/abs/2406.06484|[['memory-efficient'], ['cs.LG', 'cs.CL']]
2024-07-03|Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation|visual prompt learning without labeled data using distillation|https://arxiv.org/abs/2407.03056|[['parameter-efficient'], ['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV'], ['ECCV']]
2024-07-03|Lateralization LoRA: Interleaved Instruction Tuning with Modality-Specialized Adaptations|text image interleaved instruction data, LoRA with convolution enable generation of interleaved text and images|https://arxiv.org/abs/2407.03604|[['parameter-efficient'], ['Vision-Language', 'VLMs'], ['cs.CV', 'cs.CL']]
2024-07-03|Investigating Decoder-only Large Language Models for Speech-to-text Translation|as title|https://arxiv.org/abs/2407.03169|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-07-03|InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output|as title|https://arxiv.org/abs/2407.03320|[['Vision Language'], ['cs.CV', 'cs.CL']]
2024-07-03|DyFADet: Dynamic Feature Aggregation for Temporal Action Detection|use weighted mask to make convolution kernel have different effective receptive field based on input feature|https://arxiv.org/abs/2407.03197|[['cs.CV'], ['ECCV']]
2024-07-03|SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding|use bounding box annotation for segmentation supervision to improve visual grounding|https://arxiv.org/abs/2407.03200|[['cs.CV'], ['ECCV']]
2024-07-03|Domain-Aware Fine-Tuning of Foundation Models|use CLIP text, image encoder to generate domain embedding that help segmentation|https://arxiv.org/abs/2407.03482|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-07-03|GPTQT: Quantize Large Language Models Twice to Push the Efficiency|two stage quantization|https://arxiv.org/abs/2407.02891|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-03|Translatotron-V(ison): An End-to-End Model for In-Image Machine Translation|generate image where the text in it is translated|https://arxiv.org/abs/2407.02894|[['cs.AI', 'cs.CL']]
2024-07-03|Visual Grounding with Attention-Driven Constraint Balancing|use the region attended by text input as supervision|https://arxiv.org/abs/2407.03243|[['cs.CV']]
2024-07-03|ACTRESS: Active Retraining for Semi-supervised Visual Grounding|data selection and parameter reset pipeline for semi supervised (self training) visual grounding|https://arxiv.org/abs/2407.03251|[['cs.CV']]
2024-07-04|Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt Learning|share the same soft prompts in CLIP to do image retrieval, jigsaw puzzle as pretraining task|https://arxiv.org/abs/2407.04207|[['vision-language'], ['cs.CV'], ['ECCV']]
2024-07-04|Multi-Convformer: Extending Conformer with Multiple Convolution Kernels|module with convolution with different kernel size and gating mechanism|https://arxiv.org/abs/2407.03718|[['parameter efficient'], ['cs.AI', 'cs.LG', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-07-04|PECTP: Parameter-Efficient Cross-Task Prompts for Incremental Vision Transformer|prompt tuning for incremental learning, use module for previous tasks to initialize and constraint new module|https://arxiv.org/abs/2407.03813|[['Parameter-Efficient'], ['cs.CV']]
2024-07-04|Concept Bottleneck Models Without Predefined Concepts|convert pretrained model to concept based model in supervised manner|https://arxiv.org/abs/2407.03921|[['vision-language'], ['cs.LG', 'cs.CV']]
2024-07-04|Mixture of A Million Experts|use input as query to retrieve MoE which replacing feedforward layers|https://arxiv.org/abs/2407.04153|[['parameter efficient'], ['cs.AI', 'cs.LG']]
2024-07-04|HERA: High-efficiency Matrix Compression via Element Replacement|KV cache compression using clustering|https://arxiv.org/abs/2407.03637|[['cs.LG', 'cs.CL']]
2024-07-04|Scalable Learned Model Soup on a Single GPU: An Efficient Subspace Training Strategy|memory efficient method to train model with different hyperparameter and ensemble|https://arxiv.org/abs/2407.03641|[['Memory Efficient'], ['graph'], ['cs.LG'], ['ECCV']]
2024-07-04|Relative Difficulty Distillation for Semantic Segmentation|distillation of segmentation taking difficulty into account|https://arxiv.org/abs/2407.03719|[['cs.CV']]
2024-07-04|EMPL: A novel Efficient Meta Prompt Learning Framework for Few-shot Unsupervised Domain Adaptation|meta learning for visual prompt tuning|https://arxiv.org/abs/2407.04066|[['cs.CV']]
2024-07-04|DASS: Distilled Audio State Space Models Are Stronger and More Duration-Scalable Learners|audio SSM generalize to long audio retrieval by distillation from Transformer based model|https://arxiv.org/abs/2407.04082|[['eess.AS']]
2024-07-04|AMD: Automatic Multi-step Distillation of Large-scale Vision Models|automatically select how many parameter to pruning to form a teacher assistant for distillation|https://arxiv.org/abs/2407.04208|[['cs.CV']]
2024-07-05|Dude: Dual Distribution-Aware Context Prompt Learning For Large Vision-Language Model|refine visual prompt tuning using optimal transport between learnable prompts and detailed text description |https://arxiv.org/abs/2407.04489|[['Vision-Language'], ['cs.CV']]
2024-07-05|AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation|adaptation of CLIP using optimal transport between augmented text and image|https://arxiv.org/abs/2407.04603|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-07-05|Fine-grained Dynamic Network for Generic Event Boundary Detection|different condition for early exit model to predict event boundaries in long video|https://arxiv.org/abs/2407.04274|[['cs.CV'], ['ECCV']]
2024-07-05|PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers|use SSL model as guidance to train part level classification model|https://arxiv.org/abs/2407.04538|[['cs.AI', 'cs.LG', 'cs.CV'], ['ECCV']]
2024-07-05|Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models|rnn transducer based ASR and language model enable transcription to run ahead of audio|https://arxiv.org/abs/2407.04641|[['cs.CL', 'eess.AS'], ['Interspeech']]
2024-07-05|Revealing the Utilized Rank of Subspaces of Learning in Neural Networks|detect the utilization of model weights and compress them to a more compact space|https://arxiv.org/abs/2407.04797|[['cs.LG', 'cs.CV'], ['ICML']]
2024-07-05|Associative Recurrent Memory Transformer|transformer with memory module recurrently processes segments to handle long context|https://arxiv.org/abs/2407.04841|[['cs.AI', 'cs.LG', 'cs.CL'], ['ICML']]
2024-07-05|XLSR-Transducer: Streaming ASR for Self-Supervised Pretrained Models|use different masking in XLSR to enable streaming ability|https://arxiv.org/abs/2407.04439|[['eess.AS']]
2024-07-05|LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order|shuffle layers for the distributed scenarios where the order of execution need not be guaranteed|https://arxiv.org/abs/2407.04513|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-07-05|Multimodal Classification via Modal-Aware Interactive Enhancement|balace two modality by modify the gradient|https://arxiv.org/abs/2407.04587|[['cs.LG', 'cs.CV']]
2024-07-05|Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition|use LLM to improve ASR in different domain, language|https://arxiv.org/abs/2407.04675|[['cs.SD', 'eess.AS']]
2024-07-05|Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge|file text embedding in object masks to enhance VLM's ability to utilize fine grained external knowledge|https://arxiv.org/abs/2407.04681|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-07-05|Improving Knowledge Distillation in Transfer Learning with Layer-wise Learning Rates|adaptively adjust learning rates of each layer by output activation attribution|https://arxiv.org/abs/2407.04871|[['cs.LG', 'cs.CV']]
2024-07-05|OmChat: A Recipe to Train Multimodal Language Models with Strong Long Context and Video Understanding|VLM that can process image of different resolution, multiple image, video, interleaved text and images|https://arxiv.org/abs/2407.04923|[['cs.CV', 'cs.CL']]
2024-07-06|LoRA-GA: Low-Rank Adaptation with Gradient Approximation|initialization method of LoRA that approximate gradient of LoRA and full fine tuneing|https://arxiv.org/abs/2407.05000|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.LG', 'cs.CL']]
2024-07-06|A Study of Test-time Contrastive Concepts for Open-world, Open-vocabulary Semantic Segmentation|open vocabulary segmentation without predefining visual concept to do contrastive|https://arxiv.org/abs/2407.05061|[['VLMs'], ['cs.CV']]
2024-07-06|HiDe-PET: Continual Learning via Hierarchical Decomposition of Parameter-Efficient Tuning|theoretically decompose continual learning into three components, design objecive to optimize them|https://arxiv.org/abs/2407.05229|[['Parameter-Efficient'], ['cs.LG'], ['NeurIPS']]
2024-07-06|SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding|use LLM to construct hard query (different result with small change) to enhance temporal grounding|https://arxiv.org/abs/2407.05118|[['cs.CV'], ['ECCV']]
2024-07-06|A Layer-Anchoring Strategy for Enhancing Cross-Lingual Speech Emotion Recognition|use contrastive learning to align similar feature of different language|https://arxiv.org/abs/2407.04966|[['cs.LG', 'cs.SD', 'eess.AS']]
2024-07-06|PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference|use weight sharing mechanism to adaptively decide token mergin pruning and number of channel|https://arxiv.org/abs/2407.05010|[['cs.CV']]
2024-07-06|Enhance the Robustness of Text-Centric Multimodal Alignments|suggest to transform input of all modalities into natural language to enhance robustness|https://arxiv.org/abs/2407.05036|[['cs.LG', 'cs.CL']]
2024-07-07|Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models|residual attention prompt tuning (don't mix all KV) to avoid interfere pretrained knowledge during continual learning|https://arxiv.org/abs/2407.05342|[['Parameter Efficient'], ['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-07-07|SBoRA: Low-Rank Adaptation with Regional Weight Updates|fix one of LoRA module to one hot columns to reduce trainable parameters|https://arxiv.org/abs/2407.05413|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-07|MMAD: Multi-label Micro-Action Detection in Videos|new task and dataset multi label action detection|https://arxiv.org/abs/2407.05311|[['cs.CV']]
2024-07-07|Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition|prompt learning to handle modality missing in multi modal (audio, visual, text) emotion recognition|https://arxiv.org/abs/2407.05374|[['cs.CV', 'cs.CL']]
2024-07-07|Training Task Experts through Retrieval Based Distillation|retrieve from existing dataset and transform them into domain specific data to construct high quality domain specific training data|https://arxiv.org/abs/2407.05463|[['cs.CL']]
2024-07-07|Described Spatial-Temporal Video Detection|text query spatial question object detection with possiblely multiple objects|https://arxiv.org/abs/2407.05610|[['cs.CV']]
2024-07-08|OneDiff: A Generalist Model for Image Difference|dataset and baseline (concatenation style VLM) for image difference detection|https://arxiv.org/abs/2407.05645|[['vision-language'], ['cs.CV']]
2024-07-08|A Single Transformer for Scalable Vision-Language Modeling|7B VLM without visual encoder (only transformer decoder), trained from scratch|https://arxiv.org/abs/2407.06438|[['Vision-Language'], ['cs.LG', 'cs.CV', 'cs.CL']]
2024-07-08|Wavelet Convolutions for Large Receptive Fields|convolution in wavelet domain to achieve larger receptive field|https://arxiv.org/abs/2407.05848|[['cs.CV'], ['ECCV']]
2024-07-08|C2C: Component-to-Composition Learning for Zero-Shot Compositional Action Recognition|zero shot compositional action recognition (each class cosist of an object and a verb), dataset and training strategy of learn object and verb separately to solve|https://arxiv.org/abs/2407.06113|[['cs.CV'], ['ECCV']]
2024-07-08|DεpS: Delayed ε-Shrinking for Faster Once-For-All Training|shrink large model when partially trained to obtain multiple size of module for one training|https://arxiv.org/abs/2407.06167|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-08|MagMax: Leveraging Model Merging for Seamless Continual Learning|weight selection and model merging to prevent forgetting during continual learning|https://arxiv.org/abs/2407.06322|[['cs.AI', 'cs.LG', 'cs.CV'], ['ECCV']]
2024-07-08|General and Task-Oriented Video Segmentation|disentanglement of appearance, shape feature, and query selection for task aware video segmentation|https://arxiv.org/abs/2407.06540|[['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-08|Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations|prune the internal dimension and retain the dimension between each layer|https://arxiv.org/abs/2407.05690|[['cs.AI', 'cs.CL']]
2024-07-08|LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages|continual pretrain LLaMA on multilingual data|https://arxiv.org/abs/2407.05975|[['cs.AI', 'cs.CL']]
2024-07-08|Pseudo-triplet Guided Few-shot Composed Image Retrieval|pretrain with masked image and machine generated caption, strategy to select few shot data to annotate|https://arxiv.org/abs/2407.06001|[['cs.CV']]
2024-07-08|Leveraging Transformers for Weakly Supervised Object Localization in Unconstrained Videos|use activation score of CLIP as pseudo label for weakly supervised localization|https://arxiv.org/abs/2407.06018|[['cs.LG', 'cs.CV']]
2024-07-08|3D Vision and Language Pretraining with Large-Scale Synthetic Data|synthetic large scale 3D vision text dataset|https://arxiv.org/abs/2407.06084|[['Vision-Language'], ['3D'], ['cs.CV']]
2024-07-08|ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation|VLM that can output interleaved image text without diffusion decoder|https://arxiv.org/abs/2407.06135|[['parameter-efficient'], ['diffusion'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-07-08|Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class-Incremental Learning|use the feature of SSM that dynamically change operation based on input to enchange performance of class incremental learning|https://arxiv.org/abs/2407.06136|[['cs.CV']]
2024-07-08|B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory|unified model architecture of hybrid SSM transformer with sliding window attention and recurrent states|https://arxiv.org/abs/2407.06324|[['cs.LG', 'cs.CL']]
2024-07-09|Tailored Design of Audio-Visual Speech Recognition Models using Branchformers|model architecture with local and global branch for audio visual speech recognition|https://arxiv.org/abs/2407.06606|[['parameter-efficient'], ['Audio-Visual'], ['cs.CV', 'cs.CL']]
2024-07-09|Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer: A Disentangled Approach|model architecture for side tuning, only experiment on classification|https://arxiv.org/abs/2407.06964|[['Parameter-Efficient'], ['cs.CV'], ['ECCV']]
2024-07-09|SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training|reweight data base on degree of duplication to speedup LLM pretraining|https://arxiv.org/abs/2407.06654|[['training efficiency'], ['cs.AI', 'cs.CL']]
2024-07-09|Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization|generating pseudo label for action localization|https://arxiv.org/abs/2407.07024|[['vision-language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-07-09|ERQ: Error Reduction for Post-Training Quantization of Vision Transformers|sequentially optimize activation quantization and weight quantization|https://arxiv.org/abs/2407.06794|[['cs.CV'], ['ICML']]
2024-07-09|Rethinking Image-to-Video Adaptation: An Object-centric Perspective|extract object query and model the temporal information of object feature|https://arxiv.org/abs/2407.06871|[['cs.CV'], ['ECCV']]
2024-07-09|Dataset Quantization with Active Learning based Adaptive Sampling|active learning, reduce data of easy class, increase data of hard class|https://arxiv.org/abs/2407.07268|[['cs.CV'], ['ECCV']]
2024-07-09|Video In-context Learning|new task and baseline for text conditioned subsequent video generation|https://arxiv.org/abs/2407.07356|[['cs.CV']]
2024-07-10|Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation|predict video level instance query, then perform per frame segmentation prediction|https://arxiv.org/abs/2407.07427|[['VLM'], ['cs.CV'], ['ECCV']]
2024-07-10|SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning|using side network with anti-redundancy operation for most layer, feed feature back to backbone in the last layers|https://arxiv.org/abs/2407.07523|[['Parameter-efficient'], ['cs.CV'], ['ECCV']]
2024-07-10|AVCap: Leveraging Audio-Visual Features as Text Tokens for Captioning|as title|https://arxiv.org/abs/2407.07801|[['Audio-Visual'], ['cs.LG', 'cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-07-10|Label-anticipated Event Disentanglement for Audio-Visual Video Parsing|during decoding fuse information of label text feature|https://arxiv.org/abs/2407.08126|[['Audio-Visual'], ['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-10|Tuning Vision-Language Models with Candidate Labels by Prompt Alignment|utilize prompt learning in the schenario where each data only has a set of candidate labels|https://arxiv.org/abs/2407.07638|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-07-10|PaliGemma: A versatile 3B VLM for transfer|as title|https://arxiv.org/abs/2407.07726|[['Vision-Language', 'VLM'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-07-10|ROSA: Random Subspace Adaptation for Efficient Fine-Tuning|apply SVD on pretrained weights, only train a subspace corresponding to some singular values|https://arxiv.org/abs/2407.07802|[['Parameter efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-10|RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization|utilize rotation to remove outlier to quantize activation|https://arxiv.org/abs/2407.08044|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-10|ActionVOS: Actions as Prompts for Video Object Segmentation|text conditioned segmentation aims to segment object intereacted with camera|https://arxiv.org/abs/2407.07402|[['cs.CV'], ['ECCV']]
2024-07-10|Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation|as title|https://arxiv.org/abs/2407.07412|[['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-10|Simplifying Source-Free Domain Adaptation for Object Detection: Effective Self-Training Strategies and Performance Insights|distillation by strong and weak augmentation, and tuning batch normalization is a simple and strong baseline for source free domain adaptation|https://arxiv.org/abs/2407.07586|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-10|SUMix: Mixup with Semantic and Uncertain Information|take the semantic information and uncertainty of augmented image into account|https://arxiv.org/abs/2407.07805|[['cs.CV'], ['ECCV']]
2024-07-10|FYI: Flip Your Images for Dataset Distillation|observe common redundency pattern during data distillation, suggest to flip images to solve|https://arxiv.org/abs/2407.08113|[['cs.CV'], ['ECCV']]
2024-07-10|LokiLM: Technical Report|knowledge distillation to train a 1.4B model to achieve strong performance|https://arxiv.org/abs/2407.07370|[['cs.CL']]
2024-07-10|HAFormer: Unleashing the Power of Hierarchy-Aware Features for Lightweight Semantic Segmentation|high frame rate segmentation using hybrid CNN transformer architecture|https://arxiv.org/abs/2407.07441|[['cs.CV']]
2024-07-10|Video-to-Audio Generation with Hidden Alignment|as title|https://arxiv.org/abs/2407.07464|[['audio-visual'], ['text-to-video'], ['cs.CV', 'cs.SD', 'eess.AS']]
2024-07-10|OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion|select query based on text input, and fuse the feature of query to do detection|https://arxiv.org/abs/2407.07844|[['cs.CV']]
2024-07-10|TACLE: Task and Class-aware Exemplar-free Semi-supervised Class Incremental Learning|class incremental learning without saving previous images, solve by weighted objective|https://arxiv.org/abs/2407.08041|[['cs.CV']]
2024-07-10|MambaVision: A Hybrid Mamba-Transformer Vision Backbone|as title|https://arxiv.org/abs/2407.08083|[['cs.CV']]
2024-07-11|DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception|machine generated high quality detailed image caption dataset|https://arxiv.org/abs/2407.08303|[['vision-language'], ['cs.AI', 'cs.CV']]
2024-07-11|NODE-Adapter: Neural Ordinary Differential Equations for Better Vision-Language Reasoning|average of text prompt and example images as prototype of each class and use ODE to refine them in CLIP based classification|https://arxiv.org/abs/2407.08672|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-07-11|HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models|reshape sliding window features in latent space and do global fusion to alleviate information loss by separated window, dynamical attend to salient patches|https://arxiv.org/abs/2407.08706|[['Vision-Language'], ['cs.CV']]
2024-07-11|MAVIS: Mathematical Visual Instruction Tuning|LLaVA specialized to solve math problems|https://arxiv.org/abs/2407.08739|[['vision-language'], ['cs.CV']]
2024-07-11|Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification|retrieve relavent image text pairs in pretrain data and use semi supervised method to help downstream|https://arxiv.org/abs/2407.08787|[['Vision-Language'], ['cs.CV']]
2024-07-11|Towards stable training of parallel continual learning|continual learning with multiple source input, propose regularization for stable training|https://arxiv.org/abs/2407.08214|[['training efficiency'], ['cs.AI', 'cs.LG']]
2024-07-11|VideoMamba: Spatio-Temporal Selective State Space Model|pure Mamba architecture for video understanding|https://arxiv.org/abs/2407.08476|[['cs.CV'], ['ECCV']]
2024-07-11|Exemplar-free Continual Representation Learning via Learnable Drift Compensation|technique for alleviate forgetting by drift distribution after tuning|https://arxiv.org/abs/2407.08536|[['cs.CV'], ['ECCV']]
2024-07-11|Adaptive Parametric Activation|as title|https://arxiv.org/abs/2407.08567|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-11|ElasticAST: An Audio Spectrogram Transformer for All Length and Resolutions|stack features if they are too long to increase available input length|https://arxiv.org/abs/2407.08691|[['cs.AI', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-07-11|Lite-SAM Is Actually What You Need for Segment Everything|1.16M CNN Transformer hybrid model to segment anything|https://arxiv.org/abs/2407.08965|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-11|Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks|KV cache mergin based on KV similarity and sparsity|https://arxiv.org/abs/2407.08454|[['cs.CL']]
2024-07-11|Automatic Pruning of Fine-tuning Datasets for Transformer-based Language Models|pruning the dataset according to whether model predict correctly|https://arxiv.org/abs/2407.08887|[['cs.LG', 'cs.CL']]
2024-07-12|Textual Query-Driven Mask Transformer for Domain Generalized Segmentation|use textual prompts to help domain generalization segmentation|https://arxiv.org/abs/2407.09033|[['vision-language'], ['cs.CV'], ['ECCV']]
2024-07-12|Open Vocabulary Multi-Label Video Classification|use LLM and temporal modules to enable CLIP to do open vocabulary multilable action classification|https://arxiv.org/abs/2407.09073|[['vision-language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-07-12|Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts|detect redundant features in different experts and merge or prune them|https://arxiv.org/abs/2407.09590|[['parameter efficiency'], ['cs.LG', 'cs.CL']]
2024-07-12|On the Role of Discrete Tokenization in Visual Representation Learning|theoretically understanding effect of discrete visual token, propose new tokenizer based on the theory|https://arxiv.org/abs/2407.09087|[['cs.LG', 'cs.CV'], ['ICLR']]
2024-07-12|Enhancing Training Efficiency Using Packing with Flash Attention|discuss efficiency of batching strategy (collator)|https://arxiv.org/abs/2407.09105|[['Training Efficiency'], ['cs.AI', 'cs.LG']]
2024-07-12|SPIN: Hierarchical Segmentation with Subpart Granularity in Natural Images|dataset for segmentation with object parts subparts level|https://arxiv.org/abs/2407.09686|[['cs.CV'], ['ECCV']]
2024-07-12|Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal Alignment, Reconstruction, and Refinement Framework|use contrastive learning, reconstruction other modality to handle missing modality in emotion recognition|https://arxiv.org/abs/2407.09029|[['cs.CV', 'cs.SD', 'eess.AS']]
2024-07-12|Cs2K: Class-specific and Class-shared Knowledge Guidance for Incremental Semantic Segmentation|keep prototype of old tasks to do pseudo label on new task to prevent forgetting, fisher information to determine which weights to update|https://arxiv.org/abs/2407.09047|[['cs.CV']]
2024-07-12|H2O-Danube3 Technical Report|same language model aiming to run on edge device|https://arxiv.org/abs/2407.09276|[['cs.LG', 'cs.CL']]
2024-07-12|Transformer Layers as Painters|observe that the beginning and last layers are much different from middle layers, while middle layers seem to share the same representation space|https://arxiv.org/abs/2407.09298|[['cs.CL']]
2024-07-12|HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context|theoretically understand how SSM learn from in context learning|https://arxiv.org/abs/2407.09375|[['cs.LG'], ['ICML']]
2024-07-12|Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference|enable beam search with speculative decoding|https://arxiv.org/abs/2407.09722|[['cs.LG', 'cs.CL']]
2024-07-13|Explanation is All You Need in Distillation: Mitigating Bias and Shortcut Learning|apply explanation technique (layer relevance propagation) to distil for OOD ability preserving|https://arxiv.org/abs/2407.09788|[['vision-language'], ['cs.AI', 'cs.LG', 'cs.CV', 'eess.IV']]
2024-07-13|Low-Rank Interconnected Adaptation Across Layers|use feature of all layers to select up projection of LoRA of each layer|https://arxiv.org/abs/2407.09946|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CV']]
2024-07-13|Investigating Low-Rank Training in Transformer Language Models: Efficiency and Scaling Analysis|low rank feed forward layer in transformer trained from scratch|https://arxiv.org/abs/2407.09835|[['cs.CL'], ['ICML']]
2024-07-13|Background Adaptation with Residual Modeling for Exemplar-Free Class-Incremental Semantic Segmentation|incremental segmentation by adding prediction of previous model to focus on learning residual|https://arxiv.org/abs/2407.09838|[['cs.CV'], ['ECCV']]
2024-07-13|Eliminating Feature Ambiguity for Few-Shot Segmentation|learn a network to find the exact object in query region to segment|https://arxiv.org/abs/2407.09842|[['cs.CV'], ['ECCV']]
2024-07-13|MaskMoE: Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts|randomly maske some MoE to fully utilize all modules during training|https://arxiv.org/abs/2407.09816|[['cs.CL']]
2024-07-13|Region-aware Image-based Human Action Retrieval with Transformers|try to retrieve image by action in query image|https://arxiv.org/abs/2407.09924|[['cs.CV']]
2024-07-13|Multi-Granularity Semantic Revision for Large Language Model Distillation|LLM distillation where we use student output as new training data after correct them|https://arxiv.org/abs/2407.10068|[['cs.CL']]
2024-07-14|Plain-Det: A Plain Multi-Dataset Object Detector|object detection pipeline that is robust to train on mixed datasets by using sparse region proposal|https://arxiv.org/abs/2407.10083|[['training efficiency'], ['cs.CV'], ['ECCV']]
2024-07-14|Visual Prompt Selection for In-Context Learning Segmentation|iteratively change example for in context segmentation by evaulating result and a search agent|https://arxiv.org/abs/2407.10233|[['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-14|Beyond Prompt Learning: Continual Adapter for Efficient Rehearsal-Free Continual Learning|extendable adapter to solve continual learning without storing previous data|https://arxiv.org/abs/2407.10281|[['cs.CV'], ['ECCV']]
2024-07-18|GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model|different scan direction on different channels, distillation to stable visual Mamba training|https://arxiv.org/abs/2407.13772|[['Parameter-Efficient'], ['cs.CV']]
2024-07-18|Learning from the Web: Language Drives Weakly-Supervised Incremental Learning for Semantic Segmentation|class incremental segmentation using web images|https://arxiv.org/abs/2407.13363|[['cs.CV'], ['ECCV']]
2024-07-15|Quantized Prompt for Efficient Generalization of Vision-Language Models|quantized visual prompt tuning as regularization to improve generalizability|https://arxiv.org/abs/2407.10704|[['Vision-Language'], ['cs.CV'], ['ECCV']]
2024-07-15|Can Textual Semantics Mitigate Sounding Object Segmentation Preference?|use image captioning and LLM to deduce potential sounding object to enhance audio visual segmentation|https://arxiv.org/abs/2407.10947|[['Audio-Visual'], ['cs.CV'], ['ECCV']]
2024-07-15|Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes|text conditioned audio visual segmentation|https://arxiv.org/abs/2407.10957|[['Audio-Visual'], ['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-15|LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction|use language model to improve CLIP performance on complex (OOD) text query|https://arxiv.org/abs/2407.11335|[['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-07-15|Learning Modality-agnostic Representation for Semantic Segmentation from Any Modalities|knowledge distillation from language-bind to enable any modality conditioned segmentation|https://arxiv.org/abs/2407.11351|[['vision-language'], ['cs.CV'], ['ECCV']]
2024-07-15|Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping|improve multilingual performance by contrasting prediction probabilities between logits in final and early layers|https://arxiv.org/abs/2407.10795|[['cs.CL']]
2024-07-15|Q-Sparse: All Large Language Models can be Fully Sparsely-Activated|top k sparse activated LLM|https://arxiv.org/abs/2407.10969|[['cs.LG', 'cs.CL']]
2024-07-15|TCFormer: Visual Recognition via Token Clustering Transformer|semantic aware image patches by clustering|https://arxiv.org/abs/2407.11321|[['cs.CV']]
2024-07-16|SDPT: Synchronous Dual Prompt Tuning for Fusion-based Visual-Language Pre-trained Models|learnable token in unified space and apply inverse transform to image and text spaces|https://arxiv.org/abs/2407.11414|[['parameter-efficient', 'efficient fine-tuning'], ['Visual-Language'], ['cs.CV'], ['ECCV']]
2024-07-16|Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models|propose metrics to measure importance of token and do pruning and merging|https://arxiv.org/abs/2407.11717|[['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-07-16|Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation|decompose audio visual segmentation into two stages, sounding object segmentation, object recognition|https://arxiv.org/abs/2407.11820|[['Audio-Visual'], ['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-16|Beyond Mask: Rethinking Guidance Types in Few-shot Segmentation|unified framework of few shot segmentation given different queries (text, mask, box or image)|https://arxiv.org/abs/2407.11503|[['vision-language'], ['cs.CV']]
2024-07-16|MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models|hybrid pruning criterion using magnitude, activation, estimated gradients|https://arxiv.org/abs/2407.11681|[['Memory-Efficient', 'GPU memory'], ['cs.CL']]
2024-07-16|Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights|image captioning conditioned by user given highlights|https://arxiv.org/abs/2407.11449|[['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-16|Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection|relabel old object in new images to prevent forgetting and discard background object for future object class|https://arxiv.org/abs/2407.11499|[['cs.CV'], ['ECCV']]
2024-07-16|Relation DETR: Exploring Explicit Position Relation Prior for Object Detection|additional encoder for position prior in DETR, learn from contrastive relation|https://arxiv.org/abs/2407.11699|[['cs.CV'], ['ECCV']]
2024-07-16|Exploring Quantization for Efficient Pre-Training of Transformer Language Models|study on the impact of quantization during pretraining|https://arxiv.org/abs/2407.11722|[['training efficiency'], ['cs.LG']]
2024-07-16|Mitigating Background Shift in Class-Incremental Semantic Segmentation|selective pseudo labeling and adaptive feature distillation to strike the balance between forgetting and pseudo label noise|https://arxiv.org/abs/2407.11859|[['cs.CV'], ['ECCV']]
2024-07-16|Enhancing Parameter Efficiency and Generalization in Large-Scale Models: A Regularized and Masked Low-Rank Adaptation Approach|gradient masking and regularization to enhance LoRA performance|https://arxiv.org/abs/2407.12074|[['Parameter Efficiency'], ['cs.AI', 'cs.LG']]
2024-07-16|Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs|offload and recompute the activation to reduce GPU memory|https://arxiv.org/abs/2407.12117|[['GPU memory'], ['cs.LG']]
2024-07-16|CroMo-Mixup: Augmenting Cross-Model Representations for Continual Self-Supervised Learning|augmentation by mixup data and feature during continual self supervised learning |https://arxiv.org/abs/2407.12188|[['cs.CV'], ['ECCV']]
2024-07-16|MDPE: A Multimodal Deception Dataset with Personality and Emotional Characteristics|multimodal emotion recognition dataset taking personality into account|https://arxiv.org/abs/2407.12274|[['cs.CV'], ['NeurIPS']]
2024-07-16|Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization|adaptive control KV cache eviction probability among different heads|https://arxiv.org/abs/2407.11550|[['cs.AI', 'cs.CL']]
2024-07-16|NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks|local loss module to enable interger training of CNN|https://arxiv.org/abs/2407.11698|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-07-16|Invariant Consistency for Knowledge Distillation|contrastive learning to make teacher and student more consistent|https://arxiv.org/abs/2407.11802|[['cs.AI', 'cs.CV']]
2024-07-16|GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression|1.5B RWKV Transformer hybrid model with less cache|https://arxiv.org/abs/2407.12077|[['cs.AI', 'cs.CL']]
2024-07-17|ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference|remove residual connection, discard feed forward to improve segmentation ability of CLIP|https://arxiv.org/abs/2407.12442|[['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-07-17|Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models|self supervise learning by predicting other modality to improve missing modality performance|https://arxiv.org/abs/2407.12616|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-17|Goldfish: Vision-Language Understanding of Arbitrarily Long Videos|benchmark for long video understanding with retrieval based baseline|https://arxiv.org/abs/2407.12679|[['Vision-Language'], ['cs.CV'], ['ECCV']]
2024-07-17|Patch-Level Training for Large Language Models|compressing multiple tokens into a patch during first half of training to reduce computation cost|https://arxiv.org/abs/2407.12665|[['training efficiency'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-17|MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models|MoE for multiple visual encoder in VLM|https://arxiv.org/abs/2407.12709|[['vision-language'], ['cs.CV']]
2024-07-17|Audio-visual Generalized Zero-shot Learning the Easy Way|contrastive learning between text and audio-visual joint embedding|https://arxiv.org/abs/2407.13095|[['Audio-visual'], ['cs.LG', 'cs.CV', 'cs.SD', 'eess.AS']]
2024-07-17|Progressive Proxy Anchor Propagation for Unsupervised Semantic Segmentation|progressively expand the trustworthy positive set as target for segmentation|https://arxiv.org/abs/2407.12463|[['cs.CV'], ['ECCV']]
2024-07-17|LTRL: Boosting Long-tail Recognition via Reflective Learning|review past prediction to correct gradient conflict|https://arxiv.org/abs/2407.12568|[['cs.CV'], ['ECCV']]
2024-07-17|Weighting Pseudo-Labels via High-Activation Feature Index Similarity and Object Detection for Semi-Supervised Segmentation|as title|https://arxiv.org/abs/2407.12630|[['cs.CV'], ['ECCV']]
2024-07-17|LookupViT: Compressing visual information to a limited number of tokens|ViT using two sets of visual token, high resolution set using low computation layer and low resolution set using high computation layer|https://arxiv.org/abs/2407.12753|[['cs.AI', 'cs.LG', 'cs.CV'], ['ECCV']]
2024-07-17|AdaLog: Post-Training Quantization for Vision Transformers with Adaptive Logarithm Quantizer|quantization taking power-law-like distribution (Softmax, GELU) into account|https://arxiv.org/abs/2407.12951|[['cs.CV'], ['ECCV']]
2024-07-17|ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos|detect overlapping action event using finite state machine controled by neural network|https://arxiv.org/abs/2407.12987|[['cs.CV'], ['ECCV']]
2024-07-17|ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders|masking strategy by filtering random noise|https://arxiv.org/abs/2407.13036|[['cs.AI', 'cs.LG', 'cs.CV', 'eess.IV'], ['ECCV']]
2024-07-17|Contrastive Adversarial Training for Unsupervised Domain Adaptation|regularization to make target feature distribution similar to source feature distribution|https://arxiv.org/abs/2407.12782|[['cs.LG', 'cs.CV']]
2024-07-17|Text- and Feature-based Models for Compound Multimodal Emotion Recognition in the Wild|transform all modalities to text for emotion recogonition|https://arxiv.org/abs/2407.12927|[['cs.CV']]
2024-07-17|Improving SAM Requires Rethinking its Optimization Formulation|new objective for sharpness aware minimization|https://arxiv.org/abs/2407.12993|[['cs.LG'], ['ICML']]
2024-07-18|X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs|pretrain a VLM connector using contrastive learning and masked image modeling|https://arxiv.org/abs/2407.13851|[['vision-language'], ['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-18|Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation|select pseudo label from region proposal network to avoid biasing toward familiar objects|https://arxiv.org/abs/2407.13524|[['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-18|Training-Free Model Merging for Multi-target Domain Adaptation|layer statistics during model merging by modeling real world distribution as Gaussian|https://arxiv.org/abs/2407.13771|[['cs.CV'], ['ECCV']]
2024-07-18|Make a Strong Teacher with Label Assistance: A Novel Knowledge Distillation Approach for Semantic Segmentation|feed ground truth label to teacher to boost distillation performance|https://arxiv.org/abs/2407.13254|[['cs.CV']]
2024-07-18|Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies|scaling law between compute budge and vocabulary size, suggest vocabulary size of current model is too small|https://arxiv.org/abs/2407.13623|[['cs.AI', 'cs.CL']]
2024-07-18|Keypoint Aware Masked Image Modelling|masking according to handcrafted keyboard feature during masked image modeling |https://arxiv.org/abs/2407.13873|[['cs.LG', 'cs.CV']]
2024-07-18|MSceneSpeech: A Multi-Scene Speech Dataset For Expressive Speech Synthesis|as title|https://arxiv.org/abs/2407.14006|[['cs.SD', 'eess.AS']]
2024-07-18|ViLLa: Video Reasoning Segmentation with Large Language Model|VLM that generate multiple segmentation maskes from video given text (description or question)|https://arxiv.org/abs/2407.14500|[['cs.CV']]
2024-07-19|Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and Parameter Fusion|use text feature to measure influence of new class, decompose weights into shared or not shared parts to do merging|https://arxiv.org/abs/2407.14143|[['vision-language'], ['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-19|Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition|PEFT with early exit and sparsity forward and backward pass|https://arxiv.org/abs/2407.14302|[['Parameter-efficient'], ['cs.CV'], ['ECCV']]
2024-07-19|Straightforward Layer-wise Pruning for More Efficient Visual Adaptation|PEFT with layer pruning by tSNE clustering|https://arxiv.org/abs/2407.14330|[['Parameter-efficient'], ['cs.CV'], ['ECCV']]
2024-07-19|EVLM: An Efficient Vision-Language Model for Visual Understanding|cross attention of image and text in different LLM layers, use visual feature of different level|https://arxiv.org/abs/2407.14177|[['Vision-Language'], ['cs.CV']]
2024-07-19|Early Preparation Pays Off: New Classifier Pre-tuning for Class Incremental Semantic Segmentation|learn a transformation to initialize new classifier given old one|https://arxiv.org/abs/2407.14142|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-19|Bucketed Ranking-based Losses for Efficient Training of Object Detectors|reduce complexity of ranking based loss for object detection|https://arxiv.org/abs/2407.14204|[['cs.CV'], ['ECCV']]
2024-07-19|Efficient Audio Captioning with Encoder-Level Knowledge Distillation|distillation on feature space use MSE and contrastive loss|https://arxiv.org/abs/2407.14329|[['cs.SD', 'eess.AS'], ['Interspeech']]
2024-07-19|On Learning Discriminative Features from Synthesized Data for Self-Supervised Fine-Grained Visual Recognition|use synthesized data to guide SSL methods to focus on features critical for fine grained recognition|https://arxiv.org/abs/2407.14676|[['cs.CV'], ['ECCV']]
2024-07-19|MetaAug: Meta-Data Augmentation for Post-Training Quantization|meta learning to learn augmentation transform to prevent overfitting on calibration data|https://arxiv.org/abs/2407.14726|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-19|LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference|dynamically select KV to increase speed|https://arxiv.org/abs/2407.14057|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-21|Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization|learn a group of LoRA which orthogonal to each other and pre-trained weights|https://arxiv.org/abs/2407.15085|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CV']]
2024-07-21|Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection|use text to image generation to do contrastive learning for visual concept learning in object detection|https://arxiv.org/abs/2407.15296|[['Vision-language'], ['cs.LG', 'cs.CV', 'cs.CL'], ['ECCV']]
2024-07-21|Efficient Visual Transformer by Learnable Token Merging|objectivve for token merging by variational inference|https://arxiv.org/abs/2407.15219|[['cs.LG', 'cs.CV']]
2024-07-21|SELM: Enhancing Speech Emotion Recognition for Out-of-Domain Scenarios|suggest that using audio LM improve out of domain performance of emotion recognition|https://arxiv.org/abs/2407.15300|[['cs.SD', 'eess.AS']]
2024-07-22|SIGMA: Sinkhorn-Guided Masked Video Modeling|MAE where the target is feature obtained from learnable clustering|https://arxiv.org/abs/2407.15447|[['cs.CV'], ['ECCV']]
2024-07-22|STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay|maintain a memory bank with low entropy instance for stable test time adaptation|https://arxiv.org/abs/2407.15773|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-22|Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality|audio visual text QA with missing modality by generating missed modality from others|https://arxiv.org/abs/2407.16171|[['Audio-Visual'], ['diffusion'], ['cs.AI', 'cs.CV'], ['ECCV']]
2024-07-22|Promises and Pitfalls of Generative Masked Language Modeling: Theoretical Framework and Practical Guidelines|mathematical understanding of parallel decoding language model architecture|https://arxiv.org/abs/2407.21046|[['cs.LG', 'cs.CL'], ['ICML']]
2024-07-22|Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models|skipping deeper attention layer obtain speedup without large performance drop|https://arxiv.org/abs/2407.15516|[['cs.LG', 'cs.CL']]
2024-07-22|LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding|as title|https://arxiv.org/abs/2407.15754|[['cs.LG', 'cs.CV', 'cs.CL']]
2024-07-22|Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning|masked image modeling on latent space by patchify, model architecture and training technique (similar to BYOL)|https://arxiv.org/abs/2407.15837|[['cs.AI', 'cs.CV']]
2024-07-22|SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models|as title|https://arxiv.org/abs/2407.15841|[['cs.CV']]
2024-07-23|Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models|selectively update some heads (with some theoretical justification) in CLIP for better downstream performance|https://arxiv.org/abs/2407.16526|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-07-23|PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects|use q former to model hierachical relationship between object and parts to segment and parse them|https://arxiv.org/abs/2407.16696|[['cs.CV'], ['ECCV']]
2024-07-23|INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model|two branch (high resolution sub image and low resolution global image) in VLM for high resolution images|https://arxiv.org/abs/2407.16198|[['cs.AI', 'cs.CV']]
2024-07-23|Chameleon: Images Are What You Need For Multimodal Learning Robust To Missing Modalities|use word embeddings as image feature to reduce missing modality performance drop|https://arxiv.org/abs/2407.16243|[['cs.CV']]
2024-07-23|Harmonizing Visual Text Comprehension and Generation|modality gating LoRA to finetune a VLM that generate both text and image|https://arxiv.org/abs/2407.16364|[['cs.CV']]
2024-07-24|Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation|remove QKV projection and reduce KV resolution for efficient segmentation|https://arxiv.org/abs/2407.17261|[['cs.CV'], ['ECCV']]
2024-07-24|Multi-label Cluster Discrimination for Visual Representation Learning|CLIP but use cluster level feeature and multi label objectives|https://arxiv.org/abs/2407.17331|[['cs.CV'], ['ECCV']]
2024-07-24|Unsqueeze [CLS] Bottleneck to Learn Rich Representations|suggest information bottleneck in SSL model such as DINO leads to sharp distribution, design objective and pipeline to prevent this|https://arxiv.org/abs/2407.17671|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-07-24|Efficient LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads|attention algorithm where each head process different partition of context in parallel to speed up and reduce memory|https://arxiv.org/abs/2407.17678|[['cs.CL']]
2024-07-25|Efficient Inference of Vision Instruction-Following Models with Elastic Cache|identify important cache and apply merge or pruning for VLM|https://arxiv.org/abs/2407.18121|[['vision-language'], ['cs.CV'], ['ECCV']]
2024-07-25|LoRA-Pro: Are Low-Rank Adapters Properly Optimized?|improve LoRA optimization by minimize the difference between LoRA gradient and full finetune gradient|https://arxiv.org/abs/2407.18242|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-25|How to Train the Teacher Model for Effective Knowledge Distillation|suggest that teacher trained with MSE is better than trained with cross entropy during distillation|https://arxiv.org/abs/2407.18041|[['cs.LG'], ['ECCV']]
2024-07-28|Detached and Interactive Multimodal Learning|unimodal objective to avoid modality competetion in two stream multi modal system|https://arxiv.org/abs/2407.19514|[['audio-visual'], ['cs.CV']]
2024-07-28|Memory-efficient Training of LLMs with Larger Mini-batches|find a smaller batch that capture gradients of larger batches (by zeroth order estimation on pruned model) to reduce memory usage|https://arxiv.org/abs/2407.19580|[['Memory-efficient', 'GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-28|Classification Matters: Improving Video Action Detection with Class-Specific Attention|action query embedding to make model attend on action related region (not only the actor)|https://arxiv.org/abs/2407.19698|[['cs.CV'], ['ECCV']]
2024-07-28|LLAVADI: What Matters For Multimodal Large Language Models Distillation|experiment with different distillation strategy and suggest to add teacher generated data to instruction tuning data|https://arxiv.org/abs/2407.19409|[['cs.CV', 'cs.CL']]
2024-07-29|FlexAttention for Efficient High-Resolution Vision-Language Models|more low resolution token and less high resolution token to represent a high resolution image|https://arxiv.org/abs/2407.20228|[['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-07-29|ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2|as title|https://arxiv.org/abs/2407.19832|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-07-29|Mixture of Nested Experts: Adaptive Processing of Visual Tokens|expert of different model dimension to obtain optimal performance given compute budge|https://arxiv.org/abs/2407.19985|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-07-29|MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with Encouraging Inter-Head Attention Similarity|quantization that calibrated by synthesized data|https://arxiv.org/abs/2407.20021|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-07-29|Apple Intelligence Foundation Language Models|as title|https://arxiv.org/abs/2407.21075|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-07-30|Pruning Large Language Models with Semi-Structural Adaptive Sparse Training|dynamically select sparse weights and distillation to compress model|https://arxiv.org/abs/2407.20584|[['cs.AI', 'cs.CL']]
2024-07-30|ThinK: Thinner Key Cache by Query-Driven Pruning|query dependent key cache pruning on channel dimension|https://arxiv.org/abs/2407.21018|[['cs.AI', 'cs.CL']]
2024-07-31|MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment|text conditioned segmentation by CLIP loss on mask level representation|https://arxiv.org/abs/2407.21654|[['vision-language'], ['cs.CV'], ['ECCV']]
2024-07-31|Open-Vocabulary Audio-Visual Semantic Segmentation|concatenate universal sound object localization model and CLIP for open vocabulary aduio visual segmentation|https://arxiv.org/abs/2407.21721|[['vision-language'], ['Audio-Visual'], ['cs.AI']]
2024-07-31|Learning Video Context as Interleaved Multimodal Sequences|instruction tuning to make model understand interleaved text images sequence for understanding videos (with subtitles or other information)|https://arxiv.org/abs/2407.21757|[['cs.CV'], ['ECCV']]
2024-07-31|MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts|modality specific MoE modules in 1.4B VLM|https://arxiv.org/abs/2407.21770|[['training efficiency'], ['cs.AI', 'cs.LG']]
2024-07-31|Finch: Prompt-guided Key-Value Cache Compression|sliding window approach in prefill phase for KV cache selection|https://arxiv.org/abs/2408.00167|[['GPU memory'], ['cs.AI']]
2024-08-01|Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation|open vocabulary segmentation by tuning both visual and text encoder of CLIP|https://arxiv.org/abs/2408.00744|[['parameter-efficient'], ['vision-language'], ['cs.CV'], ['ECCV']]
2024-08-01|Scaling Backwards: Minimal Synthetic Pre-training?|show little amount of synthetic data with perturbation is on par with large dataset (may be related to dataset distillation)|https://arxiv.org/abs/2408.00677|[['cs.CV'], ['ECCV']]
2024-08-01|Text-Guided Video Masked Autoencoder|patch level CLIP score to guide masking strategy|https://arxiv.org/abs/2408.00759|[['cs.CV'], ['ECCV']]
2024-08-01|Iterative Prototype Refinement for Ambiguous Speech Emotion Recognition|contrastive learning for class prototype for ambiguously labeled emotion data|https://arxiv.org/abs/2408.00325|[['cs.SD', 'eess.AS']]
2024-08-01|SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context|learn a VAE to make LLM process on sentence level token|https://arxiv.org/abs/2408.00655|[['cs.AI', 'cs.CL']]
2024-08-02|MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a Mixture of Dyadic Experts|observe redundancy in down projection of LoRA, share down projection weights in MoE|https://arxiv.org/abs/2408.01505|[['Parameter Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CL']]
2024-08-02|POA: Pre-training Once for Models of All Sizes|self supervised learning by self distillation between teacher, student and subsampled student (similar to BYOL), construct multiple pre trained model at once|https://arxiv.org/abs/2408.01031|[['cs.CV'], ['ECCV']]
2024-08-02|An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding|make text condition key and value to reduce attention cost when condition is long, further prune visual patch by attention score|https://arxiv.org/abs/2408.01120|[['cs.CV'], ['ECCV']]
2024-08-05|Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs|introduce taxonomy problem in domain adaptation segmentation, use knoledge in VLM to relabel data|https://arxiv.org/abs/2408.02261|[['Vision Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-08-05|Online Temporal Action Localization with Memory-Augmented Transformer|as title|https://arxiv.org/abs/2408.02957|[['cs.CV'], ['ECCV']]
2024-08-05|UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model|MLM can process audio and video, use task router to achieve segmentation or editing task by other experts|https://arxiv.org/abs/2408.02503|[['cs.CL']]
2024-08-06|AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval|less latent token to represent images in dataset to reduce memory cost|https://arxiv.org/abs/2408.03282|[['Memory-Efficient'], ['cs.CV'], ['ECCV']]
2024-08-06|MoExtend: Tuning New Experts for Modality and Task Extension|extend MoE based LLM to visual input|https://arxiv.org/abs/2408.03511|[['vision-language'], ['cs.CV', 'cs.CL']]
2024-08-06|Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement|merge pretrained and fine-tuned model by decomposing weights into direction and magnitude|https://arxiv.org/abs/2408.03092|[['cs.CL']]
2024-08-06|Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters|suggenst that well designed decoding strategy (scaling test time computation) can be more efficient than scaling model parameter|https://arxiv.org/abs/2408.03314|[['cs.LG', 'cs.CL']]
2024-08-06|LLaVA-OneVision: Easy Visual Task Transfer|VLM that can process image or video|https://arxiv.org/abs/2408.03326|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-08-07|ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling|pretrain VLM by image captioning, inference by computing likelihood of attribute|https://arxiv.org/abs/2408.04102|[['vision language', 'VLM'], ['graph'], ['cs.AI', 'cs.CV'], ['ECCV']]
2024-08-08|LaDiMo: Layer-wise Distillation Inspired MoEfier|transform a dense transformer to MoE based transformer by layer wise distillation|https://arxiv.org/abs/2408.04278|[['cs.CL']]
2024-08-08|mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models|VLM for interleaved text video processing, integrate visual feature by cross attention|https://arxiv.org/abs/2408.04840|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-08-09|TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning|continual learning by evaluating weight importance and sparsely updating|https://arxiv.org/abs/2408.05200|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.AI', 'cs.CL']]
2024-08-09|ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation|utilize DINO's strong spatial feature as attention of CLIP to achieve better open vocabulary segmentation|https://arxiv.org/abs/2408.04883|[['cs.CV'], ['ECCV']]
2024-08-09|In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation|simple two stage (unsupervised segmentation then grounding) for open vocabulary segmentation|https://arxiv.org/abs/2408.04961|[['cs.CV'], ['ECCV']]
2024-08-09|Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2|train sparse autoencoder in sub layers of Gemma|https://arxiv.org/abs/2408.05147|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-08-09|VITA: Towards Open-Source Interactive Omni Multimodal LLM|try to builld an open source GPT 4o (audio visual text input, handle interrupted new question)|https://arxiv.org/abs/2408.05211|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-08-10|Sequential Representation Learning via Static-Dynamic Conditional Disentanglement|self supervised learning trying to disentangle static and dynamic feature in video with some theoretical backup|https://arxiv.org/abs/2408.05599|[['cs.AI', 'cs.LG', 'cs.CV'], ['ECCV']]
2024-08-11|Efficient and Versatile Robust Fine-Tuning of Zero-shot Models|loss and training pipeline that focus on robustness on OOD after fine-tuning|https://arxiv.org/abs/2408.05749|[['vision-language'], ['cs.LG', 'cs.CV'], ['ECCV']]
2024-08-11|Efficient Test-Time Prompt Tuning for Vision-Language Models|test time adaptation on predefined text to avoid per image training|https://arxiv.org/abs/2408.05775|[['Vision-Language'], ['cs.CV']]
2024-08-12|CROME: Cross-Modal Adapters for Efficient Multimodal LLM|VLM with text aware visual encoder|https://arxiv.org/abs/2408.06610|[['parameter efficiency'], ['vision-language'], ['cs.LG', 'cs.CV', 'cs.CL']]
2024-08-12|AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out Strategies|construct large MoE model from small dense transformer|https://arxiv.org/abs/2408.06567|[['training efficiency'], ['cs.AI', 'cs.CL']]
2024-08-12|HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization|online (causal) temporal localization with explicitly memory|https://arxiv.org/abs/2408.06437|[['cs.CV'], ['ECCV']]
2024-08-13|Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning|adaptive change compression degree during traning and inference, self distillation to retain performance|https://arxiv.org/abs/2408.06798|[['parameter-efficient'], ['cs.CV'], ['ECCV']]
2024-08-13|Dynamic and Compressive Adaptation of Transformers From Images to Videos|frame level token merging in image to video adaptation (classification task)|https://arxiv.org/abs/2408.06840|[['cs.CV']]
2024-08-16|Adaptive Layer Selection for Efficient Vision Transformer Fine-Tuning|suggest importance of layers vary based on mini batch, adaptively select layer to tune|https://arxiv.org/abs/2408.08670|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.CV']]
2024-08-16|Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling|previous candidate is likely to reoccur in the future, use adjacency matrix of cadidate to do speculative decoding|https://arxiv.org/abs/2408.08696|[['parameter-efficient'], ['cs.LG', 'cs.CL']]
2024-08-16|Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models|independently learn modules for tasks, rounter to combine them during inference|https://arxiv.org/abs/2408.09053|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-08-18|Enhancing Modal Fusion by Alignment and Label Matching for Multimodal Emotion Recognition|two stream contrastive learning then feature fusion for audiovisual emotion recognition|https://arxiv.org/abs/2408.09438|[['cs.AI', 'cs.CV', 'cs.SD']]
2024-08-18|Activated Parameter Locating via Causal Intervention for Model Merging|few shot gradient approximation to evaluate weight importance for model merging|https://arxiv.org/abs/2408.09485|[['cs.CL']]
2024-08-18|MoDeGPT: Modular Decomposition for Large Language Model Compression|modularize model and obtain low rank version of weights by reconstructing modular level output|https://arxiv.org/abs/2408.09632|[['cs.LG', 'cs.CL']]
2024-08-19|LongVILA: Scaling Long-Context Visual Language Models for Long Videos|as title|https://arxiv.org/abs/2408.10188|[['Visual Language', 'VLMs'], ['cs.CV', 'cs.CL']]
2024-08-19|Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism|prune, reactivate to reconstruct feature, then prune again|https://arxiv.org/abs/2408.10473|[['cs.LG', 'cs.CL']]
2024-08-20|TDS-CLIP: Temporal Difference Side Network for Image-to-Video Transfer Learning|as title|https://arxiv.org/abs/2408.10688|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['vision-language'], ['cs.CV']]
2024-08-20|HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments|use attention in visual encoder to drop visual tokens without additional training|https://arxiv.org/abs/2408.10945|[['GPU memory'], ['Vision-Language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-08-20|Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches|survey of memory efficient fintuning|https://arxiv.org/abs/2408.10691|[['memory-efficient', 'efficient fine-tuning', 'GPU memory'], ['cs.AI']]
2024-08-20|Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning|mention activation cache in ladder side tuning|https://arxiv.org/abs/2408.10746|[['Memory Efficient', 'PEFT', 'efficient fine-tuning'], ['cs.AI', 'cs.LG']]
2024-08-20|HMoE: Heterogeneous Mixture of Experts for Language Modeling|MoE of different size to encourage specialized experts, objective to encourage utilizing all experts|https://arxiv.org/abs/2408.10681|[['cs.LG', 'cs.CL']]
2024-08-20|MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding|sparse KV cache to enable large batch speculative decoding|https://arxiv.org/abs/2408.11049|[['cs.CL']]
2024-08-21|EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model|transform LLM to MLM without directly concatenate visual and text tokens, and initialize new cross attention module, strike balance between these two methods|https://arxiv.org/abs/2408.11795|[['visual-language'], ['cs.CV']]
2024-08-21|SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs|align CLIP feature with LLM text feature by contrsative learning|https://arxiv.org/abs/2408.11813|[['vision-language'], ['cs.CV']]
2024-08-21|The Whole Is Bigger Than the Sum of Its Parts: Modeling Individual Annotators to Capture Emotional Variability|annotator dependent emotion recogonition|https://arxiv.org/abs/2408.11956|[['cs.LG', 'eess.AS'], ['Interspeech']]
2024-08-22|Show-o: One Single Transformer to Unify Multimodal Understanding and Generation|unified text (autoregressive) image (diffusion) generator|https://arxiv.org/abs/2408.12528|[['vision-language'], ['diffusion', 'inpainting', 'text-to-image'], ['cs.CV']]
2024-08-23|Online Zero-Shot Classification with CLIP|distribution adaptation in online CLIP zero shot classification|https://arxiv.org/abs/2408.13320|[['Vision-language'], ['cs.LG', 'cs.CV'], ['ECCV']]
2024-08-23|Memory-Efficient LLM Training with Online Subspace Descent|theoretical understanding of convergence with low rank projection optimizer states|https://arxiv.org/abs/2408.12857|[['Memory-Efficient'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-08-23|SeA: Semantic Adversarial Augmentation for Last Layer Features from Unsupervised Representation Learning|feature space augmentation guided by gradient|https://arxiv.org/abs/2408.13351|[['cs.LG', 'cs.CV'], ['ECCV']]
2024-08-23|Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time|as title|https://arxiv.org/abs/2408.13233|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-08-25|Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models|investigate relation between the norm of soft prompt and performance in CLIP|https://arxiv.org/abs/2408.13979|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV'], ['ICLR']]
2024-08-26|Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models|selective finetuning sparse weight, dynamically determine weights to update by parameter importance|https://arxiv.org/abs/2408.14470|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-tuning'], ['cs.CL']]
2024-08-26|Training-Free Activation Sparsity in Large Language Models|training free magnitude based method to make activation sparse to speedup inference|https://arxiv.org/abs/2408.14690|[['cs.AI', 'cs.CL']]
2024-08-26|PAT: Pruning-Aware Tuning for Large Language Models|encourage sparisty during fine tuning to make pruning better|https://arxiv.org/abs/2408.14721|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-08-28|LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation|Direct distillation and preference distillation to make a small sparse version of LLaVA|https://arxiv.org/abs/2408.15881|[['cs.CV']]
2024-08-30|Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer|training free similarity base token reduction in ViT|https://arxiv.org/abs/2408.17062|[['cs.CV']]
2024-09-02|VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges|recurrent network for long video processing|https://arxiv.org/abs/2409.01071|[['GPU memory'], ['cs.CV', 'cs.CL']]
2024-09-02|TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval|token mergin along temporal dimension to reduce redundancy|https://arxiv.org/abs/2409.01156|[['efficient fine-tuning', 'GPU memory'], ['cs.CV']]
2024-09-02|Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information|text aware token merging in VLM|https://arxiv.org/abs/2409.01179|[['cs.CV']]
2024-09-04|LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture|hybrid Mamba and Transformer for long context with multiple images|https://arxiv.org/abs/2409.02889|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-09-04|TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations|modify positional embedding and masking to adapt image VLM to video VLM|https://arxiv.org/abs/2409.03206|[['cs.AI', 'cs.CV']]
2024-09-05|LLM-based multi-agent poetry generation in non-cooperative environments|attention with random vector in later layers does not lead to large performance degradation|https://arxiv.org/abs/2409.03659|[['cs.CL']]
2024-09-05|Sirius: Contextual Sparsity with Correction for Efficient LLMs|utilize spare model for normal inference, detect error and use full model to correct|https://arxiv.org/abs/2409.03856|[['cs.CL']]
2024-09-06|Fast Forwarding Low-Rank Training|faster training by keeping update with a gradient until loss stops improving on a tiny validation set|https://arxiv.org/abs/2409.04206|[['Parameter efficient', 'efficient finetuning'], ['cs.LG', 'cs.CL']]
2024-09-07|POINTS: Improving Your Vision-language Model with Affordable Strategies|experiment on pretraining formula for VLM|https://arxiv.org/abs/2409.04828|[['Vision-language'], ['cs.AI', 'cs.CV']]
2024-09-08|Open-World Dynamic Prompt and Continual Visual Representation Learning|dynamic prompt for continual learning with open world class during testing|https://arxiv.org/abs/2409.05312|[['cs.CV'], ['ECCV']]
2024-09-10|Data Collection-free Masked Video Modeling|synthetic video data to pretrain video model|https://arxiv.org/abs/2409.06665|[['cs.CV'], ['ECCV']]
2024-09-10|Enhancing Long Video Understanding via Hierarchical Event-Based Memory|as title|https://arxiv.org/abs/2409.06299|[['cs.AI', 'cs.CV']]
2024-09-10|SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation|selective finetuning and unstructured gradient for parameter and memory efficient finetuning for diffusion|https://arxiv.org/abs/2409.06633|[['efficient fine-tuning'], ['Diffusion'], ['cs.CV']]
2024-09-11|Self-Masking Networks for Unsupervised Adaptation|learning binary mask in a self supervised manner to replace normal finetuning|https://arxiv.org/abs/2409.07577|[['efficient fine-tuning'], ['cs.LG', 'cs.CV']]
2024-09-12|Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization|use cross modality correspondence to check event continuity and adaptive window attention for long video|https://arxiv.org/abs/2409.07967|[['Audio-Visual'], ['cs.CV']]
2024-09-12|Faster Speech-LLaMA Inference with Multi-token Prediction|multi token prediction for faster ASR using cascaded speech LLM|https://arxiv.org/abs/2409.08148|[['cs.SD', 'eess.AS'], ['ICASSP']]
2024-09-13|NEST-RQ: Next Token Prediction for Speech Self-Supervised Pre-Training|as title|https://arxiv.org/abs/2409.08680|[['cs.AI', 'cs.CL', 'eess.AS']]
2024-09-14|TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings|text guided ways to extract visual features for LLaVA|https://arxiv.org/abs/2409.09564|[['vision-language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-09-15|ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer Acceleration|per layer sparsity for ViT compression|https://arxiv.org/abs/2409.09708|[['cs.LG', 'cs.CV']]
2024-09-16|RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval|vector search algorithm to access 3% KV cache for every attention|https://arxiv.org/abs/2409.10516|[['GPU memory'], ['cs.LG', 'cs.CL']]
2024-09-16|Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models|training free token pruning by minimize divergence of attention distribution before and after pruning|https://arxiv.org/abs/2409.10197|[['cs.CV', 'cs.CL']]
2024-09-17|NVLM: Open Frontier-Class Multimodal LLMs|nvida VLM with new image embedding method and three types of transformer blocks|https://arxiv.org/abs/2409.11402|[['training efficiency'], ['vision-language'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-09-17|Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs|token pruning by CLIP score|https://arxiv.org/abs/2409.10994|[['cs.AI', 'cs.CL']]
2024-09-18|LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models|black box optimization of VLM by finetuning LLM on output distribution|https://arxiv.org/abs/2409.11919|[['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-09-19|CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs|token pruning during prefilling|https://arxiv.org/abs/2409.12490|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-09-19|CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information|structure pruning and recovery fine tuning based on activation|https://arxiv.org/abs/2409.13199|[['cs.CL']]
2024-09-23|Enabling Resource-Efficient On-Device Fine-Tuning of LLMs Using Only Inference Engines|increase parallelizability of gradient estimation for fine-tuning with only forward pass|https://arxiv.org/abs/2409.15520|[['parameter-efficient', 'efficient fine-tuning'], ['cs.LG']]
2024-09-24|CAD: Memory Efficient Convolutional Adapter for Segment Anything|ladder side tuning to finetune SAM|https://arxiv.org/abs/2409.15889|[['Memory Efficient', 'GPU memory'], ['cs.CV']]
2024-09-24|DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation|utilize patch features to produce pseudo label for segmentation|https://arxiv.org/abs/2409.15801|[['cs.CV'], ['ECCV']]
2024-09-24|Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering|decompose LoRA into weights corresponding to different rank, clustering and reassembling them into new LoRA|https://arxiv.org/abs/2409.16167|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-09-25|PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization|reduce gradient norm and align fine-tuned model with pre-trained one to enhance generalizability with theoretical backup|https://arxiv.org/abs/2409.17137|[['PArameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.LG', 'cs.CV'], ['NeurIPS']]
2024-09-25|Attention Prompting on Image for Large Vision-Language Models|make VLM focus by apply attention heatmap to input images|https://arxiv.org/abs/2409.17143|[['Vision-Language'], ['cs.AI', 'cs.CV']]
2024-09-25|Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction|token pruning based on early layers features|https://arxiv.org/abs/2409.17422|[['memory efficiency', 'GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-09-25|INT-FlashAttention: Enabling Flash Attention for INT8 Quantization|as title|https://arxiv.org/abs/2409.16997|[['GPU memory'], ['cs.AI', 'cs.LG']]
2024-09-25|MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models|learnable N:M compression of LLM using gumbel reparameterization|https://arxiv.org/abs/2409.17481|[['cs.AI', 'cs.LG', 'cs.CL'], ['NeurIPS']]
2024-09-25|Non-asymptotic Convergence of Training Transformers for Next-token Prediction|in depth analysis on training dynamics (speed of convergence) of transformer blocks|https://arxiv.org/abs/2409.17335|[['cs.LG'], ['NeurIPS']]
2024-09-25|Search for Efficient Large Language Models|training free architecture search for compressing LLM|https://arxiv.org/abs/2409.17372|[['GPU memory'], ['architecture search'], ['cs.AI'], ['NeurIPS']]
2024-09-26|Cascade Prompt Learning for Vision-Language Model Adaptation|boosting visual prompt tuning by cascade prompt distilled from teacher|https://arxiv.org/abs/2409.17805|[['Vision-Language', 'VLMs'], ['cs.CV'], ['ECCV']]
2024-09-26|MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning|dataset and frame work to deal with causality of events in long videos|https://arxiv.org/abs/2409.17647|[['cs.CV'], ['NeurIPS']]
2024-09-27|From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation|audio visual MAE for shared latent space or generation|https://arxiv.org/abs/2409.19132|[['Audio-Visual'], ['cs.LG', 'cs.CV', 'cs.SD', 'eess.AS'], ['ICML']]
2024-09-27|Exploring Token Pruning in Vision State Space Models|as title|https://arxiv.org/abs/2409.18962|[['cs.AI', 'cs.LG', 'cs.CV'], ['NeurIPS']]
2024-09-27|Learning to Obstruct Few-Shot Image Classification over Restricted Classes|try to prevent model from predicting specific classes|https://arxiv.org/abs/2409.19210|[['cs.CV'], ['ECCV']]
2024-09-28|CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling|finetune multple CLIP for different features and build an MoE version of CLIP|https://arxiv.org/abs/2409.19291|[['cs.AI', 'cs.CV']]
2024-09-29|Vision-Language Models are Strong Noisy Label Detectors|use CLIP to filter noisy label|https://arxiv.org/abs/2409.19696|[['parameter-efficient', 'efficient fine-tuning'], ['Vision-Language'], ['cs.LG', 'cs.CV'], ['NeurIPS']]
2024-09-29|Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels|CLIP for segmentation using SAM and DINO|https://arxiv.org/abs/2409.19846|[['vision-language'], ['cs.CV'], ['NeurIPS']]
2024-09-29|One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos|frame and patch level feature for video LLM, predict special token for segmentation|https://arxiv.org/abs/2409.19603|[['cs.AI', 'cs.CV']]
2024-09-30|KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head|as title|https://arxiv.org/abs/2410.00161|[['cs.CL']]
2024-10-01|TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices|inference 70B model on edge device by tensor parallelism with other devices|https://arxiv.org/abs/2410.00531|[['memory-efficient'], ['cs.AI']]
2024-10-02|House of Cards: Massive Weights in LLMs|find LLM behavior is controled by sparse large activation, apply dropout on them to improve generalizability|https://arxiv.org/abs/2410.01866|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-10-02|Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?|full rank training with low rank gradient, cannot figure out how to do it|https://arxiv.org/abs/2410.01623|[['memory efficiency'], ['cs.AI', 'cs.LG']]
2024-10-02|Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices|NAS to find linear operation that is effective and efficient|https://arxiv.org/abs/2410.02117|[['cs.LG'], ['NeurIPS']]
2024-10-03|Learning from Offline Foundation Features with Tensor Augmentations|augmentation on feature space to speed up training and reduce memory usage|https://arxiv.org/abs/2410.02527|[['GPU memory'], ['cs.CV'], ['NeurIPS']]
2024-10-03|FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech Language Model|token reduction for speech LLM|https://arxiv.org/abs/2410.03007|[['memory efficiency'], ['cs.AI', 'cs.CL', 'eess.AS'], ['EMNLP']]
2024-10-03|Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection|experiment on which layer should we do knowledge distillation|https://arxiv.org/abs/2410.02330|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-10-03|Contrastive Localized Language-Image Pre-Training|pretraining CLIP with localization ability|https://arxiv.org/abs/2410.02746|[['vision-language'], ['cs.LG', 'cs.CV']]
2024-10-03|LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy|as title|https://arxiv.org/abs/2410.03111|[['GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-10-03|Parameter Competition Balancing for Model Merging|training free model merging method that considers importance and similarity of parameters|https://arxiv.org/abs/2410.02396|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL'], ['NeurIPS']]
2024-10-03|Selective Attention Improves Transformer|subtract attention weights by a soft mask to improve performance|https://arxiv.org/abs/2410.02703|[['cs.AI', 'cs.LG', 'cs.CL']]
