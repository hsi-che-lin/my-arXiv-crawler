sep=|
date|title|note|url|keywords
2024-03-28|Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment|only has ordered list of action when training, improve pseudo label efficiency|https://arxiv.org/abs/2403.19225|[['cs.CV'], ['CVPR']]
2024-03-28|CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection|tackle class imbalance problem in DA, learning class relation and do augmentation|https://arxiv.org/abs/2403.19278|[['cs.CV'], ['CVPR']]
2024-03-28|Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality|multimodal narratives caption for long video, augmentation to handl missing modality|https://arxiv.org/abs/2403.19221|[['cs.CV']]
2024-03-28|Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models|benchmark and baseline (decide whether to zoom in) for VLM to handle high resolution, text rich images|https://arxiv.org/abs/2403.19322|[['cs.CV']]
2024-03-28|Checkpoint Merging via Bayesian Optimization in LLM Pretraining|study on merging LLM|https://arxiv.org/abs/2403.19390|[['cs.CL']]
2024-03-28|Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition|dynamically use cross attention to prevent effect between low valence modality|https://arxiv.org/abs/2403.19554|[['cs.CV']]
2024-03-28|LocCa: Visual Pretraining with Location-aware Captioners|as title|https://arxiv.org/abs/2403.19596|[['cs.CV']]
2024-03-28|Siamese Vision Transformers are Scalable Audio-visual Learners|same ViT to process audio and visual, pretrained with contrastive learning and MAE|https://arxiv.org/abs/2403.19638|[['cs.CV']]
2024-03-28|MedBN: Robust Test-Time Adaptation against Malicious Test Samples|use statistics (batch norm) to handle manipulated data in test time adaptation|https://arxiv.org/abs/2403.19326|[['attack'], ['cs.LG'], ['CVPR']]
2024-03-29|Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer|use adapter in class incremental learning, learn prototypes for old classes|https://arxiv.org/abs/2403.19979|[['parameter-efficient'], ['cs.CV'], ['CVPR']]
2024-03-29|MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning|task specific LoRA, and task agnostic LoRA for multi task learning|https://arxiv.org/abs/2403.20320|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CV'], ['CVPR']]
2024-03-29|LayerNorm: A key component in parameter-efficient fine-tuning|argue that tuning layer norm is enough for downstream tasks|https://arxiv.org/abs/2403.20284|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL']]
2024-03-29|Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations|text to image model based on can't read negated description, try to make CLIP understand negation|https://arxiv.org/abs/2403.20312|[['Vision-Language'], ['cs.CV']]
2024-03-29|Are We on the Right Way for Evaluating Large Vision-Language Models?|benchmark for VLM, avoid data leakage and make sure necessity of visual input|https://arxiv.org/abs/2403.20330|[['Vision-Language'], ['cs.CV']]
2024-03-29|Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models|make VLM refuse to answer when the question is not solvable|https://arxiv.org/abs/2403.20331|[['Vision Language', 'VLM'], ['cs.CV']]
2024-03-29|Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning|increase complexity of data augmentation as training process goes|https://arxiv.org/abs/2403.20012|[['cs.CV'], ['ICLR']]
2024-03-29|Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion|as title|https://arxiv.org/abs/2403.20015|[['cs.CL'], ['ICLR']]
2024-03-29|ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning|continual learn for segmentation using prompt tuning |https://arxiv.org/abs/2403.20126|[['cs.CV'], ['CVPR']]
2024-03-29|Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions|benchmark and simple baseline for temporal corrupted|https://arxiv.org/abs/2403.20254|[['cs.CV'], ['CVPR']]
2024-03-29|Convolutional Prompting meets Language Models for Continual Learning|convolutional hyper network to achieve layer-wise input-aware prompt tuning for continual learning|https://arxiv.org/abs/2403.20317|[['cs.CV'], ['CVPR']]
2024-03-29|On Large Language Models' Hallucination with Regard to Known Facts|detect hallucination in LLM by tracking token probabilities|https://arxiv.org/abs/2403.20009|[['cs.CL']]
2024-03-29|FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues|swap features of same concept from different modalities and train on matching loss to enhance performance|https://arxiv.org/abs/2403.20026|[['cs.CV']]
2024-03-29|Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning|prompt and finetune to make LLM learn from error during CoT|https://arxiv.org/abs/2403.20046|[['cs.CL']]
2024-04-02|T-VSL: Text-Guided Visual Sound Source Localization in Mixtures|use text as guide to disentangle audio and visual for multi source sounding localization|https://arxiv.org/abs/2404.01751|[['audio-visual'], ['cs.CV'], ['CVPR']]
2024-04-02|BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition|audio visual SSL pretraining, dual encoders MAE|https://arxiv.org/abs/2404.02098|[['audio-visual'], ['cs.CV'], ['ICASSP']]
2024-04-02|ViTamin: Designing Scalable Vision Models in the Vision-Language Era|propose evaluation protocol for visual model (using CLIP method), propose CNN Transformer hybrid model|https://arxiv.org/abs/2404.02132|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|Iterated Learning Improves Compositionality in Large Vision-Language Models|finite code book and re-initialize model during CLIP training for better compositionality ("girl in white facing man in black" vs "girl in black facing man in white")|https://arxiv.org/abs/2404.02145|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|VLRM: Vision-Language Models act as Reward Models for Image Captioning|reinforcement learning for better captioning|https://arxiv.org/abs/2404.01911|[['Vision-Language'], ['cs.CV']]
2024-04-02|Weakly-supervised Audio Separation via Bi-modal Semantic Similarity|audio separation without access to single source audio, mix multi source audio and use contrastive loss|https://arxiv.org/abs/2404.01740|[['cs.SD'], ['ICLR']]
2024-04-02|Joint-Task Regularization for Partially Labeled Multi-Task Learning|as title|https://arxiv.org/abs/2404.01976|[['cs.CV'], ['CVPR']]
2024-04-02|Using Interpretation Methods for Model Enhancement|learn to make interpretation supervisedly, different ways to generate interpretation|https://arxiv.org/abs/2404.02068|[['cs.CL'], ['EMNLP']]
2024-04-02|Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners|as title|https://arxiv.org/abs/2404.02117|[['cs.CV'], ['CVPR']]
2024-04-04|ReFT: Representation Finetuning for Language Models|can't understand but it is PEFT + LLM|https://arxiv.org/abs/2404.03592|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-04-04|Would Deep Generative Models Amplify Bias in Future Models?|find that using sythesized data may not increase bias|https://arxiv.org/abs/2404.03242|[['social biases'], ['Diffusion'], ['cs.CV'], ['CVPR']]
2024-04-04|Scaling Up Video Summarization Pretraining with Large Language Models|new dataset for long video, using automated pipeline|https://arxiv.org/abs/2404.03398|[['cs.CV'], ['CVPR']]
2024-04-04|MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens|VLM, concatenate frame feature and subtitle feature for video understanding|https://arxiv.org/abs/2404.03413|[['cs.CV']]
2024-04-04|Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought|RL, knowledge distillation for small LM to deal with multi-hop QA|https://arxiv.org/abs/2404.03414|[['cs.CL']]
2024-04-16|Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models|align visual feature with text feature GPT4 prompt; leaarable prompt for CLIP classification|https://arxiv.org/abs/2404.10357|[['Vision-Language'], ['cs.CV']]
2024-04-16|Self-Supervised Visual Preference Alignment|use data augmentation to produce data for DPO|https://arxiv.org/abs/2404.10501|[['Vision-Language', 'VLM'], ['cs.CV']]
2024-04-16|Future Language Modeling from Temporal Document History|new task of future language modeling|https://arxiv.org/abs/2404.10297|[['cs.CL'], ['ICLR']]
2024-04-16|Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation|perturbating on feature space to simulate domain shift and use adapter to rectify the perturbation and deal with domain shift|https://arxiv.org/abs/2404.10322|[['cs.CV'], ['CVPR']]
2024-04-16|Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model|locate modules that is adept for specific tasks to prevent catastrophoic forgetting|https://arxiv.org/abs/2404.10306|[['cs.CL']]
2024-04-16|Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards|LLM self-correction to do DPO|https://arxiv.org/abs/2404.10346|[['cs.CL']]
2024-04-16|Dual Modalities of Text: Visual and Textual Generative Pre-training|render text as image and do next patch, token prediction simultaneously|https://arxiv.org/abs/2404.10710|[['cs.CL']]
2024-04-16|Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study|as title|https://arxiv.org/abs/2404.10719|[['cs.CL']]
2024-04-15|Bridging Vision and Language Spaces with Assignment Prediction|align visual feature with word embedding, don't need to modify LLM's weights|https://arxiv.org/abs/2404.09632|[['vision-language'], ['cs.CV'], ['ICLR']]
2024-04-15|Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering|use proxy model to sample neighborhood question, and use consistency on neighborhood questions to measure reliability|https://arxiv.org/abs/2404.10193|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-15|Leveraging Temporal Contextualization for Video Action Recognition|extract context tokens of videos as KV for better aggregation of information|https://arxiv.org/abs/2404.09490|[['vision-language'], ['cs.CV']]
2024-04-15|Conditional Prototype Rectification Prompt Learning|learn textual prototype in semi-supervised manners to mitigate overfitting|https://arxiv.org/abs/2404.09872|[['vision-language'], ['cs.CV']]
2024-04-15|Evolving Interpretable Visual Classifiers with Large Language Models|use elvolutionary search with LLM and CLIP to build interpretable image classifier|https://arxiv.org/abs/2404.09941|[['vision-language'], ['cs.CV']]
2024-04-15|LoRA Dropout as a Sparsity Regularizer for Overfitting Control|add noise to LoRA weights to increase parameter sparisty, prevent overfitting theoretically|https://arxiv.org/abs/2404.09610|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.LG']]
2024-04-15|The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning|semi supervised method for CLIP to tackle narrow distribution during few shots learning|https://arxiv.org/abs/2404.09778|[['cs.CV'], ['ECCV']]
2024-04-15|Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model|study on quality of instruction tuning data|https://arxiv.org/abs/2404.09717|[['cs.CL']]
2024-04-15|Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations|as title|https://arxiv.org/abs/2404.09785|[['cs.CL']]
2024-04-15|TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding|use captioning, detection, and cropping to enhance VQA performance|https://arxiv.org/abs/2404.09797|[['cs.CV']]
2024-04-15|Impact of Preference Noise on the Alignment Performance of Generative Language Models|study on quality of data to do alignment|https://arxiv.org/abs/2404.09824|[['cs.CL']]
2024-04-15|CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting|study on behaviors of LLM on different languages and cultures|https://arxiv.org/abs/2404.10199|[['cs.CL']]
2024-04-15|Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models|as title|https://arxiv.org/abs/2404.09529|[['cs.LG']]
2024-04-15|LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models|low rank pruning of LLM by observation of low rank structure of MHA|https://arxiv.org/abs/2404.09695|[['cs.LG']]
2024-04-25|AAPL: Adding Attributes to Prompt Learning for Vision-Language Models|reduce bias introduced by data augmentation in previous zero shot classification method|https://arxiv.org/abs/2404.16804|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-25|Training-Free Unsupervised Prompt for Vision-Language Models|training and label free method to boost zero-shot classification, generate high confident prototype and compute similarity to adjust logits|https://arxiv.org/abs/2404.16339|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-04-25|Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class|use multiple vectors to represent a class so that different attribute in a class can be taken into account|https://arxiv.org/abs/2404.16717|[['Vision-language', 'VLM'], ['cs.CV']]
2024-04-25|Multi-Scale Representations by Varying Window Attention for Semantic Segmentation|constrain attention region for better multi-scale feature extraction in segmentation|https://arxiv.org/abs/2404.16573|[['cs.CV'], ['ICLR']]
2024-04-25|EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning|instruction tuning for visual emotional recognition, new data and model|https://arxiv.org/abs/2404.16670|[['cs.CV'], ['CVPR']]
2024-04-25|List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs|data set for set-of-mark (use visual tags on image for MLM to refer to), a trick enhance model performance and interpretability|https://arxiv.org/abs/2404.16375|[['cs.CV']]
2024-04-25|Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities|feature disentangle during distillation to handle missing modality|https://arxiv.org/abs/2404.16456|[['cs.CV']]
2024-04-25|Exploring Internal Numeracy in Language Models: A Case Study on ALBERT|study on how LM handle embeddings of numbers (increase in one direction after PCA)|https://arxiv.org/abs/2404.16574|[['cs.CL']]
2024-04-25|Understanding Privacy Risks of Embeddings Induced by Large Language Models|study on potential of LLM reconstruction pre-training data|https://arxiv.org/abs/2404.16587|[['cs.CL']]
2024-04-25|TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning|mtrain model to generate code for calculation, token merging for high resolution images|https://arxiv.org/abs/2404.16635|[['cs.CV']]
2024-04-25|Zero-Shot Distillation for Image Encoders: How to Make Effective Use of Synthetic Data|L2 loss is better than contrastive loss for distillation|https://arxiv.org/abs/2404.16637|[['cs.CV']]
2024-04-25|Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4|GPT-4 are more biased to add information when asked to improve its answers when subtraction was more efficient|https://arxiv.org/abs/2404.16692|[['cs.CL']]
2024-04-25|Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding|layer dropout in LLM during training, early exit during inference and self-speculative decoding for speed up inference|https://arxiv.org/abs/2404.16710|[['cs.CL']]
2024-04-25|SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension|as title|https://arxiv.org/abs/2404.16790|[['cs.CV']]
2024-04-25|Make Your LLM Fully Utilize the Context|fine tune LLM on long context|https://arxiv.org/abs/2404.16811|[['cs.CL']]
2024-04-25|How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites|VLM with 6B ViT, dynamic resolution, English and Chinese training data|https://arxiv.org/abs/2404.16821|[['cs.CV']]
2024-04-25|T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients|as title|https://arxiv.org/abs/2404.16495|[['cs.LG']]
2024-04-25|VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations|argue that LMs have difficulties in distinguishing between text semantic variations|https://arxiv.org/abs/2404.16365|[['vision-language', 'VLMs'], ['face'], ['cs.CL']]
2024-04-24|FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication|study about effect of deduplication on social bias|https://arxiv.org/abs/2404.16123|[['Vision-Language'], ['social biases'], ['cs.CV'], ['CVPR']]
2024-04-24|What Makes Multimodal In-Context Learning Work?|argue the lack of exploration in multimodal in context learning|https://arxiv.org/abs/2404.15736|[['cs.CV'], ['CVPR']]
2024-04-24|MoDE: CLIP Data Experts via Clustering|train multiple CLIP on some clusters of data and ensemble them at inference time|https://arxiv.org/abs/2404.16030|[['cs.CV'], ['CVPR']]
2024-04-24|CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data|train CLIP by prediction synset (BCE loss), speed up 2.7x|https://arxiv.org/abs/2404.15653|[['cs.CV']]
