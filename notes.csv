sep=|
date|title|note|url|keywords
2024-01-29|Memory-Inspired Temporal Prompt Interaction for Text-Image Classification|Prompt engineering for multi-modal interation|https://arxiv.org/abs/2401.14856|[]
2024-01-30|Synchformer: Efficient Synchronization from Sparse Cues|Sparse-clued audio visual synchronization through pre-training|https://arxiv.org/abs/2401.16423|[]
2024-01-30|InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model|"VLM, add LoRA to visual encoder, QA data"|https://arxiv.org/abs/2401.16420|[]
2024-01-30|MoE-LLaVA: Mixture of Experts for Large Vision-Language Models|Mixture of expert LLaVA (sparesly activated VLM)|https://arxiv.org/abs/2401.15947|[]
2024-01-30|Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization|VLM to deal with OOD data by synthesized augmentation features|https://arxiv.org/abs/2401.15914|[]
2024-01-30|M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining|multi-lingual (en-ch) CLIP-like VLM with 6B dataset|https://arxiv.org/abs/2401.15896|[]
2024-01-30|Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA|multi-pannel VQA (using off-the-shelf VLM)|https://arxiv.org/abs/2401.15847|[]
2024-01-30|Data-Free Generalized Zero-Shot Learning|zero-shot learning without access to pretraining dataset|https://arxiv.org/abs/2401.15657|[]
2024-01-30|SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection|as title|https://arxiv.org/abs/2401.15293|[]
2024-01-30|Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks|Continual learning of VL tasks using knowledge distillation|https://arxiv.org/abs/2401.15275|[]
2024-01-31|MouSi: Poly-Visual-Expert Vision-Language Models|VLM with multiple visual encoder (for different tasks)|https://arxiv.org/abs/2401.17221|[]
2024-01-31|Category-wise Fine-Tuning: Resisting Incorrect Pseudo-Labels in Multi-Label Image Classification with Partial Labels|Improved pseudo labeling strategy by finetuning each class individually|https://arxiv.org/abs/2401.16991|[]
2024-01-31|Reviving Undersampling for Long-Tailed Learning|as title|https://arxiv.org/abs/2401.16811|[]
2024-01-31|MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images|zero-shot anomaly detection by occurance frequency difference|https://arxiv.org/abs/2401.16753|[]
2024-01-31|Multi-granularity Correspondence Learning from Long-term Noisy Videos|long video by captioning clips|https://arxiv.org/abs/2401.16702|[]
2024-01-31|SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design|efficient vit with fewer patches|https://arxiv.org/abs/2401.16456|[]
2024-01-31|Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking|new probing method that explore VLM has ability to understand verbs|https://arxiv.org/abs/2401.16575|[]
2024-02-09|SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models|"VLM that can do detection, grounding, .... visual tasks"|https://arxiv.org/abs/2402.05935|[]
2024-02-09|Point-VOS: Pointing Up Video Object Segmentation|video segmentation with sparse annotation|https://arxiv.org/abs/2402.05917|[]
2024-02-09|Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data|mamba for multi-dimensional data|https://arxiv.org/abs/2402.05892|[]
2024-02-09|CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion|multi-modal video LLM|https://arxiv.org/abs/2402.05889|[]
2024-02-09|Memory Consolidation Enables Long-Context Video Understanding|vit with explicit memory for long videos|https://arxiv.org/abs/2402.05861|[]
2024-02-09|Question Aware Vision Transformer for Multimodal Reasoning|vqa using llm with attention-enhenced vision encoder|https://arxiv.org/abs/2402.05472|[]
2024-02-09|Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts|MoE during MAE pretraining|https://arxiv.org/abs/2402.05382|[]
2024-02-08|EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss|efficient SAM by distillation and continual training|https://arxiv.org/abs/2402.05008|[]
2024-02-08|ConvLoRA and AdaBN based Domain Adaptation via Self-Training|DA using PEFT and self-training|https://arxiv.org/abs/2402.04964|[]
2024-02-08|Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation|test time adaptation|https://arxiv.org/abs/2402.04958|[]
2024-02-08|Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation|as title|https://arxiv.org/abs/2402.04929|[]
2024-02-08|Data-efficient Large Vision Models through Sequential Autoregression|autoregressive vision model|https://arxiv.org/abs/2402.04841|[]
2024-02-08|LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors|improve open vocabulary object detection by attribute description|https://arxiv.org/abs/2402.04630|[]
2024-02-13|Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models|propose new VLM|https://arxiv.org/abs/2402.07865|[]
2024-02-13|Towards Meta-Pruning via Optimal Transport|new pruning methods using model fusion and optimal transport|https://arxiv.org/abs/2402.07839|[]
2024-02-13|PBADet: A One-Stage Anchor-Free Approach for Part-Body Association|as title|https://arxiv.org/abs/2402.07814|[]
2024-02-13|Complete Instances Mining for Weakly Supervised Instance Segmentation|instance segmentation using image-level labels|https://arxiv.org/abs/2402.07633|[]
2024-02-13|A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)|as title|https://arxiv.org/abs/2402.07410|[]
2024-02-13|Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy|new VQA benchmark|https://arxiv.org/abs/2402.07270|[]
2024-02-13|PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs|prompt VLMs with iteratively refined QAa|https://arxiv.org/abs/2402.07872|[]
2024-02-12|Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy|model compression by low rank approximation|https://arxiv.org/abs/2402.06004|[]
2024-02-14|PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs|enable detection ability of VLM|https://arxiv.org/abs/2402.08657|[]
2024-02-14|Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks|adapt VLM to different tasks by transforming data in QA pairs|https://arxiv.org/abs/2402.08360|[]
2024-02-14|Pix2Code: Learning to Compose Neural Visual Concepts as Programs|solve visual problems by composing programs|https://arxiv.org/abs/2402.08280|[]
2024-02-15|Gradient Alignment with Prototype Feature for Fully Test-time Adaptation|test time adaptation|https://arxiv.org/abs/2402.09004|[]
2024-02-15|Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision|open vocabulary segmentation using independent image-mask and image-text pairs|https://arxiv.org/abs/2402.08960|[]
2024-02-15|DoRA: Weight-Decomposed Low-Rank Adaptation|Improved LoRA by decomposing weights into direction and magnitude|https://arxiv.org/abs/2402.09353|[]
2024-02-15|Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models|as title|https://arxiv.org/abs/2402.08756|[]
2024-02-15|BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation|test time adaptation using LoRA and MoE|https://arxiv.org/abs/2402.08712|[]
2024-02-16|MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations|adding contrastive learning to MAE to boost linear probing or 1-shot learning|https://arxiv.org/abs/2402.10093|[]
2024-02-16|Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP|domain generalization of PEFT|https://arxiv.org/abs/2402.09613|[]
2024-02-16|BitDelta: Your Fine-Tune May Only Be Worth One Bit|decompose fine-tune into direction (1-bit) and a scalar|https://arxiv.org/abs/2402.10193|[]
2024-02-16|Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection|pruning neurons that leads to overfitting|https://arxiv.org/abs/2402.10062|[]
2024-02-16|Fast Vocabulary Transfer for Language Model Compression|improve efficiency by fine-tune tokenizer on downstream domain so that the tokenized sequence is shorter|https://arxiv.org/abs/2402.09977|[]
2024-02-16|Multi-Word Tokenization for Sequence Compression|improve efficiency by view multiple words as one token|https://arxiv.org/abs/2402.09949|[]
2024-01-30|SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing|freezing layers to accelerate training|https://arxiv.org/abs/2401.16720|[]
2024-02-19|PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter|tiny LM between LLaMA adapters and LLM|https://arxiv.org/abs/2402.10896|[]
2024-02-19|Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering|Video QA by captioning frames|https://arxiv.org/abs/2402.10698|[]
2024-02-19|Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond|make MLLM to do retrieval (generation) tasks|https://arxiv.org/abs/2402.10805|[]
2024-02-21|CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples|improve VLM's ability of counting and position understanding by data augmentation|https://arxiv.org/abs/2402.13254|[]
2024-02-21|Video ReCap: Recursive Captioning of Hour-Long Videos|long video captioning by hierarchical decomposing videos|https://arxiv.org/abs/2402.13250|[]
2024-02-21|VideoPrism: A Foundational Visual Encoder for Video Understanding|large video understanding model by google team|https://arxiv.org/abs/2402.13217|[]
2024-02-21|OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog|video QA requires object state understanding (?)|https://arxiv.org/abs/2402.13146|[]
2024-02-21|Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model|DA with black box API using pseudo labeling|https://arxiv.org/abs/2402.13122|[]
2024-02-21|Slot-VLM: SlowFast Slots for Video-Language Modeling|"Use slow-fast, object-centric, event-centric idea to patchify videos"|https://arxiv.org/abs/2402.13088|[]
2024-02-21|ConVQG: Contrastive Visual Question Generation with Multimodal Guidance|Use contrastive loss to generate VQA that is related to both image and text|https://arxiv.org/abs/2402.12846|[]
2024-02-21|Model Composition for Multimodal Large Language Models|combining foundation model of each domain to build a MLLM|https://arxiv.org/abs/2402.12750|[]
2024-02-21|Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering|VQA with multi-modal external knowledge source|https://arxiv.org/abs/2402.12728|[]
2024-02-21|Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition|try to learn domain-invariant physics to improve few-shot action recognition|https://arxiv.org/abs/2402.12706|[]
2024-02-21|Efficient Parameter Mining and Freezing for Continual Object Detection|continual learning by selecting parameters to freeze|https://arxiv.org/abs/2402.12624|[]
2024-02-21|Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization|MoE that is better at scaling the number of experts|https://arxiv.org/abs/2402.12550|[]
2024-02-21|Towards Cross-Domain Continual Learning|as title|https://arxiv.org/abs/2402.12490|[]
2024-02-21|Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation|teacher-agnostic knowledge distillation without original data|https://arxiv.org/abs/2402.12406|[]
2024-02-20|UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking|deal with ID switch during multi object tracking|https://arxiv.org/abs/2402.12303|[]
2024-02-20|Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers|processing long sequence by cross attention with with learnable tokens|https://arxiv.org/abs/2402.12138|[]
2024-02-20|LVCHAT: Facilitating Long Video Comprehension|enable long video by hierarchical token merging|https://arxiv.org/abs/2402.12079|[]
2024-02-22|Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation|segmentation using vision query for better generalizability|https://arxiv.org/abs/2402.13697|[]
2024-02-22|A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models|evaluate gender bias by observing output (text or image) preference|https://arxiv.org/abs/2402.13636|[]
2024-02-22|Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement|improve moment retrieval by detecting relevance in text and vision separately|https://arxiv.org/abs/2402.13576|[]
2024-02-22|Event-aware Video Corpus Moment Retrieval|improve moment retrieval by merging semantically similar scene into event|https://arxiv.org/abs/2402.13566|[]
2024-02-22|Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization|improve low precision training by considering data distribution|https://arxiv.org/abs/2402.13497|[]
2024-02-22|Unsupervised learning based object detection using Contrastive Learning|as title|https://arxiv.org/abs/2402.13465|[]
2024-02-22|Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning|use CLIP and discriminator as reward to train image captioning model|https://arxiv.org/abs/2402.13936|[]
2024-02-22|Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment|better modality alignment of VLM in addition to Q-Former|https://arxiv.org/abs/2402.13561|[]
2024-02-22|Unsupervised Concept Discovery Mitigates Spurious Correlations|improve robustness of classification by unsupervisedly clustering concepts and adjusting sample strategy|https://arxiv.org/abs/2402.13368|[]
2024-02-22|SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning|"dealing with imbalanced data withtout predefining distribution (e.g., whether long tail or not)"|https://arxiv.org/abs/2402.13505|[]
2024-02-22|LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs|make LLM deal with long video by token merging|https://arxiv.org/abs/2402.13546|[]
2024-02-20|Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before|multi-stage contrastive learning|https://arxiv.org/abs/2402.11816|[]
2024-02-20|Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models|self-consistency of VLM|https://arxiv.org/abs/2402.11622|[]
2024-02-20|CPN: Complementary Proposal Network for Unconstrained Text Detection|text detection that has irregular layout|https://arxiv.org/abs/2402.11540|[]
2024-02-20|Key Patch Proposer: Key Patches Contain Rich Information|select (without training) most important patches that minimize reconstruction loss|https://arxiv.org/abs/2402.11458|[]
2024-02-20|Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning|large video LLM using new automatic annotated large dataset|https://arxiv.org/abs/2402.11435|[]
2024-02-20|Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition|zero-shot learning by generating unseen data but use OOD detector to do regularization|https://arxiv.org/abs/2402.11424|[]
2024-02-20|Learning by Reconstruction Produces Uninformative Features For Perception|Yann LeCun argues the power of learning by reconstruction|https://arxiv.org/abs/2402.11337|[]
2024-02-20|ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition|intention driven dataset and VLM for better user experience|https://arxiv.org/abs/2402.11301|[]
2024-02-20|CoLLaVO: Crayon Large Language and Vision mOdel|using panoptic segmentation as additional clues for better zero-shot performance|https://arxiv.org/abs/2402.11248|[]
2024-02-20|A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation|UNet-like decoder for fewer computational cost segmentation|https://arxiv.org/abs/2402.11201|[]
2024-02-20|GIM: Learning Generalizable Image Matcher From Internet Videos|as title|https://arxiv.org/abs/2402.11095|[]
2024-02-20|The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test|as title|https://arxiv.org/abs/2402.11089|[]
2024-02-20|AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling|MLLM that understands and generates multimodal data|https://arxiv.org/abs/2402.12226|[]
2024-02-23|WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition|use SAM as pseudo ground truth to solve weakly supervised object detection|https://arxiv.org/abs/2402.14812|[]
2024-02-23|DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models|do detection fisrt and then zoom in to solve fine-grained VQA|https://arxiv.org/abs/2402.14767|[]
2024-02-23|Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition|adapt foundation model to downstream tasks with large domain shift (visual place recognition)|https://arxiv.org/abs/2402.14505|[]
2024-02-23|Reading Relevant Feature from Global Representation Memory for Visual Object Tracking|object tracking with memory and a module to select memory to use|https://arxiv.org/abs/2402.14392|[]
2024-02-20|The Effectiveness of Random Forgetting for Robust Generalization|reinitialize weights (forget) to mitigate attach|https://arxiv.org/abs/2402.11733|[]
2024-02-20|Aligning Modalities in Vision Large Language Models via Preference Fine-tuning|solve hallucination problem of VLM by adding noise to cause hallucinationg and guide model with ground truth|https://arxiv.org/abs/2402.11411|[]
2024-02-20|Knowledge Distillation Based on Transformed Teacher Matching|study on function of temperature used in knowledge distillation|https://arxiv.org/abs/2402.11148|[]
2024-02-23|Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning|"as title (evaluate on protein biology, chemical property prediction, and particle physics)"|https://arxiv.org/abs/2402.14789|[]
2024-02-23|OmniPred: Language Models as Universal Regressors|as title|https://arxiv.org/abs/2402.14547|[]
2024-02-23|TinyLLaVA: A Framework of Small-scale Large Multimodal Models|better data with smaller MLLM|https://arxiv.org/abs/2402.14289|[]
2024-02-23|Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model|use a intermediate teacher model between foundatioin model and student|https://arxiv.org/abs/2402.14035|[]
2024-02-26|Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding|as title|https://arxiv.org/abs/2402.15300|[]
2024-02-26|Attention-Guided Masked Autoencoders For Learning Image Representations|MAE but mask patches with high attention |https://arxiv.org/abs/2402.15172|[]
2024-02-26|Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?|combining multiple sets of LoRA|https://arxiv.org/abs/2402.15414|[]
2024-02-26|CommVQA: Situating Visual Question Answering in Communicative Contexts|VQA with communicative context|https://arxiv.org/abs/2402.15002|[]
2024-02-27|Gradient-Guided Modality Decoupling for Missing-Modality Robustness|"Missing modality when using multi-modal model, remove the conflict in gradients of different modalities"|https://arxiv.org/abs/2402.16318|[]
2024-02-27|One-stage Prompt-based Continual Learning|as title|https://arxiv.org/abs/2402.16189|[]
2024-02-27|LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding|deal with long video by making model sample relevant frames|https://arxiv.org/abs/2402.16050|[]
2024-02-27|Semi-supervised Open-World Object Detection|detection with unknown class and unlabel data|https://arxiv.org/abs/2402.16013|[]
2024-02-27|Multimodal Instruction Tuning with Conditional Mixture of LoRA|like pathway or mixture of expert using LoRA|https://arxiv.org/abs/2402.15896|[]
2024-02-27|"TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages"|as title|https://arxiv.org/abs/2402.16021|[]
2024-02-28|VRP-SAM: SAM with Visual Reference Prompt|make SAM segment according to reference (object from other pictures)|https://arxiv.org/abs/2402.17726|[]
2024-02-28|Interactive Multi-Head Self-Attention with Linear Complexity|efficient MHSA|https://arxiv.org/abs/2402.17507|[]
2024-02-28|LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning|prompt tuning while consider early block features having low level information|https://arxiv.org/abs/2402.17406|[]
2024-02-28|Scaling Supervised Local Learning with Augmented Auxiliary Networks|training with local gradient-isolated network rather than backpropagation of global loss|https://arxiv.org/abs/2402.17318|[]
2024-02-28|m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers|distillation of modular network|https://arxiv.org/abs/2402.16918|[]
2024-02-29|Gradient Reweighting: Towards Imbalanced Class-Incremental Learning|as title|https://arxiv.org/abs/2402.18528|[]
2024-02-29|Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation|weakly supervised segmentation using patch pseudo lable and contrastive learning|https://arxiv.org/abs/2402.18467|[]
2024-02-29|Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization|use text prompt to capture object feature and help generalization|https://arxiv.org/abs/2402.18447|[]
2024-02-29|Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport|try to learn a prototype of each class to do retrieval (?)|https://arxiv.org/abs/2402.18411|[]
2024-02-29|Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization|use additional masking model that take downstream validation loss into consideration|https://arxiv.org/abs/2402.18128|[]
2024-02-29|UniVS: Unified and Universal Video Segmentation with Prompts as Queries|as title|https://arxiv.org/abs/2402.18115|[]
2024-02-29|Polos: Multimodal Metric Learning from Human Feedback for Image Captioning|"new dataset with human feed back, learned based metric for captioning"|https://arxiv.org/abs/2402.18091|[]
2024-02-29|Generalizable Two-Branch Framework for Image Class-Incremental Learning|side network for continual learning|https://arxiv.org/abs/2402.18086|[]
2024-02-29|Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation|weakly supervised segmentation with refined patch-level pseudo label|https://arxiv.org/abs/2402.17891|[]
2024-02-29|REPrune: Channel Pruning via Kernel Representative Selection|"pruning for CNN according to ""maximum cluster coverage problem"""|https://arxiv.org/abs/2402.17862|[]
2024-02-29|Classes Are Not Equal: An Empirical Study on Image Recognition Fairness|as title|https://arxiv.org/abs/2402.18133|[]
2024-03-01|Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models|DeepMind's new attempt for efficient LM|https://arxiv.org/abs/2402.19427|[]
2024-03-01|Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning|about continual learning and catastrophic forgetting in PEFT|https://arxiv.org/abs/2402.18865|[]
2024-03-01|Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains|fixed classifier according to Equiangular Tight Frame simplex|https://arxiv.org/abs/2402.18614|[]
2024-03-01|FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning|System that can do inference and parameter-efficient finetuning requests in the same iteration|https://arxiv.org/abs/2402.18789|[]
2024-03-01|Learning to Compress Prompt in Natural Language Formats|as title|https://arxiv.org/abs/2402.18700|[]
2024-03-01|Simple linear attention language models balance the recall-throughput tradeoff|linear and sliding-window attention for efficient LLM|https://arxiv.org/abs/2402.18668|[]
2024-02-29|SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization|theoretically understand pruning|https://arxiv.org/abs/2402.17902|[]
2024-02-29|DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation|as title|https://arxiv.org/abs/2402.17812|[]
2024-02-29|HOP to the Next Tasks and Domains for Continual Learning in NLP|"specialized MLP for each problem, use statistical moments to capture information"|https://arxiv.org/abs/2402.18449|[]
2024-02-29|Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation|generate NLP dataset using unannotated text and task attributes|https://arxiv.org/abs/2402.18334|[]
2024-03-01|Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers|as title|https://arxiv.org/abs/2402.19479|[]
2024-03-01|PEM: Prototype-based Efficient MaskFormer for Image Segmentation|"restrict attention region for efficiency, using feature pyramid"|https://arxiv.org/abs/2402.19422|[]
2024-03-01|VIXEN: Visual Text Comparison Network for Image Difference Captioning|"summarize difference between 2 images, use synthesized data and GPT4 to do augmentation"|https://arxiv.org/abs/2402.19119|[]
2024-03-01|VideoMAC: Video Masked Autoencoders Meet ConvNets|video MAE using ConvNet which reduces required data size|https://arxiv.org/abs/2402.19082|[]
2024-03-01|Debiased Novel Category Discovering and Localization|detection taking unknown objects into account|https://arxiv.org/abs/2402.18821|[]
2024-03-01|Motion Guided Token Compression for Efficient Masked Video Modeling|use variance between patches to detect motion and mask still patches|https://arxiv.org/abs/2402.18577|[]
2024-03-01|"CAMixerSR: Only Details Need More ""Attention"""|predict which patch need to be processed by attention|https://arxiv.org/abs/2402.19289|[]
2024-03-01|Rethinking Multi-domain Generalization with A General Learning Objective|design objective that learn domain-independent feature|https://arxiv.org/abs/2402.18853|[]
2024-03-04|Can Transformers Capture Spatial Relations between Objects?|long range attention to improve spatial relation capability|https://arxiv.org/abs/2403.00729|[]
2024-03-04|Tri-Modal Motion Retrieval by Learning a Joint Embedding Space|"text, video, motion retrieval"|https://arxiv.org/abs/2403.00691|[]
2024-03-04|VisionLLaMA: A Unified LLaMA Interface for Vision Tasks|"propse ViT architecture similar to LLaMA (e.g., with 2DRoPE)"|https://arxiv.org/abs/2403.00522|[]
2024-03-04|Learning and Leveraging World Models in Visual Representation Learning|"using world model, a techinique of RL to do representation learning"|https://arxiv.org/abs/2403.00504|[]
2024-03-04|TempCompass: Do Video LLMs Really Understand Videos?|video LLM benchmark|https://arxiv.org/abs/2403.00476|[]
2024-03-04|Invariant Test-Time Adaptation for Vision-Language Model Generalization|"test time adaptation, using SAM to find foreground and background, and using mask to prevent shortcut"|https://arxiv.org/abs/2403.00376|[]
2024-03-04|Task Indicating Transformer for Task-conditional Dense Predictions|segmentation for different purpose using Task Gate Decoder|https://arxiv.org/abs/2403.00327|[]
2024-03-04|Multi-modal Attribute Prompting for Vision-Language Models|increase CLIP generalizability by alignment visual and text attribute|https://arxiv.org/abs/2403.00219|[]
2024-03-04|Few-Shot Relation Extraction with Hybrid Visual Evidence|detect objects apearing in caption and find their relation|https://arxiv.org/abs/2403.00724|[]
2024-03-04|Rethinking The Uniformity Metric in Self-Supervised Learning|objectives that improve SSL and prevent dimension collapse theoretically|https://arxiv.org/abs/2403.00642|[]
2024-03-05|RegionGPT: Towards Region Understanding Vision Language Model|using patch merging and spatial region embedding|https://arxiv.org/abs/2403.02330|[]
2024-03-05|Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training|observing result when mask RoI to reduce hallucination|https://arxiv.org/abs/2403.02325|[]
2024-03-05|Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures|as title|https://arxiv.org/abs/2403.02308|[]
2024-03-05|Non-autoregressive Sequence-to-Sequence Vision-Language Models|predetermined output lengths of learnable tokens; input info as KV in cross attention|https://arxiv.org/abs/2403.02249|[]
2024-03-05|Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations|data and benchmark for multi-person interaction understanding|https://arxiv.org/abs/2403.02090|[]
2024-03-05|VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT|VTG using image captioning and finding similarity between query and caption|https://arxiv.org/abs/2403.02076|[]
2024-03-05|A Generative Approach for Wikipedia-Scale Visual Entity Recognition|"classification with 6M class, assign ""code"" to each class and use generative model to predict code"|https://arxiv.org/abs/2403.02041|[]
2024-03-05|Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification|general image augmentation aiming for robustness|https://arxiv.org/abs/2403.01944|[]
2024-03-05|xT: Nested Tokenization for Larger Context in Large Images|"divide image into sub-region before patchify, better local & global feature"|https://arxiv.org/abs/2403.01915|[]
2024-03-05|AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation|regularized by reconstruction labeled feature from pseudo labeled feature|https://arxiv.org/abs/2403.01818|[]
2024-03-05|Training-Free Pretrained Model Merging|mergine according activatioin and weight similarity|https://arxiv.org/abs/2403.01753|[]
2024-03-05|Zero-shot Generalizable Incremental Learning for Vision-Language Object Detection|new task configuration|https://arxiv.org/abs/2403.01680|[]
2024-03-05|Logit Standardization in Knowledge Distillation|design on temperature to adjust target of student networks|https://arxiv.org/abs/2403.01427|[]
2024-03-05|Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning|use text encoder of CLIP and prompting LLM to replace image data|https://arxiv.org/abs/2403.01209|[]
2024-03-06|Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization|use two teachers to validate the pseudo label is reliable|https://arxiv.org/abs/2403.03145|[]
2024-03-06|Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization|use two model to validate the pseudo label is reliable|https://arxiv.org/abs/2403.03095|[]
2024-03-06|Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models|two pathway for different resolution of images|https://arxiv.org/abs/2403.03003|[]
2024-03-06|MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer|use cross-modal similarity as clue to pruning tokens in each layer|https://arxiv.org/abs/2403.02991|[]
2024-03-06|Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception|concatenating MLLM and a segmentation decoder|https://arxiv.org/abs/2403.02969|[]
2024-03-06|Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples|"hard negative mining by swapping predefine keywords (e.g., color, size...)"|https://arxiv.org/abs/2403.02875|[]
2024-03-06|PromptKD: Unsupervised Prompt Distillation for Vision-Language Models|distillation into learnable prompt token (like distillation + prompt tuning)|https://arxiv.org/abs/2403.02781|[]
2024-03-06|DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization|as title|https://arxiv.org/abs/2403.02714|[]
2024-03-07|MeaCap: Memory-Augmented Zero-shot Image Captioning|using CLIP to retrieve related text and refine captions|https://arxiv.org/abs/2403.03715|[]
2024-03-07|Enhancing Vision-Language Pre-training with Rich Supervisions|seeking supervision website source code|https://arxiv.org/abs/2403.03346|[]
2024-03-08|Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation|try to save distribution of previous task by saving hard negative using contrastive loss|https://arxiv.org/abs/2403.04599|[]
2024-03-08|CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?|"explore (mainly gender) bias in CLIP, define several metrics for bias"|https://arxiv.org/abs/2403.04547|[]
2024-03-08|Aligners: Decoupling LLMs and Alignment|"avoid training whole LLM, train additional module for multiple LLM"|https://arxiv.org/abs/2403.04224|[]
2024-03-07|GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection|memory efficient whole model training by decompose gradient|https://arxiv.org/abs/2403.03507|[]
2024-03-07|The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models|study on sub-network with same ID performance but better OOD performance|https://arxiv.org/abs/2403.03942|[]
2024-03-07|Learning to Decode Collaboratively with Multiple Language Models|ensemble of LLMs on token likelihodd level|https://arxiv.org/abs/2403.03870|[]
2024-02-01|Repeat After Me: Transformers are Better than State Space Models at Copying|argue that Transformer may be better than SSM on copying and retrieving|https://arxiv.org/abs/2402.01032|[]
2024-03-08|SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM|VQA dataset with fine-grain objects and prposed baseline (using augmentation)|https://arxiv.org/abs/2403.04735|[]
2024-03-08|Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level|kernel fusion of neighborhood attention|https://arxiv.org/abs/2403.04690|[]
2024-03-08|CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios|"audio visual LM on audio visual QA, audio visual instruction dataset"|https://arxiv.org/abs/2403.04640|[]
2024-03-08|LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking|PEFT combining LoRA and parameter sharing|https://arxiv.org/abs/2403.04303|[]
2024-03-08|LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition|improve contrastive learning by masking|https://arxiv.org/abs/2403.04066|[]
2024-03-08|A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition|"propose Modality Bias Hypothesis, using knowledge distillation to make model robust to missing frame and modality"|https://arxiv.org/abs/2403.04245|[]
2024-03-11|"Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos"|use image caption to obtain pseudo label for DA|https://arxiv.org/abs/2403.05535|[]
2024-03-11|Attention-guided Feature Distillation for Semantic Segmentation|as title|https://arxiv.org/abs/2403.05451|[]
2024-03-11|VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model|use VLM to refine or valide pseudo label|https://arxiv.org/abs/2403.05346|[]
2024-03-11|Debiasing Large Visual Language Models|use meaningless image to calibrate bias from LLM|https://arxiv.org/abs/2403.05262|[]
2024-03-11|Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval|use intra-modality similarity as supervision of inter-modality similarity|https://arxiv.org/abs/2403.05261|[]
2024-03-11|Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval|rematch mismatched pairs using optimal transport to calibrate data|https://arxiv.org/abs/2403.05105|[]
2024-03-11|Agile Multi-Source-Free Domain Adaptation|parameter and throughput efficient|https://arxiv.org/abs/2403.05062|[]
2024-03-11|Poly-View Contrastive Learning|"contrastive learning with more than paired view, reduce required batch size"|https://arxiv.org/abs/2403.05490|[]
2024-03-11|Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization|"learn domain-invariant prototype to do pseudo labeling, utilize labeled paired to make model robust to pseudo label"|https://arxiv.org/abs/2403.05209|[]
2024-03-11|Denoising Autoregressive Representation Learning|Causal transformer for images to do next patch prediction (denoising)|https://arxiv.org/abs/2403.05196|[]
2024-03-12|Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling|"Prompt tuning for video tasks, insert tokens in KV rather than input sequence"|https://arxiv.org/abs/2403.06978|[]
2024-03-12|VideoMamba: State Space Model for Efficient Video Understanding|"Mamba for video, MAE with CLIP as target"|https://arxiv.org/abs/2403.06977|[]
2024-03-12|Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation|further disentangle CLIP feature using orthogonal loss|https://arxiv.org/abs/2403.06946|[]
2024-03-12|LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations|use original image as anchor to prevent images after augmentation become negative pair|https://arxiv.org/abs/2403.06813|[]
2024-03-12|An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models|token pruning for VLM acceleration|https://arxiv.org/abs/2403.06764|[]
2024-03-12|Answering Diverse Questions via Text Attached with Key Audio-Visual Clues|"audio visual QA, utilize knowledge distiall for alignment"|https://arxiv.org/abs/2403.06679|[]
2024-03-12|OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation|"unsupervised segmentation using cluster and optimal transport, no pseudo label is used"|https://arxiv.org/abs/2403.06546|[]
2024-03-12|VkD:  Improving Knowledge Distillation using Orthogonal Projections|as title|https://arxiv.org/abs/2403.06213|[]
2024-03-12|RESTORE: Towards Feature Shift for Vision-Language Prompt Learning|text and image alignment of CLIP for better novel class generalizability|https://arxiv.org/abs/2403.06136|[]
2024-03-12|ClickVOS: Click Video Object Segmentation|segmentation mask translation starting with one point per object (during inference)|https://arxiv.org/abs/2403.06130|[]
2024-03-12|In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model|test time adaptation using prompt tuning with CLIP|https://arxiv.org/abs/2403.06126|[]
2024-03-12|Multisize Dataset Condensation|reduce dataset size for efficient training|https://arxiv.org/abs/2403.06075|[]
2024-03-12|Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning|test time adaptation by prompt tuning and estimation of distribution|https://arxiv.org/abs/2403.06059|[]
2024-03-12|CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot Learning|attribute and object disentangle using cycle consistency|https://arxiv.org/abs/2403.05924|[]
2024-03-12|LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content|prompt GPT4V as supervision for long-tail data|https://arxiv.org/abs/2403.05854|[]
2024-03-12|Augmentations vs Algorithms: What Works in Self-Supervised Learning|"argue that augmentation is much important than algorthm in contrastive learning, propose a unified framework to experiment"|https://arxiv.org/abs/2403.05726|[]
2024-03-13|Beyond Text: Frozen Large Language Models in Visual Signal Comprehension|translate images to natural language|https://arxiv.org/abs/2403.07874|[]
2024-03-13|Distilling the Knowledge in Data Pruning|data condensation using knowledge distillation|https://arxiv.org/abs/2403.07854|[]
2024-03-13|MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric|pruning for VLM|https://arxiv.org/abs/2403.07839|[]
2024-03-13|Multi-modal Auto-regressive Modeling via Visual Words|map image patches to distribution over dictionary and treat it as text|https://arxiv.org/abs/2403.07720|[]
2024-03-13|Masked AutoDecoder is Effective Multi-Task Vision Generalist|use causal decoder to make vision model to do multiple tasks|https://arxiv.org/abs/2403.07692|[]
2024-03-13|Unified Source-Free Domain Adaptation|problem framework of domain adaptation|https://arxiv.org/abs/2403.07601|[]
2024-03-13|Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors|test time adaptation by observing results before and after augmentation|https://arxiv.org/abs/2403.07366|[]
2024-03-13|Open-World Semantic Segmentation Including Class Similarity|use post-process to cluster unknow class|https://arxiv.org/abs/2403.07532|[]
2024-03-13|MoAI: Mixture of All Intelligence for Large Language and Vision Models|make use of multiple visual feature|https://arxiv.org/abs/2403.07508|[]
2024-03-13|Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models|decouple visual capability into task-agnostic and task-specific|https://arxiv.org/abs/2403.07304|[]
2024-03-13|IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers|quantization can be used for training and inference|https://arxiv.org/abs/2403.07339|[]
2024-03-14|DAM: Dynamic Adapter Merging for Continual Video QA Learning|Merge domain specific adapter to do continual VQA learning|https://arxiv.org/abs/2403.08755|[]
2024-03-14|Consistent Prompting for Rehearsal-Free Continual Learning|"multiple learable prompt and classifier for different domain, use cross domain prediction smoothness as regularization and auxiliary loss"|https://arxiv.org/abs/2403.08568|[]
2024-03-14|Cross-modality debiasing: using language to mitigate sub-population shifts in imaging|as title|https://arxiv.org/abs/2403.07888|[]
2024-03-14|Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection|increase pretraining speed by removing patches based on similarity to text|https://arxiv.org/abs/2403.07883|[]
2022-05-24|Large Language Models are Zero-Shot Reasoners||https://arxiv.org/abs/2205.11916|[]
2022-05-21|Least-to-Most Prompting Enables Complex Reasoning in Large Language Models||https://arxiv.org/abs/2205.10625|[]
2021-06-30|Attention Bottlenecks for Multimodal Fusion||https://arxiv.org/abs/2107.00135|[]
2019-04-16|Co-Separating Sounds of Visual Objects||https://arxiv.org/abs/1904.07750|[]
2020-11-03|Learning Representations from Audio-Visual Spatial Alignment||https://arxiv.org/abs/2011.01819|[]
2023-03-30|Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment||https://arxiv.org/abs/2303.17490|[]
2022-03-30|MAE-AST: Masked Autoencoding Audio Spectrogram Transformer||https://arxiv.org/abs/2203.16691|[]
2022-07-13|Masked Autoencoders that Listen||https://arxiv.org/abs/2207.06405|[]
2017-11-30|A Closer Look at Spatiotemporal Convolutions for Action Recognition||https://arxiv.org/abs/1711.11248|[]
2021-03-10|VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples||https://arxiv.org/abs/2103.05905|[]
2020-10-19|Self-supervised Co-training for Video Representation Learning||https://arxiv.org/abs/2010.09709|[]
2021-04-22|Multiscale Vision Transformers||https://arxiv.org/abs/2104.11227|[]
2022-03-03|BEVT: BERT Pretraining of Video Transformers||https://arxiv.org/abs/2112.01529|[]
2022-04-06|ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound||https://arxiv.org/abs/2204.02874|[]
2023-04-10|Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition||https://arxiv.org/abs/2304.04704|[]
2022-04-14|Masked Siamese Networks for Label-Efficient Learning||https://arxiv.org/abs/2204.07141|[]
2022-10-17|Token Merging: Your ViT But Faster||https://arxiv.org/abs/2210.09461|[]
2021-11-03|VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts||https://arxiv.org/abs/2111.02358|[]
2022-05-04|CoCa: Contrastive Captioners are Image-Text Foundation Models||https://arxiv.org/abs/2205.01917|[]
2023-06-01|StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners||https://arxiv.org/abs/2306.00984|[]
2022-06-16|MixGen: A New Multi-Modal Data Augmentation||https://arxiv.org/abs/2206.08358|[]
2023-04-24|A Cookbook of Self-Supervised Learning||https://arxiv.org/abs/2304.12210|[]
2022-05-21|Self-Supervised Speech Representation Learning: A Review||https://arxiv.org/abs/2205.10643|[]
2022-05-18|Masked Autoencoders As Spatiotemporal Learners||https://arxiv.org/abs/2205.09113|[]
2021-06-15|BEiT: BERT Pre-Training of Image Transformers||https://arxiv.org/abs/2106.08254|[]
2020-02-13|Self-supervised learning for audio-visual speaker diarization||https://arxiv.org/abs/2002.05314|[]
2019-01-27|Augment your batch: better training with larger batches||https://arxiv.org/abs/1901.09335|[]
2019-06-19|XLNet: Generalized Autoregressive Pretraining for Language Understanding||https://arxiv.org/abs/1906.08237|[]
2017-11-14|Decoupled Weight Decay Regularization||https://arxiv.org/abs/1711.05101|[]
2021-12-16|Masked Feature Prediction for Self-Supervised Visual Pre-Training||https://arxiv.org/abs/2112.09133|[]
2024-03-15|OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning|use prompt tuning to tackle various additional information|https://arxiv.org/abs/2403.09634|[]
2024-03-15|Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding|evaluation of video Mamba|https://arxiv.org/abs/2403.09626|[]
2024-03-15|"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"|30B LM from Apple|https://arxiv.org/abs/2403.09611|[]
2024-03-15|GiT: Towards Generalist Vision Transformer through Universal Language Interface|Multimodal LM without concatenating encoder and LLM|https://arxiv.org/abs/2403.09394|[]
2024-03-15|LocalMamba: Visual State Space Model with Windowed Selective Scan|as title|https://arxiv.org/abs/2403.09338|[]
2024-03-15|Are Vision Language Models Texture or Shape Biased and Can We Steer Them?|compare bias in vision encoder and corresponding VLM|https://arxiv.org/abs/2403.09193|[]
2024-03-15|PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation|PEFT + token pruning to achieve inference and training efficiency|https://arxiv.org/abs/2403.09192|[]
2024-03-18|Frozen Feature Augmentation for Few-Shot Image Classification|emperical study on augmentation of few shot linear probing|https://arxiv.org/abs/2403.10519|[]
2024-03-18|VideoAgent: Long-form Video Understanding with Large Language Model as Agent|"use CLIP to retrieve, use VLM to validate the candidate"|https://arxiv.org/abs/2403.10517|[]
2024-03-18|CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning|remove bias from imbalanced data making prediction on no-pattern images neutral|https://arxiv.org/abs/2403.10391|[]
2024-03-18|Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models|concatenate YOLO GPT4V SAM to do few shot dense prediction|https://arxiv.org/abs/2403.10287|[]
2024-03-18|HawkEye: Training Video-Text LLMs for Grounding Text in Videos|large scale video text dataset|https://arxiv.org/abs/2403.10228|[]
2024-03-18|Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt|disentangle learable prompts into task invariant (initialized from base classes) and task specific|https://arxiv.org/abs/2403.09857|[]
2024-03-18|Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks|"when fine-tune downsstream tasks, adaptive update class distribution, minimize intra and maximize inter distancce"|https://arxiv.org/abs/2403.10097|[]
2024-03-18|Uni-SMART: Universal Science Multimodal Analysis and Research Transformer|"Multimodal LM that can process table, chart, chemical reactions, ..."|https://arxiv.org/abs/2403.10301|[]
2024-03-18|EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba|as title|https://arxiv.org/abs/2403.09977|[]
2024-03-18|Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers|as title|https://arxiv.org/abs/2403.10030|[]
2024-03-18|Knowledge Condensation and Reasoning for Knowledge-based VQA|"VQA with external knowledge passage, summary knowledge before feed to models"|https://arxiv.org/abs/2403.10037|[]
2024-03-19|Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning|"New adapters for new class, and construct prototype to prevent forgetting"|https://arxiv.org/abs/2403.12030|[]
2024-03-19|Align and Distill: Unifying and Improving Domain Adaptive Object Detection|new benchmark and baseline for DA object detection|https://arxiv.org/abs/2403.12029|[]
2024-03-19|"FlexCap: Generating Rich, Localized, and Flexible Captions in Images"|"deepmind's length, bbox conditioned captioning model using new dataset"|https://arxiv.org/abs/2403.12026|[]
2024-03-19|SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules|try to summarize and unify mechanisms of LoRA variants|https://arxiv.org/abs/2403.11887|[]
2024-03-19|Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation|PEFT + token pruning to achieve inference and training efficiency|https://arxiv.org/abs/2403.11808|[]
2024-03-19|Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs|APE to enhance visual recognition|https://arxiv.org/abs/2403.11755|[]
2024-03-19|Towards Generalizing to Unseen Domains with Few Labels|Semi-supervise domain adaptation|https://arxiv.org/abs/2403.11674|[]
2024-03-19|Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters|use MoE and routing mechanism to deal with ID and OOD data|https://arxiv.org/abs/2403.11549|[]
2024-03-19|Semantic Prompting with Image-Token for Continual Learning|extract task agnostic feature to improve generalizibility|https://arxiv.org/abs/2403.11537|[]
2024-03-19|Do CLIPs Always Generalize Better than ImageNet Models?|argue the robustness of CLIP|https://arxiv.org/abs/2403.11497|[]
2024-03-19|Towards Generalizing to Unseen Domains with Few Labels|semi-supervise domain adaptation|https://arxiv.org/abs/2403.11674|[]
2024-03-19|VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding|as title|https://arxiv.org/abs/2403.11481|[]
2024-03-19|Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding|video grounding without timestamp label|https://arxiv.org/abs/2403.11463|[]
2024-03-19|Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration|"continual learning with missing modality, solving with PEFT"|https://arxiv.org/abs/2403.11373|[]
2024-03-19|SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant|Try to generate high quality QA to do instruction tuning|https://arxiv.org/abs/2403.11299|[]
2024-03-19|MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts|"besides static soft prompt, learn a module to generate dynamic soft prompt based on other modality"|https://arxiv.org/abs/2403.10568|[]
2024-03-19|Self-Supervised Quantization-Aware Knowledge Distillation|full precision teacher distill to low precision students|https://arxiv.org/abs/2403.11106|[]
2024-03-19|Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches|Comparing mapping images to LLM's embedding space and to natural language captions|https://arxiv.org/abs/2403.11317|[]
2024-03-19|PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation|use gradient to determine which layer to train during test time adaptation|https://arxiv.org/abs/2403.10650|[]
2024-03-19|: Source-free Domain Adaptation Through the Lens of Data Augmentation|"do cluster in embedding space, regularization is inspired by augmentation"|https://arxiv.org/abs/2403.10834|[]
2024-03-19|Rethinking Multi-view Representation Learning via Distilled Disentangling|"learn view-independent feature by cross-view MAE, then learn view-specific feature by disentangle with learnt view-independent feature"|https://arxiv.org/abs/2403.10897|[]
2024-03-19|Task-Aware Low-Rank Adaptation of Segment Anything Model|decompose LoRA into three component (one additional for different tasks)|https://arxiv.org/abs/2403.10971|[]
2024-03-19|Audio-Visual Segmentation via Unlabeled Frame Exploitation|"Self-training for AV segmentation, use optical flow for frames near labeled frames, teacher model for distant unlabeled frames"|https://arxiv.org/abs/2403.11074|[]
2024-03-19|Self-supervised co-salient object detection via feature correspondence at multiple scales|"segmentation co-occurance objects in a set of images in SSL manner, use attention weight to get object masks, and find co-occurance object using feature similarity"|https://arxiv.org/abs/2403.11107|[]
2024-03-19|Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence|modified self- and cross attention to do dense matching|https://arxiv.org/abs/2403.11120|[]
2024-03-19|DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation|two networks that generate pseudo labels for each other|https://arxiv.org/abs/2403.11184|[]
2024-03-19|TAG: Guidance-free Open-Vocabulary Semantic Segmentation|"use DINO to do segmentation, us CLIP to retrieve text and mergin the segmentation according to similarity to text"|https://arxiv.org/abs/2403.11197|[]
2024-03-19|Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias|"DA setting where target domain may not cover all class of source domain, and may contain new classes"|https://arxiv.org/abs/2403.11234|[]
2024-03-20|Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models|"let VLM output RoI first, and then ask the question again with the RoI image"|https://arxiv.org/abs/2403.12966|[]
2024-03-20|Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models|"PEFT, try to predict what the image is and is not simultaneously"|https://arxiv.org/abs/2403.12964|[]
2024-03-20|Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments|"observing top-2 prediction to identify confiusing pair, then do correction"|https://arxiv.org/abs/2403.12883|[]
2024-03-20|Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models|Test time adaptation by estimating shift between training and testing data prototype generated by text encoder|https://arxiv.org/abs/2403.12952|[]
2024-03-20|Confidence Self-Calibration for Multi-Label Class-Incremental Learning|prevent overfitting on new class by entropy regularization|https://arxiv.org/abs/2403.12559|[]
2024-03-20|UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All|Use text embedding center as anchor rather than image as in ImageBind|https://arxiv.org/abs/2403.12532|[]
2024-03-20|Towards Multimodal In-Context Learning for Vision & Language Models|"Verify off-the-shelf VLM can't do in context learning, using fine-tuning to enable them"|https://arxiv.org/abs/2403.12736|[]
2024-03-20|DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM|"overlapping information (e.g., ruler) on image can improve detection ability of GPT 4V"|https://arxiv.org/abs/2403.12488|[]
2024-03-20|CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation|as title|https://arxiv.org/abs/2403.12455|[]
2024-03-20|Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity|"using less data to train CLIP, condensation according to covariance"|https://arxiv.org/abs/2403.12267|[]
2024-03-20|Non-negative Contrastive Learning|non-negative constraint on feature space make feature sparse and interpretable|https://arxiv.org/abs/2403.12459|[]
2024-03-20|Do Generated Data Always Help Contrastive Learning?|study on relation between synthesized data and data augmentation|https://arxiv.org/abs/2403.12448|[]
2024-03-21|On Pretraining Data Diversity for Self-Supervised Learning|"although diversity may improve performance, problem of domain shift is more crucial"|https://arxiv.org/abs/2403.13808|[]
2024-03-21|SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning|"clustering unlabled data, refine the representation by contrastive loss"|https://arxiv.org/abs/2403.13684|[]
2024-03-21|VL-Mamba: Exploring State Space Models for Multimodal Learning|"replace LLaMA with Mamba, explore scan mechanism"|https://arxiv.org/abs/2403.13600|[]
2024-03-21|What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models|"use GPT4V to generate counterfactual keywords, to preven hallucination"|https://arxiv.org/abs/2403.13513|[]
2024-03-21|Scale Decoupled Distillation|use multi-scale pooling to distill fine-grained logits rather than one global logit|https://arxiv.org/abs/2403.13512|[]
2024-03-21|Improved Baselines for Data-efficient Perceptual Augmentation of LLMs|"evaluate interfaces of MLM, propose data efficient interfance"|https://arxiv.org/abs/2403.13499|[]
2024-03-21|Counting Network for Learning from Majority Label|label is given at bag-level rather than instance level|https://arxiv.org/abs/2403.13370|[]
2024-03-21|vid-TLDR: Training Free Token merging for Light-weight Video Transformer|Merge tokens according to attention map in video transformer|https://arxiv.org/abs/2403.13347|[]
2024-03-21|PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns|benchmark of abstract VQA|https://arxiv.org/abs/2403.13315|[]
2024-03-21|Rotary Position Embedding for Vision Transformer|RoPE can benefit ViT especially on high resolution images|https://arxiv.org/abs/2403.13298|[]
2024-03-21|SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models|Use self-consistency (location and description) and reinforcement learning to improve detection|https://arxiv.org/abs/2403.13263|[]
2024-03-21|When Do We Not Need Larger Vision Models?|argue that scaling resolution may surpass scaling model size|https://arxiv.org/abs/2403.13043|[]
2024-03-21|BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning|balance feature distribution using contrastive learning|https://arxiv.org/abs/2403.12986|[]
2024-03-21|Bridge the Modality and Capacity Gaps in Vision-Language Model Selection|try to select best VLM base on the label set (text) of target dataset|https://arxiv.org/abs/2403.13797|[]
2024-03-21|HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models|use module generated by HyperNetwork to project visual feature to text-like token|https://arxiv.org/abs/2403.13447|[]
2024-03-21|A Unified and General Framework for Continual Learning|"unified framework of regularization-, Bayesian-, memory-based methods, propose refresh learning (unlearn then relearn)"|https://arxiv.org/abs/2403.13249|[]
2024-03-21|RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition|"use CLIP to retrieve, use VLM to ranking, improve zero-shot ability"|https://arxiv.org/abs/2403.13805|[]
2024-03-21|Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments|"study on relation between bbox stability and accuracy, evaluate detector without ground truth"|https://arxiv.org/abs/2403.13803|[]
2024-03-22|Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification|"two new measure to quantify bias, use augmentation to reduce bias"|https://arxiv.org/abs/2403.13925|[]
2024-03-22|How to be fair? A study of label and selection bias|"theoretically understan bias, debias methods and their relation"|https://arxiv.org/abs/2403.14282|[]
2024-03-22|AI and Memory Wall|"show that in the modern architecture, DRAM rather than FLOPs is the bottleneck"|https://arxiv.org/abs/2403.14123|[]
2024-03-22|Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey|survey of PEFT|https://arxiv.org/abs/2403.14608|[]
2024-03-26|Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models|use optional memory to enhance (training free) few-shot adaptation|https://arxiv.org/abs/2403.17589|[]
2024-03-26|The Unreasonable Ineffectiveness of the Deeper Layers|emperical study on standard layer pruning + recovering strategy|https://arxiv.org/abs/2403.17887|[]
2024-03-26|Multi-Task Dense Prediction via Mixture of Low-Rank Experts|MoE (PEFT) for dense prediction|https://arxiv.org/abs/2403.17749|[]
2024-03-26|DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning|"continual learning but can not store previous data, use underfitting network with compensation to prevent forgettng"|https://arxiv.org/abs/2403.17503|[]
2024-03-25|If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions|"align LLM to CLIP, so that LLM can output high similarity description, and then analyze these description"|https://arxiv.org/abs/2403.16442|[]
2024-03-25|LLMs Are Few-Shot In-Context Low-Resource Language Learners|study on shortcoming of in-context learning on low-resourse domain|https://arxiv.org/abs/2403.16512|[]
2024-03-25|DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization|use text prompts to simulate unseen domain without access to images|https://arxiv.org/abs/2403.16697|[]
2024-03-25|Understanding Long Videos in One Multimodal Language Model Pass|use frame selection to help VLM process long video|https://arxiv.org/abs/2403.16998|[]
2024-03-25|DreamLIP: Language-Image Pre-training with Long Captions|train CLIP with longer and multiple captions|https://arxiv.org/abs/2403.17007|[]
2024-03-25|Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models|make LLM able to retrieve multimodal data|https://arxiv.org/abs/2403.17359|[]
2024-03-25|One-Shot Domain Incremental Learning|as title|https://arxiv.org/abs/2403.16707|[]
2024-03-25|LLMs Are Few-Shot In-Context Low-Resource Language Learners|evaluate different in context learning format for low resourse language|https://arxiv.org/abs/2403.16512|[]
2024-03-25|Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models|visual chain of thought dataset and benchmark|https://arxiv.org/abs/2403.16999|[]
2024-03-27|Efficient Test-Time Adaptation of Vision-Language Models|"training free test time adaptation, maintain two queue that will affect prediction distribution"|https://arxiv.org/abs/2403.18293|[]
2024-03-27|Toward Interactive Regional Understanding in Vision-Large Language Models|user indicated region aware VLM|https://arxiv.org/abs/2403.18260|[]
2024-03-27|An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM|put frames in grid and treat it as 1 image|https://arxiv.org/abs/2403.18406|[]
2024-03-27|Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation|knowledge distillation between adapter modules|https://arxiv.org/abs/2403.18804|[]
2024-03-27|Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models|"multi-resolution, patch info mining visual encoder, concatenate LLM and text to image model for any to any VLM"|https://arxiv.org/abs/2403.18814|[]
2024-03-27|CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models|"use probablistic model (mean, std of normal distribution) for robust continual learning"|https://arxiv.org/abs/2403.19137|[]
2024-03-27|Dense Vision Transformer Compression with Few Samples|compression of ViT by dropping attention and shrinking MLP in few shot scenario|https://arxiv.org/abs/2403.18708|[]
2024-03-27|Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation|use Mamba for audio. divided audio in to chucks for local and global processing|https://arxiv.org/abs/2403.18257|[]
2024-03-27|Few-Shot Recalibration of Language Models|train recalibration model to evaluate confidence and precision curve of LLM|https://arxiv.org/abs/2403.18286|[]
2024-03-27|Towards Non-Exemplar Semi-Supervised Class-Incremental Learning|semi supervised prototype and contrastive learning to avoid forgetting|https://arxiv.org/abs/2403.18291|[]
2024-03-27|ViTAR: Vision Transformer with Any Resolution|token mergin and modified positional encoding for dynamic resolution|https://arxiv.org/abs/2403.18361|[]
2024-03-27|Debiasing Sentence Embedders through Contrastive Word Pairs|as title|https://arxiv.org/abs/2403.18555|[]
2024-03-27|SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens|generate draft token and verify to speed up LLM|https://arxiv.org/abs/2403.18647|[]
2024-03-27|Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding|use prompt to exacerbate hallucinated concept for easier detection|https://arxiv.org/abs/2403.18715|[]
2024-03-27|Projective Methods for Mitigating Gender Bias in Pre-trained Language Models|apply tranditional debiasing method to BERT|https://arxiv.org/abs/2403.18803|[]
2024-03-27|Measuring Political Bias in Large Language Models: What Is Said and How It Is Said|as title|https://arxiv.org/abs/2403.18932|[]
2024-03-27|LITA: Language Instructed Temporal-Localization Assistant|temporal reasoning localization dataset|https://arxiv.org/abs/2403.19046|[]
2024-03-27|Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach|"similar to LoRA, using SVD"|https://arxiv.org/abs/2403.19067|[]
2024-03-27|FACTOID: FACtual enTailment fOr hallucInation Detection|hallucination benchmark for LLM|https://arxiv.org/abs/2403.19113|[]
2024-03-27|Compressing Large Language Models by Streamlining the Unimportant Layer|layer pruning and layer replacement for LLM|https://arxiv.org/abs/2403.19135|[]
2024-03-28|Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment|only has ordered list of action when training, improve pseudo label efficiency|https://arxiv.org/abs/2403.19225|[['cs.CV'], ['CVPR']]
2024-03-28|CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection|tackle class imbalance problem in DA, learning class relation and do augmentation|https://arxiv.org/abs/2403.19278|[['cs.CV'], ['CVPR']]
2024-03-28|Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality|multimodal narratives caption for long video, augmentation to handl missing modality|https://arxiv.org/abs/2403.19221|[['cs.CV']]
2024-03-28|Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models|benchmark and baseline (decide whether to zoom in) for VLM to handle high resolution, text rich images|https://arxiv.org/abs/2403.19322|[['cs.CV']]
2024-03-28|Checkpoint Merging via Bayesian Optimization in LLM Pretraining|study on merging LLM|https://arxiv.org/abs/2403.19390|[['cs.CL']]
2024-03-28|Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition|dynamically use cross attention to prevent effect between low valence modality|https://arxiv.org/abs/2403.19554|[['cs.CV']]
2024-03-28|LocCa: Visual Pretraining with Location-aware Captioners|as title|https://arxiv.org/abs/2403.19596|[['cs.CV']]
2024-03-28|Siamese Vision Transformers are Scalable Audio-visual Learners|same ViT to process audio and visual, pretrained with contrastive learning and MAE|https://arxiv.org/abs/2403.19638|[['cs.CV']]
2024-03-28|MedBN: Robust Test-Time Adaptation against Malicious Test Samples|use statistics (batch norm) to handle manipulated data in test time adaptation|https://arxiv.org/abs/2403.19326|[['attack'], ['cs.LG'], ['CVPR']]
2024-03-29|Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer|use adapter in class incremental learning, learn prototypes for old classes|https://arxiv.org/abs/2403.19979|[['parameter-efficient'], ['cs.CV'], ['CVPR']]
2024-03-29|MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning|task specific LoRA, and task agnostic LoRA for multi task learning|https://arxiv.org/abs/2403.20320|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CV'], ['CVPR']]
2024-03-29|LayerNorm: A key component in parameter-efficient fine-tuning|argue that tuning layer norm is enough for downstream tasks|https://arxiv.org/abs/2403.20284|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL']]
2024-03-29|Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations|text to image model based on can't read negated description, try to make CLIP understand negation|https://arxiv.org/abs/2403.20312|[['Vision-Language'], ['cs.CV']]
2024-03-29|Are We on the Right Way for Evaluating Large Vision-Language Models?|benchmark for VLM, avoid data leakage and make sure necessity of visual input|https://arxiv.org/abs/2403.20330|[['Vision-Language'], ['cs.CV']]
2024-03-29|Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models|make VLM refuse to answer when the question is not solvable|https://arxiv.org/abs/2403.20331|[['Vision Language', 'VLM'], ['cs.CV']]
2024-03-29|Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning|increase complexity of data augmentation as training process goes|https://arxiv.org/abs/2403.20012|[['cs.CV'], ['ICLR']]
2024-03-29|Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion|as title|https://arxiv.org/abs/2403.20015|[['cs.CL'], ['ICLR']]
2024-03-29|ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning|continual learn for segmentation using prompt tuning |https://arxiv.org/abs/2403.20126|[['cs.CV'], ['CVPR']]
2024-03-29|Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions|benchmark and simple baseline for temporal corrupted|https://arxiv.org/abs/2403.20254|[['cs.CV'], ['CVPR']]
2024-03-29|Convolutional Prompting meets Language Models for Continual Learning|convolutional hyper network to achieve layer-wise input-aware prompt tuning for continual learning|https://arxiv.org/abs/2403.20317|[['cs.CV'], ['CVPR']]
2024-03-29|On Large Language Models' Hallucination with Regard to Known Facts|detect hallucination in LLM by tracking token probabilities|https://arxiv.org/abs/2403.20009|[['cs.CL']]
2024-03-29|FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues|swap features of same concept from different modalities and train on matching loss to enhance performance|https://arxiv.org/abs/2403.20026|[['cs.CV']]
2024-03-29|Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning|prompt and finetune to make LLM learn from error during CoT|https://arxiv.org/abs/2403.20046|[['cs.CL']]
2024-04-02|T-VSL: Text-Guided Visual Sound Source Localization in Mixtures|use text as guide to disentangle audio and visual for multi source sounding localization|https://arxiv.org/abs/2404.01751|[['audio-visual'], ['cs.CV'], ['CVPR']]
2024-04-02|BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition|audio visual SSL pretraining, dual encoders MAE|https://arxiv.org/abs/2404.02098|[['audio-visual'], ['cs.CV'], ['ICASSP']]
2024-04-02|ViTamin: Designing Scalable Vision Models in the Vision-Language Era|propose evaluation protocol for visual model (using CLIP method), propose CNN Transformer hybrid model|https://arxiv.org/abs/2404.02132|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|Iterated Learning Improves Compositionality in Large Vision-Language Models|finite code book and re-initialize model during CLIP training for better compositionality ("girl in white facing man in black" vs "girl in black facing man in white")|https://arxiv.org/abs/2404.02145|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-02|VLRM: Vision-Language Models act as Reward Models for Image Captioning|reinforcement learning for better captioning|https://arxiv.org/abs/2404.01911|[['Vision-Language'], ['cs.CV']]
2024-04-02|Weakly-supervised Audio Separation via Bi-modal Semantic Similarity|audio separation without access to single source audio, mix multi source audio and use contrastive loss|https://arxiv.org/abs/2404.01740|[['cs.SD'], ['ICLR']]
2024-04-02|Joint-Task Regularization for Partially Labeled Multi-Task Learning|as title|https://arxiv.org/abs/2404.01976|[['cs.CV'], ['CVPR']]
2024-04-02|Using Interpretation Methods for Model Enhancement|learn to make interpretation supervisedly, different ways to generate interpretation|https://arxiv.org/abs/2404.02068|[['cs.CL'], ['EMNLP']]
2024-04-02|Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners|as title|https://arxiv.org/abs/2404.02117|[['cs.CV'], ['CVPR']]
2024-04-04|ReFT: Representation Finetuning for Language Models|can't understand but it is PEFT + LLM|https://arxiv.org/abs/2404.03592|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-04-04|Would Deep Generative Models Amplify Bias in Future Models?|find that using sythesized data may not increase bias|https://arxiv.org/abs/2404.03242|[['social biases'], ['Diffusion'], ['cs.CV'], ['CVPR']]
2024-04-04|Scaling Up Video Summarization Pretraining with Large Language Models|new dataset for long video, using automated pipeline|https://arxiv.org/abs/2404.03398|[['cs.CV'], ['CVPR']]
2024-04-04|MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens|VLM, concatenate frame feature and subtitle feature for video understanding|https://arxiv.org/abs/2404.03413|[['cs.CV']]
2024-04-04|Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought|RL, knowledge distillation for small LM to deal with multi-hop QA|https://arxiv.org/abs/2404.03414|[['cs.CL']]
2024-04-16|Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models|align visual feature with text feature GPT4 prompt; leaarable prompt for CLIP classification|https://arxiv.org/abs/2404.10357|[['Vision-Language'], ['cs.CV']]
2024-04-16|Self-Supervised Visual Preference Alignment|use data augmentation to produce data for DPO|https://arxiv.org/abs/2404.10501|[['Vision-Language', 'VLM'], ['cs.CV']]
2024-04-16|Future Language Modeling from Temporal Document History|new task of future language modeling|https://arxiv.org/abs/2404.10297|[['cs.CL'], ['ICLR']]
2024-04-16|Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation|perturbating on feature space to simulate domain shift and use adapter to rectify the perturbation and deal with domain shift|https://arxiv.org/abs/2404.10322|[['cs.CV'], ['CVPR']]
2024-04-16|Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model|locate modules that is adept for specific tasks to prevent catastrophoic forgetting|https://arxiv.org/abs/2404.10306|[['cs.CL']]
2024-04-16|Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards|LLM self-correction to do DPO|https://arxiv.org/abs/2404.10346|[['cs.CL']]
2024-04-16|Dual Modalities of Text: Visual and Textual Generative Pre-training|render text as image and do next patch, token prediction simultaneously|https://arxiv.org/abs/2404.10710|[['cs.CL']]
2024-04-16|Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study|as title|https://arxiv.org/abs/2404.10719|[['cs.CL']]
2024-04-15|Bridging Vision and Language Spaces with Assignment Prediction|align visual feature with word embedding, don't need to modify LLM's weights|https://arxiv.org/abs/2404.09632|[['vision-language'], ['cs.CV'], ['ICLR']]
2024-04-15|Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering|use proxy model to sample neighborhood question, and use consistency on neighborhood questions to measure reliability|https://arxiv.org/abs/2404.10193|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-15|Leveraging Temporal Contextualization for Video Action Recognition|extract context tokens of videos as KV for better aggregation of information|https://arxiv.org/abs/2404.09490|[['vision-language'], ['cs.CV']]
2024-04-15|Conditional Prototype Rectification Prompt Learning|learn textual prototype in semi-supervised manners to mitigate overfitting|https://arxiv.org/abs/2404.09872|[['vision-language'], ['cs.CV']]
2024-04-15|Evolving Interpretable Visual Classifiers with Large Language Models|use elvolutionary search with LLM and CLIP to build interpretable image classifier|https://arxiv.org/abs/2404.09941|[['vision-language'], ['cs.CV']]
2024-04-15|LoRA Dropout as a Sparsity Regularizer for Overfitting Control|add noise to LoRA weights to increase parameter sparisty, prevent overfitting theoretically|https://arxiv.org/abs/2404.09610|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.LG']]
2024-04-15|The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning|semi supervised method for CLIP to tackle narrow distribution during few shots learning|https://arxiv.org/abs/2404.09778|[['cs.CV'], ['ECCV']]
2024-04-15|Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model|study on quality of instruction tuning data|https://arxiv.org/abs/2404.09717|[['cs.CL']]
2024-04-15|Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations|as title|https://arxiv.org/abs/2404.09785|[['cs.CL']]
2024-04-15|TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding|use captioning, detection, and cropping to enhance VQA performance|https://arxiv.org/abs/2404.09797|[['cs.CV']]
2024-04-15|Impact of Preference Noise on the Alignment Performance of Generative Language Models|study on quality of data to do alignment|https://arxiv.org/abs/2404.09824|[['cs.CL']]
2024-04-15|CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting|study on behaviors of LLM on different languages and cultures|https://arxiv.org/abs/2404.10199|[['cs.CL']]
2024-04-15|Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models|as title|https://arxiv.org/abs/2404.09529|[['cs.LG']]
2024-04-15|LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models|low rank pruning of LLM by observation of low rank structure of MHA|https://arxiv.org/abs/2404.09695|[['cs.LG']]
2024-04-25|AAPL: Adding Attributes to Prompt Learning for Vision-Language Models|reduce bias introduced by data augmentation in previous zero shot classification method|https://arxiv.org/abs/2404.16804|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-04-25|Training-Free Unsupervised Prompt for Vision-Language Models|training and label free method to boost zero-shot classification, generate high confident prototype and compute similarity to adjust logits|https://arxiv.org/abs/2404.16339|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-04-25|Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class|use multiple vectors to represent a class so that different attribute in a class can be taken into account|https://arxiv.org/abs/2404.16717|[['Vision-language', 'VLM'], ['cs.CV']]
2024-04-25|Multi-Scale Representations by Varying Window Attention for Semantic Segmentation|constrain attention region for better multi-scale feature extraction in segmentation|https://arxiv.org/abs/2404.16573|[['cs.CV'], ['ICLR']]
2024-04-25|EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning|instruction tuning for visual emotional recognition, new data and model|https://arxiv.org/abs/2404.16670|[['cs.CV'], ['CVPR']]
2024-04-25|List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs|data set for set-of-mark (use visual tags on image for MLM to refer to), a trick enhance model performance and interpretability|https://arxiv.org/abs/2404.16375|[['cs.CV']]
2024-04-25|Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities|feature disentangle during distillation to handle missing modality|https://arxiv.org/abs/2404.16456|[['cs.CV']]
2024-04-25|Exploring Internal Numeracy in Language Models: A Case Study on ALBERT|study on how LM handle embeddings of numbers (increase in one direction after PCA)|https://arxiv.org/abs/2404.16574|[['cs.CL']]
2024-04-25|Understanding Privacy Risks of Embeddings Induced by Large Language Models|study on potential of LLM reconstruction pre-training data|https://arxiv.org/abs/2404.16587|[['cs.CL']]
2024-04-25|TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning|mtrain model to generate code for calculation, token merging for high resolution images|https://arxiv.org/abs/2404.16635|[['cs.CV']]
2024-04-25|Zero-Shot Distillation for Image Encoders: How to Make Effective Use of Synthetic Data|L2 loss is better than contrastive loss for distillation|https://arxiv.org/abs/2404.16637|[['cs.CV']]
2024-04-25|Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4|GPT-4 are more biased to add information when asked to improve its answers when subtraction was more efficient|https://arxiv.org/abs/2404.16692|[['cs.CL']]
2024-04-25|Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding|layer dropout in LLM during training, early exit during inference and self-speculative decoding for speed up inference|https://arxiv.org/abs/2404.16710|[['cs.CL']]
2024-04-25|SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension|as title|https://arxiv.org/abs/2404.16790|[['cs.CV']]
2024-04-25|Make Your LLM Fully Utilize the Context|fine tune LLM on long context|https://arxiv.org/abs/2404.16811|[['cs.CL']]
2024-04-25|How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites|VLM with 6B ViT, dynamic resolution, English and Chinese training data|https://arxiv.org/abs/2404.16821|[['cs.CV']]
2024-04-25|T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients|as title|https://arxiv.org/abs/2404.16495|[['cs.LG']]
2024-04-25|VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations|argue that LMs have difficulties in distinguishing between text semantic variations|https://arxiv.org/abs/2404.16365|[['vision-language', 'VLMs'], ['face'], ['cs.CL']]
2024-04-24|FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication|study about effect of deduplication on social bias|https://arxiv.org/abs/2404.16123|[['Vision-Language'], ['social biases'], ['cs.CV'], ['CVPR']]
2024-04-24|What Makes Multimodal In-Context Learning Work?|argue the lack of exploration in multimodal in context learning|https://arxiv.org/abs/2404.15736|[['cs.CV'], ['CVPR']]
2024-04-24|MoDE: CLIP Data Experts via Clustering|train multiple CLIP on some clusters of data and ensemble them at inference time|https://arxiv.org/abs/2404.16030|[['cs.CV'], ['CVPR']]
2024-04-24|CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data|train CLIP by prediction synset (BCE loss), speed up 2.7x|https://arxiv.org/abs/2404.15653|[['cs.CV']]
2024-05-01|CLIPArTT: Light-weight Adaptation of CLIP to New Domains at Test Time|test time adaptation by aggregate top k prediction into a new prompt to generate pseudo label,  standardize benchmarks|https://arxiv.org/abs/2405.00754|[['vision-language', 'VLMs'], ['cs.CV']]
2024-05-01|AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts|adaptive select LoRA experts rather than select fixed top k|https://arxiv.org/abs/2405.00361|[['cs.CL']]
2024-05-01|Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval|interpolate between query image and text to obtain feature to do retrieval|https://arxiv.org/abs/2405.00571|[['cs.CV']]
2024-05-01|Are Models Biased on Text without Gender-related Language?|show that LLM is biased even if correlation between gender related words in the sentences are calibrated by pretrain data, ICLR 2024|https://arxiv.org/abs/2405.00588|[['cs.CL']]
2024-05-01|Learning to Compose: Improving Object Centric Learning by Injecting Compositionality|objectives to enable decode composite images via composite representation, improve object-centricc learning, ICLR 2024|https://arxiv.org/abs/2405.00646|[['cs.CV']]
2024-05-01|DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling|tune the model to predict multiple tokens at a time to speed up inference time|https://arxiv.org/abs/2405.00888|[['cs.CL']]
2024-05-01|LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets|model and data pruning at the same time|https://arxiv.org/abs/2405.00906|[['cs.CV']]
2024-05-01|LLaVA Finds Free Lunch: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs|tune LLaVA by predicting receiver behavior|https://arxiv.org/abs/2405.00942|[['cs.CV']]
2024-05-02|Learning Object States from Actions via Large Language Models|extract objects states from action in the caption using LLM|https://arxiv.org/abs/2405.01090|[['vision-language'], ['cs.CV']]
2024-05-02|MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors|resources efficient way to tune LLM for 3D|https://arxiv.org/abs/2405.01413|[['parameter-efficient', 'efficient fine-tuning'], ['vision-language'], ['3D', 'point cloud'], ['cs.CV']]
2024-05-02|MANTIS: Interleaved Multi-Image Instruction Tuning|VLM with multiple image inputs, train on 16 A100|https://arxiv.org/abs/2405.01483|[['vision language'], ['cs.CV']]
2024-05-02|Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models|theoreticaly understand retrieval using CLIP, show that I2I is better than T2I|https://arxiv.org/abs/2405.01468|[['Vision-Language'], ['cs.LG']]
2024-05-02|WildChat: 1M ChatGPT Interaction Logs in the Wild|dataset of conversation with GPT-4|https://arxiv.org/abs/2405.01470|[['cs.CL'], ['ICLR']]
2024-05-02|Why Tabular Foundation Models Should Be a Research Priority|call for more  study on tubular modality|https://arxiv.org/abs/2405.01147|[['cs.LG']]
2024-05-02|ATOM: Attention Mixer for Efficient Dataset Distillation|different attention mechanism for feature matching in dataset distillation|https://arxiv.org/abs/2405.01373|[['architecture search'], ['cs.CV'], ['CVPR']]
2024-04-30|MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation|domain generalization with some attribute correlation with base that may confuse models|https://arxiv.org/abs/2404.19644|[['vision-language'], ['cs.CV'], ['ICLR']]
2024-04-30|MoPEFT: A Mixture-of-PEFTs for the Segment Anything Model|use different PEFT techniques and dynamically learns to activate some of them|https://arxiv.org/abs/2405.00293|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CV'], ['CVPR']]
2024-04-30|Soft Prompt Generation for Domain Generalization|learn soft prompt for different domain as target, and train generative model to produce soft prompt for different domain|https://arxiv.org/abs/2404.19286|[['vision language', 'VLMs'], ['cs.CV']]
2024-04-30|CLIP-Mamba: CLIP Pretrained Mamba Models with OOD and Hessian Evaluation|67M Mamba on par with 307M ViT on zero shot, much better on OOD|https://arxiv.org/abs/2404.19394|[['parameter efficiency'], ['cs.CV']]
2024-04-30|SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models|progressively tune adapters at different layers|https://arxiv.org/abs/2405.00201|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CL']]
2024-04-30|On Improving the Algorithm-, Model-, and Data- Efficiency of Self-Supervised Learning|new SSL method to improve performance, memory bank, square regularization|https://arxiv.org/abs/2404.19289|[['cs.CV']]
2024-04-30|StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation|highlight variance of initialization of PEFT, separate hard and soft prompt|https://arxiv.org/abs/2404.19335|[['cs.CL']]
2024-04-30|One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features|one stage open vocabulary temporal action detection rather than making proposal then identifying|https://arxiv.org/abs/2404.19542|[['cs.CV']]
2024-04-30|SemiPL: A Semi-supervised Method for Event Sound Source Localization|as title|https://arxiv.org/abs/2404.19615|[['cs.CV']]
2024-04-30|Better & Faster Large Language Models via Multi-token Prediction|pre train LLM to predict multiple tokens from scratch|https://arxiv.org/abs/2404.19737|[['cs.CL']]
2024-04-30|DOCCI: Descriptions of Connected and Contrasting Images|fine grained long text image pair dataset|https://arxiv.org/abs/2404.19753|[['Vision-language'], ['text-to-image'], ['cs.CV']]
2024-04-30|ASAM: Boosting Segment Anything Model with Adversarial Tuning|generate adversarial examples using diffusion model to boost SAM|https://arxiv.org/abs/2405.00256|[['diffusion'], ['cs.CV'], ['CVPR']]
2024-04-30|Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey|as title|https://arxiv.org/abs/2405.00314|[['cs.LG']]
2024-05-02|Early Transformers: A study on Efficient Training of Transformer Models through Early-Bird Lottery Tickets|iterative pruning, selective retraining to improve training efficiency|https://arxiv.org/abs/2405.02353|[['training efficiency'], ['cs.CL']]
2024-05-02|Enhancing User Experience in On-Device Machine Learning with Gated Compression Layers|dynamically filter non-essential inputs to improve power consumption|https://arxiv.org/abs/2405.01739|[['cs.LG']]
2024-05-02|COPAL: Continual Pruning in Large Language Generative Models|continual pruning weights with high sensitivity to new data in continual learning setting|https://arxiv.org/abs/2405.02347|[['cs.LG']]
2024-05-03|What matters when building vision-language models?|experiment with different design decision of VLM, propose a 8B VLM|https://arxiv.org/abs/2405.02246|[['vision-language', 'VLMs'], ['cs.CV']]
2024-05-03|Auto-Encoding Morph-Tokens for Multimodal LLM|VLM use quantized visual token, can generate both image and text|https://arxiv.org/abs/2405.01926|[['cs.CV']]
2024-05-03|MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot Action Recognition|few-shot action recognition, take into account different velocity|https://arxiv.org/abs/2405.02077|[['cs.CV']]
2024-05-08|THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models|hallucination benchmark for free form QA rather than multiple choice question|https://arxiv.org/abs/2405.05256|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-05-08|Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion|utilize different multimodality data (text-image, text-audio, audio-image, etc) to improve unified model (text-image-audio)|https://arxiv.org/abs/2405.04883|[['cs.CV'], ['ICML']]
2024-05-08|Estimating Noisy Class Posterior with Part-level Labels for Noisy Label Learning|partition images into distinct parts and make pseudo labels on them, and optimize model using multi-label loss to improve robustness|https://arxiv.org/abs/2405.05714|[['cs.CV'], ['CVPR']]
2024-05-08|You Only Cache Once: Decoder-Decoder Architectures for Language Models|new decoder-decoder architecture for LLM that reduces key value caches usage to improve throughput, and GPU memory|https://arxiv.org/abs/2405.05254|[['cs.CL']]
2024-05-08|Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers|use FFT to speed up computation of attention|https://arxiv.org/abs/2405.05219|[['cs.LG']]
2024-05-08|KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation|improve the speed for prefill phase (first token) by parallelizing|https://arxiv.org/abs/2405.05329|[['ICML']]
2024-05-08|Vidur: A Large-Scale Simulation Framework For LLM Inference|simulate LLM inference using experimental profiling and predictive modeling to improve inference speed|https://arxiv.org/abs/2405.05465|[['cs.LG']]
2024-05-07|The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models|study on bias of CLIP when scaling up data|https://arxiv.org/abs/2405.04623|[['VLMs'], ['cs.CY']]
2024-05-07|Differentially Private Post-Processing for Fair Regression|general method to improve fairness of regressor|https://arxiv.org/abs/2405.04034|[['cs.LG', 'cs.CY'], ['ICML']]
2024-05-07|Granite Code Models: A Family of Open Foundation Models for Code Intelligence|IBM LLM for code generation|https://arxiv.org/abs/2405.04324|[['cs.AI', 'cs.CL']]
2024-05-07|Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks|as title|https://arxiv.org/abs/2405.04403|[['Vision-Language', 'VLMs'], ['Attacks'], ['cs.CV', 'cs.CL']]
2024-05-07|Structured Click Control in Transformer-based Interactive Segmentation|use GNN to handle user clicks for segmentation|https://arxiv.org/abs/2405.04009|[['graph'], ['cs.AI', 'cs.CV'], ['NeurIPS']]
2024-05-07|Towards Stability of Parameter-free Optimization|optimizer that estimate step size from AdaGrad-Norm to avoid hyperparameter tuning|https://arxiv.org/abs/2405.04376|[['cs.LG']]
2024-05-09|Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning|merge visual feature into feed forward network to improve efficiency of VLM|https://arxiv.org/abs/2405.05615|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['Vision-Language'], ['cs.LG', 'cs.CV', 'cs.CL'], ['ICML']]
2024-05-09|DARA: Domain- and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding|domain aware and relation aware adapters for temporal grounding|https://arxiv.org/abs/2405.06217|[['Parameter-efficient'], ['vision-language'], ['cs.CV']]
2024-05-09|Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse|utilize relation between good feature and symmetric weights (neural collapse) to improve training on biased data|https://arxiv.org/abs/2405.05587|[['cs.LG', 'cs.CV'], ['CVPR']]
2024-05-09|Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference|ignore some visual tokens to improve efficiency|https://arxiv.org/abs/2405.05803|[['cs.AI', 'cs.CV']]
2024-05-09|CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts|MoE for MLM|https://arxiv.org/abs/2405.05949|[['cs.CV']]
2024-05-09|OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning|several pruning techniques to achieve high compression of LLM|https://arxiv.org/abs/2405.05957|[['cs.CL']]
2024-05-09|LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models|toolkit and benchmark for quantized LLM|https://arxiv.org/abs/2405.06001|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-09|SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models|as title|https://arxiv.org/abs/2405.06219|[['cs.LG', 'cs.CL']]
2024-05-09|MaskMatch: Boosting Semi-Supervised Learning Through Mask Autoencoder-Driven Feature Learning|semi supervised learning try to utilize all unlabeled data (no threshold for pseudo label confidence)|https://arxiv.org/abs/2405.06227|[['cs.CV']]
2024-05-13|Localizing Task Information for Improved Model Merging and Compression|find that the model after merging retaining ability of each task but may conflict or interfere each other, use compression to avoid conflict|https://arxiv.org/abs/2405.07813|[['cs.LG', 'cs.CV'], ['ICML']]
2024-05-13|Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models|adaptively train sub modules to improve efficiency|https://arxiv.org/abs/2405.07527|[['cs.AI', 'cs.LG'], ['NeurIPS']]
2024-05-13|MambaOut: Do We Really Need Mamba for Vision?|show that mamba may not be suitable for specific tasks in CV|https://arxiv.org/abs/2405.07992|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-05-13|SpeechVerse: A Large-scale Generalizable Audio Language Model|aws audio text model|https://arxiv.org/abs/2405.08295|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-05-12|CLIP-Powered TASS: Target-Aware Single-Stream Network for Audio-Visual Question Answering|AVQA, focus on audio visual fusion and region aware visual feature|https://arxiv.org/abs/2405.07451|[['vision-language', 'VLMs'], ['Audio-Visual'], ['cs.CV']]
2024-05-12|Enhanced Online Test-time Adaptation with Feature-Weight Cosine Alignment|claim that minimizing cosine similarty is better than minimizing entropy for test time adaptation|https://arxiv.org/abs/2405.07171|[['cs.CV']]
2024-05-12|Unified Video-Language Pre-training with Synchronized Audio|MAE and contrastive pretraining for audio visual and text|https://arxiv.org/abs/2405.07202|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.SD', 'eess.AS']]
2024-05-12|Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains|pretraining task for tabular data|https://arxiv.org/abs/2405.07414|[['cs.AI', 'cs.LG'], ['ICML']]
2024-05-14|Efficient Vision-Language Pre-training by Cluster Masking|masking durin CLIP pretraining to improve performance and training speed|https://arxiv.org/abs/2405.08815|[['Vision-Language'], ['cs.CV'], ['CVPR']]
2024-05-14|Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis|analyze the cost of long input for transformers|https://arxiv.org/abs/2405.08944|[['GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-14|Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation|improve segmentation by refine the attention maps by text visual or visual visual alignment|https://arxiv.org/abs/2405.08458|[['cs.CV'], ['CVPR']]
2024-05-14|Improving Transformers with Dynamically Composable Multi-Head Attention|new design for multi head self attention, refine attention in a query key dependent way|https://arxiv.org/abs/2405.08553|[['cs.LG', 'cs.CL'], ['ICML']]
2024-05-14|EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training|curriculum learning (design which data to feed during training) according to frequency|https://arxiv.org/abs/2405.08768|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICCV']]
2024-05-14|Cross-Domain Feature Augmentation for Domain Generalization|augmentation on feature space to improve domain generalization|https://arxiv.org/abs/2405.08586|[['cs.CV']]
2024-05-14|CinePile: A Long Video Question Answering Dataset and Benchmark| as title|https://arxiv.org/abs/2405.08813|[['cs.LG', 'cs.CV']]
2024-05-14|Spatial Semantic Recurrent Mining for Referring Image Segmentation|better modality features fusion for referring image segmentation (segmentation given description)|https://arxiv.org/abs/2405.09006|[['cs.CV', 'cs.CL']]
2024-05-14|AD-Aligning: Emulating Human-like Generalization for Cognitive Domain Adaptation in Deep Learning|adversarial learning and CORAL loss for domain adaptation|https://arxiv.org/abs/2405.09582|[['cs.CV', 'eess.IV']]
2024-05-14|Improving Transformers using Faithful Positional Encoding|different positional embeddings with mathematical guarantee|https://arxiv.org/abs/2405.09061|[['cs.LG']]
2024-05-15|SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge|commonsense machine generated VQA |https://arxiv.org/abs/2405.09713|[['vision-language'], ['cs.AI', 'cs.CV', 'cs.CL'], ['CVPR']]
2024-05-15|LoRA Learns Less and Forgets Less|LoRA underperform on target tasks but maintain generalizibility, including some pratices of using LoRA|https://arxiv.org/abs/2405.09673|[['parameter-efficient', 'efficient finetuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-15|ReconBoost: Boosting Can Achieve Modality Reconcilement|use idea similar to boosting, update one modality each time to prevent dominant modality|https://arxiv.org/abs/2405.09321|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-05-15|Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection|as title|https://arxiv.org/abs/2405.09782|[['cs.CV'], ['ICML']]
2024-05-15|Curriculum Dataset Distillation|progressively generate adverserial data to achieve dataset distillation|https://arxiv.org/abs/2405.09150|[['cs.CV']]
2024-05-15|HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants|evaluate LLM by calculating correlation between LLM preference (log likelihood) and human ranked response|https://arxiv.org/abs/2405.09186|[['cs.CL']]
2024-05-15|Prompting-based Synthetic Data Generation for Few-Shot Question Answering|use LLM to generate or refine QA data|https://arxiv.org/abs/2405.09335|[['cs.CL']]
2024-05-15|Spectral Editing of Activations for Large Language Model Alignment|test time editing to improve truthfulness and reduce hallucination|https://arxiv.org/abs/2405.09719|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-15|SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data|task where users can access to specific portion of data to study security and privacy|https://arxiv.org/abs/2405.09805|[['cs.CL']]
2024-05-15|Overcoming Domain Drift in Online Continual Learning|use contrastive loss and distillation in continual domain adaptation|https://arxiv.org/abs/2405.09133|[['cs.LG']]
2024-05-15|Hierarchical Emotion Prediction and Control in Text-to-Speech Synthesis|predict hierarchical emotion to help tts|https://arxiv.org/abs/2405.09171|[['Synthesis'], ['cs.SD', 'eess.AS'], ['ICASSP']]
2024-05-15|Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models Promptly|new measures for fairness using distance in feature space and its approximation algorithm|https://arxiv.org/abs/2405.09251|[['cs.LG', 'cs.CY']]
2024-05-15|Dynamic Activation Pitfalls in LLaMA Models: An Empirical Study|study on activation function and KV cache skipping in LLaMA|https://arxiv.org/abs/2405.09274|[['cs.LG']]
2024-05-15|SA-FedLora: Adaptive Parameter Allocation for Efficient Federated Learning with LoRA Tuning|as title|https://arxiv.org/abs/2405.09394|[['parameter-efficient'], ['Federated Learning'], ['cs.LG']]
2024-05-15|STAR: A Benchmark for Situated Reasoning in Real-World Videos|video QA, reasoning benchmark (NeurIPS'21)|https://arxiv.org/abs/2405.09711|[['graph'], ['cs.AI', 'cs.CV', 'cs.CL'], ['NeurIPS']]
2024-05-16|FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models|affect of negative label and low caption quality to VLM, use sigmoid loss to improve|https://arxiv.org/abs/2405.10286|[['Vision-Language'], ['cs.AI', 'cs.CV'], ['CVPR']]
2024-05-16|Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning|use VLM to interact with environment and improve it using RL|https://arxiv.org/abs/2405.10292|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-05-16|SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection|construct different level of object description to deal with various granularities in open vocabulary object detection|https://arxiv.org/abs/2405.10053|[['cs.CV'], ['CVPR']]
2024-05-16|Libra: Building Decoupled Vision System on Large Language Models|VLM decouples inner and inter modality modeling, use auto regressive modeling on both vision and language|https://arxiv.org/abs/2405.10140|[['cs.CV'], ['ICML']]
2024-05-16|DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data|synthetic data for data augmentation for instance segmentation|https://arxiv.org/abs/2405.10185|[['cs.CV'], ['CVPR']]
2024-05-16|Chameleon: Mixed-Modal Early-Fusion Foundation Models|VLM with early modality fusion (not concatenation of visual encoder and LLM), handle arbitrary modality order, can generate image, made by META|https://arxiv.org/abs/2405.09818|[['cs.CL']]
2024-05-16|Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling|generate soft negative sample (high textual quality but illogical sematics) to do contrastive to ease hallucination|https://arxiv.org/abs/2405.09848|[['cs.AI', 'cs.CL']]
2024-05-16|Towards Realistic Incremental Scenario in Class Incremental Semantic Segmentation|argue the unrealistic setting in previous class incremental segmentation (same image with different labels)|https://arxiv.org/abs/2405.09858|[['cs.LG', 'cs.CV']]
2024-05-16|Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models|let LLM do cloze to correct ASR results|https://arxiv.org/abs/2405.10025|[['cs.AI', 'cs.LG', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-05-16|Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation|as title, optimal transport to deal with misaligned pairs, ICLR'24|https://arxiv.org/abs/2405.10084|[['cs.AI', 'cs.SD', 'eess.AS']]
2024-05-16|Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection|grounding DINO 1.5|https://arxiv.org/abs/2405.10300|[['cs.CV']]
2024-05-16|Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models|prompting for fair and unbias LLM|https://arxiv.org/abs/2405.10431|[['cs.CL']]
2024-05-16|Rethinking ChatGPT's Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs' Prompting|try to explain the reason behind the power of auto regressive LLM|https://arxiv.org/abs/2405.10474|[['cs.CL']]
2024-05-16|Learnable Privacy Neurons Localization in Language Models|localize privacy information learnt by LLM through adversairal training|https://arxiv.org/abs/2405.10989|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-16|In-context Contrastive Learning for Event Causality Identification|contrastive learning to enable ability to learn from positive and negative demonstration|https://arxiv.org/abs/2405.10512|[['cs.LG']]
2024-05-17|Driving Referring Video Object Segmentation with Vision-Language Pre-trained Models|prompt tuning to make CLIP better at pixel label tasks (segmentation)|https://arxiv.org/abs/2405.10610|[['Vision-Language'], ['cs.CV']]
2024-05-17|CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly Supervised Audio-Visual Video Parsing|detect audible or visible event with video level labels, use unimodal and cross modal encoders to do contrastive learning|https://arxiv.org/abs/2405.10690|[['Audio-Visual'], ['cs.CV']]
2024-05-17|HARIS: Human-Like Attention for Reference Image Segmentation|reuse transformer layers with feed back connection|https://arxiv.org/abs/2405.10707|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CV']]
2024-05-17|Open-Vocabulary Spatio-Temporal Action Detection|benchmark for open vocabulary spatio temporal action detection and its baseline|https://arxiv.org/abs/2405.10832|[['VLM'], ['cs.CV']]
2024-05-17|Detecting Multimodal Situations with Insufficient Context and Abstaining from Baseless Predictions|argue that VQA data's answer may rely on unsupported assumption causing hallucination, collect context data for them to reduce hallucination|https://arxiv.org/abs/2405.11145|[['Vision-Language'], ['cs.AI', 'cs.CV']]
2024-05-17|Towards Modular LLMs by Building and Reusing a Library of LoRAs|learn a collection of LoRA from multi-task data and dynamically select them during inference|https://arxiv.org/abs/2405.11157|[['parameter-efficient'], ['cs.LG', 'cs.CL']]
2024-05-17|Dynamic data sampler for cross-language transfer learning in large language models|gradually transfer from continual pretrain to supervised finetune to transfer LLM to other language|https://arxiv.org/abs/2405.10626|[['cs.CL'], ['ICASSP']]
2024-05-17|DINO as a von Mises-Fisher mixture model|theoretical insight of DINO pretraining method, small change base on the insight improve performance|https://arxiv.org/abs/2405.10939|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICLR']]
2024-05-17|Acoustic modeling for Overlapping Speech Recognition: JHU Chime-5 Challenge System|as title, ICASSP'19|https://arxiv.org/abs/2405.11078|[['eess.AS'], ['ICASSP']]
2024-05-17|Improving Point-based Crowd Counting and Localization Based on Auxiliary Point Guidance|matching method to increase stability when optimizing model for crowd counting|https://arxiv.org/abs/2405.10589|[['cs.AI', 'cs.CV', 'eess.IV']]
2024-05-17|Learning Object-Centric Representation via Reverse Hierarchy Guidance|predict object mask to obtain top level feature to guide the original model|https://arxiv.org/abs/2405.10598|[['cs.CV']]
2024-05-17|Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization|as title|https://arxiv.org/abs/2405.10616|[['cs.LG', 'cs.CL']]
2024-05-17|Layer-Condensed KV Cache for Efficient Inference of Large Language Models|only compute and caches the key and values to save memory and improve throughput|https://arxiv.org/abs/2405.10637|[['cs.CL']]
2024-05-17|Feature-Adaptive and Data-Scalable In-Context Learning|feature space in context learning to avoid length limit|https://arxiv.org/abs/2405.10738|[['cs.CL']]
2024-05-17|Efficient Multimodal Large Language Models: A Survey|as title|https://arxiv.org/abs/2405.10739|[['cs.AI', 'cs.CV']]
2024-05-17|SBAAM! Eliminating Transcript Dependency in Automatic Subtitling|end to end model for translating dialogue, segmenting, estimating timestamps without intermediate ASR model|https://arxiv.org/abs/2405.10741|[['cs.CL']]
2024-05-17|Observational Scaling Laws and the Predictability of Language Model Performance|scaling laws without training process for different model family to predict model's performance|https://arxiv.org/abs/2405.10938|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-17|Prompt Exploration with Prompt Regression|predict the effect of a prompt and select a optimal combination of prompts|https://arxiv.org/abs/2405.11083|[['cs.LG', 'cs.CL']]
2024-05-17|Revisiting the Robust Generalization of Adversarial Prompt Tuning|prompt tuning to tackle adversarial sample, use KL divergence as regularization|https://arxiv.org/abs/2405.11154|[['vision-language'], ['attacks'], ['cs.AI', 'cs.CV']]
2024-05-18|Fuse & Calibrate: A bi-directional Vision-Language Guided Framework for Referring Image Segmentation|text visual feature fusion for eeter text guided segmentation|https://arxiv.org/abs/2405.11205|[['Vision-Language'], ['cs.CV']]
2024-05-18|Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts|MoE on MLM (speech visual text)|https://arxiv.org/abs/2405.11273|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-18|MBIAS: Mitigating Bias in Large Language Models While Retaining Context|instruction tuning using new data to debias LLM|https://arxiv.org/abs/2405.11290|[['cs.CL']]
2024-05-18|A Unified Approach Towards Active Learning and Out-of-Distribution Detection|Solve active learning (determine label candidates) from the view of out of distribution detection|https://arxiv.org/abs/2405.11337|[['cs.CV']]
2024-05-18|Exploring speech style spaces with language models: Emotional TTS without emotion labels|emotional TTS based on text it self rather than other emotion label (use pseudo label to solve)|https://arxiv.org/abs/2405.11413|[['cs.LG', 'eess.AS']]
2024-05-18|MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning|as title|https://arxiv.org/abs/2405.11446|[['cs.LG', 'cs.CL']]
2024-05-19|MICap: A Unified Model for Identity-aware Movie Descriptions|end to end movie caption rather than predicting characters afterward|https://arxiv.org/abs/2405.11483|[['cs.CV'], ['CVPR']]
2024-05-19|SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization|progressively replace layer norm with batch norm in training, attention without sofmax to improve the efficacy|https://arxiv.org/abs/2405.11582|[['cs.CV', 'cs.CL'], ['ICML']]
2024-05-19|Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks|study the efficiency bottleneck of transformers during training and inference|https://arxiv.org/abs/2405.11704|[['training efficiency'], ['cs.AI', 'cs.LG']]
2024-05-19|Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion|reduce length of soft prompt to improve efficiency, use subspace projection to increase performance, similar to hyper network|https://arxiv.org/abs/2405.11464|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-19|A Multi-Perspective Analysis of Memorization in Large Language Models|study on how and why LLM memorize training data|https://arxiv.org/abs/2405.11577|[['cs.AI', 'cs.CL']]
2024-05-19|Token-wise Influential Training Data Retrieval for Large Language Models|training data influence estimation|https://arxiv.org/abs/2405.11724|[['cs.AI', 'cs.CL']]
2024-05-19|Configurable Mirror Descent: Towards a Unification of Decision Making|unified framework for reinforcement learning (number of agent, how they interect...)|https://arxiv.org/abs/2405.11746|[['cs.AI', 'cs.LG'], ['ICML']]
2024-05-19|Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning|foundation model is biased toward certain classes that effect semi supervised learning, solve by modified softmax and label smoothing|https://arxiv.org/abs/2405.11756|[['cs.LG'], ['ICML']]
2024-05-19|DATR: Unsupervised Domain Adaptive Detection Transformer with Dataset-Level Adaptation and Prototypical Alignment|unsupervisedly learn domain prototype for cross domain detection|https://arxiv.org/abs/2405.11765|[['cs.CV']]
2024-05-19|Your Transformer is Secretly Linear|some block in transformer is actually linear function use linear approximation of them won't effect performance|https://arxiv.org/abs/2405.12250|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-19|Learning More Generalized Experts by Merging Experts in Mixture-of-Experts|merge MoE module according usage frequency|https://arxiv.org/abs/2405.11530|[['cs.LG']]
2024-05-20|FeTT: Continual Class Incremental Learning via Feature Transformation Tuning|Fix backbone and use non-learning based transform can do class incremental learning|https://arxiv.org/abs/2405.11822|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.CV']]
2024-05-20|SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model|as title|https://arxiv.org/abs/2405.11831|[['GPU memory'], ['cs.LG', 'eess.AS']]
2024-05-20|Rethinking Overlooked Aspects in Vision-Language Models|pipeline to select data for better sample efficiency|https://arxiv.org/abs/2405.11850|[['Vision-Language'], ['cs.CV']]
2024-05-20|Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process|combine the last two steps when training LLM|https://arxiv.org/abs/2405.11870|[['training efficiency'], ['cs.AI', 'cs.CL']]
2024-05-20|MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning|challenge the low rank assumption in LoRA|https://arxiv.org/abs/2405.12130|[['Parameter-Efficient', 'Efficient Fine-Tuning'], ['cs.LG', 'cs.CL']]
2024-05-20|Unveiling and Manipulating Prompt Influence in Large Language Models|estimate token significance based on embedding distribution to do prompt manipulation|https://arxiv.org/abs/2405.11891|[['cs.AI', 'cs.CL'], ['ICLR']]
2024-05-20|CSTA: CNN-based Spatiotemporal Attention for Video Summarization|CNN sliding windows attention for more efficiency spatiotemporal understanding|https://arxiv.org/abs/2405.11905|[['cs.CV'], ['CVPR']]
2024-05-20|xFinder: Robust and Pinpoint Answer Extraction for Large Language Models|a model for extracting answer from natural language response to robust evaluation of LLM|https://arxiv.org/abs/2405.11874|[['cs.CL']]
2024-05-20|Multiple-Choice Questions are Efficient and Robust LLM Evaluators|correlation of multiple choice prompt and natural response problem is high, maybe multiple choice is enough|https://arxiv.org/abs/2405.11966|[['cs.CL']]
2024-05-20|MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering|multilingual VQA benchmark|https://arxiv.org/abs/2405.11985|[['cs.CV']]
2024-05-20|WorldAfford: Affordance Grounding based on Natural Language Instructions|grounding according to complex human instruction|https://arxiv.org/abs/2405.12461|[['cs.AI', 'cs.CV']]
2024-05-20|TinyLLaVA Factory: A Modularized Codebase for Small-scale Large Multimodal Models|codebase for multimodal model|https://arxiv.org/abs/2405.11788|[['cs.LG']]
2024-05-20|Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices|temporal consistency of video editing base on the observation of slices on temporal dimension of video can look like natural images|https://arxiv.org/abs/2405.12211|[['Diffusion', 'synthesis', 'Video Editing', 'Text-to-Image'], ['cs.CV'], ['ICML']]
2024-05-21|PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference|adjust KV cache based on the number of important key and value in each layer|https://arxiv.org/abs/2405.12532|[['GPU memory'], ['cs.CL']]
2024-05-21|C3L: Content Correlated Vision-Language Instruction Tuning Data Generation via Contrastive Learning|note the problem of generating visual instruction tuning data from LLM and use contrastive learning to do calibrate|https://arxiv.org/abs/2405.12752|[['Vision-Language'], ['cs.CV']]
2024-05-21|Inconsistency-Aware Cross-Attention for Audio-Visual Fusion in Dimensional Emotion Recognition|different modalities may show low complementary relationship, add gating mechanism to deal with this|https://arxiv.org/abs/2405.12853|[['Audio-Visual'], ['cs.CV']]
2024-05-21|OmniGlue: Generalizable Feature Matching with Foundation Model Guidance|improve image matching on unseen domain using DINO|https://arxiv.org/abs/2405.12979|[['cs.CV'], ['CVPR']]
2024-05-21|Context-Enhanced Video Moment Retrieval with Large Language Models|use LLM to enhance query context to improve moment retrieval|https://arxiv.org/abs/2405.12540|[['cs.CV']]
2024-05-21|Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression|control outlier of KV cache to impreve KV cache quantization and decomposition|https://arxiv.org/abs/2405.12591|[['cs.CL']]
2024-05-21|Mamba in Speech: Towards an Alternative to Self-Attention|as titile|https://arxiv.org/abs/2405.12609|[['cs.SD', 'eess.AS']]
2024-05-21|Tagengo: A Multilingual Chat Dataset|dataset of prompt-response pairs in multiple languages|https://arxiv.org/abs/2405.12612|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-21|Text-Video Retrieval with Global-Local Semantic Consistent Learning|text video retriveal by considering both global and local semantics|https://arxiv.org/abs/2405.12710|[['cs.CV']]
2024-05-21|BIMM: Brain Inspired Masked Modeling for Video Representation Learning|MAE for image, video, hand-crafted features simultaneously|https://arxiv.org/abs/2405.12757|[['cs.CV']]
2024-05-21|Reducing Transformer Key-Value Cache Size with Cross-Layer Attention|reduce KV cache by sharing key value head between adjacent heads|https://arxiv.org/abs/2405.12981|[['cs.LG', 'cs.CL']]
2024-05-21|Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting|investigate different fine-tune strategy with few shot setting|https://arxiv.org/abs/2405.13181|[['cs.LG', 'cs.CL']]
2024-05-21|Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum|batching strategy for efficient pretraining of LLM, prevent naive chunk and caoncatenate|https://arxiv.org/abs/2405.13226|[['cs.LG', 'cs.CL']]
2024-05-21|MELD-ST: An Emotion-aware Speech Translation Dataset|speech translation with emotion label (english-japanese and english-german)|https://arxiv.org/abs/2405.13233|[['cs.CL']]
2024-05-21|Vision Transformer with Sparse Scan Prior|local, sparse, hierarchical attention for ViT with lower FLOPs|https://arxiv.org/abs/2405.13335|[['cs.CV']]
2024-05-21|Semantic Equitable Clustering: A Simple, Fast and Effective Strategy for Vision Transformer|semantic aware region partion for attention|https://arxiv.org/abs/2405.13337|[['cs.CV']]
2024-05-21|Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks|defense attack based on quantization by not rounding simply to nearest value|https://arxiv.org/abs/2405.12725|[['Attacks'], ['cs.CV'], ['CVPR']]
2024-05-21|ReALLM: A general framework for LLM compression and fine-tuning|unified frame work for quantization and finetuning by decompose weights and updating some of them|https://arxiv.org/abs/2405.13155|[['cs.LG']]
2024-05-22|No Filter: Cultural and Socioeconomic Diversityin Contrastive Vision-Language Models|explore performance gap of CLIP about different region, note that data filter make CLIP more bias|https://arxiv.org/abs/2405.13777|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-05-22|Dense Connector for MLLMs|use visual features from different layers other than the last one|https://arxiv.org/abs/2405.13800|[['vision-language'], ['cs.AI', 'cs.CV']]
2024-05-22|What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions|efficient gradient-based method to evaluate impoortance of data|https://arxiv.org/abs/2405.13954|[['GPU memory'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-22|Unveiling the Tapestry of Consistency in Large Vision-Language Models|benchmark consistency of VLM when facing different prompt (open ended question, multiple choice question)|https://arxiv.org/abs/2405.14156|[['Vision-Language'], ['cs.CV']]
2024-05-22|Gradient Projection For Parameter-Efficient Continual Learning|reformulate PEFT methods to preserve old tasks prediction by orthogonal gradient projection|https://arxiv.org/abs/2405.13383|[['Parameter-Efficient'], ['cs.LG']]
2024-05-22|Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation|evaluate the performance of RAG by automatically generated exam|https://arxiv.org/abs/2405.13622|[['cs.CL'], ['ICML']]
2024-05-22|Spectral Adapter: Fine-Tuning in Spectral Space|fine tune the top spectral space of SVD of pretrained weights, experiments on GLUE and difussion model|https://arxiv.org/abs/2405.13952|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.AI', 'cs.LG']]
2024-05-22|Efficient Multitask Dense Predictor via Binarization|use distiallation to obtain a binary backbone for multiple dense prediction task (depth, edge, segmention)|https://arxiv.org/abs/2405.14136|[['cs.CV'], ['CVPR']]
2024-05-22|AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs|separate outlier weights to preserve original distribution after quantization|https://arxiv.org/abs/2405.13358|[['cs.CL']]
2024-05-22|VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding|add timestamp information to visual feature, similar to timechat but apply in feature space rather than natural language, instruction dataset for temporal tasks|https://arxiv.org/abs/2405.13382|[['cs.CV']]
2024-05-22|Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning|dynamically change instruction tuning data distribution based on difficulty measured by the other LLM|https://arxiv.org/abs/2405.13448|[['cs.CL']]
2024-05-22|ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation|transplantation alignment results from one LLM to another|https://arxiv.org/abs/2405.13578|[['cs.CL']]
2024-05-22|Safety Alignment for Vision Language Models|note that visual branch of VLM is vulnerable to attach, add modules before LLM to solve|https://arxiv.org/abs/2405.13581|[['Vision Language', 'VLMs'], ['attacks'], ['cs.AI', 'cs.CV']]
2024-05-22|Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations|use corrective feedback and demonstration to align LLM|https://arxiv.org/abs/2405.13828|[['cs.AI', 'cs.CL']]
2024-05-22|TOPA: Extend Large Language Models for Video Understanding via Text-Only Pre-Alignment|use video frame captions and CLIP text encoder to mimic video data enable video alignment without video|https://arxiv.org/abs/2405.13911|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-22|One-shot Training for Video Object Segmentation|general methods by manipulating time order of pseudo labels for video segmentation model to fine-tune on single video|https://arxiv.org/abs/2405.14010|[['cs.CV']]
2024-05-22|AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability|different samples need different effort to align, learn alignment capability tokens during pretraining, and combine these token according to instruction data during alignment|https://arxiv.org/abs/2405.14129|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-22|There is HOPE to Avoid HiPPOs for Long-memory State Space Models|replace HiPPO theory so that specific initialization and exponential decaying memory is not required, and training is more stable|https://arxiv.org/abs/2405.13975|[['cs.LG']]
2024-05-23|Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models|in retrieval system using CLIP, make new embeddings compatible with old embeddings to prevent re-computing embeddings|https://arxiv.org/abs/2405.14715|[['parameter-efficient'], ['Vision-Language'], ['cs.AI', 'cs.CV']]
2024-05-23|Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations|combining MAE and CLIP pretraining method|https://arxiv.org/abs/2405.14239|[['Vision-language'], ['cs.LG', 'cs.CV']]
2024-05-23|Segformer++: Efficient Token-Merging Strategies for High-Resolution Semantic Segmentation|experiment with different token merging strategies for segmentation|https://arxiv.org/abs/2405.14467|[['training efficiency'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-05-23|Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference|layer residual connection of adapters and token merging|https://arxiv.org/abs/2405.14700|[['Parameter-efficient', 'PEFT', 'Efficient Fine-tuning', 'GPU memory'], ['cs.CV']]
2024-05-23|FLoRA: Low-Rank Core Space for N-dimension|LoRA that preserves structural locality of activation in high dimension operation like convolution|https://arxiv.org/abs/2405.14739|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CV']]
2024-05-23|Bitune: Bidirectional Instruction-Tuning|use bidirectional attention rather than causal masking during prefilling|https://arxiv.org/abs/2405.14862|[['parameter-efficient', 'PEFT', 'efficient finetuning'], ['cs.CL']]
2024-05-23|Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining|scaling law of LLM and find that data mixing as some benefit to pretrainng|https://arxiv.org/abs/2405.14908|[['training efficiency'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-23|ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning|spatiotemporal culstering, random permutation and autoregressivly prediction as pretext task|https://arxiv.org/abs/2405.15160|[['training efficiency', 'GPU memory'], ['cs.CV']]
2024-05-23|VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks|reduce parameter counts of LoRA by sharing weights in codebook (similar idea as hyper network)|https://arxiv.org/abs/2405.15179|[['Parameter Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CL']]
2024-05-23|EMR-Merging: Tuning-Free High-Performance Model Merging|elect a unified model, generate mask of weights and rescale factors for model merging|https://arxiv.org/abs/2405.17461|[['PEFT'], ['cs.LG', 'cs.CV']]
2024-05-23|ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification|use attention score to identify salient tokens to help KV cache quantization|https://arxiv.org/abs/2405.14256|[['GPU memory'], ['cs.AI', 'cs.LG']]
2024-05-23|Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models|adaptive adjust number of expert to activate (to train)|https://arxiv.org/abs/2405.14297|[['Vision-Language'], ['cs.AI', 'cs.LG']]
2024-05-23|Multi-Scale VMamba: Hierarchy in Hierarchy Visual State Space Model|use depth wise convolution to apply SSM on features of different scales|https://arxiv.org/abs/2405.14174|[['cs.CV']]
2024-05-23|From Text to Pixel: Advancing Long-Context Understanding in MLLMs|MLLM for long text and multiple images|https://arxiv.org/abs/2405.14213|[['cs.CV', 'cs.CL']]
2024-05-23|RAQ-VAE: Rate-Adaptive Vector-Quantized Variational Autoencoder|VAE whose codebook comes from clustering of other codebook or sequence to sequence model to improve scalability of VQ-VAE|https://arxiv.org/abs/2405.14222|[['cs.LG', 'cs.CV', 'eess.IV']]
2024-05-23|Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast|substract inactivated expert's output from that of activated ones to improve performance|https://arxiv.org/abs/2405.14507|[['cs.LG', 'cs.CL']]
2024-05-23|RE-Adapt: Reverse Engineered Adaptation of Large Language Models|reverse engineering to obtain LLM without instruction following ability, update domain specific knowledge on this model and add instruction tuning vector back|https://arxiv.org/abs/2405.15007|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-23|RAEE: A Training-Free Retrieval-Augmented Early Exiting Framework for Efficient Inference|early exiting by retrieving based on distribution matching|https://arxiv.org/abs/2405.15198|[['cs.CL']]
2024-05-23|Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for LLMs|parallel decoding based on pretrained LLM's original ability for faster inference|https://arxiv.org/abs/2405.15208|[['cs.AI', 'cs.CL']]
2024-05-23|Not All Language Model Features Are Linear|try to explore the property of latent representation in LLM, find that they can be circular (months of year)|https://arxiv.org/abs/2405.14860|[['cs.LG']]
2024-05-24|Learning from True-False Labels via Multi-modal Prompt Retrieving|strategy of generating pseudo label by asking VLM true or false rather than open questions|https://arxiv.org/abs/2405.15228|[['vision-language', 'VLMs'], ['cs.LG', 'cs.CV']]
2024-05-24|Sparse Matrix in Large Language Model Fine-tuning|select sparse protion of weights to fine-tune, won't plateau as trainable parameters increases like LoRA|https://arxiv.org/abs/2405.15525|[['parameter-efficient', 'PEFT', 'efficient fine-tuning', 'GPU memory'], ['cs.CL']]
2024-05-24|Streaming Long Video Understanding with Large Language Models|memory base method (similar to RNN) to handle lone videos|https://arxiv.org/abs/2405.16009|[['vision-language'], ['cs.CV']]
2024-05-24|SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models|add weights after pruning to retain performance|https://arxiv.org/abs/2405.16057|[['Parameter-Efficient', 'Efficient Fine-Tuning'], ['cs.LG', 'cs.CL']]
2024-05-24|Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation|hyper network to generate soft prompts|https://arxiv.org/abs/2405.15282|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.AI', 'cs.LG']]
2024-05-24|BiSup: Bidirectional Quantization Error Suppression for Large Language Models|weight activation qantization that takes attention mechanism into account|https://arxiv.org/abs/2405.15346|[['parameter-efficient', 'efficient fine-tuning'], ['diffusion'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-24|Less is more: Summarizing Patch Tokens for efficient Multi-Label Class-Incremental Learning|task specific sub-network for eacch task, train a token summarizer to reduce computation cost|https://arxiv.org/abs/2405.15633|[['cs.AI', 'cs.CV']]
2024-05-24|What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models|project outputs of MLM (prediction, class description, image feature) to a common feature space as the query feature (just like ensemble feature for CLIP)|https://arxiv.org/abs/2405.15668|[['cs.CV']]
2024-05-24|Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models|make the visual encoder of VLM aware of text prompt by feeding global textual feature to visual encoder and apply additional attention conditioned on text after visual encoder|https://arxiv.org/abs/2405.15684|[['cs.AI', 'cs.CV']]
2024-05-24|LM4LV: A Frozen Large Language Model for Low-level Vision Tasks|let LLM to generate images by using embedding extracted by encoder as target|https://arxiv.org/abs/2405.15734|[['vision-language'], ['text-to-image'], ['cs.CV']]
2024-05-24|ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models|hierarchical visual encoder that reduce output token length to enable high resolution input|https://arxiv.org/abs/2405.15738|[['cs.CV']]
2024-05-24|Sparse Expansion and Neuronal Disentanglement|copy weights to MoE modules and apply pruning, speed up inference while maintaining performance due to feature disentangle|https://arxiv.org/abs/2405.15756|[['cs.AI', 'cs.LG'], ['NeurIPS']]
2024-05-24|Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications|as title|https://arxiv.org/abs/2405.15877|[['cs.LG', 'cs.CL']]
2024-05-24|HyperInterval: Hypernetwork approach to training weight interval regions in continual learning|use hyper network to select interval of weights for each task to prevent forgetting|https://arxiv.org/abs/2405.15444|[['cs.AI', 'cs.LG']]
2024-05-24|MoEUT: Mixture-of-Experts Universal Transformers|combining unversal transformer (layer shared transformer) and MoE|https://arxiv.org/abs/2405.16039|[['cs.AI', 'cs.LG']]
2024-05-25|CRoFT: Robust Fine-Tuning with Concurrent Optimization for OOD Generalization and Open-Set OOD Detection|regularization of minimizing gradient magnitude of energy scores to improve OOD detection|https://arxiv.org/abs/2405.16417|[['vision-language'], ['cs.CV'], ['ICML']]
2024-05-25|DynRefer: Delving into Region-level Multi-modality Tasks via Dynamic Resolution|incorporate image of different region level for fine-grained tasks|https://arxiv.org/abs/2405.16071|[['vision-language'], ['cs.CV']]
2024-05-25|Keypoint-based Progressive Chain-of-Thought Distillation for LLMs|tricks for distilling rationale ability to small model, predict token importance and rationale difficulties|https://arxiv.org/abs/2405.16064|[['cs.CL'], ['ICML']]
2024-05-25|Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection|use LLM prefilling to assess relevance of retrieved document parallelly to accelerate RAG|https://arxiv.org/abs/2405.16178|[['cs.CL']]
2024-05-25|SpinQuant: LLM quantization with learned rotations|rotate weights make quantization easier|https://arxiv.org/abs/2405.16406|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-05-26|LoQT: Low Rank Adapters for Quantized Training|SVD gradient of the weights to initialize LoRA|https://arxiv.org/abs/2405.16528|[['cs.LG', 'cs.CL']]
2024-05-26|Compressing Lengthy Context With UltraGist|segment input text and insert special tokens to gather information by cross attention thus decreasing input length|https://arxiv.org/abs/2405.16635|[['cs.CL']]
2024-05-27|Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR|pinpoint some shortcoming of VLM to improve|https://arxiv.org/abs/2405.16934|[['Vision-Language'], ['cs.CV']]
2024-05-27|Unifying Demonstration Selection and Compression for In-Context Learning|as title|https://arxiv.org/abs/2405.17062|[['parameter-efficient'], ['cs.CL']]
2024-05-27|DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution|replace LoRA with multiple 1 rank sub-modules and then apply pruning|https://arxiv.org/abs/2405.17357|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['cs.CL']]
2024-05-27|LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters|use SVD to reduce trainable parameters in LoRA|https://arxiv.org/abs/2405.17604|[['parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-27|Visual Anchors Are Strong Information Aggregators For Multimodal Large Language Model|some tokens called visual anchors though in background region are informative, identify and use them for VLM inputs|https://arxiv.org/abs/2405.17815|[['vision-language'], ['cs.CV']]
2024-05-27|Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models|prevent VLM from focusing on few image tokens to reduce hallucination|https://arxiv.org/abs/2405.17820|[['Vision Language'], ['cs.AI', 'cs.CV']]
2024-05-27|RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs|data augmentation to reduce hallucination|https://arxiv.org/abs/2405.17821|[['Vision Language'], ['cs.AI', 'cs.CV']]
2024-05-27|$\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning|use synthetic data to transfer LoRA of a LLM to another one|https://arxiv.org/abs/2405.17258|[['Parameter Efficient', 'PEFT', 'Efficient Finetuning'], ['cs.AI', 'cs.LG']]
2024-05-27|Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention|new linear attention design for efficient training and inference|https://arxiv.org/abs/2405.17381|[['cs.CL'], ['ICML']]
2024-05-27|MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance|prevent gradient conflict when using multimodal objectives|https://arxiv.org/abs/2405.17730|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-05-27|From Obstacle to Opportunity: Enhancing Semi-supervised Learning with Synthetic Data|taking synthetic data on Web into account when performing semi supervised learning|https://arxiv.org/abs/2405.16930|[['cs.CV']]
2024-05-27|VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models|VLM that do reasoning by object grounding boxes|https://arxiv.org/abs/2405.16919|[['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-27|Compositional Few-Shot Class-Incremental Learning|decompose a class into sub concepts and construct class concept by these prototypes|https://arxiv.org/abs/2405.17022|[['cs.AI', 'cs.CV']]
2024-05-27|Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization|dataset that challenges tokenization process of LLM|https://arxiv.org/abs/2405.17067|[['cs.AI', 'cs.CL']]
2024-05-27|Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective|unified analysis for linear complexity sequence model|https://arxiv.org/abs/2405.17383|[['cs.CL']]
2024-05-28|Low-Rank Few-Shot Adaptation of Vision-Language Models|use LoRA in CLIP for few shot adaptation|https://arxiv.org/abs/2405.18541|[['Parameter-Efficient', 'PEFT', 'Efficient Fine-Tuning'], ['Vision-Language', 'VLMs'], ['cs.CV']]
2024-05-28|Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment|use prediction results of VLM with or without image input which idicates the text image correlation as weights for loss|https://arxiv.org/abs/2405.17871|[['Vision Language', 'VLMs'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-05-28|VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections|compress intermediate activateion to reduce memory usage during fine tuning|https://arxiv.org/abs/2405.17991|[['PEFT'], ['cs.AI', 'cs.CV']]
2024-05-28|IAPT: Instruction-Aware Prompt Tuning for Large Language Models|prompt generator to generate soft prompts for each layer based on input|https://arxiv.org/abs/2405.18203|[['parameter-efficient', 'efficient fine-tuning'], ['cs.CL']]
2024-05-28|Frustratingly Easy Test-Time Adaptation of Vision-Language Models|test time adaptation by augmentation, confidence filtering, marginalize after setting temperature to zero|https://arxiv.org/abs/2405.18330|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.CV']]
2024-05-28|OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning|dynamically select layers to train and project gradient to low rank space to reduce memory usage|https://arxiv.org/abs/2405.18380|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-05-28|WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization|use embedding for class name and image to find domain specific feature|https://arxiv.org/abs/2405.18405|[['vision-language'], ['cs.AI', 'cs.CV']]
2024-05-28|Why are Visually-Grounded Language Models Bad at Image Classification?|LLM with visual input do worse than CLIP on classification|https://arxiv.org/abs/2405.18415|[['VLMs'], ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL']]
2024-05-28|ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention|linear attention model designed for vision|https://arxiv.org/abs/2405.18425|[['GPU memory'], ['cs.AI', 'cs.CV']]
2024-05-28|Mitigating Object Hallucination via Data Augmented Contrastive Tuning|as title|https://arxiv.org/abs/2405.18654|[['vision-language'], ['cs.CV']]
2024-05-28|LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification|generate hierarchical visual concept by GPT4V and use text features of different level to do classification|https://arxiv.org/abs/2405.18672|[['vision-language'], ['cs.CV', 'cs.CL']]
2024-05-28|Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model|new disign of architecture for multimodal mamba|https://arxiv.org/abs/2405.18014|[['GPU memory'], ['cs.AI']]
2024-05-28|Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities|fuse two LLM (discrete speech token and text) by cross attention|https://arxiv.org/abs/2405.18669|[['cs.AI', 'cs.LG', 'cs.CL', 'eess.AS'], ['NeurIPS']]
2024-05-28|Provable Contrastive Continual Learning|as title|https://arxiv.org/abs/2405.18756|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-05-28|FocSAM: Delving Deeply into Focused Objects in Segmenting Anything|dynamically focus on query region to improve SAM in interative scenario|https://arxiv.org/abs/2405.18706|[['cs.CV'], ['CVPR']]
2024-05-28|OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision|pseudo label for unknown object in traning data to prevent detector bias on seen categories|https://arxiv.org/abs/2405.17913|[['cs.AI', 'cs.CV']]
2024-05-28|The Evolution of Multimodal Model Architectures|surevy paper for model architectures of MLLM|https://arxiv.org/abs/2405.17927|[['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'eess.AS']]
2024-05-28|DMT-JEPA: Discriminative Masked Targets for Joint-Embedding Predictive Architecture|region level (clustered by feature similarity) MAE|https://arxiv.org/abs/2405.17995|[['cs.AI', 'cs.LG', 'cs.CV', 'eess.IV']]
2024-05-28|MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution|different size of patch embedding kernel for images of different resolution|https://arxiv.org/abs/2405.18240|[['cs.CV']]
2024-05-28|Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass|use superposition of candidate embeddings to obtain multiple predictions from a single pass|https://arxiv.org/abs/2405.18400|[['cs.LG', 'cs.CL']]
2024-05-28|Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference|parallel decoding that accelerate inference without memory overhead|https://arxiv.org/abs/2405.18628|[['cs.LG', 'cs.CL']]
2024-06-13|PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation|LoRA finetune and distillation at the same time|https://arxiv.org/abs/2406.09117|[['parameter-efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.CV'], ['CVPR']]
2024-06-13|Towards Multilingual Audio-Visual Question Answering|multilingual audio visual QA and its multi branch baseline|https://arxiv.org/abs/2406.09156|[['Audio-Visual'], ['cs.LG', 'cs.CV', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-13|MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning|decompose the pretrained weights, only tune the component with smaller singular values|https://arxiv.org/abs/2406.09044|[['Parameter-Efficient', 'Efficient finetuning'], ['cs.CL']]
2024-06-13|Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA|use clustering and key frame selection for efficient long video processing|https://arxiv.org/abs/2406.09396|[['vision language', 'VLMs'], ['cs.CV']]
2024-06-13|AdaPTwin: Low-Cost Adaptive Compression of Product Twins in Transformers|weight compression with initialization from SVD|https://arxiv.org/abs/2406.08904|[['cs.LG', 'cs.SD', 'eess.AS'], ['NeurIPS']]
2024-06-13|LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks|continual pretraining for speech SSL model with alignment loss|https://arxiv.org/abs/2406.09153|[['cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-13|Adaptive Slot Attention: Object Discovery with Dynamic Slot Number|object number aware designed attention for object centric learning|https://arxiv.org/abs/2406.09196|[['cs.LG', 'cs.CV'], ['CVPR']]
2024-06-13|Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations|measurement for speaker and phone representation in speech SSL models|https://arxiv.org/abs/2406.09200|[['cs.CL'], ['Interspeech']]
2024-06-13|Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection|use modality specific LoRA to adapt LLM to multi modalities inputs (text, video, audio) while being robust to missing modality|https://arxiv.org/abs/2406.09617|[['cs.CL', 'eess.AS'], ['Interspeech']]
2024-06-13|Exploring Training on Heterogeneous Data with Mixture of Low-rank Adapters|MoE design for multiple input domain, output task, modality|https://arxiv.org/abs/2406.09679|[['cs.CV'], ['ICML']]
2024-06-13|Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning|architecture design for multilingual unseen speaker speech emotion recogonition|https://arxiv.org/abs/2406.08931|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-06-13|ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models|selective quantization and routing for memory efficient MoE|https://arxiv.org/abs/2406.09041|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-13|Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs|fine tune with retionale generated by tree of thought to improve chain of thought ability|https://arxiv.org/abs/2406.09136|[['cs.LG', 'cs.CL']]
2024-06-13|Reducing Task Discrepancy of Text Encoders for Zero-Shot Composed Image Retrieval|use text feature with noise to mimic image feature in CLIP for composed image retrieval (retrieval based on both image and text)|https://arxiv.org/abs/2406.09188|[['cs.CV']]
2024-06-13|MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction|refine quantization model by reconstructing fulle precision model|https://arxiv.org/abs/2406.09229|[['cs.CV']]
2024-06-13|You Don't Need Data-Augmentation in Self-Supervised Learning|argue that SSL in vision (DINO) don't need data augmentation as long as dataset is large enough|https://arxiv.org/abs/2406.09294|[['cs.LG', 'cs.CV']]
2024-06-13|DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding|use discrete unit for speech lanugae model|https://arxiv.org/abs/2406.09345|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-06-13|Improving Autoregressive Training with Dynamic Oracles|training technique for discrepancy generated from teacher forcing and target variation in autoregressive model|https://arxiv.org/abs/2406.09393|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-13|4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities|encoder decoder model for a lot of task and modalities (text, depth, rgb, edge...), training on feature map of SOTA models, pseudo labels...|https://arxiv.org/abs/2406.09406|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-13|Explore the Limits of Omni-modal Pretraining at Scale|pretraining with paired text, audio video data|https://arxiv.org/abs/2406.09412|[['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-13|VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding|use both image and video as visual input for VLM|https://arxiv.org/abs/2406.09418|[['cs.CV']]
2024-06-12|KernelWarehouse: Rethinking the Design of Dynamic Convolution|dynamic (input dependent) convolution kernel with shared component to reduce number of parameters|https://arxiv.org/abs/2406.07879|[['parameter efficient'], ['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-06-12|Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models|token sampler to reducen token length of videos, highlight importance of video instruction data (temporal understanding)|https://arxiv.org/abs/2406.08024|[['Vision-Language'], ['cs.AI', 'cs.CV']]
2024-06-12|VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks|VLM that is able to use other visual task expert by predicting special routing token to handle different task|https://arxiv.org/abs/2406.08394|[['Vision-Language'], ['cs.CV']]
2024-06-12|The Impact of Initialization on LoRA Finetuning Dynamics|theoretical analysis for initialization of LoRA, we should initialize the first weights to random and the second one to 0|https://arxiv.org/abs/2406.08447|[['PEFT'], ['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-12|VLind-Bench: Measuring Language Priors in Large Vision-Language Models|measuring whether VLM response based solely on textual and disregarding image|https://arxiv.org/abs/2406.08702|[['Vision-Language'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-12|Updating CLIP to Prefer Descriptions Over Captions|pinoint the difference between description (same information as image) and caption (complement information in image), fine-tune CLIP to prefer description|https://arxiv.org/abs/2406.09458|[['parameter efficient', 'efficient fine-tuning'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-12|Small Scale Data-Free Knowledge Distillation|improve process of sampling synthetic data for data free knowledge distillation|https://arxiv.org/abs/2406.07876|[['training efficiency'], ['synthesize'], ['cs.AI', 'cs.LG', 'cs.CV'], ['CVPR']]
2024-06-12|Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations|train several encoder for different features by contrastive learning to better ensemble all features|https://arxiv.org/abs/2406.07900|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-12|Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation|reuse layers in teacher model as student model to prevent alignment disagreement of CTC model|https://arxiv.org/abs/2406.07909|[['cs.CL', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-12|Self-Supervised Speech Representations are More Phonetic than Semantic|semantically similar audio show higher similarity than phonetically similar audio under speech SSL model, suggest that intent classification dataset is no adequate to measure semantic feature|https://arxiv.org/abs/2406.08619|[['cs.LG', 'cs.CL', 'eess.AS'], ['Interspeech']]
2024-06-12|An Empirical Study of Mamba-based Language Models|benchmark transformer, mamba, and hybrid model with same amount of training data|https://arxiv.org/abs/2406.07887|[['cs.LG', 'cs.CL']]
2024-06-12|LVBench: An Extreme Long Video Understanding Benchmark|as title|https://arxiv.org/abs/2406.08035|[['cs.AI', 'cs.CV']]
2024-06-12|Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking|adaptively predict whether to bypass a transformer block to speedup inference|https://arxiv.org/abs/2406.08037|[['cs.CV']]
2024-06-12|Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams|highlight the online scenario of video understanding (causality during watching video), memory based method and a benchmark|https://arxiv.org/abs/2406.08085|[['cs.CV']]
2024-06-12|What If We Recaption Billions of Web Images with LLaMA-3?|recaption image text pair to improve data quality for VLM|https://arxiv.org/abs/2406.08478|[['vision-language'], ['Diffusion', 'text-to-image'], ['cs.CV', 'cs.CL']]
2024-06-12|OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text|large scale document dataset with interleaved image and text|https://arxiv.org/abs/2406.08418|[['cs.AI', 'cs.CV']]
2024-06-12|Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models|text input dependent patch selector to deal with high resolution images|https://arxiv.org/abs/2406.08487|[['cs.CV']]
2024-06-12|Adaptive Teaching with Shared Classifier for Knowledge Distillation|knowledge distillation where teacher model adjust to better align with learning needs of student, can be applied to multiple teacher models scenario|https://arxiv.org/abs/2406.08528|[['cs.LG', 'cs.CV']]
2024-06-17|BaFTA: Backprop-Free Test-Time Adaptation For Zero-Shot Vision-Language Models|test time adaptation that utilize augmentation and clustering|https://arxiv.org/abs/2406.11309|[['Vision-Language'], ['cs.CV'], ['ICLR']]
2024-06-17|Mining Open Semantics from CLIP: A Relation Transition Perspective for Few-Shot Learning|learn a set of anchor concept (class), when encountering other class, consider its relation (similarity) with anchor to do prediction|https://arxiv.org/abs/2406.11252|[['Vision-Language'], ['cs.CV']]
2024-06-17|ClawMachine: Fetching Visual Tokens as An Entity for Referring and Grounding|make MLM directly output visual feature and retrieve from the input patches to handle grounding and referring problems|https://arxiv.org/abs/2406.11327|[['vision-language'], ['cs.CV']]
2024-06-17|Unveiling Encoder-Free Vision-Language Models|VLM without visual encoder to deal with flexible resolution and aspect ration input, start from LLM and align with visual encoder|https://arxiv.org/abs/2406.11832|[['Vision-Language', 'VLMs'], ['cs.CV']]
2024-06-17|MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs|as title|https://arxiv.org/abs/2406.11833|[['Vision-Language'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-17|MCSD: An Efficient Language Model with Diverse Fusion|architecture design for linear time complexity and constant space complexity sequence modeling|https://arxiv.org/abs/2406.12230|[['GPU memory'], ['cs.AI', 'cs.CL']]
2024-06-17|TroL: Traversal of Layers for Large Language and Vision Models|apply a transformer block multiple times and aggregate all the results|https://arxiv.org/abs/2406.12246|[['vision language'], ['cs.LG', 'cs.CV', 'cs.CL']]
2024-06-17|Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts|weight the data sampling rate based on MoE's gate load|https://arxiv.org/abs/2406.11256|[['cs.CL']]
2024-06-17|ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking|compression training checkpoints by comparing adjacent checkpoints and evaluate weight importance by optimizer|https://arxiv.org/abs/2406.11257|[['cs.LG'], ['ICML']]
2024-06-17|MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens|as title|https://arxiv.org/abs/2406.11271|[['cs.LG', 'cs.CV']]
2024-06-17|Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers|queries in a layer can interact with keys and values from a preceding layer, improve the ability to capture dependencies bewteen high and low level feature|https://arxiv.org/abs/2406.11274|[['cs.CL']]
2024-06-17|Meta Reasoning for Large Language Models|make LLM planning how to reason before reasoning|https://arxiv.org/abs/2406.11698|[['cs.CL']]
2024-06-17|GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities|audio language model uses q-former, MLP, multi level cross attention as a connector between and audio encoder LLM|https://arxiv.org/abs/2406.11768|[['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS']]
2024-06-16|Concept-skill Transferability-based Data Selection for Large Vision-Language Models|use a small VLM to do clustering, consider similarity between concept clusters, scheduled sampling from each cluster|https://arxiv.org/abs/2406.10995|[['efficient finetuning'], ['Vision-Language'], ['cs.LG', 'cs.CV']]
2024-06-16|VELOCITI: Can Video-Language Models Bind Semantic Concepts through Time?|benchmark for VLM to asscociate entities through appropriate relationships|https://arxiv.org/abs/2406.10889|[['vision-language'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-16|Few-Shot Recognition via Stage-Wise Augmented Finetuning|retrieve pretraining data based on few shot examples and finetune on examples and retrieved data|https://arxiv.org/abs/2406.11148|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV']]
2024-06-16|On the Effectiveness of Supervision in Asymmetric Non-Contrastive Learning|add supervision to BYOL, theoreticaal insight suggest it will reduce intra-class variance|https://arxiv.org/abs/2406.10815|[['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-06-16|Pick-or-Mix: Dynamic Channel Sampling for ConvNets|dynamically choose channel to use to speedup forward pass|https://arxiv.org/abs/2406.10935|[['cs.CV'], ['CVPR']]
2024-06-16|NAST: Noise Aware Speech Tokenization for Speech Language Models|as title|https://arxiv.org/abs/2406.11037|[['cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-16|Fine-grained Classes and How to Find Them|finding fine grained classes from coarse labels|https://arxiv.org/abs/2406.11070|[['cs.LG', 'cs.CV'], ['ICML']]
2024-06-16|Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation|besides using CLIP for pseudo label, use it as backbone, pseudo label from activation map|https://arxiv.org/abs/2406.11189|[['cs.CV'], ['CVPR']]
2024-06-16|Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved Tags|suggest that image connector discard fine grained detailes in images, retrieve tags by images to improve performance|https://arxiv.org/abs/2406.10839|[['cs.CV', 'cs.CL']]
2024-06-15|ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation|share LoRA weights across different layers|https://arxiv.org/abs/2406.10785|[['Parameter Efficient', 'PEFT'], ['cs.AI', 'cs.CL']]
2024-06-15|Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference|selectively load KV cache based on key values to speedup inference|https://arxiv.org/abs/2406.10774|[['cs.LG', 'cs.CL'], ['ICML']]
2024-06-15|Optimization-based Structural Pruning for Large Language Models without Back-Propagation|use policy gradient to learn a weight mask for pruning with respect to LLM loss|https://arxiv.org/abs/2406.10576|[['cs.LG', 'cs.CL']]
2024-06-15|Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models|observe that prompt gains more attention and more stable attention distribution are more generalizable, design objective toward this goal during prompt tuning|https://arxiv.org/abs/2406.10584|[['cs.CL']]
2024-06-15|BlockPruner: Fine-grained Pruning for Large Language Models|LLM pruning at MHA and MLP level, training free, based on perplexity|https://arxiv.org/abs/2406.10594|[['cs.CL']]
2024-06-15|CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training|data selection by Basyian optimization problem|https://arxiv.org/abs/2406.10670|[['cs.AI', 'cs.LG', 'cs.CL']]
2024-06-15|SemanticMIM: Marring Masked Image Modeling with Semantics Compression for General Visual Representation|in masked image modeling, a set of learnable tokens (information bottleneck) are passed to decoder rather than encoded patch features|https://arxiv.org/abs/2406.10673|[['cs.CV']]
2024-06-15|RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning|sparse LoRA that is suitable for knowledge editting (only few weights will be modified)|https://arxiv.org/abs/2406.10777|[['Parameter-efficient', 'PEFT', 'efficient fine-tuning'], ['Knowledge Editing'], ['cs.CL']]
2024-06-14|Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation|integrate visual features into Whisper for audio visual speech recognition|https://arxiv.org/abs/2406.10082|[['Audio-Visual'], ['cs.CV', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-14|Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data|intra and inter class confidence thresholding for pseudolabeling to alleviate impact of incorrect hard pseudolabel|https://arxiv.org/abs/2406.10502|[['Vision-Language', 'VLMs'], ['cs.AI', 'cs.LG', 'cs.CV'], ['ICML']]
2024-06-14|HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning|only preserve entries with higher estimated score in the attention matrix to reduce complexity, the score is estimated by a tree search like algorithm|https://arxiv.org/abs/2406.09827|[['GPU memory'], ['cs.LG', 'cs.CV', 'cs.CL']]
2024-06-14|VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models|dataset to challenge VLM's ability to understand documents with interleaved images and text|https://arxiv.org/abs/2406.10228|[['Vision-Language'], ['cs.AI', 'cs.CV', 'cs.CL']]
2024-06-14|Open-Vocabulary Semantic Segmentation with Image Embedding Balancing|weighted sum of frozen CLIP feature and finetuned feature for better generalizability|https://arxiv.org/abs/2406.09829|[['cs.CV'], ['CVPR']]
2024-06-14|ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers|merge tokens with high enough similarity|https://arxiv.org/abs/2406.09936|[['cs.CV'], ['CVPR']]
2024-06-14|Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations|panoptic segmentation aware of hierarchical relation between object and parts, use learnable query to represent each object and do part segmentation within each object|https://arxiv.org/abs/2406.10114|[['cs.CV'], ['CVPR']]
2024-06-14|One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model|nested network to avoid training models of different sizes for multiple times|https://arxiv.org/abs/2406.10160|[['cs.AI', 'cs.SD', 'eess.AS'], ['Interspeech']]
2024-06-14|When Will Gradient Regularization Be Harmful?|gradient regularization may be harmful during warmup|https://arxiv.org/abs/2406.09723|[['cs.AI', 'cs.LG'], ['ICML']]
2024-06-14|Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition|use history utterarnce to generate soft prompts for specific user to improve ASR performance|https://arxiv.org/abs/2406.09873|[['cs.AI', 'cs.SD', 'eess.AS']]
2024-06-14|What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark|benchmark speech emotion recognition performance when training on combined dataset (with different sampling methods) or only in distributon dataset|https://arxiv.org/abs/2406.09933|[['cs.AI', 'cs.LG', 'cs.SD']]
2024-06-14|Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection|training free method enable streaming ASR (predict before finish input) using Whisper, use attention weights to detect truncated words|https://arxiv.org/abs/2406.10052|[['cs.CL', 'cs.SD', 'eess.AS']]
2024-06-14|UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner|neural codec (built on LLM codebook) projecting audio to textual space enable in context learning of speech language model|https://arxiv.org/abs/2406.10056|[['cs.SD', 'eess.AS']]
2024-06-14|Localizing Events in Videos with Multimodal Queries|benchmark that use image (of different style) and text to do localization|https://arxiv.org/abs/2406.10079|[['cs.AI', 'cs.CV']]
2024-06-14|CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation|as title|https://arxiv.org/abs/2406.10462|[['cs.CV']]
2024-06-14|A Label is Worth a Thousand Images in Dataset Distillation|highlight the importance of soft labels assigned to synthetic data is more important (the way to synthesize data) during data distillation|https://arxiv.org/abs/2406.10485|[['cs.LG', 'cs.CV']]
