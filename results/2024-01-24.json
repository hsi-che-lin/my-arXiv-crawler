[
    {
        "paper id": "2401.13307",
        "abstract url": "https://arxiv.org/abs/2401.13307",
        "title": "ChatterBox: Multi-round Multimodal Referring and Grounding",
        "rating": 2,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this study, we establish a baseline for a new task named multimodal multi-round referring and grounding (MRG), opening up a promising direction for instance-level multimodal dialogues. We present a new benchmark and an efficient vision-language model for this purpose. The new benchmark, named CB-300K, spans challenges including multi-round dialogue, complex spatial relationships among multiple instances, and consistent reasoning, which are beyond those shown in existing benchmarks. The proposed model, named ChatterBox, utilizes a two-branch architecture to collaboratively handle vision and language tasks. By tokenizing instance regions, the language branch acquires the ability to perceive referential information. Meanwhile, ChatterBox feeds a query embedding in the vision branch to a token receiver for visual grounding. A two-stage optimization strategy is devised, making use of both CB-300K and auxiliary external data to improve the model's stability and capacity for instance-level understanding. Experiments show that ChatterBox outperforms existing models in MRG both quantitatively and qualitatively, paving a new path towards multimodal dialogue scenarios with complicated and precise interactions. Code, data, and model are available at: https://github.com/sunsmarterjie/ChatterBox.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "17 pages, 6 tables, 9 figurs. Code, data, and model are available at: https://github.com/sunsmarterjie/ChatterBox"
    },
    {
        "paper id": "2401.13613",
        "abstract url": "https://arxiv.org/abs/2401.13613",
        "title": "Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode",
        "rating": 2,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and computer vision for improved information retrieval in multimedia applications",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13249",
        "abstract url": "https://arxiv.org/abs/2401.13249",
        "title": "MOS-FAD: Improving Fake Audio Detection Via Automatic Mean Opinion Score Prediction",
        "rating": 1.5,
        "keywords": [
            [
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Automatic Mean Opinion Score (MOS) prediction is employed to evaluate the quality of synthetic speech. This study extends the application of predicted MOS to the task of Fake Audio Detection (FAD), as we expect that MOS can be used to assess how close synthesized speech is to the natural human voice. We propose MOS-FAD, where MOS can be leveraged at two key points in FAD: training data selection and model fusion. In training data selection, we demonstrate that MOS enables effective filtering of samples from unbalanced datasets. In the model fusion, our results demonstrate that incorporating MOS as a gating mechanism in FAD model fusion enhances overall performance.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted in ICASSP2024"
    },
    {
        "paper id": "2401.13260",
        "abstract url": "https://arxiv.org/abs/2401.13260",
        "title": "MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, ASR Error Detection, and ASR Error Correction",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "The prevalent approach in speech emotion recognition (SER) involves integrating both audio and textual information to comprehensively identify the speaker's emotion, with the text generally obtained through automatic speech recognition (ASR). An essential issue of this approach is that ASR errors from the text modality can worsen the performance of SER. Previous studies have proposed using an auxiliary ASR error detection task to adaptively assign weights of each word in ASR hypotheses. However, this approach has limited improvement potential because it does not address the coherence of semantic information in the text. Additionally, the inherent heterogeneity of different modalities leads to distribution gaps between their representations, making their fusion challenging. Therefore, in this paper, we incorporate two auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to enhance the semantic coherence of ASR text, and further introduce a novel multi-modal fusion (MF) method to learn shared representations across modalities. We refer to our method as MF-AED-AEC. Experimental results indicate that MF-AED-AEC significantly outperforms the baseline model by a margin of 4.1\\%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by ICASSP 2024"
    },
    {
        "paper id": "2401.13276",
        "abstract url": "https://arxiv.org/abs/2401.13276",
        "title": "SCNet: Sparse Compression Network for Music Source Separation",
        "rating": 1.5,
        "keywords": [
            [
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Deep learning-based methods have made significant achievements in music source separation. However, obtaining good results while maintaining a low model complexity remains challenging in super wide-band music source separation. Previous works either overlook the differences in subbands or inadequately address the problem of information loss when generating subband features. In this paper, we propose SCNet, a novel frequency-domain network to explicitly split the spectrogram of the mixture into several subbands and introduce a sparsity-based encoder to model different frequency bands. We use a higher compression ratio on subbands with less information to improve the information density and focus on modeling subbands with more information. In this way, the separation performance can be significantly improved using lower computational consumption. Experiment results show that the proposed model achieves a signal to distortion ratio (SDR) of 9.0 dB on the MUSDB18-HQ dataset without using extra data, which outperforms state-of-the-art methods. Specifically, SCNet's CPU inference time is only 48% of HT Demucs, one of the previous state-of-the-art models.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted by ICASSP 2024"
    },
    {
        "paper id": "2401.13313",
        "abstract url": "https://arxiv.org/abs/2401.13313",
        "title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI2024; project page: https://github.com/nttmdlab-nlp/InstructDoc"
    },
    {
        "paper id": "2401.13401",
        "abstract url": "https://arxiv.org/abs/2401.13401",
        "title": "Perceptually-motivated Spatial Audio Codec for Higher-Order Ambisonics Compression",
        "rating": 1.5,
        "keywords": [
            [
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Scene-based spatial audio formats, such as Ambisonics, are playback system agnostic and may therefore be favoured for delivering immersive audio experiences to a wide range of (potentially unknown) devices. The number of channels required to deliver high spatial resolution Ambisonic audio, however, can be prohibitive for low-bandwidth applications. Therefore, this paper proposes a compression codec, which is based upon the parametric higher-order Directional Audio Coding (HO-DirAC) model. The encoder downmixes the higher-order Ambisonic (HOA) input audio into a reduced number of signals, which are accompanied by perceptually-motivated scene parameters. The downmixed audio is coded using a perceptual audio coder, whereas the parameters are grouped into perceptual bands, quantized, and downsampled. On the decoder side, low Ambisonic orders are fully recovered. Not fully recoverable HOA components are synthesized according to the parameters. The results of a listening test indicate that the proposed parametric spatial audio codec can improve the adopted perceptual audio coder, especially at low to medium-high bitrates, when applied to fifth-order HOA signals.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted for publication in Proceedings of the 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2024)"
    },
    {
        "paper id": "2401.13463",
        "abstract url": "https://arxiv.org/abs/2401.13463",
        "title": "SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robust to speech recognition errors.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at ICASSP 2024"
    },
    {
        "paper id": "2401.13505",
        "abstract url": "https://arxiv.org/abs/2401.13505",
        "title": "Generative Human Motion Stylization in Latent Space",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. Building upon this, we present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. Our approach is versatile, allowing the learning of probabilistic style space from either style labeled or unlabeled motions, providing notable flexibility in stylization as well. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-art in style reenactment, content preservation, and generalization across various applications and settings. Project Page: https://murrol.github.io/GenMoStyle",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted for ICLR2024"
    },
    {
        "paper id": "2401.13611",
        "abstract url": "https://arxiv.org/abs/2401.13611",
        "title": "Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models",
        "rating": 1.5,
        "keywords": [
            [
                "cs.SD"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Accepted paper. IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), Seoul, Korea, April 2024"
    },
    {
        "paper id": "2401.13621",
        "abstract url": "https://arxiv.org/abs/2401.13621",
        "title": "DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Contrastive-learning-based methods have dominated sentence representation learning. These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks. However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples. In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective. By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form. Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in comparison to contrastive-learning-based methods. Notably, the proposed intra-sentence denoising objective complements existing inter-sentence contrastive methodologies and can be integrated with them to further enhance performance. Our code is available at https://github.com/xinghaow99/DenoSent.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "AAAI 2024"
    },
    {
        "paper id": "2401.13837",
        "abstract url": "https://arxiv.org/abs/2401.13837",
        "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Identifying subordinate-level categories from images is a longstanding task in computer vision and is referred to as fine-grained visual recognition (FGVR). It has tremendous significance in real-world applications since an average layperson does not excel at differentiating species of birds or mushrooms due to subtle differences among the species. A major bottleneck in developing FGVR systems is caused by the need of high-quality paired expert annotations. To circumvent the need of expert knowledge we propose Fine-grained Semantic Category Reasoning (FineR) that internally leverages the world knowledge of large language models (LLMs) as a proxy in order to reason about fine-grained category names. In detail, to bridge the modality gap between images and LLM, we extract part-level visual attributes from images as text and feed that information to a LLM. Based on the visual attributes and its internal world knowledge the LLM reasons about the subordinate-level category names. Our training-free FineR outperforms several state-of-the-art FGVR and language and vision assistant models and shows promise in working in the wild and in new domains where gathering expert annotation is arduous.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted as a conference paper at ICLR 2024; Project page: https://projfiner.github.io/"
    },
    {
        "paper id": "2401.14289",
        "abstract url": "https://arxiv.org/abs/2401.14289",
        "title": "Speech foundation models on intelligibility prediction for hearing-impaired listeners",
        "rating": 1.5,
        "keywords": [
            [
                "cs.SD"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "To be presented in ICASSP 2024"
    },
    {
        "paper id": "2401.13229",
        "abstract url": "https://arxiv.org/abs/2401.13229",
        "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To address these issues, this paper contributes an automatic and informed data selection architecture to build a small dataset for few-shot learning. Our proposal minimizes the quantity and maximizes diversity of data selected for human annotation, while improving model performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at PROPOR 2024 - The 16th International Conference on Computational Processing of Portuguese"
    },
    {
        "paper id": "2401.13246",
        "abstract url": "https://arxiv.org/abs/2401.13246",
        "title": "SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Elucidating the reasoning process with structured explanations from question to answer is crucial, as it significantly enhances the interpretability, traceability, and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricately structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Moreover, existing reinforcement learning (RL) based methods overlook the structured relationships, underutilizing the potential of RL in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between different reasoning steps. In addition, we introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that SEER significantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9% over RL-based methods on EntailmentBank, a 4.4% average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Ongoing Work"
    },
    {
        "paper id": "2401.13256",
        "abstract url": "https://arxiv.org/abs/2401.13256",
        "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13264",
        "abstract url": "https://arxiv.org/abs/2401.13264",
        "title": "Enhancing cross-domain detection: adaptive class-aware contrastive transformer",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently,the detection transformer has gained substantial attention for its inherent minimal post-processing requirement.However,this paradigm relies on abundant training data,yet in the context of the cross-domain adaptation,insufficient labels in the target domain exacerbate issues of class imbalance and model performance degradation.To address these challenges, we propose a novel class-aware cross domain detection transformer based on the adversarial learning and mean-teacher framework.First,considering the inconsistencies between the classification and regression tasks,we introduce an IoU-aware prediction branch and exploit the consistency of classification and location scores to filter and reweight pseudo labels.Second, we devise a dynamic category threshold refinement to adaptively manage model confidence.Third,to alleviate the class imbalance,an instance-level class-aware contrastive learning module is presented to encourage the generation of discriminative features for each class,particularly benefiting minority classes.Experimental results across diverse domain-adaptive scenarios validate our method's effectiveness in improving performance and alleviating class imbalance issues,which outperforms the state-of-the-art transformer based methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Acceptd by Icassp 2024"
    },
    {
        "paper id": "2401.13270",
        "abstract url": "https://arxiv.org/abs/2401.13270",
        "title": "Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representation is fed into the pretrained network to finally realize the audio-guided colorization. The whole process is trained in a self-supervised manner without human annotation. In addition, an audiovisual colorization dataset is established for training and testing. Experiments demonstrate that audio guidance can effectively improve the performance of automatic colorization, especially for some scenes that are difficult to understand only from visual modality.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13275",
        "abstract url": "https://arxiv.org/abs/2401.13275",
        "title": "Can AI Assistants Know What They Don't Know?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question \"Can AI assistants know what they don't know and express them through natural language?\" To answer this question, we construct a model-specific \"I don't know\" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding Idk dataset and observe whether it can refuse to answer its unknown questions after alignment. Experimental results show that after alignment with Idk datasets, the assistant can refuse to answer most its unknown questions. For questions they attempt to answer, the accuracy is significantly higher than before the alignment.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2401.13296",
        "abstract url": "https://arxiv.org/abs/2401.13296",
        "title": "Visual Objectification in Films: Towards a New AI Task for Video Interpretation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In film gender studies, the concept of 'male gaze' refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 3 figures, 2 tables"
    },
    {
        "paper id": "2401.13298",
        "abstract url": "https://arxiv.org/abs/2401.13298",
        "title": "Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. However, existing harmful meme detection methods do not present readable explanations that unveil such implicit meaning to support their detection decisions. In this paper, we propose an explainable approach to detect harmful memes, achieved through reasoning over conflicting rationales from both harmless and harmful positions. Specifically, inspired by the powerful capacity of Large Language Models (LLMs) on text generation and reasoning, we first elicit multimodal debate between LLMs to generate the explanations derived from the contradictory arguments. Then we propose to fine-tune a small language model as the debate judge for harmfulness inference, to facilitate multimodal fusion between the harmfulness rationales and the intrinsic multimodal information within memes. In this way, our model is empowered to perform dialectical reasoning over intricate and implicit harm-indicative patterns, utilizing multimodal explanations originating from both harmless and harmful arguments. Extensive experiments on three public meme datasets demonstrate that our harmful meme detection approach achieves much better performance than state-of-the-art methods and exhibits a superior capacity for explaining the meme harmfulness of the model predictions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "The first work towards explainable harmful meme detection by harnessing advanced LLMs"
    },
    {
        "paper id": "2401.13303",
        "abstract url": "https://arxiv.org/abs/2401.13303",
        "title": "MaLA-500: Massive Language Adaptation of Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our intrinsic evaluation demonstrates that MaLA-500 is better at predicting the given texts of low-resource languages than existing multilingual LLMs. Moreover, the extrinsic evaluation of in-context learning shows that MaLA-500 outperforms previous LLMs on SIB200 and Taxi1500 by a significant margin, i.e., 11.68% and 4.82% marco-average accuracy across languages. We release MaLA-500 at https://huggingface.co/MaLA-LM",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13325",
        "abstract url": "https://arxiv.org/abs/2401.13325",
        "title": "Memory Consistency Guided Divide-and-Conquer Learning for Generalized Category Discovery",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generalized category discovery (GCD) aims at addressing a more realistic and challenging setting of semi-supervised learning, where only part of the category labels are assigned to certain training samples. Previous methods generally employ naive contrastive learning or unsupervised clustering scheme for all the samples. Nevertheless, they usually ignore the inherent critical information within the historical predictions of the model being trained. Specifically, we empirically reveal that a significant number of salient unlabeled samples yield consistent historical predictions corresponding to their ground truth category. From this observation, we propose a Memory Consistency guided Divide-and-conquer Learning framework (MCDL). In this framework, we introduce two memory banks to record historical prediction of unlabeled data, which are exploited to measure the credibility of each sample in terms of its prediction consistency. With the guidance of credibility, we can design a divide-and-conquer learning strategy to fully utilize the discriminative information of unlabeled data while alleviating the negative influence of noisy labels. Extensive experimental results on multiple benchmarks demonstrate the generality and superiority of our method, where our method outperforms state-of-the-art models by a large margin on both seen and unseen classes of the generic image recognition and challenging semantic shift settings (i.e.,with +8.4% gain on CUB and +8.1% on Standford Cars).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13329",
        "abstract url": "https://arxiv.org/abs/2401.13329",
        "title": "Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval",
        "rating": 1,
        "keywords": [
            [
                "visual-language"
            ],
            [
                "Diffusion",
                "video editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video Moment Retrieval (VMR) requires precise modelling of fine-grained moment-text associations to capture intricate visual-language relationships. Due to the lack of a diverse and generalisable VMR dataset to facilitate learning scalable moment-text associations, existing methods resort to joint training on both source and target domain videos for cross-domain applications. Meanwhile, recent developments in vision-language multimodal models pre-trained on large-scale image-text and/or video-text pairs are only based on coarse associations (weakly labelled). They are inadequate to provide fine-grained moment-text correlations required for cross-domain VMR. In this work, we solve the problem of unseen cross-domain VMR, where certain visual and textual concepts do not overlap across domains, by only utilising target domain sentences (text prompts) without accessing their videos. To that end, we explore generative video diffusion for fine-grained editing of source videos controlled by the target sentences, enabling us to simulate target domain videos. We address two problems in video editing for optimising unseen domain VMR: (1) generation of high-quality simulation videos of different moments with subtle distinctions, (2) selection of simulation videos that complement existing source training videos without introducing harmful noise or unnecessary repetitions. On the first problem, we formulate a two-stage video diffusion generation controlled simultaneously by (1) the original video structure of a source video, (2) subject specifics, and (3) a target sentence prompt. This ensures fine-grained variations between video moments. On the second problem, we introduce a hybrid selection mechanism that combines two quantitative metrics for noise filtering and one qualitative metric for leveraging VMR prediction on simulation video selection.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13432",
        "abstract url": "https://arxiv.org/abs/2401.13432",
        "title": "Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation cost, we develop a semi-supervised learning scheme to improve warping quality by exploiting unlabeled data. It is formulated through dual transformation between the searched control points of unlabeled data and its graphic augmentation, yielding an implicit correction consistency constraint. Finally, we collect massive unlabeled data to exhibit the benefit of our semi-supervised scheme in rotation correction. Extensive experiments demonstrate the superiority and universality of CoupledTPS over the existing state-of-the-art (SoTA) solutions for rotation correction and beyond. The code and data will be available at https://github.com/nie-lang/CoupledTPS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13444",
        "abstract url": "https://arxiv.org/abs/2401.13444",
        "title": "Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs with fewer parameters. In some instances, even ChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4. Furthermore, the results indicate a minimal invocation frequency of CGPE on LLMs, suggesting reduced computational overhead. For organizations and individuals facing constraints in computational resources, our research offers significant practical value.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13478",
        "abstract url": "https://arxiv.org/abs/2401.13478",
        "title": "SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval",
        "rating": 1,
        "keywords": [
            [
                "visual language"
            ]
        ],
        "abstract": "Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the influence of the visual and textual encoders. All our data and checkpoints are publicly available at https://github.com/Wusiwei0410/SciMMIR.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13499",
        "abstract url": "https://arxiv.org/abs/2401.13499",
        "title": "LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Few-shot image classification has emerged as a key challenge in the field of computer vision, highlighting the capability to rapidly adapt to new tasks with minimal labeled data. Existing methods predominantly rely on image-level features or local descriptors, often overlooking the holistic context surrounding these descriptors. In this work, we introduce a novel approach termed \"Local Descriptor with Contextual Augmentation (LDCA)\". Specifically, this method bridges the gap between local and global understanding uniquely by leveraging an adaptive global contextual enhancement module. This module incorporates a visual transformer, endowing local descriptors with contextual awareness capabilities, ranging from broad global perspectives to intricate surrounding nuances. By doing so, LDCA transcends traditional descriptor-based approaches, ensuring each local feature is interpreted within its larger visual narrative. Extensive experiments underscore the efficacy of our method, showing a maximal absolute improvement of 20\\% over the next-best on fine-grained classification datasets, thus demonstrating significant advancements in few-shot classification tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13503",
        "abstract url": "https://arxiv.org/abs/2401.13503",
        "title": "Learning Representations for Clustering via Partial Information Discrimination and Cross-Level Interaction",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we present a novel deep image clustering approach termed PICI, which enforces the partial information discrimination and the cross-level interaction in a joint learning framework. In particular, we leverage a Transformer encoder as the backbone, through which the masked image modeling with two paralleled augmented views is formulated. After deriving the class tokens from the masked images by the Transformer encoder, three partial information learning modules are further incorporated, including the PISD module for training the auto-encoder via masked image reconstruction, the PICD module for employing two levels of contrastive learning, and the CLI module for mutual interaction between the instance-level and cluster-level subspaces. Extensive experiments have been conducted on six real-world image datasets, which demononstrate the superior clustering performance of the proposed PICI approach over the state-of-the-art deep clustering approaches. The source code is available at https://github.com/Regan-Zhang/PICI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13504",
        "abstract url": "https://arxiv.org/abs/2401.13504",
        "title": "Research about the Ability of LLM in the Tamper-Detection Area",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, particularly since the early 2020s, Large Language Models (LLMs) have emerged as the most powerful AI tools in addressing a diverse range of challenges, from natural language processing to complex problem-solving in various domains. In the field of tamper detection, LLMs are capable of identifying basic tampering activities.To assess the capabilities of LLMs in more specialized domains, we have collected five different LLMs developed by various companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. This diverse range of models allows for a comprehensive evaluation of their performance in detecting sophisticated tampering instances.We devised two domains of detection: AI-Generated Content (AIGC) detection and manipulation detection. AIGC detection aims to test the ability to distinguish whether an image is real or AI-generated. Manipulation detection, on the other hand, focuses on identifying tampered images. According to our experiments, most LLMs can identify composite pictures that are inconsistent with logic, and only more powerful LLMs can distinguish logical, but visible signs of tampering to the human eye. All of the LLMs can't identify carefully forged images and very realistic images generated by AI. In the area of tamper detection, LLMs still have a long way to go, particularly in reliably identifying highly sophisticated forgeries and AI-generated images that closely mimic reality.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13527",
        "abstract url": "https://arxiv.org/abs/2401.13527",
        "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "work in progress"
    },
    {
        "paper id": "2401.13554",
        "abstract url": "https://arxiv.org/abs/2401.13554",
        "title": "PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present the PanAf20K dataset, the largest and most diverse open-access annotated video dataset of great apes in their natural environment. It comprises more than 7 million frames across ~20,000 camera trap videos of chimpanzees and gorillas collected at 14 field sites in tropical Africa as part of the Pan African Programme: The Cultured Chimpanzee. The footage is accompanied by a rich set of annotations and benchmarks making it suitable for training and testing a variety of challenging and ecologically important computer vision tasks including ape detection and behaviour recognition. Furthering AI analysis of camera trap information is critical given the International Union for Conservation of Nature now lists all species in the great ape family as either Endangered or Critically Endangered. We hope the dataset can form a solid basis for engagement of the AI community to improve performance, efficiency, and result interpretation in order to support assessments of great ape presence, abundance, distribution, and behaviour and thereby aid conservation efforts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at IJCV"
    },
    {
        "paper id": "2401.13581",
        "abstract url": "https://arxiv.org/abs/2401.13581",
        "title": "Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Previous contrastive deep clustering methods mostly focus on instance-level information while overlooking the member relationship within groups/clusters, which may significantly undermine their representation learning and clustering capability. Recently, some group-contrastive methods have been developed, which, however, typically rely on the samples of the entire dataset to obtain pseudo labels and lack the ability to efficiently update the group assignments in a batch-wise manner. To tackle these critical issues, we present a novel end-to-end deep clustering framework with dynamic grouping and prototype aggregation, termed as DigPro. Specifically, the proposed dynamic grouping extends contrastive learning from instance-level to group-level, which is effective and efficient for timely updating groups. Meanwhile, we perform contrastive learning on prototypes in a spherical feature space, termed as prototype aggregation, which aims to maximize the inter-cluster distance. Notably, with an expectation-maximization framework, DigPro simultaneously takes advantage of compact intra-cluster connections, well-separated clusters, and efficient group updating during the self-supervised training. Extensive experiments on six image benchmarks demonstrate the superior performance of our approach over the state-of-the-art. Code is available at https://github.com/Regan-Zhang/DigPro.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13598",
        "abstract url": "https://arxiv.org/abs/2401.13598",
        "title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by WWW 2024"
    },
    {
        "paper id": "2401.13601",
        "abstract url": "https://arxiv.org/abs/2401.13601",
        "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing $122$ MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2401.13616",
        "abstract url": "https://arxiv.org/abs/2401.13616",
        "title": "FLLIC: Functionally Lossless Image Compression",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Recently, DNN models for lossless image coding have surpassed their traditional counterparts in compression performance, reducing the bit rate by about ten percent for natural color images. But even with these advances, mathematically lossless image compression (MLLIC) ratios for natural images still fall short of the bandwidth and cost-effectiveness requirements of most practical imaging and vision systems at present and beyond. To break the bottleneck of MLLIC in compression performance, we question the necessity of MLLIC, as almost all digital sensors inherently introduce acquisition noises, making mathematically lossless compression counterproductive. Therefore, in contrast to MLLIC, we propose a new paradigm of joint denoising and compression called functionally lossless image compression (FLLIC), which performs lossless compression of optimally denoised images (the optimality may be task-specific). Although not literally lossless with respect to the noisy input, FLLIC aims to achieve the best possible reconstruction of the latent noise-free original image. Extensive experiments show that FLLIC achieves state-of-the-art performance in joint denoising and compression of noisy images and does so at a lower computational cost.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13660",
        "abstract url": "https://arxiv.org/abs/2401.13660",
        "title": "MambaByte: Token-free Selective State Space Model",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13666",
        "abstract url": "https://arxiv.org/abs/2401.13666",
        "title": "Algebraic methods for solving recognition problems with non-crossing classes",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we propose to consider various models of pattern recognition. At the same time, it is proposed to consider models in the form of two operators: a recognizing operator and a decision rule. Algebraic operations are introduced on recognizing operators, and based on the application of these operators, a family of recognizing algorithms is created. An upper estimate is constructed for the model, which guarantees the completeness of the extension.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "I will rework and improve it and post it again"
    },
    {
        "paper id": "2401.13721",
        "abstract url": "https://arxiv.org/abs/2401.13721",
        "title": "Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evidential Learning framework, which outputs both predictions and uncertainties for each input sample. We propose aligning the parameters of higher-order evidential distributions between the source and target domains using traditional alignment methods at the feature or posterior level. Additionally, we propose to augment the feature space representation by mixing source samples with pseudo-labeled target samples based on label similarity. This cross-domain mixing strategy produces more realistic samples than random mixing and introduces higher uncertainty, facilitating further alignment. We demonstrate the effectiveness of our approach on four benchmarks for UDAR, on which we outperform existing methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13766",
        "abstract url": "https://arxiv.org/abs/2401.13766",
        "title": "Bayesian adaptive learning to latent variables via Variational Bayes and Maximum a Posteriori",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "In this work, we aim to establish a Bayesian adaptive learning framework by focusing on estimating latent variables in deep neural network (DNN) models. Latent variables indeed encode both transferable distributional information and structural relationships. Thus the distributions of the source latent variables (prior) can be combined with the knowledge learned from the target data (likelihood) to yield the distributions of the target latent variables (posterior) with the goal of addressing acoustic mismatches between training and testing conditions. The prior knowledge transfer is accomplished through Variational Bayes (VB). In addition, we also investigate Maximum a Posteriori (MAP) based Bayesian adaptation. Experimental results on device adaptation in acoustic scene classification show that our proposed approaches can obtain good improvements on target devices, and consistently outperforms other cut-edging algorithms.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "ASRU2023 Bayesian Symposium. arXiv admin note: text overlap with arXiv:2110.08598"
    },
    {
        "paper id": "2401.13789",
        "abstract url": "https://arxiv.org/abs/2401.13789",
        "title": "A Unified Approach to Emotion Detection and Task-Oriented Dialogue Modeling",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In current text-based task-oriented dialogue (TOD) systems, user emotion detection (ED) is often overlooked or is typically treated as a separate and independent task, requiring additional training. In contrast, our work demonstrates that seamlessly unifying ED and TOD modeling brings about mutual benefits, and is therefore an alternative to be considered. Our method consists in augmenting SimpleToD, an end-to-end TOD system, by extending belief state tracking to include ED, relying on a single language model. We evaluate our approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ annotated with emotions. Our results reveal a general increase in performance for ED and task results. Our findings also indicate that user emotions provide useful contextual conditioning for system responses, and can be leveraged to further refine responses in terms of empathy.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted @ IWSDS 2024"
    },
    {
        "paper id": "2401.13849",
        "abstract url": "https://arxiv.org/abs/2401.13849",
        "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the student model begins making inferences, TPD requires no further intervention from the teacher LLM or humans. Through extensive experiments across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared to standard chain-of-thought prompting, TPD significantly improves the student model's performance, achieving $6.2\\%$ improvement on average.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13856",
        "abstract url": "https://arxiv.org/abs/2401.13856",
        "title": "LAA-Net: Localized Artifact Attention Network for High-Quality Deepfakes Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces a novel approach for high-quality deepfake detection called Localized Artifact Attention Network (LAA-Net). Existing methods for high-quality deepfake detection are mainly based on a supervised binary classifier coupled with an implicit attention mechanism. As a result, they do not generalize well to unseen manipulations. To handle this issue, two main contributions are made. First, an explicit attention mechanism within a multi-task learning framework is proposed. By combining heatmap-based and self-consistency attention strategies, LAA-Net is forced to focus on a few small artifact-prone vulnerable regions. Second, an Enhanced Feature Pyramid Network (E-FPN) is proposed as a simple and effective mechanism for spreading discriminative low-level features into the final feature output, with the advantage of limiting redundancy. Experiments performed on several benchmarks show the superiority of our approach in terms of Area Under the Curve (AUC) and Average Precision (AP). The code will be released soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13905",
        "abstract url": "https://arxiv.org/abs/2401.13905",
        "title": "Dynamic embedded topic models and change-point detection for exploring literary-historical hypotheses",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present a novel combination of dynamic embedded topic models and change-point detection to explore diachronic change of lexical semantic modality in classical and early Christian Latin. We demonstrate several methods for finding and characterizing patterns in the output, and relating them to traditional scholarship in Comparative Literature and Classics. This simple approach to unsupervised models of semantic change can be applied to any suitable corpus, and we conclude with future directions and refinements aiming to allow noisier, less-curated materials to meet that threshold.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LaTeCH@EACL2024"
    },
    {
        "paper id": "2401.13907",
        "abstract url": "https://arxiv.org/abs/2401.13907",
        "title": "No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Researchers recently found out that sometimes language models achieve high accuracy on benchmark data set, but they can not generalize very well with even little changes to the original data set. This is sometimes due to data artifacts, model is learning the spurious correlation between tokens and labels, instead of the semantics and logic. In this work, we analyzed SNLI data and visualized such spurious correlations. We proposed an adaptive up-sampling algorithm to correct the data artifacts, which is simple and effective, and does not need human edits or annotation. We did an experiment applying the algorithm to fix the data artifacts in SNLI data and the model trained with corrected data performed significantly better than the model trained with raw SNLI data, overall, as well as on the subset we corrected.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13919",
        "abstract url": "https://arxiv.org/abs/2401.13919",
        "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13921",
        "abstract url": "https://arxiv.org/abs/2401.13921",
        "title": "Intelli-Z: Toward Intelligible Zero-Shot TTS",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Although numerous recent studies have suggested new frameworks for zero-shot TTS using large-scale, real-world data, studies that focus on the intelligibility of zero-shot TTS are relatively scarce. Zero-shot TTS demands additional efforts to ensure clear pronunciation and speech quality due to its inherent requirement of replacing a core parameter (speaker embedding or acoustic prompt) with a new one at the inference stage. In this study, we propose a zero-shot TTS model focused on intelligibility, which we refer to as Intelli-Z. Intelli-Z learns speaker embeddings by using multi-speaker TTS as its teacher and is trained with a cycle-consistency loss to include mismatched text-speech pairs for training. Additionally, it selectively aggregates speaker embeddings along the temporal dimension to minimize the interference of the text content of reference speech at the inference stage. We substantiate the effectiveness of the proposed methods with an ablation study. The Mean Opinion Score (MOS) increases by 9% for unseen speakers when the first two methods are applied, and it further improves by 16% when selective temporal aggregation is applied.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13937",
        "abstract url": "https://arxiv.org/abs/2401.13937",
        "title": "Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "under review"
    },
    {
        "paper id": "2401.13942",
        "abstract url": "https://arxiv.org/abs/2401.13942",
        "title": "StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion Models",
        "rating": 1,
        "keywords": [
            [
                "Parameter Efficient"
            ],
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The ability to fine-tune generative models for text-to-image generation tasks is crucial, particularly facing the complexity involved in accurately interpreting and visualizing textual inputs. While LoRA is efficient for language model adaptation, it often falls short in text-to-image tasks due to the intricate demands of image generation, such as accommodating a broad spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a specialized fine-tuning approach tailored for text-to-image models. StyleInject comprises multiple parallel low-rank parameter matrices, maintaining the diversity of visual features. It dynamically adapts to varying styles by adjusting the variance of visual features based on the characteristics of the input signal. This approach significantly minimizes the impact on the original model's text-image alignment capabilities while adeptly adapting to various styles in transfer learning. StyleInject proves particularly effective in learning from and enhancing a range of advanced, community-fine-tuned generative models. Our comprehensive experiments, including both small-sample and large-scale data fine-tuning as well as base model distillation, show that StyleInject surpasses traditional LoRA in both text-image semantic consistency and human preference evaluation, all while ensuring greater parameter efficiency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "15 pages, 12 figures"
    },
    {
        "paper id": "2401.14426",
        "abstract url": "https://arxiv.org/abs/2401.14426",
        "title": "M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Uplift modeling is a technique used to predict the effect of a treatment (e.g., discounts) on an individual's response. Although several methods have been proposed for multi-valued treatment, they are extended from binary treatment methods. There are still some limitations. Firstly, existing methods calculate uplift based on predicted responses, which may not guarantee a consistent uplift distribution between treatment and control groups. Moreover, this may cause cumulative errors for multi-valued treatment. Secondly, the model parameters become numerous with many prediction heads, leading to reduced efficiency. To address these issues, we propose a novel \\underline{M}ulti-gate \\underline{M}ixture-of-Experts based \\underline{M}ulti-valued \\underline{T}reatment \\underline{N}etwork (M$^3$TN). M$^3$TN consists of two components: 1) a feature representation module with Multi-gate Mixture-of-Experts to improve the efficiency; 2) a reparameterization module by modeling uplift explicitly to improve the effectiveness. We also conduct extensive experiments to demonstrate the effectiveness and efficiency of our M$^3$TN.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICASSP 2024"
    },
    {
        "paper id": "2402.01697",
        "abstract url": "https://arxiv.org/abs/2402.01697",
        "title": "APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by showing how it can be extended to support additional tuning mechanisms.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by WWW 2024; Camera-ready version"
    },
    {
        "paper id": "2402.01698",
        "abstract url": "https://arxiv.org/abs/2402.01698",
        "title": "Large language model empowered participatory urban planning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "26 pages, 7 figures, 2 tables"
    },
    {
        "paper id": "2402.01706",
        "abstract url": "https://arxiv.org/abs/2402.01706",
        "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both effectiveness and efficiency. In addition, our results indicate that existing LLMs are extremely vulnerable to nesting worlds and programming language worlds. They imply that existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploited.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13239",
        "abstract url": "https://arxiv.org/abs/2401.13239",
        "title": "Adaptive Crowdsourcing Via Self-Supervised Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Common crowdsourcing systems average estimates of a latent quantity of interest provided by many crowdworkers to produce a group estimate. We develop a new approach -- predict-each-worker -- that leverages self-supervised learning and a novel aggregation scheme. This approach adapts weights assigned to crowdworkers based on estimates they provided for previous quantities. When skills vary across crowdworkers or their estimates correlate, the weighted sum offers a more accurate group estimate than the average. Existing algorithms such as expectation maximization can, at least in principle, produce similarly accurate group estimates. However, their computational requirements become onerous when complex models, such as neural networks, are required to express relationships among crowdworkers. Predict-each-worker accommodates such complexity as well as many other practical challenges. We analyze the efficacy of predict-each-worker through theoretical and computational studies. Among other things, we establish asymptotic optimality as the number of engagements per crowdworker grows.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "33 pages, 3 figures"
    },
    {
        "paper id": "2401.13248",
        "abstract url": "https://arxiv.org/abs/2401.13248",
        "title": "\"Here's Your Evidence\": False Consensus in Public Twitter Discussions of COVID-19 Science",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The COVID-19 pandemic brought about an extraordinary rate of scientific papers on the topic that were discussed among the general public, although often in biased or misinformed ways. In this paper, we present a mixed-methods analysis aimed at examining whether public discussions were commensurate with the scientific consensus on several COVID-19 issues. We estimate scientific consensus based on samples of abstracts from preprint servers and compare against the volume of public discussions on Twitter mentioning these papers. We find that anti-consensus posts and users, though overall less numerous than pro-consensus ones, are vastly over-represented on Twitter, thus producing a false consensus effect. This transpires with favorable papers being disproportionately amplified, along with an influx of new anti-consensus user sign-ups. Finally, our content analysis highlights that anti-consensus users misrepresent scientific findings or question scientists' integrity in their efforts to substantiate their claims.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13334",
        "abstract url": "https://arxiv.org/abs/2401.13334",
        "title": "Explainable Bayesian Optimization",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In industry, Bayesian optimization (BO) is widely applied in the human-AI collaborative parameter tuning of cyber-physical systems. However, BO's solutions may deviate from human experts' actual goal due to approximation errors and simplified objectives, requiring subsequent tuning. The black-box nature of BO limits the collaborative tuning process because the expert does not trust the BO recommendations. Current explainable AI (XAI) methods are not tailored for optimization and thus fall short of addressing this gap. To bridge this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based explainability method that produces high quality explanations through multiobjective optimization. Our evaluation of benchmark optimization problems and real-world hyperparameter optimization tasks demonstrates TNTRules' superiority over state-of-the-art XAI methods in generating high quality explanations. This work contributes to the intersection of BO and XAI, providing interpretable optimization techniques for real-world applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13335",
        "abstract url": "https://arxiv.org/abs/2401.13335",
        "title": "Full Bayesian Significance Testing for Neural Networks",
        "rating": 0.5,
        "keywords": [
            [
                "AAAI"
            ]
        ],
        "abstract": "Significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. However, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. In this paper, we propose to conduct Full Bayesian Significance Testing for neural networks, called \\textit{n}FBST, to overcome the limitation in relationship characterization of traditional approaches. A Bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. Besides, \\textit{n}FBST can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. Moreover, \\textit{n}FBST is a general framework that can be extended based on the measures selected, such as Grad-\\textit{n}FBST, LRP-\\textit{n}FBST, DeepLIFT-\\textit{n}FBST, LIME-\\textit{n}FBST. A range of experiments on both simulated and real data are conducted to show the advantages of our method.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "Published as a conference paper at AAAI 2024"
    },
    {
        "paper id": "2401.13360",
        "abstract url": "https://arxiv.org/abs/2401.13360",
        "title": "Debiased Sample Selection for Combating Noisy Labels",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strategy based on two weight-based data samplers. By training on the mixture of two class-discriminative mini-batches, the model mitigates the effect of the imbalanced training set while avoiding sparse representations that are easily caused by sampling strategies. Extensive experiments and analyses demonstrate the effectiveness of ITEM. Our code is available at this url \\href{https://github.com/1998v7/ITEM}{ITEM}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13408",
        "abstract url": "https://arxiv.org/abs/2401.13408",
        "title": "Causal Perception",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems. In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging. In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual. We also formalize individual experience as additional causal knowledge that comes with and is used by an individual. Further, we define and discuss loaded attributes, which are attributes prone to evoke perception. Sensitive attributes, such as gender and race, are clear examples of loaded attributes. We define two kinds of causal perception, unfaithful and inconsistent, based on the causal properties of faithfulness and consistency. We illustrate our framework through a series of decision-making examples and discuss relevant fairness applications. The goal of this work is to position perception as a parameter of interest, useful for extending the standard, single interpretation ADM problem formulation.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2305.09535 by other authors"
    },
    {
        "paper id": "2401.13447",
        "abstract url": "https://arxiv.org/abs/2401.13447",
        "title": "Symbolic Equation Solving via Reinforcement Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine-learning methods are gradually being adopted in a great variety of social, economic, and scientific contexts, yet they are notorious for struggling with exact mathematics. A typical example is computer algebra, which includes tasks like simplifying mathematical terms, calculating formal derivatives, or finding exact solutions of algebraic equations. Traditional software packages for these purposes are commonly based on a huge database of rules for how a specific operation (e.g., differentiation) transforms a certain term (e.g., sine function) into another one (e.g., cosine function). Thus far, these rules have usually needed to be discovered and subsequently programmed by humans. Focusing on the paradigmatic example of solving linear equations in symbolic form, we demonstrate how the process of finding elementary transformation rules and step-by-step solutions can be automated using reinforcement learning with deep neural networks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 4 figures + appendices 17 pages, 1 figure, 16 tables"
    },
    {
        "paper id": "2401.13460",
        "abstract url": "https://arxiv.org/abs/2401.13460",
        "title": "Multi-Agent Diagnostics for Robustness via Illuminated Diversity",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one of the most complex environments for multi-agent reinforcement learning. Specifically, we employ MADRID for generating a diverse array of adversarial settings for TiZero, the state-of-the-art approach which \"masters\" the game through 45 days of training on a large-scale distributed infrastructure. We expose key shortcomings in TiZero's tactical decision-making, underlining the crucial importance of rigorous evaluation in multi-agent systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13481",
        "abstract url": "https://arxiv.org/abs/2401.13481",
        "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI, and that participants were more likely to knowingly adopt AI ideas when the task was difficult. Our findings suggest that introducing AI ideas into society may increase collective diversity but not individual creativity.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13498",
        "abstract url": "https://arxiv.org/abs/2401.13498",
        "title": "Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion",
                "Synthesis"
            ],
            [
                "cs.SD"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Accepted to ICASSP 2024"
    },
    {
        "paper id": "2401.13530",
        "abstract url": "https://arxiv.org/abs/2401.13530",
        "title": "Continuous-time Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recently, optimization on the Riemannian manifold has provided new insights to the optimization community. In this regard, the manifold taken as the probability measure metric space equipped with the second-order Wasserstein distance is of particular interest, since optimization on it can be linked to practical sampling processes. In general, the oracle (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence). In this paper, we aim to enrich the continuous optimization methods in the Wasserstein space by extending the gradient flow into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow. The two flows on Euclidean space are standard stochastic optimization methods, while their Riemannian counterparts are not explored yet. By leveraging the structures in Wasserstein space, we construct a stochastic differential equation (SDE) to approximate the discrete dynamics of desired stochastic methods in the corresponded random vector space. Then, the flows of probability measures are naturally obtained by applying Fokker-Planck equation to such SDE. Furthermore, the convergence rates of the proposed Riemannian stochastic flows are proven, and they match the results in Euclidean space.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13558",
        "abstract url": "https://arxiv.org/abs/2401.13558",
        "title": "Task structure and nonlinearity jointly determine learned representational geometry",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic behavior of ReLU, which leads feature neurons to specialize for different regions of input space. By contrast, feature neurons in Tanh networks tend to inherit the task label structure. Consequently, when the target outputs are low dimensional, Tanh networks generate neural representations that are more disentangled than those obtained with a ReLU nonlinearity. Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13586",
        "abstract url": "https://arxiv.org/abs/2401.13586",
        "title": "Instruction Fine-Tuning: Does Prompt Loss Matter?",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a study analyzing the effects of prompt loss weighting (PLW) on supervised instruction fine-tuning. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 and multiple instruction datasets. We found that performance of models fine-tuned on our short-completion dataset had a statistically significant negative quadratic relationship with PLW, but performance of models fine-tuned on medium- and long-completion data did not show any relationship with PLW. I.e., prompt loss can be safely ignored for many datasets. For short-completion data, small values (0.01-0.1) of PLW were optimal for multiple-choice and short-generation tasks while large values (~ 1.0) of PLW were optimal for long-generation tasks. We concluded that low non-zero PLW encourages models to not diverge from pre-trained model weights during training and high PLW reduces overfitting. Finally, we present a rough guide for selecting PLW values based on the completion-prompt length ratio of fine-tuning data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages of content. 12 pages supporting. 30 figures"
    },
    {
        "paper id": "2401.13604",
        "abstract url": "https://arxiv.org/abs/2401.13604",
        "title": "Stream-based perception for cognitive agents in mobile ecosystems",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements. Experiments with real smartphone data demonstrate the benefits of stream-based agent perception.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13649",
        "abstract url": "https://arxiv.org/abs/2401.13649",
        "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "24 pages. Project page: https://jykoh.com/vwa"
    },
    {
        "paper id": "2401.13656",
        "abstract url": "https://arxiv.org/abs/2401.13656",
        "title": "Navigating Multidimensional Ideologies with Reddit's Political Compass: Economic Conflict and Social Affinity",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "The prevalent perspective in quantitative research on opinion dynamics flattens the landscape of the online political discourse into a traditional left--right dichotomy. While this approach helps simplify the analysis and modeling effort, it also neglects the intrinsic multidimensional richness of ideologies. In this study, we analyze social interactions on Reddit, under the lens of a multi-dimensional ideological framework: the political compass. We examine over 8 million comments posted on the subreddits /r/PoliticalCompass and /r/PoliticalCompassMemes during 2020--2022. By leveraging their self-declarations, we disentangle the ideological dimensions of users into economic (left--right) and social (libertarian--authoritarian) axes. In addition, we characterize users by their demographic attributes (age, gender, and affluence). We find significant homophily for interactions along the social axis of the political compass and demographic attributes. Compared to a null model, interactions among individuals of similar ideology surpass expectations by 6%. In contrast, we uncover a significant heterophily along the economic axis: left/right interactions exceed expectations by 10%. Furthermore, heterophilic interactions are characterized by a higher language toxicity than homophilic interactions, which hints at a conflictual discourse between every opposite ideology. Our results help reconcile apparent contradictions in recent literature, which found a superposition of homophilic and heterophilic interactions in online political discussions. By disentangling such interactions into the economic and social axes we pave the way for a deeper understanding of opinion dynamics on social media.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13662",
        "abstract url": "https://arxiv.org/abs/2401.13662",
        "title": "The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13744",
        "abstract url": "https://arxiv.org/abs/2401.13744",
        "title": "Conformal Prediction Sets Improve Human Decision Making",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Code available at https://github.com/layer6ai-labs/hitl-conformal-prediction"
    },
    {
        "paper id": "2401.13770",
        "abstract url": "https://arxiv.org/abs/2401.13770",
        "title": "AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS) based Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving challenging combinatorial problems. Despite the tremendous success of CnC solvers in solving a variety of hard combinatorial problems, the lookahead cubing techniques at the heart of CnC have not evolved much for many years. Part of the reason is the sheer difficulty of coming up with new cubing techniques that are both low-cost and effective in partitioning input formulas into sub-formulas, such that the overall runtime is minimized. Lookahead cubing techniques used by current state-of-the-art CnC solvers, such as March, keep their cubing costs low by constraining the search for the optimal splitting variables. By contrast, our key innovation is a deductively-driven MCTS-based lookahead cubing technique, that performs a deeper heuristic search to find effective cubes, while keeping the cubing cost low. We perform an extensive comparison of AlphaMapleSAT against the March CnC solver on challenging combinatorial problems such as the minimum Kochen-Specker and Ramsey problems. We also perform ablation studies to verify the efficacy of the MCTS heuristic search for the cubing problem. Results show up to 2.3x speedup in parallel (and up to 27x in sequential) elapsed real time.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13796",
        "abstract url": "https://arxiv.org/abs/2401.13796",
        "title": "Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a \"push the button\" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning, and compares standard inductive ML with transductive ML frameworks. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "under rev"
    },
    {
        "paper id": "2401.13799",
        "abstract url": "https://arxiv.org/abs/2401.13799",
        "title": "Who Changed the Destiny of Rural Students, and How?: Unpacking ICT-Mediated Remote Education in Rural China",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The proliferation of Information and Communication Technologies (ICTs) has shown great promise in addressing educational challenges facing rural areas. However, the complex rural context poses significant challenges to the effective utilization of these technologies. This paper examines the empirical integration of live-streaming-based remote classrooms (LSRC) through a qualitative study in rural China. Our findings suggest that while LSRC enables rural students equal access to high-quality educational resources, its practical integration faces numerous challenges. In particular, we emphasize the crucial role of local teachers in addressing these challenges, ultimately achieving the desired improvement of students' learning outcomes. We also examine the impact of LSRC on the original rural education ecosystem. Building upon our findings, we call for a reconsideration of interaction paradigms and evaluation systems of ICT-mediated rural education, emphasizing the significance of rural teachers. We conclude by discussing the implications for future ICT-mediated technology interventions in rural settings.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "In submission"
    },
    {
        "paper id": "2401.13835",
        "abstract url": "https://arxiv.org/abs/2401.13835",
        "title": "The Calibration Gap between Model and Human Confidence in Large Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "27 pages, 10 figures"
    },
    {
        "paper id": "2401.13843",
        "abstract url": "https://arxiv.org/abs/2401.13843",
        "title": "Enumerating the k-fold configurations in multi-class classification problems",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "K-fold cross-validation is a widely used tool for assessing classifier performance. The reproducibility crisis faced by artificial intelligence partly results from the irreproducibility of reported k-fold cross-validation-based performance scores. Recently, we introduced numerical techniques to test the consistency of claimed performance scores and experimental setups. In a crucial use case, the method relies on the combinatorial enumeration of all k-fold configurations, for which we proposed an algorithm in the binary classification case.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13850",
        "abstract url": "https://arxiv.org/abs/2401.13850",
        "title": "PADTHAI-MM: A Principled Approach for Designing Trustable, Human-centered AI systems using the MAST Methodology",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Designing for AI trustworthiness is challenging, with a lack of practical guidance despite extensive literature on trust. The Multisource AI Scorecard Table (MAST), a checklist rating system, addresses this gap in designing and evaluating AI-enabled decision support systems. We propose the Principled Approach for Designing Trustable Human-centered AI systems using MAST Methodology (PADTHAI-MM), a nine-step framework what we demonstrate through the iterative design of a text analysis platform called the REporting Assistant for Defense and Intelligence Tasks (READIT). We designed two versions of READIT, high-MAST including AI context and explanations, and low-MAST resembling a \"black box\" type system. Participant feedback and state-of-the-art AI knowledge was integrated in the design process, leading to a redesigned prototype tested by participants in an intelligence reporting task. Results show that MAST-guided design can improve trust perceptions, and that MAST criteria can be linked to performance, process, and purpose information, providing a practical and theory-informed basis for AI system design.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13851",
        "abstract url": "https://arxiv.org/abs/2401.13851",
        "title": "Scaling NVIDIA's Multi-speaker Multi-lingual TTS Systems with Zero-Shot TTS to Indic Languages",
        "rating": 0.5,
        "keywords": [
            [
                "GAN"
            ],
            [
                "cs.SD"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "In this paper, we describe the TTS models developed by NVIDIA for the MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024 Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by training additionally on 5 minutes of target speaker data. In Track 3, we utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as well as external datasets. We use HiFi-GAN vocoders for all submissions. RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS) of 3.62.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Presentation accepted at ICASSP 2024"
    },
    {
        "paper id": "2401.13883",
        "abstract url": "https://arxiv.org/abs/2401.13883",
        "title": "Domain-Independent Dynamic Programming",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and both MIP and CP in seven.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Manuscript submitted to JACM"
    },
    {
        "paper id": "2401.13913",
        "abstract url": "https://arxiv.org/abs/2401.13913",
        "title": "Spectral Clustering for Discrete Distributions",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13920",
        "abstract url": "https://arxiv.org/abs/2401.13920",
        "title": "LocMoE: A Low-overhead MoE for Large Language Model Training",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We port these modifications on the PanGu-Sigma model based on the MindSpore framework with multi-level routing and conduct experiments on Ascend clusters. The experiment results demonstrate that the proposed LocMoE reduces training time per epoch by 12.68% to 22.24% compared to classical routers, such as hash router and switch router, without impacting the model accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13935",
        "abstract url": "https://arxiv.org/abs/2401.13935",
        "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Counterfactuals and counterfactual reasoning underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions -- like interventions on race -- may not translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14424",
        "abstract url": "https://arxiv.org/abs/2401.14424",
        "title": "Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. In the previous year, a novel symbolic regression methodology utilizing Monte Carlo Tree Search (MCTS) was advanced, achieving state-of-the-art results on a diverse range of datasets. although this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To optimize the trade-off between efficiency and versatility, we introduce SR-GPT, a novel algorithm for symbolic regression that integrates Monte Carlo Tree Search (MCTS) with a Generative Pre-Trained Transformer (GPT). By using GPT to guide the MCTS, the search efficiency of MCTS is significantly improved. Next, we utilize the MCTS results to further refine the GPT, enhancing its capabilities and providing more accurate guidance for the MCTS. MCTS and GPT are coupled together and optimize each other until the target expression is successfully determined. We conducted extensive evaluations of SR-GPT using 222 expressions sourced from over 10 different symbolic regression datasets. The experimental results demonstrate that SR-GPT outperforms existing state-of-the-art algorithms in accurately recovering symbolic expressions both with and without added noise.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "24 pages"
    },
    {
        "paper id": "2401.15098",
        "abstract url": "https://arxiv.org/abs/2401.15098",
        "title": "Hierarchical Continual Reinforcement Learning via Large Language Model",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The ability to learn continuously in dynamic environments is a crucial requirement for reinforcement learning (RL) agents applying in the real world. Despite the progress in continual reinforcement learning (CRL), existing methods often suffer from insufficient knowledge transfer, particularly when the tasks are diverse. To address this challenge, we propose a new framework, Hierarchical Continual reinforcement learning via large language model (Hi-Core), designed to facilitate the transfer of high-level knowledge. Hi-Core orchestrates a twolayer structure: high-level policy formulation by a large language model (LLM), which represents agenerates a sequence of goals, and low-level policy learning that closely aligns with goal-oriented RL practices, producing the agent's actions in response to the goals set forth. The framework employs feedback to iteratively adjust and verify highlevel policies, storing them along with low-level policies within a skill library. When encountering a new task, Hi-Core retrieves relevant experience from this library to help to learning. Through experiments on Minigrid, Hi-Core has demonstrated its effectiveness in handling diverse CRL tasks, which outperforms popular baselines.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.00048",
        "abstract url": "https://arxiv.org/abs/2402.00048",
        "title": "IICONGRAPH: improved Iconographic and Iconological Statements in Knowledge Graphs",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Iconography and iconology are fundamental domains when it comes to understanding artifacts of cultural heritage. Iconography deals with the study and interpretation of visual elements depicted in artifacts and their symbolism, while iconology delves deeper, exploring the underlying cultural and historical meanings. Despite the advances in representing cultural heritage with Linked Open Data (LOD), recent studies show persistent gaps in the representation of iconographic and iconological statements in current knowledge graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was created by refining and extending the iconographic and iconological statements of ArCo (the Italian KG of cultural heritage) and Wikidata. The development of IICONGRAPH was also driven by a series of requirements emerging from research case studies that were unattainable in the non-reengineered versions of the KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms ArCo and Wikidata through domain-specific assessments from the literature but also serves as a robust platform for addressing the formulated research questions. IICONGRAPH is released and documented in accordance with the FAIR principles to guarantee the resource's reusability. The algorithms used to create it and assess the research questions have also been made available to ensure transparency and reproducibility. While future work focuses on ingesting more data into the KG, and on implementing it as a backbone of LLM-based question answering systems, the current version of IICONGRAPH still emerges as a valuable asset, contributing to the evolving landscape of cultural heritage representation within Knowledge Graphs, the Semantic Web, and beyond.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2402.01703",
        "abstract url": "https://arxiv.org/abs/2402.01703",
        "title": "A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both community members and police officers. We move away from modeling approaches built around the existence of a single ground truth and instead utilize new advances in soft labeling to incorporate variation in how different observers perceive the same interactions. We argue that this inclusive approach to the conceptualization and design of new ML tools is broadly applicable to the study of communication and development of analytic tools across domains of human interaction, including education, medicine, and the workplace.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2402.01705",
        "abstract url": "https://arxiv.org/abs/2402.01705",
        "title": "Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement and mitigation praxis.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "23 pages, 7 figures"
    },
    {
        "paper id": "2402.09432",
        "abstract url": "https://arxiv.org/abs/2402.09432",
        "title": "An Enhanced Analysis of Traffic Intelligence in Smart Cities Using Sustainable Deep Radial Function",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Smart cities have revolutionized urban living by incorporating sophisticated technologies to optimize various aspects of urban infrastructure, such as transportation systems. Effective traffic management is a crucial component of smart cities, as it has a direct impact on the quality of life of residents and tourists. Utilizing deep radial basis function (RBF) networks, this paper describes a novel strategy for enhancing traffic intelligence in smart cities. Traditional methods of traffic analysis frequently rely on simplistic models that are incapable of capturing the intricate patterns and dynamics of urban traffic systems. Deep learning techniques, such as deep RBF networks, have the potential to extract valuable insights from traffic data and enable more precise predictions and decisions. In this paper, we propose an RBF based method for enhancing smart city traffic intelligence. Deep RBF networks combine the adaptability and generalization capabilities of deep learning with the discriminative capability of radial basis functions. The proposed method can effectively learn intricate relationships and nonlinear patterns in traffic data by leveraging the hierarchical structure of deep neural networks. The deep RBF model can learn to predict traffic conditions, identify congestion patterns, and make informed recommendations for optimizing traffic management strategies by incorporating these rich and diverse data To evaluate the efficacy of our proposed method, extensive experiments and comparisons with real world traffic datasets from a smart city environment were conducted. In terms of prediction accuracy and efficiency, the results demonstrate that the deep RBF based approach outperforms conventional traffic analysis methods. Smart city traffic intelligence is enhanced by the model capacity to capture nonlinear relationships and manage large scale data sets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "25 pages, 6 figures, and 3 Tables"
    },
    {
        "paper id": "2401.13311",
        "abstract url": "https://arxiv.org/abs/2401.13311",
        "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models",
        "rating": 0,
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In addition to human evaluations, we also employed automatic evaluation metrics using GPT-4, uncovering similar trends in performance disparities. We also perform a fine-grained evaluation across diverse visual contexts and provide qualitative analysis which provides a robust framework for future advancements in the LMM design. https://con-textual.github.io/",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13363",
        "abstract url": "https://arxiv.org/abs/2401.13363",
        "title": "Do You Guys Want to Dance: Zero-Shot Compositional Human Dance Generation with Multiple Persons",
        "rating": 0,
        "keywords": [
            [
                "synthesize"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human dance generation (HDG) aims to synthesize realistic videos from images and sequences of driving poses. Despite great success, existing methods are limited to generating videos of a single person with specific backgrounds, while the generalizability for real-world scenarios with multiple persons and complex backgrounds remains unclear. To systematically measure the generalizability of HDG models, we introduce a new task, dataset, and evaluation protocol of compositional human dance generation (cHDG). Evaluating the state-of-the-art methods on cHDG, we empirically find that they fail to generalize to real-world scenarios. To tackle the issue, we propose a novel zero-shot framework, dubbed MultiDance-Zero, that can synthesize videos consistent with arbitrary multiple persons and background while precisely following the driving poses. Specifically, in contrast to straightforward DDIM or null-text inversion, we first present a pose-aware inversion method to obtain the noisy latent code and initialization text embeddings, which can accurately reconstruct the composed reference image. Since directly generating videos from them will lead to severe appearance inconsistency, we propose a compositional augmentation strategy to generate augmented images and utilize them to optimize a set of generalizable text embeddings. In addition, consistency-guided sampling is elaborated to encourage the background and keypoints of the estimated clean image at each reverse step to be close to those of the reference image, further improving the temporal consistency of generated videos. Extensive qualitative and quantitative results demonstrate the effectiveness and superiority of our approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13388",
        "abstract url": "https://arxiv.org/abs/2401.13388",
        "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "synthesizing",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://unimo-ptm.github.io/"
    },
    {
        "paper id": "2401.13398",
        "abstract url": "https://arxiv.org/abs/2401.13398",
        "title": "Text Categorization Can Enhance Domain-Agnostic Stopword Extraction",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper investigates the role of text categorization in streamlining stopword extraction in natural language processing (NLP), specifically focusing on nine African languages alongside French. By leveraging the MasakhaNEWS, African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that text categorization effectively identifies domain-agnostic stopwords with over 80% detection success rate for most examined languages. Nevertheless, linguistic variances result in lower detection rates for certain languages. Interestingly, we find that while over 40% of stopwords are common across news categories, less than 15% are unique to a single category. Uncommon stopwords add depth to text but their classification as stopwords depends on context. Therefore combining statistical and linguistic approaches creates comprehensive stopword lists, highlighting the value of our hybrid method. This research enhances NLP for African languages and underscores the importance of text categorization in stopword extraction.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "A Project Report for the Masakhane Research Community"
    },
    {
        "paper id": "2401.13405",
        "abstract url": "https://arxiv.org/abs/2401.13405",
        "title": "Synthetic data enables faster annotation and robust segmentation for multi-object grasping in clutter",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Object recognition and object pose estimation in robotic grasping continue to be significant challenges, since building a labelled dataset can be time consuming and financially costly in terms of data collection and annotation. In this work, we propose a synthetic data generation method that minimizes human intervention and makes downstream image segmentation algorithms more robust by combining a generated synthetic dataset with a smaller real-world dataset (hybrid dataset). Annotation experiments show that the proposed synthetic scene generation can diminish labelling time dramatically. RGB image segmentation is trained with hybrid dataset and combined with depth information to produce pixel-to-point correspondence of individual segmented objects. The object to grasp is then determined by the confidence score of the segmentation algorithm. Pick-and-place experiments demonstrate that segmentation trained on our hybrid dataset (98.9%, 70%) outperforms the real dataset and a publicly available dataset by (6.7%, 18.8%) and (2.8%, 10%) in terms of labelling and grasping success rate, respectively. Supplementary material is available at https://sites.google.com/view/synthetic-dataset-generation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted for 2024 10th International Conference on Mechatronics and Robotics Engineering (ICMRE)"
    },
    {
        "paper id": "2401.13548",
        "abstract url": "https://arxiv.org/abs/2401.13548",
        "title": "A Phoneme-Scale Assessment of Multichannel Speech Enhancement Algorithms",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.SD"
            ]
        ],
        "abstract": "In the intricate acoustic landscapes where speech intelligibility is challenged by noise and reverberation, multichannel speech enhancement emerges as a promising solution for individuals with hearing loss. Such algorithms are commonly evaluated at the utterance level. However, this approach overlooks the granular acoustic nuances revealed by phoneme-specific analysis, potentially obscuring key insights into their performance. This paper presents an in-depth phoneme-scale evaluation of 3 state-of-the-art multichannel speech enhancement algorithms. These algorithms -- FasNet, MVDR, and Tango -- are extensively evaluated across different noise conditions and spatial setups, employing realistic acoustic simulations with measured room impulse responses, and leveraging diversity offered by multiple microphones in a binaural hearing setup. The study emphasizes the fine-grained phoneme-level analysis, revealing that while some phonemes like plosives are heavily impacted by environmental acoustics and challenging to deal with by the algorithms, others like nasals and sibilants see substantial improvements after enhancement. These investigations demonstrate important improvements in phoneme clarity in noisy conditions, with insights that could drive the development of more personalized and phoneme-aware hearing aid technologies.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "This is the preprint of the paper that we submitted to the Trends in Hearing Journal"
    },
    {
        "paper id": "2401.13551",
        "abstract url": "https://arxiv.org/abs/2401.13551",
        "title": "Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection",
        "rating": 0,
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13594",
        "abstract url": "https://arxiv.org/abs/2401.13594",
        "title": "Graph Guided Question Answer Generation for Procedural Question-Answering",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller. 2) semantic coverage is the key indicator for downstream QA performance. Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model. In contrast, the higher semantic coverage provided by our method is critical for QA performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to EACL 2024 as long paper. 25 pages including appendix"
    },
    {
        "paper id": "2401.13795",
        "abstract url": "https://arxiv.org/abs/2401.13795",
        "title": "Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "As online shopping is growing, the ability for buyers to virtually visualize products in their settings-a phenomenon we define as \"Virtual Try-All\"-has become crucial. Recent diffusion models inherently contain a world model, rendering them suitable for this task within an inpainting context. However, traditional image-conditioned diffusion models often fail to capture the fine-grained details of products. In contrast, personalization-driven models such as DreamPaint are good at preserving the item's details but they are not optimized for real-time applications. We present \"Diffuse to Choose,\" a novel diffusion-based image-conditioned inpainting model that efficiently balances fast inference with the retention of high-fidelity details in a given reference item while ensuring accurate semantic manipulations in the given scene content. Our approach is based on incorporating fine-grained features from the reference image directly into the latent feature maps of the main diffusion model, alongside with a perceptual loss to further preserve the reference item's details. We conduct extensive testing on both in-house and publicly available datasets, and show that Diffuse to Choose is superior to existing zero-shot diffusion inpainting methods as well as few-shot diffusion personalization algorithms like DreamPaint.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13888",
        "abstract url": "https://arxiv.org/abs/2401.13888",
        "title": "Knowledge Guided Entity-aware Video Captioning and A Basketball Benchmark",
        "rating": 0,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite the recent emergence of video captioning models, how to generate the text description with specific entity names and fine-grained actions is far from being solved, which however has great applications such as basketball live text broadcast. In this paper, a new multimodal knowledge graph supported basketball benchmark for video captioning is proposed. Specifically, we construct a multimodal basketball game knowledge graph (KG_NBA_2022) to provide additional knowledge beyond videos. Then, a multimodal basketball game video captioning (VC_NBA_2022) dataset that contains 9 types of fine-grained shooting events and 286 players' knowledge (i.e., images and names) is constructed based on KG_NBA_2022. We develop a knowledge guided entity-aware video captioning network (KEANet) based on a candidate player list in encoder-decoder form for basketball live text broadcast. The temporal contextual information in video is encoded by introducing the bi-directional GRU (Bi-GRU) module. And the entity-aware module is designed to model the relationships among the players and highlight the key players. Extensive experiments on multiple sports benchmarks demonstrate that KEANet effectively leverages extera knowledge and outperforms advanced video captioning models. The proposed dataset and corresponding codes will be publicly available soon",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13927",
        "abstract url": "https://arxiv.org/abs/2401.13927",
        "title": "Adaptive Text Watermark for Large Language Models",
        "rating": 0,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of \\emph{un-watermarked} LLMs while maintaining security even under various attacks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01702",
        "abstract url": "https://arxiv.org/abs/2402.01702",
        "title": "Fluent dreaming for language models",
        "rating": 0,
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Feature visualization, also known as \"dreaming\", offers insights into vision models by optimizing the inputs to maximize a neuron's activation or other internal component. However, dreaming has not been successfully applied to language models because the input space is discrete. We extend Greedy Coordinate Gradient, a method from the language model adversarial attack literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO optimizes the input prompt to simultaneously maximize the Pareto frontier between a chosen internal feature and prompt fluency, enabling fluent dreaming for language models. We demonstrate dreaming with neurons, output logits and arbitrary directions in activation space. We measure the fluency of the resulting prompts and compare language model dreaming with max-activating dataset examples. Critically, fluent dreaming allows automatically exploring the behavior of model internals in reaction to mildly out-of-distribution prompts. Code for running EPO is available at https://github.com/Confirm-Solutions/dreamy. A companion page demonstrating code usage is at https://confirmlabs.org/posts/dreamy.html",
        "subjects": [
            "cs.CL"
        ],
        "comment": "11 pages, 6 figures, 4 tables"
    },
    {
        "paper id": "2402.01704",
        "abstract url": "https://arxiv.org/abs/2402.01704",
        "title": "States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers",
        "rating": 0,
        "keywords": [
            [
                "synthesize"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which are grounded in real world applications. In this work, we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "32 pages, 8 figures, code available @ https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py"
    },
    {
        "paper id": "2401.13236",
        "abstract url": "https://arxiv.org/abs/2401.13236",
        "title": "How to Collaborate: Towards Maximizing the Generalization Performance in Cross-Silo Federated Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) has attracted vivid attention as a privacy-preserving distributed learning framework. In this work, we focus on cross-silo FL, where clients become the model owners after training and are only concerned about the model's generalization performance on their local data. Due to the data heterogeneity issue, asking all the clients to join a single FL training process may result in model performance degradation. To investigate the effectiveness of collaboration, we first derive a generalization bound for each client when collaborating with others or when training independently. We show that the generalization performance of a client can be improved only by collaborating with other clients that have more training data and similar data distribution. Our analysis allows us to formulate a client utility maximization problem by partitioning clients into multiple collaborating groups. A hierarchical clustering-based collaborative training (HCCT) scheme is then proposed, which does not need to fix in advance the number of groups. We further analyze the convergence of HCCT for general non-convex loss functions which unveils the effect of data similarity among clients. Extensive simulations show that HCCT achieves better generalization performance than baseline schemes, whereas it degenerates to independent training and conventional FL in specific scenarios.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13254",
        "abstract url": "https://arxiv.org/abs/2401.13254",
        "title": "A modular architecture for IMU-based data gloves",
        "rating": -0.5,
        "keywords": [
            [
                "workshop"
            ]
        ],
        "abstract": "The flexibility and range of motion in human hands play a crucial role in human interaction with the environment and have been studied across different fields. Researchers explored various technological solutions for gathering information from the hands. These solutions include tracking hand motion through cameras or wearable sensors and using wearable sensors to measure the position and pressure of contact points. Data gloves can collect both types of information by utilizing inertial measurement units, flex sensors, magnetic trackers for motion tracking, and force resistors or touch sensors for contact measurement. Although there are commercially available data gloves, researchers often create custom data gloves to achieve the desired flexibility and control over the hardware. However, the existing literature lacks standardization and the reuse of previously designed data gloves. As a result, many gloves with unclear characteristics exist, which makes replication challenging and negatively impacts the reproducibility of studies. This work proposes a modular, open hardware and software architecture for creating customized data gloves based on IMU technology. We also provide an architecture implementation along with an experimental protocol to evaluate device performance.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "Mechatronics Topic Group workshop at European Robotics Forum 2024"
    },
    {
        "paper id": "2401.13280",
        "abstract url": "https://arxiv.org/abs/2401.13280",
        "title": "DDI-CoCo: A Dataset For Understanding The Effect Of Color Contrast In Machine-Assisted Skin Disease Detection",
        "rating": -0.5,
        "keywords": [
            [
                "Disease"
            ],
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Skin tone as a demographic bias and inconsistent human labeling poses challenges in dermatology AI. We take another angle to investigate color contrast's impact, beyond skin tones, on malignancy detection in skin disease datasets: We hypothesize that in addition to skin tones, the color difference between the lesion area and skin also plays a role in malignancy detection performance of dermatology AI models. To study this, we first propose a robust labeling method to quantify color contrast scores of each image and validate our method by showing small labeling variations. More importantly, applying our method to \\textit{the only} diverse-skin tone and pathologically-confirmed skin disease dataset DDI, yields \\textbf{DDI-CoCo Dataset}, and we observe a performance gap between the high and low color difference groups. This disparity remains consistent across various state-of-the-art (SoTA) image classification models, which supports our hypothesis. Furthermore, we study the interaction between skin tone and color difference effects and suggest that color difference can be an additional reason behind model performance bias between skin tones. Our work provides a complementary angle to dermatology AI for improving skin disease detection.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 4 figures, 2 tables, Accepted to ICASSP 2024"
    },
    {
        "paper id": "2401.13330",
        "abstract url": "https://arxiv.org/abs/2401.13330",
        "title": "NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks",
        "rating": -0.5,
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS), the first NAS framework for the design of optimal EENNs satisfying constraints on the accuracy and the number of Multiply and Accumulate (MAC) operations performed by the EENNs at inference time. In particular, this provides the joint design of backbone and EECs to select a set of admissible (i.e., respecting the constraints) Pareto Optimal Solutions in terms of best tradeoff between the accuracy and number of MACs. The results show that the models designed by NACHOS are competitive with the state-of-the-art EENNs. Additionally, this work investigates the effectiveness of two novel regularization terms designed for the optimization of the auxiliary classifiers of the EENN",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13391",
        "abstract url": "https://arxiv.org/abs/2401.13391",
        "title": "Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics",
        "rating": -0.5,
        "keywords": [
            [
                "radar"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a paradigm shift: initially, we should focus on generating the most precise ranking for each subgroup. Following this, individuals should be chosen from these rankings to meet both fairness standards and practical considerations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13627",
        "abstract url": "https://arxiv.org/abs/2401.13627",
        "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild",
        "rating": -0.5,
        "keywords": [
            [
                "Image Restoration"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This paper has been accepted by CVPR 2024"
    },
    {
        "paper id": "2401.13652",
        "abstract url": "https://arxiv.org/abs/2401.13652",
        "title": "Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13751",
        "abstract url": "https://arxiv.org/abs/2401.13751",
        "title": "A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a function of time and relate that to a novel metric that allows us to quickly determine whether or not the cost of training a model outweighs the cost of attacking it. Using this approach, we are able to approximate the expected failure rate using a small number of specially crafted samples rather than increasingly larger benchmark datasets. We demonstrate the efficacy of this technique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit floating-point numbers, various data pre-processing techniques, and several attacks on five configurations of the ResNet model. Then, using empirical measurements, we examine the various trade-offs between cost, robustness, latency, and reliability to find that larger models do not significantly aid in adversarial robustness despite costing significantly more to train.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13794",
        "abstract url": "https://arxiv.org/abs/2401.13794",
        "title": "Traffic Pattern Classification in Smart Cities Using Deep Recurrent Neural Network",
        "rating": -0.5,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper examines the use of deep recurrent neural networks to classify traffic patterns in smart cities. We propose a novel approach to traffic pattern classification based on deep recurrent neural networks, which can effectively capture traffic patterns' dynamic and sequential features. The proposed model combines convolutional and recurrent layers to extract features from traffic pattern data and a SoftMax layer to classify traffic patterns. Experimental results show that the proposed model outperforms existing methods regarding accuracy, precision, recall, and F1 score. Furthermore, we provide an in depth analysis of the results and discuss the implications of the proposed model for smart cities. The results show that the proposed model can accurately classify traffic patterns in smart cities with a precision of as high as 95%. The proposed model is evaluated on a real world traffic pattern dataset and compared with existing classification methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "18 pages, 6 figures, 3 tables"
    },
    {
        "paper id": "2401.13854",
        "abstract url": "https://arxiv.org/abs/2401.13854",
        "title": "Embedding Attack Project (Work Report)",
        "rating": -0.5,
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This report summarizes all the MIA experiments (Membership Inference Attacks) of the Embedding Attack Project, including threat models, experimental setup, experimental results, findings and discussion. Current results cover the evaluation of two main MIA strategies (loss-based and embedding-based MIAs) on 6 AI models ranging from Computer Vision to Language Modelling. There are two ongoing experiments on MIA defense and neighborhood-comparison embedding attacks. These are ongoing projects. The current work on MIA and PIA can be summarized into six conclusions: (1) Amount of overfitting is directly proportional to model's vulnerability; (2) early embedding layers in the model are less susceptible to privacy leaks; (3) Deeper model layers contain more membership information; (4) Models are more vulnerable to MIA if both embeddings and corresponding training labels are compromised; (5) it is possible to use pseudo-labels to increase the MIA success; and (6) although MIA and PIA success rates are proportional, reducing the MIA does not necessarily reduce the PIA.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "13 pages, 5 figures"
    },
    {
        "paper id": "2401.13898",
        "abstract url": "https://arxiv.org/abs/2401.13898",
        "title": "Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multimodal federated learning (MFL) has emerged as a decentralized machine learning paradigm, allowing multiple clients with different modalities to collaborate on training a machine learning model across diverse data sources without sharing their private data. However, challenges, such as data heterogeneity and severely missing modalities, pose crucial hindrances to the robustness of MFL, significantly impacting the performance of global model. The absence of a modality introduces misalignment during the local training phase, stemming from zero-filling in the case of clients with missing modalities. Consequently, achieving robust generalization in global model becomes imperative, especially when dealing with clients that have incomplete data. In this paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a novel approach for MFL under severely missing modalities by conducting the complete prototypes to provide diverse modality knowledge in modality-shared level with the cross-modal regularization and modality-specific level with cross-modal contrastive mechanism. Additionally, our approach introduces the cross-modal alignment to provide regularization for modality-specific features, thereby enhancing overall performance, particularly in scenarios involving severely missing modalities. Through extensive experiments on three multimodal datasets, we demonstrate the effectiveness of MFCPL in mitigating these challenges and improving the overall performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 8 figures, 5 tables"
    },
    {
        "paper id": "2401.14429",
        "abstract url": "https://arxiv.org/abs/2401.14429",
        "title": "[Re] The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear and Non-Gaussian Observation Models",
        "rating": -0.5,
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Kalman filters provide a straightforward and interpretable means to estimate hidden or latent variables, and have found numerous applications in control, robotics, signal processing, and machine learning. One such application is neural decoding for neuroprostheses. In 2020, Burkhart et al. thoroughly evaluated their new version of the Kalman filter that leverages Bayes' theorem to improve filter performance for highly non-linear or non-Gaussian observation models. This work provides an open-source Python alternative to the authors' MATLAB algorithm. Specifically, we reproduce their most salient results for neuroscientific contexts and further examine the efficacy of their filter using multiple random seeds and previously unused trials from the authors' dataset. All experiments were performed offline on a single computer.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13231",
        "abstract url": "https://arxiv.org/abs/2401.13231",
        "title": "DittoGym: Learning to Control Soft Shape-Shifting Robots",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to accomplish the tasks. Finally, we evaluate our proposed coarse-to-fine algorithm on DittoGym and demonstrate robots that learn to change their morphology several times within a sequence, uniquely enabled by our RL algorithm. More results are available at https://dittogym.github.io.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13255",
        "abstract url": "https://arxiv.org/abs/2401.13255",
        "title": "Constructing a fully homomorphic encryption scheme with the Yoneda Lemma",
        "rating": -1,
        "keywords": [
            [
                "synthesis"
            ]
        ],
        "abstract": "This paper redefines the foundations of asymmetric cryptography's homomorphic cryptosystems through the application of the Yoneda Lemma. It explicitly illustrates that widely adopted systems, including ElGamal, RSA, Benaloh, Regev's LWE, and NTRUEncrypt, directly derive from the principles of the Yoneda Lemma. This synthesis gives rise to a holistic homomorphic encryption framework named the Yoneda Encryption Scheme. Within this scheme, encryption is elucidated through the bijective maps of the Yoneda Lemma Isomorphism, and decryption seamlessly follows from the naturality of these maps. This unification suggests a conjecture for a unified model theory framework, providing a basis for reasoning about both homomorphic and fully homomorphic encryption (FHE) schemes. As a practical demonstration, the paper introduces an FHE scheme capable of processing arbitrary finite sequences of encrypted multiplications and additions without the need for additional tweaking techniques, such as squashing or bootstrapping. This not only underscores the practical implications of the proposed theoretical advancements but also introduces new possibilities for leveraging model theory and forcing techniques in cryptography to facilitate the design of FHE schemes.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "43 pages; improved phrasing, corrected typos, added clarifications; removed unused assumptions and changed the construction of section 5 (to address a type of attacks)"
    },
    {
        "paper id": "2401.13267",
        "abstract url": "https://arxiv.org/abs/2401.13267",
        "title": "Dual-modal Dynamic Traceback Learning for Medical Report Generation",
        "rating": -1,
        "keywords": [
            [
                "Medical",
                "clinical",
                "pathological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With increasing reliance on medical imaging in clinical practices, automated report generation from medical images is in great demand. Existing report generation methods typically adopt an encoder-decoder deep learning framework to build a uni-directional image-to-report mapping. However, such a framework ignores the bi-directional mutual associations between images and reports, thus incurring difficulties in associating the intrinsic medical meanings between them. Recent generative representation learning methods have demonstrated the benefits of dual-modal learning from both image and text modalities. However, these methods exhibit two major drawbacks for medical report generation: 1) they tend to capture morphological information and have difficulties in capturing subtle pathological semantic information, and 2) they predict masked text rely on both unmasked images and text, inevitably degrading performance when inference is based solely on images. In this study, we propose a new report generation framework with dual-modal dynamic traceback learning (DTrace) to overcome the two identified drawbacks and enable dual-modal learning for medical report generation. To achieve this, our DTrace introduces a traceback mechanism to control the semantic validity of generated content via self-assessment. Further, our DTrace introduces a dynamic learning strategy to adapt to various proportions of image and text input, enabling report generation without reliance on textual input during inference. Extensive experiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that our DTrace outperforms state-of-the-art medical report generation methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13268",
        "abstract url": "https://arxiv.org/abs/2401.13268",
        "title": "Loss Allocation in Submarine Armored Three-core HVAC Power Cables",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "depth"
            ]
        ],
        "abstract": "Loss allocation of the three different components (conductor, sheaths and armor) of solidly bonded three-core separated lead-sheathed armored cables, frequently employed in offshore wind farms, is challenging due to the lack of accurate enough analytical expressions in the IEC standard. Also, loss allocation through experimental tests leads to inaccurate results since it is based on questionable assumptions. This paper improves both the IEC formulae and experimental methods by means of different analytical corrections in the conductor and sheath loss expressions. To this aim, an ad hoc application interface (Virtual Lab) based on 3D numerical simulations (finite element method) has been developed. This tool virtualizes and automates different test setups to emulate, in few seconds, the most employed experimental procedures in this type of cable. The analytical corrections have been derived from an in-depth analysis of a first set of 368 cables, ranging from 30 to 275 kV. The new loss expressions were successfully applied to a second set of 645 armored cables of quite diverse features (voltages from 10 to 275 kV, sections and dimensional parameters), hence bringing a general framework for any kind of three-core armored cable.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13285",
        "abstract url": "https://arxiv.org/abs/2401.13285",
        "title": "Small Object Tracking in LiDAR Point Cloud: Learning the Target-awareness Prototype and Fine-grained Search Region",
        "rating": -1,
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Single Object Tracking in LiDAR point cloud is one of the most essential parts of environmental perception, in which small objects are inevitable in real-world scenarios and will bring a significant barrier to the accurate location. However, the existing methods concentrate more on exploring universal architectures for common categories and overlook the challenges that small objects have long been thorny due to the relative deficiency of foreground points and a low tolerance for disturbances. To this end, we propose a Siamese network-based method for small object tracking in the LiDAR point cloud, which is composed of the target-awareness prototype mining (TAPM) module and the regional grid subdivision (RGS) module. The TAPM module adopts the reconstruction mechanism of the masked decoder to learn the prototype in the feature space, aiming to highlight the presence of foreground points that will facilitate the subsequent location of small objects. Through the above prototype is capable of accentuating the small object of interest, the positioning deviation in feature maps still leads to high tracking errors. To alleviate this issue, the RGS module is proposed to recover the fine-grained features of the search region based on ViT and pixel shuffle layers. In addition, apart from the normal settings, we elaborately design a scaling experiment to evaluate the robustness of the different trackers on small objects. Extensive experiments on KITTI and nuScenes demonstrate that our method can effectively improve the tracking performance of small targets without affecting normal-sized objects.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13302",
        "abstract url": "https://arxiv.org/abs/2401.13302",
        "title": "A Lagrange-Newton Approach to Smoothing-and-Mapping",
        "rating": -1,
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "In this report we explore the application of the Lagrange-Newton method to the SAM (smoothing-and-mapping) problem in mobile robotics. In Lagrange-Newton SAM, the angular component of each pose vector is expressed by orientation vectors and treated through Lagrange constraints. This is different from the typical Gauss-Newton approach where variations need to be mapped back and forth between Euclidean space and a manifold suitable for rotational components. We derive equations for five different types of measurements between robot poses: translation, distance, and rotation from odometry in the plane, as well as home-vector angle and compass angle from visual homing. We demonstrate the feasibility of the Lagrange-Newton approach for a simple example related to a cleaning robot scenario.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13312",
        "abstract url": "https://arxiv.org/abs/2401.13312",
        "title": "Evaluation of the power frequency magnetic field generated by three-core armored cables through 3D finite element simulations",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "depth"
            ]
        ],
        "abstract": "The great expansion in offshore power plants is raising the concern regarding the cumulative effect of the electromagnetic field emissions caused by submarine power cables. In this sense, owners are required to predict these emissions during the permitting and consenting process of new power plants. This is a challenging task, especially in the case of HVAC three-core armored cables due to their complex geometry. Customarily, 2D approaches based on the finite element method (FEM) have been employed for evaluating the magnetic field emissions caused by these cables. However, inaccurate results are obtained since the phase conductors and armor twisting is omitted. This work develops, for the first time in the literature, an in-depth analysis of the magnetic field caused by this type of cable through an ultra-shortened 3D-FEM model, which is also faced to experimental measurements taken on an actual 132 kV, 800 mm2 three-core armored cable. Relevant conclusions are derived regarding the impact of the cable design on the magnetic field emissions, including material properties, as well as single and double-layer armors, presenting the proposed model not only as a valuable tool for predicting purposes, but also for optimizing cable design in terms of magnetic field emissions.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13315",
        "abstract url": "https://arxiv.org/abs/2401.13315",
        "title": "Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging",
        "rating": -1,
        "keywords": [
            [
                "cancer"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "To cope with the growing prevalence of colorectal cancer (CRC), screening programs for polyp detection and removal have proven their usefulness. Colonoscopy is considered the best-performing procedure for CRC screening. To ease the examination, deep learning based methods for automatic polyp detection have been developed for conventional white-light imaging (WLI). Compared with WLI, narrow-band imaging (NBI) can improve polyp classification during colonoscopy but requires special equipment. We propose a CycleGAN-based framework to convert images captured with regular WLI to synthetic NBI (SNBI) as a pre-processing method for improving object detection on WLI when NBI is unavailable. This paper first shows that better results for polyp detection can be achieved on NBI compared to a relatively similar dataset of WLI. Secondly, experimental results demonstrate that our proposed modality translation can achieve improved polyp detection on SNBI images generated from WLI compared to the original WLI. This is because our WLI-to-SNBI translation model can enhance the observation of polyp surface patterns in the generated SNBI images.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13341",
        "abstract url": "https://arxiv.org/abs/2401.13341",
        "title": "Evaluation of depth perception in crowded volumes",
        "rating": -1,
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "Depth perception in volumetric visualization plays a crucial role in the understanding and interpretation of volumetric data. Numerous visualization techniques, many of which rely on physically based optical effects, promise to improve depth perception but often do so without considering camera movement or the content of the volume. As a result, the findings from previous studies may not be directly applicable to crowded volumes, where a large number of contained structures disrupts spatial perception. Crowded volumes therefore require special analysis and visualization tools with sparsification capabilities. Interactivity is an integral part of visualizing and exploring crowded spaces, but has received little attention in previous studies. To address this gap, we conducted a study to assess the impact of different rendering techniques on depth perception in crowded volumes, with a particular focus on the effects of camera movement. The results show that depth perception considering camera motion depends much more on the content of the volume than on the chosen visualization technique. Furthermore, we found that traditional rendering techniques, which have often performed poorly in previous studies, showed comparable performance to physically based methods in our study.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13357",
        "abstract url": "https://arxiv.org/abs/2401.13357",
        "title": "Linear Relative Pose Estimation Founded on Pose-only Imaging Geometry",
        "rating": -1,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "How to efficiently and accurately handle image matching outliers is a critical issue in two-view relative estimation. The prevailing RANSAC method necessitates that the minimal point pairs be inliers. This paper introduces a linear relative pose estimation algorithm for n $( n \\geq 6$) point pairs, which is founded on the recent pose-only imaging geometry to filter out outliers by proper reweighting. The proposed algorithm is able to handle planar degenerate scenes, and enhance robustness and accuracy in the presence of a substantial ratio of outliers. Specifically, we embed the linear global translation (LiGT) constraint into the strategies of iteratively reweighted least-squares (IRLS) and RANSAC so as to realize robust outlier removal. Simulations and real tests of the Strecha dataset show that the proposed algorithm achieves relative rotation accuracy improvement of 2 $\\sim$ 10 times in face of as large as 80% outliers.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13362",
        "abstract url": "https://arxiv.org/abs/2401.13362",
        "title": "TraKDis: A Transformer-based Knowledge Distillation Approach for Visual Reinforcement Learning with Application to Cloth Manipulation",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Approaching robotic cloth manipulation using reinforcement learning based on visual feedback is appealing as robot perception and control can be learned simultaneously. However, major challenges result due to the intricate dynamics of cloth and the high dimensionality of the corresponding states, what shadows the practicality of the idea. To tackle these issues, we propose TraKDis, a novel Transformer-based Knowledge Distillation approach that decomposes the visual reinforcement learning problem into two distinct stages. In the first stage, a privileged agent is trained, which possesses complete knowledge of the cloth state information. This privileged agent acts as a teacher, providing valuable guidance and training signals for subsequent stages. The second stage involves a knowledge distillation procedure, where the knowledge acquired by the privileged agent is transferred to a vision-based agent by leveraging pre-trained state estimation and weight initialization. TraKDis demonstrates better performance when compared to state-of-the-art RL techniques, showing a higher performance of 21.9%, 13.8%, and 8.3% in cloth folding tasks in simulation. Furthermore, to validate robustness, we evaluate the agent in a noisy environment; the results indicate its ability to handle and adapt to environmental uncertainties effectively. Real robot experiments are also conducted to showcase the efficiency of our method in real-world scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted for IEEE Robotics and Automation Letters in January 2024"
    },
    {
        "paper id": "2401.13365",
        "abstract url": "https://arxiv.org/abs/2401.13365",
        "title": "Organizing Scientific Knowledge From Energy System Research Using the Open Research Knowledge Graph",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Engineering sciences, such as energy system research, play an important role in developing solutions to technical, environmental, economic, and social challenges of our modern society. In this context, the transformation of energy systems into climate-neutral systems is one of the key strategies for mitigating climate change. For the transformation of energy systems, engineers model, simulate and analyze scenarios and transformation pathways to initiate debates about possible transformation strategies. For these debates and research in general, all steps of the research process must be traceable to guarantee the trustworthiness of published results, avoid redundancies, and ensure their social acceptance. However, the analysis of energy systems is an interdisciplinary field as the investigations of large, complex energy systems often require the use of different software applications and large amounts of heterogeneous data. Engineers must therefore communicate, understand, and (re)use heterogeneous scientific knowledge and data. Although the importance of FAIR scientific knowledge and data in the engineering sciences and energy system research is increasing, little research has been conducted on this topic. When it comes to publishing scientific knowledge and data from publications, software, and datasets (such as models, scenarios, and simulations) openly available and transparent, energy system research lags behind other research domains. According to Schmitt et al. and Nie\u00dfe et al., engineers need technical support in the form of infrastructures, services, and terminologies to improve communication, understanding, and (re)use of scientific knowledge and data.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "1. NFDI4Energy Conference"
    },
    {
        "paper id": "2401.13386",
        "abstract url": "https://arxiv.org/abs/2401.13386",
        "title": "Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain",
        "rating": -1,
        "keywords": [
            [
                "biometric",
                "Face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Face recognition technology has been deployed in various real-life applications. The most sophisticated deep learning-based face recognition systems rely on training millions of face images through complex deep neural networks to achieve high accuracy. It is quite common for clients to upload face images to the service provider in order to access the model inference. However, the face image is a type of sensitive biometric attribute tied to the identity information of each user. Directly exposing the raw face image to the service provider poses a threat to the user's privacy. Current privacy-preserving approaches to face recognition focus on either concealing visual information on model input or protecting model output face embedding. The noticeable drop in recognition accuracy is a pitfall for most methods. This paper proposes a hybrid frequency-color fusion approach to reduce the input dimensionality of face recognition in the frequency domain. Moreover, sparse color information is also introduced to alleviate significant accuracy degradation after adding differential privacy noise. Besides, an identity-specific embedding mapping scheme is applied to protect original face embedding by enlarging the distance among identities. Lastly, secure multiparty computation is implemented for safely computing the embedding distance during model inference. The proposed method performs well on multiple widely used verification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy than the state-of-the-art in the 1:N verification scenario.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This work is already accepted at the conference International Conference on Computer Vision Theory and Applications (VISAPP) 2024 as a regular paper"
    },
    {
        "paper id": "2401.13400",
        "abstract url": "https://arxiv.org/abs/2401.13400",
        "title": "On fixed point theory in partially ordered sets and an application to asymptotic complexity of algorithms",
        "rating": -1,
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "The celebrated Kleene fixed point theorem is crucial in the mathematical modelling of recursive specifications in Denotational Semantics. In this paper we discuss whether the hypothesis of the aforementioned result can be weakened. An affirmative answer to the aforesaid inquiry is provided so that a characterization of those properties that a self-mapping must satisfy in order to guarantee that its set of fixed points is non-empty when no notion of completeness are assumed to be satisfied by the partially ordered set. Moreover, the case in which the partially ordered set is coming from a quasi-metric space is treated in depth. Finally, an application of the exposed theory is obtained. Concretely, a mathematical method to discuss the asymptotic complexity of those algorithms whose running time of computing fulfills a recurrence equation is presented. Moreover, the aforesaid method retrieves the fixed point based methods that appear in the literature for asymptotic complexity analysis of algorithms. However, our new method improves the aforesaid methods because it imposes fewer requirements than those that have been assumed in the literature and, in addition, it allows to state simultaneously upper and lower asymptotic bounds for the running time computing.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13403",
        "abstract url": "https://arxiv.org/abs/2401.13403",
        "title": "SEDNet: Shallow Encoder-Decoder Network for Brain Tumor Segmentation",
        "rating": -1,
        "keywords": [
            [
                "diagnosis",
                "clinical",
                "Tumor"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Despite the advancement in computational modeling towards brain tumor segmentation, of which several models have been developed, it is evident from the computational complexity of existing models which are still at an all-time high, that performance and efficiency under clinical application scenarios are limited. Therefore, this paper proposes a shallow encoder and decoder network named SEDNet for brain tumor segmentation. The proposed network is adapted from the U-Net structure. Though brain tumors do not assume complex structures like the task the traditional U-Net was designed for, their variance in appearance, shape, and ambiguity of boundaries makes it a compelling complex task to solve. SEDNet architecture design is inspired by the localized nature of brain tumors in brain images, thus consists of sufficient hierarchical convolutional blocks in the encoding pathway capable of learning the intrinsic features of brain tumors in brain slices, and a decoding pathway with selective skip path sufficient for capturing miniature local-level spatial features alongside the global-level features of brain tumor. SEDNet with the integration of the proposed preprocessing algorithm and optimization function on the BraTS2020 set reserved for testing achieves impressive dice and Hausdorff scores of 0.9308, 0.9451, 0.9026, and 0.7040, 1.2866, 0.7762 for non-enhancing tumor core (NTC), peritumoral edema (ED), and enhancing tumor (ET), respectively. Furthermore, through transfer learning with initialized SEDNet pre-trained weights, termed SEDNetX, a performance increase is observed. The dice and Hausdorff scores recorded are 0.9336, 0.9478, 0.9061, 0.6983, 1.2691, and 0.7711 for NTC, ED, and ET, respectively. With about 1.3 million parameters and impressive performance in comparison to the state-of-the-art, SEDNet(X) is shown to be computationally efficient for real-time clinical diagnosis.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "8 pages, 7 figures, 3 Tables"
    },
    {
        "paper id": "2401.13418",
        "abstract url": "https://arxiv.org/abs/2401.13418",
        "title": "Serial fusion of multi-modal biometric systems",
        "rating": -1,
        "keywords": [
            [
                "biometric"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Serial, or sequential, fusion of multiple biometric matchers has been not thoroughly investigated so far. However, this approach exhibits some advantages with respect to the widely adopted parallel approaches. In this paper, we propose a novel theoretical framework for the assessment of performance of such systems, based on a previous work of the authors. Benefits in terms of performance are theoretically evaluated, as well as estimation errors in the model parameters computation. Model is analyzed from the viewpoint of its pros and cons, by mean of preliminary experiments performed on NIST Biometric Score Set 1.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13439",
        "abstract url": "https://arxiv.org/abs/2401.13439",
        "title": "Model Predictive Wave Disturbance Rejection for Underwater Soft Robotic Manipulators",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Inspired by the octopus and other animals living in water, soft robots should naturally lend themselves to underwater operations, as supported by encouraging validations in deep water scenarios. This work deals with equipping soft arms with the intelligence necessary to move precisely in wave-dominated environments, such as shallow waters where marine renewable devices are located. This scenario is substantially more challenging than calm deep water since, at low operational depths, hydrodynamic wave disturbances can represent a significant impediment. We propose a control strategy based on Nonlinear Model Predictive Control that can account for wave disturbances explicitly, optimising control actions by considering an estimate of oncoming hydrodynamic loads. The proposed strategy is validated through a set of tasks covering set-point regulation, trajectory tracking and mechanical failure compensation, all under a broad range of varying significant wave heights and peak spectral periods. The proposed control methodology displays positional error reductions as large as 84% with respect to a baseline controller, proving the effectiveness of the method. These initial findings present a first step in the development and deployment of soft manipulators for performing tasks in hazardous water environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "To be presented at RoboSoft 2024, San Diego"
    },
    {
        "paper id": "2401.13451",
        "abstract url": "https://arxiv.org/abs/2401.13451",
        "title": "Experimental validation of ultra-shortened 3D finite element models for frequency-domain analyses of three-core armored cables",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "Recently, large offshore wind power plants have been installed far from the shore, using long HVAC three-core armored cables to export power. Its high capacitance may contribute to the appearance of unwanted phenomena, such as overvoltages or resonances at low frequencies. To adequately assess these problems, detailed and reliable cable models are required to develop time-domain/frequency-domain analyses on this type of cables. This paper presents, for the first time in the literature, an assessment on the performance of 3D finite element method-based (3D-FEM) models for developing frequency-domain analyses on three-core armored cables, confronting simulation results with experimental measurements found in the literature for three real cables. To this aim, a simplified ultra-shortened 3D-FEM model is proposed to reduce the simulation time during frequency sweeps, through which relevant aspects never analyzed before with frequency-domain 3D-FEM simulations are addressed, such as total losses, induced sheath current, magnetic field around the power cable, positive and zero sequence harmonic impedances, as well as resonant frequencies. Also, a time-domain example derived from the frequency-domain analysis is provided. Remarkable results are obtained when comparing computed values and measurements, presenting the simplified ultra-shortened 3DFEM model as a valuable tool for the frequency-domain analysis of these cables.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13462",
        "abstract url": "https://arxiv.org/abs/2401.13462",
        "title": "Growing from Exploration: A self-exploring framework for robots based on foundation models",
        "rating": -1,
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "Intelligent robot is the ultimate goal in the robotics field. Existing works leverage learning-based or optimization-based methods to accomplish human-defined tasks. However, the challenge of enabling robots to explore various environments autonomously remains unresolved. In this work, we propose a framework named GExp, which enables robots to explore and learn autonomously without human intervention. To achieve this goal, we devise modules including self-exploration, knowledge-base-building, and close-loop feedback based on foundation models. Inspired by the way that infants interact with the world, GExp encourages robots to understand and explore the environment with a series of self-generated tasks. During the process of exploration, the robot will acquire skills from beneficial experiences that are useful in the future. GExp provides robots with the ability to solve complex tasks through self-exploration. GExp work is independent of prior interactive knowledge and human intervention, allowing it to adapt directly to different scenarios, unlike previous studies that provided in-context examples as few-shot learning. In addition, we propose a workflow of deploying the real-world robot system with self-learned skills as an embodied assistant.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2401.13472",
        "abstract url": "https://arxiv.org/abs/2401.13472",
        "title": "Segmenting Cardiac Muscle Z-disks with Deep Neural Networks",
        "rating": -1,
        "keywords": [
            [
                "disease",
                "Cardiac"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Z-disks are complex structures that delineate repeating sarcomeres in striated muscle. They play significant roles in cardiomyocytes such as providing mechanical stability for the contracting sarcomere, cell signalling and autophagy. Changes in Z-disk architecture have been associated with impaired cardiac function. Hence, there is a strong need to create tools to segment Z-disks from microscopy images, that overcome traditional limitations such as variability in image brightness and staining technique. In this study, we apply deep learning based segmentation models to extract Z-disks in images of striated muscle tissue. We leverage a novel Airyscan confocal dataset, which comprises high resolution images of Z-disks of healthy heart tissue, stained with Affimers for specific Z-disk proteins. We employed an interactive labelling tool, Ilastik to obtain ground truth segmentation masks and use the resulting data set to train and evaluate the performance of several state-of-the-art segmentation networks. On the test set, UNet++ achieves best segmentation performance for Z-disks in cardiomyocytes, with an average Dice score of 0.91 and outperforms other established segmentation methods including UNet, FPN, DeepLabv3+ and pix2pix. However, pix2pix demonstrates improved generalisation, when tested on an additional dataset of cardiomyocytes with a titin mutation. This is the first study to demonstrate that automated machine learning-based segmentation approaches may be used effectively to segment Z-disks in confocal microscopy images. Automated segmentation approaches and predicted segmentation masks could be used to derive morphological features of Z-disks (e.g. width and orientation), and subsequently, to quantify disease-related changes to cardiac microstructure.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13502",
        "abstract url": "https://arxiv.org/abs/2401.13502",
        "title": "Faster Combinatorial k-Clique Algorithms",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Detecting if a graph contains a $k$-Clique is one of the most fundamental problems in computer science. The asymptotically fastest algorithm runs in time $O(n^{\u03c9k/3})$, where $\u03c9$ is the exponent of Boolean matrix multiplication. To date, this is the only technique capable of beating the trivial $O(n^k)$ bound by a polynomial factor. Due to this technique's various limitations, much effort has gone into designing \"combinatorial\" algorithms that improve over exhaustive search via other techniques. The first contribution of this work is a faster combinatorial algorithm for $k$-Clique, improving Vassilevska's bound of $O(n^{k}/\\log^{k-1}{n})$ by two log factors. Technically, our main result is a new reduction from $k$-Clique to Triangle detection that exploits the same divide-and-conquer at the core of recent combinatorial algorithms by Chan (SODA'15) and Yu (ICALP'15). Our second contribution is exploiting combinatorial techniques to improve the state-of-the-art (even of non-combinatorial algorithms) for generalizations of the $k$-Clique problem. In particular, we give the first $o(n^k)$ algorithm for $k$-clique in hypergraphs and an $O(n^3/\\log^{2.25}{n} + t)$ algorithm for listing $t$ triangles in a graph.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13511",
        "abstract url": "https://arxiv.org/abs/2401.13511",
        "title": "Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images",
        "rating": -1,
        "keywords": [
            [
                "Whole Slide"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Tissue segmentation is a routine preprocessing step to reduce the computational cost of whole slide image (WSI) analysis by excluding background regions. Traditional image processing techniques are commonly used for tissue segmentation, but often require manual adjustments to parameter values for atypical cases, fail to exclude all slide and scanning artifacts from the background, and are unable to segment adipose tissue. Pen marking artifacts in particular can be a potential source of bias for subsequent analyses if not removed. In addition, several applications require the separation of individual cross-sections, which can be challenging due to tissue fragmentation and adjacent positioning. To address these problems, we develop a convolutional neural network for tissue and pen marking segmentation using a dataset of 200 H&E stained WSIs. For separating tissue cross-sections, we propose a novel post-processing method based on clustering predicted centroid locations of the cross-sections in a 2D histogram. On an independent test set, the model achieved a mean Dice score of 0.981$\\pm$0.033 for tissue segmentation and a mean Dice score of 0.912$\\pm$0.090 for pen marking segmentation. The mean absolute difference between the number of annotated and separated cross-sections was 0.075$\\pm$0.350. Our results demonstrate that the proposed model can accurately segment H&E stained tissue cross-sections and pen markings in WSIs while being robust to many common slide and scanning artifacts. The model with trained model parameters and post-processing method are made publicly available as a Python package called SlideSegmenter.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "6 pages, 3 figures"
    },
    {
        "paper id": "2401.13512",
        "abstract url": "https://arxiv.org/abs/2401.13512",
        "title": "Can GPT-3.5 Generate and Code Discharge Summaries?",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Objective: To investigate GPT-3.5 in generating and coding medical documents with ICD-10 codes for data augmentation on low-resources labels. Materials and Methods: Employing GPT-3.5 we generated and coded 9,606 discharge summaries based on lists of ICD-10 code descriptions of patients with infrequent (generation) codes within the MIMIC-IV dataset. Combined with the baseline training set, this formed an augmented training set. Neural coding models were trained on baseline and augmented data and evaluated on a MIMIC-IV test set. We report micro- and macro-F1 scores on the full codeset, generation codes, and their families. Weak Hierarchical Confusion Matrices were employed to determine within-family and outside-of-family coding errors in the latter codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided self-generated data and real MIMIC-IV data. Clinical professionals evaluated the clinical acceptability of the generated documents. Results: Augmentation slightly hinders the overall performance of the models but improves performance for the generation candidate codes and their families, including one unseen in the baseline training data. Augmented models display lower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the prompted descriptions, but performs poorly on real data. Evaluators note the correctness of generated concepts while suffering in variety, supporting information, and narrative. Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding. Augmentation positively affects generation code families but mainly benefits codes with existing examples. Augmentation reduces out-of-family errors. Discharge summaries generated by GPT-3.5 state prompted concepts correctly but lack variety, and authenticity in narratives. They are unsuitable for clinical practice.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "15 pages; 250 words in abstract; 3,929 words in main body; 2 figures (0 black and white, 2 colour); 4 tables; 34 references"
    },
    {
        "paper id": "2401.13516",
        "abstract url": "https://arxiv.org/abs/2401.13516",
        "title": "Delocate: Detection and Localization for Deepfake Videos with Randomly-Located Tampered Traces",
        "rating": -1,
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deepfake videos are becoming increasingly realistic, showing subtle tampering traces on facial areasthat vary between frames. Consequently, many existing Deepfake detection methods struggle to detect unknown domain Deepfake videos while accurately locating the tampered region. To address thislimitation, we propose Delocate, a novel Deepfake detection model that can both recognize andlocalize unknown domain Deepfake videos. Ourmethod consists of two stages named recoveringand localization. In the recovering stage, the modelrandomly masks regions of interest (ROIs) and reconstructs real faces without tampering traces, resulting in a relatively good recovery effect for realfaces and a poor recovery effect for fake faces. Inthe localization stage, the output of the recoveryphase and the forgery ground truth mask serve assupervision to guide the forgery localization process. This process strategically emphasizes the recovery phase of fake faces with poor recovery, facilitating the localization of tampered regions. Ourextensive experiments on four widely used benchmark datasets demonstrate that Delocate not onlyexcels in localizing tampered areas but also enhances cross-domain detection performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2308.09921, arXiv:2305.05943"
    },
    {
        "paper id": "2401.13540",
        "abstract url": "https://arxiv.org/abs/2401.13540",
        "title": "State Estimation for Continuum Multi-Robot Systems on SE(3)",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "In contrast to conventional robots, accurately modeling the kinematics and statics of continuum robots is challenging due to partially unknown material properties, parasitic effects, or unknown forces acting on the continuous body. Consequentially, state estimation approaches that utilize additional sensor information to predict the shape of continuum robots have garnered significant interest. This paper presents a novel approach to state estimation for systems with multiple coupled continuum robots, which allows estimating the shape and strain variables of multiple continuum robots in an arbitrary coupled topology. Simulations and experiments demonstrate the capabilities and versatility of the proposed method, while achieving accurate and continuous estimates for the state of such systems, resulting in average end-effector errors of 3.3 mm and 5.02\u00b0 depending on the sensor setup. It is further shown, that the approach offers fast computation times of below 10 ms, enabling its utilization in quasi-static real-time scenarios with average update rates of 100-200 Hz. An open-source C++ implementation of the proposed state estimation method is made publicly available to the community.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "18 pages, 15 figures, submitted to IEEE Transactions on Robotics"
    },
    {
        "paper id": "2401.13555",
        "abstract url": "https://arxiv.org/abs/2401.13555",
        "title": "Benchmarking the Fairness of Image Upsampling Methods",
        "rating": -1,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\\unicode{x2013}$inspired by their supervised fairness counterparts$\\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13565",
        "abstract url": "https://arxiv.org/abs/2401.13565",
        "title": "Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding",
        "rating": -1,
        "keywords": [
            [
                "grammar"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we present significant advancements in the pretraining of Mistral 7B, a large-scale language model, using a dataset of 32.6 GB, equivalent to 1.1 billion tokens. We explore the impact of extending the context length, releasing models with context lengths of 4096 and 32768 tokens, and further refining performance with a specialized 16384 context length instruction-tuned model, we called it Malaysian Mistral. Our experiments demonstrate the efficacy of continue pretraining and the influence of extended context lengths on Mistral 7B's language understanding capabilities. Additionally, we release a model specifically tuned with a 16384 context length instruction, showcasing its potential for capturing nuanced language intricacies. Furthermore, our research contributes to the benchmarking of Malaysian Mistral against prominent language models, including ChatGPT3.5 and Claude 2. We present compelling results indicating Malaysian Mistral's superior performance on Tatabahasa (Malay grammar) test set, particularly when fine-tuned with instructions. All models released at https://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13578",
        "abstract url": "https://arxiv.org/abs/2401.13578",
        "title": "WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition",
        "rating": -1,
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack. In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference. Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods. In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings. We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions. Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram; 2) trigger generation; 3) the generation of the poisoned dataset. Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods. Besides, we also provide visualization analyses to explain why our method works.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "13 pages, 21 figures"
    },
    {
        "paper id": "2401.13585",
        "abstract url": "https://arxiv.org/abs/2401.13585",
        "title": "Latency vs precision: Stability preserving perception scheduling",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "In robotic systems, perception latency is a term that refers to the computing time measured from the data acquisition to the moment in which perception output is ready to be used to compute control commands. There is a compromise between perception latency, precision for the overall robotic system, and computational resource usage referred to here as the latency-precision trade-off. In this work, we analyze a robot model given by a linear system, a zero-order hold controller, and measurements taken by several perception mode possibilities with different noise levels. We show that the analysis of this system is reduced to studying an equivalent switching system. Our goal is to schedule perception modes such that stability is attained while optimizing a cost function that models the latency-precision trade-off. Our solution framework comprises three main tools: the construction of perception scheduling policy candidates, admissibility verification for policy candidates, and optimal strategies based on admissible policies.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This is the accepted version of an already published manuscript. See journal reference"
    },
    {
        "paper id": "2401.13609",
        "abstract url": "https://arxiv.org/abs/2401.13609",
        "title": "Building Contextual Knowledge Graphs for Personalized Learning Recommendations using Text Mining and Semantic Graph Completion",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated. We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13641",
        "abstract url": "https://arxiv.org/abs/2401.13641",
        "title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability",
        "rating": -1,
        "keywords": [
            [
                "Biometrics",
                "Face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning). The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of automatic decisions in human scenarios. Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field. The results achieved in this study show the potential of LLMs such as ChatGPT for face biometrics, especially to enhance explainability. For reproducibility reasons, we release all the code in GitHub.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13643",
        "abstract url": "https://arxiv.org/abs/2401.13643",
        "title": "Design, Development, and Deployment of Context-Adaptive AI Systems for Enhanced End-User Adoption",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "My research centers on the development of context-adaptive AI systems to improve end-user adoption through the integration of technical methods. I deploy these AI systems across various interaction modalities, including user interfaces and embodied agents like robots, to expand their practical applicability. My research unfolds in three key stages: design, development, and deployment. In the design phase, user-centered approaches were used to understand user experiences with AI systems and create design tools for user participation in crafting AI explanations. In the ongoing development stage, a safety-guaranteed AI system for a robot agent was created to automatically provide adaptive solutions and explanations for unforeseen scenarios. The next steps will involve the implementation and evaluation of context-adaptive AI systems in various interaction forms. I seek to prioritize human needs in technology development, creating AI systems that tangibly benefit end-users in real-world applications and enhance interaction experiences.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 5 pages"
    },
    {
        "paper id": "2401.13650",
        "abstract url": "https://arxiv.org/abs/2401.13650",
        "title": "Tyche: Stochastic In-Context Learning for Medical Image Segmentation",
        "rating": -1,
        "keywords": [
            [
                "Medical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Existing learning-based solutions to medical image segmentation have two important shortcomings. First, for most new segmentation task, a new model has to be trained or fine-tuned. This requires extensive resources and machine learning expertise, and is therefore often infeasible for medical researchers and clinicians. Second, most existing segmentation methods produce a single deterministic segmentation mask for a given image. In practice however, there is often considerable uncertainty about what constitutes the correct segmentation, and different expert annotators will often segment the same image differently. We tackle both of these problems with Tyche, a model that uses a context set to generate stochastic predictions for previously unseen tasks without the need to retrain. Tyche differs from other in-context segmentation methods in two important ways. (1) We introduce a novel convolution block architecture that enables interactions among predictions. (2) We introduce in-context test-time augmentation, a new mechanism to provide prediction stochasticity. When combined with appropriate model design and loss functions, Tyche can predict a set of plausible diverse segmentation candidates for new or unseen medical images and segmentation tasks without the need to retrain.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13761",
        "abstract url": "https://arxiv.org/abs/2401.13761",
        "title": "Experimental validation of ultra-shortened 3D finite element electromagnetic modeling of three-core armored cables at power frequency",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "Due to recent advances, the numerical analysis of submarine three-core armored cables can nowadays be developed through the finite element method (FEM) in a small slice of the cable. This strongly reduces the computational burden and simulation time. However, the performance of this ultra-shortened 3D-FEM model is still to be fully assessed with experimental measurements. This paper focuses on this validation for an extensive variety of situations through the experimental measurements available in the specialized literature for up to 10 actual cables. In particular, it deals not only with relevant calculations at power frequency, like the series resistance and inductive reactance or the induced sheath current, but also with other aspects never analyzed before through 3D-FEM simulations, such as the zero sequence impedance, the magnetic field distribution around the power cable, as well as side effects due to the nonlinear properties of the armor wires. All this considering different armoring and sheath bonding configurations. Results show a very good agreement between measured and computed values, presenting the ultra-shortened 3D-FEM model as a suitable tool for the analysis and design of three-core armored cables, and opening the possibility to reduce the need of extensive experimental tests in the design stage of new cables.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13769",
        "abstract url": "https://arxiv.org/abs/2401.13769",
        "title": "Multiview Graph Learning with Consensus Graph",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Graph topology inference, i.e., learning graphs from a given set of nodal observations, is a significant task in many application domains. Existing approaches are mostly limited to learning a single graph assuming that the observed data is homogeneous. This is problematic because many modern datasets are heterogeneous or mixed and involve multiple related graphs, i.e., multiview graphs. Recent work proposing to learn multiview graphs ensures the similarity of learned view graphs through pairwise regularization, where each pair of views is encouraged to have similar structures. However, this approach cannot infer the shared structure across views. In this work, we propose an alternative method based on consensus regularization, where views are ensured to be similar through a learned consensus graph representing the common structure of the views. In particular, we propose an optimization problem, where graph data is assumed to be smooth over the multiview graph and the topology of the individual views and that of the consensus graph are learned, simultaneously. Our optimization problem is designed to be general in the sense that different regularization functions can be used depending on what the shared structure across views is. Moreover, we propose two regularization functions that extend fused and group graphical lasso to consensus based regularization. Proposed multiview graph learning is evaluated on simulated data and shown to have better performance than existing methods. It is also employed to infer the functional brain connectivity networks of multiple subjects from their electroencephalogram (EEG) recordings. The proposed method reveals the structure shared by subjects as well as the characteristics unique to each subject.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13785",
        "abstract url": "https://arxiv.org/abs/2401.13785",
        "title": "Unified Spatio-Temporal Tri-Perspective View Representation for 3D Semantic Occupancy Prediction",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Holistic understanding and reasoning in 3D scenes play a vital role in the success of autonomous driving systems. The evolution of 3D semantic occupancy prediction as a pretraining task for autonomous driving and robotic downstream tasks capture finer 3D details compared to methods like 3D detection. Existing approaches predominantly focus on spatial cues such as tri-perspective view embeddings (TPV), often overlooking temporal cues. This study introduces a spatiotemporal transformer architecture S2TPVFormer for temporally coherent 3D semantic occupancy prediction. We enrich the prior process by including temporal cues using a novel temporal cross-view hybrid attention mechanism (TCVHA) and generate spatiotemporal TPV embeddings (i.e. S2TPV embeddings). Experimental evaluations on the nuScenes dataset demonstrate a substantial 4.1% improvement in mean Intersection over Union (mIoU) for 3D Semantic Occupancy compared to TPVFormer, confirming the effectiveness of the proposed S2TPVFormer in enhancing 3D scene perception.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13786",
        "abstract url": "https://arxiv.org/abs/2401.13786",
        "title": "FoVA-Depth: Field-of-View Agnostic Depth Estimation for Cross-Dataset Generalization",
        "rating": -1,
        "keywords": [
            [
                "Depth"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Wide field-of-view (FoV) cameras efficiently capture large portions of the scene, which makes them attractive in multiple domains, such as automotive and robotics. For such applications, estimating depth from multiple images is a critical task, and therefore, a large amount of ground truth (GT) data is available. Unfortunately, most of the GT data is for pinhole cameras, making it impossible to properly train depth estimation models for large-FoV cameras. We propose the first method to train a stereo depth estimation model on the widely available pinhole data, and to generalize it to data captured with larger FoVs. Our intuition is simple: We warp the training data to a canonical, large-FoV representation and augment it to allow a single network to reason about diverse types of distortions that otherwise would prevent generalization. We show strong generalization ability of our approach on both indoor and outdoor datasets, which was not possible with previous methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "3DV 2024 (Oral); Project Website: https://research.nvidia.com/labs/lpr/fova-depth/"
    },
    {
        "paper id": "2401.13810",
        "abstract url": "https://arxiv.org/abs/2401.13810",
        "title": "Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4",
        "rating": -1,
        "keywords": [
            [
                "diagnosis"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model's immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13822",
        "abstract url": "https://arxiv.org/abs/2401.13822",
        "title": "Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face",
        "rating": -1.0,
        "keywords": [
            [
                "Face"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face -- one of the largest platforms for sharing and collaborating on ML models and datasets -- as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, while the Considerations for Using the Data section receives the lowest proportion of content. (3) By analyzing the subsections within each section and utilizing topic modeling to identify key topics, we uncover what is discussed in each section, and underscore significant themes encompassing both technical and social impacts, as well as limitations within the Considerations for Using the Data section. (4) Our findings also highlight the need for improved accessibility and reproducibility of datasets in the Usage sections. (5) In addition, our human annotation evaluation emphasizes the pivotal role of comprehensive dataset content in shaping individuals' perceptions of a dataset card's overall quality. Overall, our study offers a unique perspective on analyzing dataset documentation through large-scale data science analysis and underlines the need for more thorough dataset documentation in machine learning research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to the main conference of ICLR 2024"
    },
    {
        "paper id": "2401.13839",
        "abstract url": "https://arxiv.org/abs/2401.13839",
        "title": "Edge-coloring sparse graphs with $\u0394$ colors in quasilinear time",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In this paper we show that every graph $G$ of bounded maximum average degree ${\\rm mad}(G)$ and with maximum degree $\u0394$ can be edge-colored using the optimal number of $\u0394$ colors in quasilinear expected time, whenever $\u0394\\ge 2{\\rm mad}(G)$. The maximum average degree is within a multiplicative constant of other popular graph sparsity parameters like arboricity, degeneracy or maximum density. Our algorithm extends previous results of Chrobak and Nishizeki [J. Algorithms, 1990] and Bhattacharya, Costa, Panski and Solomon [arXiv, 2023].",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13865",
        "abstract url": "https://arxiv.org/abs/2401.13865",
        "title": "Appearance Debiased Gaze Estimation via Stochastic Subject-Wise Adversarial Learning",
        "rating": -1,
        "keywords": [
            [
                "Face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13867",
        "abstract url": "https://arxiv.org/abs/2401.13867",
        "title": "Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation",
        "rating": -1,
        "keywords": [
            [
                "Medical",
                "healthcare",
                "survival"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Through both qualitative and quantitative analyses, we find that these models tend to project higher costs and longer hospitalizations for White populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain races, and disparities in treatment recommendations, etc. Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all patients.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13868",
        "abstract url": "https://arxiv.org/abs/2401.13868",
        "title": "Shell topology optimization based on level set method",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "This paper proposes a level set-based method for optimizing shell structures with large design changes in shape and topology. Conventional shell optimization methods, whether parametric or nonparametric, often only allow limited design changes in shape. In the proposed method, the shell structure is defined as the isosurface of a level set function. The level set function is iteratively updated based on the shape sensitivity on the surface mesh. Therefore, the proposed method can represent an arbitrary manifold surface while dealing with topological changes, for example, from a spherical surface to a toroidal surface. We applied the proposed method to the mean compliance minimization problems of 3D shell structural designs for dome, bending plate and cantilever beam examples to demonstrate its efficacy of the proposed method.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "13 pages, 13 figures"
    },
    {
        "paper id": "2401.13887",
        "abstract url": "https://arxiv.org/abs/2401.13887",
        "title": "A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification",
        "rating": -1,
        "keywords": [
            [
                "cancer",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the differences were more prominent. Frequent sources of GPT-4 errors included inferences from multiple samples and complex task design. On complex tasks where large annotated datasets cannot be easily collected, LLMs can reduce the burden of large-scale data labeling. However, if the use of LLMs is prohibitive, the use of simpler supervised models with large annotated datasets can provide comparable results. LLMs demonstrated the potential to speed up the execution of clinical NLP studies by reducing the need for curating large annotated datasets. This may result in an increase in the utilization of NLP-based variables and outcomes in observational clinical studies.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13893",
        "abstract url": "https://arxiv.org/abs/2401.13893",
        "title": "A Survey on Indoor Visible Light Positioning Systems: Fundamentals, Applications, and Challenges",
        "rating": -1,
        "keywords": [
            [
                "robot",
                "navigation"
            ]
        ],
        "abstract": "The growing demand for location-based services in areas like virtual reality, robot control, and navigation has intensified the focus on indoor localization. Visible light positioning (VLP), leveraging visible light communications (VLC), becomes a promising indoor positioning technology due to its high accuracy and low cost. This paper provides a comprehensive survey of VLP systems. In particular, since VLC lays the foundation for VLP, we first present a detailed overview of the principles of VLC. The performance of each positioning algorithm is also compared in terms of various metrics such as accuracy, coverage, and orientation limitation. Beyond the physical layer studies, the network design for a VLP system is also investigated, including multi-access technologies resource allocation, and light-emitting diode (LED) placements. Next, the applications of the VLP systems are overviewed. Finally, this paper outlines open issues, challenges, and future research directions for the research field. In a nutshell, this paper constitutes the first holistic survey on VLP from state-of-the-art studies to practical uses.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13922",
        "abstract url": "https://arxiv.org/abs/2401.13922",
        "title": "Simplified Successive Cancellation List Decoding of PAC Codes",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Polar codes are the first class of structured channel codes that achieve the symmetric capacity of binary channels with efficient encoding and decoding. In 2019, Arikan proposed a new polar coding scheme referred to as polarization-adjusted convolutional (PAC)} codes. In contrast to polar codes, PAC codes precode the information word using a convolutional code prior to polar encoding. This results in material coding gain over polar code under Fano sequential decoding as well as successive cancellation list (SCL) decoding. Given the advantages of SCL decoding over Fano decoding in certain scenarios such as low-SNR regime or where a constraint on the worst case decoding latency exists, in this paper, we focus on SCL decoding and present a simplified SCL (SSCL) decoding algorithm for PAC codes. SSCL decoding of PAC codes reduces the decoding latency by identifying special nodes in the decoding tree and processing them at the intermediate stages of the graph. Our simulation results show that the performance of PAC codes under SSCL decoding is almost similar to the SCL decoding while having lower decoding latency.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "7 pages, 3 figures"
    },
    {
        "paper id": "2401.14425",
        "abstract url": "https://arxiv.org/abs/2401.14425",
        "title": "No Longer Trending on Artstation: Prompt Analysis of Generative AI Art",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "Image generation using generative AI is rapidly becoming a major new source of visual media, with billions of AI generated images created using diffusion models such as Stable Diffusion and Midjourney over the last few years. In this paper we collect and analyse over 3 million prompts and the images they generate. Using natural language processing, topic analysis and visualisation methods we aim to understand collectively how people are using text prompts, the impact of these systems on artists, and more broadly on the visual cultures they promote. Our study shows that prompting focuses largely on surface aesthetics, reinforcing cultural norms, popular conventional representations and imagery. We also find that many users focus on popular topics (such as making colouring books, fantasy art, or Christmas cards), suggesting that the dominant use for the systems analysed is recreational rather than artistic.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Paper accepted for EvoMUSART 2024, Aberystwyth, Wales, United Kingdom, 3-5 April 2024"
    },
    {
        "paper id": "2402.16871",
        "abstract url": "https://arxiv.org/abs/2402.16871",
        "title": "Bike3S: A Tool for Bike Sharing Systems Simulation",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "Vehicle sharing systems are becoming increasingly popular. The effectiveness of such systems depends, among other factors, on different strategic and operational management decisions and policies, like the dimension of the fleet or the distribution of vehicles. It is of foremost importance to be able to anticipate and evaluate the potential effects of such strategies before they can be successfully deployed. In this paper we present Bike3S, a simulator for a station-based bike sharing system. The simulator performs semi-realistic simulations of the operation of a bike sharing system and allows for evaluating and testing different management decisions and strategies. In particular, the simulator has been designed to test different station capacities, station distributions, and balancing strategies. The simulator carries out microscopic agent-based simulations, where users of different types can be defined that act according to their individual goals and objectives which influences the overall dynamics of the whole system.",
        "subjects": [
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13480",
        "abstract url": "https://arxiv.org/abs/2401.13480",
        "title": "The Dynamics of (Not) Unfollowing Misinformation Spreaders",
        "rating": -1.5,
        "keywords": [
            [
                "health"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Many studies explore how people 'come into' misinformation exposure. But much less is known about how people 'come out of' misinformation exposure. Do people organically sever ties to misinformation spreaders? And what predicts doing so? Over six months, we tracked the frequency and predictors of ~900K followers unfollowing ~5K health misinformation spreaders on Twitter. We found that misinformation ties are persistent. Monthly unfollowing rates are just 0.52%. In other words, 99.5% of misinformation ties persist each month. Users are also 31% more likely to unfollow non-misinformation spreaders than they are to unfollow misinformation spreaders. Although generally infrequent, the factors most associated with unfollowing misinformation spreaders are (1) redundancy and (2) ideology. First, users initially following many spreaders, or who follow spreaders that tweet often, are most likely to unfollow later. Second, liberals are more likely to unfollow than conservatives. Overall, we observe a strong persistence of misinformation ties. The fact that users rarely unfollow misinformation spreaders suggests a need for external nudges and the importance of preventing exposure from arising in the first place.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "WWW 2024"
    },
    {
        "paper id": "2401.13544",
        "abstract url": "https://arxiv.org/abs/2401.13544",
        "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?",
        "rating": -1.5,
        "keywords": [
            [
                "X-ray"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of the proposed techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes can be as intervenable and more performant than CBMs.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13605",
        "abstract url": "https://arxiv.org/abs/2401.13605",
        "title": "Regulating AI-Based Remote Biometric Identification. Investigating the Public Demand for Bans, Audits, and Public Database Registrations",
        "rating": -1.5,
        "keywords": [
            [
                "Biometric"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "AI is increasingly being used in the public sector, including public security. In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology. RBI systems are used to identify criminal activity in public spaces, but are criticised for inheriting biases and violating fundamental human rights. It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised. While there is a consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI. As a possible counterweight, public opinion can have a decisive influence on policymakers to establish boundaries and conditions under which AI systems should be used -- if at all. However, we know little about the conditions that lead to regulatory demand for AI systems. In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology. In addition, we explore the mediating effects of discrimination perceptions regarding RBI. We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens. We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation. Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13657",
        "abstract url": "https://arxiv.org/abs/2401.13657",
        "title": "Inadequacy of common stochastic neural networks for reliable clinical decision support",
        "rating": -1.5,
        "keywords": [
            [
                "medical",
                "healthcare",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such as Bayesian neural network layers and model ensembles. Our models achieve state of the art performance in terms of discrimination performance (AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality prediction benchmark. However, epistemic uncertainty is critically underestimated by the selected stochastic deep learning methods. A heuristic proof for the responsible collapse of the posterior distribution is provided. Our findings reveal the inadequacy of commonly used stochastic deep learning approaches to reliably recognize OoD samples. In both methods, unsubstantiated model confidence is not prevented due to strongly biased functional posteriors, rendering them inappropriate for reliable clinical decision support. This highlights the need for approaches with more strictly enforced or inherent distance-awareness to known data points, e.g., using kernel-based techniques.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Keywords: probabilistic inference, uncertainty estimation, uncertainty quantification, epistemic uncertainty, clinical prognosis, electronic health records"
    },
    {
        "paper id": "2401.13752",
        "abstract url": "https://arxiv.org/abs/2401.13752",
        "title": "Explaining Image Classifiers",
        "rating": -1.5,
        "keywords": [
            [
                "tumor"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We focus on explaining image classifiers, taking the work of Mothilal et al. [2021] (MMTS) as our point of departure. We observe that, although MMTS claim to be using the definition of explanation proposed by Halpern [2016], they do not quite do so. Roughly speaking, Halpern's definition has a necessity clause and a sufficiency clause. MMTS replace the necessity clause by a requirement that, as we show, implies it. Halpern's definition also allows agents to restrict the set of options considered. While these difference may seem minor, as we show, they can have a nontrivial impact on explanations. We also show that, essentially without change, Halpern's definition can handle two issues that have proved difficult for other approaches: explanations of absence (when, for example, an image classifier for tumors outputs \"no tumor\") and explanations of rare events (such as tumors).",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13756",
        "abstract url": "https://arxiv.org/abs/2401.13756",
        "title": "NLICE: Synthetic Medical Record Generation for Effective Primary Healthcare Differential Diagnosis",
        "rating": -1.5,
        "keywords": [
            [
                "Medical",
                "Healthcare",
                "Diagnosis",
                "disease"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper offers a systematic method for creating medical knowledge-grounded patient records for use in activities involving differential diagnosis. Additionally, an assessment of machine learning models that can differentiate between various conditions based on given symptoms is also provided. We use a public disease-symptom data source called SymCat in combination with Synthea to construct the patients records. In order to increase the expressive nature of the synthetic data, we use a medically-standardized symptom modeling method called NLICE to augment the synthetic data with additional contextual information for each condition. In addition, Naive Bayes and Random Forest models are evaluated and compared on the synthetic data. The paper shows how to successfully construct SymCat-based and NLICE-based datasets. We also show results for the effectiveness of using the datasets to train predictive disease models. The SymCat-based dataset is able to train a Naive Bayes and Random Forest model yielding a 58.8% and 57.1% Top-1 accuracy score, respectively. In contrast, the NLICE-based dataset improves the results, with a Top-1 accuracy of 82.0% and Top-5 accuracy values of more than 90% for both models. Our proposed data generation approach solves a major barrier to the application of artificial intelligence methods in the healthcare domain. Our novel NLICE symptom modeling approach addresses the incomplete and insufficient information problem in the current binary symptom representation approach. The NLICE code is open sourced at https://github.com/guozhuoran918/NLICE.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13805",
        "abstract url": "https://arxiv.org/abs/2401.13805",
        "title": "Longitudinal Sentiment Topic Modelling of Reddit Posts",
        "rating": -1.5,
        "keywords": [
            [
                "health"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "In this study, we analyze texts of Reddit posts written by students of four major Canadian universities. We gauge the emotional tone and uncover prevailing themes and discussions through longitudinal topic modeling of posts textual data. Our study focuses on four years, 2020-2023, covering COVID-19 pandemic and after pandemic years. Our results highlight a gradual uptick in discussions related to mental health.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "21 pages, 4 figures, 13 tables. arXiv admin note: text overlap with arXiv:2401.12382"
    },
    {
        "paper id": "2401.13848",
        "abstract url": "https://arxiv.org/abs/2401.13848",
        "title": "A V2X-based Privacy Preserving Federated Measuring and Learning System",
        "rating": -1.5,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Future autonomous vehicles (AVs) will use a variety of sensors that generate a vast amount of data. Naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. Consequently, vehicles shall exchange their measurement data over Vehicle-to-Everything (V2X) technologies. Moreover, predicting the state of the road network might be beneficial too. With such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. That would decrease transportation costs as well as reduce its environmental impact. In this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V) communication while also operating a federated learning (FL) scheme over the Vehicle-to-Network (V2N) link to create a predictive model of the transportation network. As we are yet to have real-world AV data, we model it with a non-IID (independent and identically distributed) dataset to evaluate the capabilities of the proposed system in terms of performance and privacy. Results indicate that the proposed FL scheme improves learning performance and prevents eavesdropping at the aggregator server side.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2401.13858",
        "abstract url": "https://arxiv.org/abs/2401.13858",
        "title": "Inverse Molecular Design with Multi-Conditional Diffusion Guidance",
        "rating": -1.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. An inverse polymer design task for gas separation with feedback from domain experts further demonstrates its practical utility.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "20 pages, 8 figures, 7 tables"
    },
    {
        "paper id": "2401.13872",
        "abstract url": "https://arxiv.org/abs/2401.13872",
        "title": "Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection",
        "rating": -1.5,
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the rapid advancement in cyber-physical systems, the increasing number of sensors has significantly complicated manual monitoring of system states. Consequently, graph-based time-series anomaly detection methods have gained attention due to their ability to explicitly represent relationships between sensors. However, these methods often apply a uniform source node representation across all connected target nodes, even when updating different target node representations. Moreover, the graph attention mechanism, commonly used to infer unknown graph structures, could constrain the diversity of source node representations. In this paper, we introduce the Edge Conditional Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge conditional node update module, dynamically transforms source node representations based on connected edges to represent target nodes aptly. We validate performance on three real-world datasets: SWaT, WADI, and PSM. Our model demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively, compared to best F1 baseline models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13903",
        "abstract url": "https://arxiv.org/abs/2401.13903",
        "title": "Alternative Interfaces for Human-initiated Natural Language Communication and Robot-initiated Haptic Feedback: Towards Better Situational Awareness in Human-Robot Collaboration",
        "rating": -1.5,
        "keywords": [
            [
                "Robot"
            ],
            [
                "workshop"
            ]
        ],
        "abstract": "This article presents an implementation of a natural-language speech interface and a haptic feedback interface that enables a human supervisor to provide guidance to, request information, and receive status updates from a Spot robot. We provide insights gained during preliminary user testing of the interface in a realistic robot exploration scenario.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Peer reviewed and published at \"Empowering People in Human-Robot Collaboration: Why, How, When, and for Whom\" workshop at OzCHI 2023 conference"
    },
    {
        "paper id": "2401.13929",
        "abstract url": "https://arxiv.org/abs/2401.13929",
        "title": "Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics",
        "rating": -1.5,
        "keywords": [
            [
                "diagnosis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) within the EMBARC study, we propose a novel RL-HMM framework for analyzing reward-based decision-making. Our model accommodates learning strategy switching between two distinct approaches under a hidden Markov model (HMM): subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient EM algorithm for parameter estimation and employ a nonparametric bootstrap for inference. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01701",
        "abstract url": "https://arxiv.org/abs/2402.01701",
        "title": "Nie pozw\u00f3l algorytmom rz\u0105dzi\u0107 Twoim koszykiem: systemy rekomendacyjne w dobie Omnibusa",
        "rating": -1.5,
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "The Omnibus Directive is an essential part of the European Union's New Deal for Consumers. The Directive introduces new regulations in trade, including e-commerce, with the main goal being to increase transparency, fairness and consumer protection. The authors critically draw attention to a significant oversight in the Omnibus Directive, namely the lack of consideration of recommendation systems. Recommendation engines can be a source of potentially harmful practices affecting consumers, hence the need for a directive extension. The proposals presented in this article include the introduction of ethical supervision over recommendation systems to minimize the risk of negative effects of their recommendations, as well as a clear explanation of the criteria on which recommendations are made -- similar to search result rankings. -- Dyrektywa Omnibus stanowi istotn\u0105 cz\u0119\u015b\u0107 Nowego \u0141adu dla Konsument\u00f3w (ang. \\emph{New Deal for Consumers}) Unii Europejskiej. Dyrektywa wprowadza nowe regulacje w handlu, w tym e-commerce, kt\u00f3rych g\u0142\u00f3wnym celem jest zwi\u0119kszenie przejrzysto\u015bci, uczciwo\u015bci i ochrony konsument\u00f3w. Autorzy krytycznie zwracaj\u0105 uwag\u0119 na istotne zaniedbanie w dyrektywie Omnibus, jakim jest brak uwzgl\u0119dnienia system\u00f3w rekomendacyjnych. Silniki rekomendacyjne mog\u0105 by\u0107 \u017ar\u00f3d\u0142em potencjalnie szkodliwych praktyk uderzaj\u0105cych w konsument\u00f3w, st\u0105d niezb\u0119dne jest rozszerzenie dyrektywy. Propozycje przedstawione w niniejszym artykule obejmuj\u0105 wprowadzenie etycznego nadzoru nad systemami rekomenduj\u0105cymi, aby zminimalizowa\u0107 ryzyko negatywnych skutk\u00f3w ich rekomendacji, a tak\u017ce jasne wyja\u015bnienie kryteri\u00f3w, na podstawie kt\u00f3rych dokonywane s\u0105 rekomendacje -- analogicznie do ranking\u00f3w wynik\u00f3w wyszukiwania.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "7 pages, in Polish"
    },
    {
        "paper id": "2402.05940",
        "abstract url": "https://arxiv.org/abs/2402.05940",
        "title": "Causal Relationship Network of Risk Factors Impacting Workday Loss in Underground Coal Mines",
        "rating": -1.5,
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study aims to establish the causal relationship network between various factors leading to workday loss in underground coal mines using a novel causal artificial intelligence (AI) method. The analysis utilizes data obtained from the National Institute for Occupational Safety and Health (NIOSH). A total of 101,010 injury records from 3,982 unique underground coal mines spanning the years from 1990 to 2020 were extracted from the NIOSH database. Causal relationships were analyzed and visualized using a novel causal AI method called Grouped Greedy Equivalence Search (GGES). The impact of each variable on workday loss was assessed through intervention do-calculus adjustment (IDA) scores. Model training and validation were performed using the 10-fold cross-validation technique. Performance metrics, including adjacency precision (AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall (AHR), were utilized to evaluate the models. Findings revealed that after 2006, key direct causes of workday loss among mining employees included total mining experience, mean office employees, mean underground employees, county, and total mining experience (years). Total mining experience emerged as the most influential factor, whereas mean employees per mine exhibited the least influence. The analyses emphasized the significant role of total mining experience in determining workday loss. The models achieved optimal performance, with AP, AR, AHP, and AHR values measuring 0.694, 0.653, 0.386, and 0.345, respectively. This study demonstrates the feasibility of utilizing the new GGES method to clarify the causal factors behind the workday loss by analyzing employment demographics and injury records and establish their causal relationship network.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "5 figures 5 tables"
    },
    {
        "paper id": "2401.13237",
        "abstract url": "https://arxiv.org/abs/2401.13237",
        "title": "Quantum natural gradient without monotonicity",
        "rating": -2,
        "keywords": [
            [
                "Quantum",
                "physics"
            ]
        ],
        "abstract": "Natural gradient (NG) is an information-geometric optimization method that plays a crucial role, especially in the estimation of parameters for machine learning models like neural networks. To apply NG to quantum systems, the quantum natural gradient (QNG) was introduced and utilized for noisy intermediate-scale devices. Additionally, a mathematically equivalent approach to QNG, known as the stochastic reconfiguration method, has been implemented to enhance the performance of quantum Monte Carlo methods. It is worth noting that these methods are based on the symmetric logarithmic derivative (SLD) metric, which is one of the monotone metrics. So far, monotonicity has been believed to be a guiding principle to construct a geometry in physics. In this paper, we propose generalized QNG by removing the condition of monotonicity. Initially, we demonstrate that monotonicity is a crucial condition for conventional QNG to be optimal. Subsequently, we provide analytical and numerical evidence showing that non-monotone QNG outperforms conventional QNG based on the SLD metric in terms of convergence speed.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "6 pages, 3 figures"
    },
    {
        "paper id": "2401.13245",
        "abstract url": "https://arxiv.org/abs/2401.13245",
        "title": "GraphiMind: LLM-centric Interface for Information Graphics Design",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Information graphics are pivotal in effective information dissemination and storytelling. However, creating such graphics is extremely challenging for non-professionals, since the design process requires multifaceted skills and comprehensive knowledge. Thus, despite the many available authoring tools, a significant gap remains in enabling non-experts to produce compelling information graphics seamlessly, especially from scratch. Recent breakthroughs show that Large Language Models (LLMs), especially when tool-augmented, can autonomously engage with external tools, making them promising candidates for enabling innovative graphic design applications. In this work, we propose a LLM-centric interface with the agent GraphiMind for automatic generation, recommendation, and composition of information graphics design resources, based on user intent expressed through natural language. Our GraphiMind integrates a Textual Conversational Interface, powered by tool-augmented LLM, with a traditional Graphical Manipulation Interface, streamlining the entire design process from raw resource curation to composition and refinement. Extensive evaluations highlight our tool's proficiency in simplifying the design process, opening avenues for its use by non-professional users. Moreover, we spotlight the potential of LLMs in reshaping the domain of information graphics design, offering a blend of automation, versatility, and user-centric interactivity.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13247",
        "abstract url": "https://arxiv.org/abs/2401.13247",
        "title": "A Human-Centered Review of Algorithms in Homelessness Research",
        "rating": -2,
        "keywords": [
            [
                "face"
            ]
        ],
        "abstract": "Homelessness is a humanitarian challenge affecting an estimated 1.6 billion people worldwide. In the face of rising homeless populations in developed nations and a strain on social services, government agencies are increasingly adopting data-driven models to determine one's risk of experiencing homelessness and assigning scarce resources to those in need. We conducted a systematic literature review of 57 papers to understand the evolution of these decision-making algorithms. We investigated trends in computational methods, predictor variables, and target outcomes used to develop the models using a human-centered lens and found that only 9 papers (15.7%) investigated model fairness and bias. We uncovered tensions between explainability and ecological validity wherein predictive risk models (53.4%) focused on reductive explainability while resource allocation models (25.9%) were dependent on unrealistic assumptions and simulated data that are not useful in practice. Further, we discuss research challenges and opportunities for developing human-centered algorithms in this area.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "In CHI '24 Proceedings of the CHI Conference on Human Factors in Computing Systems Honolulu, HI, USA"
    },
    {
        "paper id": "2401.13310",
        "abstract url": "https://arxiv.org/abs/2401.13310",
        "title": "Lessons Learned Migrating CUDA to SYCL: A HEP Case Study with ROOT RDataFrame",
        "rating": -2,
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "The world's largest particle accelerator, located at CERN, produces petabytes of data that need to be analysed efficiently, to study the fundamental structures of our universe. ROOT is an open-source C++ data analysis framework, developed for this purpose. Its high-level data analysis interface, RDataFrame, currently only supports CPU parallelism. Given the increasing heterogeneity in computing facilities, it becomes crucial to efficiently support GPGPUs to take advantage of the available resources. SYCL allows for a single-source implementation, which enables support for different architectures. In this paper, we describe a CUDA implementation and the migration process to SYCL, focusing on a core high energy physics operation in RDataFrame -- histogramming. We detail the challenges that we faced when integrating SYCL into a large and complex code base. Furthermore, we perform an extensive comparative performance analysis of two SYCL compilers, AdaptiveCpp and DPC++, and the reference CUDA implementation. We highlight the performance bottlenecks that we encountered, and the methodology used to detect these. Based on our findings, we provide actionable insights for developers of SYCL applications.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13324",
        "abstract url": "https://arxiv.org/abs/2401.13324",
        "title": "Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the \"XAI Novice Question Bank\": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced participants' perceptions of the systems' risks and benefits, which they confirmed or changed depending on the use case. When risks were perceived as high, participants expressed particular interest in explanations about intention, such as why and to what end a system was put in place. With this work, we aim to support the inclusion of affected stakeholders into explainability by contributing an overview of information and challenges relevant to them when deciding on the adoption of ADM systems. We close by summarizing our findings in a list of six key implications that inform the design of future explanations for affected stakeholder audiences.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Main text: 21 pages, 3 figures. Supplementary material is provided. Manuscript submitted for review to IJHCS"
    },
    {
        "paper id": "2401.13352",
        "abstract url": "https://arxiv.org/abs/2401.13352",
        "title": "EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction",
        "rating": -2,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "medical",
                "surgery",
                "Endoscopic"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13366",
        "abstract url": "https://arxiv.org/abs/2401.13366",
        "title": "Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems",
        "rating": -2.0,
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Federated Learning"
            ],
            [
                "face"
            ],
            [
                "cs.LG"
            ],
            [
                "workshop"
            ]
        ],
        "abstract": "Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable global model training despite limiting client resources and statistical data heterogeneity. This improves robustness and scalability for real-world FL deployments.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "6 pages, 5 figures. This work has been accepted by PerCom PerconAI workshop 2024"
    },
    {
        "paper id": "2401.13410",
        "abstract url": "https://arxiv.org/abs/2401.13410",
        "title": "How to Forget Clients in Federated Online Learning to Rank?",
        "rating": -2,
        "keywords": [
            [
                "unlearning"
            ],
            [
                "attack"
            ]
        ],
        "abstract": "Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \\textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation. In this paper, we study an effective and efficient unlearning method that can remove a client's contribution without compromising the overall ranker effectiveness and without needing to retrain the global ranker from scratch. A key challenge is how to measure whether the model has unlearned the contributions from the client $c^*$ that has requested removal. For this, we instruct $c^*$ to perform a poisoning attack (add noise to this client updates) and then we measure whether the impact of the attack is lessened when the unlearning process has taken place. Through experiments on four datasets, we demonstrate the effectiveness and efficiency of the unlearning strategy under different combinations of parameter settings.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted in ECIR 2024"
    },
    {
        "paper id": "2401.13414",
        "abstract url": "https://arxiv.org/abs/2401.13414",
        "title": "GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Current datasets for action recognition tasks face limitations stemming from traditional collection and generation methods, including the constrained range of action classes, absence of multi-viewpoint recordings, limited diversity, poor video quality, and labor-intensive manually collection. To address these challenges, we introduce GTAutoAct, a innovative dataset generation framework leveraging game engine technology to facilitate advancements in action recognition. GTAutoAct excels in automatically creating large-scale, well-annotated datasets with extensive action classes and superior video quality. Our framework's distinctive contributions encompass: (1) it innovatively transforms readily available coordinate-based 3D human motion into rotation-orientated representation with enhanced suitability in multiple viewpoints; (2) it employs dynamic segmentation and interpolation of rotation sequences to create smooth and realistic animations of action; (3) it offers extensively customizable animation scenes; (4) it implements an autonomous video capture and processing pipeline, featuring a randomly navigating camera, with auto-trimming and labeling functionalities. Experimental results underscore the framework's robustness and highlights its potential to significantly improve action recognition model training.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13416",
        "abstract url": "https://arxiv.org/abs/2401.13416",
        "title": "Characterizing Perspective Error in Voxel-Based Lidar Scan Matching",
        "rating": -2,
        "keywords": [
            [
                "Voxel"
            ],
            [
                "Lidar",
                "vehicle"
            ]
        ],
        "abstract": "This paper quantifies an error source that limits the accuracy of lidar scan matching, particularly for voxel-based methods. Lidar scan matching, which is used in dead reckoning (also known as lidar odometry) and mapping, computes the rotation and translation that best align a pair of point clouds. Perspective errors occur when a scene is viewed from different angles, with different surfaces becoming visible or occluded from each viewpoint. To explain perspective anomalies observed in data, this paper models perspective errors for two objects representative of urban landscapes: a cylindrical column and a dual-wall corner. For each object, we provide an analytical model of the perspective error for voxel-based lidar scan matching. We then analyze how perspective errors accumulate as a lidar-equipped vehicle moves past these objects.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13420",
        "abstract url": "https://arxiv.org/abs/2401.13420",
        "title": "Distributed network for measuring climatic parameters in heterogeneous environments: Application in a greenhouse",
        "rating": -2,
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "In Mediterranean countries of Southern Europe, the climatic conditions are usually favourable to cultivate greenhouse vegetables but not always for workers. The aim of this study was to design a network of weather stations capable of gathering data of environmental parameters related to the wellbeing of workers in greenhouses in south-eastern Spain. The unevenness of the thermal environment was studied both vertically as well as horizontally following guideline ISO 7726. The results indicate that the greenhouse should be considered a heterogeneous environment, implying that, for an evaluation of the environmental conditions related to thermal stress of the workers inside the greenhouse, measurements should be taken at different points within the greenhouse at three heights (ankle, abdomen, and head).",
        "subjects": [
            "cs.DC"
        ],
        "comment": "47 pages, 15 figures"
    },
    {
        "paper id": "2401.13438",
        "abstract url": "https://arxiv.org/abs/2401.13438",
        "title": "Keeping Energy-Neutral Devices Operational: a Coherent Massive Beamforming Approach",
        "rating": -2,
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Keeping the batteries on the shelf: this is the holy grail for low-cost Internet of Things (IoT) nodes. In this paper we study the potential of radio frequency (RF)-based wireless power transfer implementing coherent beamforming with many antennas to realize this ambitious target. We optimize the deployment of the antennas to charge electronic shelf labels (ESLs), considering actual regulatory constraints. The results confirm the feasibility to create power spots that are sufficient to keep the high density of battery-less devices operational.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13448",
        "abstract url": "https://arxiv.org/abs/2401.13448",
        "title": "Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can impede knowledge exchange and lead to sub-optimal performance. To address this gap, we introduce the Decentralized Collaborative Learning with Adaptive Reference Data (DARD) framework, which crafts adaptive reference data for effective user collaboration. It first generates a desensitized public reference data pool with transformation and probability data generation methods. For each user, the selection of adaptive reference data is executed in parallel by training loss tracking and influence function. Local models are trained with individual private data and collaboratively with the geographical and semantic neighbors. During the collaboration between two users, they exchange soft decisions based on a combined set of their adaptive reference data. Our evaluations across two real-world datasets highlight DARD's superiority in recommendation performance and addressing the scarcity of available reference data.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13536",
        "abstract url": "https://arxiv.org/abs/2401.13536",
        "title": "Finetuning Foundation Models for Joint Analysis Optimization",
        "rating": -2,
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four $b$-jets.",
        "subjects": [
            "hep-ex"
        ],
        "comment": "13 pages, 12 figures"
    },
    {
        "paper id": "2401.13537",
        "abstract url": "https://arxiv.org/abs/2401.13537",
        "title": "Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models",
        "rating": -2,
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "We propose masked particle modeling (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.",
        "subjects": [
            "hep-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13552",
        "abstract url": "https://arxiv.org/abs/2401.13552",
        "title": "On the Constrained CAV Platoon Control Problem",
        "rating": -2,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "vehicle"
            ]
        ],
        "abstract": "The main objective of the connected and automated vehicle (CAV) platoon control problem is to regulate CAVs' position while ensuring stability and accounting for vehicle dynamics. Although this problem has been studied in the literature, existing research has some limitations. This paper presents two new theoretical results that address these limitations: (i) the synthesis of unrealistic high-gain control parameters due to the lack of a systematic way to incorporate the lower and upper bounds on the control parameters, and (ii) the performance sensitivity to the communication delay due to inaccurate Taylor series approximation. To be more precise, taking advantage of the wellknown Pade approximation, this paper proposes a constrained CAV platoon controller synthesis that (i) systematically incorporates the lower and upper bounds on the control parameters, and (ii) significantly improves the performance sensitivity to the communication delay. The effectiveness of the presented results is verified through conducting extensive numerical simulations. The proposed controller effectively attenuates the stop-and-go disturbance -- a single cycle of deceleration followed by acceleration -- amplification throughout the mixed platoon (consisting of CAVs and human-driven vehicles). Modern transportation systems will benefit from the proposed CAV controls in terms of effective disturbance attenuation as it will potentially reduce collisions.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13560",
        "abstract url": "https://arxiv.org/abs/2401.13560",
        "title": "SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The Transformer architecture has shown a remarkable ability in modeling global relationships. However, it poses a significant computational challenge when processing high-dimensional medical images. This hinders its development and widespread adoption in this task. Mamba, as a State Space Model (SSM), recently emerged as a notable manner for long-range dependencies in sequential modeling, excelling in natural language processing filed with its remarkable memory efficiency and computational speed. Inspired by its success, we introduce SegMamba, a novel 3D medical image \\textbf{Seg}mentation \\textbf{Mamba} model, designed to effectively capture long-range dependencies within whole volume features at every scale. Our SegMamba, in contrast to Transformer-based methods, excels in whole volume feature modeling from a state space model standpoint, maintaining superior processing speed, even with volume features at a resolution of {$64\\times 64\\times 64$}. Comprehensive experiments on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our SegMamba. The code for SegMamba is available at: https://github.com/ge-xing/SegMamba",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code has released"
    },
    {
        "paper id": "2401.13566",
        "abstract url": "https://arxiv.org/abs/2401.13566",
        "title": "A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in Recommendation",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups. In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results. Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists. In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models. This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity. Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility. Source code and pre-processed data can be retrieved at https://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted at the 46th European Conference on Information Retrieval (ECIR 2024)"
    },
    {
        "paper id": "2401.13569",
        "abstract url": "https://arxiv.org/abs/2401.13569",
        "title": "SPARC-LoRa: A Scalable, Power-efficient, Affordable, Reliable, and Cloud Service-enabled LoRa Networking System for Agriculture Applications",
        "rating": -2,
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "With the rapid development of cloud and edge computing, Internet of Things (IoT) applications have been deployed in various aspects of human life. In this paper, we design and implement a holistic LoRa-based IoT system with LoRa communication capabilities, named SPARC-LoRa, which consists of field sensor nodes and a gateway connected to the Internet. SPARC-LoRa has the following important features. First, the proposed wireless network of SPARC-LoRa is even-driven and using off-the-shelf microcontroller and LoRa communication modules with a customized PCB design to integrate all the hardware. This enables SPARC-LoRa to achieve low power consumption, long range communication, and low cost. With a new connection-based upper layer protocol design, the scalability and communication reliability of SPARC-loRa can be achieved. Second, an open source software including sensor nodes and servers is designed based on Docker container with cloud storage, computing, and LTE functionalities. In order to achieve reliable wireless communication under extreme conditions, a relay module is designed and applied to SPARC-LoRa to forward the data from sensor nodes to the gateway node. The system design and implementation is completely open source and hosted on the DigitalOcean Droplet Cloud. Hence, the proposed system enables further research and applications in both academia and industry. The proposed system has been tested in real fields under different and extreme environmental conditions in Salt Lake City, Utah and the University of Nebraska-Lincoln. The experimental results validate the features of SPARC-LoRa including low power, reliability, and cloud services provided by SPARC-LoRa.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "6 pages, 8 figures, submitted for publication"
    },
    {
        "paper id": "2401.13570",
        "abstract url": "https://arxiv.org/abs/2401.13570",
        "title": "Guided Diffusion for Fast Inverse Design of Density-based Mechanical Metamaterials",
        "rating": -2,
        "keywords": [
            [
                "voxel"
            ],
            [
                "Diffusion"
            ]
        ],
        "abstract": "Mechanical metamaterial is a synthetic material that can possess extraordinary physical characteristics, such as abnormal elasticity, stiffness, and stability, by carefully designing its internal structure. To make metamaterials contain delicate local structures with unique mechanical properties, it is a potential method to represent them through high-resolution voxels. However, it brings a substantial computational burden. To this end, this paper proposes a fast inverse design method, whose core is an advanced deep generative AI algorithm, to generate voxel-based mechanical metamaterials. Specifically, we use the self-conditioned diffusion model, capable of generating a microstructure with a resolution of $128^3$ to approach the specified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid reverse design tool facilitates the exploration of extreme metamaterials, the sequence interpolation in metamaterials, and the generation of diverse microstructures for multi-scale design. This flexible and adaptive generative tool is of great value in structural engineering or other mechanical systems and can stimulate more subsequent research.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "13 pages, 6 figures"
    },
    {
        "paper id": "2401.13588",
        "abstract url": "https://arxiv.org/abs/2401.13588",
        "title": "Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "medical",
                "Health",
                "healthcare",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance. However, their performance in actual clinical applications has been underexplored. Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts. This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings. Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication. Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes. Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians. Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an in-depth analysis. Results: GPT-4 showed overall superior performance compared to other LLMs. In contrast, both GPT-3.5 and text-davinci-003 exhibit enhanced performance when the appropriate prompting strategies are employed. The GPT family models have demonstrated considerable efficiency, evidenced by their cost-effectiveness and time-saving capabilities. Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized. This framework goes beyond singular performance aspects. With expert annotations, this methodology not only validates LLMs' capabilities in processing complex medical data but also establishes a benchmark for future LLM evaluations across specialized domains.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13612",
        "abstract url": "https://arxiv.org/abs/2401.13612",
        "title": "Intermittent Connectivity Maintenance With Heterogeneous Robots",
        "rating": -2,
        "keywords": [
            [
                "robot"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "We consider a scenario of cooperative task servicing, with a team of heterogeneous robots with different maximum speeds and communication radii, in charge of keeping the network intermittently connected. We abstract the task locations into a $1D$ cycle graph that is traversed by the communicating robots, and we discuss intermittent communication strategies so that each task location is periodically visited, with a worst--case revisiting time. Robots move forward and backward along the cycle graph, exchanging data with their previous and next neighbors when they meet, and updating their region boundaries. Asymptotically, each robot is in charge of a region of the cycle graph, depending on its capabilities. The method is distributed, and robots only exchange data when they meet.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13614",
        "abstract url": "https://arxiv.org/abs/2401.13614",
        "title": "Equitable Persistent Coverage of Non-Convex Environments with Graph-Based Planning",
        "rating": -2,
        "keywords": [
            [
                "robot"
            ],
            [
                "Graph"
            ]
        ],
        "abstract": "In this paper we tackle the problem of persistently covering a complex non-convex environment with a team of robots. We consider scenarios where the coverage quality of the environment deteriorates with time, requiring to constantly revisit every point. As a first step, our solution finds a partition of the environment where the amount of work for each robot, weighted by the importance of each point, is equal. This is achieved using a power diagram and finding an equitable partition through a provably correct distributed control law on the power weights. Compared to other existing partitioning methods, our solution considers a continuous environment formulation with non-convex obstacles. In the second step, each robot computes a graph that gathers sweep-like paths and covers its entire partition. At each planning time, the coverage error at the graph vertices is assigned as weights of the corresponding edges. Then, our solution is capable of efficiently finding the optimal open coverage path through the graph with respect to the coverage error per distance traversed. Simulation and experimental results are presented to support our proposal.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This is the accepted version an already published manuscript. See journal reference for details"
    },
    {
        "paper id": "2401.13639",
        "abstract url": "https://arxiv.org/abs/2401.13639",
        "title": "Winding Clearness for Differentiable Point Cloud Optimization",
        "rating": -2,
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "diffusion"
            ]
        ],
        "abstract": "We propose to explore the properties of raw point clouds through the \\emph{winding clearness}, a concept we first introduce for assessing the clarity of the interior/exterior relationships represented by the winding number field of the point cloud. In geometric modeling, the winding number is a powerful tool for distinguishing the interior and exterior of a given surface $\\partial \u03a9$, and it has been previously used for point normal orientation and surface reconstruction. In this work, we introduce a novel approach to assess and optimize the quality of point clouds based on the winding clearness. We observe that point clouds with reduced noise tend to exhibit improved winding clearness. Accordingly, we propose an objective function that quantifies the error in winding clearness, solely utilizing the positions of the point clouds. Moreover, we demonstrate that the winding clearness error is differentiable and can serve as a loss function in optimization-based and learning-based point cloud processing. In the optimization-based method, the loss function is directly back-propagated to update the point positions, resulting in an overall improvement of the point cloud. In the learning-based method, we incorporate the winding clearness as a geometric constraint in the diffusion-based 3D generative model. Experimental results demonstrate the effectiveness of optimizing the winding clearness in enhancing the quality of the point clouds. Our method exhibits superior performance in handling noisy point clouds with thin structures, highlighting the benefits of the global perspective enabled by the winding number.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13722",
        "abstract url": "https://arxiv.org/abs/2401.13722",
        "title": "Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion Monitoring",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "This research project aims to tackle the growing mental health challenges in today's digital age. It employs a modified pre-trained BERT model to detect depressive text within social media and users' web browsing data, achieving an impressive 93% test accuracy. Simultaneously, the project aims to incorporate physiological signals from wearable devices, such as smartwatches and EEG sensors, to provide long-term tracking and prognosis of mood disorders and emotional states. This comprehensive approach holds promise for enhancing early detection of depression and advancing overall mental health outcomes.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13748",
        "abstract url": "https://arxiv.org/abs/2401.13748",
        "title": "Log-Log Domain Sum-Product Algorithm for Information Reconciliation in Continuous-Variable Quantum Key Distribution",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "In this paper, we present a novel log-log domain sum-product algorithm (SPA) for decoding low-density parity-check (LDPC) codes in continuous-variable quantum key distribution (CV-QKD) systems. This algorithm reduces the fractional bit width of decoder messages, leading to a smaller memory footprint and a lower resource consumption in hardware implementation. We also provide practical insights for fixed-point arithmetic and compare our algorithm with the conventional SPA in terms of performance and complexity. Our results show that our algorithm achieves comparable or better decoding accuracy than the conventional SPA while saving at least $25\\%$ of the fractional bit width.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Accepted and to be presented at the 58th Conference on Information Sciences and Systems"
    },
    {
        "paper id": "2401.13762",
        "abstract url": "https://arxiv.org/abs/2401.13762",
        "title": "Fast System Level Synthesis: Robust Model Predictive Control using Riccati Recursions",
        "rating": -2,
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "trajectory"
            ]
        ],
        "abstract": "System Level Synthesis (SLS) enables improved robust MPC formulations by allowing for joint optimization of the nominal trajectory and controller. This paper introduces a tailored algorithm for solving the corresponding disturbance feedback optimization problem. The proposed algorithm builds on a recently proposed joint optimization scheme and iterates between optimizing the controller and the nominal trajectory while converging q-linearly to an optimal solution. We show that the controller optimization can be solved through Riccati recursions leading to a horizon-length, state, and input scalability of $\\mathcal{O}(N^2 ( n_x^3 + n_u ^3 ) )$ for each iterate. On a numerical example, the proposed algorithm exhibits computational speedups of order $10$ to $10^3$ compared to general-purpose commercial solvers.",
        "subjects": [
            "math.OC"
        ],
        "comment": "Submitted to IFAC Conference on Nonlinear Model Predictive Control (NMPC) 2024"
    },
    {
        "paper id": "2401.13792",
        "abstract url": "https://arxiv.org/abs/2401.13792",
        "title": "Probabilistic Mobility Load Balancing for Multi-band 5G and Beyond Networks",
        "rating": -2,
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "The ever-increasing demand for data services and the proliferation of user equipment (UE) have resulted in a significant rise in the volume of mobile traffic. Moreover, in multi-band networks, non-uniform traffic distribution among different operational bands can lead to congestion, which can adversely impact the user's quality of experience. Load balancing is a critical aspect of network optimization, where it ensures that the traffic is evenly distributed among different bands, avoiding congestion and ensuring better user experience. Traditional load balancing approaches rely only on the band channel quality as a load indicator and to move UEs between bands, which disregards the UE's demands and the band resource, and hence, leading to a suboptimal balancing and utilization of resources. To address this challenge, we propose an event-based algorithm, in which we model the load balancing problem as a multi-objective stochastic optimization, and assign UEs to bands in a probabilistic manner. The goal is to evenly distribute traffic across available bands according to their resources, while maintaining minimal number of inter-frequency handovers to avoid the signaling overhead and the interruption time. Simulation results show that the proposed algorithm enhances the network's performance and outperforms traditional load balancing approaches in terms of throughput and interruption time.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13800",
        "abstract url": "https://arxiv.org/abs/2401.13800",
        "title": "Multi-Object Navigation in real environments using hybrid policies",
        "rating": -2,
        "keywords": [
            [
                "SLAM"
            ],
            [
                "robotics",
                "Navigation"
            ]
        ],
        "abstract": "Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments. In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13803",
        "abstract url": "https://arxiv.org/abs/2401.13803",
        "title": "Synergizing Human Expertise and AI Efficiency with Language Model for Microscopy Operation and Automated Experiment Design",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "synthesis"
            ]
        ],
        "abstract": "With the advent of large language models (LLMs), in both the open source and proprietary domains, attention is turning to how to exploit such artificial intelligence (AI) systems in assisting complex scientific tasks, such as material synthesis, characterization, analysis and discovery. Here, we explore the utility of LLM, particularly ChatGPT4, in combination with application program interfaces (APIs) in tasks of experimental design, programming workflows, and data analysis in scanning probe microscopy, using both in-house developed API and API given by a commercial vendor for instrument control. We find that the LLM can be especially useful in converting ideations of experimental workflows to executable code on microscope APIs. Beyond code generation, we find that the GPT4 is capable of analyzing microscopy images in a generic sense. At the same time, we find that GPT4 suffers from inability to extend beyond basic analyses or more in-depth technical experimental design. We argue that a LLM specifically fine-tuned for individual scientific domains can potentially be a better language interface for converting scientific ideations from human experts to executable workflows, such a synergy between human expertise and LLM efficiency in experimentation can open new door for accelerating scientific research, enabling effective experimental protocols archive and sharing in scientific community.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "16 pages; 7 figures"
    },
    {
        "paper id": "2401.13832",
        "abstract url": "https://arxiv.org/abs/2401.13832",
        "title": "Algorithmically Curated Lies: How Search Engines Handle Misinformation about US Biolabs in Ukraine",
        "rating": -2,
        "keywords": [
            [
                "Biolabs"
            ]
        ],
        "abstract": "The growing volume of online content prompts the need for adopting algorithmic systems of information curation. These systems range from web search engines to recommender systems and are integral for helping users stay informed about important societal developments. However, unlike journalistic editing the algorithmic information curation systems (AICSs) are known to be subject to different forms of malperformance which make them vulnerable to possible manipulation. The risk of manipulation is particularly prominent in the case when AICSs have to deal with information about false claims that underpin propaganda campaigns of authoritarian regimes. Using as a case study of the Russian disinformation campaign concerning the US biolabs in Ukraine, we investigate how one of the most commonly used forms of AICSs - i.e. web search engines - curate misinformation-related content. For this aim, we conduct virtual agent-based algorithm audits of Google, Bing, and Yandex search outputs in June 2022. Our findings highlight the troubling performance of search engines. Even though some search engines, like Google, were less likely to return misinformation results, across all languages and locations, the three search engines still mentioned or promoted a considerable share of false content (33% on Google; 44% on Bing, and 70% on Yandex). We also find significant disparities in misinformation exposure based on the language of search, with all search engines presenting a higher number of false stories in Russian. Location matters as well with users from Germany being more likely to be exposed to search results promoting false information. These observations stress the possibility of AICSs being vulnerable to manipulation, in particular in the case of the unfolding propaganda campaigns, and underline the importance of monitoring performance of these systems to prevent it.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "19 pages, 5 figures"
    },
    {
        "paper id": "2401.13836",
        "abstract url": "https://arxiv.org/abs/2401.13836",
        "title": "Machine learning for industrial sensing and control: A survey and practical perspective",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "With the rise of deep learning, there has been renewed interest within the process industries to utilize data on large-scale nonlinear sensing and control problems. We identify key statistical and machine learning techniques that have seen practical success in the process industries. To do so, we start with hybrid modeling to provide a methodological framework underlying core application areas: soft sensing, process optimization, and control. Soft sensing contains a wealth of industrial applications of statistical and machine learning methods. We quantitatively identify research trends, allowing insight into the most successful techniques in practice. We consider two distinct flavors for data-driven optimization and control: hybrid modeling in conjunction with mathematical programming techniques and reinforcement learning. Throughout these application areas, we discuss their respective industrial requirements and challenges. A common challenge is the interpretability and efficiency of purely data-driven methods. This suggests a need to carefully balance deep learning techniques with domain knowledge. As a result, we highlight ways prior knowledge may be integrated into industrial machine learning applications. The treatment of methods, problems, and applications presented here is poised to inform and inspire practitioners and researchers to develop impactful data-driven sensing, optimization, and control solutions in the process industries.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "48 pages"
    },
    {
        "paper id": "2401.13891",
        "abstract url": "https://arxiv.org/abs/2401.13891",
        "title": "Text to speech synthesis",
        "rating": -2,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "navigation"
            ]
        ],
        "abstract": "Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13928",
        "abstract url": "https://arxiv.org/abs/2401.13928",
        "title": "Image based Crop Monitoring Technologies in Protected Horticulture: A Review",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Future food security is a major concern of the 21st century with the growing global population and climate changes. In addressing these challenges, protected cropping ensures food production year-round and increases crop production per land area by controlling environment conditions. Maintaining the growth and health of crops in these facilities is essential to ensure optimum food production. However, this is a laborious work and is currently done manually. Image-based non-destructive plant phenotyping is an emerging research area that reduces the skilled labour cost while enhancing the monitoring of crop growth, health, and identifying phenotype-genotype relations for plant breeding. With the proliferations of protected infrastructures and targeted plants, different technologies and sensor setups are needed for image-based crop monitoring. Conveyor-type plant-to-sensor systems, bench-top or gantry-based systems are commonly found in research facilities focussing on phenotyping of small, relatively short, or movable model plants. This review examines the literature on crop monitoring and phenotyping platforms in both field and protected facilities and explains different camera technologies and their ability to extract different plant traits. The review highlights the future research directions of image-based monitoring of commercial scale protected crops where crops can be relatively tall or vertically supported under semi controlled environments, which presents new challenges and is rarely covered in the literature.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13934",
        "abstract url": "https://arxiv.org/abs/2401.13934",
        "title": "MambaMorph: a Mamba-based Framework for Medical MR-CT Deformable Registration",
        "rating": -2,
        "keywords": [
            [
                "voxel"
            ],
            [
                "Medical",
                "CT",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Capturing voxel-wise spatial correspondence across distinct modalities is crucial for medical image analysis. However, current registration approaches are not practical enough in terms of registration accuracy and clinical applicability. In this paper, we introduce MambaMorph, a novel multi-modality deformable registration framework. Specifically, MambaMorph utilizes a Mamba-based registration module and a fine-grained, yet simple, feature extractor for efficient long-range correspondence modeling and high-dimensional feature learning, respectively. Additionally, we develop a well-annotated brain MR-CT registration dataset, SR-Reg, to address the scarcity of data in multi-modality registration. To validate MambaMorph's multi-modality registration capabilities, we conduct quantitative experiments on both our SR-Reg dataset and a public T1-T2 dataset. The experimental results on both datasets demonstrate that MambaMorph significantly outperforms the current state-of-the-art learning-based registration methods in terms of registration accuracy. Further study underscores the efficiency of the Mamba-based registration module and the lightweight feature extractor, which achieve notable registration quality while maintaining reasonable computational costs and speeds. We believe that MambaMorph holds significant potential for practical applications in medical image registration. The code for MambaMorph is available at: https://github.com/Guo-Stone/MambaMorph.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01700",
        "abstract url": "https://arxiv.org/abs/2402.01700",
        "title": "Question answering systems for health professionals at the point of care -- a systematic review",
        "rating": -2,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "biomedical",
                "medical",
                "health",
                "healthcare",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Objective: Question answering (QA) systems have the potential to improve the quality of clinical care by providing health professionals with the latest and most relevant evidence. However, QA systems have not been widely adopted. This systematic review aims to characterize current medical QA systems, assess their suitability for healthcare, and identify areas of improvement. Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library, ACL Anthology and forward and backward citations on 7th February 2023. We included peer-reviewed journal and conference papers describing the design and evaluation of biomedical QA systems. Two reviewers screened titles, abstracts, and full-text articles. We conducted a narrative synthesis and risk of bias assessment for each study. We assessed the utility of biomedical QA systems. Results: We included 79 studies and identified themes, including question realism, answer reliability, answer utility, clinical specialism, systems, usability, and evaluation methods. Clinicians' questions used to train and evaluate QA systems were restricted to certain sources, types and complexity levels. No system communicated confidence levels in the answers or sources. Many studies suffered from high risks of bias and applicability concerns. Only 8 studies completely satisfied any criterion for clinical utility, and only 7 reported user evaluations. Most systems were built with limited input from clinicians. Discussion: While machine learning methods have led to increased accuracy, most studies imperfectly reflected real-world healthcare information needs. Key research priorities include developing more realistic healthcare QA datasets and considering the reliability of answer sources, rather than merely focusing on accuracy.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to the Journal of the American Medical Informatics Association (JAMIA)"
    },
    {
        "paper id": "2402.03347",
        "abstract url": "https://arxiv.org/abs/2402.03347",
        "title": "Transfer Learning With Densenet201 Architecture Model For Potato Leaf Disease Classification",
        "rating": -2,
        "keywords": [
            [
                "attacks"
            ],
            [
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Potato plants are plants that are beneficial to humans. Like other plants in general, potato plants also have diseases; if this disease is not treated immediately, there will be a significant decrease in food production. Therefore, it is necessary to detect diseases quickly and precisely so that disease control can be carried out effectively and efficiently. Classification of potato leaf disease can be done directly. Still, the symptoms cannot always explain the type of disease that attacks potato leaves because there are many types of diseases with symptoms that look the same. Humans also have deficiencies in determining the results of identification of potato leaf disease, so sometimes the results of identification between individuals can be different. Therefore, the use of Deep Learning for the classification process of potato leaf disease is expected to shorten the time and have a high classification accuracy. This study uses a deep learning method with the DenseNet201 architecture. The choice to use the DenseNet201 algorithm in this study is because the model can identify important features of potato leaves and recognize early signs of emerging diseases. This study aimed to evaluate the effectiveness of the transfer learning method with the DenseNet201 architecture in increasing the classification accuracy of potato leaf disease compared to traditional classification methods. This study uses two types of scenarios, namely, comparing the number of dropouts and comparing the three optimizers. This test produces the best model using dropout 0.1 and Adam optimizer with an accuracy of 99.5% for training, 95.2% for validation, and 96% for the confusion matrix. In this study, using data testing, as many as 40 images were tested into the model that has been built. The test results on this model resulted in a new accuracy for classifying potato leaf disease, namely 92.5%.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09427",
        "abstract url": "https://arxiv.org/abs/2402.09427",
        "title": "DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications",
        "rating": -2,
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Many Internet of Things applications utilize low-cost, micro, electro-mechanical inertial sensors. A common task is orientation estimation. To tackle such a task, attitude and heading reference system algorithms are applied. Relying on the gyroscope readings, the accelerometer readings are used to update the attitude angles, and magnetometer measurements are utilized to update the heading angle. In indoor environments, magnetometers suffer from interference that degrades their performance. This mainly influences applications focused on estimating the heading angle like finding the heading angle of a closet or fridge door. To circumvent such situations, we propose DoorINet, an end-to-end deep-learning framework to calculate the heading angle from door-mounted, low-cost inertial sensors without using magnetometers. To evaluate our approach, we record a unique dataset containing 391 minutes of accelerometer and gyroscope measurements and corresponding ground-truth heading angle. We show that our proposed approach outperforms commonly used, model based approaches and data-driven methods.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "10 pages, 14 figures, 4 tables"
    },
    {
        "paper id": "2402.09430",
        "abstract url": "https://arxiv.org/abs/2402.09430",
        "title": "WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing",
        "rating": -2,
        "keywords": [
            [
                "healthcare"
            ]
        ],
        "abstract": "WiFi-based human sensing has exhibited remarkable potential to analyze user behaviors in a non-intrusive and device-free manner, benefiting applications as diverse as smart homes and healthcare. However, most previous works focus on single-user sensing, which has limited practicability in scenarios involving multiple users. Although recent studies have begun to investigate WiFi-based multi-user sensing, there remains a lack of benchmark datasets to facilitate reproducible and comparable research. To bridge this gap, we present WiMANS, to our knowledge, the first dataset for multi-user sensing based on WiFi. WiMANS contains over 9.4 hours of dual-band WiFi Channel State Information (CSI), as well as synchronized videos, monitoring simultaneous activities of multiple users. We exploit WiMANS to benchmark the performance of state-of-the-art WiFi-based human sensing models and video-based models, posing new challenges and opportunities for future work. We believe WiMANS can push the boundaries of current studies and catalyze the research on WiFi-based multi-user sensing.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "We present WiMANS, to our knowledge, the first dataset for multi-user activity sensing based on WiFi"
    },
    {
        "paper id": "2401.13282",
        "abstract url": "https://arxiv.org/abs/2401.13282",
        "title": "RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing",
        "rating": -2.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Forecasting complex system dynamics, particularly for long-term predictions, is persistently hindered by error accumulation and computational burdens. This study presents RefreshNet, a multiscale framework developed to overcome these challenges, delivering an unprecedented balance between computational efficiency and predictive accuracy. RefreshNet incorporates convolutional autoencoders to identify a reduced order latent space capturing essential features of the dynamics, and strategically employs multiple recurrent neural network (RNN) blocks operating at varying temporal resolutions within the latent space, thus allowing the capture of latent dynamics at multiple temporal scales. The unique \"refreshing\" mechanism in RefreshNet allows coarser blocks to reset inputs of finer blocks, effectively controlling and alleviating error accumulation. This design demonstrates superiority over existing techniques regarding computational efficiency and predictive accuracy, especially in long-term forecasting. The framework is validated using three benchmark applications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and Kuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms state-of-the-art methods in long-term forecasting accuracy and speed, marking a significant advancement in modeling complex systems and opening new avenues in understanding and predicting their behavior.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13301",
        "abstract url": "https://arxiv.org/abs/2401.13301",
        "title": "Classification of Radiologically Isolated Syndrome and Clinically Isolated Syndrome with Machine-Learning Techniques",
        "rating": -2.5,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "biomarkers",
                "diagnosis",
                "MRI",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Background and purpose: The unanticipated detection by magnetic resonance imaging (MRI) in the brain of asymptomatic subjects of white matter lesions suggestive of multiple sclerosis (MS) has been named radiologically isolated syndrome (RIS). As the difference between early MS [i.e. clinically isolated syndrome (CIS)] and RIS is the occurrence of a clinical event, it is logical to improve detection of the subclinical form without interfering with MRI as there are radiological diagnostic criteria for that. Our objective was to use machine-learning classification methods to identify morphometric measures that help to discriminate patients with RIS from those with CIS. Methods: We used a multimodal 3-T MRI approach by combining MRI biomarkers (cortical thickness, cortical and subcortical grey matter volume, and white matter integrity) of a cohort of 17 patients with RIS and 17 patients with CIS for single-subject level classification. Results: The best proposed models to predict the diagnosis of CIS and RIS were based on the Naive Bayes, Bagging and Multilayer Perceptron classifiers using only three features: the left rostral middle frontal gyrus volume and the fractional anisotropy values in the right amygdala and right lingual gyrus. The Naive Bayes obtained the highest accuracy [overall classification, 0.765; area under the receiver operating characteristic (AUROC), 0.782]. Conclusions: A machine-learning approach applied to multimodal MRI data may differentiate between the earliest clinical expressions of MS (CIS and RIS) with an accuracy of 78%. Keywords: Bagging; Multilayer Perceptron; Naive Bayes classifier; clinically isolated syndrome; diffusion tensor imaging; machine-learning; magnetic resonance imaging; multiple sclerosis; radiologically isolated syndrome.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "24 pages, 2 tables"
    },
    {
        "paper id": "2401.13327",
        "abstract url": "https://arxiv.org/abs/2401.13327",
        "title": "Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection",
        "rating": -2.5,
        "keywords": [
            [
                "GAN"
            ],
            [
                "medical",
                "Health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Smartwatch health sensor data is increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprises sensitive personal information and is resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress. Our method involves the generation of synthetic sequence data through Generative Adversarial Networks (GANs), coupled with the implementation of Differential Privacy (DP) safeguards for protecting patient information during model training. To ensure the integrity of our synthetic data, we employ a range of quality assessments and monitor the plausibility between synthetic and original data. To test the usefulness, we create private machine learning models on a commonly used, albeit small, stress detection dataset, exploring strategies for enhancing the existing data foundation with our synthetic data. Through our GAN-based augmentation methods, we observe improvements in model performance, both in non-private (0.45% F1) and private (11.90-15.48% F1) training scenarios. We underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with limited availability of real training samples.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13531",
        "abstract url": "https://arxiv.org/abs/2401.13531",
        "title": "QAGait: Revisit Gait Recognition from a Quality Perspective",
        "rating": -2.5,
        "keywords": [
            [
                "biometric"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns. Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research. However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion). To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait. Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures. We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space. Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement. Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority. Code is available at https://github.com/wzb-bupt/QAGait.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI 2024"
    },
    {
        "paper id": "2401.13912",
        "abstract url": "https://arxiv.org/abs/2401.13912",
        "title": "A Survey of Deep Learning and Foundation Models for Time Series Forecasting",
        "rating": -2.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13923",
        "abstract url": "https://arxiv.org/abs/2401.13923",
        "title": "Towards 3D Molecule-Text Interpretation in Language Models",
        "rating": -2.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "biomolecular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including molecule-text retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties. We release our codes and datasets at https://github.com/lsh0520/3D-MoLM.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13244",
        "abstract url": "https://arxiv.org/abs/2401.13244",
        "title": "Automating Unrealizability Logic: Hoare-style Proof Synthesis for Infinite Sets of Programs",
        "rating": -3,
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "grammar"
            ]
        ],
        "abstract": "Unrealizability logic (UL) was proposed by Kim et al. as the first Hoare-style proof system for proving properties that hold for an infinite set of programs (defined by a regular tree grammar). The goal of our work is to automate reasoning and proof generation for UL. A key ingredient in UL is the notion of nonterminal summaries-inductive facts that characterize recursive nonterminals in the grammar that defines the set of programs. They are analogous to procedure summaries in Hoare logic. The goal of automating UL led us to reformulate the inference rules-in particular, introducing a unified rule for nonterminal summaries, called the rule of adaptation, which draws inspiration from how procedure summaries are handled in Hoare logic. In the same way that verification conditions can be used to synthesize loop invariants for Hoare logic proofs, our reformulation of UL reduces the problem of synthesizing a nonterminal summary to a Syntax-Guided Synthesis problem. We implement Wuldo, the first checker and synthesizer for UL. Wuldo can express proofs beyond the reach of existing tools, including proofs that establish how infinitely many programs behave on infinitely many inputs, and in some cases Wuldo can even synthesize the needed nonterminal summaries.",
        "subjects": [
            "cs.PL"
        ],
        "comment": "30 pages, 5 figures, 2 tables"
    },
    {
        "paper id": "2401.13306",
        "abstract url": "https://arxiv.org/abs/2401.13306",
        "title": "POSTER: Towards Secure 5G Infrastructures for Production Systems",
        "rating": -3,
        "keywords": [
            [
                "attacks"
            ],
            [
                "5G",
                "industrial"
            ]
        ],
        "abstract": "To meet the requirements of modern production, industrial communication increasingly shifts from wired fieldbus to wireless 5G communication. Besides tremendous benefits, this shift introduces severe novel risks, ranging from limited reliability over new security vulnerabilities to a lack of accountability. To address these risks, we present approaches to (i) prevent attacks through authentication and redundant communication, (ii) detect anomalies and jamming, and (iii) respond to detected attacks through device exclusion and accountability measures.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted to the poster session of the 22nd International Conference on Applied Cryptography and Network Security (ACNS 2024)"
    },
    {
        "paper id": "2401.13421",
        "abstract url": "https://arxiv.org/abs/2401.13421",
        "title": "Federated learning with distributed fixed design quantum chips and quantum channels",
        "rating": -3,
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "The privacy in classical federated learning can be breached through the use of local gradient results along with engineered queries to the clients. However, quantum communication channels are considered more secure because a measurement on the channel causes a loss of information, which can be detected by the sender. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\\log N$ entangled qubits, which can potentially provide exponential efficiency if the data vector is utilized as quantum states. In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model parameters, but instead sends the operator as a quantum state, the clients are not required to share the model. This allows for the creation of asynchronous learning models. In addition, the model as a quantum state is fed into client-side chips directly; therefore, it does not require measurements on the upcoming quantum state to obtain model parameters in order to compute gradients. This can provide efficiency over the models where the parameter vector is sent via classical or quantum channels and local gradients are obtained through the obtained values of these parameters.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "a few typos are corrected"
    },
    {
        "paper id": "2401.13441",
        "abstract url": "https://arxiv.org/abs/2401.13441",
        "title": "Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance Control",
        "rating": -3,
        "keywords": [
            [
                "robot"
            ],
            [
                "clinical"
            ]
        ],
        "abstract": "Integrating Brain-Machine Interfaces into non-clinical applications like robot motion control remains difficult - despite remarkable advancements in clinical settings. Specifically, EEG-based motor imagery systems are still error-prone, posing safety risks when rigid robots operate near humans. This work presents an alternative pathway towards safe and effective operation by combining wearable EEG with physically embodied safety in soft robots. We introduce and test a pipeline that allows a user to move a soft robot's end effector in real time via brain waves that are measured by as few as three EEG channels. A robust motor imagery algorithm interprets the user's intentions to move the position of a virtual attractor to which the end effector is attracted, thanks to a new Cartesian impedance controller. We specifically focus here on planar soft robot-based architected metamaterials, which require the development of a novel control architecture to deal with the peculiar nonlinearities - e.g., non-affinity in control. We preliminarily but quantitatively evaluate the approach on the task of setpoint regulation. We observe that the user reaches the proximity of the setpoint in 66% of steps and that for successful steps, the average response time is 21.5s. We also demonstrate the execution of simple real-world tasks involving interaction with the environment, which would be extremely hard to perform if it were not for the robot's softness.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, presented at 7th IEEE-RAS International Conference on Soft Robotics (2024)"
    },
    {
        "paper id": "2401.13493",
        "abstract url": "https://arxiv.org/abs/2401.13493",
        "title": "Towards an Autonomous Compost Turner: Current State of Research",
        "rating": -3,
        "keywords": [
            [
                "navigation"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "This preprint presents the current status of research into the development and application of an autonomous, self-driving compost turner. The aim is to overcome challenges in the composting industry, such as adverse working conditions, by automating the composting process. The preprint provides a comprehensive overview of the overall concept of the self-driving compost turner, including the hardware architecture with sensors, navigation module and control module. In addition, the methodical development of the architecture of concepts, models and their subsequent software integration in ROS using model-based systems engineering is described. The validation and verification of the overall system is carried out in an industrial environment using three scenarios. The capabilities of the compost turner are demonstrated by autonomously following predefined trajectories in the composting plant and performing the required composting tasks. The results show that the autonomous compost turner is capable of performing the required activities. In addition, the compost turner has intelligent processing capabilities for compost data as well as its transmission, visualization and storage in a cloud server. It is important to note that this work is a preprint that represents the current state of research. The authors aim to publish the full paper in a peer-reviewed journal in the near future.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13518",
        "abstract url": "https://arxiv.org/abs/2401.13518",
        "title": "Addressing Data Quality Challenges in Observational Ambulatory Studies: Analysis, Methodologies and Practical Solutions for Wrist-worn Wearable Monitoring",
        "rating": -3,
        "keywords": [
            [
                "depth"
            ],
            [
                "health",
                "disease"
            ]
        ],
        "abstract": "Chronic disease management and follow-up are vital for realizing sustained patient well-being and optimal health outcomes. Recent advancements in wearable sensing technologies, particularly wrist-worn devices, offer promising solutions for longitudinal patient follow-up by shifting from subjective, intermittent self-reporting to objective, continuous monitoring. However, collecting and analyzing wearable data presents unique challenges, such as data entry errors, non-wear periods, missing wearable data, and wearable artifacts. We therefore present an in-depth exploration of data analysis challenges tied to wrist-worn wearables and ambulatory label acquisition, using two real-world datasets (i.e., mBrain21 and ETRI lifelog2020). We introduce novel practical countermeasures, including participant compliance visualizations, interaction-triggered questionnaires to assess personal bias, and an optimized wearable non-wear detection pipeline. Further, we propose a visual analytics approach to validate processing pipelines using scalable tools such as tsflex and Plotly-Resampler. Lastly, we investigate the impact of missing wearable data on \"window-of-interest\" analysis methodologies. Prioritizing transparency and reproducibility, we offer open access to our detailed code examples, facilitating adaptation in future wearable research. In conclusion, our contributions provide actionable approaches for wearable data collection and analysis in chronic disease management.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "29 pages, 16 figures"
    },
    {
        "paper id": "2401.13719",
        "abstract url": "https://arxiv.org/abs/2401.13719",
        "title": "Inference Attacks Against Face Recognition Model without Classification Layers",
        "rating": -3,
        "keywords": [
            [
                "GAN"
            ],
            [
                "Attacks"
            ],
            [
                "Face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Face recognition (FR) has been applied to nearly every aspect of daily life, but it is always accompanied by the underlying risk of leaking private information. At present, almost all attack models against FR rely heavily on the presence of a classification layer. However, in practice, the FR model can obtain complex features of the input via the model backbone, and then compare it with the target for inference, which does not explicitly involve the outputs of the classification layer adopting logit or other losses. In this work, we advocate a novel inference attack composed of two stages for practical FR models without a classification layer. The first stage is the membership inference attack. Specifically, We analyze the distances between the intermediate features and batch normalization (BN) parameters. The results indicate that this distance is a critical metric for membership inference. We thus design a simple but effective attack model that can determine whether a face image is from the training dataset or not. The second stage is the model inversion attack, where sensitive private data is reconstructed using a pre-trained generative adversarial network (GAN) guided by the attack model in the first stage. To the best of our knowledge, the proposed attack model is the very first in the literature developed for FR models without a classification layer. We illustrate the application of the proposed attack model in the establishment of privacy-preserving FR techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13801",
        "abstract url": "https://arxiv.org/abs/2401.13801",
        "title": "Exploring Adversarial Threat Models in Cyber Physical Battery Systems",
        "rating": -3,
        "keywords": [
            [
                "attacks"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "Technological advancements like the Internet of Things (IoT) have facilitated data exchange across various platforms. This data exchange across various platforms has transformed the traditional battery system into a cyber physical system. Such connectivity makes modern cyber physical battery systems vulnerable to cyber threats where a cyber attacker can manipulate sensing and actuation signals to bring the battery system into an unsafe operating condition. Hence, it is essential to build resilience in modern cyber physical battery systems (CPBS) under cyber attacks. The first step of building such resilience is to analyze potential adversarial behavior, that is, how the adversaries can inject attacks into the battery systems. However, it has been found that in this under-explored area of battery cyber physical security, such an adversarial threat model has not been studied in a systematic manner. In this study, we address this gap and explore adversarial attack generation policies based on optimal control framework. The framework is developed by performing theoretical analysis, which is subsequently supported by evaluation with experimental data generated from a commercial battery cell.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13807",
        "abstract url": "https://arxiv.org/abs/2401.13807",
        "title": "Depth-Optimal Addressing of 2D Qubit Array with 1D Controls Based on Exact Binary Matrix Factorization",
        "rating": -3,
        "keywords": [
            [
                "Depth"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "Reducing control complexity is essential for achieving large-scale quantum computing. However, reducing control knobs may compromise the ability to independently address each qubit. Recent progress in neutral atom-based platforms suggests that rectangular (row-column) addressing may strike a balance between control granularity and flexibility for 2D qubit arrays. This scheme allows addressing qubits on the intersections of a set of rows and columns each time. While quadratically reducing controls, it may necessitate more depth. We formulate the depth-optimal rectangular addressing problem as exact binary matrix factorization, an NP-hard problem also appearing in communication complexity and combinatorial optimization. We introduce a satisfiability modulo theories-based solver for this problem, and a heuristic, row packing, performing close to the optimal solver on various benchmarks. Furthermore, we discuss rectangular addressing in the context of fault-tolerant quantum computing, leveraging a natural two-level structure.",
        "subjects": [
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13877",
        "abstract url": "https://arxiv.org/abs/2401.13877",
        "title": "AscDAMs: Advanced SLAM-based channel detection and mapping system",
        "rating": -3,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "SLAM"
            ],
            [
                "satellite",
                "drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13941",
        "abstract url": "https://arxiv.org/abs/2401.13941",
        "title": "AC-Driven Series Elastic Electrohydraulic Actuator for Stable and Smooth Displacement Output",
        "rating": -3,
        "keywords": [
            [
                "robot"
            ],
            [
                "biopsy",
                "surgical",
                "MRI"
            ]
        ],
        "abstract": "Soft electrohydraulic actuators known as HASEL actuators have attracted widespread research interest due to their outstanding dynamic performance and high output power. However, the displacement of electrohydraulic actuators usually declines with time under constant DC voltage, which hampers its prospective application. A mathematical model is firstly established to not only explain the decrease in displacement under DC voltage but also predict the relatively stable displacement with oscillation under AC square wave voltage. The mathematical model is validated since the actual displacement confirms the trend observed by our model. To smooth the displacement oscillation introduced by AC voltage, a serial elastic component is incorporated to form a SE-HASEL actuator. A feedback control with a proportion-integration algorithm enables the SE-HASEL actuator to eliminate the obstinate displacement hysteresis. Our results revealed that, through our methodology, the SE-HASEL actuator can give stable and smooth displacement and is capable of absorbing external impact disturbance simultaneously. A rotary joint based on the SE-HASEL actuator is developed to reflect its possibility to generate a common rotary motion for wide robotic applications. More importantly, this paper also proposes a highly accurate needle biopsy robot that can be utilized in MRI-guide surgical procedures. Overall, we have achieved AC-driven series elastic electrohydraulic actuators that can exhibit stable and smooth displacement output.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13575",
        "abstract url": "https://arxiv.org/abs/2401.13575",
        "title": "CNN architecture extraction on edge GPU",
        "rating": -3.5,
        "keywords": [
            [
                "attack"
            ],
            [
                "forecasting"
            ],
            [
                "workshop"
            ]
        ],
        "abstract": "Neural networks have become popular due to their versatility and state-of-the-art results in many applications, such as image classification, natural language processing, speech recognition, forecasting, etc. These applications are also used in resource-constrained environments such as embedded devices. In this work, the susceptibility of neural network implementations to reverse engineering is explored on the NVIDIA Jetson Nano microcomputer via side-channel analysis. To this end, an architecture extraction attack is presented. In the attack, 15 popular convolutional neural network architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented on the GPU of Jetson Nano and the electromagnetic radiation of the GPU is analyzed during the inference operation of the neural networks. The results of the analysis show that neural network architectures are easily distinguishable using deep learning-based side-channel analysis.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Will appear at the AIHWS 2024 workshop at ACNS 2024"
    },
    {
        "paper id": "2401.13716",
        "abstract url": "https://arxiv.org/abs/2401.13716",
        "title": "Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare",
        "rating": -3.5,
        "keywords": [
            [
                "healthcare",
                "Cancer"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Registry. We present a conceptual framework for quality assurance of SD for AI applications in healthcare that aligns diverging taxonomies, expands on common quality dimensions to include the dimensions of Fairness and Carbon footprint, and proposes stages necessary to support real-life applications. Building trust in synthetic data by increasing transparency and reducing the safety risk will accelerate the development and uptake of trustworthy AI tools for the benefit of patients. Despite the growing emphasis on algorithmic fairness and carbon footprint, these metrics were scarce in the literature review. The overwhelming focus was on statistical similarity using distance metrics while sequential logic detection was scarce. A consensus-backed framework that includes all relevant quality dimensions can provide assurance for safe and responsible real-life applications of SD.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13904",
        "abstract url": "https://arxiv.org/abs/2401.13904",
        "title": "Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression",
        "rating": -3.5,
        "keywords": [
            [
                "face"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Thin-layer chromatography (TLC) is a crucial technique in molecular polarity analysis. Despite its importance, the interpretability of predictive models for TLC, especially those driven by artificial intelligence, remains a challenge. Current approaches, utilizing either high-dimensional molecular fingerprints or domain-knowledge-driven feature engineering, often face a dilemma between expressiveness and interpretability. To bridge this gap, we introduce Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical neural networks and symbolic regression. UHiSR automatically distills chemical-intuitive polarity indices, and discovers interpretable equations that link molecular structure to chromatographic behavior.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "33 pages, 6 figures"
    },
    {
        "paper id": "2401.13486",
        "abstract url": "https://arxiv.org/abs/2401.13486",
        "title": "Separable Physics-Informed Neural Networks for the solution of elasticity problems",
        "rating": -4,
        "keywords": [
            [
                "industrial"
            ],
            [
                "Physics"
            ]
        ],
        "abstract": "A method for solving elasticity problems based on separable physics-informed neural networks (SPINN) in conjunction with the deep energy method (DEM) is presented. Numerical experiments have been carried out for a number of problems showing that this method has a significantly higher convergence rate and accuracy than the vanilla physics-informed neural networks (PINN) and even SPINN based on a system of partial differential equations (PDEs). In addition, using the SPINN in the framework of DEM approach it is possible to solve problems of the linear theory of elasticity on complex geometries, which is unachievable with the help of PINNs in frames of partial differential equations. Considered problems are very close to the industrial problems in terms of geometry, loading, and material parameters.",
        "subjects": [
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13823",
        "abstract url": "https://arxiv.org/abs/2401.13823",
        "title": "Robustness in Fairness against Edge-level Perturbations in GNN-based Recommendation",
        "rating": -4,
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "attacks"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Efforts in the recommendation community are shifting from the sole emphasis on utility to considering beyond-utility factors, such as fairness and robustness. Robustness of recommendation models is typically linked to their ability to maintain the original utility when subjected to attacks. Limited research has explored the robustness of a recommendation model in terms of fairness, e.g., the parity in performance across groups, under attack scenarios. In this paper, we aim to assess the robustness of graph-based recommender systems concerning fairness, when exposed to attacks based on edge-level perturbations. To this end, we considered four different fairness operationalizations, including both consumer and provider perspectives. Experiments on three datasets shed light on the impact of perturbations on the targeted fairness notion, uncovering key shortcomings in existing evaluation protocols for robustness. As an example, we observed perturbations affect consumer fairness on a higher extent than provider fairness, with alarming unfairness for the former. Source code: https://github.com/jackmedda/CPFairRobust",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13853",
        "abstract url": "https://arxiv.org/abs/2401.13853",
        "title": "Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception",
        "rating": -4,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "navigation"
            ],
            [
                "thermal"
            ]
        ],
        "abstract": "Conventional cameras employed in autonomous vehicle (AV) systems support many perception tasks, but are challenged by low-light or high dynamic range scenes, adverse weather, and fast motion. Novel sensors, such as event and thermal cameras, offer capabilities with the potential to address these scenarios, but they remain to be fully exploited. This paper introduces the Novel Sensors for Autonomous Vehicle Perception (NSAVP) dataset to facilitate future research on this topic. The dataset was captured with a platform including stereo event, thermal, monochrome, and RGB cameras as well as a high precision navigation system providing ground truth poses. The data was collected by repeatedly driving two ~8 km routes and includes varied lighting conditions and opposing viewpoint perspectives. We provide benchmarking experiments on the task of place recognition to demonstrate challenges and opportunities for novel sensors to enhance critical AV perception tasks. To our knowledge, the NSAVP dataset is the first to include stereo thermal cameras together with stereo event and monochrome cameras. The dataset and supporting software suite is available at: https://umautobots.github.io/nsavp",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2401.13870",
        "abstract url": "https://arxiv.org/abs/2401.13870",
        "title": "Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation",
        "rating": -4,
        "keywords": [
            [
                "face"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Conventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance. In this paper, we introduce a general and model-agnostic framework known as \\textbf{L}arge \\textbf{la}nguage model with \\textbf{m}utual augmentation and \\textbf{a}daptive aggregation for \\textbf{Rec}ommendation (\\textbf{Llama4Rec}). Llama4Rec synergistically combines conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec, demonstrating its consistent outperformance of baseline methods and significant improvements in recommendation performance.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.08429",
        "abstract url": "https://arxiv.org/abs/2402.08429",
        "title": "Is 3-(F)WL Enough to Distinguish All 3D Graphs?",
        "rating": -4,
        "keywords": [
            [
                "3D"
            ],
            [
                "graph"
            ],
            [
                "chemical"
            ]
        ],
        "abstract": "The problem of graph isomorphism is an important but challenging problem in the field of graph analysis, for example: analyzing the similarity of two chemical molecules, or studying the expressive ability of graph neural networks. WL test is a method to judge whether two graphs are isomorphic, but it cannot distinguish all non-isomorphic graphs. As an improvement of WL, k-WL has stronger isomorphism discrimination ability, and as k increases, its discrimination ability is strictly increasing. However, whether the isomorphic discrimination power of k-WL is strictly increasing for more complex 3D graphs, or whether there exists k that can discriminate all 3D graphs, remains unexplored. This paper attempts to explore this problem from the perspective of graph generation.",
        "subjects": [
            "cs.OH"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13346",
        "abstract url": "https://arxiv.org/abs/2401.13346",
        "title": "Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed",
        "rating": -4.5,
        "keywords": [
            [
                "robot"
            ],
            [
                "Federated Learning"
            ],
            [
                "IoT"
            ],
            [
                "workshop"
            ]
        ],
        "abstract": "UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative robots, and edge-intelligence-enabled devices. This paper provides a guide to the implemented and prospective artificial intelligence (AI) capabilities of UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are presented in detail: 1) An automated streetlight monitoring for detecting issues and triggering maintenance alerts; 2) A Digital twin of building environments providing enhanced air quality sensing with reduced cost; 3) A large-scale Federated Learning framework for reducing communication overhead; and 4) An intrusion detection for containerised applications identifying malicious activities. Additionally, the potential of UMBRELLA is outlined for future smart city and multi-robot crowdsensing applications enhanced by semantic communications and multi-agent planning. Finally, to realise the above use-cases we discuss the need for a tailored MLOps platform to automate UMBRELLA model pipelines and establish trust.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "6 pgaes, 4 figures. This work has been accepted by PerCom TrustSense workshop 2024"
    },
    {
        "paper id": "2401.13827",
        "abstract url": "https://arxiv.org/abs/2401.13827",
        "title": "Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models",
        "rating": -4.5,
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "IoT"
            ],
            [
                "UAV"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The age of information (AoI) is used to measure the freshness of the data. In IoT networks, the traditional resource management schemes rely on a message exchange between the devices and the base station (BS) before communication which causes high AoI, high energy consumption, and low reliability. Unmanned aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the AoI, energy-saving, and throughput improvement. In this paper, we present a novel learning-based framework that estimates the traffic arrival of IoT devices based on Markovian events. The learning proceeds to optimize the trajectory of multiple UAVs and their scheduling policy. First, the BS predicts the future traffic of the devices. We compare two traffic predictors: the forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we propose a deep reinforcement learning (DRL) approach to optimize the optimal policy of each UAV. Finally, we manipulate the optimum reward function for the proposed DRL approach. Simulation results show that the proposed algorithm outperforms the random-walk (RW) baseline model regarding the AoI, scheduling accuracy, and transmission power.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13584",
        "abstract url": "https://arxiv.org/abs/2401.13584",
        "title": "Securing the Invisible Thread: A Comprehensive Analysis of BLE Tracker Security in Apple AirTags and Samsung SmartTags",
        "rating": -6,
        "keywords": [
            [
                "depth"
            ],
            [
                "attack"
            ],
            [
                "face"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "This study presents an in-depth analysis of the security landscape in Bluetooth Low Energy (BLE) tracking systems, with a particular emphasis on Apple AirTags and Samsung SmartTags, including their cryptographic frameworks. Our investigation traverses a wide spectrum of attack vectors such as physical tampering, firmware exploitation, signal spoofing, eavesdropping, jamming, app security flaws, Bluetooth security weaknesses, location spoofing, threats to owner devices, and cloud-related vulnerabilities. Moreover, we delve into the security implications of the cryptographic methods utilized in these systems. Our findings reveal that while BLE trackers like AirTags and SmartTags offer substantial utility, they also pose significant security risks. Notably, Apple's approach, which prioritizes user privacy by removing intermediaries, inadvertently leads to device authentication challenges, evidenced by successful AirTag spoofing instances. Conversely, Samsung SmartTags, designed to thwart beacon spoofing, raise critical concerns about cloud security and user privacy. Our analysis also highlights the constraints faced by these devices due to their design focus on battery life conservation, particularly the absence of secure boot processes, which leaves them susceptible to OS modification and a range of potential attacks. The paper concludes with insights into the anticipated evolution of these tracking systems. We predict that future enhancements will likely focus on bolstering security features, especially as these devices become increasingly integrated into the broader IoT ecosystem and face evolving privacy regulations. This shift is imperative to address the intricate balance between functionality and security in next-generation BLE tracking systems.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13791",
        "abstract url": "https://arxiv.org/abs/2401.13791",
        "title": "Synthetic Waveform Generation for Satellite, HAPS, and 5G Base Station Positioning Reference Signal Using QuaDRiGa",
        "rating": -6,
        "keywords": [
            [
                "3D"
            ],
            [
                "navigation"
            ],
            [
                "5G"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "Waveform generation is essential for studying signal propagation and channel characteristics, particularly for objects that are conceptualized but still need to be operational. We introduce a comprehensive guide on creating synthetic signals using channel and delay coefficients derived from the Quasi-Deterministic Radio Channel Generator (QuaDRiGa), which is recognized as a 3GPP-3D and 3GPP 38.901 reference implementation. The effectiveness of the proposed synthetic waveform generation method is validated through accurate estimation of code delay and Doppler shift. This validation is achieved using both the parallel code phase search technique and the conventional tracking method applied to satellites. As the method of integrating channel and delay coefficients to create synthetic waveforms is the same for satellite, HAPS, and gNB PRS, validating this method on synthetic satellite signals could potentially be extended to HAPS and gNB PRS as well. This study could significantly contribute to the field of heterogeneous navigation systems.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "6 pages, 31 figures, conference"
    },
    {
        "paper id": "2401.13232",
        "abstract url": "https://arxiv.org/abs/2401.13232",
        "title": "Distributed Source Coding Using Constrained-Random-Number Generators",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper investigates the general distributed lossless/lossy source coding formulated by Jana and Blahut. Their multi-letter rate-distortion region, an alternative to the region derived by Yang and Qin, is characterized by entropy functions for arbitrary general correlated sources. Achievability is shown by constructing a code based on constrained-random number generators.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "25 pages, this is the extended version of the paper submitted to ISIT2024, appendices are the revision of arXiv:2206.00792, (v2) add Refs. [10][31]. arXiv admin note: text overlap with arXiv:2206.00792"
    },
    {
        "paper id": "2401.13238",
        "abstract url": "https://arxiv.org/abs/2401.13238",
        "title": "Minimal spanning arborescence",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the minimal spanning arborescence which is the directed analogue of the minimal spanning tree, with a particular focus on its infinite volume limit and its geometric properties. We prove that in a certain large class of transient trees, the infinite volume limit exists almost surely. We also prove that for nonamenable, unimodular graphs, the limit is almost surely one-ended assuming a certain sufficient condition that guarantees the existence of the limit. This object cannot be studied using well-known algorithms, such as Kruskal's or Prim's algorithm, to sample the minimal spanning tree which has been instrumental in getting analogous results about them (Lyons, Peres, and Schramm). Instead, we use a recursive algorithm due to Chu, Liu, Edmonds, and Bock, which leads to a novel stochastic process which we call the \\emph{loop contracting random walk}. This is similar to the well-known and widely studied loop erased random walk, except instead of erasing loops we contract them. The full algorithm bears similarities with the celebrated Wilson's algorithm to generate uniform spanning trees and can be seen as a certain limit of the original Wilson's algorithm.",
        "subjects": [
            "math.PR"
        ],
        "comment": "47 pages, many figures, 2 simulations"
    },
    {
        "paper id": "2401.13262",
        "abstract url": "https://arxiv.org/abs/2401.13262",
        "title": "Designing Redistribution Mechanisms for Reducing Transaction Fees in Blockchains",
        "rating": -10,
        "keywords": [],
        "abstract": "Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user transactions to include in blocks and determine their payments (i.e., transaction fees). Increasing demand and scarce block resources have led to high user transaction fees. As these blockchains are a public resource, it may be preferable to reduce these transaction fees. To this end, we introduce Transaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG payments collected from such TFM as rebates to minimize transaction fees. Classic redistribution mechanisms (RMs) achieve this while ensuring Allocative Efficiency (AE) and User Incentive Compatibility (UIC). Our first result shows the non-triviality of applying RM in TFMs. More concretely, we prove that it is impossible to reduce transaction fees when (i) transactions that are not confirmed do not receive rebates and (ii) the miner can strategically manipulate the mechanism. Driven by this, we propose \\emph{Robust} TFRM (\\textsf{R-TFRM}): a mechanism that compromises on an honest miner's individual rationality to guarantee strictly positive rebates to the users. We then introduce \\emph{robust} and \\emph{rational} TFRM (\\textsf{R}$^2$\\textsf{-TFRM}) that uses trusted on-chain randomness that additionally guarantees miner's individual rationality (in expectation) and strictly positive rebates. Our results show that TFRMs provide a promising new direction for reducing transaction fees in public blockchains.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "Full Paper (AAMAS '24)"
    },
    {
        "paper id": "2401.13266",
        "abstract url": "https://arxiv.org/abs/2401.13266",
        "title": "SpecLLM: Exploring Generation and Review of VLSI Design Specification with Large Language Model",
        "rating": -10,
        "keywords": [],
        "abstract": "The development of architecture specifications is an initial and fundamental stage of the integrated circuit (IC) design process. Traditionally, architecture specifications are crafted by experienced chip architects, a process that is not only time-consuming but also error-prone. Mistakes in these specifications may significantly affect subsequent stages of chip design. Despite the presence of advanced electronic design automation (EDA) tools, effective solutions to these specification-related challenges remain scarce. Since writing architecture specifications is naturally a natural language processing (NLP) task, this paper pioneers the automation of architecture specification development with the advanced capabilities of large language models (LLMs). Leveraging our definition and dataset, we explore the application of LLMs in two key aspects of architecture specification development: (1) Generating architecture specifications, which includes both writing specifications from scratch and converting RTL code into detailed specifications. (2) Reviewing existing architecture specifications. We got promising results indicating that LLMs may revolutionize how these critical specification documents are developed in IC design nowadays. By reducing the effort required, LLMs open up new possibilities for efficiency and accuracy in this crucial aspect of chip design.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13320",
        "abstract url": "https://arxiv.org/abs/2401.13320",
        "title": "A Big Data Architecture for Early Identification and Categorization of Dark Web Sites",
        "rating": -10,
        "keywords": [],
        "abstract": "The dark web has become notorious for its association with illicit activities and there is a growing need for systems to automate the monitoring of this space. This paper proposes an end-to-end scalable architecture for the early identification of new Tor sites and the daily analysis of their content. The solution is built using an Open Source Big Data stack for data serving with Kubernetes, Kafka, Kubeflow, and MinIO, continuously discovering onion addresses in different sources (threat intelligence, code repositories, web-Tor gateways, and Tor repositories), downloading the HTML from Tor and deduplicating the content using MinHash LSH, and categorizing with the BERTopic modeling (SBERT embedding, UMAP dimensionality reduction, HDBSCAN document clustering and c-TF-IDF topic keywords). In 93 days, the system identified 80,049 onion services and characterized 90% of them, addressing the challenge of Tor volatility. A disproportionate amount of repeated content is found, with only 6.1% unique sites. From the HTML files of the dark sites, 31 different low-topics are extracted, manually labeled, and grouped into 11 high-level topics. The five most popular included sexual and violent content, repositories, search engines, carding, cryptocurrencies, and marketplaces. During the experiments, we identified 14 sites with 13,946 clones that shared a suspiciously similar mirroring rate per day, suggesting an extensive common phishing network. Among the related works, this study is the most representative characterization of onion services based on topics to date.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13328",
        "abstract url": "https://arxiv.org/abs/2401.13328",
        "title": "Rank-decreasing transductions",
        "rating": -10,
        "keywords": [],
        "abstract": "We propose to study transformations on graphs, and more generally structures, by looking at how the cut-rank (as introduced by Oum) of subsets is affected when going from the input structure to the output structure. We consider transformations in which the underlying sets are the same for both the input and output, and so the cut-ranks of subsets can be easily compared. The purpose of this paper is to give a characterisation of logically defined transductions that is expressed in purely structural terms, without referring to logic: transformations which decrease the cut-rank, in the asymptotic sense, are exactly those that can be defined in monadic second-order logic. This characterisation assumes that the transduction has inputs of bounded treewidth; we also show that the characterisation fails in the absence of any assumptions.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "22 pages, 13 figures"
    },
    {
        "paper id": "2401.13343",
        "abstract url": "https://arxiv.org/abs/2401.13343",
        "title": "Lessons on Datasets and Paradigms in Machine Learning for Symbolic Computation: A Case Study on CAD",
        "rating": -10,
        "keywords": [],
        "abstract": "Symbolic Computation algorithms and their implementation in computer algebra systems often contain choices which do not affect the correctness of the output but can significantly impact the resources required: such choices can benefit from having them made separately for each problem via a machine learning model. This study reports lessons on such use of machine learning in symbolic computation, in particular on the importance of analysing datasets prior to machine learning and on the different machine learning paradigms that may be utilised. We present results for a particular case study, the selection of variable ordering for cylindrical algebraic decomposition, but expect that the lessons learned are applicable to other decisions in symbolic computation. We utilise an existing dataset of examples derived from applications which was found to be imbalanced with respect to the variable ordering decision. We introduce an augmentation technique for polynomial systems problems that allows us to balance and further augment the dataset, improving the machine learning results by 28\\% and 38\\% on average, respectively. We then demonstrate how the existing machine learning methodology used for the problem $-$ classification $-$ might be recast into the regression paradigm. While this does not have a radical change on the performance, it does widen the scope in which the methodology can be applied to make choices.",
        "subjects": [
            "cs.SC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13345",
        "abstract url": "https://arxiv.org/abs/2401.13345",
        "title": "FPGA Implementation of an Intelligent Traffic Light Controller (I-TLC) in Verilog",
        "rating": -10,
        "keywords": [],
        "abstract": "The objective of this paper is to design and implement an intelligent Traffic Light Controller system for a four way road intersection. The design is carried out using Verilog, and the hardware is implemented on a FPGA. The chosen intersection involves a 'main road' (heavy traffic flow) and a 'side road' (less traffic flow), which is equipped with sensors to detect the presence of traffic or pedestrians. The functionality of the system has undergone thorough verification through simulations conducted in the Xilinx ISE Design Studio software environment. Furthermore, it has been physically deployed on a Xilinx Spartan-3E FPGA board xc3s500e-4-fg320. A traffic light controller can be realized through the use of a microcontroller, Application-Specific Integrated Circuits (ASICs), or Field-Programmable Gate Arrays (FPGAs). FPGAs however offer significant advantages in terms of re-programmability, speed, and parallel processing capabilities, making them ideally suited for implementing complex, adaptive logic required by smart traffic management systems; thus, making this model of TLC extremely adaptive and cost efficient at the same time as compared to other existing models with reduced hardware usage and delay constraints.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "The nature of the changes in the updated version involves incorporating synthesis work. Additionally, hardware implementation results on the FPGA board have been added. Moderate changes have been made, as they introduce new aspects related to synthesis and provide valuable insights into the hardware implementation, but they do not alter or affect the existing simulation results"
    },
    {
        "paper id": "2401.13351",
        "abstract url": "https://arxiv.org/abs/2401.13351",
        "title": "Predicting IR Personalization Performance using Pre-retrieval Query Predictors",
        "rating": -10,
        "keywords": [],
        "abstract": "Personalization generally improves the performance of queries but in a few cases it may also harms it. If we are able to predict and therefore to disable personalization for those situations, the overall performance will be higher and users will be more satisfied with personalized systems. We use some state-of-the-art pre-retrieval query performance predictors and propose some others including the user profile information for the previous purpose. We study the correlations among these predictors and the difference between the personalized and the original queries. We also use classification and regression techniques to improve the results and finally reach a bit more than one third of the maximum ideal performance. We think this is a good starting point within this research line, which certainly needs more effort and improvements.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13354",
        "abstract url": "https://arxiv.org/abs/2401.13354",
        "title": "Characterizing Network Requirements for GPU API Remoting in AI Applications",
        "rating": -10,
        "keywords": [],
        "abstract": "GPU remoting is a promising technique for supporting AI applications. Networking plays a key role in enabling remoting. However, for efficient remoting, the network requirements in terms of latency and bandwidth are unknown. In this paper, we take a GPU-centric approach to derive the minimum latency and bandwidth requirements for GPU remoting, while ensuring no (or little) performance degradation for AI applications. Our study including theoretical model demonstrates that, with careful remoting design, unmodified AI applications can run on the remoting setup using commodity networking hardware without any overhead or even with better performance, with low network demands.",
        "subjects": [
            "cs.OS"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13355",
        "abstract url": "https://arxiv.org/abs/2401.13355",
        "title": "Considering Capacitive Effects in Foil Winding Homogenization",
        "rating": -10,
        "keywords": [],
        "abstract": "In conventional finite element simulations, foil windings with a thin foil and many turns require many mesh elements. This renders models quickly computationally infeasible. With the use of homogenization approaches, the finite element mesh does not need to resolve the small-scale structure of the foil winding domain. Present homogenization approaches take resistive and inductive effects into account. With an increase of the operation frequency of foil windings, however, capacitive effects between adjacent turns in the foil winding become relevant. This paper presents an extension to the standard foil winding model that covers the capacitive behavior of foil windings.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "8 pages, 12 figures"
    },
    {
        "paper id": "2401.13359",
        "abstract url": "https://arxiv.org/abs/2401.13359",
        "title": "Reconfigurable routing in data center networks",
        "rating": -10,
        "keywords": [],
        "abstract": "The Reconfigurable Routing Problem (RRP) in hybrid networks is, in short, the problem of finding settings for optical switches augmenting a static network so as to achieve optimal delivery of some given workload. The problem has previously been studied in various scenarios with both tractable and NP-hardness results obtained. However, the data center and interconnection networks to which the problem is most relevant are almost always such that the static network is highly structured whereas all previous results assume that the static network can be arbitrary (which makes existing computational hardness results less technologically relevant and also easier to obtain). In this paper, and for the first time, we prove various intractability results for RRP where the underlying static network is highly structured, for example consisting of a hypercube, and also extend some existing tractability results.",
        "subjects": [
            "cs.CC"
        ],
        "comment": "30 pages, 6 figures"
    },
    {
        "paper id": "2401.13368",
        "abstract url": "https://arxiv.org/abs/2401.13368",
        "title": "Towards Optimal Pilot Spacing and Power Control in Multi-Antenna Systems Operating Over Non-Stationary Rician Aging Channels",
        "rating": -10,
        "keywords": [],
        "abstract": "Several previous works have addressed the inherent trade-off between allocating resources in the power and time domains to pilot and data signals in multiple input multiple output systems over block-fading channels. In particular, when the channel changes rapidly in time, channel aging degrades the performance in terms of spectral efficiency without proper pilot spacing and power control. Despite recognizing non-stationary stochastic processes as more accurate models for time-varying wireless channels, the problem of pilot spacing and power control in multi-antenna systems operating over non-stationary channels is not addressed in the literature. In this paper, we address this gap by introducing a refined first-order autoregressive model that exploits the inherent temporal correlations over non-stationary Rician aging channels. We design a multi-frame structure for data transmission that better reflects the non-stationary fading environment than previously developed single-frame structures. Subsequently, to determine optimal pilot spacing and power control within this multi-frame structure, we develop an optimization framework and an efficient algorithm based on maximizing a deterministic equivalent expression for the spectral efficiency, demonstrating its generality by encompassing previous channel aging results. Our numerical results indicate the efficacy of the proposed method in terms of spectral efficiency gains over the single frame structure.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13369",
        "abstract url": "https://arxiv.org/abs/2401.13369",
        "title": "Dynamic Epistemic Logic of Resource Bounded Information Mining Agents",
        "rating": -10,
        "keywords": [],
        "abstract": "Logics for resource-bounded agents have been getting more and more attention in recent years since they provide us with more realistic tools for modelling and reasoning about multi-agent systems. While many existing approaches are based on the idea of agents as imperfect reasoners, who must spend their resources to perform logical inference, this is not the only way to introduce resource constraints into logical settings. In this paper we study agents as perfect reasoners, who may purchase a new piece of information from a trustworthy source. For this purpose we propose dynamic epistemic logic for semi-public queries for resource-bounded agents. In this logic (groups of) agents can perform a query (ask a question) about whether some formula is true and receive a correct answer. These queries are called semi-public, because the very fact of the query is public, while the answer is private. We also assume that every query has a cost and every agent has a budget constraint. Finally, our framework allows us to reason about group queries, in which agents may share resources to obtain a new piece of information together. We demonstrate that our logic is complete, decidable and has an efficient model checking procedure.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13371",
        "abstract url": "https://arxiv.org/abs/2401.13371",
        "title": "SVARM-IQ: Efficient Approximation of Any-order Shapley Interactions through Stratification",
        "rating": -10,
        "keywords": [],
        "abstract": "Addressing the limitations of individual attribution scores via the Shapley value (SV), the field of explainable AI (XAI) has recently explored intricate interactions of features or data points. In particular, extensions of the SV, such as the Shapley Interaction Index (SII), have been proposed as a measure to still benefit from the axiomatic basis of the SV. However, similar to the SV, their exact computation remains computationally prohibitive. Hence, we propose with SVARM-IQ a sampling-based approach to efficiently approximate Shapley-based interaction indices of any order. SVARM-IQ can be applied to a broad class of interaction indices, including the SII, by leveraging a novel stratified representation. We provide non-asymptotic theoretical guarantees on its approximation quality and empirically demonstrate that SVARM-IQ achieves state-of-the-art estimation results in practical XAI scenarios on different model classes and application domains.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13382",
        "abstract url": "https://arxiv.org/abs/2401.13382",
        "title": "A proof theory of right-linear (omega-)grammars via cyclic proofs",
        "rating": -10,
        "keywords": [],
        "abstract": "Right-linear (or left-linear) grammars are a well-known class of context-free grammars computing just the regular languages. They may naturally be written as expressions with (least) fixed points but with products restricted to letters as left arguments, giving an alternative to the syntax of regular expressions. In this work, we investigate the resulting logical theory of this syntax. Namely, we propose a theory of right-linear algebras (RLA) over of this syntax and a cyclic proof system CRLA for reasoning about them. We show that CRLA is sound and complete for the intended model of regular languages. From here we recover the same completeness result for RLA by extracting inductive invariants from cyclic proofs, rendering the model of regular languages the free right-linear algebra. Finally, we extend system CRLA by greatest fixed points, nuCRLA, naturally modelled by languages of omega-words thanks to right-linearity. We show a similar soundness and completeness result of (the guarded fragment of) nuCRLA for the model of omega-regular languages, employing game theoretic techniques.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "34 pages, 3 figures"
    },
    {
        "paper id": "2401.13384",
        "abstract url": "https://arxiv.org/abs/2401.13384",
        "title": "Randomized learning-augmented auctions with revenue guarantees",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider the fundamental problem of designing a truthful single-item auction with the challenging objective of extracting a large fraction of the highest agent valuation as revenue. Following a recent trend in algorithm design, we assume that the agent valuations belong to a known interval, and a (possibly erroneous) prediction for the highest valuation is available. Then, auction design aims for high consistency and robustness, meaning that, for appropriate pairs of values $\u03b3$ and $\u03c1$, the extracted revenue should be at least a $\u03b3$- or $\u03c1$-fraction of the highest valuation when the prediction is correct for the input instance or not. We characterize all pairs of parameters $\u03b3$ and $\u03c1$ so that a randomized $\u03b3$-consistent and $\u03c1$-robust auction exists. Furthermore, for the setting in which robustness can be a function of the prediction error, we give sufficient and necessary conditions for the existence of robust auctions and present randomized auctions that extract a revenue that is only a polylogarithmic (in terms of the prediction error) factor away from the highest agent valuation.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13387",
        "abstract url": "https://arxiv.org/abs/2401.13387",
        "title": "A Mathematical Theory of Semantic Communication",
        "rating": -10,
        "keywords": [],
        "abstract": "The year 1948 witnessed the historic moment of the birth of classic information theory (CIT). Guided by CIT, modern communication techniques have approached the theoretic limitations, such as, entropy function $H(U)$, channel capacity $C=\\max_{p(x)}I(X;Y)$ and rate-distortion function $R(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d(x,\\hat{x})\\leq D} I(X;\\hat{X})$. Semantic communication paves a new direction for future communication techniques whereas the guided theory is missed. In this paper, we try to establish a systematic framework of semantic information theory (SIT). We investigate the behavior of semantic communication and find that synonym is the basic feature so we define the synonymous mapping between semantic information and syntactic information. Stemming from this core concept, synonymous mapping $f$, we introduce the measures of semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down semantic mutual information $I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity $C_s=\\max_{f_{xy}}\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\\min_{\\{f_x,f_{\\hat{x}}\\}}\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq D}I_s(\\tilde{X};\\hat{\\tilde{X}})$. Furthermore, we prove three coding theorems of SIT by using random coding and (jointly) typical decoding/encoding, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of SIT are extended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of semantic information theory. In addition, we discuss the semantic information measures in the continuous case. For the band-limited Gaussian channel, we obtain a new channel capacity formula, $C_s=B\\log\\left[S^4\\left(1+\\frac{P}{N_0B}\\right)\\right]$.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "(version 2.0 updated) 96 pages, 18 figures. This paper is submitted to IEEE Transactions on Information Theory (TIT)"
    },
    {
        "paper id": "2401.13390",
        "abstract url": "https://arxiv.org/abs/2401.13390",
        "title": "Memoryless Strategies in Stochastic Reachability Games",
        "rating": -10,
        "keywords": [],
        "abstract": "We study concurrent stochastic reachability games played on finite graphs. Two players, Max and Min, seek respectively to maximize and minimize the probability of reaching a set of target states. We prove that Max has a memoryless strategy that is optimal from all states that have an optimal strategy. Our construction provides an alternative proof of this result by Bordais, Bouyer and Le Roux, and strengthens it, as we allow Max's action sets to be countably infinite.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13394",
        "abstract url": "https://arxiv.org/abs/2401.13394",
        "title": "Determining hulls of generalized Reed-Solomon codes from algebraic geometry codes",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we provide conditions that hulls of generalized Reed-Solomon (GRS) codes are also GRS codes from algebraic geometry codes. If the conditions are not satisfied, we provide a method of linear algebra to find the bases of hulls of GRS codes and give formulas to compute their dimensions. Besides, we explain that the conditions are too good to be improved by some examples. Moreover, we show self-orthogonal and self-dual GRS codes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13399",
        "abstract url": "https://arxiv.org/abs/2401.13399",
        "title": "Real-time Risk Metrics for Programmatic Stablecoin Crypto Asset-Liability Management (CALM)",
        "rating": -10,
        "keywords": [],
        "abstract": "Stablecoins have turned out to be the \"killer\" use case of the growing digital asset space. However, risk management frameworks, including regulatory ones, have been largely absent. In this paper, we address the critical question of measuring and managing risk in stablecoin protocols, which operate on public blockchain infrastructure. The on-chain environment makes it possible to monitor risk and automate its management via transparent smart-contracts in real-time. We propose two risk metrics covering capitalization and liquidity of stablecoin protocols. We then explore in a case-study type analysis how our risk management framework can be applied to DAI, the biggest decentralized stablecoin by market capitalisation to-date, governed by MakerDAO. Based on our findings, we recommend that the protocol explores implementing automatic capital buffer adjustments and dynamic maturity gap matching. Our analysis demonstrates the practical benefits for scalable (prudential) risk management stemming from real-time availability of high-quality, granular, tamper-resistant on-chain data in the digital asset space. We name this approach Crypto Asset-Liability Management (CALM).",
        "subjects": [
            "q-fin.RM"
        ],
        "comment": "The authors would like to thank Professor Moorad Choudhry for review comments on an earlier draft. Submitted for the SNB-CIF Conference on Cryptoassets and Financial Innovation, 24 May 2024"
    },
    {
        "paper id": "2401.13407",
        "abstract url": "https://arxiv.org/abs/2401.13407",
        "title": "Increasing, not Diminishing: Investigating the Returns of Highly Maintainable Code",
        "rating": -10,
        "keywords": [],
        "abstract": "Understanding and effectively managing Technical Debt (TD) remains a vital challenge in software engineering. While many studies on code-level TD have been published, few illustrate the business impact of low-quality source code. In this study, we combine two publicly available datasets to study the association between code quality on the one hand, and defect count and implementation time on the other hand. We introduce a value-creation model, derived from regression analyses, to explore relative changes from a baseline. Our results show that the associations vary across different intervals of code quality. Furthermore, the value model suggests strong non-linearities at the extremes of the code quality spectrum. Most importantly, the model suggests amplified returns on investment in the upper end. We discuss the findings within the context of the \"broken windows\" theory and recommend organizations to diligently prevent the introduction of code smells in files with high churn. Finally, we argue that the value-creation model can be used to initiate discussions regarding the return on investment in refactoring efforts.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Paper accepted at the 7th International Conference on Technical Debt 2024, Lisbon, Portugal, May 14-15, 2024. The replication package is available here: https://zenodo.org/records/10560722"
    },
    {
        "paper id": "2401.13429",
        "abstract url": "https://arxiv.org/abs/2401.13429",
        "title": "Detection of Correlated Random Vectors",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we investigate the problem of deciding whether two standard normal random vectors $\\mathsf{X}\\in\\mathbb{R}^{n}$ and $\\mathsf{Y}\\in\\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\\mathsf{X}$ and a randomly and uniformly permuted version of $\\mathsf{Y}$, are correlated with correlation $\u03c1$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\u03c1$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices, and furthermore allow for partial correlations between these two.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "35 pages"
    },
    {
        "paper id": "2401.13434",
        "abstract url": "https://arxiv.org/abs/2401.13434",
        "title": "Query Exposure Prediction for Groups of Documents in Rankings",
        "rating": -10,
        "keywords": [],
        "abstract": "The main objective of an Information Retrieval system is to provide a user with the most relevant documents to the user's query. To do this, modern IR systems typically deploy a re-ranking pipeline in which a set of documents is retrieved by a lightweight first-stage retrieval process and then re-ranked by a more effective but expensive model. However, the success of a re-ranking pipeline is heavily dependent on the performance of the first stage retrieval, since new documents are not usually identified during the re-ranking stage. Moreover, this can impact the amount of exposure that a particular group of documents, such as documents from a particular demographic group, can receive in the final ranking. For example, the fair allocation of exposure becomes more challenging or impossible if the first stage retrieval returns too few documents from certain groups, since the number of group documents in the ranking affects the exposure more than the documents' positions. With this in mind, it is beneficial to predict the amount of exposure that a group of documents is likely to receive in the results of the first stage retrieval process, in order to ensure that there are a sufficient number of documents included from each of the groups. In this paper, we introduce the novel task of query exposure prediction (QEP). Specifically, we propose the first approach for predicting the distribution of exposure that groups of documents will receive for a given query. Our new approach, called GEP, uses lexical information from individual groups of documents to estimate the exposure the groups will receive in a ranking. Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our proposed GEP approach results in exposure predictions that are up to 40 % more accurate than the predictions of adapted existing query performance prediction and resource allocation approaches.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13442",
        "abstract url": "https://arxiv.org/abs/2401.13442",
        "title": "Finite-Precision Arithmetic Transceiver for Massive MIMO Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Efficient implementation of massive multiple-input-multiple-output (MIMO) transceivers is essential for the next-generation wireless networks. To reduce the high computational complexity of the massive MIMO transceiver, in this paper, we propose a new massive MIMO architecture using finite-precision arithmetic. First, we conduct the rounding error analysis and derive the lower bound of the achievable rate for single-input-multiple-output (SIMO) using maximal ratio combining (MRC) and multiple-input-single-output (MISO) systems using maximal ratio transmission (MRT) with finite-precision arithmetic. Then, considering the multi-user scenario, the rounding error analysis of zero-forcing (ZF) detection and precoding is derived by using the normal equations (NE) method. The corresponding lower bounds of the achievable sum rate are also derived and asymptotic analyses are presented. Built upon insights from these analyses and lower bounds, we propose a mixed-precision architecture for massive MIMO systems to offset performance gaps due to finite-precision arithmetic. The corresponding analysis of rounding errors and computational costs is obtained. Simulation results validate the derived bounds and underscore the superiority of the proposed mixed-precision architecture to the conventional structure.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "16 pages, 8 figures. Submitted to IEEE JSAC for possible publication"
    },
    {
        "paper id": "2401.13464",
        "abstract url": "https://arxiv.org/abs/2401.13464",
        "title": "Analysis and implementation of the Buck-Boost Modified Series Forward converter applied to photovoltaic systems",
        "rating": -10,
        "keywords": [],
        "abstract": "The mismatching phenomenon is one of the main issues in photovoltaic (PV) applications. It could reduce the generated power of a string when a PV panel has different performances from the other PV panels connected to the same string. Distributed Maximum Power Point Tracking (DMPPT) architectures are one of the most promising solutions to overcome the drawbacks associated with mismatching phenomena in PV applications. In this kind of architectures, a DC-DC module integrated converter (MIC) manages each PV panel, isolating it from the rest of the PV panels, for harvesting the maximum available power from the Sun. Due to the high number of DCDC converters used in a grid-tied PV installation, the most desired MIC requirements are high efficiency, low cost and the capability of voltage step-up and step-down. This paper proposes the Buck-Boost Modified Forward (BBMSF) converter as a good candidate to be applied in DMPPT architectures. A complete analysis of the BBMSF converter is carried out, including the steady-state analysis as well as the small signal analysis in continuous conduction mode. The main advantages of the BBMSF converter are its step-up and step-down voltage transfer function; a higher simplicity, since it only includes a single controlled switch; the soft switching characteristics in all the diodes and MOSFET, reaching in some cases ZVS and ZCS, and yielding high efficiencies; the use of an autotransformer, with better performances than a typical Forward transformer; and the good dynamic performance, like the Forward converter ones. The theoretical analyses are validated through the experimental results in a 225 W BBMSF prototype designed and built under the requirements of a 100 kW grid-tied PV installation, achieving an efficiency up to 93.6%.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This work has been supported by the Ministry of Economy and Competitiveness and FEDER funds through the research project \"Storage and Energy Management for Hybrid Electric Vehicles based on Fuel Cell, Battery and Supercapacitors\" - ELECTRICAR-AG- (DPI2014-53685-C2-1-R)"
    },
    {
        "paper id": "2401.13488",
        "abstract url": "https://arxiv.org/abs/2401.13488",
        "title": "Fast Inverse Model Transformation: Algebraic Framework for Fast Data Plane Verification",
        "rating": -10,
        "keywords": [],
        "abstract": "Data plane verification (DPV) analyzes routing tables and detects routing abnormalities and policy violations during network operation and planning. Thus, it has become an important tool to harden the networking infrastructure and the computing systems building on top. Substantial advancements have been made in the last decade and state-of-the-art DPV systems can achieve sub-us verification for an update of a single forwarding rule. In this paper, we introduce fast inverse model transformation (FIMT), the first theoretical framework to systematically model and analyze centralized DPV systems. FIMT reveals the algebraic structure in the model update process, a key step in fast DPV systems. Thus, it can systematically analyze the correctness of several DPV systems, using algebraic properties. The theory also guides the design and implementation of NeoFlash, a refactored version of Flash with new optimization techniques. Evaluations show that NeoFlash outperforms existing state-of-the-art centralized DPV systems in various datasets and reveal insights to key techniques towards fast DPV.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "12 pages pre-reference"
    },
    {
        "paper id": "2401.13490",
        "abstract url": "https://arxiv.org/abs/2401.13490",
        "title": "Visualization of rank-citation curves for fast detection of h-index anomalies in university metrics",
        "rating": -10,
        "keywords": [],
        "abstract": "University rankings, despite facing criticism, continue to maintain their popularity. In the 2023 Scopus Ranking of Ukrainian Universities, certain institutions stood out due to their high h-index, despite modest publication and citation numbers. This phenomenon can be attributed to influential research topics or involvement in international collaborative research. However, these results may also be due to the authors' own efforts to increase the number of citations of their publications in order to improve their h-index. To investigate this, the publications from the top 30 universities in the ranking were analysed, revealing humpback rank-citation curves for two universities. These humpbacks indicate unusual trends in the citation data, especially considering the high percentage of self-citations and FWCI of analysed papers. While quantitative analysis has limitations, the combination of humped rank-citation curves, self-citations, FWCI, and previous research findings raises concerns about the possible causes of these anomalies in the citation data of the analysed universities. The method presented in this paper can aid ranking compilers and citation databases managers in identifying potential instances of citation data anomalies, emphasizing the importance of expert assessment for accurate conclusions.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13509",
        "abstract url": "https://arxiv.org/abs/2401.13509",
        "title": "TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13524",
        "abstract url": "https://arxiv.org/abs/2401.13524",
        "title": "Combinatorics on words and generating Dirichlet series of automatic sequences",
        "rating": -10,
        "keywords": [],
        "abstract": "Generating series are crucial in enumerative combinatorics, analytic combinatorics, and combinatorics on words. Though it might seem at first view that generating Dirichlet series are less used in these fields than ordinary and exponential generating series, there are many notable papers where they play a fundamental role, as can be seen in particular in the work of Flajolet and several of his co-authors. In this paper, we study Dirichlet series of integers with missing digits or blocks of digits in some integer base $b$, i.e., where the summation ranges over the integers whose expansions form some language strictly included in the set of all words on the alphabet $\\{0, 1, \\dots, b-1\\}$ that do not begin with a $0$. We show how to unify and extend results proved by Nathanson in 2021 and by K\u00f6hler and Spilker in 2009. En route, we encounter several sequences from Sloane's On-Line Encyclopedia of Integer Sequences, as well as some famous $q$-automatic sequences or $q$-regular sequences.",
        "subjects": [
            "math.CO"
        ],
        "comment": "22 pages, 3 figures"
    },
    {
        "paper id": "2401.13535",
        "abstract url": "https://arxiv.org/abs/2401.13535",
        "title": "On the Approximate Core and Nucleon of Flow Games",
        "rating": -10,
        "keywords": [],
        "abstract": "The flow game with public arcs is a cooperative revenue game derived from a flow network. In this game, each player possesses an arc, while certain arcs, known as public arcs, are not owned by any specific player and are accessible to any coalition. The aim of this game is to maximize the flow that can be routed in the network through strategic coalition formation. By exploring its connection to the maximum partially disjoint path problem, we investigate the approximate core and nucleon of the flow game with public arcs. The approximate core is an extension of the core that allows for some deviation in group rationality, while the nucleon is a multiplicative analogue of the nucleolus. In this paper, we provide two complete characterizations for the optimal approximate core and show that the nucleon can be computed in polynomial time.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13539",
        "abstract url": "https://arxiv.org/abs/2401.13539",
        "title": "Dynamic Risk Management in Cyber Physical Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Cyber Physical Systems (CPS) enable new kinds of applications as well as significant improvements of existing ones in numerous different application domains. A major trait of upcoming CPS is an increasing degree of automation up to the point of autonomy, as there is a huge potential for economic success as well as for ecologic and societal improvements. However, to unlock the full potential of such (cooperative and automated) CPS, we first need to overcome several significant engineering challenges, where safety assurance is a particularly important one. Unfortunately, established safety assurance methods and standards do not live up to this task, as they have been designed with closed and less complex systems in mind. This paper structures safety assurance challenges of cooperative automated CPS, provides an overview on our vision of dynamic risk management and describes already existing building blocks.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13545",
        "abstract url": "https://arxiv.org/abs/2401.13545",
        "title": "Fine-grained Contract NER using instruction based model",
        "rating": -10,
        "keywords": [],
        "abstract": "Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences with task-specific instructions and answer choices, allowing for the identification of entities and their types within natural language. We harness the strength of LLMs by integrating supervised learning within them. The goal of this combined strategy is to boost the performance of LLMs in extraction tasks like NER while simultaneously addressing hallucination issues often observed in LLM-generated content. A novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released along with our baseline models. Our models and dataset are available to the community for future research * .",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13546",
        "abstract url": "https://arxiv.org/abs/2401.13546",
        "title": "Analysis, design, and implementation of the AFZ converter applied to photovoltaic systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Grid-tied photovoltaic (PV) installations with Distributed Maximum Power Point Tracking (DMPPT) architectures include a DC-DC Module Integrated Converter (MIC) for managing each PV panel, isolating it from the others, reducing the mismatching effect and maximizing the harvested power. In this paper, the Autotransformer Forward converter with type-Zeta resonant reset (AFZ) is proposed as a DMPPT architecture MIC candidate. The main characteristics of the AFZ converter are the high versatility due to its voltage step-up and step-down capability; the use of an optimized autotransformer with only two windings, reducing the complexity and power losses of this component; the good dynamic performances, like the Forward converter ones; the low number of components and the simplicity and high feasibility associated to the use of just one active switch. Besides, soft switching transitions are achieved thanks to the autotransformer type-Zeta resonant reset. The steady-state theoretical analysis, considering the effect of the autotransformer leakage inductance, is presented. The converter is also studied in the frequency domain, obtaining the small-signal transfer functions. A design procedure based on the requirements of a 100 kW grid-tied photovoltaic installation is described, yielding in a 225 W prototype with efficiencies up to 95.6 %. Experimental results validate the theoretical analysis.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This work was supported in part by the Spanish Ministry of Economy and Competitiveness and FEDER funds through the research project: Modeling and Control Strategies for the Stabilization of the Interconnection of Power Electronic Converters CONEXPOT under Grant DPI2017-84572-C2-2-R. copyright: 2020 IEEE"
    },
    {
        "paper id": "2401.13556",
        "abstract url": "https://arxiv.org/abs/2401.13556",
        "title": "Extension of the Injected-Absorbed-Current Method applied to DC-DC Converters with Input Filter, Output Post-filter and Feedforward Compensations",
        "rating": -10,
        "keywords": [],
        "abstract": "In railway applications, it is common to use an LC filter connected between the catenary and the input port of the main converter of the auxiliary and traction systems. In addition, in the auxiliary systems, there is a converter operating as a battery charger, which requires a very low ripple in the output current and output voltage, so a postfilter may be placed at the output port of the converter. This article proposes a step-by-step methodology to extend the injected-absorbed-current (IAC) method in order to obtain transfer functions that consider the effects of the input filter, output postfilter, and some feedforward compensations. The proposed methodology allows reusing the characteristic coefficients of the DC-DC converter model derived from the existing IAC method. One of the advantages of the proposed methodology is that the transfer functions obtained in this article are valid for cases where both, one or none of the filters, are implemented. Finally, for the experimental validation of the proposed methodology, the phase-shifted full-bridge converter was selected as a convenient example. Furthermore, the experimental measurements have been performed on two prototypes.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This work was supported in part by the European Regional Development Fund (FEDER), in part by the Ministry of Science, Innovation and Universities, and in part by the State Research Agency through the Research Project: Modeling and Control Strategies for the Stabilization of the Interconnection of Power Electronic Converters CONEXPOT-2 under Grant DPI2017-84572-C2-2-R (AEI/FEDER, UE)"
    },
    {
        "paper id": "2401.13561",
        "abstract url": "https://arxiv.org/abs/2401.13561",
        "title": "Pricing of Short Circuit Current in High IBR-Penetrated System",
        "rating": -10,
        "keywords": [],
        "abstract": "With the growing penetration of Inverter-Based Resources (IBRs) in power systems, stability service markets have emerged to incentivize technologies that ensure power system stability and reliability. Among the various challenges faced in power system operation and stability, a prominent issue raised from the increasing integration of large-scale IBRs is the significant reduction of the Short-Circuit Current (SCC) level in the system, which poses a considerable threat to system voltage stability and protection. Thus, a proper market mechanism to incentivize the provision of SCC as a stability service is desired. However, the pricing of this service within the future stability market has not yet been fully developed, due to the nonconvex nature of SCC constraints and the locational property of SCC. To address these problems, this work aims to explore, for the first time, a pricing model for SCC service by incorporating a linearized SCC constraint into the Unit Commitment (UC) problem, to achieve the desired SCC level and extract the shadow price for SCC through different pricing methods.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13564",
        "abstract url": "https://arxiv.org/abs/2401.13564",
        "title": "RIS Empowered Near-Field Covert Communications",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper studies an extremely large-scale reconfigurable intelligent surface (XL-RIS) empowered covert communication system in the near-field region. Alice covertly transmits messages to Bob with the assistance of the XL-RIS, while evading detection by Willie. To enhance the covert communication performance, we maximize the achievable covert rate by jointly optimizing the hybrid analog and digital beamformers at Alice, as well as the reflection coefficient matrix at the XL-RIS. An alternating optimization algorithm is proposed to solve the joint beamforming design problem. For the hybrid beamformer design, a semi-closed-form solution for fully digital beamformer is first obtained by a weighted minimum mean-square error based algorithm, then the baseband digital and analog beamformers at Alice are designed by approximating the fully digital beamformer via manifold optimization. For the XL-RIS's reflection coefficient matrix design, a low-complexity alternating direction method of multipliers based algorithm is proposed to address the challenge of large-scale variables and unit-modulus constraints. Numerical results unveil that i) the near-field communications can achieve a higher covert rate than the far-field covert communications in general, and still realize covert transmission even if Willie is located at the same direction as Bob and closer to the XL-RIS; ii) the proposed algorithm can enhance the covert rate significantly compared to the benchmark schemes; iii) the proposed algorithm leads to a beam diffraction pattern that can bypass Willie and achieve high-rate covert transmission to Bob.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "15 pages, 8 figures, submitted to IEEE journal"
    },
    {
        "paper id": "2401.13568",
        "abstract url": "https://arxiv.org/abs/2401.13568",
        "title": "Investigating the Performance of Soft Robotic Adaptive Feet with Longitudinal and Transverse Arches",
        "rating": -10,
        "keywords": [],
        "abstract": "Biped robots usually adopt feet with a rigid structure that simplifies walking on flat grounds and yet hinders ground adaptation in unstructured environments, thus jeopardizing stability. We recently explored in the SoftFoot the idea of adapting a robotic foot to ground irregularities along the sagittal plane. Building on the previous results, we propose in this paper a novel robotic foot able to adapt both in the sagittal and frontal planes, similarly to the human foot. It features five parallel modules with intrinsic longitudinal adaptability that can be combined in many possible designs through optional rigid or elastic connections. By following a methodological design approach, we narrow down the design space to five candidate foot designs and implement them on a modular system. Prototypes are tested experimentally via controlled application of force, through a robotic arm, onto a sensorized plate endowed with different obstacles. Their performance is compared, using also a rigid foot and the previous SoftFoot as a baseline. Analysis of footprint stability shows that the introduction of the transverse arch, by elastically connecting the five parallel modules, is advantageous for obstacle negotiation, especially when obstacles are located under the forefoot. In addition to biped robots' locomotion, this finding might also benefit lower-limb prostheses design.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to Frontiers in Robotics and AI"
    },
    {
        "paper id": "2401.13573",
        "abstract url": "https://arxiv.org/abs/2401.13573",
        "title": "Distributed matrix multiplication with straggler tolerance using algebraic function fields",
        "rating": -10,
        "keywords": [],
        "abstract": "The problem of straggler mitigation in distributed matrix multiplication (DMM) is considered for a large number of worker nodes and a fixed small finite field. Polynomial codes and matdot codes are generalized by making use of algebraic function fields (i.e., algebraic functions over an algebraic curve) over a finite field. The construction of optimal solutions is translated to a combinatorial problem on the Weierstrass semigroups of the corresponding algebraic curves. Optimal or almost optimal solutions are provided. These have the same computational complexity per worker as classical polynomial and matdot codes, and their recovery thresholds are almost optimal in the asymptotic regime (growing number of workers and a fixed finite field).",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13587",
        "abstract url": "https://arxiv.org/abs/2401.13587",
        "title": "Deep Learning Based Adaptive Joint mmWave Beam Alignment",
        "rating": -10,
        "keywords": [],
        "abstract": "The challenging propagation environment, combined with the hardware limitations of mmWave systems, gives rise to the need for accurate initial access beam alignment strategies with low latency and high achievable beamforming gain. Much of the recent work in this area either focuses on one-sided beam alignment, or, joint beam alignment methods where both sides of the link perform a sequence of fixed channel probing steps. Codebook-based non-adaptive beam alignment schemes have the potential to allow multiple user equipment (UE) to perform initial access beam alignment in parallel whereas adaptive schemes are favourable in achievable beamforming gain. This work introduces a novel deep learning based joint beam alignment scheme that aims to combine the benefits of adaptive, codebook-free beam alignment at the UE side with the advantages of a codebook-sweep based scheme at the base station. The proposed end-to-end trainable scheme is compatible with current cellular standard signaling and can be readily integrated into the standard without requiring significant changes to it. Extensive simulations demonstrate superior performance of the proposed approach over purely codebook-based ones.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13596",
        "abstract url": "https://arxiv.org/abs/2401.13596",
        "title": "PLATE: A perception-latency aware estimator,",
        "rating": -10,
        "keywords": [],
        "abstract": "Target tracking is a popular problem with many potential applications. There has been a lot of effort on improving the quality of the detection of targets using cameras through different techniques. In general, with higher computational effort applied, i.e., a longer perception-latency, a better detection accuracy is obtained. However, it is not always useful to apply the longest perception-latency allowed, particularly when the environment doesn't require to and when the computational resources are shared between other tasks. In this work, we propose a new Perception-LATency aware Estimator (PLATE), which uses different perception configurations in different moments of time in order to optimize a certain performance measure. This measure takes into account a perception-latency and accuracy trade-off aiming for a good compromise between quality and resource usage. Compared to other heuristic frame-skipping techniques, PLATE comes with a formal complexity and optimality analysis. The advantages of PLATE are verified by several experiments including an evaluation over a standard benchmark with real data and using state of the art deep learning object detection methods for the perception stage.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This is the accepted version an already published manuscript. See journal reference for details"
    },
    {
        "paper id": "2401.13597",
        "abstract url": "https://arxiv.org/abs/2401.13597",
        "title": "Base-extension Semantics for Modal Logic",
        "rating": -10,
        "keywords": [],
        "abstract": "In proof-theoretic semantics, meaning is based on inference. It may seen as the mathematical expression of the inferentialist interpretation of logic. Much recent work has focused on base-extension semantics, in which the validity of formulas is given by an inductive definition generated by provability in a `base' of atomic rules. Base-extension semantics for classical and intuitionistic propositional logic have been explored by several authors. In this paper, we develop base-extension semantics for the classical propositional modal systems K, KT , K4, and S4, with $\\square$ as the primary modal operator. We establish appropriate soundness and completeness theorems and establish the duality between $\\square$ and a natural presentation of $\\lozenge$. We also show that our semantics is in its current form not complete with respect to euclidean modal logics. Our formulation makes essential use of relational structures on bases.",
        "subjects": [
            "math.LO"
        ],
        "comment": "Accepted to be published in the Logic Journal of the IGPL"
    },
    {
        "paper id": "2401.13602",
        "abstract url": "https://arxiv.org/abs/2401.13602",
        "title": "Perception-latency aware distributed target tracking",
        "rating": -10,
        "keywords": [],
        "abstract": "This work is devoted to the problem of distributed target tracking when a team of robots detect the target through a variable perception-latency mechanism. A reference for the robots to track is constructed in terms of a desired formation around the estimation of the target position. However, it is noted that due to the perception-latency, classical estimation techniques have smoothness issues which prevent asymptotic stability for the formation control. We propose a near-optimal smooth-output estimator which circumvents this issue. Moreover, local estimations are fused using novel dynamic consensus techniques. The advantages of the proposal as well as a comparison with a non-smooth optimal alternative are discussed through simulation examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This is the accepted version an already published manuscript. See journal reference for details"
    },
    {
        "paper id": "2401.13606",
        "abstract url": "https://arxiv.org/abs/2401.13606",
        "title": "Run-to-Run Control With Bayesian Optimization for Soft Landing of Short-Stroke Reluctance Actuators",
        "rating": -10,
        "keywords": [],
        "abstract": "There is great interest in minimizing the impact forces of reluctance actuators during commutations, in order to reduce contact bouncing, acoustic noise and mechanical wear. In this regard, a run-to-run control algorithm is proposed to decrease the contact velocity, by exploiting the repetitive operations of these devices. The complete control is presented, with special focus on the optimization method and the input definition. The search method is based on Bayesian optimization, and several additions are introduced for its application in run-to-run control, e.g. the removal of stored points and the definition of a new acquisition function. Additionally, methods for the input parametrization and dimension reduction are presented. For analysis, Monte Carlo simulations are performed using a dynamic model of a commercial solenoid valve, comparing the proposed search method with two alternatives. Furthermore, the control strategy is validated through experimental testing, using several devices from the same ensemble of solenoid valves.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This is the accepted version an already published manuscript. See journal reference for details"
    },
    {
        "paper id": "2401.13610",
        "abstract url": "https://arxiv.org/abs/2401.13610",
        "title": "Scale-free vision-based aerial control of a ground formation with hybrid topology",
        "rating": -10,
        "keywords": [],
        "abstract": "We present a novel vision-based control method to make a group of ground mobile robots achieve a specified formation shape with unspecified size. Our approach uses multiple aerial control units equipped with downward-facing cameras, each observing a partial subset of the multirobot team. The units compute the control commands from the ground robots' image projections, using neither calibration nor scene scale information, and transmit them to the robots. The control strategy relies on the calculation of image similarity transformations, and we show it to be asymptotically stable if the overlaps between the subsets of controlled robots satisfy certain conditions. The presence of the supervisory units, which coordinate their motions to guarantee a correct control performance, gives rise to a hybrid system topology. All in all, the proposed system provides relevant practical advantages in simplicity and flexibility. Within the problem of controlling a team shape, our contribution lies in addressing several simultaneous challenges: the controller needs only partial information of the robotic group, does not use distance measurements or global reference frames, is designed for unicycle agents, and can accommodate topology changes. We present illustrative simulation results.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This is the accepted version an already published manuscript. See journal reference for details"
    },
    {
        "paper id": "2401.13622",
        "abstract url": "https://arxiv.org/abs/2401.13622",
        "title": "Cooperative Periodic Coverage With Collision Avoidance",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper we propose a periodic solution to the problem of persistently covering a finite set of interest points with a group of autonomous mobile agents. These agents visit periodically the points and spend some time carrying out the coverage task, which we call coverage time. Since this periodic persistent coverage problem is NP-hard, we split it into three subproblems to counteract its complexity. In the first place, we plan individual closed paths for the agents to cover all the points. Second, we formulate a quadratically constrained linear program to find the optimal coverage times and actions that satisfy the coverage objective. Finally, we join together the individual plans of the agents in a periodic team plan by obtaining a schedule that guarantees collision avoidance. To this end, we solve a mixed integer linear program that minimizes the time in which two or more agents move at the same time. Eventually, we apply the proposed solution to an induction hob with mobile inductors for a domestic heating application and show its performance with experiments on a real prototype.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This is the accepted version an already published manuscript. See journal reference for details"
    },
    {
        "paper id": "2401.13623",
        "abstract url": "https://arxiv.org/abs/2401.13623",
        "title": "What Makes a Great Software Quality Assurance Engineer?",
        "rating": -10,
        "keywords": [],
        "abstract": "Software Quality Assurance (SQA) Engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities. In general, a great SQA engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end. Recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked. As software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an SQA engineer, but what makes them great? We addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world. We use the data collected from these activities to derive a comprehensive set of attributes that are considered important. As a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes. Through a rating survey, we confirmed that the distinguishing characteristics of great SQA engineers are curiosity, the ability to communicate effectively, and critical thinking skills. This work will guide further studies with SQA practitioners, by considering contextual factors and providing some implications for research and practice.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "17 pages, 6 figures, 12 tables"
    },
    {
        "paper id": "2401.13624",
        "abstract url": "https://arxiv.org/abs/2401.13624",
        "title": "Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint",
        "rating": -10,
        "keywords": [],
        "abstract": "Adversarial training is a widely used method to improve the robustness of deep neural networks (DNNs) over adversarial perturbations. However, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \\textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising. In this paper, we provide a theoretical understanding of the question of whether overfitted DNNs in adversarial training can generalize from an approximation viewpoint. Specifically, our main results are summarized into three folds: i) For classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized DNNs that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level. ii) Linear over-parameterization (meaning that the number of parameters is only slightly larger than the sample size) is enough to ensure such existence if the target function is smooth enough. iii) For regression, our results demonstrate that there also exist infinitely many overfitted DNNs with linear over-parameterization in adversarial training that can achieve almost optimal rates of convergence for the standard generalization error. Overall, our analysis points out that robust overfitting can be avoided but the required model capacity will depend on the smoothness of the target function, while a robust generalization gap is inevitable. We hope our analysis will give a better understanding of the mathematical foundations of robustness in DNNs from an approximation view.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13630",
        "abstract url": "https://arxiv.org/abs/2401.13630",
        "title": "Enabling Seamless Data Security, Consensus, and Trading in Vehicular Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "Cooperative driving is an emerging paradigm to enhance the safety and efficiency of autonomous vehicles. To ensure successful cooperation, road users must reach a consensus for making collective decisions, while recording vehicular data to analyze and address failures related to such agreements. This data has the potential to provide valuable insights into various vehicular events, while also potentially improving accountability measures. Furthermore, vehicles may benefit from the ability to negotiate and trade services among themselves, adding value to the cooperative driving framework. However, the majority of proposed systems aiming to ensure data security, consensus, or service trading, lack efficient and thoroughly validated mechanisms that consider the distinctive characteristics of vehicular networks. These limitations are amplified by a dependency on the centralized support provided by the infrastructure. Furthermore, corresponding mechanisms must diligently address security concerns, especially regarding potential malicious or misbehaving nodes, while also considering inherent constraints of the wireless medium. We introduce the Verifiable Event Extension (VEE), an applicational extension designed for Intelligent Transportation System (ITS) messages. The VEE operates seamlessly with any existing standardized vehicular communications protocol, addressing crucial aspects of data security, consensus, and trading with minimal overhead. To achieve this, we employ blockchain techniques, Byzantine fault tolerance (BFT) consensus protocols, and cryptocurrency-based mechanics. To assess our proposal's feasibility and lightweight nature, we employed a hardware-in-the-loop setup for analysis. Experimental results demonstrate the viability and efficiency of the VEE extension in overcoming the challenges posed by the distributed and opportunistic nature of wireless vehicular communications.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13631",
        "abstract url": "https://arxiv.org/abs/2401.13631",
        "title": "Quantifying the Impact of Frame Preemption on Combined TSN Shapers",
        "rating": -10,
        "keywords": [],
        "abstract": "Different scheduling mechanisms in Time Sensitive Networking (TSN) can be integrated together to design and support complex architectures with enhanced capabilities for mixed critical networks. Integrating Frame Preemption (FP) with Credit-Based Shaper (CBS) and Gate Control List (GCL) opens up different modes and configuration choices resulting in a complex evaluation of several possibilities and their impact on the Quality of Service (QoS). In this paper, we implement and quantify the integration of preemptive CBS with GCL by incorporating FP into the architecture. Our experiments show that the end-to-end delay of Audio Video Bridging (AVB) flows shaped by CBS reduces significantly (up to 40\\%) when AVB flows are set to preemptable class. We further show that the jitter of Time Triggered (TT) traffic remains unaffected in \"with Hold/Release\" mode. Furthermore, we propose to introduce Guardband (GB) in the \"without Hold/Release\" to reduce the jitter of the TT flow. We compare all the different integration modes, starting with CBS with GCL, extending it further to FP. We evaluate all feasible combinations in both synthetic and realistic scenarios and offer recommendations for practical configuration methods.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Accepted in IEEE/IFIP Network Operations and Management Symposium (NOMS) 2024"
    },
    {
        "paper id": "2401.13645",
        "abstract url": "https://arxiv.org/abs/2401.13645",
        "title": "Employing polyhedral methods to optimize stencils on FPGAs with stencil-specific caches, data reuse, and wide data bursts",
        "rating": -10,
        "keywords": [],
        "abstract": "It is well known that to accelerate stencil codes on CPUs or GPUs and to exploit hardware caches and their lines optimizers must find spatial and temporal locality of array accesses to harvest data-reuse opportunities. On FPGAs there is the burden that there are no built-in caches (or only pre-built hardware descriptions for cache blocks that are inefficient for stencil codes). But this paper demonstrates that this lack is also a chance as polyhedral methods can be used to generate stencil-specific cache-structures of the right sizes on the FPGA and to fill and flush them efficiently with wide bursts during stencil execution. The paper shows how to derive the appropriate directives and code restructurings from stencil codes so that the FPGA compiler generates fast stencil hardware. Switching on our optimization improves the runtime of a set of 10 stencils by between 43x and 156x.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13653",
        "abstract url": "https://arxiv.org/abs/2401.13653",
        "title": "HetDAPAC: Distributed Attribute-Based Private Access Control with Heterogeneous Attributes",
        "rating": -10,
        "keywords": [],
        "abstract": "Verifying user attributes to provide fine-grained access control to databases is fundamental to an attribute-based authentication system. In such systems, either a single (central) authority verifies all attributes, or multiple independent authorities verify individual attributes distributedly to allow a user to access records stored on the servers. While a \\emph{central} setup is more communication cost efficient, it causes privacy breach of \\emph{all} user attributes to a central authority. Recently, Jafarpisheh et al. studied an information theoretic formulation of the \\emph{distributed} multi-authority setup with $N$ non-colluding authorities, $N$ attributes and $K$ possible values for each attribute, called an $(N,K)$ distributed attribute-based private access control (DAPAC) system, where each server learns only one attribute value that it verifies, and remains oblivious to the remaining $N-1$ attributes. We show that off-loading a subset of attributes to a central server for verification improves the achievable rate from $\\frac{1}{2K}$ in Jafarpisheh et al. to $\\frac{1}{K+1}$ in this paper, thus \\emph{almost doubling the rate} for relatively large $K$, while sacrificing the privacy of a few possibly non-sensitive attributes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13667",
        "abstract url": "https://arxiv.org/abs/2401.13667",
        "title": "Predicting the Impact of Crashes Across Release Channels",
        "rating": -10,
        "keywords": [],
        "abstract": "Software maintenance faces a persistent challenge with crash bugs, especially across diverse release channels catering to distinct user bases. Nightly builds, favoured by enthusiasts, often reveal crashes that are cheaper to fix but may differ significantly from those in stable releases. In this paper, we emphasize the need for a data-driven solution to predict the impact of crashes happening on nightly channels once they are released to stable channels. We also list the challenges that need to be considered when approaching this problem.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13715",
        "abstract url": "https://arxiv.org/abs/2401.13715",
        "title": "A tabu search-based LED selection approach safeguarding visible light communication systems",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we investigate the secrecy performance of a single-input single-output visible light communication (VLC) channel in the presence of an eavesdropper. The studied VLC system comprises distributed light-emitting diodes (LEDs) and multiple randomly located users (UEs) within an indoor environment. A sum secrecy rate maximization problem is formulated to enhance confidential transmission by selecting the optimal LED for each UE. To address the non-convex and non-continuous nature of this problem, we propose a tabu search-based algorithm that prevents entrapment in local optima by organizing the trial vectors from previous iterations. Furthermore, we develop three straightforward LED selection strategies that reduce computational complexity by employing fixed criteria to choose one LED for each UE. We also examine the convergence and complexity analysis of the proposed algorithm and strategies. The results demonstrate that the secrecy performance of our proposed algorithm is very close to the global optimal value and surpasses that of the developed strategies.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "17 pages, 8 figures"
    },
    {
        "paper id": "2401.13726",
        "abstract url": "https://arxiv.org/abs/2401.13726",
        "title": "Supporting Sensemaking of Large Language Model Outputs at Scale",
        "rating": -10,
        "keywords": [],
        "abstract": "Large language models (LLMs) are capable of generating multiple responses to a single prompt, yet little effort has been expended to help end-users or system designers make use of this capability. In this paper, we explore how to present many LLM responses at once. We design five features, which include both pre-existing and novel methods for computing similarities and differences across textual documents, as well as how to render their outputs. We report on a controlled user study (n=24) and eight case studies evaluating these features and how they support users in different tasks. We find that the features support a wide variety of sensemaking tasks and even make tasks previously considered to be too difficult by our participants now tractable. Finally, we present design guidelines to inform future explorations of new LLM interfaces.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "34 pages, 13 figures, conditionally accepted to ACM Conference on Human Factors in Computing Systems 2024"
    },
    {
        "paper id": "2401.13743",
        "abstract url": "https://arxiv.org/abs/2401.13743",
        "title": "Intermittency versus Path Loss in RIS-aided THz Communication: A Data Significance Approach",
        "rating": -10,
        "keywords": [],
        "abstract": "The transition to Terahertz (THz) frequencies, providing an ultra-wide bandwidth, is a key driver for future wireless communication networks. However, the specific properties of the THz channel, such as severe path loss and vulnerability to blockage, pose a significant challenge in balancing data rate and reliability. This work considers reconfigurable intelligent surface (RIS)-aided THz communication, where the effective exploitation of a strong, but intermittent line-of-sight (LOS) path versus a reliable, yet weaker RIS-path is studied. We introduce a mixed-criticality superposition coding scheme that addresses this tradeoff from a data significance perspective. The results show that the proposed scheme enables reliable transmission for a portion of high-criticality data without significantly impacting the overall achievable sum rate and queuing delay. Additionally, we gain insights into how the LOS blockage probability and the channel gain of the RIS-link influence the rate performance of our scheme.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "6 pages, 5 figures (accepted for publication at IEEE ICC 2024)"
    },
    {
        "paper id": "2401.13747",
        "abstract url": "https://arxiv.org/abs/2401.13747",
        "title": "Searching in trees with monotonic query times",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider the following generalization of binary search in sorted arrays to tree domains. In each step of the search, an algorithm is querying a vertex $q$, and as a reply, it receives an answer, which either states that $q$ is the desired target, or it gives the neighbor of $q$ that is closer to the target than $q$. A further generalization assumes that a vertex-weight function $\u03c9$ gives the query costs, i.e., the cost of querying $q$ is $\u03c9(q)$. The goal is to find an adaptive search strategy requiring the minimum cost in the worst case. This problem is NP-complete for general weight functions and one of the challenging open questions is whether there exists a polynomial-time constant factor approximation algorithm for an arbitrary tree? In this work, we prove that there exist a constant-factor approximation algorithm for trees with a monotonic cost function, i.e., when the tree has a vertex $v$ such that the weights of the subsequent vertices on the path from $v$ to any leaf give a monotonic (non-increasing or non-decreasing) sequence $S$. This gives a constant factor approximation algorithm for trees with cost functions such that each such sequence $S$ has a fixed number of monotonic segments. Finally, we combine several earlier results to show that the problem is NP-complete when the number of monotonic segments in $S$ is at least $4$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13754",
        "abstract url": "https://arxiv.org/abs/2401.13754",
        "title": "Multi-Function Multi-Way Analog Technology for Sustainable Machine Intelligence Computation",
        "rating": -10,
        "keywords": [],
        "abstract": "Numerical computation is essential to many areas of artificial intelligence (AI), whose computing demands continue to grow dramatically, yet their continued scaling is jeopardized by the slowdown in Moore's law. Multi-function multi-way analog (MFMWA) technology, a computing architecture comprising arrays of memristors supporting in-memory computation of matrix operations, can offer tremendous improvements in computation and energy, but at the expense of inherent unpredictability and noise. We devise novel randomized algorithms tailored to MFMWA architectures that mitigate the detrimental impact of imperfect analog computations while realizing their potential benefits across various areas of AI, such as applications in computer vision. Through analysis, measurements from analog devices, and simulations of larger systems, we demonstrate orders of magnitude reduction in both computation and energy with accuracy similar to digital computers.",
        "subjects": [
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13758",
        "abstract url": "https://arxiv.org/abs/2401.13758",
        "title": "Assumptions and Bounds in the Instrumental Variable Model",
        "rating": -10,
        "keywords": [],
        "abstract": "In this note we give proofs for results relating to the Instrumental Variable (IV) model with binary response $Y$ and binary treatment $X$, but with an instrument $Z$ with $K$ states. These results were originally stated in Richardson & Robins (2014), \"ACE Bounds; SEMS with Equilibrium Conditions,\" arXiv:1410.0470.",
        "subjects": [
            "math.ST"
        ],
        "comment": "27 pages, 1 figure, 1 table. Proofs of Theorems 1 and 2 stated in Richardson and Robins (2014) [arXiv:1410.0470]. v2 improves the writing in a few places"
    },
    {
        "paper id": "2401.13773",
        "abstract url": "https://arxiv.org/abs/2401.13773",
        "title": "New Sequence-Independent Lifting Techniques for Cutting Planes and When They Induce Facets",
        "rating": -10,
        "keywords": [],
        "abstract": "Sequence-independent lifting is a procedure for strengthening valid inequalities of an integer program. We generalize the sequence-independent lifting method of Gu, Nemhauser, and Savelsbergh (GNS lifting) for cover inequalities and correct an error in their proposed generalization. We obtain a new sequence-independent lifting technique -- piecewise-constant (PC) lifting -- with a number of interesting properties. We derive a broad set of sufficient conditions under which PC lifting is facet defining. To our knowledge, this is the first characterization of facet-defining sequence-independent liftings that are efficiently computable from the underlying cover. Finally, we demonstrate via experiments that PC lifting can be a useful alternative to GNS lifting. We test our new lifting techniques atop a number of novel cover cut generation routines, which prove to be effective in experiments with CPLEX.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13775",
        "abstract url": "https://arxiv.org/abs/2401.13775",
        "title": "The Friis Transmission Formula, Active Antenna Available Power, Reciprocity in Multiantenna Systems, and the Unnamed Power Gain",
        "rating": -10,
        "keywords": [],
        "abstract": "It is well known that reciprocal antenna systems have a symmetric impedance matrix. What is less well understood is how the system reciprocity manifests in the bidirectionally transferred powers with a beamformed system such as a massive multiple input multiple output (MIMO) array. To answer this question, we connect four disparate ideas, Lorentz reciprocity, the Friis transmission formula, noise-based active antenna parameters, and the active antenna available power. This results in an unnamed power gain that is connected with available gain and transducer gain but is unmentioned in the theory of two-port amplifiers. This quantity is symmetric under link direction reversal in the near field, as well as the far field, and generalizes the Friis transmission formula to beamformed multiport antenna systems in an arbitrary reciprocal propagation environment.",
        "subjects": [
            "physics.class-ph"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2401.13779",
        "abstract url": "https://arxiv.org/abs/2401.13779",
        "title": "Faster Convergence with Less Communication: Broadcast-Based Subgraph Sampling for Decentralized Learning over Wireless Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely adopted algorithm for decentralized training of machine learning models across networked agents. A crucial part of D-SGD is the consensus-based model averaging, which heavily relies on information exchange and fusion among the nodes. Specifically, for consensus averaging over wireless networks, communication coordination is necessary to determine when and how a node can access the channel and transmit (or receive) information to (or from) its neighbors. In this work, we propose $\\texttt{BASS}$, a broadcast-based subgraph sampling method designed to accelerate the convergence of D-SGD while considering the actual communication cost per iteration. $\\texttt{BASS}$ creates a set of mixing matrix candidates that represent sparser subgraphs of the base topology. In each consensus iteration, one mixing matrix is sampled, leading to a specific scheduling decision that activates multiple collision-free subsets of nodes. The sampling occurs in a probabilistic manner, and the elements of the mixing matrices, along with their sampling probabilities, are jointly optimized. Simulation results demonstrate that $\\texttt{BASS}$ enables faster convergence with fewer transmission slots compared to existing link-based scheduling methods. In conclusion, the inherent broadcasting nature of wireless channels offers intrinsic advantages in accelerating the convergence of decentralized optimization and learning.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "11 pages, 5 figures, submitted for possible journal publication. arXiv admin note: text overlap with arXiv:2310.16106"
    },
    {
        "paper id": "2401.13782",
        "abstract url": "https://arxiv.org/abs/2401.13782",
        "title": "Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility",
        "rating": -10,
        "keywords": [],
        "abstract": "As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside controls precisely matched by 9 key covariates. Our statistical and causal inference analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. Given these findings, we advocate for a responsible approach to curation, encouraging influencers to uphold the journalistic standard that includes showcasing diverse research topics, authors, and institutions.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "10 Pages, 14 Figures"
    },
    {
        "paper id": "2401.13784",
        "abstract url": "https://arxiv.org/abs/2401.13784",
        "title": "On the Predictive Capability of Dynamic Mode Decomposition for Nonlinear Periodic Systems with Focus on Orbital Mechanics",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper discusses the predictive capability of Dynamic Mode Decomposition (DMD) in the context of orbital mechanics. The focus is specifically on the Hankel variant of DMD which uses a stacked set of time-delayed observations for system identification and subsequent prediction. A theory on the minimum number of time delays required for accurate reconstruction of periodic trajectories of nonlinear systems is presented and corroborated using experimental analysis. In addition, the window size for training and prediction regions, respectively, is presented. The need for a meticulous approach while using DMD is emphasized by drawing comparisons between its performance on two candidate satellites, the ISS and MOLNIYA-3-50.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13790",
        "abstract url": "https://arxiv.org/abs/2401.13790",
        "title": "Orthogonal Time-Frequency-Space (OTFS) and Related Signaling",
        "rating": -10,
        "keywords": [],
        "abstract": "The principle of orthogonal time-frequency-space (OTFS) signaling is firstly analyzed, followed by explaining that OTFS embeds another signaling scheme referred to as orthogonal short-time Fourier (OSTF). Then, the relationship among OTFS, OSTF, orthogonal frequency-division multiplexing (OFDM) and single-carrier frequency-division multiple-access (SC-FDMA) is explored, demonstrating that OSTF/OTFS are fundamentally the extensions of OFDM/SC-FDMA from one-dimensional (1D) signaling to two-dimensional (2D) signaling. Hence, the characteristics and performance of OSTF/OTFS schemes can be perceived from the well-understood OFDM/SC-FDMA schemes. Accordingly, the advantages and disadvantages of OSTF/OTFS are discussed. Furthermore, from the principles of OFDM/SC-FDMA, the multiuser multiplexing in OSTF/OTFS systems is analyzed with respect to uplink and downlink, respectively. Added on this, a range of generalized multiplexing schemes are presented, whose characteristics are briefly analyzed.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13802",
        "abstract url": "https://arxiv.org/abs/2401.13802",
        "title": "Investigating the Efficacy of Large Language Models for Code Clone Detection",
        "rating": -10,
        "keywords": [],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We then conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language CCD attaining an F1-score of 0.877 and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the prompt and the difficulty level of the problems has an impact on the performance of ChatGPT. Finally we provide insights and future directions based on our initial analysis",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13804",
        "abstract url": "https://arxiv.org/abs/2401.13804",
        "title": "Exploring Parent's Needs for Children-Centered AI to Support Preschoolers' Storytelling and Reading Activities",
        "rating": -10,
        "keywords": [],
        "abstract": "Interactive storytelling is vital for preschooler development. While children's interactive partners have traditionally been their parents and teachers, recent advances in artificial intelligence (AI) have sparked a surge of AI-based storytelling technologies. As these technologies become increasingly ubiquitous in preschoolers' lives, questions arise regarding how they function in practical storytelling scenarios and, in particular, how parents, the most critical stakeholders, experience and perceive these technologies. This paper investigates these questions through a qualitative study with 17 parents of children aged 3-6. Our findings suggest that even though AI-based storytelling technologies provide more immersive and engaging interaction, they still cannot meet parents' expectations due to a series of interactive, functional, and algorithmic challenges. We elaborate on these challenges and discuss the possible implications of future AI-based storytelling technologies for preschoolers. We conclude by highlighting the design implications for future AI-based storytelling technologies.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13812",
        "abstract url": "https://arxiv.org/abs/2401.13812",
        "title": "Optimal Queueing Regimes",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider an M/M/1 queueing model where customers can strategically decide whether to join the queue or balk and when to renege. We characterize the class of queueing regimes such that, for any parameters of the model, the socially efficient behavior is an equilibrium outcome.",
        "subjects": [
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13815",
        "abstract url": "https://arxiv.org/abs/2401.13815",
        "title": "SoK: Game-Theoretic Cybersecurity: Assumptions, Models, Gaps, and Bridges",
        "rating": -10,
        "keywords": [],
        "abstract": "The discipline of game theory was introduced in the context of economics, and has been applied to study cyber attacker and defender behaviors. While adaptions have been made to accommodate features in the cyber domain, these studies are inherently limited by the root of game theory in economic systems where players (i.e., agents) may be selfish but not malicious. In this SoK, we systematize the major cybersecurity problems that have been studied with the game-theoretic approach, the assumptions that have been made, the models and solution concepts that have been proposed. The systematization leads to a characterization of the technical gaps that must be addressed in order to make game-theoretic cybersecurity models truly useful. We explore bridges to address them.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "21 pages, Finished October 17th, 2023"
    },
    {
        "paper id": "2401.13819",
        "abstract url": "https://arxiv.org/abs/2401.13819",
        "title": "Separating $k$-Median from the Supplier Version",
        "rating": -10,
        "keywords": [],
        "abstract": "Given a metric space $(V, d)$ along with an integer $k$, the $k$-Median problem asks to open $k$ centers $C \\subseteq V$ to minimize $\\sum_{v \\in V} d(v, C)$, where $d(v, C) := \\min_{c \\in C} d(v, c)$. While the best-known approximation ratio of $2.613$ holds for the more general supplier version where an additional set $F \\subseteq V$ is given with the restriction $C \\subseteq F$, the best known hardness for these two versions are $1+1/e \\approx 1.36$ and $1+2/e \\approx 1.73$ respectively, using the same reduction from Max $k$-Coverage. We prove the following two results separating them. First, we show a $1.546$-parameterized approximation algorithm that runs in time $f(k) n^{O(1)}$. Since $1+2/e$ is proved to be the optimal approximation ratio for the supplier version in the parameterized setting, this result separates the original $k$-Median from the supplier version. Next, we prove a $1.416$-hardness for polynomial-time algorithms assuming the Unique Games Conjecture. This is achieved via a new fine-grained hardness of Max-$k$-Coverage for small set sizes. Our upper bound and lower bound are derived from almost the same expression, with the only difference coming from the well-known separation between the powers of LP and SDP on (hypergraph) vertex cover.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "20 pages; To appear at IPCO 2024"
    },
    {
        "paper id": "2401.13842",
        "abstract url": "https://arxiv.org/abs/2401.13842",
        "title": "Tight Competitive and Variance Analyses of Matching Policies in Gig Platforms",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we propose an online-matching-based model to tackle the two fundamental issues, matching and pricing, existing in a wide range of real-world gig platforms, including ride-hailing (matching riders and drivers), crowdsourcing markets (pairing workers and tasks), and online recommendations (offering items to customers). Our model assumes the arriving distributions of dynamic agents (e.g., riders, workers, and buyers) are accessible in advance, and they can change over time, which is referred to as \\emph{Known Heterogeneous Distributions} (KHD). In this paper, we initiate variance analysis for online matching algorithms under KHD. Unlike the popular competitive-ratio (CR) metric, the variance of online algorithms' performance is rarely studied due to inherent technical challenges, though it is well linked to robustness. We focus on two natural parameterized sampling policies, denoted by $\\mathsf{ATT}(\u03b3)$ and $\\mathsf{SAMP}(\u03b3)$, which appear as foundational bedrock in online algorithm design. We offer rigorous competitive ratio (CR) and variance analyses for both policies. Specifically, we show that $\\mathsf{ATT}(\u03b3)$ with $\u03b3\\in [0,1/2]$ achieves a CR of $\u03b3$ and a variance of $\u03b3\\cdot (1-\u03b3) \\cdot B$ on the total number of matches with $B$ being the total matching capacity. In contrast, $\\mathsf{SAMP}(\u03b3)$ with $\u03b3\\in [0,1]$ accomplishes a CR of $\u03b3(1-\u03b3)$ and a variance of $\\bar\u03b3 (1-\\bar\u03b3)\\cdot B$ with $\\bar\u03b3=\\min(\u03b3,1/2)$. All CR and variance analyses are tight and unconditional of any benchmark. As a byproduct, we prove that $\\mathsf{ATT}(\u03b3=1/2)$ achieves an optimal CR of $1/2$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "This paper was accepted to the 2024 ACM Web Conference"
    },
    {
        "paper id": "2401.13875",
        "abstract url": "https://arxiv.org/abs/2401.13875",
        "title": "Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?",
        "rating": -10,
        "keywords": [],
        "abstract": "Dense-to-sparse gating mixture of experts (MoE) has recently become an effective alternative to a well-known sparse MoE. Rather than fixing the number of activated experts as in the latter model, which could limit the investigation of potential experts, the former model utilizes the temperature to control the softmax weight distribution and the sparsity of the MoE during training in order to stabilize the expert specialization. Nevertheless, while there are previous attempts to theoretically comprehend the sparse MoE, a comprehensive analysis of the dense-to-sparse gating MoE has remained elusive. Therefore, we aim to explore the impacts of the dense-to-sparse gate on the maximum likelihood estimation under the Gaussian MoE in this paper. We demonstrate that due to interactions between the temperature and other model parameters via some partial differential equations, the convergence rates of parameter estimations are slower than any polynomial rates, and could be as slow as $\\mathcal{O}(1/\\log(n))$, where $n$ denotes the sample size. To address this issue, we propose using a novel activation dense-to-sparse gate, which routes the output of a linear layer to an activation function before delivering them to the softmax function. By imposing linearly independence conditions on the activation function and its derivatives, we show that the parameter estimation rates are significantly improved to polynomial rates.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "53 pages"
    },
    {
        "paper id": "2401.13882",
        "abstract url": "https://arxiv.org/abs/2401.13882",
        "title": "Robust Transmission Design for RIS-Assisted Integrated Sensing and Communication Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "As a critical technology for next-generation communication networks, integrated sensing and communication (ISAC) aims to achieve the harmonious coexistence of communication and sensing. The degrees-of-freedom (DoF) of ISAC is limited due to multiple performance metrics used for communication and sensing. Reconfigurable Intelligent Surfaces (RIS) composed of metamaterials can enhance the DoF in the spatial domain of ISAC systems. However, the availability of perfect Channel State Information (CSI) is a prerequisite for the gain brought by RIS, which is not realistic in practical environments. Therefore, under the imperfect CSI condition, we propose a decomposition-based large deviation inequality approach to eliminate the impact of CSI error on communication rate and sensing Cram\u00e9r-Rao bound (CRB). Then, an alternating optimization (AO) algorithm based on semi-definite relaxation (SDR) and gradient extrapolated majorization-maximization (GEMM) is proposed to solve the transmit beamforming and discrete RIS beamforming problems. We also analyze the complexity and convergence of the proposed algorithm. Simulation results show that the proposed algorithms can effectively eliminate the influence of CSI error and have good convergence performance. Notably, when CSI error exists, the gain brought by RIS will decrease with the increase of the number of RIS elements. Finally, we summarize and outline future research directions.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "This paper has been submitted to a IEEE journal. arXiv admin note: text overlap with arXiv:2303.01771"
    },
    {
        "paper id": "2401.13884",
        "abstract url": "https://arxiv.org/abs/2401.13884",
        "title": "Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation",
        "rating": -10,
        "keywords": [],
        "abstract": "Stochastic Approximation (SA) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (RL). Among RL algorithms, Q-learning is particularly popular due to its empirical success. In this paper, we study asynchronous Q-learning with constant stepsize, which is commonly used in practice for its fast convergence. By connecting the constant stepsize Q-learning to a time-homogeneous Markov chain, we show the distributional convergence of the iterates in Wasserstein distance and establish its exponential convergence rate. We also establish a Central Limit Theory for Q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. Moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. Specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. This precise characterization of the bias allows the application of Richardson-Romberg (RR) extrapolation technique to construct a new estimate that is provably closer to the optimal Q function. Numerical results corroborate our theoretical finding on the improvement of the RR extrapolation method.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "41 pages, 3 figures"
    },
    {
        "paper id": "2401.13897",
        "abstract url": "https://arxiv.org/abs/2401.13897",
        "title": "Computationally-Efficient Linear Periodically Time-Variant Digital PLL Modeling Using Conversion Matrices and Uncorrelated Upsampling",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper introduces a conversion matrix method for linear periodically time-variant (LPTV) digital phase-locked loop (DPLL) phase noise modeling that offers precise and computationally efficient results to enable rapid design iteration and optimization. Unlike many previous studies, which either assume linear time-invariance (LTI) and therefore overlook phase noise aliasing effects, or solve LPTV systems with noise folding and multiple sampling rate conversions that heightens modeling and computational complexity, the proposed conversion matrix method allows the designer to represent the LPTV systems using intuitive LTI-like transfer functions with excellent accuracy. Additionally, computational efficiency is improved through the uncorrelated upsampling method, which eliminates the need to consider beat frequency of noise sources with different sampling rates. The proposed algorithm is applied to modeling a DPLL with time-varying proportional loop gain, and the modeling accuracy is validated with Simulink transient simulations.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "10 pages, 16 figures"
    },
    {
        "paper id": "2401.13914",
        "abstract url": "https://arxiv.org/abs/2401.13914",
        "title": "Analog Beamforming for In-Band Full-Duplex Phased Arrays with Quantized Phase Shifters under a Per-Antenna Received Power Constraint",
        "rating": -10,
        "keywords": [],
        "abstract": "This letter develops a novel transmit beamforming (BF) design for canceling self-interference (SI) in analog in-band full-duplex phased arrays. Our design maximizes transmit BF gain in a desired direction while simultaneously reducing SI power to below a specified threshold on per-antenna basis to avoid saturating receive-chain components, such as LNAs. Core to our approach is that it accounts for real-world phase shifters used in analog phased array systems, whose limited resolution imposes non-convex constraints on BF design. We overcome this by transforming these non-convex constraints into convex polygon constraints, which we then solve through semidefinite relaxation and a rank refinement procedure. Numerical results show that our proposed BF scheme reliably cancels SI to the target power threshold at each receive antenna while sacrificing little in transmit BF gain, even with modest phase shifter resolution.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This paper has been submitted to the IEEE for review; copyright may change without notice"
    },
    {
        "paper id": "2401.13924",
        "abstract url": "https://arxiv.org/abs/2401.13924",
        "title": "ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "In recent years, large language models (LLMs), such as ChatGPT, have been pivotal in advancing various artificial intelligence applications, including natural language processing and software engineering. A promising yet underexplored area is utilizing LLMs in software testing, particularly in black-box testing. This paper explores the test cases devised by ChatGPT in comparison to those created by human participants. In this study, ChatGPT (GPT-4) and four participants each created black-box test cases for three applications based on specifications written by the authors. The goal was to evaluate the real-world applicability of the proposed test cases, identify potential shortcomings, and comprehend how ChatGPT could enhance human testing strategies. ChatGPT can generate test cases that generally match or slightly surpass those created by human participants in terms of test viewpoint coverage. Additionally, our experiments demonstrated that when ChatGPT cooperates with humans, it can cover considerably more test viewpoints than each can achieve alone, suggesting that collaboration between humans and ChatGPT may be more effective than human pairs working together. Nevertheless, we noticed that the test cases generated by ChatGPT have certain issues that require addressing before use.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.13926",
        "abstract url": "https://arxiv.org/abs/2401.13926",
        "title": "Iterative Methods in GPU-Resident Linear Solvers for Nonlinear Constrained Optimization",
        "rating": -10,
        "keywords": [],
        "abstract": "Linear solvers are major computational bottlenecks in a wide range of decision support and optimization computations. The challenges become even more pronounced on heterogeneous hardware, where traditional sparse numerical linear algebra methods are often inefficient. For example, methods for solving ill-conditioned linear systems have relied on conditional branching, which degrades performance on hardware accelerators such as graphical processing units (GPUs). To improve the efficiency of solving ill-conditioned systems, our computational strategy separates computations that are efficient on GPUs from those that need to run on traditional central processing units (CPUs). Our strategy maximizes the reuse of expensive CPU computations. Iterative methods, which thus far have not been broadly used for ill-conditioned linear systems, play an important role in our approach. In particular, we extend ideas from [1] to implement iterative refinement using inexact LU factors and flexible generalized minimal residual (FGMRES), with the aim of efficient performance on GPUs. We focus on solutions that are effective within broader application contexts, and discuss how early performance tests could be improved to be more predictive of the performance in a realistic environment",
        "subjects": [
            "cs.CE"
        ],
        "comment": "15 pages, 8 figures, 5 tables"
    },
    {
        "paper id": "2401.13931",
        "abstract url": "https://arxiv.org/abs/2401.13931",
        "title": "Precise Robotic Weed Spot-Spraying for Reduced Herbicide Usage and Improved Environmental Outcomes -- A Real-World Case Study",
        "rating": -10,
        "keywords": [],
        "abstract": "Precise robotic weed control plays an essential role in precision agriculture. It can help significantly reduce the environmental impact of herbicides while reducing weed management costs for farmers. In this paper, we demonstrate that a custom-designed robotic spot spraying tool based on computer vision and deep learning can significantly reduce herbicide usage on sugarcane farms. We present results from field trials that compare robotic spot spraying against industry-standard broadcast spraying, by measuring the weed control efficacy, the reduction in herbicide usage, and the water quality improvements in irrigation runoff. The average results across 25 hectares of field trials show that spot spraying on sugarcane farms is 97% as effective as broadcast spraying and reduces herbicide usage by 35%, proportionally to the weed density. For specific trial strips with lower weed pressure, spot spraying reduced herbicide usage by up to 65%. Water quality measurements of irrigation-induced runoff, three to six days after spraying, showed reductions in the mean concentration and mean load of herbicides of 39% and 54%, respectively, compared to broadcast spraying. These promising results reveal the capability of spot spraying technology to reduce herbicide usage on sugarcane farms without impacting weed control and potentially providing sustained water quality benefits.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "33 pages, 17 figures, 4 tables. Submitted to the Computers and Electronics in Agriculture journal"
    },
    {
        "paper id": "2401.13936",
        "abstract url": "https://arxiv.org/abs/2401.13936",
        "title": "Learning-based sensing and computing decision for data freshness in edge computing-enabled networks",
        "rating": -10,
        "keywords": [],
        "abstract": "As the demand on artificial intelligence (AI)-based applications increases, the freshness of sensed data becomes crucial in the wireless sensor networks. Since those applications require a large amount of computation for processing the sensed data, it is essential to offload the computation load to the edge computing (EC) server. In this paper, we propose the sensing and computing decision (SCD) algorithms for data freshness in the EC-enabled wireless sensor networks. We define the \u03b7-coverage probability to show the probability of maintaining fresh data for more than \u03b7 ratio of the network, where the spatial-temporal correlation of information is considered. We then propose the probability-based SCD for the single pre-charged sensor case with providing the optimal point after deriving the \u03b7-coverage probability. We also propose the reinforcement learning (RL)- based SCD by training the SCD policy of sensors for both the single pre-charged and multiple energy harvesting (EH) sensor cases, to make a real-time decision based on its observation. Our simulation results verify the performance of the proposed algorithms under various environment settings, and show that the RL-based SCD algorithm achieves higher performance compared to baseline algorithms for both the single pre-charged sensor and multiple EH sensor cases.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "15 pages"
    },
    {
        "paper id": "2401.13940",
        "abstract url": "https://arxiv.org/abs/2401.13940",
        "title": "How Are Paid and Volunteer Open Source Developers Different? A Study of the Rust Project",
        "rating": -10,
        "keywords": [],
        "abstract": "It is now commonplace for organizations to pay developers to work on specific open source software (OSS) projects to pursue their business goals. Such paid developers work alongside voluntary contributors, but given the different motivations of these two groups of developers, conflict may arise, which may pose a threat to a project's sustainability. This paper presents an empirical study of paid developers and volunteers in Rust, a popular open source programming language project. Rust is a particularly interesting case given considerable concerns about corporate participation. We compare volunteers and paid developers through contribution characteristics and long-term participation, and solicit volunteers' perceptions on paid developers. We find that core paid developers tend to contribute more frequently; commits contributed by one-time paid developers have bigger sizes; peripheral paid developers implement more features; and being paid plays a positive role in becoming a long-term contributor. We also find that volunteers do have some prejudices against paid developers. This study suggests that the dichotomous view of paid vs. volunteer developers is too simplistic and that further subgroups can be identified. Companies should become more sensitive to how they engage with OSS communities, in certain ways as suggested by this study.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14423",
        "abstract url": "https://arxiv.org/abs/2401.14423",
        "title": "Prompt Design and Engineering: Introduction and Advanced Methods",
        "rating": -10,
        "keywords": [],
        "abstract": "Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14427",
        "abstract url": "https://arxiv.org/abs/2401.14427",
        "title": "Beimingwu: A Learnware Dock System",
        "rating": -10,
        "keywords": [],
        "abstract": "The learnware paradigm proposed by Zhou [2016] aims to enable users to reuse numerous existing well-trained models instead of building machine learning models from scratch, with the hope of solving new user tasks even beyond models' original purposes. In this paradigm, developers worldwide can submit their high-performing models spontaneously to the learnware dock system (formerly known as learnware market) without revealing their training data. Once the dock system accepts the model, it assigns a specification and accommodates the model. This specification allows the model to be adequately identified and assembled to reuse according to future users' needs, even if they have no prior knowledge of the model. This paradigm greatly differs from the current big model direction and it is expected that a learnware dock system housing millions or more high-performing models could offer excellent capabilities for both planned tasks where big models are applicable; and unplanned, specialized, data-sensitive scenarios where big models are not present or applicable. This paper describes Beimingwu, the first open-source learnware dock system providing foundational support for future research of learnware paradigm.The system significantly streamlines the model development for new user tasks, thanks to its integrated architecture and engine design, extensive engineering implementations and optimizations, and the integration of various algorithms for learnware identification and reuse. Notably, this is possible even for users with limited data and minimal expertise in machine learning, without compromising the raw data's security. Beimingwu supports the entire process of learnware paradigm. The system lays the foundation for future research in learnware-related algorithms and systems, and prepares the ground for hosting a vast array of learnwares and establishing a learnware ecosystem.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14428",
        "abstract url": "https://arxiv.org/abs/2401.14428",
        "title": "The Landscape of Compute-near-memory and Compute-in-memory: A Research and Commercial Overview",
        "rating": -10,
        "keywords": [],
        "abstract": "In today's data-centric world, where data fuels numerous application domains, with machine learning at the forefront, handling the enormous volume of data efficiently in terms of time and energy presents a formidable challenge. Conventional computing systems and accelerators are continually being pushed to their limits to stay competitive. In this context, computing near-memory (CNM) and computing-in-memory (CIM) have emerged as potentially game-changing paradigms. This survey introduces the basics of CNM and CIM architectures, including their underlying technologies and working principles. We focus particularly on CIM and CNM architectures that have either been prototyped or commercialized. While surveying the evolving CIM and CNM landscape in academia and industry, we discuss the potential benefits in terms of performance, energy, and cost, along with the challenges associated with these cutting-edge computing paradigms.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14432",
        "abstract url": "https://arxiv.org/abs/2401.14432",
        "title": "A2C: A Modular Multi-stage Collaborative Decision Framework for Human-AI Teams",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper introduces A2C, a multi-stage collaborative decision framework designed to enable robust decision-making within human-AI teams. Drawing inspiration from concepts such as rejection learning and learning to defer, A2C incorporates AI systems trained to recognise uncertainty in their decisions and defer to human experts when needed. Moreover, A2C caters to scenarios where even human experts encounter limitations, such as in incident detection and response in cyber Security Operations Centres (SOC). In such scenarios, A2C facilitates collaborative explorations, enabling collective resolution of complex challenges. With support for three distinct decision-making modes in human-AI teams: Automated, Augmented, and Collaborative, A2C offers a flexible platform for developing effective strategies for human-AI collaboration. By harnessing the strengths of both humans and AI, it significantly improves the efficiency and effectiveness of complex decision-making in dynamic and evolving environments. To validate A2C's capabilities, we conducted extensive simulative experiments using benchmark datasets. The results clearly demonstrate that all three modes of decision-making can be effectively supported by A2C. Most notably, collaborative exploration by (simulated) human experts and AI achieves superior performance compared to AI in isolation, underscoring the framework's potential to enhance decision-making within human-AI teams.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15096",
        "abstract url": "https://arxiv.org/abs/2401.15096",
        "title": "Jet space extensions of infinite-dimensional Hamiltonian systems",
        "rating": -10,
        "keywords": [],
        "abstract": "We analyze infinite-dimensional Hamiltonian systems corresponding to partial differential equations on one-dimensional spatial domains formulated with formally skew-adjoint Hamiltonian operators and nonlinear Hamiltonian density. In various applications, the Hamiltonian density can depend on spatial derivatives of the state such that these systems can not straightforwardly be formulated as boundary port-Hamiltonian system using a Stokes-Dirac structure. In this work, we show that any Hamiltonian system of the above class can be reformulated as a Hamiltonian system on the jet space, in which the Hamiltonian density only depends on the extended state variable itself and not on its derivatives. Consequently, well-known geometric formulations with Stokes- Dirac structures are applicable. Additionally, we provide a similar result for dissipative systems. We illustrate the developed theory by means of the the Boussinesq equation, the dynamics of an elastic rod and the Allen-Cahn equation.",
        "subjects": [
            "math.AP"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2402.00049",
        "abstract url": "https://arxiv.org/abs/2402.00049",
        "title": "Hybrid Dynamical Model for Reluctance Actuators Including Saturation, Hysteresis and Eddy Currents",
        "rating": -10,
        "keywords": [],
        "abstract": "A novel hybrid dynamical model for single-coil, short-stroke reluctance actuators is presented in this paper. The model, which is partially based on the principles of magnetic equivalent circuits, includes the magnetic phenomena of hysteresis and saturation by means of the generalized Preisach model. In addition, the eddy currents induced in the iron core are also considered, and the flux fringing effect in the air is incorporated by using results from finite element simulations. An explicit solution of the dynamics without need of inverting the Preisach model is derived, and the hybrid automaton that results from combining the electromagnetic and motion equations is presented and discussed. Finally, an identification method to determine the model parameters is proposed and experimentally illustrated on a real actuator. The results are presented and the advantages of our modeling method are emphasized.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "11 pages, 13 figures. This is the accepted version of an already published paper (see Journal reference)"
    },
    {
        "paper id": "2402.00050",
        "abstract url": "https://arxiv.org/abs/2402.00050",
        "title": "Real-Time Electromagnetic Estimation for Reluctance Actuators",
        "rating": -10,
        "keywords": [],
        "abstract": "Several modeling, estimation, and control strategies have been recently presented for simple reluctance devices like solenoid valves and electromagnetic switches. In this paper, we present a new algorithm to online estimate the flux linkage and the electrical time-variant parameters of these devices, namely the resistance and the inductance, only by making use of discrete-time measurements of voltage and current. The algorithm, which is robust against measurement noise, is able to deal with temperature variations of the device and provides accurate estimations during the motion of the armature. Additionally, an integral {estimator} that uses the start of each operation of the actuator as reset condition has been also implemented for comparative purposes. The performances of both estimation methods are studied and compared by means of simulations and experimental tests, and the benefits of our proposal are emphasized. Possible uses of the estimates and further modeling developments are also described and discussed.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "10 pages, 7 figures This is the accepted version of an already published paper (see Journal reference)"
    },
    {
        "paper id": "2402.00051",
        "abstract url": "https://arxiv.org/abs/2402.00051",
        "title": "Design and Implementation of Hardware Accelerators for Neural Processing Applications",
        "rating": -10,
        "keywords": [],
        "abstract": "Primary motivation for this work was the need to implement hardware accelerators for a newly proposed ANN structure called Auto Resonance Network (ARN) for robotic motion planning. ARN is an approximating feed-forward hierarchical and explainable network. It can be used in various AI applications but the application base was small. Therefore, the objective of the research was twofold: to develop a new application using ARN and to implement a hardware accelerator for ARN. As per the suggestions given by the Doctoral Committee, an image recognition system using ARN has been implemented. An accuracy of around 94% was achieved with only 2 layers of ARN. The network also required a small training data set of about 500 images. Publicly available MNIST dataset was used for this experiment. All the coding was done in Python. Massive parallelism seen in ANNs presents several challenges to CPU design. For a given functionality, e.g., multiplication, several copies of serial modules can be realized within the same area as a parallel module. Advantage of using serial modules compared to parallel modules under area constraints has been discussed. One of the module often useful in ANNs is a multi-operand addition. One problem in its implementation is that the estimation of carry bits when the number of operands changes. A theorem to calculate exact number of carry bits required for a multi-operand addition has been presented in the thesis which alleviates this problem. The main advantage of the modular approach to multi-operand addition is the possibility of pipelined addition with low reconfiguration overhead. This results in overall increase in throughput for large number of additions, typically seen in several DNN configurations.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01353",
        "abstract url": "https://arxiv.org/abs/2402.01353",
        "title": "Efficient compilation of expressive problem space specifications to neural network solvers",
        "rating": -10,
        "keywords": [],
        "abstract": "Recent work has described the presence of the embedding gap in neural network verification. On one side of the gap is a high-level specification about the network's behaviour, written by a domain expert in terms of the interpretable problem space. On the other side are a logically-equivalent set of satisfiability queries, expressed in the uninterpretable embedding space in a form suitable for neural network solvers. In this paper we describe an algorithm for compiling the former to the latter. We explore and overcome complications that arise from targeting neural network solvers as opposed to standard SMT solvers.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01699",
        "abstract url": "https://arxiv.org/abs/2402.01699",
        "title": "Intergenerational Preferences and Continuity: Reconciling Order and Topology",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper we focus our efforts on studying how a preorder and topology can be made compatible. Thus we provide a characterization of those that are continuous-compatible. Such a characterization states that such topologies must be finer than the so-called upper topology induced by the preorder and, thus, it clarifies which topology is the smallest one among those that make the preorder continuous. Moreover, we provide sufficient conditions that allows us to discard in an easy way the continuity of a preference. In the light of the obtained results, we provide possibility counterparts of the a few celebrate impossibility theorems for continuous social social intergenerational preferences due to P. Diamond, L.G. Svensson and T. Sakai. Furthermore, we suggest quasi-pseudo-metrics as appropriate quantitative tool for reconciling topology and social intergenerational preferences. Thus, we develop a metric type method which is able to guarantee possibility counterparts of the aforesaid impossibility theorems and, in addition, it is able to give numerical quantifications of the improvement of welfare. We also show that our method makes always the intergenerational preferences semi-continuous multi-utility representables in the sense of \u00d6zg\u00fc Evern and Efe O. Ok. Finally, in order to keep close to the classical way of measuring in the literature, a refinement of the previous method is presented in such a way that metrics are involved.",
        "subjects": [
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2402.03345",
        "abstract url": "https://arxiv.org/abs/2402.03345",
        "title": "Weakly supervised covariance matrices alignment through Stiefel matrices estimation for MEG applications",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper introduces a novel domain adaptation technique for time series data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the challenge of limited labeled signals in the target dataset. Leveraging a domain-dependent mixing model and the optimal transport domain adaptation assumption, we exploit abundant unlabeled data in the target domain to ensure effective prediction by establishing pairwise correspondence with equivalent signal variances between domains. Theoretical foundations are laid for identifying crucial Stiefel matrices, essential for recovering underlying signal variances from a Riemannian representation of observed signal covariances. We propose an integrated cost function that simultaneously learns these matrices, pairwise domain relationships, and a predictor, classifier, or regressor, depending on the task. Applied to neuroscience problems, MSA outperforms recent methods in brain-age regression with task variations using magnetoencephalography (MEG) signals from the Cam-CAN dataset.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    }
]