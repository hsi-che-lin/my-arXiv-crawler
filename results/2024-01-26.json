[
    {
        "paper id": "2401.14664",
        "abstract url": "https://arxiv.org/abs/2401.14664",
        "title": "UNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit Normalization",
        "rating": 1.5,
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "GAN"
            ],
            [
                "cs.SD"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Dysarthric speech reconstruction (DSR) systems aim to automatically convert dysarthric speech into normal-sounding speech. The technology eases communication with speakers affected by the neuromotor disorder and enhances their social inclusion. NED-based (Neural Encoder-Decoder) systems have significantly improved the intelligibility of the reconstructed speech as compared with GAN-based (Generative Adversarial Network) approaches, but the approach is still limited by training inefficiency caused by the cascaded pipeline and auxiliary tasks of the content encoder, which may in turn affect the quality of reconstruction. Inspired by self-supervised speech representation learning and discrete speech units, we propose a Unit-DSR system, which harnesses the powerful domain-adaptation capacity of HuBERT for training efficiency improvement and utilizes speech units to constrain the dysarthric content restoration in a discrete linguistic space. Compared with NED approaches, the Unit-DSR system only consists of a speech unit normalizer and a Unit HiFi-GAN vocoder, which is considerably simpler without cascaded sub-modules or auxiliary tasks. Results on the UASpeech corpus indicate that Unit-DSR outperforms competitive baselines in terms of content restoration, reaching a 28.2% relative average word error rate reduction when compared to original dysarthric speech, and shows robustness against speed perturbation and noise.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Accepted to ICASSP 2024"
    },
    {
        "paper id": "2401.14717",
        "abstract url": "https://arxiv.org/abs/2401.14717",
        "title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To appear in IEEE ICASSP 2024"
    },
    {
        "paper id": "2401.15207",
        "abstract url": "https://arxiv.org/abs/2401.15207",
        "title": "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy",
        "rating": 1.5,
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including AdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\\% GPU memory compared with standard full-parameter fine-tuning for 7B model. (4) HiFT enables full-parameter fine-tuning of a 7B model on single 48G A6000 with a precision of 32 using the AdamW optimizer, without using any memory saving techniques.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "under progress"
    },
    {
        "paper id": "2401.16438",
        "abstract url": "https://arxiv.org/abs/2401.16438",
        "title": "Do deep neural networks utilize the weight space efficiently?",
        "rating": 1.5,
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning models like Transformers and Convolutional Neural Networks (CNNs) have revolutionized various domains, but their parameter-intensive nature hampers deployment in resource-constrained settings. In this paper, we introduce a novel concept utilizes column space and row space of weight matrices, which allows for a substantial reduction in model parameters without compromising performance. Leveraging this paradigm, we achieve parameter-efficient deep learning models.. Our approach applies to both Bottleneck and Attention layers, effectively halving the parameters while incurring only minor performance degradation. Extensive experiments conducted on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of our method, showcasing competitive performance when compared to traditional models. This approach not only addresses the pressing demand for parameter efficient deep learning solutions but also holds great promise for practical deployment in real-world scenarios.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14654",
        "abstract url": "https://arxiv.org/abs/2401.14654",
        "title": "A Korean Legal Judgment Prediction Dataset for Insurance Disputes",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces a Korean legal judgment prediction (LJP) dataset for insurance disputes. Successful LJP models on insurance disputes can benefit insurance companies and their customers. It can save both sides' time and money by allowing them to predict how the result would come out if they proceed to the dispute mediation process. As is often the case with low-resource languages, there is a limitation on the amount of data available for this specific task. To mitigate this issue, we investigate how one can achieve a good performance despite the limitation in data. In our experiment, we demonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al., 2022) is a good alternative to standard fine-tuning when training data are limited. The models fine-tuned with the SetFit approach on our data show similar performance to the Korean LJP benchmark models (Hwang et al., 2022) despite the much smaller data size.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "5 pages, 1 figure"
    },
    {
        "paper id": "2401.14680",
        "abstract url": "https://arxiv.org/abs/2401.14680",
        "title": "MaLLaM -- Malaysia Large Language Model",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Addressing the gap in Large Language Model pretrained from scratch with Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion parameters on a substantial 349GB dataset, equivalent to 90 billion tokens based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch. MaLLaM contributes to enhanced natural language understanding and generation tasks in the Malay language. Although trained on a smaller dataset of 90 billion tokens, our instruction-tuned MaLLaM models perform competitively. When compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models demonstrate notable proficiency, underscoring the effectiveness of our approach in capturing and understanding the nuances of the Malaysian language. MaLLaM models mark a significant contribution to the field, providing comprehensive language representations grounded in Malaysian context. This endeavor aims to pave the way for enhanced natural language understanding and generation tasks specific to the linguistic nuances present in Malaysia. We discuss the training methodology, dataset composition, and the potential impact of MaLLaM in advancing the capabilities of large language models within the context of the Malay language. All models released at https://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14681",
        "abstract url": "https://arxiv.org/abs/2401.14681",
        "title": "MasonTigers@LT-EDI-2024: An Ensemble Approach Towards Detecting Homophobia and Transphobia in Social Media Comments",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we describe our approaches and results for Task 2 of the LT-EDI 2024 Workshop, aimed at detecting homophobia and/or transphobia across ten languages. Our methodologies include monolingual transformers and ensemble methods, capitalizing on the strengths of each to enhance the performance of the models. The ensemble models worked well, placing our team, MasonTigers, in the top five for eight of the ten languages, as measured by the macro F1 score. Our work emphasizes the efficacy of ensemble methods in multilingual scenarios, addressing the complexities of language-specific tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14686",
        "abstract url": "https://arxiv.org/abs/2401.14686",
        "title": "SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduced SSR, which utilizes SAM (segment-anything) as a strong regularizer during training, to greatly enhance the robustness of the image encoder for handling various domains. Specifically, given the fact that SAM is pre-trained with a large number of images over the internet, which cover a diverse variety of domains, the feature encoding extracted by the SAM is obviously less dependent on specific domains when compared to the traditional ImageNet pre-trained image encoder. Meanwhile, the ImageNet pre-trained image encoder is still a mature choice of backbone for the semantic segmentation task, especially when the SAM is category-irrelevant. As a result, our SSR provides a simple yet highly effective design. It uses the ImageNet pre-trained image encoder as the backbone, and the intermediate feature of each stage (ie there are 4 stages in MiT-B5) is regularized by SAM during training. After extensive experimentation on GTA5$\\rightarrow$Cityscapes, our SSR significantly improved performance over the baseline without introducing any extra inference overhead.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14688",
        "abstract url": "https://arxiv.org/abs/2401.14688",
        "title": "Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support",
        "rating": 1,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilingual image generation capabilities of Taiyi-Diffusion-XL surpass previous models. This research leads to the development and open-sourcing of the Taiyi-Diffusion-XL model, representing a notable advancement in the field of image generation, particularly for Chinese language applications. This contribution is a step forward in addressing the need for more diverse language support in multimodal research. The model and demonstration are made publicly available at \\href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this https URL}, fostering further research and collaboration in this domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Taiyi-Diffusion-XL Tech Report"
    },
    {
        "paper id": "2401.14698",
        "abstract url": "https://arxiv.org/abs/2401.14698",
        "title": "Under the Surface: Tracking the Artifactuality of LLM-Generated Data",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This work delves into the expanding role of large language models (LLMs) in generating artificial data. LLMs are increasingly employed to create a variety of outputs, including annotations, preferences, instruction prompts, simulated dialogues, and free text. As these forms of LLM-generated data often intersect in their application, they exert mutual influence on each other and raise significant concerns about the quality and diversity of the artificial data incorporated into training cycles, leading to an artificial data ecosystem. To the best of our knowledge, this is the first study to aggregate various types of LLM-generated text data, from more tightly constrained data like \"task labels\" to more lightly constrained \"free-form text\". We then stress test the quality and implications of LLM-generated artificial data, comparing it with human data across various existing benchmarks. Despite artificial data's capability to match human performance, this paper reveals significant hidden disparities, especially in complex tasks where LLMs often miss the nuanced understanding of intrinsic human-generated content. This study critically examines diverse LLM-generated data and emphasizes the need for ethical practices in data creation and when using LLMs. It highlights the LLMs' shortcomings in replicating human traits and behaviors, underscoring the importance of addressing biases and artifacts produced in LLM-generated content for future research and development. All data and code are available on our project page.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Core Authors: Debarati Das, Karin De Langis, Anna Martin-Boyle, Jaehyung Kim, Minhwa Lee and Zae Myung Kim | Project lead : Debarati Das | PI : Dongyeop Kang"
    },
    {
        "paper id": "2401.14707",
        "abstract url": "https://arxiv.org/abs/2401.14707",
        "title": "Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical evaluations on three benchmark datasets demonstrate that our approach surpasses existing adversarial fine-tuning methods and adversarial training baselines.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures"
    },
    {
        "paper id": "2401.14718",
        "abstract url": "https://arxiv.org/abs/2401.14718",
        "title": "A Survey on Video Prediction: From Deterministic to Generative Approaches",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video prediction, a fundamental task in computer vision, aims to enable models to generate sequences of future frames based on existing video content. This task has garnered widespread application across various domains. In this paper, we comprehensively survey both historical and contemporary works in this field, encompassing the most widely used datasets and algorithms. Our survey scrutinizes the challenges and evolving landscape of video prediction within the realm of computer vision. We propose a novel taxonomy centered on the stochastic nature of video prediction algorithms. This taxonomy accentuates the gradual transition from deterministic to generative prediction methodologies, underlining significant advancements and shifts in approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "under review"
    },
    {
        "paper id": "2401.14729",
        "abstract url": "https://arxiv.org/abs/2401.14729",
        "title": "Sketch and Refine: Towards Fast and Accurate Lane Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Lane detection is to determine the precise location and shape of lanes on the road. Despite efforts made by current methods, it remains a challenging task due to the complexity of real-world scenarios. Existing approaches, whether proposal-based or keypoint-based, suffer from depicting lanes effectively and efficiently. Proposal-based methods detect lanes by distinguishing and regressing a collection of proposals in a streamlined top-down way, yet lack sufficient flexibility in lane representation. Keypoint-based methods, on the other hand, construct lanes flexibly from local descriptors, which typically entail complicated post-processing. In this paper, we present a \"Sketch-and-Refine\" paradigm that utilizes the merits of both keypoint-based and proposal-based methods. The motivation is that local directions of lanes are semantically simple and clear. At the \"Sketch\" stage, local directions of keypoints can be easily estimated by fast convolutional layers. Then we can build a set of lane proposals accordingly with moderate accuracy. At the \"Refine\" stage, we further optimize these proposals via a novel Lane Segment Association Module (LSAM), which allows adaptive lane segment adjustment. Last but not least, we propose multi-level feature integration to enrich lane feature representations more efficiently. Based on the proposed \"Sketch and Refine\" paradigm, we propose a fast yet effective lane detector dubbed \"SRLane\". Experiments show that our SRLane can run at a fast speed (i.e., 278 FPS) while yielding an F1 score of 78.9\\%. The source code is available at: https://github.com/passerer/SRLane.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14733",
        "abstract url": "https://arxiv.org/abs/2401.14733",
        "title": "Personality Perception in Human Videos Altered by Motion Transfer Networks",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The successful portrayal of personality in digital characters improves communication and immersion. Current research focuses on expressing personality through modifying animations using heuristic rules or data-driven models. While studies suggest motion style highly influences the apparent personality, the role of appearance can be similarly essential. This work analyzes the influence of movement and appearance on the perceived personality of short videos altered by motion transfer networks. We label the personalities in conference video clips with a user study to determine the samples that best represent the Five-Factor model's high, neutral, and low traits. We alter these videos using the Thin-Plate Spline Motion Model, utilizing the selected samples as the source and driving inputs. We follow five different cases to study the influence of motion and appearance on personality perception. Our comparative study reveals that motion and appearance influence different factors: motion strongly affects perceived extraversion, and appearance helps convey agreeableness and neuroticism.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14758",
        "abstract url": "https://arxiv.org/abs/2401.14758",
        "title": "Off-Policy Primal-Dual Safe Reinforcement Learning",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose conservative policy optimization, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce local policy convexification to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and further verify them by extensive experiments. Results on benchmark tasks show that our method not only achieves an asymptotic performance comparable to state-of-the-art on-policy methods while using much fewer samples, but also significantly reduces constraint violation during training. Our code is available at https://github.com/ZifanWu/CAL.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICLR 2024 Poster"
    },
    {
        "paper id": "2401.14772",
        "abstract url": "https://arxiv.org/abs/2401.14772",
        "title": "Spatial Transcriptomics Analysis of Zero-shot Gene Expression Prediction",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Spatial transcriptomics (ST) captures gene expression within distinct regions (i.e., windows) of a tissue slide. Traditional supervised learning frameworks applied to model ST are constrained to predicting expression from slide image windows for gene types seen during training, failing to generalize to unseen gene types. To overcome this limitation, we propose a semantic guided network (SGN), a pioneering zero-shot framework for predicting gene expression from slide image windows. Considering a gene type can be described by functionality and phenotype, we dynamically embed a gene type to a vector per its functionality and phenotype, and employ this vector to project slide image windows to gene expression in feature space, unleashing zero-shot expression prediction for unseen gene types. The gene type functionality and phenotype are queried with a carefully designed prompt from a pre-trained large language model (LLM). On standard benchmark datasets, we demonstrate competitive zero-shot performance compared to past state-of-the-art supervised learning approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14777",
        "abstract url": "https://arxiv.org/abs/2401.14777",
        "title": "Large Language Model Adaptation for Financial Sentiment Analysis",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14807",
        "abstract url": "https://arxiv.org/abs/2401.14807",
        "title": "PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Few-Shot Class-Incremental Learning (FSCIL) aims to enable deep neural networks to learn new tasks incrementally from a small number of labeled samples without forgetting previously learned tasks, closely mimicking human learning patterns. In this paper, we propose a novel approach called Prompt Learning for FSCIL (PL-FSCIL), which harnesses the power of prompts in conjunction with a pre-trained Vision Transformer (ViT) model to address the challenges of FSCIL effectively. Our work pioneers the use of visual prompts in FSCIL, which is characterized by its notable simplicity. PL-FSCIL consists of two distinct prompts: the Domain Prompt and the FSCIL Prompt. Both are vectors that augment the model by embedding themselves into the attention layer of the ViT model. Specifically, the Domain Prompt assists the ViT model in adapting to new data domains. The task-specific FSCIL Prompt, coupled with a prototype classifier, amplifies the model's ability to effectively handle FSCIL tasks. We validate the efficacy of PL-FSCIL on widely used benchmark datasets such as CIFAR-100 and CUB-200. The results showcase competitive performance, underscoring its promising potential for real-world applications where high-quality data is often scarce. The source code is available at: https://github.com/TianSongS/PL-FSCIL.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14846",
        "abstract url": "https://arxiv.org/abs/2401.14846",
        "title": "Understanding Domain Generalization: A Noise Robustness Perspective",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to the 12th International Conference on Learning Representations (ICLR 2024). Code is available at https://github.com/qiaoruiyt/NoiseRobustDG"
    },
    {
        "paper id": "2401.14856",
        "abstract url": "https://arxiv.org/abs/2401.14856",
        "title": "Memory-Inspired Temporal Prompt Interaction for Text-Image Classification",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, large-scale pre-trained multimodal models (LMM) generally emerge to integrate the vision and language modalities, achieving considerable success in various natural language processing and computer vision tasks. The growing size of LMMs, however, results in a significant computational cost for fine-tuning these models for downstream tasks. Hence, prompt-based interaction strategy is studied to align modalities more efficiently. In this contex, we propose a novel prompt-based multimodal interaction strategy inspired by human memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our proposed method involves in two stages as in human memory strategy: the acquiring stage, and the consolidation and activation stage. We utilize temporal prompts on intermediate layers to imitate the acquiring stage, leverage similarity-based prompt interaction to imitate memory consolidation, and employ prompt generation strategy to imitate memory activation. The main strength of our paper is that we interact the prompt vectors on intermediate layers to leverage sufficient information exchange between modalities, with compressed trainable parameters and memory usage. We achieve competitive results on several datasets with relatively small memory usage and 2.0M of trainable parameters (about 1% of the pre-trained foundation model).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14869",
        "abstract url": "https://arxiv.org/abs/2401.14869",
        "title": "F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs' fundamental abilities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "21 pages, 7 figures"
    },
    {
        "paper id": "2401.14890",
        "abstract url": "https://arxiv.org/abs/2401.14890",
        "title": "Comparison of parameters of vowel sounds of russian and english languages",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "In multilingual speech recognition systems, a situation can often arise when the language is not known in advance, but the signal has already been received and is being processed. For such cases, some generalized model is needed that will be able to respond to phonetic differences and, depending on them, correctly recog-nize speech in the desired language. To build such a model, it is necessary to set the values of phonetic parameters, and then compare similar sounds, establishing significant differences.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "7 pages, 1 figures, 3 tables"
    },
    {
        "paper id": "2401.14895",
        "abstract url": "https://arxiv.org/abs/2401.14895",
        "title": "MPTQ-ViT: Mixed-Precision Post-Training Quantization for Vision Transformer",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "While vision transformers (ViTs) have shown great potential in computer vision tasks, their intense computation and memory requirements pose challenges for practical applications. Existing post-training quantization methods leverage value redistribution or specialized quantizers to address the non-normal distribution in ViTs. However, without considering the asymmetry in activations and relying on hand-crafted settings, these methods often struggle to maintain performance under low-bit quantization. To overcome these challenges, we introduce SmoothQuant with bias term (SQ-b) to alleviate the asymmetry issue and reduce the clamping loss. We also introduce optimal scaling factor ratio search (OPT-m) to determine quantization parameters by a data-dependent mechanism automatically. To further enhance the compressibility, we incorporate the above-mentioned techniques and propose a mixed-precision post-training quantization framework for vision transformers (MPTQ-ViT). We develop greedy mixed-precision quantization (Greedy MP) to allocate layer-wise bit-width considering both model performance and compressibility. Our experiments on ViT, DeiT, and Swin demonstrate significant accuracy improvements compared with SOTA on the ImageNet dataset. Specifically, our proposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on 4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fully quantized ViTs with mixed-precision.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14931",
        "abstract url": "https://arxiv.org/abs/2401.14931",
        "title": "Do LLMs Dream of Ontologies?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have recently revolutionized automated text understanding and generation. The performance of these models relies on the high number of parameters of the underlying neural architectures, which allows LLMs to memorize part of the vast quantity of data seen during the training. This paper investigates whether and to what extent general-purpose pre-trained LLMs have memorized information from known ontologies. Our results show that LLMs partially know ontologies: they can, and do indeed, memorize concepts from ontologies mentioned in the text, but the level of memorization of their concepts seems to vary proportionally to their popularity on the Web, the primary source of their training material. We additionally propose new metrics to estimate the degree of memorization of ontological information in LLMs by measuring the consistency of the output produced across different prompt repetitions, query languages, and degrees of determinism.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14966",
        "abstract url": "https://arxiv.org/abs/2401.14966",
        "title": "Masked Pre-trained Model Enables Universal Zero-shot Denoiser",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we observe that the model, which is trained on vast general images using masking strategy, has been naturally embedded with the distribution knowledge regarding natural images, and thus spontaneously attains the underlying potential for strong image denoising. Based on this observation, we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative fill (MPI). MPI pre-trains a model with masking and fine-tunes it for denoising of a single image with unseen noise degradation. Concretely, the proposed MPI comprises two key procedures: 1) Masked Pre-training involves training a model on multiple natural images with random masks to gather generalizable representations, allowing for practical applications in varying noise degradation and even in distinct image types. 2) Iterative filling is devised to efficiently fuse pre-trained knowledge for denoising. Similar to but distinct from pre-training, random masking is retained to bridge the gap, but only the predicted parts covered by masks are assembled for efficiency, which enables high-quality denoising within a limited number of iterations. Comprehensive experiments across various noisy scenarios underscore the notable advances of proposed MPI over previous approaches with a marked reduction in inference time. Code is available at https://github.com/krennic999/MPI.git.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "11 pages, 9 figures"
    },
    {
        "paper id": "2401.15006",
        "abstract url": "https://arxiv.org/abs/2401.15006",
        "title": "Airavata: Introducing Hindi Instruction-tuned LLM",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We announce the initial release of \"Airavata,\" an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2401.15030",
        "abstract url": "https://arxiv.org/abs/2401.15030",
        "title": "On the generalization capacity of neural networks during generic multimodal reasoning",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures). We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, either cross-modal attention or models with deeper attention layers are key architectural features required to integrate multimodal inputs. On the other hand, neither of these architectural features led to productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide Generic COG (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICLR 2024"
    },
    {
        "paper id": "2401.15055",
        "abstract url": "https://arxiv.org/abs/2401.15055",
        "title": "Deep learning-based approach for tomato classification in complex scenes",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Tracking ripening tomatoes is time consuming and labor intensive. Artificial intelligence technologies combined with those of computer vision can help users optimize the process of monitoring the ripening status of plants. To this end, we have proposed a tomato ripening monitoring approach based on deep learning in complex scenes. The objective is to detect mature tomatoes and harvest them in a timely manner. The proposed approach is declined in two parts. Firstly, the images of the scene are transmitted to the pre-processing layer. This process allows the detection of areas of interest (area of the image containing tomatoes). Then, these images are used as input to the maturity detection layer. This layer, based on a deep neural network learning algorithm, classifies the tomato thumbnails provided to it in one of the following five categories: green, brittle, pink, pale red, mature red. The experiments are based on images collected from the internet gathered through searches using tomato state across diverse languages including English, German, French, and Spanish. The experimental results of the maturity detection layer on a dataset composed of images of tomatoes taken under the extreme conditions, gave a good classification rate.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15068",
        "abstract url": "https://arxiv.org/abs/2401.15068",
        "title": "Pairing Orthographically Variant Literary Words to Standard Equivalents Using Neural Edit Distance Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present a novel corpus consisting of orthographically variant words found in works of 19th century U.S. literature annotated with their corresponding \"standard\" word pair. We train a set of neural edit distance models to pair these variants with their standard forms, and compare the performance of these models to the performance of a set of neural edit distance models trained on a corpus of orthographic errors made by L2 English learners. Finally, we analyze the relative performance of these models in the light of different negative training sample generation strategies, and offer concluding remarks on the unique challenge literary orthographic variation poses to string pairing methodologies.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LaTeCH@EACL2024"
    },
    {
        "paper id": "2401.15071",
        "abstract url": "https://arxiv.org/abs/2401.15071",
        "title": "From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-modal Large Language Models (MLLMs) have shown impressive abilities in generating reasonable responses with respect to multi-modal contents. However, there is still a wide gap between the performance of recent MLLM-based applications and the expectation of the broad public, even though the most powerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper strives to enhance understanding of the gap through the lens of a qualitative study on the generalizability, trustworthiness, and causal reasoning capabilities of recent proprietary and open-source MLLMs across four modalities: ie, text, code, image, and video, ultimately aiming to improve the transparency of MLLMs. We believe these properties are several representative factors that define the reliability of MLLMs, in supporting various downstream applications. To be specific, we evaluate the closed-source GPT-4 and Gemini and 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed cases, where the qualitative results are then summarized into 12 scores (ie, 4 modalities times 3 properties). In total, we uncover 14 empirical findings that are useful to understand the capabilities and limitations of both proprietary and open-source MLLMs, towards more reliable downstream multi-modal applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15164",
        "abstract url": "https://arxiv.org/abs/2401.15164",
        "title": "AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its temporal evolution are not only influenced by the individual but also by external contexts like audience reaction and context of the ongoing conversation. To meet this challenge, we propose a Multimodal Attention Network that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN injects cross-modal attention via its Peripheral key-value pairs within each layer of a mode-specific Central query network. The resulting cross-attended mode-specific descriptors are then combined using an Adaptive Fusion technique that enables the model to integrate the discriminative and complementary mode-specific data patterns within an instance-specific multimodal descriptor. Given a dialogue represented by a sequence of utterances, the proposed AMuSE model condenses both spatial and temporal features into two dense descriptors: speaker-level and utterance-level. This helps not only in delivering better classification performance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy) in large-scale public datasets but also helps the users in understanding the reasoning behind each emotion prediction made by the model via its Multimodal Explainability Visualization module.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15170",
        "abstract url": "https://arxiv.org/abs/2401.15170",
        "title": "Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\u03ba\\geq 0.79$) for 3 of 9 codes, and substantial reliability ($\u03ba\\geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\u03ba) = 0.34$; $max(\u03ba) = 0.55$). Importantly, we find that coding fidelity improves considerably when the LLM is prompted to give rationale justifying its coding decisions (chain-of-thought reasoning). We present these and other findings along with a set of best practices for adapting traditional codebooks for LLMs. Our results indicate that for certain codebooks, state-of-the-art LLMs are already adept at large-scale content analysis. Furthermore, they suggest the next generation of models will likely render AI coding a viable option for a majority of codebooks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15175",
        "abstract url": "https://arxiv.org/abs/2401.15175",
        "title": "Kitchen Food Waste Image Segmentation and Classification for Compost Nutrients Estimation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The escalating global concern over extensive food wastage necessitates innovative solutions to foster a net-zero lifestyle and reduce emissions. The LILA home composter presents a convenient means of recycling kitchen scraps and daily food waste into nutrient-rich, high-quality compost. To capture the nutritional information of the produced compost, we have created and annotated a large high-resolution image dataset of kitchen food waste with segmentation masks of 19 nutrition-rich categories. Leveraging this dataset, we benchmarked four state-of-the-art semantic segmentation models on food waste segmentation, contributing to the assessment of compost quality of Nitrogen, Phosphorus, or Potassium. The experiments demonstrate promising results of using segmentation models to discern food waste produced in our daily lives. Based on the experiments, SegFormer, utilizing MIT-B5 backbone, yields the best performance with a mean Intersection over Union (mIoU) of 67.09. Class-based results are also provided to facilitate further analysis of different food waste classes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15266",
        "abstract url": "https://arxiv.org/abs/2401.15266",
        "title": "SAM-based instance segmentation models for the automation of structural damage detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Automating visual inspection for capturing defects based on civil structures appearance is crucial due to its currently labour-intensive and time-consuming nature. An important aspect of automated inspection is image acquisition, which is rapid and cost-effective considering the pervasive developments in both software and hardware computing in recent years. Previous studies largely focused on concrete and asphalt, with less attention to masonry cracks. The latter also lacks publicly available datasets. In this paper, we first present a corresponding data set for instance segmentation with 1,300 annotated images (640 pixels x 640 pixels), named as MCrack1300, covering bricks, broken bricks, and cracks. We then test several leading algorithms for benchmarking, including the latest large-scale model, the prompt-based Segment Anything Model (SAM). We fine-tune the encoder using Low-Rank Adaptation (LoRA) and proposed two novel methods for automation of SAM execution. The first method involves abandoning the prompt encoder and connecting the SAM encoder to other decoders, while the second method introduces a learnable self-generating prompter. In order to ensure the seamless integration of the two proposed methods with SAM encoder section, we redesign the feature extractor. Both proposed methods exceed state-of-the-art performance, surpassing the best benchmark by approximately 3% for all classes and around 6% for cracks specifically. Based on successful detection, we propose a method based on a monocular camera and the Hough Line Transform to automatically transform images into orthographic projection maps. By incorporating known real sizes of brick units, we accurately estimate crack dimensions, with the results differing by less than 10% from those obtained by laser scanning. Overall, we address important research gaps in automated masonry crack detection and size estimation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15273",
        "abstract url": "https://arxiv.org/abs/2401.15273",
        "title": "Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-time error analysis. Notably, we establish that FedSARSA converges to a policy that is near-optimal for all agents, with the extent of near-optimality proportional to the level of heterogeneity. Furthermore, we prove that FedSARSA leverages agent collaboration to enable linear speedups as the number of agents increases, which holds for both fixed and adaptive step-size configurations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Published as a conference paper at ICLR 2024"
    },
    {
        "paper id": "2401.15275",
        "abstract url": "https://arxiv.org/abs/2401.15275",
        "title": "Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transformer neural networks are increasingly replacing prior architectures in a wide range of applications in different data modalities. The increasing size and computational demands of fine-tuning large pre-trained transformer neural networks pose significant challenges for the widespread adoption of these models for applications that demand on-edge computing. To tackle this challenge, continual learning (CL) emerges as a solution by facilitating the transfer of knowledge across tasks that arrive sequentially for an autonomously learning agent. However, current CL methods mainly focus on learning tasks that are exclusively vision-based or language-based. We propose a transformer-based CL framework focusing on learning tasks that involve both vision and language, known as Vision-and-Language (VaL) tasks. Due to the success of transformers in other modalities, our architecture has the potential to be used in multimodal learning settings. In our framework, we benefit from introducing extra parameters to a base transformer to specialize the network for each task. As a result, we enable dynamic model expansion to learn several tasks in a sequence. We also use knowledge distillation to benefit from relevant past experiences to learn the current task more efficiently. Our proposed method, Task Attentive Multimodal Continual Learning (TAM-CL), allows for the exchange of information between tasks while mitigating the problem of catastrophic forgetting. Notably, our approach is scalable, incurring minimal memory and time overhead. TAM-CL achieves state-of-the-art (SOTA) performance on challenging multimodal tasks",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15287",
        "abstract url": "https://arxiv.org/abs/2401.15287",
        "title": "Applications of Tao General Difference in Discrete Domain",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Numerical difference computation is one of the cores and indispensable in the modern digital era. Tao general difference (TGD) is a novel theory and approach to difference computation for discrete sequences and arrays in multidimensional space. Built on the solid theoretical foundation of the general difference in a finite interval, the TGD operators demonstrate exceptional signal processing capabilities in real-world applications. A novel smoothness property of a sequence is defined on the first- and second TGD. This property is used to denoise one-dimensional signals, where the noise is the non-smooth points in the sequence. Meanwhile, the center of the gradient in a finite interval can be accurately location via TGD calculation. This solves a traditional challenge in computer vision, which is the precise localization of image edges with noise robustness. Furthermore, the power of TGD operators extends to spatio-temporal edge detection in three-dimensional arrays, enabling the identification of kinetic edges in video data. These diverse applications highlight the properties of TGD in discrete domain and the significant promise of TGD for the computation across signal processing, image analysis, and video analytic.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This paper is the application part of the paper \"Tao General Differential and Difference: Theory and Application\". The theory part of the paper is renamed as \"A Theory of General Difference in Continuous and Discrete Domain\", which is Arxived in arXiv:2305.08098v2"
    },
    {
        "paper id": "2401.15288",
        "abstract url": "https://arxiv.org/abs/2401.15288",
        "title": "STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose an efficient cross-cameras surveillance system called,STAC, that leverages spatio-temporal associations between multiple cameras to provide real-time analytics and inference under constrained network environments. STAC is built using the proposed omni-scale feature learning people reidentification (reid) algorithm that allows accurate detection, tracking and re-identification of people across cameras using the spatio-temporal characteristics of video frames. We integrate STAC with frame filtering and state-of-the-art compression for streaming technique (that is, ffmpeg libx264 codec) to remove redundant information from cross-camera frames. This helps in optimizing the cost of video transmission as well as compute/processing, while maintaining high accuracy for real-time query inference. The introduction of AICity Challenge 2023 Data [1] by NVIDIA has allowed exploration of systems utilizing multi-camera people tracking algorithms. We evaluate the performance of STAC using this dataset to measure the accuracy metrics and inference rate for reid. Additionally, we quantify the reduction in video streams achieved through frame filtering and compression using FFmpeg compared to the raw camera streams. For completeness, we make available our repository to reproduce the results, available at https://github.com/VolodymyrVakhniuk/CS444_Final_Project.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15293",
        "abstract url": "https://arxiv.org/abs/2401.15293",
        "title": "SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dropping 55% of the tokens while gaining more than 13% training throughput and maintaining classification accuracy at the level of the baseline model on Huawei Ascend910A.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01717",
        "abstract url": "https://arxiv.org/abs/2402.01717",
        "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Total number of pages: 9. Total number of figures: 2. For the source code and experimental results of this paper, see https://github.com/jwoongkim11/QA-RAG. For the dataset used in training and evaluating the model, see https://huggingface.co/datasets/Jaymax/FDA Pharmaceuticals FAQ"
    },
    {
        "paper id": "2402.01722",
        "abstract url": "https://arxiv.org/abs/2402.01722",
        "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14696",
        "abstract url": "https://arxiv.org/abs/2401.14696",
        "title": "Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the feature space, the collapse between features invokes critical problems in representation learning by remaining the features undistinguished. Interpolation-based augmentation methods such as mixup have shown their effectiveness in relieving the collapse problem between different classes, called inter-class collapse. However, intra-class collapse raised in coarse-to-fine transfer learning has not been discussed in the augmentation approach. To address them, we propose a better feature augmentation method, asymptotic midpoint mixup. The method generates augmented features by interpolation but gradually moves them toward the midpoint of inter-class feature pairs. As a result, the method induces two effects: 1) balancing the margin for all classes and 2) only moderately broadening the margin until it holds maximal confidence. We empirically analyze the collapse effects by measuring alignment and uniformity with visualizing representations. Then, we validate the intra-class collapse effects in coarse-to-fine transfer learning and the inter-class collapse effects in imbalanced learning on long-tailed datasets. In both tasks, our method shows better performance than other augmentation methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14713",
        "abstract url": "https://arxiv.org/abs/2401.14713",
        "title": "A safety risk assessment framework for children's online safety based on a novel safety weakness assessment approach",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "This paper addresses the problem of children's online safety in the context of the growing digital landscape. With a surge in the use of digital technology among children, there has been an increase in online safety harms, risks and criminal incidents despite existing data protection and online privacy protection regulations. Most general security and privacy assessment approaches/standards focus mainly on protecting businesses from financial loss, but there remains a notable gap in methodologies specifically designed to cater to the unique challenges faced by children in the online space. To fill this gap, we propose a safety risk assessment approach that focuses specifically on children's online safety. The key novelty of our approach is providing an explainable and systematic evaluation of potential safety weaknesses of online services and applications based on precise automated mathematical reasoning. This framework has the potential to assist online service and app designers during the system design phase enabling them to proactively ensure Safety-by-Design, as well as auditors and users to understand the risks posed by existing services/apps, promoting further research on designing age-appropriate warnings and education materials for children and parents.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14732",
        "abstract url": "https://arxiv.org/abs/2401.14732",
        "title": "Residual Quantization with Implicit Neural Codebooks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods increase the rate by representing each vector using codewords across multiple codebooks. Residual quantization (RQ) is one such method, which increases accuracy by iteratively quantizing the error of the previous step. The error distribution is dependent on previously selected codewords. This dependency is, however, not accounted for in conventional RQ as it uses a generic codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant which predicts specialized codebooks per vector using a neural network that is conditioned on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12 bytes codes than other methods using 16 bytes on the BigANN and Deep1B dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14811",
        "abstract url": "https://arxiv.org/abs/2401.14811",
        "title": "On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14816",
        "abstract url": "https://arxiv.org/abs/2401.14816",
        "title": "Unleashing Data Journalism's Potential: COVID-19 as Catalyst for Newsroom Transformation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "In the context of journalism, the COVID-19 pandemic brought unprecedented challenges, necessitating rapid adaptations in newsrooms. Data journalism emerged as a pivotal approach for effectively conveying complex information to the public. Here, we show the profound impact of COVID-19 on data journalism, revealing a surge in data-driven publications and heightened collaboration between data and science journalists. Employing a quantitative methodology, including negative binomial regression and Relational hyperevent models (RHEM), on byline data of articles co-authored by data journalists, we comprehensively analyze data journalism outputs, authorship trends, and collaboration networks to address five key research questions. The findings reveal a significant increase in data journalistic pieces during and after the pandemic, in particular with a rise in publications within scientific departments. Collaborative efforts among data and science journalists intensified, evident through increased authorship and co-authorship trends. Prior common authorship experiences somewhat influenced the likelihood of future co-authorships, underscoring the importance of building collaborative communities of practice. These quantitative insights provide an understanding of the transformational role of data journalism during COVID-19, contributing to the growing body of literature in computational communication science and journalism practice.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "22 pages, 3 figures, 3 tables"
    },
    {
        "paper id": "2401.14832",
        "abstract url": "https://arxiv.org/abs/2401.14832",
        "title": "Text Image Inpainting via Global Structure-Guided Diffusion Models",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion",
                "Inpainting"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: https://github.com/blackprotoss/GSDM.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI-24"
    },
    {
        "paper id": "2401.14847",
        "abstract url": "https://arxiv.org/abs/2401.14847",
        "title": "Extracting Process-Aware Decision Models from Object-Centric Process Data",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Organizations execute decisions within business processes on a daily basis whilst having to take into account multiple stakeholders who might require multiple point of views of the same process. Moreover, the complexity of the information systems running these business processes is generally high as they are linked to databases storing all the relevant data and aspects of the processes. Given the presence of multiple objects within an information system which support the processes in their enactment, decisions are naturally influenced by both these perspectives, logged in object-centric process logs. However, the discovery of such decisions from object-centric process logs is not straightforward as it requires to correctly link the involved objects whilst considering the sequential constraints that business processes impose as well as correctly discovering what a decision actually does. This paper proposes the first object-centric decision-mining algorithm called Integrated Object-centric Decision Discovery Algorithm (IODDA). IODDA is able to discover how a decision is structured as well as how a decision is made. Moreover, IODDA is able to discover which activities and object types are involved in the decision-making process. Next, IODDA is demonstrated with the first artificial knowledge-intensive process logs whose log generators are provided to the research community.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14893",
        "abstract url": "https://arxiv.org/abs/2401.14893",
        "title": "A structured regression approach for evaluating model performance across intersectional subgroups",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Disaggregated evaluation is a central task in AI fairness assessment, with the goal to measure an AI system's performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups. We evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data. The results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and goodness-of-fit testing helps identify the key factors that drive differences in performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14908",
        "abstract url": "https://arxiv.org/abs/2401.14908",
        "title": "A Framework for Assurance Audits of Algorithmic Systems",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "An increasing number of regulations propose the notion of AI audits as an enforcement mechanism for achieving transparency and accountability for AI systems. Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently have little to no agreed upon practices, procedures, taxonomies, and standards. We propose the criterion audit as an operationalizable compliance and assurance external audit framework. We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations' ability to govern their algorithms in ways that mitigate harms and uphold human values. We discuss the necessary conditions for the criterion audit, and provide a procedural blueprint for performing an audit engagement in practice. We illustrate how this framework can be adapted to current regulations by deriving the criteria on which bias audits for hiring algorithms can be performed, as required by the recently effective New York City Local Law 144 of 2021. We conclude by offering critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge. Our discussion as informed by experiences in performing these audits in practice highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of such methodology.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14919",
        "abstract url": "https://arxiv.org/abs/2401.14919",
        "title": "PARSAC: Accelerating Robust Multi-Model Fitting with Parallel Sample Consensus",
        "rating": 0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "We present a real-time method for robust estimation of multiple instances of geometric models from noisy data. Geometric models such as vanishing points, planar homographies or fundamental matrices are essential for 3D scene analysis. Previous approaches discover distinct model instances in an iterative manner, thus limiting their potential for speedup via parallel computation. In contrast, our method detects all model instances independently and in parallel. A neural network segments the input data into clusters representing potential model instances by predicting multiple sets of sample and inlier weights. Using the predicted weights, we determine the model parameters for each potential instance separately in a RANSAC-like fashion. We train the neural network via task-specific loss functions, i.e. we do not require a ground-truth segmentation of the input data. As suitable training data for homography and fundamental matrix fitting is scarce, we additionally present two new synthetic datasets. We demonstrate state-of-the-art performance on these as well as multiple established datasets, with inference times as small as five milliseconds per image.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "AAAI 2024"
    },
    {
        "paper id": "2401.14923",
        "abstract url": "https://arxiv.org/abs/2401.14923",
        "title": "Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "In AAMAS 2024"
    },
    {
        "paper id": "2401.14925",
        "abstract url": "https://arxiv.org/abs/2401.14925",
        "title": "More inclusive and on wider sources: A Comparative Analysis of Data and Political Journalists on Twitter in Germany",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Women are underrepresented in many areas of journalistic newsrooms. In this paper, we examine if this established effect continues in the new forms of journalistic communication, Social Media Networks. We used mentions, retweets, and hashtags as journalistic amplification and legitimation measures. Furthermore, we compared two groups of journalists in different stages of development: political and data journalists in Germany in 2021. Our results show that journalists regarded as women tend to favor their other women in mentions and retweets on Twitter, compared to men. While both professions are dominated by many men and a high share of men-authored tweets, women are mentioning and retweeting other women to a more extensive degree than their male colleagues. Women data journalists also leveraged different sources than men. In addition, we have found data journalists to be more inclusive towards non-member sources in their network compared to political journalists.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "25 pages, 3 figures, 10 tables"
    },
    {
        "paper id": "2401.14953",
        "abstract url": "https://arxiv.org/abs/2401.14953",
        "title": "Learning Universal Predictors",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "32 pages, 11 figures"
    },
    {
        "paper id": "2401.14989",
        "abstract url": "https://arxiv.org/abs/2401.14989",
        "title": "Mapping-to-Parameter Nonlinear Functional Regression with Novel B-spline Free Knot Placement Algorithm",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a novel approach to nonlinear functional regression, called the Mapping-to-Parameter function model, which addresses complex and nonlinear functional regression problems in parameter space by employing any supervised learning technique. Central to this model is the mapping of function data from an infinite-dimensional function space to a finite-dimensional parameter space. This is accomplished by concurrently approximating multiple functions with a common set of B-spline basis functions by any chosen order, with their knot distribution determined by the Iterative Local Placement Algorithm, a newly proposed free knot placement algorithm. In contrast to the conventional equidistant knot placement strategy that uniformly distributes knot locations based on a predefined number of knots, our proposed algorithms determine knot location according to the local complexity of the input or output functions. The performance of our knot placement algorithms is shown to be robust in both single-function approximation and multiple-function approximation contexts. Furthermore, the effectiveness and advantage of the proposed prediction model in handling both function-on-scalar regression and function-on-function regression problems are demonstrated through several real data applications, in comparison with four groups of state-of-the-art methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15059",
        "abstract url": "https://arxiv.org/abs/2401.15059",
        "title": "Fully Independent Communication in Multi-Agent Reinforcement Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication may not always be needed and that the chosen agent network sizes need to be considered when used together with communication in order to achieve efficient learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Extended version of the paper appearing on AAMAS 2024 with the same title. 11 pages, 8 figures"
    },
    {
        "paper id": "2401.15077",
        "abstract url": "https://arxiv.org/abs/2401.15077",
        "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15121",
        "abstract url": "https://arxiv.org/abs/2401.15121",
        "title": "Expressive Power of ReLU and Step Networks under Floating-Point Operations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations use finite bits for both significand and exponent; these results are applicable to many popular floating-point formats such as those defined in the IEEE 754 standard (e.g., 32-bit single-precision format) and bfloat16.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15196",
        "abstract url": "https://arxiv.org/abs/2401.15196",
        "title": "Regularized Q-Learning with Linear Function Approximation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15229",
        "abstract url": "https://arxiv.org/abs/2401.15229",
        "title": "Evolving AI Risk Management: A Maturity Model based on the NIST AI Risk Management Framework",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Researchers, government bodies, and organizations have been repeatedly calling for a shift in the responsible AI community from general principles to tangible and operationalizable practices in mitigating the potential sociotechnical harms of AI. Frameworks like the NIST AI RMF embody an emerging consensus on recommended practices in operationalizing sociotechnical harm mitigation. However, private sector organizations currently lag far behind this emerging consensus. Implementation is sporadic and selective at best. At worst, it is ineffective and can risk serving as a misleading veneer of trustworthy processes, providing an appearance of legitimacy to substantively harmful practices. In this paper, we provide a foundation for a framework for evaluating where organizations sit relative to the emerging consensus on sociotechnical harm mitigation best practices: a flexible maturity model based on the NIST AI RMF.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15238",
        "abstract url": "https://arxiv.org/abs/2401.15238",
        "title": "Deep Learning with Tabular Data: A Self-supervised Approach",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to examine performance of the TabTransformer model against baseline models such as MLP and supervised TabTransformer. The research has presented with a novel approach by creating various variants of TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT which has helped to increase the effective capturing of the underlying relationship between various features of the tabular dataset by constructing optimal inputs. And further we have employed a self-supervised learning approach in the form of a masking-based unsupervised setting for tabular data. The findings shed light on the best way to represent categorical and numerical features, emphasizing the TabTransormer performance when compared to established machine learning models and other self-supervised learning methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15240",
        "abstract url": "https://arxiv.org/abs/2401.15240",
        "title": "Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study policy optimization algorithms for computing correlated equilibria in multi-player general-sum Markov Games. Previous results achieve $O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated $O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated equilibrium. In this paper, we improve both results significantly by providing an uncoupled policy optimization algorithm that attains a near-optimal $\\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium. Our algorithm is constructed by combining two main elements (i) smooth value updates and (ii) the optimistic-follow-the-regularized-leader algorithm with the log barrier regularizer.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "AISTATS 2024 Oral"
    },
    {
        "paper id": "2401.15246",
        "abstract url": "https://arxiv.org/abs/2401.15246",
        "title": "Training Differentially Private Ad Prediction Models with Semi-Sensitive Features",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Motivated by problems arising in digital advertising, we introduce the task of training differentially private (DP) machine learning models with semi-sensitive features. In this setting, a subset of the features is known to the attacker (and thus need not be protected) while the remaining features as well as the label are unknown to the attacker and should be protected by the DP guarantee. This task interpolates between training the model with full DP (where the label and all features should be protected) or with label DP (where all the features are considered known, and only the label should be protected). We present a new algorithm for training DP models with semi-sensitive features. Through an empirical evaluation on real ads datasets, we demonstrate that our algorithm surpasses in utility the baselines of (i) DP stochastic gradient descent (DP-SGD) run on all features (known and unknown), and (ii) a label DP algorithm run only on the known features (while discarding the unknown ones).",
        "subjects": [
            "cs.LG"
        ],
        "comment": "7 pages, 4 figures"
    },
    {
        "paper id": "2401.15284",
        "abstract url": "https://arxiv.org/abs/2401.15284",
        "title": "Five ethical principles for generative AI in scientific research",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Generative artificial intelligence tools like large language models are rapidly transforming academic research and real world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting the benefits of AI while safeguarding research integrity.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "9 pages, 2 tables"
    },
    {
        "paper id": "2401.15292",
        "abstract url": "https://arxiv.org/abs/2401.15292",
        "title": "Adaptive Block Sparse Regularization under Arbitrary Linear Transform",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a convex and fast signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the similar existing method and can reconstruct signals with block sparsity under non-invertible transforms, unlike the existing method. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "5 pages, 4 figures"
    },
    {
        "paper id": "2401.16244",
        "abstract url": "https://arxiv.org/abs/2401.16244",
        "title": "Employing Iterative Feature Selection in Fuzzy Rule-Based Binary Classification",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The feature selection in a traditional binary classification algorithm is always used in the stage of dataset preprocessing, which makes the obtained features not necessarily the best ones for the classification algorithm, thus affecting the classification performance. For a traditional rule-based binary classification algorithm, classification rules are usually deterministic, which results in the fuzzy information contained in the rules being ignored. To do so, this paper employs iterative feature selection in fuzzy rule-based binary classification. The proposed algorithm combines feature selection based on fuzzy correlation family with rule mining based on biclustering. It first conducts biclustering on the dataset after feature selection. Then it conducts feature selection again for the biclusters according to the feedback of biclusters evaluation. In this way, an iterative feature selection framework is build. During the iteration process, it stops until the obtained bicluster meets the requirements. In addition, the rule membership function is introduced to extract vectorized fuzzy rules from the bicluster and construct weak classifiers. The weak classifiers with good classification performance are selected by Adaptive Boosting and the strong classifier is constructed by \"weighted average\". Finally, we perform the proposed algorithm on different datasets and compare it with other peers. Experimental results show that it achieves good classification performance and outperforms its peers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01716",
        "abstract url": "https://arxiv.org/abs/2402.01716",
        "title": "Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Online discussion forums are widely used for active textual interaction between lecturers and students, and to see how the students have progressed in a learning process. The objective of this study is to compare appropriate machine-learning models to assess sentiments and Bloom\u015b epistemic taxonomy based on textual comments in educational discussion forums. Our proposed method is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis (BE-Sent). The research methodology consists of three main steps. The first step is the data collection from the internal discussion forum and YouTube comments of a Web Programming channel. The next step is text preprocessing to annotate the text and clear unimportant words. Furthermore, with the text dataset that has been successfully cleaned, sentiment analysis and epistemic categorization will be done in each sentence of the text. Sentiment analysis is divided into three categories: positive, negative, and neutral. Bloom\u015b epistemic is divided into six categories: remembering, understanding, applying, analyzing, evaluating, and creating. This research has succeeded in producing a course learning subsystem that assesses opinions based on text reviews of discussion forums according to the category of sentiment and epistemic analysis.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "11 pages, 7 figures"
    },
    {
        "paper id": "2402.01719",
        "abstract url": "https://arxiv.org/abs/2402.01719",
        "title": "Measuring Moral Inconsistencies in Large Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "A Large Language Model (LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracy. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no \"correct\" answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy (SGE) to measure the consistency of an LLM in moral scenarios. We leverage \"Rules of Thumb\" (RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies and propose improvements.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at BlackBoxNLP 2023, Co-located with EMNLP 2023"
    },
    {
        "paper id": "2402.01721",
        "abstract url": "https://arxiv.org/abs/2402.01721",
        "title": "Non-Consensual Synthetic Intimate Imagery: Prevalence, Attitudes, and Knowledge in 10 Countries",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Deepfake technologies have become ubiquitous, \"democratizing\" the ability to manipulate photos and videos. One popular use of deepfake technology is the creation of sexually explicit content, which can then be posted and shared widely on the internet. Drawing on a survey of over 16,000 respondents in 10 different countries, this article examines attitudes and behaviors related to \"deepfake pornography\" as a specific form of non-consensual synthetic intimate imagery (NSII). Our study found that deepfake pornography behaviors were considered harmful by respondents, despite nascent societal awareness. Regarding the prevalence of deepfake porn victimization and perpetration, 2.2% of all respondents indicated personal victimization, and 1.8% all of respondents indicated perpetration behaviors. Respondents from countries with specific legislation still reported perpetration and victimization experiences, suggesting NSII laws are inadequate to deter perpetration. Approaches to prevent and reduce harms may include digital literacy education, as well as enforced platform policies, practices, and tools which better detect, prevent, and respond to NSII content.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14675",
        "abstract url": "https://arxiv.org/abs/2401.14675",
        "title": "Multi-model learning by sequential reading of untrimmed videos for action recognition",
        "rating": 0,
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a new method for learning videos by aggregating multiple models by sequentially extracting video clips from untrimmed video. The proposed method reduces the correlation between clips by feeding clips to multiple models in turn and synchronizes these models through federated learning. Experimental results show that the proposed method improves the performance compared to the no synchronization.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The International Workshop on Frontiers of Computer Vision (IW-FCV2024)"
    },
    {
        "paper id": "2401.14719",
        "abstract url": "https://arxiv.org/abs/2401.14719",
        "title": "pLitterStreet: Street Level Plastic Litter Detection and Mapping",
        "rating": 0,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Plastic pollution is a critical environmental issue, and detecting and monitoring plastic litter is crucial to mitigate its impact. This paper presents the methodology of mapping street-level litter, focusing primarily on plastic waste and the location of trash bins. Our methodology involves employing a deep learning technique to identify litter and trash bins from street-level imagery taken by a camera mounted on a vehicle. Subsequently, we utilized heat maps to visually represent the distribution of litter and trash bins throughout cities. Additionally, we provide details about the creation of an open-source dataset (\"pLitterStreet\") which was developed and utilized in our approach. The dataset contains more than 13,000 fully annotated images collected from vehicle-mounted cameras and includes bounding box labels. To evaluate the effectiveness of our dataset, we tested four well known state-of-the-art object detection algorithms (Faster R-CNN, RetinaNet, YOLOv3, and YOLOv5), achieving an average precision (AP) above 40%. While the results show average metrics, our experiments demonstrated the reliability of using vehicle-mounted cameras for plastic litter mapping. The \"pLitterStreet\" can also be a valuable resource for researchers and practitioners to develop and further improve existing machine learning models for detecting and mapping plastic litter in an urban environment. The dataset is open-source and more details about the dataset and trained models can be found at https://github.com/gicait/pLitter.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14785",
        "abstract url": "https://arxiv.org/abs/2401.14785",
        "title": "SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Our work addresses the problem of egocentric human pose estimation from downwards-facing cameras on head-mounted devices (HMD). This presents a challenging scenario, as parts of the body often fall outside of the image or are occluded. Previous solutions minimize this problem by using fish-eye camera lenses to capture a wider view, but these can present hardware design issues. They also predict 2D heat-maps per joint and lift them to 3D space to deal with self-occlusions, but this requires large network architectures which are impractical to deploy on resource-constrained HMDs. We predict pose from images captured with conventional rectilinear camera lenses. This resolves hardware design issues, but means body parts are often out of frame. As such, we directly regress probabilistic joint rotations represented as matrix Fisher distributions for a parameterized body model. This allows us to quantify pose uncertainties and explain out-of-frame or occluded joints. This also removes the need to compute 2D heat-maps and allows for simplified DNN architectures which require less compute. Given the lack of egocentric datasets using rectilinear camera lenses, we introduce the SynthEgo dataset, a synthetic dataset with 60K stereo images containing high diversity of pose, shape, clothing and skin tone. Our approach achieves state-of-the-art results for this challenging configuration, reducing mean per-joint position error by 23% overall and 58% for the lower body. Our architecture also has eight times fewer parameters and runs twice as fast as the current state-of-the-art. Experiments show that training on our synthetic dataset leads to good generalization to real world images without fine-tuning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted in 3DV 2024"
    },
    {
        "paper id": "2401.14814",
        "abstract url": "https://arxiv.org/abs/2401.14814",
        "title": "Towards Robust Hyperspectral Anomaly Detection: Decomposing Background, Anomaly, and Mixed Noise via Convex Optimization",
        "rating": 0,
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "We propose a novel hyperspectral (HS) anomaly detection method that is robust to various types of noise. Most of existing HS anomaly detection methods are designed for cases where a given HS image is noise-free or is contaminated only by small Gaussian noise. However, in real-world situations, observed HS images are often degraded by various types of noise, such as sparse noise and stripe noise, due to sensor failure or calibration errors, significantly affecting the detection performance. To address this problem, this article establishes a robust HS anomaly detection method with a mechanism that can properly remove mixed noise while separating background and anomaly parts. Specifically, we newly formulate a constrained convex optimization problem to decompose background and anomaly parts, and three types of noise from a given HS image. Then, we develop an efficient algorithm based on a preconditioned variant of a primal-dual splitting method to solve this problem. Through comparison with existing methods, including state-of-the-art ones, we illustrate that the proposed method achieves a detection accuracy comparable to state-of-the-art methods in noise-free cases and is significantly more robust than these methods in noisy cases.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
        "paper id": "2401.14828",
        "abstract url": "https://arxiv.org/abs/2401.14828",
        "title": "TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "Gaussian splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIPEditor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accpeted by Siggraph 2024 & ACM Transactions on Graphics"
    },
    {
        "paper id": "2401.14838",
        "abstract url": "https://arxiv.org/abs/2401.14838",
        "title": "Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring",
        "rating": 0,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Driver Action Recognition (DAR) is crucial in vehicle cabin monitoring systems. In real-world applications, it is common for vehicle cabins to be equipped with cameras featuring different modalities. However, multi-modality fusion strategies for the DAR task within car cabins have rarely been studied. In this paper, we propose a novel yet efficient multi-modality driver action recognition method based on dual feature shift, named DFS. DFS first integrates complementary features across modalities by performing modality feature interaction. Meanwhile, DFS achieves the neighbour feature propagation within single modalities, by feature shifting among temporal frames. To learn common patterns and improve model efficiency, DFS shares feature extracting stages among multiple modalities. Extensive experiments have been carried out to verify the effectiveness of the proposed DFS model on the Drive\\&Act dataset. The results demonstrate that DFS achieves good performance and improves the efficiency of multi-modality driver action recognition.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14845",
        "abstract url": "https://arxiv.org/abs/2401.14845",
        "title": "Adaptive Point Transformer",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces computational complexity while maintaining competitive accuracy compared to standard PTs. The code for AdaPT is made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "26 pages, 8 figures, submitted to Neural Networs"
    },
    {
        "paper id": "2401.14948",
        "abstract url": "https://arxiv.org/abs/2401.14948",
        "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training",
        "rating": 0.0,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating \"robust overfitting\". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted as a conference paper at ICLR 2024"
    },
    {
        "paper id": "2401.15002",
        "abstract url": "https://arxiv.org/abs/2401.15002",
        "title": "BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning",
        "rating": 0,
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "As an emerging and vital topic for studying deep neural networks' vulnerability (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons, and unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 16 attack and 27 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5 poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of evaluations in total. 3) Based on above evaluations, we present abundant analysis from 8 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at http://backdoorbench.com, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15018",
        "abstract url": "https://arxiv.org/abs/2401.15018",
        "title": "Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers",
        "rating": 0,
        "keywords": [
            [
                "SVM",
                "Support Vector Machine"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "Speaker Verification (SV) systems involve mainly two individual stages: feature extraction and classification. In this paper, we explore these two modules with the aim of improving the performance of a speaker verification system under noisy conditions. On the one hand, the choice of the most appropriate acoustic features is a crucial factor for performing robust speaker verification. The acoustic parameters used in the proposed system are: Mel Frequency Cepstral Coefficients (MFCC), their first and second derivatives (Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC), Perceptual Linear Predictive (PLP), and Relative Spectral Transform - Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison of different combinations of the previous features is discussed. On the other hand, the major weakness of a conventional Support Vector Machine (SVM) classifier is the use of generic traditional kernel functions to compute the distances among data points. However, the kernel function of an SVM has great influence on its performance. In this work, we propose the combination of two SVM-based classifiers with different kernel functions: Linear kernel and Gaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR) classifier. The combination is carried out by means of a parallel structure approach, in which different voting rules to take the final decision are considered. Results show that significant improvement in the performance of the SV system is achieved by using the combined features with the combined classifiers either with clean speech or in the presence of noise. Finally, to enhance the system more in noisy environments, the inclusion of the multiband noise removal technique as a preprocessing stage is proposed.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15023",
        "abstract url": "https://arxiv.org/abs/2401.15023",
        "title": "Spatial Analysis and Synthesis Methods: Subjective and Objective Evaluations Using Various Microphone Arrays in the Auralization of a Critical Listening Room",
        "rating": 0,
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "Parametric sound field synthesis methods, such as the Spatial Decomposition Method (SDM) and Higher-Order Spatial Impulse Response Rendering (HO-SIRR), are widely used for the analysis and auralization of sound fields. This paper studies the performances of various sound field synthesis methods in the context of the auralization of a critical listening room. The influence on the perceived spatial and timbral fidelity of the following factors is considered: the rendering framework, direction of arrival (DOA) estimation method, microphone array structure, and use of a dedicated center reference microphone with SDM. Listening tests compare the synthesized sound fields to a reference binaural rendering condition. Several acoustic parameters are measured to gain insights into objective differences between methods. A high-quality pressure microphone improves the SDM framework's timbral fidelity. Additionally, SDM and HO-SIRR show similarities in spatial fidelity. Performance variation between SDM configurations is influenced by the DOA estimation method and microphone array construction. The binaural SDM (BSDM) presentations display temporal artifacts impacting sound quality.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "13 pages, 6 figures"
    },
    {
        "paper id": "2401.15075",
        "abstract url": "https://arxiv.org/abs/2401.15075",
        "title": "Annotated Hands for Generative Models",
        "rating": 0,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generative models such as GANs and diffusion models have demonstrated impressive image generation capabilities. Despite these successes, these systems are surprisingly poor at creating images with hands. We propose a novel training framework for generative models that substantially improves the ability of such systems to create hand images. Our approach is to augment the training images with three additional channels that provide annotations to hands in the image. These annotations provide additional structure that coax the generative model to produce higher quality hand images. We demonstrate this approach on two different generative models: a generative adversarial network and a diffusion model. We demonstrate our method both on a new synthetic dataset of hand images and also on real photographs that contain hands. We measure the improved quality of the generated hands through higher confidence in finger joint identification using an off-the-shelf hand detector.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15241",
        "abstract url": "https://arxiv.org/abs/2401.15241",
        "title": "Unlearning Reveals the Influential Training Data of Language Models",
        "rating": 0,
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, under review"
    },
    {
        "paper id": "2401.15261",
        "abstract url": "https://arxiv.org/abs/2401.15261",
        "title": "Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes",
        "rating": 0,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The estimation of implicit cross-frame correspondences and the high computational cost have long been major challenges in video semantic segmentation (VSS) for driving scenes. Prior works utilize keyframes, feature propagation, or cross-frame attention to address these issues. By contrast, we are the first to harness vanishing point (VP) priors for more effective segmentation. Intuitively, objects near VPs (i.e., away from the vehicle) are less discernible. Moreover, they tend to move radially away from the VP over time in the usual case of a forward-facing camera, a straight road, and linear forward motion of the vehicle. Our novel, efficient network for VSS, named VPSeg, incorporates two modules that utilize exactly this pair of static and dynamic VP priors: sparse-to-dense feature mining (DenseVP) and VP-guided motion fusion (MotionVP). MotionVP employs VP-guided motion estimation to establish explicit correspondences across frames and help attend to the most relevant features from neighboring frames, while DenseVP enhances weak dynamic features in distant regions around VPs. These modules operate within a context-detail framework, which separates contextual features from high-resolution local features at different input resolutions to reduce computational costs. Contextual and local features are integrated through contextualized motion attention (CMA) for the final prediction. Extensive experiments on two popular driving segmentation benchmarks, Cityscapes and ACDC, demonstrate that VPSeg outperforms previous SOTA methods, with only modest computational overhead.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15282",
        "abstract url": "https://arxiv.org/abs/2401.15282",
        "title": "GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting glass regions is a challenging task due to the ambiguity of their transparency and reflection properties. These transparent glasses share the visual appearance of both transmitted arbitrary background scenes and reflected objects, thus having no fixed patterns.Recent visual foundation models, which are trained on vast amounts of data, have manifested stunning performance in terms of image perception and image generation. To segment glass surfaces with higher accuracy, we make full use of two visual foundation models: Segment Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass surface segmentor named GEM, which only consists of a SAM backbone, a simple feature pyramid, a discerning query selection module, and a mask decoder. The discerning query selection can adaptively identify glass surface features, assigning them as initialized queries in the mask decoder. We also propose a Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed S-GSD via diffusion model with four different scales, which contain 1x, 5x, 10x, and 20x of the original real data size. This dataset is a feasible source for transfer learning. The scale of synthetic data has positive impacts on transfer learning, while the improvement will gradually saturate as the amount of data increases. Extensive experiments demonstrate that GEM achieves a new state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets are available at: https://github.com/isbrycee/GEM-Glass-Segmentor.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 9 figures, 7 tables"
    },
    {
        "paper id": "2401.15296",
        "abstract url": "https://arxiv.org/abs/2401.15296",
        "title": "A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Person re-identification via 3D skeletons is an important emerging research area that triggers great interest in the pattern recognition community. With distinctive advantages for many application scenarios, a great diversity of 3D skeleton based person re-identification (SRID) methods have been proposed in recent years, effectively addressing prominent problems in skeleton modeling and feature learning. Despite recent advances, to the best of our knowledge, little effort has been made to comprehensively summarize these studies and their challenges. In this paper, we attempt to fill this gap by providing a systematic survey on current SRID approaches, model designs, challenges, and future directions. Specifically, we first formulate the SRID problem, and propose a taxonomy of SRID research with a summary of benchmark datasets, commonly-used model architectures, and an analytical review of different methods' characteristics. Then, we elaborate on the design principles of SRID models from multiple aspects to offer key insights for model improvement. Finally, we identify critical challenges confronting current studies and discuss several promising directions for future research of SRID.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "A up-to-date resource (papers, codes, data, etc.) of this survey is provided at https://github.com/Kali-Hac/3D-skeleton-based-person-re-ID-survey"
    },
    {
        "paper id": "2401.14792",
        "abstract url": "https://arxiv.org/abs/2401.14792",
        "title": "Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition",
        "rating": -0.5,
        "keywords": [
            [
                "facial",
                "Face"
            ],
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "In this study, we harness the information-theoretic Privacy Funnel (PF) model to develop a method for privacy-preserving representation learning using an end-to-end training framework. We rigorously address the trade-off between obfuscation and utility. Both are quantified through the logarithmic loss, a measure also recognized as self-information loss. This exploration deepens the interplay between information-theoretic privacy and representation learning, offering substantive insights into data protection mechanisms for both discriminative and generative models. Importantly, we apply our model to state-of-the-art face recognition systems. The model demonstrates adaptability across diverse inputs, from raw facial images to both derived or refined embeddings, and is competent in tasks such as classification, reconstruction, and generation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "IEEE ICASSP 2024"
    },
    {
        "paper id": "2401.14961",
        "abstract url": "https://arxiv.org/abs/2401.14961",
        "title": "Set-Based Training for Neural Network Verification",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can significantly affect the outputs of a neural network. In safety-critical environments, the inputs often contain noisy sensor data; hence, in this case, neural networks that are robust against input perturbations are required. To ensure safety, the robustness of a neural network must be formally verified. However, training and formally verifying robust neural networks is challenging. We address both of these challenges by employing, for the first time, an end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure trains neural networks, which can be easily verified using simple polynomial-time verification algorithms. Moreover, our extensive evaluation demonstrates that our set-based training procedure effectively trains robust neural networks, which are easier to verify. Set-based trained neural networks consistently match or outperform those trained with state-of-the-art robust training approaches.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14992",
        "abstract url": "https://arxiv.org/abs/2401.14992",
        "title": "Graph-based Active Learning for Entity Cluster Repair",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Cluster repair methods aim to determine errors in clusters and modify them so that each cluster consists of records representing the same entity. Current cluster repair methodologies primarily assume duplicate-free data sources, where each record from one source corresponds to a unique record from another. However, real-world data often deviates from this assumption due to quality issues. Recent approaches apply clustering methods in combination with link categorization methods so they can be applied to data sources with duplicates. Nevertheless, the results do not show a clear picture since the quality highly varies depending on the configuration and dataset. In this study, we introduce a novel approach for cluster repair that utilizes graph metrics derived from the underlying similarity graphs. These metrics are pivotal in constructing a classification model to distinguish between correct and incorrect edges. To address the challenge of limited training data, we integrate an active learning mechanism tailored to cluster-specific attributes. The evaluation shows that the method outperforms existing cluster repair methods without distinguishing between duplicate-free or dirty data sources. Notably, our modified active learning strategy exhibits enhanced performance when dealing with datasets containing duplicates, showcasing its effectiveness in such scenarios.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15050",
        "abstract url": "https://arxiv.org/abs/2401.15050",
        "title": "LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents",
        "rating": -0.5,
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Document AI is a growing research field that focuses on the comprehension and extraction of information from scanned and digital documents to make everyday business operations more efficient. Numerous downstream tasks and datasets have been introduced to facilitate the training of AI models capable of parsing and extracting information from various document types such as receipts and scanned forms. Despite these advancements, both existing datasets and models fail to address critical challenges that arise in industrial contexts. Existing datasets primarily comprise short documents consisting of a single page, while existing models are constrained by a limited maximum length, often set at 512 tokens. Consequently, the practical application of these methods in financial services, where documents can span multiple pages, is severely impeded. To overcome these challenges, we introduce LongFin, a multimodal document AI model capable of encoding up to 4K tokens. We also propose the LongForms dataset, a comprehensive financial dataset that encapsulates several industrial challenges in financial documents. Through an extensive evaluation, we demonstrate the effectiveness of the LongFin model on the LongForms dataset, surpassing the performance of existing public models while maintaining comparable results on existing single-page benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at AAAI 2024 Workshop on AI in Finance for Social Impact"
    },
    {
        "paper id": "2401.15123",
        "abstract url": "https://arxiv.org/abs/2401.15123",
        "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
        "rating": -0.5,
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) We use synthetic anomalies to enlarge the representation gap between the two networks. AnomalyLLM demonstrates state-of-the-art performance on 15 datasets, improving accuracy by at least 14.5\\% in the UCR dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 5 figures"
    },
    {
        "paper id": "2401.15248",
        "abstract url": "https://arxiv.org/abs/2401.15248",
        "title": "Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., \\cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "To appear in AISTATS2024"
    },
    {
        "paper id": "2401.15295",
        "abstract url": "https://arxiv.org/abs/2401.15295",
        "title": "Multi-Trigger Backdoor Attacks: More Triggers, More Threats",
        "rating": -0.5,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Backdoor attacks have emerged as a primary threat to (pre-)training and deployment of deep neural networks (DNNs). While backdoor attacks have been extensively studied in a body of works, most of them were focused on single-trigger attacks that poison a dataset using a single type of trigger. Arguably, real-world backdoor attacks can be much more complex, e.g., the existence of multiple adversaries for the same dataset if it is of high value. In this work, we investigate the practical threat of backdoor attacks under the setting of \\textbf{multi-trigger attacks} where multiple adversaries leverage different types of triggers to poison the same dataset. By proposing and investigating three types of multi-trigger attacks, including parallel, sequential, and hybrid attacks, we provide a set of important understandings of the coexisting, overwriting, and cross-activating effects between different triggers on the same dataset. Moreover, we show that single-trigger attacks tend to cause overly optimistic views of the security of current defense techniques, as all examined defense methods struggle to defend against multi-trigger attacks. Finally, we create a multi-trigger backdoor poisoning dataset to help future evaluation of backdoor attacks and defenses. Although our work is purely empirical, we hope it can help steer backdoor research toward more realistic settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.01720",
        "abstract url": "https://arxiv.org/abs/2402.01720",
        "title": "Deep Learning Based Amharic Chatbot for FAQs in Universities",
        "rating": -0.5,
        "keywords": [
            [
                "SVM",
                "Support Vector Machine"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\u00efve Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the best results with 91.55% accuracy and a validation loss of 0.3548 using an Adam optimizer and SoftMax activation function. The chatbot model was integrated with Facebook Messenger and deployed on a Heroku server for 24-hour accessibility. The experimental results demonstrate that the chatbot framework achieved its objectives and effectively addressed challenges such as Amharic Fidel variation, morphological variation, and lexical gaps. Future research could explore the integration of Amharic WordNet to narrow the lexical gap and support more complex questions.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14666",
        "abstract url": "https://arxiv.org/abs/2401.14666",
        "title": "Joint Transmitter Design for Robust Secure Radar-Communication Coexistence Systems",
        "rating": -1,
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "This paper investigates the spectrum sharing between a multiple-input single-output (MISO) secure communication system and a multiple-input multiple-output (MIMO) radar system in the presence of one suspicious eavesdropper. We jointly design the radar waveform and communication beamforming vector at the two systems, such that the interference between the base station (BS) and radar is reduced, and the detrimental radar interference to the communication system is enhanced to jam the eavesdropper, thereby increasing secure information transmission performance. In particular, by considering the imperfect channel state information (CSI) for the user and eavesdropper, we maximize the worst-case secrecy rate at the user, while ensuring the detection performance of radar system. To tackle this challenging problem, we propose a two-layer robust cooperative algorithm based on the S-lemma and semidefinite relaxation techniques. Simulation results demonstrate that the proposed algorithm achieves significant secrecy rate gains over the non-robust scheme. Furthermore, we illustrate the trade-off between secrecy rate and detection probability.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14673",
        "abstract url": "https://arxiv.org/abs/2401.14673",
        "title": "Generative Expressive Robot Behaviors using Large Language Models",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "People employ expressive behaviors to effectively communicate and coordinate their actions with others, such as nodding to acknowledge a person glancing at them or saying \"excuse me\" to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is adaptable and composable, building upon each other. Our approach utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that users found to be competent and easy to understand. Supplementary material can be found at https://generative-expressive-motion.github.io/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14712",
        "abstract url": "https://arxiv.org/abs/2401.14712",
        "title": "A comprehensive review of electricity storage applications in island systems",
        "rating": -1,
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "Electricity storage is crucial for power systems to achieve higher levels of renewable energy penetration. This is especially significant for non-interconnected island (NII) systems, which are electrically isolated and vulnerable to the fluctuations of intermittent renewable generation. This paper comprehensively reviews existing literature on electricity storage in island systems, documenting relevant storage applications worldwide and emphasizing the role of storage in transitioning NII towards a fossil-fuel-independent electricity sector. On this topic, the literature review indicates that the implementation of storage is a prerequisite for attaining renewable penetration rates of over 50% due to the amplified requirements for system flexibility and renewable energy arbitrage. The analysis also identifies potential storage services and classifies applicable storage architectures for islands. Amongst the available storage designs, two have emerged as particularly important for further investigation; standalone, centrally managed storage stations and storage combined with renewables to form a hybrid plant that operates indivisibly in the market. For each design, the operating principles, remuneration schemes, investment feasibility, and applications discussed in the literature are presented in-depth, while possible implementation barriers are acknowledged. The literature on hybrid power plants mainly focuses on wind-powered pumped-hydro stations. However, recently, PV-powered battery-based hybrid plants have gained momentum due to the decreasing cost of Li-ion technology. On the other hand, standalone storage establishments rely heavily on battery technology and are mainly used to provide flexibility to the island grid. Nevertheless, these investments often suffer from insufficient remunerating frameworks, making it challenging for storage projects to be financially secure.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "55 pages, 10 figures"
    },
    {
        "paper id": "2401.14722",
        "abstract url": "https://arxiv.org/abs/2401.14722",
        "title": "A Nonparametric Bayes Approach to Online Activity Prediction",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Accurately predicting the onset of specific activities within defined timeframes holds significant importance in several applied contexts. In particular, accurate prediction of the number of future users that will be exposed to an intervention is an important piece of information for experimenters running online experiments (A/B tests). In this work, we propose a novel approach to predict the number of users that will be active in a given time period, as well as the temporal trajectory needed to attain a desired user participation threshold. We model user activity using a Bayesian nonparametric approach which allows us to capture the underlying heterogeneity in user engagement. We derive closed-form expressions for the number of new users expected in a given period, and a simple Monte Carlo algorithm targeting the posterior distribution of the number of days needed to attain a desired number of users; the latter is important for experimental planning. We illustrate the performance of our approach via several experiments on synthetic and real world data, in which we show that our novel method outperforms existing competitors.",
        "subjects": [
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14726",
        "abstract url": "https://arxiv.org/abs/2401.14726",
        "title": "3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "NeRF"
            ],
            [
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "20 pages, 8 figures"
    },
    {
        "paper id": "2401.14749",
        "abstract url": "https://arxiv.org/abs/2401.14749",
        "title": "Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "This paper presents a method for achieving equilibrium in the ISING Hamiltonian when confronted with unevenly distributed charges on an irregular grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our approach involves dimensionally expanding the system, substituting charges with circulants, and representing distances through circulant shifts. This results in a systematic mapping of the charge system onto a space, transforming the irregular grid into a uniform configuration, applicable to Torical and Circular Hyperboloid Topologies. The paper covers fundamental definitions and notations related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine. It explores the marginalization problem in code on the graph probabilistic models for evaluating the partition function, encompassing exact and approximate estimation techniques. Rigorous proof is provided for the attainability of equilibrium states for the Boltzmann machine under Torical and Circular Hyperboloid, paving the way for the application of our methodology. Practical applications of our approach are investigated in Finite Geometry QC-LDPC Codes, specifically in Material Science. The paper further explores its effectiveness in the realm of Natural Language Processing Transformer Deep Neural Networks, examining Generalized Repeat Accumulate Codes, Spatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful nature of our topology-aware hardware-efficient quasi-cycle codes equilibrium method is showcased across diverse scientific domains without the use of specific section delineations.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "16 pages, 29 figures. arXiv admin note: text overlap with arXiv:2307.15778"
    },
    {
        "paper id": "2401.14750",
        "abstract url": "https://arxiv.org/abs/2401.14750",
        "title": "Decentralized Zeno-Free Event-Triggered Control For Multiple Networks Subject to Stochastic Network Delays and Poisson Pulsing Attacks",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "By designing the decentralized time-regularized (Zeno-free) event-triggered strategies for the state-feedback control law, this paper considers the stochastic stabilization of a class of networked control systems, where two sources of randomness exist in multiple decentralized networks that operate asynchronously and independently: the communication channels are constrained by the stochastic network delays and also by Poisson pulsing denial-of-service (Pp-DoS) attacks. The time delay in the network denotes the length from a transmission instant to the corresponding update instant, and is supposed to be a continuous random variable subject to certain continuous probability distribution; while the attacks' cardinal number is a discrete random variable supposed to be subject to Poisson distribution, so the inter-attack time, i.e., the time between two consecutive attack instants, is subject to exponential distribution. The considered system is modeled as a stochastic hybrid formalism, where the randomness enters through the jump map into the reset value (the inter-attack time directly related) of each triggered strategy. By only sampling/transmitting state measurements when needed and simultaneously by taking the specific medium access protocols into account, the designed event-triggered strategies are synthesized in a state-based and decentralized form, which are robust (tolerable well) to stochastic network delays, under different tradeoff-conditions between the minimum inter-event times, maximum allowable delays (i.e., potentially tolerable delays) and the frequencies of attacks. Using stochastic hybrid tools to combine attack-active parts with attack-over parts, the designed triggered strategies, if designed well according to the actual system needs, can tolerate (be resilient to) the Pp-DoS attacks and stochastic network delays without jeopardizing the stability and Zeno-freeness.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "18 pages, 13 figures"
    },
    {
        "paper id": "2401.14754",
        "abstract url": "https://arxiv.org/abs/2401.14754",
        "title": "VJT: A Video Transformer on Joint Tasks of Deblurring, Low-light Enhancement and Denoising",
        "rating": -1,
        "keywords": [
            [
                "Low-light Enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video restoration task aims to recover high-quality videos from low-quality observations. This contains various important sub-tasks, such as video denoising, deblurring and low-light enhancement, since video often faces different types of degradation, such as blur, low light, and noise. Even worse, these kinds of degradation could happen simultaneously when taking videos in extreme environments. This poses significant challenges if one wants to remove these artifacts at the same time. In this paper, to the best of our knowledge, we are the first to propose an efficient end-to-end video transformer approach for the joint task of video deblurring, low-light enhancement, and denoising. This work builds a novel multi-tier transformer where each tier uses a different level of degraded video as a target to learn the features of video effectively. Moreover, we carefully design a new tier-to-tier feature fusion scheme to learn video features incrementally and accelerate the training process with a suitable adaptive weighting scheme. We also provide a new Multiscene-Lowlight-Blur-Noise (MLBN) dataset, which is generated according to the characteristics of the joint task based on the RealBlur dataset and YouTube videos to simulate realistic scenes as far as possible. We have conducted extensive experiments, compared with many previous state-of-the-art methods, to show the effectiveness of our approach clearly.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages,8 figures"
    },
    {
        "paper id": "2401.14762",
        "abstract url": "https://arxiv.org/abs/2401.14762",
        "title": "A Comparative Study of Compressive Sensing Algorithms for Hyperspectral Imaging Reconstruction",
        "rating": -1,
        "keywords": [
            [
                "Hyperspectral Imaging"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Hyperspectral Imaging comprises excessive data consequently leading to significant challenges for data processing, storage and transmission. Compressive Sensing has been used in the field of Hyperspectral Imaging as a technique to compress the large amount of data. This work addresses the recovery of hyperspectral images 2.5x compressed. A comparative study in terms of the accuracy and the performance of the convex FISTA/ADMM in addition to the greedy gOMP/BIHT/CoSaMP recovery algorithms is presented. The results indicate that the algorithms recover successfully the compressed data, yet the gOMP algorithm achieves superior accuracy and faster recovery in comparison to the other algorithms at the expense of high dependence on unknown sparsity level of the data to recover.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Hyperspectral Imaging, Compressive Sensing, Convex Algorithms, Greedy Algorithms, FISTA, ADMM, gOMP, BIHT, CoSaMP, IEEE-copyrighted material (2022), IVMSP Workshop (26-29 June 2022)"
    },
    {
        "paper id": "2401.14786",
        "abstract url": "https://arxiv.org/abs/2401.14786",
        "title": "Study of the gOMP Algorithm for Recovery of Compressed Sensed Hyperspectral Images",
        "rating": -1,
        "keywords": [
            [
                "remote sensing",
                "Hyperspectral Images"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Hyperspectral Imaging (HSI) is used in a wide range of applications such as remote sensing, yet the transmission of the HS images by communication data links becomes challenging due to the large number of spectral bands that the HS images contain together with the limited data bandwidth available in real applications. Compressive Sensing reduces the images by randomly subsampling the spectral bands of each spatial pixel and then it performs the image reconstruction of all the bands using recovery algorithms which impose sparsity in a certain transform domain. Since the image pixels are not strictly sparse, this work studies a data sparsification pre-processing stage prior to compression to ensure the sparsity of the pixels. The sparsified images are compressed $2.5\\times$ and then recovered using the Generalized Orthogonal Matching Pursuit algorithm (gOMP) characterized by high accuracy, low computational requirements and fast convergence. The experiments are performed in five conventional hyperspectral images where the effect of different sparsification levels in the quality of the uncompressed as well as the recovered images is studied. It is concluded that the gOMP algorithm reconstructs the hyperspectral images with higher accuracy as well as faster convergence when the pixels are highly sparsified and hence at the expense of reducing the quality of the recovered images with respect to the original images.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Hyperspectral Imaging, Compressive Sensing, Greedy Algorithms, Generalized Orthogonal Matching Pursuit (gOMP), Sparsity, Sparsification, IEEE-copyrighted material (2022), WHISPERS Workshop (13-16 September 2022)"
    },
    {
        "paper id": "2401.14813",
        "abstract url": "https://arxiv.org/abs/2401.14813",
        "title": "Symbol-Specific Sparsification of Interprocedural Distributive Environment Problems",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Previous work has shown that one can often greatly speed up static analysis by computing data flows not for every edge in the program's control-flow graph but instead only along definition-use chains. This yields a so-called sparse static analysis. Recent work on SparseDroid has shown that specifically taint analysis can be \"sparsified\" with extraordinary effectiveness because the taint state of one variable does not depend on those of others. This allows one to soundly omit more flow-function computations than in the general case. In this work, we now assess whether this result carries over to the more generic setting of so-called Interprocedural Distributive Environment (IDE) problems. Opposed to taint analysis, IDE comprises distributive problems with large or even infinitely broad domains, such as typestate analysis or linear constant propagation. Specifically, this paper presents Sparse IDE, a framework that realizes sparsification for any static analysis that fits the IDE framework. We implement Sparse IDE in SparseHeros, as an extension to the popular Heros IDE solver, and evaluate its performance on real-world Java libraries by comparing it to the baseline IDE algorithm. To this end, we design, implement and evaluate a linear constant propagation analysis client on top of SparseHeros. Our experiments show that, although IDE analyses can only be sparsified with respect to symbols and not (numeric) values, Sparse IDE can nonetheless yield significantly lower runtimes and often also memory consumptions compared to the original IDE.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "To be published in ICSE 2024"
    },
    {
        "paper id": "2401.14818",
        "abstract url": "https://arxiv.org/abs/2401.14818",
        "title": "ChemDFM: Dialogue Foundation Model for Chemistry",
        "rating": -1,
        "keywords": [
            [
                "Chemical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference. Further qualitative evaluations demonstrate the efficiency and effectiveness of ChemDFM in real-world research scenarios. We will open-source the ChemDFM model soon.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages, 12 figures, 13 tables. Under Review"
    },
    {
        "paper id": "2401.14825",
        "abstract url": "https://arxiv.org/abs/2401.14825",
        "title": "Keeping the Harmony Between Neighbors: Local Fairness in Graph Fair Division",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "We study the problem of allocating indivisible resources under the connectivity constraints of a graph $G$. This model, initially introduced by Bouveret et al. (published in IJCAI, 2017), effectively encompasses a diverse array of scenarios characterized by spatial or temporal limitations, including the division of land plots and the allocation of time plots. In this paper, we introduce a novel fairness concept that integrates local comparisons within the social network formed by a connected allocation of the item graph. Our particular focus is to achieve pairwise-maximin fair share (PMMS) among the \"neighbors\" within this network. For any underlying graph structure, we show that a connected allocation that maximizes Nash welfare guarantees a $(1/2)$-PMMS fairness. Moreover, for two agents, we establish that a $(3/4)$-PMMS allocation can be efficiently computed. Additionally, we demonstrate that for three agents and the items aligned on a path, a PMMS allocation is always attainable and can be computed in polynomial time. Lastly, when agents have identical additive utilities, we present a pseudo-polynomial-time algorithm for a $(3/4)$-PMMS allocation, irrespective of the underlying graph $G$. Furthermore, we provide a polynomial-time algorithm for obtaining a PMMS allocation when $G$ is a tree.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "Full version of paper accepted for presentation at AAMAS 2024"
    },
    {
        "paper id": "2401.14826",
        "abstract url": "https://arxiv.org/abs/2401.14826",
        "title": "Expressivity-aware Music Performance Retrieval using Mid-level Perceptual Features and Emotion Word Embeddings",
        "rating": -1,
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.SD"
            ]
        ],
        "abstract": "This paper explores a specific sub-task of cross-modal music retrieval. We consider the delicate task of retrieving a performance or rendition of a musical piece based on a description of its style, expressive character, or emotion from a set of different performances of the same piece. We observe that a general purpose cross-modal system trained to learn a common text-audio embedding space does not yield optimal results for this task. By introducing two changes -- one each to the text encoder and the audio encoder -- we demonstrate improved performance on a dataset of piano performances and associated free-text descriptions. On the text side, we use emotion-enriched word embeddings (EWE) and on the audio side, we extract mid-level perceptual features instead of generic audio embeddings. Our results highlight the effectiveness of mid-level perceptual features learnt from music and emotion enriched word embeddings learnt from emotion-labelled text in capturing musical expression in a cross-modal setting. Additionally, our interpretable mid-level features provide a route for introducing explainability in the retrieval and downstream recommendation processes.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Presented at FIRE 2023 (Forum for Information Retrieval Evaluation) conference, Goa, India"
    },
    {
        "paper id": "2401.14858",
        "abstract url": "https://arxiv.org/abs/2401.14858",
        "title": "RESPRECT: Speeding-up Multi-fingered Grasping with Residual Reinforcement Learning",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Deep Reinforcement Learning (DRL) has proven effective in learning control policies using robotic grippers, but much less practical for solving the problem of grasping with dexterous hands -- especially on real robotic platforms -- due to the high dimensionality of the problem. In this work, we focus on the multi-fingered grasping task with the anthropomorphic hand of the iCub humanoid. We propose the RESidual learning with PREtrained CriTics (RESPRECT) method that, starting from a policy pre-trained on a large set of objects, can learn a residual policy to grasp a novel object in a fraction ($\\sim 5 \\times$ faster) of the timesteps required to train a policy from scratch, without requiring any task demonstration. To our knowledge, this is the first Residual Reinforcement Learning (RRL) approach that learns a residual policy on top of another policy pre-trained with DRL. We exploit some components of the pre-trained policy during residual learning that further speed-up the training. We benchmark our results in the iCub simulated environment, and we show that RESPRECT can be effectively used to learn a multi-fingered grasping policy on the real iCub robot. The code to reproduce the experiments is released together with the paper with an open source license.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "IEEE Robotics and Automation Letters"
    },
    {
        "paper id": "2401.14879",
        "abstract url": "https://arxiv.org/abs/2401.14879",
        "title": "Fast Long-Term Multi-Scenario Prediction for Maneuver Planning at Unsignalized Intersections",
        "rating": -1,
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "Motion prediction for intelligent vehicles typically focuses on estimating the most probable future evolutions of a traffic scenario. Estimating the gap acceptance, i.e., whether a vehicle merges or crosses before another vehicle with the right of way, is often handled implicitly in the prediction. However, an infrastructure-based maneuver planning can assign artificial priorities between cooperative vehicles, so it needs to evaluate many more potential scenarios. Additionally, the prediction horizon has to be long enough to assess the impact of a maneuver. We, therefore, present a novel long-term prediction approach handling the gap acceptance estimation and the velocity prediction in two separate stages. Thereby, the behavior of regular vehicles as well as priority assignments of cooperative vehicles can be considered. We train both stages on real-world traffic observations to achieve realistic prediction results. Our method has a competitive accuracy and is fast enough to predict a multitude of scenarios in a short time, making it suitable to be used in a maneuver planning framework.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14882",
        "abstract url": "https://arxiv.org/abs/2401.14882",
        "title": "Analyzing the concept of technical debt in the context of agile software development: A systematic literature review",
        "rating": -1,
        "keywords": [
            [
                "synthesize"
            ]
        ],
        "abstract": "Technical debt (TD) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders. In recent years, it has gained significant attention in agile software development (ASD). The purpose of this study is to analyze and synthesize the state of the art of TD, and its causes, consequences, and management strategies in the context of ASD. Using a systematic literature review (SLR), 38 primary studies, out of 346 studies, were identified and analyzed. We found five research areas of interest related to the literature of TD in ASD. Among those areas, managing TD in ASD received the highest attention, followed by architecture in ASD and its relationship with TD. In addition, eight categories regarding the causes and five categories regarding the consequences of incurring TD in ASD were identified. Focus on quick delivery and architectural and design issues were the most popular causes of incurring TD in ASD. Reduced productivity, system degradation and increased maintenance cost were identified as significant consequences of incurring TD in ASD. Additionally, we found 12 strategies for managing TD in the context of ASD, out of which refactoring and enhancing the visibility of TD were the most significant. The results of this study provide a structured synthesis of TD and its management in the context of ASD as well as potential research areas for further investigation.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14884",
        "abstract url": "https://arxiv.org/abs/2401.14884",
        "title": "P3LS: Partial Least Squares under Privacy Preservation",
        "rating": -1,
        "keywords": [
            [
                "federated learning"
            ]
        ],
        "abstract": "Modern manufacturing value chains require intelligent orchestration of processes across company borders in order to maximize profits while fostering social and environmental sustainability. However, the implementation of integrated, systems-level approaches for data-informed decision-making along value chains is currently hampered by privacy concerns associated with cross-organizational data exchange and integration. We here propose Privacy-Preserving Partial Least Squares (P3LS) regression, a novel federated learning technique that enables cross-organizational data integration and process modeling with privacy guarantees. P3LS involves a singular value decomposition (SVD) based PLS algorithm and employs removable, random masks generated by a trusted authority in order to protect the privacy of the data contributed by each data holder. We demonstrate the capability of P3LS to vertically integrate process data along a hypothetical value chain consisting of three parties and to improve the prediction performance on several process-related key performance indicators. Furthermore, we show the numerical equivalence of P3LS and PLS model components on simulated data and provide a thorough privacy analysis of the former. Moreover, we propose a mechanism for determining the relevance of the contributed data to the problem being addressed, thus creating a basis for quantifying the contribution of participants.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "15 pages, 8 figures"
    },
    {
        "paper id": "2401.14885",
        "abstract url": "https://arxiv.org/abs/2401.14885",
        "title": "Neuromorphic quadratic programming for efficient and scalable model predictive control",
        "rating": -1,
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "Applications in robotics or other size-, weight- and power-constrained autonomous systems at the edge often require real-time and low-energy solutions to large optimization problems. Event-based and memory-integrated neuromorphic architectures promise to solve such optimization problems with superior energy efficiency and performance compared to conventional von Neumann architectures. Here, we present a method to solve convex continuous optimization problems with quadratic cost functions and linear constraints on Intel's scalable neuromorphic research chip Loihi 2. When applied to model predictive control (MPC) problems for the quadruped robotic platform ANYmal, this method achieves over two orders of magnitude reduction in combined energy-delay product compared to the state-of-the-art solver, OSQP, on (edge) CPUs and GPUs with solution times under ten milliseconds for various problem sizes. These results demonstrate the benefit of non-von-Neumann architectures for robotic control applications.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Submitted to Robotics and Automation Magazine on 22-12-2023"
    },
    {
        "paper id": "2401.14886",
        "abstract url": "https://arxiv.org/abs/2401.14886",
        "title": "Coca: Improving and Explaining Graph Neural Network-Based Vulnerability Detection Systems",
        "rating": -1,
        "keywords": [
            [
                "GNN",
                "Graph"
            ]
        ],
        "abstract": "Recently, Graph Neural Network (GNN)-based vulnerability detection systems have achieved remarkable success. However, the lack of explainability poses a critical challenge to deploy black-box models in security-related domains. For this reason, several approaches have been proposed to explain the decision logic of the detection model by providing a set of crucial statements positively contributing to its predictions. Unfortunately, due to the weakly-robust detection models and suboptimal explanation strategy, they have the danger of revealing spurious correlations and redundancy issue. In this paper, we propose Coca, a general framework aiming to 1) enhance the robustness of existing GNN-based vulnerability detection models to avoid spurious explanations; and 2) provide both concise and effective explanations to reason about the detected vulnerabilities. \\sysname consists of two core parts referred to as Trainer and Explainer. The former aims to train a detection model which is robust to random perturbation based on combinatorial contrastive learning, while the latter builds an explainer to derive crucial code statements that are most decisive to the detected vulnerability via dual-view causal inference as explanations. We apply Coca over three typical GNN-based vulnerability detectors. Experimental results show that Coca can effectively mitigate the spurious correlation issue, and provide more useful high-quality explanations.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "To appear in the Technical Track of ICSE 2024"
    },
    {
        "paper id": "2401.14935",
        "abstract url": "https://arxiv.org/abs/2401.14935",
        "title": "Appropriateness of LLM-equipped Robotic Well-being Coach Language in the Workplace: A Qualitative Evaluation",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Robotic coaches have been recently investigated to promote mental well-being in various contexts such as workplaces and homes. With the widespread use of Large Language Models (LLMs), HRI researchers are called to consider language appropriateness when using such generated language for robotic mental well-being coaches in the real world. Therefore, this paper presents the first work that investigated the language appropriateness of robot mental well-being coach in the workplace. To this end, we conducted an empirical study that involved 17 employees who interacted over 4 weeks with a robotic mental well-being coach equipped with LLM-based capabilities. After the study, we individually interviewed them and we conducted a focus group of 1.5 hours with 11 of them. The focus group consisted of: i) an ice-breaking activity, ii) evaluation of robotic coach language appropriateness in various scenarios, and iii) listing shoulds and shouldn'ts for designing appropriate robotic coach language for mental well-being. From our qualitative evaluation, we found that a language-appropriate robotic coach should (1) ask deep questions which explore feelings of the coachees, rather than superficial questions, (2) express and show emotional and empathic understanding of the context, and (3) not make any assumptions without clarifying with follow-up questions to avoid bias and stereotyping. These results can inform the design of language-appropriate robotic coach to promote mental well-being in real-world contexts.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14964",
        "abstract url": "https://arxiv.org/abs/2401.14964",
        "title": "AiRLIHockey: Highly Reactive Contact Control and Stochastic Optimal Shooting",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Air hockey is a highly reactive game which requires the player to quickly reason over stochastic puck and contact dynamics. We implement a hierarchical framework which combines stochastic optimal control for planning shooting angles and sampling-based model-predictive control for continuously generating constrained mallet trajectories. Our agent was deployed and evaluated in simulation and on a physical setup as part of the Robot Air-Hockey challenge competition at NeurIPS 2023.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14988",
        "abstract url": "https://arxiv.org/abs/2401.14988",
        "title": "Modulating function based algebraic observer coupled with stable output predictor for LTV and sampled-data systems",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "This paper proposes an algebraic observer-based modulating function approach for linear time-variant systems and a class of nonlinear systems with discrete measurements. The underlying idea lies in constructing an observability transformation that infers some properties of the modulating function approach for designing such algebraic observers. First, we investigate the algebraic observer design for linear time-variant systems under an observable canonical form for continuous-time measurements. Then, we provide the convergence of the observation error in an L2-gain stability sense. Next, we develop an exponentially stable sampled-data observer which relies on the design of the algebraic observer and an output predictor to achieve state estimation from available measurements and under small inter-sampling periods. Using a trajectory-based approach, we prove the convergence of the observation error within a convergence rate that can be adjusted through the fixed time-horizon length of the modulating function and the upper bound of the sampling period. Furthermore, robustness of the sampled-data algebraic observer, which yields input-to-state stability, is inherited by the modulating kernel and the closed-loop output predictor design. Finally, we discuss the implementation procedure of the MF-based observer realization, demonstrate the applicability of the algebraic observer, and illustrate its performance through two examples given by linear time-invariant and linear time-variant systems with nonlinear input-output injection terms.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "15 pages, 9 figures, submitted to Automatica"
    },
    {
        "paper id": "2401.15022",
        "abstract url": "https://arxiv.org/abs/2401.15022",
        "title": "Applications of artificial intelligence in the analysis of histopathology images of gliomas: a review",
        "rating": -1,
        "keywords": [
            [
                "survival",
                "diagnosis",
                "Cancer",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "In recent years, the diagnosis of gliomas has become increasingly complex. Analysis of glioma histopathology images using artificial intelligence (AI) offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies that have proposed AI-based methods for whole-slide histopathology images of human gliomas, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in isolation (10/70) or in addition to the TCGA datasets (11/70). Current approaches mostly rely on convolutional neural networks (53/70) for analyzing tissue at 20x magnification (30/70). A new field of research is the integration of clinical data, omics data, or magnetic resonance imaging (27/70). So far, AI-based methods have achieved promising results, but are not yet used in real clinical settings. Future work should focus on the independent validation of methods on larger, multi-site datasets with high-quality and up-to-date clinical and molecular pathology annotations to demonstrate routine applicability.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15024",
        "abstract url": "https://arxiv.org/abs/2401.15024",
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
        "rating": -1.0,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression",
        "subjects": [
            "cs.LG"
        ],
        "comment": "22 pages, 8 figures, accepted at ICLR24"
    },
    {
        "paper id": "2401.15026",
        "abstract url": "https://arxiv.org/abs/2401.15026",
        "title": "Multi-Agent Coordination for a Partially Observable and Dynamic Robot Soccer Environment with Limited Communication",
        "rating": -1,
        "keywords": [
            [
                "robotics",
                "Robot"
            ]
        ],
        "abstract": "RoboCup represents an International testbed for advancing research in AI and robotics, focusing on a definite goal: developing a robot team that can win against the human world soccer champion team by the year 2050. To achieve this goal, autonomous humanoid robots' coordination is crucial. This paper explores novel solutions within the RoboCup Standard Platform League (SPL), where a reduction in WiFi communication is imperative, leading to the development of new coordination paradigms. The SPL has experienced a substantial decrease in network packet rate, compelling the need for advanced coordination architectures to maintain optimal team functionality in dynamic environments. Inspired by market-based task assignment, we introduce a novel distributed coordination system to orchestrate autonomous robots' actions efficiently in low communication scenarios. This approach has been tested with NAO robots during official RoboCup competitions and in the SimRobot simulator, demonstrating a notable reduction in task overlaps in limited communication settings.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "International Conference of the Italian Association for Artificial Intelligence (AIxIA 2023) - Italian Workshop on Artificial Intelligence and Robotics (AIRO) Rome, 6 - 9 November, 2023"
    },
    {
        "paper id": "2401.15036",
        "abstract url": "https://arxiv.org/abs/2401.15036",
        "title": "Distributed Simultaneous Localisation and Auto-Calibration using Gaussian Belief Propagation",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "We present a novel scalable, fully distributed, and online method for simultaneous localisation and extrinsic calibration for multi-robot setups. Individual a priori unknown robot poses are probabilistically inferred as robots sense each other while simultaneously calibrating their sensors and markers extrinsic using Gaussian Belief Propagation. In the presented experiments, we show how our method not only yields accurate robot localisation and auto-calibration but also is able to perform under challenging circumstances such as highly noisy measurements, significant communication failures or limited communication range.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Published in IEEE Robotics and Automation Letters (RA-L) 2024"
    },
    {
        "paper id": "2401.15043",
        "abstract url": "https://arxiv.org/abs/2401.15043",
        "title": "Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning",
        "rating": -1,
        "keywords": [
            [
                "Health",
                "Cancer",
                "Disease"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Objective: The reading level of health educational materials significantly influences the understandability and accessibility of the information, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality. Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research, comprising educational content from the American Cancer Society, Centers for Disease Control and Prevention, and National Cancer Institute. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based approaches. Our experimentation encompasses Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a lightweight model adept at distinguishing between original and simplified texts, thereby enhancing the model's effectiveness with unlabeled data. Results: Fine-tuned Llama 2 models demonstrated high performance across various metrics. Our innovative RLHF reward function surpassed existing RL text simplification reward functions in effectiveness. The results underscore that RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text and improving performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15048",
        "abstract url": "https://arxiv.org/abs/2401.15048",
        "title": "Unrecognizable Yet Identifiable: Image Distortion with Preserved Embeddings",
        "rating": -1,
        "keywords": [
            [
                "biometric",
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the realm of security applications, biometric authentication systems play a crucial role, yet one often encounters challenges concerning privacy and security while developing one. One of the most fundamental challenges lies in avoiding storing biometrics directly in the storage but still achieving decently high accuracy. Addressing this issue, we contribute to both artificial intelligence and engineering fields. We introduce an innovative image distortion technique that effectively renders facial images unrecognizable to the eye while maintaining their identifiability by neural network models. From the theoretical perspective, we explore how reliable state-of-the-art biometrics recognition neural networks are by checking the maximal degree of image distortion, which leaves the predicted identity unchanged. On the other hand, applying this technique demonstrates a practical solution to the engineering challenge of balancing security, precision, and performance in biometric authentication systems. Through experimenting on the widely used datasets, we assess the effectiveness of our method in preserving AI feature representation and distorting relative to conventional metrics. We also compare our method with previously used approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15166",
        "abstract url": "https://arxiv.org/abs/2401.15166",
        "title": "Probabilistic Design of Multi-Dimensional Spatially-Coupled Codes",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Because of their excellent asymptotic and finite-length performance, spatially-coupled (SC) codes are a class of low-density parity-check codes that is gaining increasing attention. Multi-dimensional (MD) SC codes are constructed by connecting copies of an SC code via relocations in order to mitigate various sources of non-uniformity and improve performance in many data storage and data transmission systems. As the number of degrees of freedom in the MD-SC code design increases, appropriately exploiting them becomes more difficult because of the complexity growth of the design process. In this paper, we propose a probabilistic framework for the MD-SC code design, which is based on the gradient-descent (GD) algorithm, to design better MD codes and address this challenge. In particular, we express the expected number of short cycles, which we seek to minimize, in the graph representation of the code in terms of entries of a probability-distribution matrix that characterizes the MD-SC code design. We then find a locally-optimal probability distribution, which serves as the starting point of a finite-length algorithmic optimizer that produces the final MD-SC code. We offer the theoretical analysis as well as the algorithms, and we present experimental results demonstrating that our MD codes, conveniently called GD-MD codes, have notably lower short cycle numbers compared with the available state-of-the-art. Moreover, our algorithms converge on solutions in few iterations, which confirms the complexity reduction as a result of limiting the search space via the locally-optimal GD-MD distributions.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "12 pages (double column), 5 figures, the short version has been submitted to the IEEE International Symposium on Information Theory (ISIT)"
    },
    {
        "paper id": "2401.15168",
        "abstract url": "https://arxiv.org/abs/2401.15168",
        "title": "A Self-Healing Mesh Network without Global-Time Synchronization",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In this paper, we propose a slot-based protocol that does not rely on global-time synchronization to achieve a self-healing mesh network. With the proposed protocol, each node synchronizes with its neighbors locally by adjusting its time to transmit based on the reception instant of a decoded beacon signal. Also, it determines its slots without any coordinator to avoid collisions. Finally, to communicate the messages over the mesh network, it identifies the forwarding nodes on the shortest path without knowing the entire communication graph. We show that the proposed protocol can effectively resolve collisions over time while enabling nodes to synchronize with each other in a distributed manner. We numerically analyze the performance of the proposed protocol for different configurations under a realistic channel model considering asymmetrical links. We also implement the proposed method in practice with \\ac{LoRa} devices. We demonstrate that the nodes adapt themselves to changes in the network and deliver a message from a sensing node to a reference node via multi-hop routing.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This work has been accepted for presentation at IEEE ICC 2024"
    },
    {
        "paper id": "2401.15174",
        "abstract url": "https://arxiv.org/abs/2401.15174",
        "title": "LaMI: Large Language Models for Multi-Modal Human-Robot Interaction",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "This paper presents an innovative large language model (LLM)-based robotic system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI systems relied on complex designs for intent estimation, reasoning, and behavior generation, which were resource-intensive. In contrast, our system empowers researchers and practitioners to regulate robot behavior through three key aspects: providing high-level linguistic guidance, creating \"atomic actions\" and expressions the robot can use, and offering a set of examples. Implemented on a physical robot, it demonstrates proficiency in adapting to multi-modal inputs and determining the appropriate manner of action to assist humans with its arms, following researchers' defined guidelines. Simultaneously, it coordinates the robot's lid, neck, and ear movements with speech output to produce dynamic, multi-modal expressions. This showcases the system's potential to revolutionize HRI by shifting from conventional, manual state-and-flow design methods to an intuitive, guidance-based, and example-driven approach. Supplementary material can be found at https://hri-eu.github.io/Lami/",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2401.15185",
        "abstract url": "https://arxiv.org/abs/2401.15185",
        "title": "Towards a Theory of Control Architecture: A quantitative framework for layered multi-rate control",
        "rating": -1,
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "This paper focuses on the need for a rigorous theory of layered control architectures (LCAs) for complex engineered and natural systems, such as power systems, communication networks, autonomous robotics, bacteria, and human sensorimotor control. All deliver extraordinary capabilities, but they lack a coherent theory of analysis and design, partly due to the diverse domains across which LCAs can be found. In contrast, there is a core universal set of control concepts and theory that applies very broadly and accommodates necessary domain-specific specializations. However, control methods are typically used only to design algorithms in components within a larger system designed by others, typically with minimal or no theory. This points towards a need for natural but large extensions of robust performance from control to the full decision and control stack. It is encouraging that the successes of extant architectures from bacteria to the Internet are due to strikingly universal mechanisms and design patterns. This is largely due to convergent evolution by natural selection and not intelligent design, particularly when compared with the sophisticated design of components. Our aim here is to describe the universals of architecture and sketch tentative paths towards a useful design theory.",
        "subjects": [
            "math.OC"
        ],
        "comment": "Submitted to IEEE Control Systems Magazine"
    },
    {
        "paper id": "2401.15203",
        "abstract url": "https://arxiv.org/abs/2401.15203",
        "title": "FedGT: Federated Node Classification with Scalable Graph Transformer",
        "rating": -1.0,
        "keywords": [
            [
                "federated learning"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Graphs are widely used to model relational data. As graphs are getting larger and larger in real-world scenarios, there is a trend to store and compute subgraphs in multiple local systems. For example, recently proposed \\emph{subgraph federated learning} methods train Graph Neural Networks (GNNs) distributively on local subgraphs and aggregate GNN parameters with a central server. However, existing methods have the following limitations: (1) The links between local subgraphs are missing in subgraph federated learning. This could severely damage the performance of GNNs that follow message-passing paradigms to update node/edge features. (2) Most existing methods overlook the subgraph heterogeneity issue, brought by subgraphs being from different parts of the whole graph. To address the aforementioned challenges, we propose a scalable \\textbf{Fed}erated \\textbf{G}raph \\textbf{T}ransformer (\\textbf{FedGT}) in the paper. Firstly, we design a hybrid attention scheme to reduce the complexity of the Graph Transformer to linear while ensuring a global receptive field with theoretical bounds. Specifically, each node attends to the sampled local neighbors and a set of curated global nodes to learn both local and global information and be robust to missing links. The global nodes are dynamically updated during training with an online clustering algorithm to capture the data distribution of the corresponding local subgraph. Secondly, FedGT computes clients' similarity based on the aligned global nodes with optimal transport. The similarity is then used to perform weighted averaging for personalized aggregation, which well addresses the data heterogeneity problem. Moreover, local differential privacy is applied to further protect the privacy of clients. Finally, extensive experimental results on 6 datasets and 2 subgraph settings demonstrate the superiority of FedGT.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICLR 24 submission"
    },
    {
        "paper id": "2401.15204",
        "abstract url": "https://arxiv.org/abs/2401.15204",
        "title": "LYT-Net: Lightweight YUV Transformer-based Network for Low-Light Image Enhancement",
        "rating": -1,
        "keywords": [
            [
                "Image Enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, deep learning-based solutions have proven successful in the domains of image enhancement. This paper introduces LYT-Net, or Lightweight YUV Transformer-based Network, as a novel approach for low-light image enhancement. The proposed architecture, distinct from conventional Retinex-based models, leverages the YUV color space's natural separation of luminance (Y) and chrominance (U and V) to simplify the intricate task of disentangling light and color information in images. By utilizing the strengths of transformers, known for their capability to capture long-range dependencies, LYT-Net ensures a comprehensive contextual understanding of the image while maintaining reduced model complexity. By employing a novel hybrid loss function, our proposed method achieves state-of-the-art results on low-light image enhancement datasets, all while being considerably more compact than its counterparts. The source code and pre-trained models are available at https://github.com/albrateanu/LYT-Net",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 6 figures, submitted to ICIP"
    },
    {
        "paper id": "2401.15206",
        "abstract url": "https://arxiv.org/abs/2401.15206",
        "title": "Backscatter Measurements and Models for RF Sensing Applications in Cluttered Environments",
        "rating": -1,
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "A statistical backscatter channel model for indoor clutter is developed for indoor RF sensing applications based on measurements. A narrowband 28 GHz sounder used a quazi-monostatic radar arrangement with an omnidirectional transmit antenna illuminating an indoor scene and a spinning horn receive antenna less than 1 m away collecting backscattered power as a function of azimuth. Median average backscatter power was found to vary over a 12 dB range, with average power generally decreasing with increasing room size. A deterministic model of average backscattered power dependent on distance to nearest wall and wall reflection coefficient reproduces observations with 4.0 dB RMS error. Distribution of power variation in azimuth around this average is reproduced within 1 dB by a random azimuth spectrum with a lognormal amplitude distribution and uniformly random phase. The model is extended to provide power distribution over both azimuth and delay (conveying range to scatterer) by combining azimuthal distribution with published results on power delay profiles in reverberant environments. The statistical model does not require a detailed room layout description, aiming to reproduce backscatter clutter statistics, as opposed to a deterministic response.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15222",
        "abstract url": "https://arxiv.org/abs/2401.15222",
        "title": "Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection",
        "rating": -1,
        "keywords": [
            [
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier. Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared. Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy, 1.7% on unweighted accuracy, and 10% on micro F1 scores. Conclusions: We show that learned weights from our shared model can be effectively transferred to a new partially matched data set, validating the use of transfer learning for clinical text modifiers",
        "subjects": [
            "cs.CL"
        ],
        "comment": "18 pages, 2 figures, 6 tables. To be submitted to the Journal of Biomedical Semantics"
    },
    {
        "paper id": "2401.15235",
        "abstract url": "https://arxiv.org/abs/2401.15235",
        "title": "CascadedGaze: Efficiency in Global Context Extraction for Image Restoration",
        "rating": -1,
        "keywords": [
            [
                "Image Restoration"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Image restoration tasks traditionally rely on convolutional neural networks. However, given the local nature of the convolutional operator, they struggle to capture global information. The promise of attention mechanisms in Transformers is to circumvent this problem, but it comes at the cost of intensive computational overhead. Many recent studies in image restoration have focused on solving the challenge of balancing performance and computational cost via Transformer variants. In this paper, we present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to capture global information for image restoration. The GCE module leverages small kernels across convolutional layers to learn global dependencies, without requiring self-attention. Extensive experimental results show that our approach outperforms a range of state-of-the-art methods on denoising benchmark datasets including both real image denoising and synthetic image denoising, as well as on image deblurring task, while being more computationally efficient.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "16 pages. ArXiV preprint"
    },
    {
        "paper id": "2401.15236",
        "abstract url": "https://arxiv.org/abs/2401.15236",
        "title": "Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones",
        "rating": -1,
        "keywords": [
            [
                "drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Sub-10cm diameter nano-drones are gaining momentum thanks to their applicability in scenarios prevented to bigger flying drones, such as in narrow environments and close to humans. However, their tiny form factor also brings their major drawback: ultra-constrained memory and processors for the onboard execution of their perception pipelines. Therefore, lightweight deep learning-based approaches are becoming increasingly popular, stressing how computational efficiency and energy-saving are paramount as they can make the difference between a fully working closed-loop system and a failing one. In this work, to maximize the exploitation of the ultra-limited resources aboard nano-drones, we present a novel adaptive deep learning-based mechanism for the efficient execution of a vision-based human pose estimation task. We leverage two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different regression performance vs. computational costs trade-offs. By combining these CNNs with three novel adaptation strategies based on the output's temporal consistency and on auxiliary tasks to swap the CNN being executed proactively, we present six different systems. On a real-world dataset and the actual nano-drone hardware, our best-performing system, compared to executing only the bigger and most accurate SoA model, shows 28% latency reduction while keeping the same mean absolute error (MAE), 3% MAE reduction while being iso-latency, and the absolute peak performance, i.e., 6% better than SoA model.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted for publication in the 2024 Design, Automation and Test in Europe (DATE) conference"
    },
    {
        "paper id": "2401.15239",
        "abstract url": "https://arxiv.org/abs/2401.15239",
        "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
        "rating": -1,
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "To Appear in IEEE Symposium on Security and Privacy 2024 (IEEE S&P 2024), MAY 20-23, 2024, SAN FRANCISCO, CA, USA"
    },
    {
        "paper id": "2401.15245",
        "abstract url": "https://arxiv.org/abs/2401.15245",
        "title": "GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "This paper presents a plugin that adds a representation of homogeneous and heterogeneous, optically thick, translucent materials on the Blender 3D modeling tool. The working principle of this plugin is based on a combination of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based subsurface scattering method (GenSSS). The proposed plugin has been implemented using Mitsuba renderer, which is an open source rendering software. The proposed plugin has been validated on measured subsurface scattering data. It's shown that the proposed plugin visualizes homogeneous and heterogeneous subsurface scattering effects, accurately, compactly and computationally efficiently.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15262",
        "abstract url": "https://arxiv.org/abs/2401.15262",
        "title": "Asymptotic Behavior of Adversarial Training Estimator under $\\ell_\\infty$-Perturbation",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Adversarial training has been proposed to hedge against adversarial attacks in machine learning and statistical models. This paper focuses on adversarial training under $\\ell_\\infty$-perturbation, which has recently attracted much research attention. The asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. The results imply that the limiting distribution of the adversarial training estimator under $\\ell_\\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. Alternatively, a two-step procedure is proposed -- adaptive adversarial training, which could further improve the performance of adversarial training under $\\ell_\\infty$-perturbation. Specifically, the proposed procedure could achieve asymptotic unbiasedness and variable-selection consistency. Numerical experiments are conducted to show the sparsity-recovery ability of adversarial training under $\\ell_\\infty$-perturbation and to compare the empirical performance between classic adversarial training and adaptive adversarial training.",
        "subjects": [
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15269",
        "abstract url": "https://arxiv.org/abs/2401.15269",
        "title": "Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models",
        "rating": -1,
        "keywords": [
            [
                "biomedical",
                "Medical",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15270",
        "abstract url": "https://arxiv.org/abs/2401.15270",
        "title": "SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models",
        "rating": -1.0,
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to AAAI 2024 (preprint)"
    },
    {
        "paper id": "2401.15285",
        "abstract url": "https://arxiv.org/abs/2401.15285",
        "title": "Ransomware threat mitigation through network traffic analysis and machine learning techniques",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "In recent years, there has been a noticeable increase in cyberattacks using ransomware. Attackers use this malicious software to break into networks and harm computer systems. This has caused significant and lasting damage to various organizations, including government, private companies, and regular users. These attacks often lead to the loss or exposure of sensitive information, disruptions in normal operations, and persistent vulnerabilities. This paper focuses on a method for recognizing and identifying ransomware in computer networks. The approach relies on using machine learning algorithms and analyzing the patterns of network traffic. By collecting and studying this traffic, and then applying machine learning models, we can accurately identify and detect ransomware. The results of implementing this method show that machine learning algorithms can effectively pinpoint ransomware based on network traffic, achieving high levels of precision and accuracy.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.16437",
        "abstract url": "https://arxiv.org/abs/2401.16437",
        "title": "A Benchmark Dataset for Tornado Detection and Prediction using Full-Resolution Polarimetric Weather Radar Data",
        "rating": -1,
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "Weather radar is the primary tool used by forecasters to detect and warn for tornadoes in near-real time. In order to assist forecasters in warning the public, several algorithms have been developed to automatically detect tornadic signatures in weather radar observations. Recently, Machine Learning (ML) algorithms, which learn directly from large amounts of labeled data, have been shown to be highly effective for this purpose. Since tornadoes are extremely rare events within the corpus of all available radar observations, the selection and design of training datasets for ML applications is critical for the performance, robustness, and ultimate acceptance of ML algorithms. This study introduces a new benchmark dataset, TorNet to support development of ML algorithms in tornado detection and prediction. TorNet contains full-resolution, polarimetric, Level-II WSR-88D data sampled from 10 years of reported storm events. A number of ML baselines for tornado detection are developed and compared, including a novel deep learning (DL) architecture capable of processing raw radar imagery without the need for manual feature extraction required for existing ML algorithms. Despite not benefiting from manual feature engineering or other preprocessing, the DL model shows increased detection performance compared to non-DL and operational baselines. The TorNet dataset, as well as source code and model weights of the DL baseline trained in this work, are made freely available.",
        "subjects": [
            "physics.ao-ph"
        ],
        "comment": "37 pages, 15 Figures, 2 Tables"
    },
    {
        "paper id": "2402.01723",
        "abstract url": "https://arxiv.org/abs/2402.01723",
        "title": "An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios",
        "rating": -1,
        "keywords": [
            [
                "Industrial"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions with variants to evaluate LLM robustness. In total, we evaluated 9 different LLMs developed by Chinese vendors, as well as four different LLMs developed by global vendors. Our major findings include: (1) Current LLMs exhibit low accuracy in Chinese industrial contexts, with all LLMs scoring less than 0.6. (2) The robustness scores vary across industrial sectors, and local LLMs overall perform worse than global ones. (3) LLM robustness differs significantly across abilities. Global LLMs are more robust under logical-related variants, while advanced local LLMs perform better on problems related to understanding Chinese industrial terminology. Our study results provide valuable guidance for understanding and promoting the industrial domain capabilities of LLMs from both development and industrial enterprise perspectives. The results further motivate possible research directions and tooling support.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.03352",
        "abstract url": "https://arxiv.org/abs/2402.03352",
        "title": "Zeroth-Order primal-dual Alternating Projection Gradient Algorithms for Nonconvex Minimax Problems with Coupled linear Constraints",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "In this paper, we study zeroth-order algorithms for nonconvex minimax problems with coupled linear constraints under the deterministic and stochastic settings, which have attracted wide attention in machine learning, signal processing and many other fields in recent years, e.g., adversarial attacks in resource allocation problems and network flow problems etc. We propose two single-loop algorithms, namely the zero-order primal-dual alternating projected gradient (ZO-PDAPG) algorithm and the zero-order regularized momentum primal-dual projected gradient algorithm (ZO-RMPDPG), for solving deterministic and stochastic nonconvex-(strongly) concave minimax problems with coupled linear constraints. The iteration complexity of the two proposed algorithms to obtain an $\\varepsilon$-stationary point are proved to be $\\mathcal{O}(\\varepsilon ^{-2})$ (resp. $\\mathcal{O}(\\varepsilon ^{-4})$) for solving nonconvex-strongly concave (resp. nonconvex-concave) minimax problems with coupled linear constraints under deterministic settings and $\\tilde{\\mathcal{O}}(\\varepsilon ^{-3})$ (resp. $\\tilde{\\mathcal{O}}(\\varepsilon ^{-6.5})$) under stochastic settings respectively. To the best of our knowledge, they are the first two zeroth-order algorithms with iterative complexity guarantees for solving nonconvex-(strongly) concave minimax problems with coupled linear constraints under the deterministic and stochastic settings.",
        "subjects": [
            "math.OC"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2212.04672"
    },
    {
        "paper id": "2402.09434",
        "abstract url": "https://arxiv.org/abs/2402.09434",
        "title": "Disentangling Imperfect: A Wavelet-Infused Multilevel Heterogeneous Network for Human Activity Recognition in Flawed Wearable Sensor Data",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "The popularity and diffusion of wearable devices provides new opportunities for sensor-based human activity recognition that leverages deep learning-based algorithms. Although impressive advances have been made, two major challenges remain. First, sensor data is often incomplete or noisy due to sensor placement and other issues as well as data transmission failure, calling for imputation of missing values, which also introduces noise. Second, human activity has multi-scale characteristics. Thus, different groups of people and even the same person may behave differently under different circumstances. To address these challenges, we propose a multilevel heterogeneous neural network, called MHNN, for sensor data analysis. We utilize multilevel discrete wavelet decomposition to extract multi-resolution features from sensor data. This enables distinguishing signals with different frequencies, thereby suppressing noise. As the components resulting from the decomposition are heterogeneous, we equip the proposed model with heterogeneous feature extractors that enable the learning of multi-scale features. Due to the complementarity of these features, we also include a cross aggregation module for enhancing their interactions. An experimental study using seven publicly available datasets offers evidence that MHNN can outperform other cutting-edge models and offers evidence of robustness to missing values and noise. An ablation study confirms the importance of each module.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "14 pages, 7 figures"
    },
    {
        "paper id": "2401.14694",
        "abstract url": "https://arxiv.org/abs/2401.14694",
        "title": "TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records",
        "rating": -1.5,
        "keywords": [
            [
                "medical",
                "Health",
                "healthcare",
                "diagnosis",
                "disease",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elapsed times between visits. For interpretability, we propose employing a dual-level attention mechanism that operates between visits and features within each visit. Results: The results of the experiments conducted on Alzheimer's Disease Neuroimaging Initiative (ADNI) and National Alzheimer's Coordinating Center (NACC) datasets indicated superior performance of proposed models for predicting Alzheimer's Disease (AD) compared to state-of-the-art and baseline approaches based on F2 and sensitivity. Additionally, TA-RNN showed superior performance on Medical Information Mart for Intensive Care (MIMIC-III) dataset for mortality prediction. In our ablation study, we observed enhanced predictive performance by incorporating time embedding and attention mechanisms. Finally, investigating attention weights helped identify influential visits and features in predictions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14702",
        "abstract url": "https://arxiv.org/abs/2401.14702",
        "title": "FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently",
        "rating": -1.5,
        "keywords": [
            [
                "depth"
            ],
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and more important concern as GCNs are adopted in many crucial applications. Societal biases against sensitive groups may exist in many real world graphs. GCNs trained on those graphs may be vulnerable to being affected by such biases. In this paper, we adopt the well-known fairness notion of demographic parity and tackle the challenge of training fair and accurate GCNs efficiently. We present an in-depth analysis on how graph structure bias, node attribute bias, and model parameters may affect the demographic parity of GCNs. Our insights lead to FairSample, a framework that jointly mitigates the three types of biases. We employ two intuitive strategies to rectify graph structures. First, we inject edges across nodes that are in different sensitive groups but similar in node features. Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learning. To address the bias in node features and model parameters, FairSample is complemented by a regularization objective to optimize fairness.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by TKDE 2023"
    },
    {
        "paper id": "2401.14743",
        "abstract url": "https://arxiv.org/abs/2401.14743",
        "title": "Synthetic Multimodal Dataset for Empowering Safety and Well-being in Home Environments",
        "rating": -1.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents a synthetic multimodal dataset of daily activities that fuses video data from a 3D virtual space simulator with knowledge graphs depicting the spatiotemporal context of the activities. The dataset is developed for the Knowledge Graph Reasoning Challenge for Social Issues (KGRC4SI), which focuses on identifying and addressing hazardous situations in the home environment. The dataset is available to the public as a valuable resource for researchers and practitioners developing innovative solutions recognizing human behaviors to enhance safety and well-being in",
        "subjects": [
            "cs.AI"
        ],
        "comment": "7 pages, 2 figures,4 tables"
    },
    {
        "paper id": "2401.14876",
        "abstract url": "https://arxiv.org/abs/2401.14876",
        "title": "Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem",
        "rating": -1.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "kernel learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the context of GCNs. This serves as a foundation for combining it with the attribute-based filter to capture the adaptive-frequency information. Finally, we derive the cross-space filter via an effective multiple-kernel learning strategy, which unifies the attribute-based high-pass filter and the topology-based low-pass filter. This helps to address the over-smoothing problem while maintaining effectiveness. Extensive experiments demonstrate that CSF not only successfully alleviates the over-smoothing problem but also promotes the effectiveness of the node classification task.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to WWW 2024. V2: update the results on GCN-BC based on our rebuttal on OpenReview. Our code is available at https://github.com/huangzichun/Cross-Space-Adaptive-Filter"
    },
    {
        "paper id": "2401.14933",
        "abstract url": "https://arxiv.org/abs/2401.14933",
        "title": "SSDOnt: an Ontology for representing Single-Subject Design Studies",
        "rating": -1.5,
        "keywords": [
            [
                "biomedicine"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Background: Single-Subject Design is used in several areas such as education and biomedicine. However, no suited formal vocabulary exists for annotating the detailed configuration and the results of this type of research studies with the appropriate granularity for looking for information about them. Therefore, the search for those study designs relies heavily on a syntactical search on the abstract, keywords or full text of the publications about the study, which entails some limitations. Objective: To present SSDOnt, a specific purpose ontology for describing and annotating single-subject design studies, so that complex questions can be asked about them afterwards. Methods: The ontology was developed following the NeOn methodology. Once the requirements of the ontology were defined, a formal model was described in a Description Logic and later implemented in the ontology language OWL 2 DL. Results: We show how the ontology provides a reference model with a suitable terminology for the annotation and searching of single-subject design studies and their main components, such as the phases, the intervention types, the outcomes and the results. Some mappings with terms of related ontologies have been established. We show as proof-of-concept that classes in the ontology can be easily extended to annotate more precise information about specific interventions and outcomes such as those related to autism. Moreover, we provide examples of some types of queries that can be posed to the ontology. Conclusions: SSDOnt has achieved the purpose of covering the descriptions of the domain of single-subject research studies.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "This document is the Accepted Manuscript version of a Published Work that appeared in final form in Methods of Information in Medicine 57(01/02) : 55-61 (2018), copyright 2018 Schattauer. To access the final edited and published work see https://doi.org/10.3414/ME17-01-0109"
    },
    {
        "paper id": "2401.15062",
        "abstract url": "https://arxiv.org/abs/2401.15062",
        "title": "Expert with Clustering: Hierarchical Online Preference Learning Framework",
        "rating": -1.5,
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes. Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation. In this study, we consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users. We introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice. EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric. This metric is instrumental in generating more representative cluster centroids. In a recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options, our algorithm achieves a regret bound of $O(N\\sqrt{T\\log K} + NT)$. This bound consists of two parts: the first term is the regret from the Hedge algorithm, and the second term depends on the average loss from clustering. The algorithm performs with low regret, especially when a latent hierarchical structure exists among users. This regret bound underscores the theoretical and experimental efficacy of EWC, particularly in scenarios that demand rapid learning and adaptation. Experimental results highlight that EWC can substantially reduce regret by 27.57% compared to the LinUCB baseline. Our work offers a data-efficient approach to capturing both individual and collective behaviors, making it highly applicable to contexts with hierarchical structures. We expect the algorithm to be applicable to other settings with layered nuances of user preferences and information.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This work was submitted to L4DC 2024"
    },
    {
        "paper id": "2401.15268",
        "abstract url": "https://arxiv.org/abs/2401.15268",
        "title": "Towards Stable Preferences for Stakeholder-aligned Machine Learning",
        "rating": -1.5,
        "keywords": [
            [
                "organ"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of organ transplantation, contributing to more transparent and equitable practices while promoting the integration of moral values into algorithmic decision-making.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Work in Progress"
    },
    {
        "paper id": "2401.15290",
        "abstract url": "https://arxiv.org/abs/2401.15290",
        "title": "Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset",
        "rating": -1.5,
        "keywords": [
            [
                "Medical",
                "health",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Electronic health record (EHR) is more and more popular, and it comes with applying machine learning solutions to resolve various problems in the domain. This growing research area also raises the need for EHRs accessibility. Medical Information Mart for Intensive Care (MIMIC) dataset is a popular, public, and free EHR dataset in a raw format that has been used in numerous studies. However, despite of its popularity, it is lacking benchmarking work, especially with recent state of the art works in the field of deep learning with time-series tabular data. The aim of this work is to fill this lack by providing a benchmark for latest version of MIMIC dataset, MIMIC-IV. We also give a detailed literature survey about studies that has been already done for MIIMIC-III.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "7 pages, 1 figure, 3 tables"
    },
    {
        "paper id": "2402.01718",
        "abstract url": "https://arxiv.org/abs/2402.01718",
        "title": "Business Models for Digitalization Enabled Energy Efficiency and Flexibility in Industry: A Survey with Nine Case Studies",
        "rating": -1.5,
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Digitalization is challenging in heavy industrial sectors, and many pi-lot projects facing difficulties to be replicated and scaled. Case studies are strong pedagogical vehicles for learning and sharing experience & knowledge, but rarely available in the literature. Therefore, this paper conducts a survey to gather a diverse set of nine industry cases, which are subsequently subjected to analysis using the business model canvas (BMC). The cases are summarized and compared based on nine BMC components, and a Value of Business Model (VBM) evaluation index is proposed to assess the business potential of industrial digital solutions. The results show that the main partners are industry stakeholders, IT companies and academic institutes. Their key activities for digital solutions include big-data analysis, machine learning algorithms, digital twins, and internet of things developments. The value propositions of most cases are improving energy efficiency and enabling energy flexibility. Moreover, the technology readiness levels of six industrial digital solutions are under level 7, indicating that they need further validation in real-world environments. Building upon these insights, this paper proposes six recommendations for future industrial digital solution development: fostering cross-sector collaboration, prioritizing comprehensive testing and validation, extending value propositions, enhancing product adaptability, providing user-friendly platforms, and adopting transparent recommendations.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.03355",
        "abstract url": "https://arxiv.org/abs/2402.03355",
        "title": "Unlocking Criminal Hierarchies: A Survey, Experimental, and Comparative Exploration of Techniques for Identifying Leaders within Criminal Networks",
        "rating": -1.5,
        "keywords": [
            [
                "crime"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "This survey paper offers a thorough analysis of techniques and algorithms used in the identification of crime leaders within criminal networks. For each technique, the paper examines its effectiveness, limitations, potential for improvement, and future prospects. The main challenge faced by existing survey papers focusing on algorithms for identifying crime leaders and predicting crimes is effectively categorizing these algorithms. To address this limitation, this paper proposes a new methodological taxonomy that hierarchically classifies algorithms into more detailed categories and specific techniques. The paper includes empirical and experimental evaluations to rank the different techniques. The combination of the methodological taxonomy, empirical evaluations, and experimental comparisons allows for a nuanced and comprehensive understanding of the techniques and algorithms for identifying crime leaders, assisting researchers in making informed decisions. Moreover, the paper offers valuable insights into the future prospects of techniques for identifying crime leaders, emphasizing potential advancements and opportunities for further research. Here's an overview of our empirical analysis findings and experimental insights, along with the solution we've devised: (1) PageRank and Eigenvector centrality are reliable for mapping network connections, (2) Katz Centrality can effectively identify influential criminals through indirect links, stressing their significance in criminal networks, (3) current models fail to account for the specific impacts of criminal influence levels, the importance of socio-economic context, and the dynamic nature of criminal networks and hierarchies, and (4) we propose enhancements, such as incorporating temporal dynamics and sentiment analysis to reflect the fluidity of criminal activities and relationships",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10921",
        "abstract url": "https://arxiv.org/abs/2402.10921",
        "title": "AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning",
        "rating": -1.5,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, the model learns to emphasize on the correlated patterns across modalities, which may help align the cross-attended mode-specific descriptors pairwise within a joint-embedding space and thereby compensate for missing modalities during inference. By leveraging the spatio-temporal details at the dialogue level, the proposed AM^2-EmoJE not only demonstrates superior performance compared to the best-performing state-of-the-art multimodal methods, by effectively leveraging body language in place of face expression, it also exhibits an enhanced privacy feature. By reporting around 2-5% improvement in the weighted-F1 score, the proposed multimodal joint embedding module facilitates an impressive performance gain in a variety of missing-modality query scenarios during test time.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14661",
        "abstract url": "https://arxiv.org/abs/2401.14661",
        "title": "From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution",
        "rating": -2,
        "keywords": [
            [
                "Super Resolution"
            ],
            [
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The demand for accurate object detection in aerial imagery has surged with the widespread use of drones and satellite technology. Traditional object detection models, trained on datasets biased towards large objects, struggle to perform optimally in aerial scenarios where small, densely clustered objects are prevalent. To address this challenge, we present an innovative approach that combines super-resolution and an adapted lightweight YOLOv5 architecture. We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5 architecture features Transformer encoder blocks, allowing the model to capture global context and context information, leading to improved detection results, especially in high-density, occluded conditions. This lightweight model not only delivers improved accuracy but also ensures efficient resource utilization, making it well-suited for real-time applications. Our experimental results demonstrate the model's superior performance in detecting small and densely clustered objects, underlining the significance of dataset choice and architectural adaptation for this specific task. In particular, the method achieves 52.5% mAP on VisDrone, exceeding top prior works. This approach promises to significantly advance object detection in aerial imagery, contributing to more accurate and reliable results in a variety of real-world applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14683",
        "abstract url": "https://arxiv.org/abs/2401.14683",
        "title": "Generating Shuttling Procedures for Constrained Silicon Quantum Dot Array",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "In silicon quantum computers, a single electron is trapped in a microstructure called a quantum dot, and its spin is used as a qubit. For large-scale integration of qubits, we previously proposed an approach of arranging the quantum dots in a two-dimensional array and sharing a control gate in a row or column of the array. In our array, the shuttling of electrons is a useful technique to operate the target qubit independently and avoid crosstalk. However, since the shuttling is also conducted using shared control gates, the movement of qubits is complexly constrained. We therefore propose a formal model on the basis of state transition systems to describe those constraints and operation procedures on the array. We also present an approach to generate operation procedures under the constraints. Utilizing this approach, we present a concrete method for our 16 $\\times$ 8 quantum dot array. By implementing the proposed method as a quantum compiler, we confirmed that it is possible to generate operation procedures in a practical amount of time for arbitrary quantum circuits. We also demonstrated that crosstalk can be avoided by shuttling and that the fidelity in that case is higher than when crosstalk is not avoided.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14705",
        "abstract url": "https://arxiv.org/abs/2401.14705",
        "title": "Additional Look into GAN-based Augmentation for Deep Learning COVID-19 Image Classification",
        "rating": -2,
        "keywords": [
            [
                "GAN"
            ],
            [
                "medical",
                "X-ray"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The availability of training data is one of the main limitations in deep learning applications for medical imaging. Data augmentation is a popular approach to overcome this problem. A new approach is a Machine Learning based augmentation, in particular usage of Generative Adversarial Networks (GAN). In this case, GANs generate images similar to the original dataset so that the overall training data amount is bigger, which leads to better performance of trained networks. A GAN model consists of two networks, a generator and a discriminator interconnected in a feedback loop which creates a competitive environment. This work is a continuation of the previous research where we trained StyleGAN2-ADA by Nvidia on the limited COVID-19 chest X-ray image dataset. In this paper, we study the dependence of the GAN-based augmentation performance on dataset size with a focus on small samples. Two datasets are considered, one with 1000 images per class (4000 images in total) and the second with 500 images per class (2000 images in total). We train StyleGAN2-ADA with both sets and then, after validating the quality of generated images, we use trained GANs as one of the augmentations approaches in multi-class classification problems. We compare the quality of the GAN-based augmentation approach to two different approaches (classical augmentation and no augmentation at all) by employing transfer learning-based classification of COVID-19 chest X-ray images. The results are quantified using different classification quality metrics and compared to the results from the literature. The GAN-based augmentation approach is found to be comparable with classical augmentation in the case of medium and large datasets but underperforms in the case of smaller datasets. The correlation between the size of the original dataset and the quality of classification is visible independently from the augmentation approach.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Submitted to Machine Graphics & Vision. Version with updated acknowledgments"
    },
    {
        "paper id": "2401.14736",
        "abstract url": "https://arxiv.org/abs/2401.14736",
        "title": "How does Simulation-based Testing for Self-driving Cars match Human Perception?",
        "rating": -2,
        "keywords": [
            [
                "quality assessment"
            ]
        ],
        "abstract": "Software metrics such as coverage and mutation scores have been extensively explored for the automated quality assessment of test suites. While traditional tools rely on such quantifiable software metrics, the field of self-driving cars (SDCs) has primarily focused on simulation-based test case generation using quality metrics such as the out-of-bound (OOB) parameter to determine if a test case fails or passes. However, it remains unclear to what extent this quality metric aligns with the human perception of the safety and realism of SDCs, which are critical aspects in assessing SDC behavior. To address this gap, we conducted an empirical study involving 50 participants to investigate the factors that determine how humans perceive SDC test cases as safe, unsafe, realistic, or unrealistic. To this aim, we developed a framework leveraging virtual reality (VR) technologies, called SDC-Alabaster, to immerse the study participants into the virtual environment of SDC simulators. Our findings indicate that the human assessment of the safety and realism of failing and passing test cases can vary based on different factors, such as the test's complexity and the possibility of interacting with the SDC. Especially for the assessment of realism, the participants' age as a confounding factor leads to a different perception. This study highlights the need for more research on SDC simulation testing quality metrics and the importance of human perception in evaluating SDC behavior.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14819",
        "abstract url": "https://arxiv.org/abs/2401.14819",
        "title": "Endowing Protein Language Models with Structural Knowledge",
        "rating": -2,
        "keywords": [
            [
                "parameter efficiency"
            ],
            [
                "graph"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules. This refined model, termed Protein Structure Transformer (PST), is further pretrained on a small protein structure database, using the same masked language modeling objective as traditional protein language models. Empirical evaluations of PST demonstrate its superior parameter efficiency relative to protein language models, despite being pretrained on a dataset comprising only 542K structures. Notably, PST consistently outperforms the state-of-the-art foundation model for protein sequences, ESM-2, setting a new benchmark in protein function prediction. Our findings underscore the potential of integrating structural information into protein language models, paving the way for more effective and efficient protein modeling Code and pretrained models are available at https://github.com/BorgwardtLab/PST.",
        "subjects": [
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14823",
        "abstract url": "https://arxiv.org/abs/2401.14823",
        "title": "A Deep Reinforcement Learning-based Approach for Adaptive Handover Protocols in Mobile Networks",
        "rating": -2,
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Due to an ever-increasing number of participants and new areas of application, the demands on mobile communications systems are continually increasing. In order to deliver higher data rates, enable mobility and guarantee QoS requirements of subscribers, these systems and the protocols used are becoming more complex. By using higher frequency spectrums, cells become smaller and more base stations have to be deployed. This leads to an increased number of handovers of user equipments between base stations in order to enable mobility, resulting in potentially more frequent radio link failures and rate reduction. The persistent switching between the same base stations, commonly referred to as \"ping-pong\", leads to a consistent reduction of data rates. In this work, we propose a method for handover optimization by using proximal policy optimization in mobile communications to learn an adaptive handover protocol. The resulting agent is highly flexible regarding different travelling speeds of user equipments, while outperforming the standard 5G NR handover protocol by 3GPP in terms of average data rate and number of radio link failures. Furthermore, the design of the proposed environment demonstrates remarkable accuracy, ensuring a fair comparison with the standard 3GPP protocol.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Submitted to EuCNC"
    },
    {
        "paper id": "2401.14831",
        "abstract url": "https://arxiv.org/abs/2401.14831",
        "title": "The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances",
        "rating": -2,
        "keywords": [
            [
                "Automated Driving"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of our model delivers a structured exploration of entities in a specific domain, their relationships and assigning results of a MV-under-test to construct an entity-relationship graph. Through clustering patterns of relations in the graph general MV deficits are arguable. In Summary, our work contributes to a more nuanced and systematized identification of deficits of a MV test object in correlation to holistic circumstances in HAD operating domains.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted at IEEE IV 2024"
    },
    {
        "paper id": "2401.14898",
        "abstract url": "https://arxiv.org/abs/2401.14898",
        "title": "Decentralized real-time iterations for distributed nonlinear model predictive control",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "This article presents a Real-Time Iteration (RTI) scheme for distributed Nonlinear Model Predictive Control (NMPC). The scheme transfers the well-known RTI approach, a key enabler for many industrial real-time NMPC implementations, to the setting of cooperative distributed control. At each sampling instant, one outer iteration of a bi-level decentralized Sequential Quadratic Programming (dSQP) method is applied to a centralized optimal control croblem. This ensures that real-time requirements are met and it facilitates cooperation between subsystems. Combining novel dSQP convergence results with RTI stability guarantees, we prove local exponential stability under standard assumptions on the MPC design with and without terminal constraints. The proposed scheme only requires neighbor-to-neighbor communication and avoids a central coordinator. A numerical example with coupled inverted pendulums demonstrates the efficacy of the approach.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14906",
        "abstract url": "https://arxiv.org/abs/2401.14906",
        "title": "A High-Performance SurfaceNets Discrete Isocontouring Algorithm",
        "rating": -2,
        "keywords": [
            [
                "biological",
                "medical"
            ]
        ],
        "abstract": "Isocontouring is one of the most widely used visualization techniques. However, many popular contouring algorithms were created prior to the advent of ubiquitous parallel approaches, such as multi-core, shared memory computing systems. With increasing data sizes and computational loads, it is essential to reimagine such algorithms to leverage the increased computing capabilities available today. To this end we have redesigned the SurfaceNets algorithm, a powerful technique which is often employed to isocontour non-continuous, discrete, volumetric scalar fields such as segmentation label maps. Label maps are ubiquitous to medical computing and biological analysis, used in applications ranging from anatomical atlas creation to brain connectomics. This novel Parallel SurfaceNets algorithm has been redesigned using concepts from the high-performance Flying Edges continuous isocontouring algorrithm. It consists of two basic steps, surface extraction followed by constrained smoothing, parallelized over volume edges and employing a double-buffering smoothing approach to guarantee determinism. The algorithm can extract and smooth multiple segmented objects in a single execution, producing a polygonal (triangular/quadrilateral) mesh with points and polygons fully shared between neighboring objects. Performance is typically one to two orders of magnitude faster than the current sequential algorithms for discrete isosurface extraction on small core-count commodity CPU hardware. We demonstrate the effectiveness of the algorithm on five different datasets including human torso and brain atlases, mouse brain segmentation, and electron microscopy connectomics. The software is currently available under a permissive, open source license in the VTK visualization system.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14913",
        "abstract url": "https://arxiv.org/abs/2401.14913",
        "title": "On Repairing Quantum Programs Using ChatGPT",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Automated Program Repair (APR) is a vital area in software engineering aimed at generating automatic patches for vulnerable programs. While numerous techniques have been proposed for repairing classical programs, the realm of quantum programming lacks a comparable automated repair technique. In this initial exploration, we investigate the use of ChatGPT for quantum program repair and evaluate its performance on Bugs4Q, a benchmark suite of quantum program bugs. Our findings demonstrate the feasibility of employing ChatGPT for quantum program repair. Specifically, we assess ChatGPT's ability to address bugs within the Bugs4Q benchmark, revealing its success in repairing 29 out of 38 bugs. This research represents a promising step towards automating the repair process for quantum programs.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "The 5th International Workshop on Quantum Software Engineering (Q-SE 2024)"
    },
    {
        "paper id": "2401.14972",
        "abstract url": "https://arxiv.org/abs/2401.14972",
        "title": "\"It's Sink or Swim'': Exploring Patients' Challenges and Tool Needs for Self-Management of Postoperative Acute Pain",
        "rating": -2,
        "keywords": [
            [
                "health",
                "healthcare"
            ]
        ],
        "abstract": "Poorly managed postoperative acute pain can have long-lasting negative impacts and pose a major healthcare issue. There is limited investigation to understand and address the unique needs of patients experiencing acute pain. In this paper, we tackle this gap through an interview study with 14 patients who recently underwent postoperative acute pain to understand their challenges in pain self-management and their need for supportive tools. Our analysis identified various factors associated with the major aspects of acute pain self-management. Together, our findings indicated that tools for supporting these patients need to carefully consider information and support delivery to adapt to rapid changes in pain experiences, offer personalized and dynamic assistance that adapts to individual situations in context, and monitor emotion when promoting motivation. Overall, our work provided valuable knowledge to address the less-investigated but highly-needed problem of designing technology for the self-management of acute pain and similar health conditions.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "11 pages, CHI '24"
    },
    {
        "paper id": "2401.14979",
        "abstract url": "https://arxiv.org/abs/2401.14979",
        "title": "Creating a vulnerable node based on the vulnerability MS17-010",
        "rating": -2,
        "keywords": [
            [
                "graph"
            ],
            [
                "Attack"
            ]
        ],
        "abstract": "The creation of a vulnerable node has been demonstrated through the analysis and implementation of the MS17-010 (CVE-2017-0144) vulnerability, affecting the SMBv1 protocol on various Windows operating systems. The principle and methodology of exploiting the vulnerability are described, with a formalized representation of the exploitation in the form of a Meta Attack Language (MAL) graph. Additionally, the attacker's implementation is outlined as the execution of an automated script in Python using the Metasploit Framework. Basic security measures for systems utilizing the SMBv1 protocol are provided.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "6 pages, 2 figures"
    },
    {
        "paper id": "2401.15029",
        "abstract url": "https://arxiv.org/abs/2401.15029",
        "title": "Learning Neural Radiance Fields of Forest Structure for Scalable and Fine Monitoring",
        "rating": -2,
        "keywords": [
            [
                "3D",
                "Radiance Fields"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This work leverages neural radiance fields and remote sensing for forestry applications. Here, we show neural radiance fields offer a wide range of possibilities to improve upon existing remote sensing methods in forest monitoring. We present experiments that demonstrate their potential to: (1) express fine features of forest 3D structure, (2) fuse available remote sensing modalities and (3), improve upon 3D structure derived forest metrics. Altogether, these properties make neural fields an attractive computational tool with great potential to further advance the scalability and accuracy of forest monitoring programs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15038",
        "abstract url": "https://arxiv.org/abs/2401.15038",
        "title": "Physical Layer Encryption for Industrial Ethernet in Gigabit Optical Links",
        "rating": -2,
        "keywords": [
            [
                "Industrial"
            ]
        ],
        "abstract": "Industrial Ethernet is a technology widely spread in factory floors and critical infrastructures where a high amount of data need to be collected and transported. Fiber optic networks at gigabit rates fit well with that type of environments where speed, system performance and reliability are critical. In this work a new encryption method for high speed optical communications suitable for such kind of networks is proposed. This new encryption method consists of a symmetric streaming encryption of the 8b/10b data flow at PCS (Physical Coding Sublayer) level. It is carried out thanks to an FPE (Format Preserving Encryption) blockcipher working in CTR (Counter) mode. The overall system has been simulated and implemented in an FPGA (Field Programmable Gate Array). Thanks to experimental results it can be concluded that it is possible to cipher traffic at this physical level in a secure way. In addition, no overhead is introduced during encryption, getting minimum latency and maximum throughput.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15039",
        "abstract url": "https://arxiv.org/abs/2401.15039",
        "title": "Chaotic Encryption Applied to Optical Ethernet in Industrial Control Systems",
        "rating": -2,
        "keywords": [
            [
                "Industrial"
            ]
        ],
        "abstract": "In the past decades, Ethernet has become an alternative technology for the field buses traditionally used in industrial control systems and distributed measurement systems. Among different transmission media in Ethernet standards, optical fiber provides the best bandwidth, excellent immunity to electromagnetic interference, and less signal loses than other wired media. Due to the absence of a standard that provides security at the physical layer of optical Ethernet links, the main motivation of this paper is to propose and implement the necessary modifications to introduce encryption in Ethernet 1000Base-X standard. This has consisted of symmetric streaming encryption of the 8b10b symbols flow at physical coding sublayer level, thanks to a keystream generator based on chaotic algorithm. The overall system has been implemented and tested in an field programmable gate array and Ethernet traffic has been encrypted and transmitted over an optical link. The experimental results show that it is possible to cipher traffic at this level and hide the complete Ethernet traffic pattern from passive eavesdroppers. In addition, no space overhead is introduced in data frames during encryption, achieving the maximum throughput.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15042",
        "abstract url": "https://arxiv.org/abs/2401.15042",
        "title": "PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \\textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \\textit{meta-questions} spanning various domains. Each meta-question contains corresponding \\textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \\textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \\textit{proxy-questions}. We examine multiple LLMs, emphasizing \\textsc{ProxyQA}'s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that evaluating through \\textit{proxy-questions} is a highly self-consistent and human-criteria-correlated validation method. The dataset and leaderboard will be available at \\url{https://github.com/Namco0816/ProxyQA}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15061",
        "abstract url": "https://arxiv.org/abs/2401.15061",
        "title": "Digital-analog hybrid matrix multiplication processor for optical neural networks",
        "rating": -2,
        "keywords": [
            [
                "face"
            ]
        ],
        "abstract": "The computational demands of modern AI have spurred interest in optical neural networks (ONNs) which offer the potential benefits of increased speed and lower power consumption. However, current ONNs face various challenges,most significantly a limited calculation precision (typically around 4 bits) and the requirement for high-resolution signal format converters (digital-to-analogue conversions (DACs) and analogue-to-digital conversions (ADCs)). These challenges are inherent to their analog computing nature and pose significant obstacles in practical implementation. Here, we propose a digital-analog hybrid optical computing architecture for ONNs, which utilizes digital optical inputs in the form of binary words. By introducing the logic levels and decisions based on thresholding, the calculation precision can be significantly enhanced. The DACs for input data can be removed and the resolution of the ADCs can be greatly reduced. This can increase the operating speed at a high calculation precision and facilitate the compatibility with microelectronics. To validate our approach, we have fabricated a proof-of-concept photonic chip and built up a hybrid optical processor (HOP) system for neural network applications. We have demonstrated an unprecedented 16-bit calculation precision for high-definition image processing, with a pixel error rate (PER) as low as $1.8\\times10^{-3}$ at an signal-to-noise ratio (SNR) of 18.2 dB. We have also implemented a convolutional neural network for handwritten digit recognition that shows the same accuracy as the one achieved by a desktop computer. The concept of the digital-analog hybrid optical computing architecture offers a methodology that could potentially be applied to various ONN implementations and may intrigue new research into efficient and accurate domain-specific optical computing architectures for neural networks.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15124",
        "abstract url": "https://arxiv.org/abs/2401.15124",
        "title": "Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle Strength Training Activities",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Maintaining a high quality of life through physical activities (PA) to prevent health decline is crucial. However, the relationship between individuals health status, PA preferences, and motion factors is complex. PA discussions consistently show a positive correlation with healthy aging experiences, but no explicit relation to specific types of musculoskeletal exercises. Taking advantage of the increasingly widespread existence of smartphones, especially in Indonesia, this research utilizes embedded sensors for Human Activity Recognition (HAR). Based on 25 participants data, performing nine types of selected motion, this study has successfully identified important sensor attributes that play important roles in the right and left hands for muscle strength motions as the basis for developing machine learning models with the LSTM algorithm.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "9 pages, 4 figures, AHFE International Conference on Human Factors in Design, Engineering, and Computing"
    },
    {
        "paper id": "2401.15139",
        "abstract url": "https://arxiv.org/abs/2401.15139",
        "title": "FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking",
        "rating": -2,
        "keywords": [
            [
                "biomedical"
            ]
        ],
        "abstract": "In high-dimensional data analysis, such as financial index tracking or biomedical applications, it is crucial to select the few relevant variables while maintaining control over the false discovery rate (FDR). In these applications, strong dependencies often exist among the variables (e.g., stock returns), which can undermine the FDR control property of existing methods like the model-X knockoff method or the T-Rex selector. To address this issue, we have expanded the T-Rex framework to accommodate overlapping groups of highly correlated variables. This is achieved by integrating a nearest neighbors penalization mechanism into the framework, which provably controls the FDR at the user-defined target level. A real-world example of sparse index tracking demonstrates the proposed method's ability to accurately track the S&P 500 index over the past 20 years based on a small number of stocks. An open-source implementation is provided within the R package TRexSelector on CRAN.",
        "subjects": [
            "q-fin.PM"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15183",
        "abstract url": "https://arxiv.org/abs/2401.15183",
        "title": "Moment-based metrics for molecules computable from cryo-EM images",
        "rating": -2,
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "Single particle cryogenic electron microscopy (cryo-EM) is an imaging technique capable of recovering the high-resolution 3-D structure of biological macromolecules from many noisy and randomly oriented projection images. One notable approach to 3-D reconstruction, known as Kam's method, relies on the moments of the 2-D images. Inspired by Kam's method, we introduce a rotationally invariant metric between two molecular structures, which does not require 3-D alignment. Further, we introduce a metric between a stack of projection images and a molecular structure, which is invariant to rotations and reflections and does not require performing 3-D reconstruction. Additionally, the latter metric does not assume a uniform distribution of viewing angles. We demonstrate uses of the new metrics on synthetic and experimental datasets, highlighting their ability to measure structural similarity.",
        "subjects": [
            "q-bio.BM"
        ],
        "comment": "21 Pages, 9 Figures, 2 Algorithms, and 3 Tables"
    },
    {
        "paper id": "2401.15193",
        "abstract url": "https://arxiv.org/abs/2401.15193",
        "title": "Overview of Sensing Attacks on Autonomous Vehicle Technologies and Impact on Traffic Flow",
        "rating": -2,
        "keywords": [
            [
                "LiDAR",
                "Radar",
                "Vehicle"
            ],
            [
                "Attacks"
            ]
        ],
        "abstract": "While perception systems in Connected and Autonomous Vehicles (CAVs), which encompass both communication technologies and advanced sensors, promise to significantly reduce human driving errors, they also expose CAVs to various cyberattacks. These include both communication and sensing attacks, which potentially jeopardize not only individual vehicles but also overall traffic safety and efficiency. While much research has focused on communication attacks, sensing attacks, which are equally critical, have garnered less attention. To address this gap, this study offers a comprehensive review of potential sensing attacks and their impact on target vehicles, focusing on commonly deployed sensors in CAVs such as cameras, LiDAR, Radar, ultrasonic sensors, and GPS. Based on this review, we discuss the feasibility of integrating hardware-in-the-loop experiments with microscopic traffic simulations. We also design baseline scenarios to analyze the macro-level impact of sensing attacks on traffic flow. This study aims to bridge the research gap between individual vehicle sensing attacks and broader macroscopic impacts, thereby laying the foundation for future systemic understanding and mitigation.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15194",
        "abstract url": "https://arxiv.org/abs/2401.15194",
        "title": "Multimodality in Group Communication Research",
        "rating": -2,
        "keywords": [
            [
                "biomedical"
            ]
        ],
        "abstract": "Team interactions are often multisensory, requiring members to pick up on verbal, visual, spatial and body language cues. Multimodal research, research that captures multiple modes of communication such as audio and visual signals, is therefore integral to understanding these multisensory group communication processes. This type of research has gained traction in biomedical engineering and neuroscience, but it is unclear the extent to which communication and management researchers conduct multimodal research. Our study finds that despite its' utility, multimodal research is underutilized in the communication and management literature's. This paper then covers introductory guidelines for creating new multimodal research including considerations for sensors, data integration and ethical considerations.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "27 pages, 3 figures"
    },
    {
        "paper id": "2401.15201",
        "abstract url": "https://arxiv.org/abs/2401.15201",
        "title": "Automatically Detecting Confusion and Conflict During Collaborative Learning Using Linguistic, Prosodic, and Facial Cues",
        "rating": -2,
        "keywords": [
            [
                "Facial"
            ]
        ],
        "abstract": "During collaborative learning, confusion and conflict emerge naturally. However, persistent confusion or conflict have the potential to generate frustration and significantly impede learners' performance. Early automatic detection of confusion and conflict would allow us to support early interventions which can in turn improve students' experience with and outcomes from collaborative learning. Despite the extensive studies modeling confusion during solo learning, there is a need for further work in collaborative learning. This paper presents a multimodal machine-learning framework that automatically detects confusion and conflict during collaborative learning. We used data from 38 elementary school learners who collaborated on a series of programming tasks in classrooms. We trained deep multimodal learning models to detect confusion and conflict using features that were automatically extracted from learners' collaborative dialogues, including (1) language-derived features including TF-IDF, lexical semantics, and sentiment, (2) audio-derived features including acoustic-prosodic features, and (3) video-derived features including eye gaze, head pose, and facial expressions. Our results show that multimodal models that combine semantics, pitch, and facial expressions detected confusion and conflict with the highest accuracy, outperforming all unimodal models. We also found that prosodic cues are more predictive of conflict, and facial cues are more predictive of confusion. This study contributes to the automated modeling of collaborative learning processes and the development of real-time adaptive support to enhance learners' collaborative learning experience in classroom contexts.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "27 pages, 7 figures, 7 tables"
    },
    {
        "paper id": "2401.15265",
        "abstract url": "https://arxiv.org/abs/2401.15265",
        "title": "A method for constructing quaternary Hermitian self-dual codes and an application to quantum codes",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "We introduce quaternary modified four $\u03bc$-circulant codes as a modification of four circulant codes. We give basic properties of quaternary modified four $\u03bc$-circulant Hermitian self-dual codes. We also construct quaternary modified four $\u03bc$-circulant Hermitian self-dual codes having large minimum weights. Two quaternary Hermitian self-dual $[56,28,16]$ codes are constructed for the first time. These codes improve the previously known lower bound on the largest minimum weight among all quaternary (linear) $[56,28]$ codes. In addition, these codes imply the existence of a quantum $[[56,0,16]]$ code.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15279",
        "abstract url": "https://arxiv.org/abs/2401.15279",
        "title": "FabHacks: Transform Everyday Objects into Functional Fixtures",
        "rating": -2,
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Storage, organizing, and decorating are an important part of home design. While one can buy commercial items for many of these tasks, this can be costly, and re-use is more sustainable. An alternative is a \"home hack\", a functional assembly that can be constructed from existing household items. However, coming up with such hacks requires combining objects to make a physically valid design, which might be difficult to test if they are large, require nailing or screwing something to the wall, or the designer has mobility limitations. In this work, we present a design and visualization system for creating workable functional assemblies, FabHacks, which is based on a solver-aided domain-specific language (S-DSL) FabHaL. By analyzing existing home hacks shared online, we create a design abstraction for connecting household items using predefined types of connections. We provide a UI for FabHaL that can be used to design assemblies that fulfill a given specification. Our system leverages a physics-based solver that takes an assembly design and finds its expected physical configuration. Our validation includes a user study showing that users can create assemblies successfully using our UI and explore a range of designs.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.16435",
        "abstract url": "https://arxiv.org/abs/2401.16435",
        "title": "Heuristics for the Run-length Encoded Burrows-Wheeler Transform Alphabet Ordering Problem",
        "rating": -2,
        "keywords": [
            [
                "bioinformatics"
            ]
        ],
        "abstract": "The Burrows-Wheeler Transform (BWT) is a string transformation technique widely used in areas such as bioinformatics and file compression. Many applications combine a run-length encoding (RLE) with the BWT in a way which preserves the ability to query the compressed data efficiently. However, these methods may not take full advantage of the compressibility of the BWT as they do not modify the alphabet ordering for the sorting step embedded in computing the BWT. Indeed, any such alteration of the alphabet ordering can have a considerable impact on the output of the BWT, in particular on the number of runs. For an alphabet $\u03a3$ containing $\u03c3$ characters, the space of all alphabet orderings is of size $\u03c3!$. While for small alphabets an exhaustive investigation is possible, finding the optimal ordering for larger alphabets is not feasible. Therefore, there is a need for a more informed search strategy than brute-force sampling the entire space, which motivates a new heuristic approach. In this paper, we explore the non-trivial cases for the problem of minimizing the size of a run-length encoded BWT (RLBWT) via selecting a new ordering for the alphabet. We show that random sampling of the space of alphabet orderings usually gives sub-optimal orderings for compression and that a local search strategy can provide a large improvement in relatively few steps. We also inspect a selection of initial alphabet orderings, including ASCII, letter appearance, and letter frequency. While this alphabet ordering problem is computationally hard we demonstrate gain in compressibility.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "32 pages, 8 figures"
    },
    {
        "paper id": "2402.04267",
        "abstract url": "https://arxiv.org/abs/2402.04267",
        "title": "Application analysis of ai technology combined with spiral CT scanning in early lung cancer screening",
        "rating": -2,
        "keywords": [
            [
                "medical",
                "survival",
                "diagnosis",
                "CT",
                "cancer",
                "clinical",
                "organ"
            ]
        ],
        "abstract": "At present, the incidence and fatality rate of lung cancer in China rank first among all malignant tumors. Despite the continuous development and improvement of China's medical level, the overall 5-year survival rate of lung cancer patients is still lower than 20% and is staged. A number of studies have confirmed that early diagnosis and treatment of early stage lung cancer is of great significance to improve the prognosis of patients. In recent years, artificial intelligence technology has gradually begun to be applied in oncology. ai is used in cancer screening, clinical diagnosis, radiation therapy (image acquisition, at-risk organ segmentation, image calibration and delivery) and other aspects of rapid development. However, whether medical ai can be socialized depends on the public's attitude and acceptance to a certain extent. However, at present, there are few studies on the diagnosis of early lung cancer by AI technology combined with SCT scanning. In view of this, this study applied the combined method in early lung cancer screening, aiming to find a safe and efficient screening mode and provide a reference for clinical diagnosis and treatment.",
        "subjects": [
            "physics.med-ph"
        ],
        "comment": "This article was accepted by Frontiers in Computing and Intelligent Systems https://drpress.org/ojs/index.php/fcis/article/view/15781. arXiv admin note: text overlap with arXiv:nlin/0508031 by other authors"
    },
    {
        "paper id": "2403.08780",
        "abstract url": "https://arxiv.org/abs/2403.08780",
        "title": "5 Year Update to the Next Steps in Quantum Computing",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "It has been 5 years since the Computing Community Consortium (CCC) Workshop on Next Steps in Quantum Computing, and significant progress has been made in closing the gap between useful quantum algorithms and quantum hardware. Yet much remains to be done, in particular in terms of mitigating errors and moving towards error-corrected machines. As we begin to transition from the Noisy-Intermediate Scale Quantum (NISQ) era to a future of fault-tolerant machines, now is an opportune time to reflect on how to apply what we have learned thus far and what research needs to be done to realize computational advantage with quantum machines.",
        "subjects": [
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14840",
        "abstract url": "https://arxiv.org/abs/2401.14840",
        "title": "GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption",
        "rating": -2.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "disease"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine Learning (ML) has emerged as one of data science's most transformative and influential domains. However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models. To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models. One such approach is the use of Homomorphic Encryption (HE). However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios. Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges. Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices. We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all while preserving the privacy of the input data and ML model. We demonstrate the real-world applicability of our construction by developing and evaluating an HHE-based PPML application for classifying heart disease based on sensitive ECG data. Notably, our evaluations revealed a slight reduction in accuracy compared to inference on plaintext data. Additionally, both the analyst and end devices experience minimal communication and computation costs, underscoring the practical viability of our approach. The successful integration of HHE into PPML provides a glimpse into a more secure and privacy-conscious future for machine learning on relatively constrained end devices.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, accepted at The 39th ACM/SIGAPP Symposium on Applied Computing (SAC) conference"
    },
    {
        "paper id": "2401.15122",
        "abstract url": "https://arxiv.org/abs/2401.15122",
        "title": "A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics",
        "rating": -2.5,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by utilizing machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations in protein-ligand binding. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory under Newtonian mechanics. For the experiment, we design ten single-trajectory and three multi-trajectory binding simulation tasks. We show the efficiency and effectiveness of NeuralMD, with a 2000$\\times$ speedup over standard numerical MD simulation and outperforming all other ML approaches by up to 80% under the stability metric. We further qualitatively show that NeuralMD reaches more stable binding predictions compared to other machine learning methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15199",
        "abstract url": "https://arxiv.org/abs/2401.15199",
        "title": "SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance",
        "rating": -2.5,
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "survival"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a description of a real-world, multivariate time series dataset collected from an anonymized engine component (called Component X) of a fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables capturing detailed operational data, repair records, and specifications of trucks while maintaining confidentiality by anonymization. It is well-suited for a range of machine learning applications, such as classification, regression, survival analysis, and anomaly detection, particularly when applied to predictive maintenance scenarios. The large population size and variety of features in the format of histograms and numerical counters, along with the inclusion of temporal information, make this real-world dataset unique in the field. The objective of releasing this dataset is to give a broad range of researchers the possibility of working with real-world data from an internationally well-known company and introduce a standard benchmark to the predictive maintenance field, fostering reproducible research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 8 figures"
    },
    {
        "paper id": "2401.14665",
        "abstract url": "https://arxiv.org/abs/2401.14665",
        "title": "PepGB: Facilitating peptide drug discovery via graph neural networks",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "biomedical",
                "face"
            ]
        ],
        "abstract": "Peptides offer great biomedical potential and serve as promising drug candidates. Currently, the majority of approved peptide drugs are directly derived from well-explored natural human peptides. It is quite necessary to utilize advanced deep learning techniques to identify novel peptide drugs in the vast, unexplored biochemical space. Despite various in silico methods having been developed to accelerate peptide early drug discovery, existing models face challenges of overfitting and lacking generalizability due to the limited size, imbalanced distribution and inconsistent quality of experimental data. In this study, we propose PepGB, a deep learning framework to facilitate peptide early drug discovery by predicting peptide-protein interactions (PepPIs). Employing graph neural networks, PepGB incorporates a fine-grained perturbation module and a dual-view objective with contrastive learning-based peptide pre-trained representation to predict PepPIs. Through rigorous evaluations, we demonstrated that PepGB greatly outperforms baselines and can accurately identify PepPIs for novel targets and peptide hits, thereby contributing to the target identification and hit discovery processes. Next, we derive an extended version, diPepGB, to tackle the bottleneck of modeling highly imbalanced data prevalent in lead generation and optimization processes. Utilizing directed edges to represent relative binding strength between two peptide nodes, diPepGB achieves superior performance in real-world assays. In summary, our proposed frameworks can serve as potent tools to facilitate peptide early drug discovery.",
        "subjects": [
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14678",
        "abstract url": "https://arxiv.org/abs/2401.14678",
        "title": "Prompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation",
        "rating": -3,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Cross-domain Recommendation (CDR) as one of the effective techniques in alleviating the data sparsity issues has been widely studied in recent years. However, previous works may cause domain privacy leakage since they necessitate the aggregation of diverse domain data into a centralized server during the training process. Though several studies have conducted privacy preserving CDR via Federated Learning (FL), they still have the following limitations: 1) They need to upload users' personal information to the central server, posing the risk of leaking user privacy. 2) Existing federated methods mainly rely on atomic item IDs to represent items, which prevents them from modeling items in a unified feature space, increasing the challenge of knowledge transfer among domains. 3) They are all based on the premise of knowing overlapped users between domains, which proves impractical in real-world applications. To address the above limitations, we focus on Privacy-preserving Cross-domain Recommendation (PCDR) and propose PFCR as our solution. For Limitation 1, we develop a FL schema by exclusively utilizing users' interactions with local clients and devising an encryption method for gradient encryption. For Limitation 2, we model items in a universal feature space by their description texts. For Limitation 3, we initially learn federated content representations, harnessing the generality of natural language to establish bridges between domains. Subsequently, we craft two prompt fine-tuning strategies to tailor the pre-trained model to the target domain. Extensive experiments on two real-world datasets demonstrate the superiority of our PFCR method compared to the SOTA approaches.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "11 pages, 3 figures, accepted by WWW 2024"
    },
    {
        "paper id": "2401.14780",
        "abstract url": "https://arxiv.org/abs/2401.14780",
        "title": "Adversarial Attacks and Defenses in 6G Network-Assisted IoT Systems",
        "rating": -3,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "6G",
                "IoT"
            ]
        ],
        "abstract": "The Internet of Things (IoT) and massive IoT systems are key to sixth-generation (6G) networks due to dense connectivity, ultra-reliability, low latency, and high throughput. Artificial intelligence, including deep learning and machine learning, offers solutions for optimizing and deploying cutting-edge technologies for future radio communications. However, these techniques are vulnerable to adversarial attacks, leading to degraded performance and erroneous predictions, outcomes unacceptable for ubiquitous networks. This survey extensively addresses adversarial attacks and defense methods in 6G network-assisted IoT systems. The theoretical background and up-to-date research on adversarial attacks and defenses are discussed. Furthermore, we provide Monte Carlo simulations to validate the effectiveness of adversarial attacks compared to jamming attacks. Additionally, we examine the vulnerability of 6G IoT systems by demonstrating attack strategies applicable to key technologies, including reconfigurable intelligent surfaces, massive multiple-input multiple-output (MIMO)/cell-free massive MIMO, satellites, the metaverse, and semantic communications. Finally, we outline the challenges and future developments associated with adversarial attacks and defenses in 6G IoT systems.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "17 pages, 5 figures, and 4 tables. Submitted for publications"
    },
    {
        "paper id": "2401.14829",
        "abstract url": "https://arxiv.org/abs/2401.14829",
        "title": "UMBRELLA: A One-stop Shop Bridging the Gap from Lab to Real-World IoT Experimentation",
        "rating": -3,
        "keywords": [
            [
                "Robotics"
            ],
            [
                "5G",
                "Industrial",
                "IoT"
            ]
        ],
        "abstract": "UMBRELLA is an open, large-scale IoT ecosystem deployed across South Gloucestershire, UK. It is intended to accelerate innovation across multiple technology domains. UMBRELLA is built to bridge the gap between existing specialised testbeds and address holistically real-world technological challenges in a System-of-Systems (SoS) fashion. UMBRELLA provides open access to real-world devices and infrastructure, enabling researchers and the industry to evaluate solutions for Smart Cities, Robotics, Wireless Communications, Edge Intelligence, and more. Key features include over 200 multi-sensor nodes installed on public infrastructure, a robotics arena with 20 mobile robots, a 5G network-in-a-box solution, and a unified backend platform for management, control and secure user access. The heterogeneity of hardware components, including diverse sensors, communication interfaces, and GPU-enabled edge devices, coupled with tools like digital twins, allows for comprehensive experimentation and benchmarking of innovative solutions not viable in lab environments. This paper provides a comprehensive overview of UMBRELLA's multi-domain architecture and capabilities, making it an ideal playground for Internet of Things (IoT) and Industrial IoT (IIoT) innovation. It discusses the challenges in designing, developing and operating UMBRELLA as an open, sustainable testbed and shares lessons learned to guide similar future initiatives. With its unique openness, heterogeneity, realism and tools, UMBRELLA aims to continue accelerating cutting-edge technology research, development and translation into real-world progress.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Submitted for publication to IEEE Access"
    },
    {
        "paper id": "2401.14857",
        "abstract url": "https://arxiv.org/abs/2401.14857",
        "title": "LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "LiDAR",
                "SLAM"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion. This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initial poses for surface Gaussian scenes are obtained using a LiDAR-inertial system with size-adaptive voxels. Then, we optimized and refined the Gaussians by visual-derived photometric gradients to optimize the quality and density of LiDAR measurements. Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality while also holding potential applicability in real-time SLAM and robotics domains. We release our software and hardware and self-collected datasets on Github\\footnote[3]{https://github.com/sheng00125/LIV-GaussMap} to benefit the community.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14861",
        "abstract url": "https://arxiv.org/abs/2401.14861",
        "title": "Implicit Neural Representation for Physics-driven Actuated Soft Bodies",
        "rating": -3,
        "keywords": [
            [
                "facial"
            ],
            [
                "Physics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to SIGGRAPH 2022. Project page: https://studios.disneyresearch.com/2022/07/24/implicit-neural-representation-for-physics-driven-actuated-soft-bodies/ Video: https://www.youtube.com/watch?v=9EERe_CTazk"
    },
    {
        "paper id": "2401.14939",
        "abstract url": "https://arxiv.org/abs/2401.14939",
        "title": "Macro Graph Neural Networks for Online Billion-Scale Recommender Systems",
        "rating": -3,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "industrial",
                "recommendation"
            ]
        ],
        "abstract": "Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as \"micro recommendation graph\" and introduce a more suitable MAcro Recommendation Graph (MAG) for billion-scale recommendations. MAG resolves the computational complexity problems in the infrastructure by reducing the node count from billions to hundreds. Specifically, MAG groups micro nodes (users and items) with similar behavior patterns to form macro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "11 pages, 7 figures, accepted by The Web Conference 2024"
    },
    {
        "paper id": "2401.15045",
        "abstract url": "https://arxiv.org/abs/2401.15045",
        "title": "Emulating Complex Synapses Using Interlinked Proton Conductors",
        "rating": -3,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "face"
            ]
        ],
        "abstract": "In terms of energy efficiency and computational speed, neuromorphic electronics based on non-volatile memory devices is expected to be one of most promising hardware candidates for future artificial intelligence (AI). However, catastrophic forgetting, networks rapidly overwriting previously learned weights when learning new tasks, remains as a pivotal hurdle in either digital or analog AI chips for unleashing the true power of brain-like computing. To address catastrophic forgetting in the context of online memory storage, a complex synapse model (the Benna-Fusi model) has been proposed recently[1], whose synaptic weight and internal variables evolve following a diffusion dynamics. In this work, by designing a proton transistor with a series of charge-diffusion-controlled storage components, we have experimentally realized the Benna-Fusi artificial complex synapse. The memory consolidation from coupled storage components is revealed by both numerical simulations and experimental observations. Different memory timescales for the complex synapse are engineered by the diffusion length of charge carriers, the capacity and number of coupled storage components. The advantage of the demonstrated complex synapse in both memory capacity and memory consolidation is revealed by neural network simulations of face familiarity detection. Our experimental realization of the complex synapse suggests a promising approach to enhance memory capacity and to enable continual learning.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "6 figures"
    },
    {
        "paper id": "2401.15159",
        "abstract url": "https://arxiv.org/abs/2401.15159",
        "title": "RABBIT: A Robot-Assisted Bed Bathing System with Multimodal Perception and Integrated Compliance",
        "rating": -3,
        "keywords": [
            [
                "Robot"
            ],
            [
                "thermal"
            ]
        ],
        "abstract": "This paper introduces RABBIT, a novel robot-assisted bed bathing system designed to address the growing need for assistive technologies in personal hygiene tasks. It combines multimodal perception and dual (software and hardware) compliance to perform safe and comfortable physical human-robot interaction. Using RGB and thermal imaging to segment dry, soapy, and wet skin regions accurately, RABBIT can effectively execute washing, rinsing, and drying tasks in line with expert caregiving practices. Our system includes custom-designed motion primitives inspired by human caregiving techniques, and a novel compliant end-effector called Scrubby, optimized for gentle and effective interactions. We conducted a user study with 12 participants, including one participant with severe mobility limitations, demonstrating the system's effectiveness and perceived comfort. Supplementary material and videos can be found on our website https://emprise.cs.cornell.edu/rabbit.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 8 figures, 19th Annual ACM/IEEE International Conference on Human Robot Interaction (HRI)"
    },
    {
        "paper id": "2401.15223",
        "abstract url": "https://arxiv.org/abs/2401.15223",
        "title": "Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis",
        "rating": -3,
        "keywords": [
            [
                "Biological"
            ],
            [
                "remote sensing",
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, machine learning has become crucial in remote sensing analysis, particularly in the domain of Land-use/Land-cover (LULC). The synergy of machine learning and satellite imagery analysis has demonstrated significant productivity in this field, as evidenced by several studies. A notable challenge within this area is the semantic segmentation mapping of land usage over extensive territories, where the accessibility of accurate land-use data and the reliability of ground truth land-use labels pose significant difficulties. For example, providing a detailed and accurate pixel-wise labeled dataset of the Flanders region, a first-level administrative division of Belgium, can be particularly insightful. Yet there is a notable lack of regulated, formalized datasets and workflows for such studies in many regions globally. This paper introduces a comprehensive approach to addressing these gaps. We present a densely labeled ground truth map of Flanders paired with Sentinel-2 satellite imagery. Our methodology includes a formalized dataset division and sampling method, utilizing the topographic map layout 'Kaartbladversnijdingen,' and a detailed semantic segmentation model training pipeline. Preliminary benchmarking results are also provided to demonstrate the efficacy of our approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.00054",
        "abstract url": "https://arxiv.org/abs/2402.00054",
        "title": "Predicting loss-of-function impact of genetic mutations: a machine learning approach",
        "rating": -3,
        "keywords": [
            [
                "SVM",
                "Support Vector Machine"
            ],
            [
                "medical"
            ]
        ],
        "abstract": "The innovation of next-generation sequencing (NGS) techniques has significantly reduced the price of genome sequencing, lowering barriers to future medical research; it is now feasible to apply genome sequencing to studies where it would have previously been cost-inefficient. Identifying damaging or pathogenic mutations in vast amounts of complex, high-dimensional genome sequencing data may be of particular interest to researchers. Thus, this paper's aims were to train machine learning models on the attributes of a genetic mutation to predict LoFtool scores (which measure a gene's intolerance to loss-of-function mutations). These attributes included, but were not limited to, the position of a mutation on a chromosome, changes in amino acids, and changes in codons caused by the mutation. Models were built using the univariate feature selection technique f-regression combined with K-nearest neighbors (KNN), Support Vector Machine (SVM), Random Sample Consensus (RANSAC), Decision Trees, Random Forest, and Extreme Gradient Boosting (XGBoost). These models were evaluated using five-fold cross-validated averages of r-squared, mean squared error, root mean squared error, mean absolute error, and explained variance. The findings of this study include the training of multiple models with testing set r-squared values of 0.97.",
        "subjects": [
            "q-bio.GN"
        ],
        "comment": "Index Terms: Machine Learning, Prediction Algorithms, Supervised Learning, Support vector machines, K-Nearest Neighbors, RANSAC, Decision Trees, Random Forest, Ge- netic mutations, LoFtool, Next Generation Sequencing"
    },
    {
        "paper id": "2401.15188",
        "abstract url": "https://arxiv.org/abs/2401.15188",
        "title": "CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health",
        "rating": -3.5,
        "keywords": [
            [
                "Health",
                "clinical"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The COVID-19 pandemic has intensified the urgency for effective and accessible mental health interventions in people's daily lives. Mobile Health (mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained traction as they expand beyond traditional clinical settings to support daily life. However, the effectiveness of current mHealth solutions is impeded by the lack of context-awareness, personalization, and modularity to foster their reusability. This paper introduces CAREForMe, a contextual multi-armed bandit (CMAB) recommendation framework for mental health. Designed with context-awareness, personalization, and modularity at its core, CAREForMe harnesses mobile sensing and integrates online learning algorithms with user clustering capability to deliver timely, personalized recommendations. With its modular design, CAREForMe serves as both a customizable recommendation framework to guide future research, and a collaborative platform to facilitate interdisciplinary contributions in mHealth research. We showcase CAREForMe's versatility through its implementation across various platforms (e.g., Discord, Telegram) and its customization to diverse recommendation features.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "MOBILESoft 2024"
    },
    {
        "paper id": "2401.14656",
        "abstract url": "https://arxiv.org/abs/2401.14656",
        "title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
        "rating": -4,
        "keywords": [
            [
                "depth"
            ],
            [
                "Biological"
            ],
            [
                "Chemical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of \"scientific language\", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14938",
        "abstract url": "https://arxiv.org/abs/2401.14938",
        "title": "DAM: Diffusion Activation Maximization for 3D Global Explanations",
        "rating": -4,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Diffusion"
            ],
            [
                "autonomous driving"
            ],
            [
                "healthcare"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, the performance of point cloud models has been rapidly improved. However, due to the limited amount of relevant explainability studies, the unreliability and opacity of these black-box models may lead to potential risks in applications where human lives are at stake, e.g. autonomous driving or healthcare. This work proposes a DDPM-based point cloud global explainability method (DAM) that leverages Point Diffusion Transformer (PDT), a novel point-wise symmetric model, with dual-classifier guidance to generate high-quality global explanations. In addition, an adapted path gradient integration method for DAM is proposed, which not only provides a global overview of the saliency maps for point cloud categories, but also sheds light on how the attributions of the explanations vary during the generation process. Extensive experiments indicate that our method outperforms existing ones in terms of perceptibility, representativeness, and diversity, with a significant reduction in generation time. Our code is available at: https://github.com/Explain3D/DAM",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14968",
        "abstract url": "https://arxiv.org/abs/2401.14968",
        "title": "Atmosphere: Context and situational-aware collaborative IoT architecture for edge-fog-cloud computing",
        "rating": -4,
        "keywords": [
            [
                "disease"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "The Internet of Things (IoT) has grown significantly in popularity, accompanied by increased capacity and lower cost of communications, and overwhelming development of technologies. At the same time, big data and real-time data analysis have taken on great importance and have been accompanied by unprecedented interest in sharing data among citizens, public administrations and other organisms, giving rise to what is known as the Collaborative Internet of Things. This growth in data and infrastructure must be accompanied by a software architecture that allows its exploitation. Although there are various proposals focused on the exploitation of the IoT at edge, fog and/or cloud levels, it is not easy to find a software solution that exploits the three tiers together, taking maximum advantage not only of the analysis of contextual and situational data at each tier, but also of two-way communications between adjacent ones. In this paper, we propose an architecture that solves these deficiencies by proposing novel technologies which are appropriate for managing the resources of each tier: edge, fog and cloud. In addition, the fact that two-way communications along the three tiers of the architecture is allowed considerably enriches the contextual and situational information in each layer, and substantially assists decision making in real time. The paper illustrates the proposed software architecture through a case study of respiratory disease surveillance in hospitals. As a result, the proposed architecture permits efficient communications between the different tiers responding to the needs of these types of IoT scenarios.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15219",
        "abstract url": "https://arxiv.org/abs/2401.15219",
        "title": "Harnessing Deep Learning of Point Clouds for Inverse Control of 3D Shape Morphing",
        "rating": -4,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "robotics"
            ],
            [
                "biomimetic"
            ]
        ],
        "abstract": "Shape-morphing devices, a crucial branch in soft robotics, hold significant application value in areas like human-machine interfaces, biomimetic robotics, and tools for interacting with biological systems. To achieve three-dimensional (3D) programmable shape morphing (PSM), the deployment of array-based actuators is essential. However, a critical knowledge gap impeding the development of 3D PSM is the challenge of controlling the complex systems formed by these soft actuator arrays. This study introduces a novel approach, for the first time, representing the configuration of shape morphing devices using point cloud data and employing deep learning to map these configurations to control inputs. We propose Shape Morphing Net (SMNet), a method that realizes the regression from point cloud data to high-dimensional continuous vectors. Applied to previous 2D PSM actuator arrays, SMNet significantly enhances control precision from 82.23% to 97.68%. Further, we extend its application to 3D PSM devices with three different actuator mechanisms, demonstrating the universal applicability of SMNet to the control of 3D shape morphing technologies. In our demonstrations, we confirm the efficacy of inverse control, where 3D PSM devices successfully replicate target shapes. These shapes are obtained either through 3D scanning of physical objects or via 3D modeling software. The results show that within the deformable range of 3D PSM devices, accurate reproduction of the desired shapes is achievable. The findings of this research represent a substantial advancement in soft robotics, particularly for applications demanding intricate 3D shape transformations, and establish a foundational framework for future developments in the field.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.03353",
        "abstract url": "https://arxiv.org/abs/2402.03353",
        "title": "Tweet Influence on Market Trends: Analyzing the Impact of Social Media Sentiment on Biotech Stocks",
        "rating": -4,
        "keywords": [
            [
                "Biotech"
            ],
            [
                "forecast"
            ]
        ],
        "abstract": "This study investigates the relationship between tweet sentiment across diverse categories: news, company opinions, CEO opinions, competitor opinions, and stock market behavior in the biotechnology sector, with a focus on understanding the impact of social media discourse on investor sentiment and decision-making processes. We analyzed historical stock market data for ten of the largest and most influential pharmaceutical companies alongside Twitter data related to COVID-19, vaccines, the companies, and their respective CEOs. Using VADER sentiment analysis, we examined the sentiment scores of tweets and assessed their relationships with stock market performance. We employed ARIMA (AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression) models to forecast stock market performance, incorporating sentiment covariates to improve predictions. Our findings revealed a complex interplay between tweet sentiment, news, biotech companies, their CEOs, and stock market performance, emphasizing the importance of considering diverse factors when modeling and predicting stock prices. This study provides valuable insights into the influence of social media on the financial sector and lays a foundation for future research aimed at refining stock price prediction models.",
        "subjects": [
            "q-fin.ST"
        ],
        "comment": "This submission includes 51 pages and 24 figures"
    },
    {
        "paper id": "2401.14695",
        "abstract url": "https://arxiv.org/abs/2401.14695",
        "title": "Continuously Evolving Graph Neural Controlled Differential Equations for Traffic Forecasting",
        "rating": -4.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "industrial"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As a crucial technique for developing a smart city, traffic forecasting has become a popular research focus in academic and industrial communities for decades. This task is highly challenging due to complex and dynamic spatial-temporal dependencies in traffic networks. Existing works ignore continuous temporal dependencies and spatial dependencies evolving over time. In this paper, we propose Continuously Evolving Graph Neural Controlled Differential Equations (CEGNCDE) to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Specifically, a continuously evolving graph generator (CEGG) based on NCDE is introduced to generate the spatial dependencies graph that continuously evolves over time from discrete historical observations. Then, a graph neural controlled differential equations (GNCDE) framework is introduced to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Extensive experiments demonstrate that CEGNCDE outperforms the SOTA methods by average 2.34% relative MAE reduction, 0.97% relative RMSE reduction, and 3.17% relative MAPE reduction.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2401.15216",
        "abstract url": "https://arxiv.org/abs/2401.15216",
        "title": "Quantum-Assisted Adaptive Beamforming in UASs Network: Enhancing Airborne Communication via Collaborative UASs for NextG IoT",
        "rating": -6,
        "keywords": [
            [
                "face"
            ],
            [
                "IoT"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "This paper introduces a novel quantum-based method for dynamic beamforming and re-forming in Unmanned Aircraft Systems (UASs), specifically addressing the critical challenges posed by the unavoidable hovering characteristics of UAVs. Hovering creates significant beam path distortions, impacting the reliability and quality of distributed beamforming in airborne networks. To overcome these challenges, our Quantum Search for UAS Beamforming (QSUB) employs quantum superposition, entanglement, and amplitude amplification. It adaptively reconfigures beams, enhancing beam quality and maintaining robust communication links in the face of rapid UAS state changes due to hovering. Furthermore, we propose an optimized framework, Quantum-Position-Locked Loop (Q-P-LL), that is based on the principle of the Nelder-Mead optimization method for adaptive search to reduce prediction error and improve resilience against angle-of-arrival estimation errors, crucial under dynamic hovering conditions. We also demonstrate the scalability of the system performance and computation complexity by comparing various numbers of active UASs. Importantly, QSUB and Q-P-LL can be applied to both classical and quantum computing architectures. Comparative analyses with conventional Maximum Ratio Transmission (MRT) schemes demonstrate the superior performance and scalability of our quantum approaches, marking significant advancements in the next-generation Internet of Things (IoT) applications requiring reliable airborne communication networks.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14652",
        "abstract url": "https://arxiv.org/abs/2401.14652",
        "title": "LitE-SNN: Designing Lightweight and Efficient Spiking Neural Network through Spatial-Temporal Compressive Network Search and Joint Optimization",
        "rating": -10,
        "keywords": [],
        "abstract": "Spiking Neural Networks (SNNs) mimic the information-processing mechanisms of the human brain and are highly energy-efficient, making them well-suited for low-power edge devices. However, the pursuit of accuracy in current studies leads to large, long-timestep SNNs, conflicting with the resource constraints of these devices. In order to design lightweight and efficient SNNs, we propose a new approach named LitESNN that incorporates both spatial and temporal compression into the automated network design process. Spatially, we present a novel Compressive Convolution block (CompConv) to expand the search space to support pruning and mixed-precision quantization while utilizing the shared weights and pruning mask to reduce the computation. Temporally, we are the first to propose a compressive timestep search to identify the optimal number of timesteps under specific computation cost constraints. Finally, we formulate a joint optimization to simultaneously learn the architecture parameters and spatial-temporal compression strategies to achieve high performance while minimizing memory and computation costs. Experimental results on CIFAR10, CIFAR100, and Google Speech Command datasets demonstrate our proposed LitESNNs can achieve competitive or even higher accuracy with remarkably smaller model sizes and fewer computation costs. Furthermore, we validate the effectiveness of our LitESNN on the trade-off between accuracy and resource cost and show the superiority of our joint optimization. Additionally, we conduct energy analysis to further confirm the energy efficiency of LitESNN",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14657",
        "abstract url": "https://arxiv.org/abs/2401.14657",
        "title": "Validating Climate Models with Spherical Convolutional Wasserstein Distance",
        "rating": -10,
        "keywords": [],
        "abstract": "The validation of global climate models is crucial to ensure the accuracy and efficacy of model output. We introduce the spherical convolutional Wasserstein distance to more comprehensively measure differences between climate models and reanalysis data. This new similarity measure accounts for spatial variability using convolutional projections and quantifies local differences in the distribution of climate variables. We apply this method to evaluate the historical model outputs of the Coupled Model Intercomparison Project (CMIP) members by comparing them to observational and reanalysis data products. Additionally, we investigate the progression from CMIP phase 5 to phase 6 and find modest improvements in the phase 6 models regarding their ability to produce realistic climatologies.",
        "subjects": [
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14663",
        "abstract url": "https://arxiv.org/abs/2401.14663",
        "title": "Two classes of LCD BCH codes over finite fields",
        "rating": -10,
        "keywords": [],
        "abstract": "BCH codes form an important subclass of cyclic codes, and are widely used in compact discs, digital audio tapes and other data storage systems to improve data reliability. As far as we know, there are few results on $q$-ary BCH codes of length $n=\\frac{q^{m}+1}{q+1}$. This is because it is harder to deal with BCH codes of such length. In this paper, we study $q$-ary BCH codes with lengths $n=\\frac{q^{m}+1}{q+1}$ and $n=q^m+1$. These two classes of BCH codes are always LCD codes. For $n=\\frac{q^{m}+1}{q+1}$, the dimensions of narrow-sense BCH codes of length $n$ with designed distance $\u03b4=\\ell q^{\\frac{m-1}{2}}+1$ are determined, where $q>2$ and $2\\leq \\ell \\leq q-1$. Moreover, the largest coset leader is given for $m=3$ and the first two largest coset leaders are given for $q=2$. The parameters of BCH codes related to the first few largest coset leaders are investigated. Some binary BCH codes of length $n=\\frac{2^m+1}{3}$ have optimal parameters. For ternary narrow-sense BCH codes of length $n=3^m+1$, a lower bound on the minimum distance of their dual codes is developed, which is good in some cases.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14669",
        "abstract url": "https://arxiv.org/abs/2401.14669",
        "title": "Hidden Markov Models and the Bayes Filter in Categorical Probability",
        "rating": -10,
        "keywords": [],
        "abstract": "We use Markov categories to develop generalizations of the theory of Markov chains and hidden Markov models in an abstract setting. This comprises characterizations of hidden Markov models in terms of local and global conditional independences as well as existing algorithms for Bayesian filtering and smoothing applicable in all Markov categories with conditionals. We show that these algorithms specialize to existing ones such as the Kalman filter, forward-backward algorithm, and the Rauch-Tung-Striebel smoother when instantiated in appropriate Markov categories. Under slightly stronger assumptions, we also prove that the sequence of outputs of the Bayes filter is itself a Markov chain with a concrete formula for its transition maps. There are two main features of this categorical framework. The first is its generality, as it can be used in any Markov category with conditionals. In particular, it provides a systematic unified account of hidden Markov models and algorithms for filtering and smoothing in discrete probability, Gaussian probability, measure-theoretic probability, possibilistic nondeterminism and others at the same time. The second feature is the intuitive visual representation of information flow in these algorithms in terms of string diagrams.",
        "subjects": [
            "math.ST"
        ],
        "comment": "76 pages. v2: added Examples 3.8 and 3.23 on possibilistic filter and Section 1.5 on implementation"
    },
    {
        "paper id": "2401.14671",
        "abstract url": "https://arxiv.org/abs/2401.14671",
        "title": "The dual codes of two classes of LCD BCH codes",
        "rating": -10,
        "keywords": [],
        "abstract": "Cyclic BCH codes and negacyclic BCH codes form important subclasses of cyclic codes and negacyclic codes, respectively, and can produce optimal linear codes in many cases. To the best of our knowledge, there are few results on the dual codes of cyclic and negacyclic BCH codes. In this paper, we study the dual codes of narrow-sense cyclic BCH codes of length $q^m+1$ over a finite field $\\mathbb{F}_q$, where $q$ is an odd prime power, and the dual codes of narrow-sense negacyclic BCH codes of length $\\frac{q^{m}+1}{2}$ over $\\mathbb{F}_q$, where $q$ is an odd prime power satisfying $q\\equiv 3~({\\rm mod}~4)$. Some lower bounds on the minimum distances of the dual codes are established, which are very close to the true minimum distances of the dual codes in many cases. Sufficient and necessary conditions for the even-like subcodes of narrow-sense cyclic BCH codes of length $q^{m}+1$ being cyclic dually-BCH codes are given in terms of designed distances, where $q$ is odd and $m$ is odd or $m\\equiv 2~({\\rm mod~}4)$. The concept of negacyclic dually-BCH codes is proposed, and sufficient and necessary conditions in terms of designed distances are presented to ensure that narrow-sense negacyclic BCH codes of length $\\frac{q^{m}+1}{2}$ are dually-BCH codes, where $q\\equiv 3~({\\rm mod}~4)$.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14682",
        "abstract url": "https://arxiv.org/abs/2401.14682",
        "title": "Diversity-guided Search Exploration for Self-driving Cars Test Generation through Frenet Space Encoding",
        "rating": -10,
        "keywords": [],
        "abstract": "The rise of self-driving cars (SDCs) presents important safety challenges to address in dynamic environments. While field testing is essential, current methods lack diversity in assessing critical SDC scenarios. Prior research introduced simulation-based testing for SDCs, with Frenetic, a test generation approach based on Frenet space encoding, achieving a relatively high percentage of valid tests (approximately 50%) characterized by naturally smooth curves. The \"minimal out-of-bound distance\" is often taken as a fitness function, which we argue to be a sub-optimal metric. Instead, we show that the likelihood of leading to an out-of-bound condition can be learned by the deep-learning vanilla transformer model. We combine this \"inherently learned metric\" with a genetic algorithm, which has been shown to produce a high diversity of tests. To validate our approach, we conducted a large-scale empirical evaluation on a dataset comprising over 1,174 simulated test cases created to challenge the SDCs behavior. Our investigation revealed that our approach demonstrates a substantial reduction in generating non-valid test cases, increased diversity, and high accuracy in identifying safety violations during SDC test execution.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14710",
        "abstract url": "https://arxiv.org/abs/2401.14710",
        "title": "Lower Bounds on Mutual Information for Linear Codes Transmitted over Binary Input Channels, and for Information Combining",
        "rating": -10,
        "keywords": [],
        "abstract": "It has been known for a long time that the mutual information between the input sequence and output of a binary symmetric channel (BSC) is upper bounded by the mutual information between the same input sequence and the output of a binary erasure channel (BEC) with the same capacity. Recently, Samorodintsky discovered that one may also lower bound the BSC mutual information in terms of the mutual information between the same input sequence and a more capable BEC. In this paper, we strengthen Samordnitsky's bound for the special case where the input to the channel is distributed uniformly over a linear code. Furthermore, for a general (not necessarily binary) input distribution $P_X$ and channel $W_{Y|X}$, we derive a new lower bound on the mutual information $I(X;Y^n)$ for $n$ transmissions of $X\\sim P_X$ through the channel $W_{Y|X}$.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14723",
        "abstract url": "https://arxiv.org/abs/2401.14723",
        "title": "Sliding Secure Symmetric Multilevel Diversity Coding",
        "rating": -10,
        "keywords": [],
        "abstract": "Symmetric multilevel diversity coding (SMDC) is a source coding problem where the independent sources are ordered according to their importance. It was shown that separately encoding independent sources (referred to as ``\\textit{superposition coding}\") is optimal. In this paper, we consider an $(L,s)$ \\textit{sliding secure} SMDC problem with security priority, where each source $X_\u03b1~(s\\leq \u03b1\\leq L)$ is kept perfectly secure if no more than $\u03b1-s$ encoders are accessible. The reconstruction requirements of the $L$ sources are the same as classical SMDC. A special case of an $(L,s)$ sliding secure SMDC problem that the first $s-1$ sources are constants is called the $(L,s)$ \\textit{multilevel secret sharing} problem. For $s=1$, the two problems coincide, and we show that superposition coding is optimal. The rate regions for the $(3,2)$ problems are characterized. It is shown that superposition coding is suboptimal for both problems. The main idea that joint encoding can reduce coding rates is that we can use the previous source $X_{\u03b1-1}$ as the secret key of $X_\u03b1$. Based on this idea, we propose a coding scheme that achieves the minimum sum rate of the general $(L,s)$ multilevel secret sharing problem. Moreover, superposition coding of the $s$ sets of sources $X_1$, $X_2$, $\\cdots$, $X_{s-1}$, $(X_s, X_{s+1}, \\cdots, X_L)$ achieves the minimum sum rate of the general sliding secure SMDC problem.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14727",
        "abstract url": "https://arxiv.org/abs/2401.14727",
        "title": "SparseCoder: Identifier-Aware Sparse Transformer for File-Level Code Summarization",
        "rating": -10,
        "keywords": [],
        "abstract": "Code summarization aims to generate natural language descriptions of source code, facilitating programmers to understand and maintain it rapidly. While previous code summarization efforts have predominantly focused on method-level, this paper studies file-level code summarization, which can assist programmers in understanding and maintaining large source code projects. Unlike method-level code summarization,file-level code summarization typically involves long source code within a single file, which makes it challenging for Transformer-based models to understand the code semantics for the maximum input length of these models is difficult to set to a large number that can handle long code input well, due to the quadratic scaling of computational complexity with the input sequence length. To address this challenge, we propose SparseCoder, an identifier-aware sparse transformer for effectively handling long code sequences. Specifically, the SparseCoder employs a sliding window mechanism for self-attention to model short-term dependencies and leverages the structure message of code to capture long-term dependencies among source code identifiers by introducing two types of sparse attention patterns named global and identifier attention. To evaluate the performance of SparseCoder, we construct a new dataset FILE-CS for file-level code summarization in Python. Experimental results show that our SparseCoder model achieves state-of-the-art performance compared with other pre-trained models, including full self-attention and sparse models. Additionally, our model has low memory overhead and achieves comparable performance with models using full self-attention mechanism.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "To appear in SANER'24"
    },
    {
        "paper id": "2401.14737",
        "abstract url": "https://arxiv.org/abs/2401.14737",
        "title": "Deterministic Parikh automata on infinite words",
        "rating": -10,
        "keywords": [],
        "abstract": "Various variants of Parikh automata on infinite words have recently been introduced and studied in the literature. However, with some exceptions only their non-deterministic versions have been studied. In this paper we study the deterministic versions of all variants of Parikh automata on infinite words that have not yet been studied in the literature. We compare the expressiveness of the deterministic models, study their closure properties and decision problems with applications to model checking. The model of deterministic limit Parikh automata turns out to be most interesting, as it is the only deterministic Parikh model generalizing the $\u03c9$-regular languages, the only deterministic Parikh model closed under the Boolean operations and the only deterministic Parikh model for which all common decision problems are decidable.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14746",
        "abstract url": "https://arxiv.org/abs/2401.14746",
        "title": "Listening to the Voices: Describing Ethical Caveats of Conversational User Interfaces According to Experts and Frequent Users",
        "rating": -10,
        "keywords": [],
        "abstract": "Advances in natural language processing and understanding have led to a rapid growth in the popularity of conversational user interfaces (CUIs). While CUIs introduce novel benefits, they also yield risks that may exploit people's trust. Although research looking at unethical design deployed through graphical user interfaces (GUIs) established a thorough understanding of so-called dark patterns, there is a need to continue this discourse within the CUI community to understand potentially problematic interactions. Addressing this gap, we interviewed 27 participants from three cohorts: researchers, practitioners, and frequent users of CUIs. Applying thematic analysis, we construct five themes reflecting each cohort's insights about ethical design challenges and introduce the CUI Expectation Cycle, bridging system capabilities and user expectations while considering each theme's ethical caveats. This research aims to inform future development of CUIs to consider ethical constraints while adopting a human-centred approach.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "18 pages; 4 tables; and 1 figure. This is the author's version and pre-print of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record will be published in Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA, https://doi.org/https://doi.org/10.1145/3613904.3642542"
    },
    {
        "paper id": "2401.14763",
        "abstract url": "https://arxiv.org/abs/2401.14763",
        "title": "Comparing Session Type Systems derived from Linear Logic",
        "rating": -10,
        "keywords": [],
        "abstract": "Session types are a typed approach to message-passing concurrency, where types describe sequences of intended exchanges over channels. Session type systems have been given strong logical foundations via Curry-Howard correspondences with linear logic, a resource-aware logic that naturally captures structured interactions. These logical foundations provide an elegant framework to specify and (statically) verify message-passing processes. In this paper, we rigorously compare different type systems for concurrency derived from the Curry-Howard correspondence between linear logic and session types. We address the main divide between these type systems: the classical and intuitionistic presentations of linear logic. Along the years, these presentations have given rise to separate research strands on logical foundations for concurrency; the differences between their derived type systems have only been addressed informally. To formally assess these differences, we develop $\u03c0\\mathsf{ULL}$, a session type system that encompasses type systems derived from classical and intuitionistic interpretations of linear logic. Based on a fragment of Girard's Logic of Unity, $\u03c0\\mathsf{ULL}$ provides a basic reference framework: we compare existing session type systems by characterizing fragments of $\u03c0\\mathsf{ULL}$ that coincide with classical and intuitionistic formulations. We analyze the significance of our characterizations by considering the locality principle (enforced by intuitionistic interpretations but not by classical ones) and forms of process composition induced by the interpretations.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "Revised/extended version of https://doi.org/10.4204/EPTCS.314.1"
    },
    {
        "paper id": "2401.14791",
        "abstract url": "https://arxiv.org/abs/2401.14791",
        "title": "ISP pricing and Platform pricing interaction under net neutrality",
        "rating": -10,
        "keywords": [],
        "abstract": "We analyze the effects of enforcing vs. exempting access ISP from net neutrality regulations when platforms are present and operate two-sided pricing in their business models. This study is conducted in a scenario where users and Content Providers (CPs) have access to the internet by means of their serving ISPs and to a platform that intermediates and matches users and CPs, among other service offerings. Our hypothesis is that platform two-sided pricing interacts in a relevant manner with the access ISP, which may be allowed (an hypothetical non-neutrality scenario) or not (the current neutrality regulation status) to apply two-sided pricing on its service business model. We preliminarily conclude that the platforms are extracting surplus from the CPs under the current net neutrality regime for the ISP, and that the platforms would not be able to do so under the counter-factual situation where the ISPs could apply two-sided prices.",
        "subjects": [
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14799",
        "abstract url": "https://arxiv.org/abs/2401.14799",
        "title": "Linearity and Classification of $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-Linear Hadamard Codes",
        "rating": -10,
        "keywords": [],
        "abstract": "The $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive codes are subgroups of $\\mathbb{Z}_2^{\u03b1_1} \\times \\mathbb{Z}_4^{\u03b1_2} \\times \\mathbb{Z}_8^{\u03b1_3}$. A $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard code is a Hadamard code which is the Gray map image of a $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive code. A recursive construction of $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-additive Hadamard codes of type $(\u03b1_1,\u03b1_2, \u03b1_3;t_1,t_2, t_3)$ with $\u03b1_1 \\neq 0$, $\u03b1_2 \\neq 0$, $\u03b1_3 \\neq 0$, $t_1\\geq 1$, $t_2 \\geq 0$, and $t_3\\geq 1$ is known. In this paper, we generalize some known results for $\\mathbb{Z}_2\\mathbb{Z}_4$-linear Hadamard codes to $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes with $\u03b1_1 \\neq 0$, $\u03b1_2 \\neq 0$, and $\u03b1_3 \\neq 0$. First, we show for which types the corresponding $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes of length $2^t$ are nonlinear. For these codes, we compute the kernel and its dimension, which allows us to give a partial classification of these codes. Moreover, for $3 \\leq t \\leq 11$, we give a complete classification by providing the exact amount of nonequivalent such codes. We also prove the existence of several families of infinite such nonlinear $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard codes, which are not equivalent to any other constructed $\\mathbb{Z}_2\\mathbb{Z}_4\\mathbb{Z}_8$-linear Hadamard code, nor to any $\\mathbb{Z}_2\\mathbb{Z}_4$-linear Hadamard code, nor to any previously constructed $\\mathbb{Z}_{2^s}$-linear Hadamard code with $s\\geq 2$, with the same length $2^t$.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2301.09404"
    },
    {
        "paper id": "2401.14805",
        "abstract url": "https://arxiv.org/abs/2401.14805",
        "title": "Pointwise Redundancy in One-Shot Lossy Compression via Poisson Functional Representation",
        "rating": -10,
        "keywords": [],
        "abstract": "We study different notions of pointwise redundancy in variable-length lossy source coding. We present a construction of one-shot variable-length lossy source coding schemes using the Poisson functional representation, and give bounds on its pointwise redundancy for various definitions of pointwise redundancy. This allows us to describe the distribution of the encoding length in a precise manner. We also generalize the result to the one-shot lossy Gray-Wyner system.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "9 pages, short version to be presented at 2024 International Zurich Seminar on Information and Communication"
    },
    {
        "paper id": "2401.14810",
        "abstract url": "https://arxiv.org/abs/2401.14810",
        "title": "Cyclic Group Projection for Enumerating Quasi-Cyclic Codes Trapping Sets",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper introduces a novel approach to enumerate and assess Trapping sets in quasi-cyclic codes, those with circulant sizes that are non-prime numbers. Leveraging the quasi-cyclic properties, the method employs a tabular technique to streamline the importance sampling step for estimating the pseudo-codeword weight of Trapping sets. The presented methodology draws on the mathematical framework established in the provided theorem, which elucidates the behavior of projection and lifting transformations on pseudo-codewords",
        "subjects": [
            "cs.IT"
        ],
        "comment": "7 pages, 3 tables"
    },
    {
        "paper id": "2401.14815",
        "abstract url": "https://arxiv.org/abs/2401.14815",
        "title": "Faster Fr\u00e9chet Distance Approximation through Truncated Smoothing",
        "rating": -10,
        "keywords": [],
        "abstract": "The Fr\u00e9chet distance is a popular distance measure for curves. Computing the Fr\u00e9chet distance between two polygonal curves of $n$ vertices takes roughly quadratic time, and conditional lower bounds suggest that even approximating to within a factor $3$ cannot be done in strongly-subquadratic time, even in one dimension. The current best approximation algorithms present trade-offs between approximation quality and running time. Recently, van der Horst $\\textit{et al.}$ (SODA, 2023) presented an $O((n^2 / \u03b1) \\log^3 n)$ time $\u03b1$-approximate algorithm for curves in arbitrary dimensions, for any $\u03b1\\in [1, n]$. Our main contribution is an approximation algorithm for curves in one dimension, with a significantly faster running time of $O(n \\log^3 n + (n^2 / \u03b1^3) \\log^2 n \\log \\log n)$. Additionally, we give an algorithm for curves in arbitrary dimensions that improves upon the state-of-the-art running time by a logarithmic factor, to $O((n^2 / \u03b1) \\log^2 n)$. Both of our algorithms rely on a linear-time simplification procedure that in one dimension reduces the complexity of the reachable free space to $O(n^2 / \u03b1)$ without making sacrifices in the asymptotic approximation factor.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "27 pages, 11 figures"
    },
    {
        "paper id": "2401.14834",
        "abstract url": "https://arxiv.org/abs/2401.14834",
        "title": "From Differential Linear Logic to Coherent Differentiation",
        "rating": -10,
        "keywords": [],
        "abstract": "In this survey, we present in a unified way the categorical and syntactical settings of coherent differentiation introduced recently, which shows that the basic ideas of differential linear logic and of the differential lambda-calculus are compatible with determinism. Indeed, due to the Leibniz rule of the differential calculus, differential linear logic and the differential lambda-calculus feature an operation of addition of proofs or terms operationally interpreted as a strong form of nondeterminism. The main idea of coherent differentiation is that these sums can be controlled and kept in the realm of determinism by means of a notion of summability, upon enforcing summability restrictions on the derivatives which can be written in the models and in the syntax.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14871",
        "abstract url": "https://arxiv.org/abs/2401.14871",
        "title": "Data-Enabled Policy Optimization for Direct Adaptive Learning of the LQR",
        "rating": -10,
        "keywords": [],
        "abstract": "Direct data-driven design methods for the linear quadratic regulator (LQR) mainly use offline or episodic data batches, and their online adaptation has been acknowledged as an open problem. In this paper, we propose a direct adaptive method to learn the LQR from online closed-loop data. First, we propose a new policy parameterization based on the sample covariance to formulate a direct data-driven LQR problem, which is shown to be equivalent to the certainty-equivalence LQR with optimal non-asymptotic guarantees. Second, we design a novel data-enabled policy optimization (DeePO) method to directly update the policy, where the gradient is explicitly computed using only a batch of persistently exciting (PE) data. Third, we establish its global convergence via a projected gradient dominance property. Importantly, we efficiently use DeePO to adaptively learn the LQR by performing only one-step projected gradient descent per sample of the closed-loop system, which also leads to an explicit recursive update of the policy. Under PE inputs and for bounded noise, we show that the average regret of the LQR cost is upper-bounded by two terms signifying a sublinear decrease in time $\\mathcal{O}(1/\\sqrt{T})$ plus a bias scaling inversely with signal-to-noise ratio (SNR), which are independent of the noise statistics. Finally, we perform simulations to validate the theoretical results and demonstrate the computational and sample efficiency of our method.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14878",
        "abstract url": "https://arxiv.org/abs/2401.14878",
        "title": "Hades Again and Again: A Study on Frustration Tolerance, Physiology and Player Experience",
        "rating": -10,
        "keywords": [],
        "abstract": "Accurately quantifying player experience is challenging for many reasons: identifying a ground truth and building validated and reliable scales are both challenging tasks; on top of that, empirical results are often moderated by individual factors. In this article, we present a study on the rogue-like game Hades designed to investigate the impact of individual differences in the operationalisation of player experience by cross-referencing multiple modalities (i.e., questionnaires, gameplay, and heart rate) and identifying the interplay between their scales.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14881",
        "abstract url": "https://arxiv.org/abs/2401.14881",
        "title": "Online Bin Covering with Frequency Predictions",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the discrete bin covering problem where a multiset of items from a fixed set $S \\subseteq (0,1]$ must be split into disjoint subsets while maximizing the number of subsets whose contents sum to at least $1$. We study the online discrete variant, where $S$ is finite, and items arrive sequentially. In the purely online setting, we show that the competitive ratios of best deterministic (and randomized) algorithms converge to $\\frac{1}{2}$ for large $S$, similar to the continuous setting. Therefore, we consider the problem under the prediction setting, where algorithms may access a vector of frequencies predicting the frequency of items of each size in the instance. In this setting, we introduce a family of online algorithms that perform near-optimally when the predictions are correct. Further, we introduce a second family of more robust algorithms that presents a tradeoff between the performance guarantees when the predictions are perfect and when predictions are adversarial. Finally, we consider a stochastic setting where items are drawn independently from any fixed but unknown distribution of $S$. Using results from the PAC-learnability of probabilities in discrete distributions, we also introduce a purely online algorithm whose average-case performance is near-optimal with high probability for all finite sets $S$ and all distributions of $S$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "27 pages"
    },
    {
        "paper id": "2401.14887",
        "abstract url": "https://arxiv.org/abs/2401.14887",
        "title": "The Power of Noise: Redefining Retrieval for RAG Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting our initial assumption of diminished quality. These results underscore the need for developing specialized strategies to integrate retrieval with language generation models, thereby laying the groundwork for future research in this field.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14888",
        "abstract url": "https://arxiv.org/abs/2401.14888",
        "title": "HOPE: Holistic STT-RAM Architecture Exploration Framework for Future Cross-Platform Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "Spin Transfer Torque Random Access Memory (STT-RAM) is an emerging Non-Volatile Memory (NVM) technology that has garnered attention to overcome the drawbacks of conventional CMOS-based technologies. However, such technologies must be evaluated before deployment under real workloads and architecture. But there is a lack of available open-source STT-RAM-based system evaluation framework, which hampers research and experimentation and impacts the adoption of STT- RAM in a system. This paper proposes a novel, extendable STT-RAM memory controller design integrated inside the gem5 simulator. Our framework enables understanding various aspects of STT-RAM, i.e., power, delay, clock cycles, energy, and system throughput. We will open-source our HOPE framework, which will fuel research and aid in accelerating the development of future system architectures based on STT-RAM. It will also facilitate the user for further tool enhancement.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14901",
        "abstract url": "https://arxiv.org/abs/2401.14901",
        "title": "Augmenting Bankruptcy Prediction using Reported Behavior of Corporate Restructuring",
        "rating": -10,
        "keywords": [],
        "abstract": "Credit risk assessment of a company is commonly conducted by utilizing financial ratios that are derived from its financial statements. However, this approach may not fully encompass other significant aspects of a company. We propose the utilization of a hybrid dataset that combines financial statements with information about corporate restructuring behavior in order to construct diverse machine learning models to predict bankruptcy. Utilizing a hybrid data set provides a more comprehensive and holistic perspective on a company's financial position and the dynamics of its business operations. The experiments were carried out using publicly available records of all the files submitted by small and medium-sized enterprises to Luxembourg Business Registers. We conduct a comparative analysis of bankruptcy prediction using six machine learning models. Furthermore, we validate the effectiveness of the hybrid dataset. In addition to the conventional testing set, we deliberately chose the timeframe encompassing the years of the Covid-19 pandemic as an additional testing set in order to evaluate the robustness of the models. The experimental results demonstrate that the hybrid data set can improve the performance of the model by 4%-13% compared to a single source data set. We also identify suitable models for predicting bankruptcy.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14903",
        "abstract url": "https://arxiv.org/abs/2401.14903",
        "title": "Energy Flexibility Potential in the Brewery Sector: A Multi-agent Based Simulation of 239 Danish Breweries",
        "rating": -10,
        "keywords": [],
        "abstract": "The beverage industry is a typical food processing industry, accounts for significant energy consumption, and has flexible demands. However, the deployment of energy flexibility in the beverage industry is complex and challenging. Furthermore, activation of energy flexibility from the whole brewery industry is necessary to ensure grid stability. Therefore, this paper assesses the energy flexibility potential of Denmark's brewery sector based on a multi-agent-based simulation. 239 individual brewery facilities are simulated, and each facility, as an agent, can interact with the energy system market and make decisions based on its underlying parameters and operational restrictions. The results show that the Danish breweries could save 1.56 % of electricity costs annually while maintaining operational security and reducing approximately 1745 tonnes of CO2 emissions. Furthermore, medium-size breweries could obtain higher relative benefits by providing energy flexibility, especially those producing lager and ale. The result also shows that the breweries' relative saving potential is electricity market-dependent.",
        "subjects": [
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14907",
        "abstract url": "https://arxiv.org/abs/2401.14907",
        "title": "Learning Local Control Barrier Functions for Safety Control of Hybrid Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Hybrid dynamical systems are ubiquitous as practical robotic applications often involve both continuous states and discrete switchings. Safety is a primary concern for hybrid robotic systems. Existing safety-critical control approaches for hybrid systems are either computationally inefficient, detrimental to system performance, or limited to small-scale systems. To amend these drawbacks, in this paper, we propose a learningenabled approach to construct local Control Barrier Functions (CBFs) to guarantee the safety of a wide class of nonlinear hybrid dynamical systems. The end result is a safe neural CBFbased switching controller. Our approach is computationally efficient, minimally invasive to any reference controller, and applicable to large-scale systems. We empirically evaluate our framework and demonstrate its efficacy and flexibility through two robotic examples including a high-dimensional autonomous racing case, against other CBF-based approaches and model predictive control.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14909",
        "abstract url": "https://arxiv.org/abs/2401.14909",
        "title": "A Simulation Preorder for Koopman-like Lifted Control Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper introduces a simulation preorder among lifted systems, a generalization of finite-dimensional Koopman approximations (also known as approximate immersions) to systems with inputs. It is proved that this simulation relation implies the containment of both the open- and closed-loop behaviors. Optimization-based sufficient conditions are derived to verify the simulation relation in two special cases: i) a nonlinear (unlifted) system and an affine lifted system and, ii) two affine lifted systems. Numerical examples demonstrate the approach using backward reachable sets.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 1 figure"
    },
    {
        "paper id": "2401.14915",
        "abstract url": "https://arxiv.org/abs/2401.14915",
        "title": "Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students",
        "rating": -10,
        "keywords": [],
        "abstract": "The increasing use of Artificial Intelligence (AI) by students in learning presents new challenges for assessing their learning outcomes in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students' AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students' use of AI in PBL and ways of analyzing these uses grounded by students' vision of education goal transformation. We also found students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand the use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Conditionally accepted by CHI '24"
    },
    {
        "paper id": "2401.14916",
        "abstract url": "https://arxiv.org/abs/2401.14916",
        "title": "Proving Information Inequalities by Gaussian Elimination",
        "rating": -10,
        "keywords": [],
        "abstract": "The proof of information inequalities and identities under linear constraints on the information measures is an important problem in information theory. For this purpose, ITIP and other variant algorithms have been developed and implemented, which are all based on solving a linear program (LP). In this paper, we develop a method with symbolic computation. Compared with the known methods, our approach can completely avoids the use of linear programming which may cause numerical errors. Our procedures are also more efficient computationally.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2202.02786"
    },
    {
        "paper id": "2401.14920",
        "abstract url": "https://arxiv.org/abs/2401.14920",
        "title": "Hold Tight: Identifying Behavioral Patterns During Prolonged Work in VR through Video Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "VR devices have recently been actively promoted as tools for knowledge workers and prior work has demonstrated that VR can support some knowledge worker tasks. However, only a few studies have explored the effects of prolonged use of VR such as a study observing 16 participant working in VR and a physical environment for one work-week each and reporting mainly on subjective feedback. As a nuanced understanding of participants' behavior in VR and how it evolves over time is still missing, we report on the results from an analysis of 559 hours of video material obtained in this prior study. Among other findings, we report that (1) the frequency of actions related to adjusting the headset reduced by 46% and the frequency of actions related to supporting the headset reduced by 42% over the five days; (2) the HMD was removed 31% less frequently over the five days but for 41% longer periods; (3) wearing an HMD is disruptive to normal patterns of eating and drinking, but not to social interactions, such as talking. The combined findings in this work demonstrate the value of long-term studies of deployed VR systems and can be used to inform the design of better, more ergonomic VR systems as tools for knowledge workers.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14926",
        "abstract url": "https://arxiv.org/abs/2401.14926",
        "title": "Frictional contact of soft polymeric shells",
        "rating": -10,
        "keywords": [],
        "abstract": "The classical Hertzian contact model establishes a monotonic correlation between contact force and area. Here, we showed that the interplay between local friction and structural instability can deliberately lead to unconventional contact behavior when a soft elastic shell comes into contact with a flat surface. The deviation from Hertzian contact first arises from bending within the contact area, followed by the second transition induced by buckling, resulting in a notable decrease in the contact area despite increased contact force. Friction delays both transitions and introduces hysteresis during unloading. However, a high amount of friction suppresses both buckling and dissipation. Different contact regimes are discussed in terms of rolling and sliding mechanisms, providing insights for tailoring contact behaviors in soft shells.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14936",
        "abstract url": "https://arxiv.org/abs/2401.14936",
        "title": "Reassessing Java Code Readability Models with a Human-Centered Approach",
        "rating": -10,
        "keywords": [],
        "abstract": "To ensure that Large Language Models (LLMs) effectively support user productivity, they need to be adjusted. Existing Code Readability (CR) models can guide this alignment. However, there are concerns about their relevance in modern software engineering since they often miss the developers' notion of readability and rely on outdated code. This research assesses existing Java CR models for LLM adjustments, measuring the correlation between their and developers' evaluations of AI-generated Java code. Using the Repertory Grid Technique with 15 developers, we identified 12 key code aspects influencing CR that were consequently assessed by 390 programmers when labeling 120 AI-generated snippets. Our findings indicate that when AI generates concise and executable code, it is often considered readable by CR models and developers. However, a limited correlation between these evaluations underscores the importance of future research on learning objectives for adjusting LLMs and on the aspects influencing CR evaluations included in predictive models.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Accepted to ICPC'24, co-located with ICSE'24. 11 pages, 1 figure"
    },
    {
        "paper id": "2401.14943",
        "abstract url": "https://arxiv.org/abs/2401.14943",
        "title": "Analysing the Influence of Macroeconomic Factors on Credit Risk in the UK Banking Sector",
        "rating": -10,
        "keywords": [],
        "abstract": "Macroeconomic factors have a critical impact on banking credit risk, which cannot be directly controlled by banks, and therefore, there is a need for an early credit risk warning system based on the macroeconomy. By comparing different predictive models (traditional statistical and machine learning algorithms), this study aims to examine the macroeconomic determinants impact on the UK banking credit risk and assess the most accurate credit risk estimate using predictive analytics. This study found that the variance-based multi-split decision tree algorithm is the most precise predictive model with interpretable, reliable, and robust results. Our model performance achieved 95% accuracy and evidenced that unemployment and inflation rate are significant credit risk predictors in the UK banking context. Our findings provided valuable insights such as a positive association between credit risk and inflation, the unemployment rate, and national savings, as well as a negative relationship between credit risk and national debt, total trade deficit, and national income. In addition, we empirically showed the relationship between national savings and non-performing loans, thus proving the paradox of thrift. These findings benefit the credit risk management team in monitoring the macroeconomic factors thresholds and implementing critical reforms to mitigate credit risk.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "21 pages, 10 figures, published in Analytics 2024, Volume 3, Issue 1, 63-83"
    },
    {
        "paper id": "2401.14949",
        "abstract url": "https://arxiv.org/abs/2401.14949",
        "title": "Renewable energy exporting consumption-oriented transfer limit switching control: A unsupervised learning-based method",
        "rating": -10,
        "keywords": [],
        "abstract": "A method for generating unsupervised conditional mapping rules for multi-inter-corridor transfer limits and their integration into unit commitment through banding-switching is proposed in this paper. The method starts by using Ant colony clustering(ACC) to identify different operating modes with renewable energy penetration. For each sub-pattern, coupling inter-corridors are determined using correlation coefficients. An algorithm for constructing coupled inter-corridors' limits boundaries, employing grid partitioning, is proposed to establish conditional mappings from sub-patterns to multi-inter-corridor limits. Additionally, a banding matching model is proposed, incorporating distance criteria and the Big-M method. It also includes a limit-switching method based on Lagrange multipliers. Case studies on the IEEE 39-node system illustrate the effectiveness of this method in increasing consumption of renewable energy and reducing operational costs while adhering to stability verification requirements.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14952",
        "abstract url": "https://arxiv.org/abs/2401.14952",
        "title": "Could AI change the scientific publishing market once and for all?",
        "rating": -10,
        "keywords": [],
        "abstract": "Artificial-intelligence tools in research like ChatGPT are playing an increasingly transformative role in revolutionizing scientific publishing and re-shaping its economic background. They can help academics to tackle such issues as limited space in academic journals, accessibility of knowledge, delayed dissemination, or the exponential growth of academic output. Moreover, AI tools could potentially change scientific communication and academic publishing market as we know them. They can help to promote Open Access (OA) in the form of preprints, dethrone the entrenched journals and publishers, as well as introduce novel approaches to the assessment of research output. It is also imperative that they should do just that, once and for all.",
        "subjects": [
            "econ.TH"
        ],
        "comment": "5 pages, 1 figure, 1 table"
    },
    {
        "paper id": "2401.14957",
        "abstract url": "https://arxiv.org/abs/2401.14957",
        "title": "Declassification Policy for Program Complexity Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "In automated complexity analysis, noninterference-based type systems statically guarantee, via soundness, the property that well-typed programs compute functions of a given complexity class, e.g., the class FP of functions computable in polynomial time. These characterizations are also extensionally complete -- they capture all functions -- but are not intensionally complete as some polytime algorithms are rejected. This impact on expressive power is an unavoidable cost of achieving a tractable characterization. To overcome this issue, an avenue arising from security applications is to find a relaxation of noninterference based on a declassification mechanism that allows critical data to be released in a safe and controlled manner. Following this path, we present a new and intuitive declassification policy preserving FP-soundness and capturing strictly more programs than existing noninterference-based systems. We show the versatility of the approach: it also provides a new characterization of the class BFF of second-order polynomial time computable functions in a second-order imperative language, with first-order procedure calls. Type inference is tractable: it can be done in polynomial time.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14962",
        "abstract url": "https://arxiv.org/abs/2401.14962",
        "title": "Joint Data and Semantics Lossy Compression: Nonasymptotic and Second-Order Achievability Bounds",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper studies a joint data and semantics lossy compression problem in the finite blocklength regime, where the data and semantic sources are correlated, and only the data source can be observed by the encoder. We first introduce an information-theoretic nonasymptotic analysis framework to investigate the nonasymptotic fundamental limits of our studied problem. Within this framework, general nonasymptotic achievability bounds valid for general sources and distortion measures are derived. Moreover, we provide a second-order achievability bound in the standard block coding setting by applying the two-dimensional Berry-Esseen theorem to our nonasymptotic bounds. Compared with first-order asymptotic bounds, our results have the potential to provide unique insights for the design of practical semantic communication systems.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "12 pages, 1 figure"
    },
    {
        "paper id": "2401.14963",
        "abstract url": "https://arxiv.org/abs/2401.14963",
        "title": "On the Hardness of Gray Code Problems for Combinatorial Objects",
        "rating": -10,
        "keywords": [],
        "abstract": "Can a list of binary strings be ordered so that consecutive strings differ in a single bit? Can a list of permutations be ordered so that consecutive permutations differ by a swap? Can a list of non-crossing set partitions be ordered so that consecutive partitions differ by refinement? These are examples of Gray coding problems: Can a list of combinatorial objects (of a particular type and size) be ordered so that consecutive objects differ by a flip (of a particular type)? For example, 000, 001, 010, 100 is a no instance of the first question, while 1234, 1324, 1243 is a yes instance of the second question due to the order 1243, 1234, 1324. We prove that a variety of Gray coding problems are NP-complete using a new tool we call a Gray code reduction.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "15 pages, 5 figures, WALCOM 2024"
    },
    {
        "paper id": "2401.14965",
        "abstract url": "https://arxiv.org/abs/2401.14965",
        "title": "An Improved Lower Bound on Oblivious Transfer Capacity via Interactive Erasure Emulation",
        "rating": -10,
        "keywords": [],
        "abstract": "We revisit the oblivious transfer (OT) capacities of noisy channels against the passive adversary, which have been identified only for a limited class of channels. In the literature, the general construction of oblivious transfer has been known only for generalized erasure channels (GECs); for other channels, we first convert a given channel to a GEC via alphabet extension and erasure emulation, and then apply the general construction for GEC. In this paper, we derive an improved lower bound on the OT capacity of the binary symmetric channel (BSC) and binary symmetric erasure channel (BSEC) by proposing a new protocol; by using interactive communication between the sender and the receiver, our protocol emulates erasure events recursively in multiple rounds. We also discuss a potential necessity of multiple rounds interactive communication to attain the OT capacity.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "6 pages, 2 figures"
    },
    {
        "paper id": "2401.14970",
        "abstract url": "https://arxiv.org/abs/2401.14970",
        "title": "Microwave lymphedema assessment using deep learning with contour assisted backprojection",
        "rating": -10,
        "keywords": [],
        "abstract": "We present a method for early detection of lymphatic fluid accumulation in lymphedema patients based on microwave imaging of the limb volume across an air gap. The proposed algorithm uses contour information of the imaged limb surface to approximate the wave propagation velocity locally to solve the eikonal equation for implementing the adjoint imaging operator. This modified backprojection algorithm results in focused imagery close to the limb surface where lymphatic fluid accumulation presents itself. Next, a deep neural network based on U-Net architecture is employed to identify the location and extent of the lymphatic fluid. Simulation studies with various upper and lower arm profiles compare the focusing performance of the proposed contour assisted backprojection imaging with the baseline imaging approach that assumes homogeneous media. The empirical results of the simulation experiments show that the proposed imaging method significantly improves the ability of the deepnet model to identify the location and the volume of the excess fluid in the limb.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "6 pages, 6 figures, accepted RadarConf"
    },
    {
        "paper id": "2401.14973",
        "abstract url": "https://arxiv.org/abs/2401.14973",
        "title": "Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models",
        "rating": -10,
        "keywords": [],
        "abstract": "We seek to model a collection of time series arising from multiple entities interacting over the same time period. Recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities. To address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics. We employ a latent system-level discrete state Markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series. Feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions. Our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of individual time series. This is asymptotically no more costly than fitting separate models for each entity. Experiments on synthetic and real datasets show that our model can produce better forecasts of future entity behavior than existing methods. Moreover, the availability of latent state chains at both the entity and system level enables interpretation of group dynamics.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14975",
        "abstract url": "https://arxiv.org/abs/2401.14975",
        "title": "Embedding-based search in JetBrains IDEs",
        "rating": -10,
        "keywords": [],
        "abstract": "Most modern Integrated Development Environments (IDEs) and code editors have a feature to search across available functionality and items in an open project. In JetBrains IDEs, this feature is called Search Everywhere: it allows users to search for files, actions, classes, symbols, settings, and anything from VCS history from a single entry point. However, it works with the candidates obtained by algorithms that don't account for semantics, e.g., synonyms, complex word permutations, part of the speech modifications, and typos. In this work, we describe the machine learning approach we implemented to improve the discoverability of search items. We also share the obstacles encountered during this process and how we overcame them.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.14978",
        "abstract url": "https://arxiv.org/abs/2401.14978",
        "title": "Robust Dual-Modal Speech Keyword Spotting for XR Headsets",
        "rating": -10,
        "keywords": [],
        "abstract": "While speech interaction finds widespread utility within the Extended Reality (XR) domain, conventional vocal speech keyword spotting systems continue to grapple with formidable challenges, including suboptimal performance in noisy environments, impracticality in situations requiring silence, and susceptibility to inadvertent activations when others speak nearby. These challenges, however, can potentially be surmounted through the cost-effective fusion of voice and lip movement information. Consequently, we propose a novel vocal-echoic dual-modal keyword spotting system designed for XR headsets. We devise two different modal fusion approches and conduct experiments to test the system's performance across diverse scenarios. The results show that our dual-modal system not only consistently outperforms its single-modal counterparts, demonstrating higher precision in both typical and noisy environments, but also excels in accurately identifying silent utterances. Furthermore, we have successfully applied the system in real-time demonstrations, achieving promising results. The code is available at https://github.com/caizhuojiang/VE-KWS.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Accepted to IEEE VR 2024"
    },
    {
        "paper id": "2401.14983",
        "abstract url": "https://arxiv.org/abs/2401.14983",
        "title": "Quota management in dCache or making a perfectly normal file system normal",
        "rating": -10,
        "keywords": [],
        "abstract": "dCache (https://dcache.org) is a highly scalable storage system providing location-independent access to data. The data are stored across multiple data servers as complete files presented to the end-user via a single-rooted namespace. From its inception, dCache has been designed as a caching disk buffer to a tertiary tape storage system with the assumption that the latter has virtually unlimited capacity. dCache can also be configured as a disk-only storage system with no tape backend. Owing to the idea that a tape resource is infinite, or purely physically limited by budget considerations, the system has never provided for any restrictions on how much data can be stored on tape. Likewise, in the disk-only configuration, the capacity of the system is only limited by the aggregate disk capacity of the data servers. In a multi-user environment, however, this has become problematic. This presentation will describe the design and implementation of a user- and group-based quota system, that allows to manage tape and disk space allocations, as part of dCache namespace.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "26th Intl Conf Computing High Energy & Nuclear Phys (CHEP 2023)"
    },
    {
        "paper id": "2401.14996",
        "abstract url": "https://arxiv.org/abs/2401.14996",
        "title": "A Resolution-Based Interactive Proof System for UNSAT",
        "rating": -10,
        "keywords": [],
        "abstract": "Modern SAT or QBF solvers are expected to produce correctness certificates. However, certificates have worst-case exponential size (unless $\\textsf{NP}=\\textsf{coNP}$), and at recent SAT competitions the largest certificates of unsatisfiability are starting to reach terabyte size. Recently, Couillard, Czerner, Esparza, and Majumdar have suggested to replace certificates with interactive proof systems based on the $\\textsf{IP}=\\textsf{PSPACE}$ theorem. They have presented an interactive protocol between a prover and a verifier for an extension of QBF. The overall running time of the protocol is linear in the time needed by a standard BDD-based algorithm, and the time invested by the verifier is polynomial in the size of the formula. (So, in particular, the verifier never has to read or process exponentially long certificates). We call such an interactive protocol competitive with the BDD algorithm for solving QBF. While BDD-algorithms are state-of-the-art for certain classes of QBF instances, no modern (UN)SAT solver is based on BDDs. For this reason, we initiate the study of interactive certification for more practical SAT algorithms. In particular, we address the question whether interactive protocols can be competitive with some variant of resolution. We present two contributions. First, we prove a theorem that reduces the problem of finding competitive interactive protocols to finding an arithmetisation of formulas satisfying certain commutativity properties. (Arithmetisation is the fundamental technique underlying the $\\textsf{IP}=\\textsf{PSPACE}$ theorem.) Then, we apply the theorem to give the first interactive protocol for the Davis-Putnam resolution procedure.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "21 pages"
    },
    {
        "paper id": "2401.14999",
        "abstract url": "https://arxiv.org/abs/2401.14999",
        "title": "The causal role of the Reddit collective action on the GameStop short squeeze",
        "rating": -10,
        "keywords": [],
        "abstract": "In early 2021, the stock prices of GameStop, AMC, Nokia, and BlackBerry experienced dramatic increases, triggered by short squeeze operations that have been largely attributed to Reddit's retail investors. These events showcased, for the first time, the potential of online social networks to catalyze financial collective action. How, when and to what extent Reddit users played a causal role in driving up these prices, however, remains unclear. To address these questions, we employ causal inference techniques, leveraging data capturing activity on Reddit and Twitter, and trading volume with a high temporal resolution. We find that Reddit discussions foreshadowed trading volume before the GameStop short squeeze, with their predictive power being particularly strong on hourly time scales. This effect emerged abruptly and became prominent a few weeks before the event, but waned once the community of investors gained widespread visibility through Twitter. As the causal link unfolded, the collective investment of the Reddit community, quantified through each user's financial position on GameStop, closely mirrored the market capitalization of the stock. The evidence from our study suggests that Reddit users fueled the GameStop short squeeze, and thereby Reddit served as a coordination hub for a shared financial strategy. Towards the end of January, users talking about GameStop contributed to raise the popularity of BlackBerry, AMC and Nokia, which emerged as the most popular stocks as the community gained global recognition. Overall, our results shed light on the dynamics behind the first large-scale financial collective action driven by social media users.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15008",
        "abstract url": "https://arxiv.org/abs/2401.15008",
        "title": "Reinforcement Learning-based Relay Selection for Cooperative WSNs in the Presence of Bursty Impulsive Noise",
        "rating": -10,
        "keywords": [],
        "abstract": "The problem of relay selection is pivotal in the realm of cooperative communication. However, this issue has not been thoroughly examined, particularly when the background noise is assumed to possess an impulsive characteristic with consistent memory as observed in smart grid communications and some other wireless communication scenarios. In this paper, we investigate the impact of this specific type of noise on the performance of cooperative Wireless Sensor Networks (WSNs) with the Decode and Forward (DF) relaying scheme, considering Symbol-Error-Rate (SER) and battery power consumption fairness across all nodes as the performance metrics. We introduce two innovative relay selection methods that depend on noise state detection and the residual battery power of each relay. The first method encompasses the adaptation of the Max-Min criterion to this specific context, whereas the second employs Reinforcement Learning (RL) to surmount this challenge. Our empirical outcomes demonstrate that the impacts of bursty impulsive noise on the SER performance can be effectively mitigated and that a balance in battery power consumption among all nodes can be established using the proposed methods.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Accepted in 2024 IEEE Wireless Communications and Networking Conference"
    },
    {
        "paper id": "2401.15028",
        "abstract url": "https://arxiv.org/abs/2401.15028",
        "title": "User Association Optimization for IRS-aided Terahertz Networks: A Matching Theory Approach",
        "rating": -10,
        "keywords": [],
        "abstract": "Terahertz (THz) communication is a promising technology for future wireless communications, offering data rates of up to several terabits-per-second (Tbps). However, the range of THz band communications is often limited by high pathloss and molecular absorption. To overcome these challenges, this paper proposes intelligent reconfigurable surfaces (IRSs) to enhance THz communication systems. Specifically, we introduce an angle-based trigonometric channel model to evaluate the effectiveness of IRS-aided THz networks. Additionally, to maximize the sum rate, we formulate the source-IRS-destination matching problem, which is a mixed-integer nonlinear programming (MINLP) problem. To solve this non-deterministic polynomial-time hard (NP-hard) problem, the paper proposes a Gale-Shapley-based solution that obtains stable matches between sources and IRSs, as well as between destinations and IRSs in the first and second sub-problems, respectively.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15032",
        "abstract url": "https://arxiv.org/abs/2401.15032",
        "title": "Color Maker: a Mixed-Initiative Approach to Creating Accessible Color Maps",
        "rating": -10,
        "keywords": [],
        "abstract": "Quantitative data is frequently represented using color, yet designing effective color mappings is a challenging task, requiring one to balance perceptual standards with personal color preference. Current design tools either overwhelm novices with complexity or offer limited customization options. We present ColorMaker, a mixed-initiative approach for creating colormaps. ColorMaker combines fluid user interaction with real-time optimization to generate smooth, continuous color ramps. Users specify their loose color preferences while leaving the algorithm to generate precise color sequences, meeting both designer needs and established guidelines. ColorMaker can create new colormaps, including designs accessible for people with color-vision deficiencies, starting from scratch or with only partial input, thus supporting ideation and iterative refinement. We show that our approach can generate designs with similar or superior perceptual characteristics to standard colormaps. A user study demonstrates how designers of varying skill levels can use this tool to create custom, high-quality colormaps. ColorMaker is available at https://colormaker.org",
        "subjects": [
            "cs.HC"
        ],
        "comment": "To appear at the ACM CHI '24 Conference on Human Factors in Computing Systems"
    },
    {
        "paper id": "2401.15034",
        "abstract url": "https://arxiv.org/abs/2401.15034",
        "title": "Explicit Subcodes of Reed-Solomon Codes that Efficiently Achieve List Decoding Capacity",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we introduce a novel explicit family of subcodes of Reed-Solomon (RS) codes that efficiently achieve list decoding capacity with a constant output list size. Our approach builds upon the idea of large linear subcodes of RS codes evaluated on a subfield, similar to the method employed by Guruswami and Xing (STOC 2013). However, our approach diverges by leveraging the idea of {\\it permuted product codes}, thereby simplifying the construction by avoiding the need of {\\it subspace designs}. Specifically, the codes are constructed by initially forming the tensor product of two RS codes with carefully selected evaluation sets, followed by specific cyclic shifts to the codeword rows. This process results in each codeword column being treated as an individual coordinate, reminiscent of prior capacity-achieving codes, such as folded RS codes and univariate multiplicity codes. This construction is easily shown to be a subcode of an interleaved RS code, equivalently, an RS code evaluated on a subfield. Alternatively, the codes can be constructed by the evaluation of bivariate polynomials over orbits generated by \\emph{two} affine transformations with coprime orders, extending the earlier use of a single affine transformation in folded RS codes and the recent affine folded RS codes introduced by Bhandari {\\it et al.} (IEEE T-IT, Feb.~2024). While our codes require large, yet constant characteristic, the two affine transformations facilitate achieving code length equal to the field size, without the restriction of the field being prime, contrasting with univariate multiplicity codes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "20 pages"
    },
    {
        "paper id": "2401.15035",
        "abstract url": "https://arxiv.org/abs/2401.15035",
        "title": "Chaos-Based Bitwise Dynamical Pseudorandom Number Generator on FPGA",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, a new pseudorandom number generator (PRNG) based on the logistic map has been proposed. To prevent the system to fall into short period orbits as well as increasing the randomness of the generated sequences, the proposed algorithm dynamically changes the parameters of the chaotic system. This PRNG has been implemented in a Virtex 7 field-programmable gate array (FPGA) with a 32-bit fixed point precision, using a total of 510 lookup tables (LUTs) and 120 registers. The sequences generated by the proposed algorithm have been subjected to the National Institute of Standards and Technology (NIST) randomness tests, passing all of them. By comparing the randomness with the sequences generated by a raw 32-bit logistic map, it is shown that, by using only an additional 16% of LUTs, the proposed PRNG obtains a much better performance in terms of randomness, increasing the NIST passing rate from 0.252 to 0.989. Finally, the proposed bitwise dynamical PRNG is compared with other chaos-based realizations previously proposed, showing great improvement in terms of resources and randomness.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15037",
        "abstract url": "https://arxiv.org/abs/2401.15037",
        "title": "On symmetries of spheres in univalent foundations",
        "rating": -10,
        "keywords": [],
        "abstract": "Working in univalent foundations, we investigate the symmetries of spheres, i.e., the types of the form $\\mathbb{S}^n = \\mathbb{S}^n$. The case of the circle has a slick answer: the symmetries of the circle form two copies of the circle. For higher-dimensional spheres, the type of symmetries has again two connected components, namely the components of the maps of degree plus or minus one. Each of the two components has $\\mathbb{Z}/2\\mathbb{Z}$ as fundamental group. For the latter result, we develop an EHP long exact sequence.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "comments welcome"
    },
    {
        "paper id": "2401.15041",
        "abstract url": "https://arxiv.org/abs/2401.15041",
        "title": "Computationally Bounded Robust Compilation and Universally Composable Security",
        "rating": -10,
        "keywords": [],
        "abstract": "Universal Composability (UC) is the gold standard for cryptographic security, but mechanizing proofs of UC is notoriously difficult. A recently-discovered connection between UC and Robust Compilation (RC)$\\unicode{x2014}$a novel theory of secure compilation$\\unicode{x2014}$provides a means to verify UC proofs using tools that mechanize equality results. Unfortunately, the existing methods apply only to perfect UC security, and real-world protocols relying on cryptography are only computationally secure. This paper addresses this gap by lifting the connection between UC and RC to the computational setting, extending techniques from the RC setting to apply to computational UC security. Moreover, it further generalizes the UC$\\unicode{x2013}$RC connection beyond computational security to arbitrary equalities, providing a framework to subsume the existing perfect case, and to instantiate future theories with more complex notions of security. This connection allows the use of tools for proofs of computational indistinguishability to properly mechanize proofs of computational UC security. We demonstrate this power by using CryptoVerif to mechanize a proof that parts of the Wireguard protocol are computationally UC secure. Finally, all proofs of the framework itself are verified in Isabelle/HOL.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15056",
        "abstract url": "https://arxiv.org/abs/2401.15056",
        "title": "Subset Adaptive Relaying for Streaming Erasure Codes",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper investigates adaptive streaming codes over a three-node relayed network. In this setting, a source transmits a sequence of message packets through a relay under a delay constraint of $T$ time slots per packet. The source-to-relay and relay-to-destination links are unreliable and introduce a maximum of $N_1$ and $N_2$ packet erasures respectively. Recent work has proposed adaptive (time variant) and nonadaptive (time invariant) code constructions for this setting and has shown that adaptive codes can achieve higher rates. However, the adaptive construction deals with many possibilities, leading to an impractical code with very large block lengths. In this work, we propose a simplified adaptive code construction which greatly improves the practicality of the code, with only a small cost to the achievable rates. We analyze the construction in terms of the achievable rates and field size requirements, and perform numerical simulations over statistical channels to estimate packet loss probabilities.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15127",
        "abstract url": "https://arxiv.org/abs/2401.15127",
        "title": "Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness",
        "rating": -10,
        "keywords": [],
        "abstract": "Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence (CTI). In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly, Stanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary classification and Named Entity Recognition (NER) tasks performed using Open Source INTelligence (OSINT). We utilize well-established data collected in previous research from Twitter to assess the competitiveness of these chatbots when compared to specialized models trained for those tasks. In binary classification experiments, Chatbot GPT-4 as a commercial model achieved an acceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1 score of 0.90. However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective. This study demonstrates the capability of chatbots for OSINT binary classification and shows that they require further improvement in NER to effectively replace specially trained models. Our results shed light on the limitations of the LLM chatbots when compared to specialized models, and can help researchers improve chatbots technology with the objective to reduce the required effort to integrate machine learning in OSINT-based CTI tools.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15129",
        "abstract url": "https://arxiv.org/abs/2401.15129",
        "title": "Instantaneous Power Theory Revisited with Classical Mechanics",
        "rating": -10,
        "keywords": [],
        "abstract": "The paper revisits the concepts of instantaneous active and reactive powers and provides a novel definition for basic circuit elements based on quantities utilized in classical mechanics, such as absolute and relative velocity, momentum density, angular momentum and apparent forces. The discussion leverages from recent publications by the authors that interpret the voltage and current as velocities in generalized Lagrangian coordinates. The main result of the paper is a general and compact expression for the instantaneous active and reactive power of inductances, capacitances and resistances as a multivector proportional to the generalized kinetic energy and the geometric frequency multivector. Several numerical examples considering stationary and transient sinusoidal and non-sinusoidal conditions are discussed in the case study.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "14 pages, 18 figures"
    },
    {
        "paper id": "2401.15130",
        "abstract url": "https://arxiv.org/abs/2401.15130",
        "title": "Dichromatic Number and Cycle Inversions",
        "rating": -10,
        "keywords": [],
        "abstract": "The results of this note were stated in the first author PhD manuscript in 2006 but never published. The writing of a proof given there was slightly careless and the proof itself scattered across the document, the goal of this note is to give a short and clear proof using Farkas Lemma. The first result is a characterization of the acyclic chromatic number of a digraph in terms of cyclic ordering. Using this theorem we prove that for any digraph, one can sequentially reverse the orientations of the arcs of a family of directed cycles so that the resulting digraph has acyclic chromatic number at most 2.",
        "subjects": [
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15132",
        "abstract url": "https://arxiv.org/abs/2401.15132",
        "title": "On the Emergence of Symmetrical Reality",
        "rating": -10,
        "keywords": [],
        "abstract": "Artificial intelligence (AI) has revolutionized human cognitive abilities and facilitated the development of new AI entities capable of interacting with humans in both physical and virtual environments. Despite the existence of virtual reality, mixed reality, and augmented reality for several years, integrating these technical fields remains a formidable challenge due to their disparate application directions. The advent of AI agents, capable of autonomous perception and action, further compounds this issue by exposing the limitations of traditional human-centered research approaches. It is imperative to establish a comprehensive framework that accommodates the dual perceptual centers of humans and AI agents in both physical and virtual worlds. In this paper, we introduce the symmetrical reality framework, which offers a unified representation encompassing various forms of physical-virtual amalgamations. This framework enables researchers to better comprehend how AI agents can collaborate with humans and how distinct technical pathways of physical-virtual integration can be consolidated from a broader perspective. We then delve into the coexistence of humans and AI, demonstrating a prototype system that exemplifies the operation of symmetrical reality systems for specific tasks, such as pouring water. Subsequently, we propose an instance of an AI-driven active assistance service that illustrates the potential applications of symmetrical reality. This paper aims to offer beneficial perspectives and guidance for researchers and practitioners in different fields, thus contributing to the ongoing research about human-AI coexistence in both physical and virtual environments.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "IEEE VR 2024"
    },
    {
        "paper id": "2401.15138",
        "abstract url": "https://arxiv.org/abs/2401.15138",
        "title": "Chaotic Encryption for 10-Gb Ethernet Optical Links",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, a new physical layer encryption method for optical 10-Gb Ethernet links is proposed. Necessary modifications to introduce encryption in Ethernet 10GBase-R standard have been considered. This security enhancement has consisted of a symmetric streaming encryption of the 64b/66b data flow at physical coding sublayer level thanks to two keystream generators based on a chaotic algorithm. The overall system has been implemented and tested in a field programmable gate array. Ethernet traffic has been encrypted, transmitted, and decrypted over a multimode optical link. Experimental results are analyzed concluding that it is possible to cipher traffic at this level and hide the complete Ethernet traffic pattern from any passive eavesdropper. In addition, no overhead is introduced during encryption, getting no losses in the total throughput.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15140",
        "abstract url": "https://arxiv.org/abs/2401.15140",
        "title": "Link Prediction Accuracy on Real-World Networks Under Non-Uniform Missing Edge Patterns",
        "rating": -10,
        "keywords": [],
        "abstract": "Real-world network datasets are typically obtained in ways that fail to capture all links, and there are many different non-uniform ways in which real data might be missing. Nevertheless, uniform missing data is a common assumption made when no additional information is available about the underlying ''missingness function.'' To investigate the impact of different missingness patterns on link prediction accuracy, we employ 9 link prediction algorithms from 4 different families to analyze 20 different missingness functions categorized into 5 groups. By studying 250 real-world network datasets, we illustrate that different prediction algorithms exhibit significant differences in accuracy contingent upon both the dataset domain and the nature of the missingness pattern. Our study thereby provides guidance for selecting appropriate prediction algorithms when encountering diverse patterns of missing data across various domains, emphasizing the importance of considering the specific characteristics of the dataset for effective link prediction.",
        "subjects": [
            "math.DS"
        ],
        "comment": "Submitted to IEEE Transaction on Network Science and Engineering"
    },
    {
        "paper id": "2401.15154",
        "abstract url": "https://arxiv.org/abs/2401.15154",
        "title": "Enhancing the Secrecy Rate with Direction-range Focusing with FDA and RIS",
        "rating": -10,
        "keywords": [],
        "abstract": "One of the great potentials to improve the confidentiality in mmWave/THz at the physical layer of technical communication, measured by the secrecy rate, lies in the use of reconfigurable intelligent surfaces (RISs). However, an important open problem arises when the eavesdropper is aligned with the legitimate user or in proximity to the RIS or legitimate user. The limitation comes, on one hand, from the high directional gain caused by the dominant line-of-sight (LOS) path in high-frequency transmission, and, on the other hand, from the high energy leakage in the proximity of the RIS and the legitimate user. To address these issues, we employ the concept of frequency diverse arrays (FDA) at the base station (BS) associated with random inverted transmit beamforming and reflective element subset selection (RIBES). More specifically, we consider a passive eavesdropper with unknown location, and design the transmit beamforming and RIS configuration based on the channel information of the legitimate user only. In this context, the secrecy rate with the proposed transmission technique is evaluated in the case of deterministic eavesdropper channel, demonstrating that we can ensure a secure transmission regarding both direction and range. Furthermore, assuming no prior information about the eavesdropper, we describe the wiretap region and derive the worst-case secrecy rate in closed form. The latter is further optimized by determining the optimal subset sizes of the transmit antennas and reflective elements. Simulations verify the correctness of the closed-form expressions and demonstrate that we can effectively improve the secrecy rate, especially when the eavesdropper is close to the RIS or the legitimate user.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15169",
        "abstract url": "https://arxiv.org/abs/2401.15169",
        "title": "Estimating Cloth Elasticity Parameters From Homogenized Yarn-Level Models",
        "rating": -10,
        "keywords": [],
        "abstract": "Virtual garment simulation has become increasingly important with applications in garment design and virtual try-on. However, reproducing garments faithfully remains a cumbersome process. We propose an end-to-end method for estimating parameters of shell material models corresponding to real fabrics with minimal priors. Our method determines yarn model properties from information directly obtained from real fabrics, unlike methods that require expensive specialized capture systems. We use an extended homogenization method to match yarn-level and shell-level hyperelastic energies with respect to a range of surface deformations represented by the first and second fundamental forms, including bending along the diagonal to warp and weft directions. We optimize the parameters of a shell deformation model involving uncoupled bending and membrane energies. This allows the simulated model to exhibit nonlinearity and anisotropy seen in real cloth. Finally, we validate our results with quantitative and visual comparisons against real world fabrics through stretch tests and drape experiments. Our homogenized shell models not only capture the characteristics of underlying yarn patterns, but also exhibit distinct behaviors for different yarn materials.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15182",
        "abstract url": "https://arxiv.org/abs/2401.15182",
        "title": "App Planner: Utilizing Generative AI in K-12 Mobile App Development Education",
        "rating": -10,
        "keywords": [],
        "abstract": "App Planner is an interactive support tool for K-12 students, designed to assist in creating mobile applications. By utilizing generative AI, App Planner helps students articulate the problem and solution through guided conversations via a chat-based interface. It assists them in brainstorming and formulating new ideas for applications, provides feedback on those ideas, and stimulates creative thinking. Here we report usability tests from our preliminary study with high-school students who appreciated App Planner for aiding the app design process and providing new viewpoints on human aspects especially the potential negative impact of their creation.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "7 pages, 1 figure, 1 table"
    },
    {
        "paper id": "2401.15186",
        "abstract url": "https://arxiv.org/abs/2401.15186",
        "title": "Algebraic Approach to Approximation",
        "rating": -10,
        "keywords": [],
        "abstract": "Following the success of the so-called algebraic approach to the study of decision constraint satisfaction problems (CSPs), exact optimization of valued CSPs, and most recently promise CSPs, we propose an algebraic framework for valued promise CSPs. To every valued promise CSP we associate an algebraic object, its so-called valued minion. Our main result shows that the existence of a homomorphism between the associated valued minions implies a polynomial-time reduction between the original CSPs. We also show that this general reduction theorem includes important inapproximability results, for instance, the inapproximability of almost solvable systems of linear equations beyond the random assignment threshold.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15189",
        "abstract url": "https://arxiv.org/abs/2401.15189",
        "title": "SBFT Tool Competition 2024 -- Python Test Case Generation Track",
        "rating": -10,
        "keywords": [],
        "abstract": "Test case generation (TCG) for Python poses distinctive challenges due to the language's dynamic nature and the absence of strict type information. Previous research has successfully explored automated unit TCG for Python, with solutions outperforming random test generation methods. Nevertheless, fundamental issues persist, hindering the practical adoption of existing test case generators. To address these challenges, we report on the organization, challenges, and results of the first edition of the Python Testing Competition. Four tools, namely UTBotPython, Klara, Hypothesis Ghostwriter, and Pynguin were executed on a benchmark set consisting of 35 Python source files sampled from 7 open-source Python projects for a time budget of 400 seconds. We considered one configuration of each tool for each test subject and evaluated the tools' effectiveness in terms of code and mutation coverage. This paper describes our methodology, the analysis of the results together with the competing tools, and the challenges faced while running the competition experiments.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "4 pages, to appear in the Proceedings of the 17th International Workshop on Search-Based and Fuzz Testing (SBFT@ICSE 2024)"
    },
    {
        "paper id": "2401.15195",
        "abstract url": "https://arxiv.org/abs/2401.15195",
        "title": "Bounded-degree Low Rank Parity Check Codes",
        "rating": -10,
        "keywords": [],
        "abstract": "Low rank parity check (LRPC) codes are the rank-metric analogue of low density parity check codes. In this paper we investigate a sub-family of LRPC codes, which have a parity check matrix defined over a subspace $V_{\u03b1,d}=\\langle 1,\u03b1, \\ldots, \u03b1^{d-1}\\rangle_{\\mathbb{F}_q}\\subsetneq \\mathbb{F}_{q^m}$, where $\\mathbb{F}_{q^m}$ is the finite field of $q^m$ elements and $d$ is significantly smaller than $m $. These codes are named bounded-degree LRPC (BD-LRPC) codes and are the same as the standard LRPC codes of density $2$ when the degree $d=2$, while BD-LRPC codes of degree $d>2$ constitute a proper subset of LRPC codes of density $d$. Exploiting the particular structure of their parity check matrix, we show that the BD-LRPC codes of degree $d$ can uniquely correct errors of rank weight $r$ when $n-k \\geq r + u$ for certain $u \\geq 1$, in contrast to the condition $n-k\\geq dr$ required for the standard LRPC codes, where $d\\geq n/(n-k)$. This underscores the superior decoding capability of the proposed BD-LRPC codes. As the code length $n$ approaches infinity, when $n/m\\rightarrow 0$, it is shown that $u$ can be chosen as a certain constant, which indicates that the BD-LRPC codes with a code rate of $R$ can be, with a high probability, uniquely decodable with the decoding radius $\u03c1=r/n$ approaching the Singleton bound $1-R$ for $n \\to \\infty$; and when $b= n/m$ is a constant, the BD-LRPC codes can have unique decoding radius $\u03c1= 1-R-\u03b5$ for a small $\u03b5$, which can easily lead to $\u03c1>(1-R)/2$ with properly chosen parameters.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Currently under review"
    },
    {
        "paper id": "2401.15202",
        "abstract url": "https://arxiv.org/abs/2401.15202",
        "title": "A Cross Entropy Interpretation of R{\u00e9}nyi Entropy for $\u03b1$-leakage",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes an $\u03b1$-leakage measure for $\u03b1\\in[0,\\infty)$ by a cross entropy interpretation of R{\u00e9}nyi entropy. While R\u00e9nyi entropy was originally defined as an $f$-mean for $f(t) = \\exp((1-\u03b1)t)$, we reveal that it is also a $\\tilde{f}$-mean cross entropy measure for $\\tilde{f}(t) = \\exp(\\frac{1-\u03b1}\u03b1t)$. Minimizing this R\u00e9nyi cross-entropy gives R\u00e9nyi entropy, by which the prior and posterior uncertainty measures are defined corresponding to the adversary's knowledge gain on sensitive attribute before and after data release, respectively. The $\u03b1$-leakage is proposed as the difference between $\\tilde{f}$-mean prior and posterior uncertainty measures, which is exactly the Arimoto mutual information. This not only extends the existing $\u03b1$-leakage from $\u03b1\\in [1,\\infty)$ to the overall R{\u00e9}nyi order range $\u03b1\\in [0,\\infty)$ in a well-founded way with $\u03b1=0$ referring to nonstochastic leakage, but also reveals that the existing maximal leakage is a $\\tilde{f}$-mean of an elementary $\u03b1$-leakage for all $\u03b1\\in [0,\\infty)$, which generalizes the existing pointwise maximal leakage.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "7 pages; 1 figure"
    },
    {
        "paper id": "2401.15210",
        "abstract url": "https://arxiv.org/abs/2401.15210",
        "title": "Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model",
        "rating": -10,
        "keywords": [],
        "abstract": "Query optimizers in relational database management systems (RDBMSs) search for execution plans expected to be optimal for a given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select execution plans that are suboptimal at runtime, when these estimates and assumptions are not valid, which may result in poor query performance. Therefore, query optimizers do not sufficiently support robust query optimization. Recent years have seen a surge of interest in using machine learning (ML) to improve efficiency of data systems and reduce their maintenance overheads, with promising results obtained in the area of query optimization in particular. In this paper, inspired by these advancements, and based on several years of experience of IBM Db2 in this journey, we propose Robust Optimization of Queries, (Roq), a holistic framework that enables robust query optimization based on a risk-aware learning approach. Roq includes a novel formalization of the notion of robustness in the context of query optimization and a principled approach for its quantification and measurement based on approximate probabilistic ML. It also includes novel strategies and algorithms for query plan evaluation and selection. Roq also includes a novel learned cost model that is designed to predict query execution cost and the associated risks and performs query optimization accordingly. We demonstrate experimentally that Roq provides significant improvements to robust query optimization compared to the state-of-the-art.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "13 pages, 9 figures, submitted to SIGMOD 2024"
    },
    {
        "paper id": "2401.15212",
        "abstract url": "https://arxiv.org/abs/2401.15212",
        "title": "Speed-based Filtration and DBSCAN of Event-based Camera Data with Neuromorphic Computing",
        "rating": -10,
        "keywords": [],
        "abstract": "Spiking neural networks are powerful computational elements that pair well with event-based cameras (EBCs). In this work, we present two spiking neural network architectures that process events from EBCs: one that isolates and filters out events based on their speeds, and another that clusters events based on the DBSCAN algorithm.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "8 pages, 5 figures, Submitted to Neuro Inspired Computational Elements Conference 2024"
    },
    {
        "paper id": "2401.15221",
        "abstract url": "https://arxiv.org/abs/2401.15221",
        "title": "Designing and Testing a Mobile Application for Collecting WhatsApp Chat Data While Preserving Privacy",
        "rating": -10,
        "keywords": [],
        "abstract": "It is common practice for researchers to join public WhatsApp chats and scrape their contents for analysis. However, research shows collecting data this way contradicts user expectations and preferences, even if the data is effectively public. To overcome these issues, we outline design considerations for collecting WhatsApp chat data with improved user privacy by heightening user control and oversight of data collection and taking care to minimize the data researchers collect and process off a user's device. We refer to these design principles as User-Centered Data Sharing (UCDS). To evaluate our UCDS principles, we implemented a mobile application representing one possible instance of these improved data collection techniques and evaluated the viability of using the app to collect WhatsApp chat data. Second, we surveyed WhatsApp users to gather user perceptions on common existing WhatsApp data collection methods as well as UCDS methods. Our results show that we were able to glean similar informative insights into WhatsApp chats using UCDS principles in our prototype app to common, less privacy-preserving methods. Our survey showed that methods following the UCDS principles are preferred by users because they offered users more control over the data collection process. Future user studies could further expand upon UCDS principles to overcome complications of researcher-to-group communication in research on WhatsApp chats and evaluate these principles in other data sharing contexts.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15232",
        "abstract url": "https://arxiv.org/abs/2401.15232",
        "title": "How Beginning Programmers and Code LLMs (Mis)read Each Other",
        "rating": -10,
        "keywords": [],
        "abstract": "Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Conditionally Accepted to CHI 2024"
    },
    {
        "paper id": "2401.15234",
        "abstract url": "https://arxiv.org/abs/2401.15234",
        "title": "Moving beyond Deletions: Program Simplification via Diverse Program Transformations",
        "rating": -10,
        "keywords": [],
        "abstract": "To reduce the complexity of software, Developers manually simplify program (known as developer-induced program simplification in this paper) to reduce its code size yet preserving its functionality but manual simplification is time-consuming and error-prone. To reduce manual effort, rule-based approaches (e.g., refactoring) and deletion-based approaches (e.g., delta debugging) can be potentially applied to automate developer-induced program simplification. However, as there is little study on how developers simplify programs in Open-source Software (OSS) projects, it is unclear whether these approaches can be effectively used for developer-induced program simplification. Hence, we present the first study of developer-induced program simplification in OSS projects, focusing on the types of program transformations used, the motivations behind simplifications, and the set of program transformations covered by existing refactoring types. Our study of 382 pull requests from 296 projects reveals that there exist gaps in applying existing approaches for automating developer-induced program simplification. and outlines the criteria for designing automatic program simplification techniques. Inspired by our study and to reduce the manual effort in developer-induced program simplification, we propose SimpT5, a tool that can automatically produce simplified programs (semantically-equivalent programs with reduced source lines of code). SimpT5 is trained based on our collected dataset of 92,485 simplified programs with two heuristics: (1) simplified line localization that encodes lines changed in simplified programs, and (2)checkers that measure the quality of generated programs. Our evaluation shows that SimpT5 are more effective than prior approaches in automating developer-induced program simplification.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15254",
        "abstract url": "https://arxiv.org/abs/2401.15254",
        "title": "Finite Sample Confidence Regions for Linear Regression Parameters Using Arbitrary Predictors",
        "rating": -10,
        "keywords": [],
        "abstract": "We explore a novel methodology for constructing confidence regions for parameters of linear models, using predictions from any arbitrary predictor. Our framework requires minimal assumptions on the noise and can be extended to functions deviating from strict linearity up to some adjustable threshold, thereby accommodating a comprehensive and pragmatically relevant set of functions. The derived confidence regions can be cast as constraints within a Mixed Integer Linear Programming framework, enabling optimisation of linear objectives. This representation enables robust optimization and the extraction of confidence intervals for specific parameter coordinates. Unlike previous methods, the confidence region can be empty, which can be used for hypothesis testing. Finally, we validate the empirical applicability of our method on synthetic data.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15258",
        "abstract url": "https://arxiv.org/abs/2401.15258",
        "title": "Foundations of Substructural Dependent Type Theory",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper presents preliminary work on a general system for integrating dependent types into substructural type systems such as linear logic and linear type theory. Prior work on this front has generally managed to deliver type systems possessing either syntax or semantics inclusive of certain practical applications, but has struggled to combine these all in one and the same system. Toward resolving this difficulty, I propose a novel categorical interpretation of substructural dependent types, analogous to the use of monoidal categories as models of linear and ordered logic, that encompasses a wide class of mathematical and computational examples. On this basis, I develop a general framework for substructural dependent type theories, and proceed to prove some essential metatheoretic properties thereof. As an application of this framework, I show how it can be used to construct a type theory that satisfactorily addresses the problem of effectively representing cut admissibility for linear sequent calculus in a logical framework.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15278",
        "abstract url": "https://arxiv.org/abs/2401.15278",
        "title": "Online Data-Driven Adaptive Control for Unknown Linear Time-Varying Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes a novel online data-driven adaptive control for unknown linear time-varying systems. Initialized with an empirical feedback gain, the algorithm periodically updates this gain based on the data collected over a short time window before each update. Meanwhile, the stability of the closed-loop system is analyzed in detail, which shows that under some mild assumptions, the proposed online data-driven adaptive control scheme can guarantee practical global exponential stability. Finally, the proposed algorithm is demonstrated by numerical simulations and its performance is compared with other control algorithms for unknown linear time-varying systems.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Technical report for the conference paper in 62nd IEEE CDC"
    },
    {
        "paper id": "2401.15280",
        "abstract url": "https://arxiv.org/abs/2401.15280",
        "title": "Analytical Framework for Effective Degrees of Freedom in Near-Field XL-MIMO",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we develop an effective degrees of freedom (EDoF) performance analysis framework specifically tailored for near-field XL-MIMO systems. We explore five representative distinct XL-MIMO hardware designs, including uniform planar array (UPA)-based with point antennas, two-dimensional (2D) continuous aperture (CAP) plane-based, UPA-based with patch antennas, uniform linear array (ULA)-based, and one-dimensional (1D) CAP line segment-based XL-MIMO systems. Our analysis encompasses two near-field channel models: the scalar and dyadic Green's function-based channel models. More importantly, when applying the scalar Green's function-based channel, we derive EDoF expressions in the closed-form, characterizing the impacts of the physical size of the transceiver, the transmitting distance, and the carrier frequency. In our numerical results, we evaluate and compare the EDoF performance across all examined XL-MIMO designs, confirming the accuracy of our proposed closed-form expressions. Furthermore, we observe that with an increasing number of antennas, the EDoF performance for both UPA-based and ULA-based systems approaches that of 2D CAP plane and 1D CAP line segment-based systems, respectively. Moreover, we unveil that the EDoF performance for near-field XL-MIMO systems is predominantly determined by the array aperture size rather than the sheer number of antennas.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "32 pages, 11 figures. This paper has been submitted to IEEE journal for possible publication"
    },
    {
        "paper id": "2401.15289",
        "abstract url": "https://arxiv.org/abs/2401.15289",
        "title": "Where's the \"up\"?! A Comprehensive (bottom-up) Study on the Security of Arm Cortex-M Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Arm Cortex-M processors are the most widely used 32-bit microcontrollers among embedded and Internetof-Things devices. Despite the widespread usage, there has been little effort in summarizing their hardware security features, characterizing the limitations and vulnerabilities of their hardware and software stack, and systematizing the research on securing these systems. The goals and contributions of this paper are multi-fold. First, we analyze the hardware security limitations and issues of Cortex-M systems. Second, we conducted a deep study of the software stack designed for Cortex-M and revealed its limitations, which is accompanied by an empirical analysis of 1,797 real-world firmware from seven hardware vendors. Third, we categorize the reported bugs in Cortex-M software systems. Finally, we systematize the efforts that aim at securing Cortex-M systems and evaluate them in terms of the protections they offer, run-time performance, required hardware features, etc. Based on the insights, we develop a set of recommendations for the research community and MCU software developers.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15291",
        "abstract url": "https://arxiv.org/abs/2401.15291",
        "title": "Improved Construction of Robust Gray Code",
        "rating": -10,
        "keywords": [],
        "abstract": "A robust Gray code, formally introduced by (Lolck and Pagh, SODA 2024), is a Gray code that additionally has the property that, given a noisy version of the encoding of an integer $j$, it is possible to reconstruct $\\hat{j}$ so that $|j - \\hat{j}|$ is small with high probability. That work presented a transformation that transforms a binary code $C$ of rate $R$ to a robust Gray code with rate $\u03a9(R)$, where the constant in the $\u03a9(\\cdot)$ can be at most $1/4$. We improve upon their construction by presenting a transformation from a (linear) binary code $C$ to a robust Gray code with similar robustness guarantees, but with rate that can approach $R/2$.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2401.15294",
        "abstract url": "https://arxiv.org/abs/2401.15294",
        "title": "Integral Operator Approaches for Scattered Data Fitting on Spheres",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper focuses on scattered data fitting problems on spheres. We study the approximation performance of a class of weighted spectral filter algorithms, including Tikhonov regularization, Landaweber iteration, spectral cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded random noise. For the analysis, we develop an integral operator approach that can be regarded as an extension of the widely used sampling inequality approach and norming set method in the community of scattered data fitting. After providing an equivalence between the operator differences and quadrature rules, we succeed in deriving optimal Sobolev-type error estimates of weighted spectral filter algorithms. Our derived error estimates do not suffer from the saturation phenomenon for Tikhonov regularization in the literature, native-space-barrier for existing error analysis and adapts to different embedding spaces. We also propose a divide-and-conquer scheme to equip weighted spectral filter algorithms to reduce their computational burden and present the optimal approximation error bounds.",
        "subjects": [
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2401.16434",
        "abstract url": "https://arxiv.org/abs/2401.16434",
        "title": "A novel ANROA based control approach for grid-tied multi-functional solar energy conversion system",
        "rating": -10,
        "keywords": [],
        "abstract": "An adaptive control approach for a three-phase grid-interfaced solar photovoltaic system based on the new Neuro-Fuzzy Inference System with Rain Optimization Algorithm (ANROA) methodology is proposed and discussed in this manuscript. This method incorporates an Adaptive Neuro-fuzzy Inference System (ANFIS) with a Rain Optimization Algorithm (ROA). The ANFIS controller has excellent maximum tracking capability because it includes features of both neural and fuzzy techniques. The ROA technique is in charge of controlling the voltage source converter switching. Avoiding power quality problems including voltage fluctuations, harmonics, and flickers as well as unbalanced loads and reactive power usage is the major goal. Besides, the proposed method performs at zero voltage regulation and unity power factor modes. The suggested control approach has been modeled and simulated, and its performance has been assessed using existing alternative methods. A statistical analysis of proposed and existing techniques has been also presented and discussed. The results of the simulations demonstrate that, when compared to alternative approaches, the suggested strategy may properly and effectively identify the best global solutions. Furthermore, the system's robustness has been studied by using MATLAB/SIMULINK environment and experimentally by Field Programmable Gate Arrays Controller (FPGA)-based Hardware-in-Loop (HLL).",
        "subjects": [
            "eess.SY"
        ],
        "comment": "The paper was published in Energy Reports journal (ELSEVIER). Cite as: Prasad, D., Kumar, N., Sharma, R., Malik, H., M\u00e1rquez, F. P. G., & Pinar-P\u00e9rez, J. M. (2023). A novel ANROA based control approach for grid-tied multi-functional solar energy conversion system. Energy Reports, 9, 2044-2057"
    },
    {
        "paper id": "2402.03354",
        "abstract url": "https://arxiv.org/abs/2402.03354",
        "title": "Leveraging Uncertainty in Collective Opinion Dynamics with Heterogeneity",
        "rating": -10,
        "keywords": [],
        "abstract": "Natural and artificial collectives exhibit heterogeneities across different dimensions, contributing to the complexity of their behavior. We investigate the effect of two such heterogeneities on collective opinion dynamics: heterogeneity of the quality of agents' prior information and of centrality in the network, i.e., the number of immediate neighbors. To study these heterogeneities, we not only consider them in our model, proposing a novel network generator with heterogeneous centrality, but also introduce uncertainty as an additional dimension. By quantifying the uncertainty of each agent, we provide a mechanism for agents to adaptively weigh their individual against social information. As uncertainties develop according to the interactions between agents, they capture information on heterogeneities. Therefore, uncertainty is a relevant additional observable in the study of complex collective opinion dynamics that we use to show the bidirectional relationship of heterogeneous centrality and information. Furthermore, we demonstrate that uncertainty-driven adaptive weighting leads to increased accuracy and speed of consensus, especially under heterogeneity, and provide guidelines for avoiding performance-decreasing errors in uncertainty modeling. These opportunities for improved performance and observability suggest the importance of uncertainty both for the study of natural and the design of artificial heterogeneous systems.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": "15 pages, 7 figures"
    },
    {
        "paper id": "2402.08687",
        "abstract url": "https://arxiv.org/abs/2402.08687",
        "title": "Fuzzy clustering of circular time series based on a new dependence measure with applications to wind data",
        "rating": -10,
        "keywords": [],
        "abstract": "Time series clustering is an essential machine learning task with applications in many disciplines. While the majority of the methods focus on time series taking values on the real line, very few works consider time series defined on the unit circle, although the latter objects frequently arise in many applications. In this paper, the problem of clustering circular time series is addressed. To this aim, a distance between circular series is introduced and used to construct a clustering procedure. The metric relies on a new measure of serial dependence considering circular arcs, thus taking advantage of the directional character inherent to the series range. Since the dynamics of the series may vary over the time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting clustering algorithm is able to group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of models. An extensive simulation study shows that the proposed method outperforms several alternative techniques, besides being computationally efficient. Two interesting applications involving time series of wind direction in Saudi Arabia highlight the potential of the proposed approach.",
        "subjects": [
            "stat.AP"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2304.12249"
    }
]