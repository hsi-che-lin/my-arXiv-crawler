[
    {
        "paper id": "2402.10193",
        "abstract url": "https://arxiv.org/abs/2402.10193",
        "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
        "rating": "2",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19669",
        "abstract url": "https://arxiv.org/abs/2403.19669",
        "title": "Analyzing the Roles of Language and Vision in Learning from Limited Data",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoning.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2402.09811",
        "abstract url": "https://arxiv.org/abs/2402.09811",
        "title": "TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at the WACV 2024 Conference"
    },
    {
        "paper id": "2402.09977",
        "abstract url": "https://arxiv.org/abs/2402.09977",
        "title": "Fast Vocabulary Transfer for Language Model Compression",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)"
    },
    {
        "paper id": "2402.10401",
        "abstract url": "https://arxiv.org/abs/2402.10401",
        "title": "ManiFPT: Defining and Analyzing Fingerprints of Generative Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model attribution) compared to existing methods. Additionally, we study the structure of the fingerprints, and observe that it is very predictive of the effect of different design choices on the generative process.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024"
    },
    {
        "paper id": "2404.03663",
        "abstract url": "https://arxiv.org/abs/2404.03663",
        "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
        "subjects": [
            "cs.NE",
            "cs.CV"
        ],
        "comment": "Accepted by ICLR2024. Code and Model: https://github.com/BICLab/Spike-Driven-Transformer-V2"
    },
    {
        "paper id": "2404.18935",
        "abstract url": "https://arxiv.org/abs/2404.18935",
        "title": "What's in the Flow? Exploiting Temporal Motion Cues for Unsupervised Generic Event Boundary Detection",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Generic Event Boundary Detection (GEBD) task aims to recognize generic, taxonomy-free boundaries that segment a video into meaningful events. Current methods typically involve a neural model trained on a large volume of data, demanding substantial computational power and storage space. We explore two pivotal questions pertaining to GEBD: Can non-parametric algorithms outperform unsupervised neural methods? Does motion information alone suffice for high performance? This inquiry drives us to algorithmically harness motion cues for identifying generic event boundaries in videos. In this work, we propose FlowGEBD, a non-parametric, unsupervised technique for GEBD. Our approach entails two algorithms utilizing optical flow: (i) Pixel Tracking and (ii) Flow Normalization. By conducting thorough experimentation on the challenging Kinetics-GEBD and TAPOS datasets, our results establish FlowGEBD as the new state-of-the-art (SOTA) among unsupervised methods. FlowGEBD exceeds the neural models on the Kinetics-GEBD dataset by obtaining an F1@0.05 score of 0.713 with an absolute gain of 31.7% compared to the unsupervised baseline and achieves an average F1 score of 0.623 on the TAPOS validation dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted in WACV-2024. Supplementary at https://openaccess.thecvf.com/content/WACV2024/supplemental/Gothe_Whats_in_the_WACV_2024_supplemental.pdf"
    },
    {
        "paper id": "2402.09709",
        "abstract url": "https://arxiv.org/abs/2402.09709",
        "title": "ME-ViT: A Single-Load Memory-Efficient FPGA Accelerator for Vision Transformers",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Vision Transformers (ViTs) have emerged as a state-of-the-art solution for object classification tasks. However, their computational demands and high parameter count make them unsuitable for real-time inference, prompting the need for efficient hardware implementations. Existing hardware accelerators for ViTs suffer from frequent off-chip memory access, restricting the achievable throughput by memory bandwidth. In devices with a high compute-to-communication ratio (e.g., edge FPGAs with limited bandwidth), off-chip memory access imposes a severe bottleneck on overall throughput. This work proposes ME-ViT, a novel \\underline{M}emory \\underline{E}fficient FPGA accelerator for \\underline{ViT} inference that minimizes memory traffic. We propose a \\textit{single-load policy} in designing ME-ViT: model parameters are only loaded once, intermediate results are stored on-chip, and all operations are implemented in a single processing element. To achieve this goal, we design a memory-efficient processing element (ME-PE), which processes multiple key operations of ViT inference on the same architecture through the reuse of \\textit{multi-purpose buffers}. We also integrate the Softmax and LayerNorm functions into the ME-PE, minimizing stalls between matrix multiplications. We evaluate ME-ViT on systolic array sizes of 32 and 16, achieving up to a 9.22$\\times$ and 17.89$\\times$ overall improvement in memory bandwidth, and a 2.16$\\times$ improvement in throughput per DSP for both designs over state-of-the-art ViT accelerators on FPGA. ME-ViT achieves a power efficiency improvement of up to 4.00$\\times$ (1.03$\\times$) over a GPU (FPGA) baseline. ME-ViT enables up to 5 ME-PE instantiations on a Xilinx Alveo U200, achieving a 5.10$\\times$ improvement in throughput over the state-of-the art FPGA baseline, and a 5.85$\\times$ (1.51$\\times$) improvement in power efficiency over the GPU (FPGA) baseline.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09717",
        "abstract url": "https://arxiv.org/abs/2402.09717",
        "title": "Visually Dehallucinative Instruction Generation: Know What You Don't Know",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "\"When did the emperor Napoleon invented iPhone?\" Such hallucination-inducing question is well known challenge in generative language modeling. In this study, we present an innovative concept of visual hallucination, referred to as \"I Know (IK)\" hallucination, to address scenarios where \"I Don't Know\" is the desired response. To effectively tackle this issue, we propose the VQAv2-IDK benchmark, the subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators. Stepping further, we present the visually dehallucinative instruction generation method for IK hallucination and introduce the IDK-Instructions visual instruction database. Our experiments show that current methods struggle with IK hallucination. Yet, our approach effectively reduces these hallucinations, proving its versatility across different frameworks and datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09723",
        "abstract url": "https://arxiv.org/abs/2402.09723",
        "title": "Best Arm Identification for Prompt Learning under a Limited Budget",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and function approximation. Extensive experiments on multiple well-adopted tasks using both GPT 3.5 and Llama2 demonstrate the significant performance improvement of TRIPLE over the previous baselines while satisfying the limited budget constraints.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09724",
        "abstract url": "https://arxiv.org/abs/2402.09724",
        "title": "Region Feature Descriptor Adapted to High Affine Transformations",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "To address the issue of feature descriptors being ineffective in representing grayscale feature information when images undergo high affine transformations, leading to a rapid decline in feature matching accuracy, this paper proposes a region feature descriptor based on simulating affine transformations using classification. The proposed method initially categorizes images with different affine degrees to simulate affine transformations and generate a new set of images. Subsequently, it calculates neighborhood information for feature points on this new image set. Finally, the descriptor is generated by combining the grayscale histogram of the maximum stable extremal region to which the feature point belongs and the normalized position relative to the grayscale centroid of the feature point's region. Experimental results, comparing feature matching metrics under affine transformation scenarios, demonstrate that the proposed descriptor exhibits higher precision and robustness compared to existing classical descriptors. Additionally, it shows robustness when integrated with other descriptors.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09725",
        "abstract url": "https://arxiv.org/abs/2402.09725",
        "title": "Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the Transformer. The experiments results demonstrate the effectiveness of our method.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09727",
        "abstract url": "https://arxiv.org/abs/2402.09727",
        "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": "Website: https://read-agent.github.io"
    },
    {
        "paper id": "2402.09731",
        "abstract url": "https://arxiv.org/abs/2402.09731",
        "title": "POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep convolutional neural networks (CNNs) based approaches have achieved great performance in video matting. Many of these methods can produce accurate alpha estimation for the target body but typically yield fuzzy or incorrect target edges. This is usually caused by the following reasons: 1) The current methods always treat the target body and edge indiscriminately; 2) Target body dominates the whole target with only a tiny proportion target edge. For the first problem, we propose a CNN-based module that separately optimizes the matting target body and edge (SOBE). And on this basis, we introduce a real-time, trimap-free video matting method via progressively optimizing the matting target body and edge (POBEVM) that is much lighter than previous approaches and achieves significant improvements in the predicted target edge. For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces our network on the matting target edge. Experiments demonstrate our method outperforms prior trimap-free matting methods on both Distinctions-646 (D646) and VideoMatte240K(VM) dataset, especially in edge optimization.",
        "subjects": [
            "cs.CV",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09733",
        "abstract url": "https://arxiv.org/abs/2402.09733",
        "title": "Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidden representation space to mitigate hallucination. We believe this work provides insights into how LLMs produce hallucinated answers and how to make them occur less often.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, 8 figures, 2 tables (13 pages, 12 figures, 13 tables including references and appendices)"
    },
    {
        "paper id": "2402.09738",
        "abstract url": "https://arxiv.org/abs/2402.09738",
        "title": "Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities. Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages. This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two benchmark hateful meme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English). Evaluation results demonstrate our proposed approach's effectiveness with F1-scores of $69.7$% and $70.3$% for the MUTE and MultiOFF datasets. The scores show approximately $2.5$% and $3.2$% performance improvement over the state-of-the-art systems on these datasets. Our implementation is available at https://github.com/eftekhar-hossain/Bengali-Hateful-Memes.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to EACL-SRW, 2024"
    },
    {
        "paper id": "2402.09739",
        "abstract url": "https://arxiv.org/abs/2402.09739",
        "title": "QuRating: Selecting High-Quality Data for Training Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results. When we sample using quality ratings as logits over documents, our models achieve lower perplexity and stronger in-context learning performance than baselines. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "The code, models and data are available at https://github.com/princeton-nlp/QuRating"
    },
    {
        "paper id": "2402.09748",
        "abstract url": "https://arxiv.org/abs/2402.09748",
        "title": "Model Compression and Efficient Inference for Large Language Models: A Survey",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning-free algorithms. (2) Large models emphasize versatility and generalization rather than performance on a single task. Hence, many algorithms, such as knowledge distillation, focus on how to preserving their versatility and generalization after compression. Since these two characteristics were not very pronounced in early large models, we further distinguish large language models into medium models and ``real'' large models. Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can support basic compression or acceleration algorithms, greatly facilitating model deployment for users.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.PF"
        ],
        "comment": "47 pages, review 380 papers. The work is ongoing"
    },
    {
        "paper id": "2402.09759",
        "abstract url": "https://arxiv.org/abs/2402.09759",
        "title": "Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs by training just 1.2% of its parameters. To contribute to the community's collaborative progress, the model has been released as open-source.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2402.09760",
        "abstract url": "https://arxiv.org/abs/2402.09760",
        "title": "Grounding Language Model with Chunking-Free In-Context Retrieval",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval. CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained. Our evaluations of CFIC on a range of open QA datasets demonstrate its superiority in retrieving relevant and accurate evidence, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09773",
        "abstract url": "https://arxiv.org/abs/2402.09773",
        "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance. Extensive experiments across various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09781",
        "abstract url": "https://arxiv.org/abs/2402.09781",
        "title": "A Comprehensive Review on Computer Vision Analysis of Aerial Data",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data. This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis. While addressing fundamental aspects such as object detection and tracking, the primary focus is on pivotal tasks like change detection, object segmentation, and scene-level analysis. The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks. A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise. The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis. Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights. The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions. Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis.",
        "subjects": [
            "cs.CV",
            "cs.IT"
        ],
        "comment": "112 pages"
    },
    {
        "paper id": "2402.09792",
        "abstract url": "https://arxiv.org/abs/2402.09792",
        "title": "System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "eess.IV"
            ]
        ],
        "abstract": "Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU) architecture is energy-efficient as it utilizes dedicated neuromorphic hardware and stochastic computation of weight updates for in-memory computing. Charge Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs. However, prior work has shown that the weight updates (V_T) in CTF-based RPU are impacted by the non-ideal program time of CTF. The non-ideal program time is affected by two factors of CTF. Firstly, the effects of the number of input pulses (N) or pulse width (pw), and secondly, the gap between successive update pulses (t_gap) used for the stochastic computation of weight updates. Therefore, the impact of this non-ideal program time must be studied for neural network training simulations. In this study, Firstly, we propose a pulse-train design compensation technique to reduce the total error caused by non-ideal program time of CTF and stochastic variance of a network. Secondly, we simulate RPU-based DNN with non-ideal program time of CTF on MNIST and Fashion-MNIST datasets. We find that for larger N (~1000), learning performance approaches the ideal (software-level) training level and, therefore, is not much impacted by the choice of t_gap used to implement RPU-based weight updates. However, for lower N (<500), learning performance depends on T_gap of the pulses. Finally, we also performed an ablation study to isolate the causal factor of the improved learning performance. We conclude that the lower noise level in the weight updates is the most likely significant factor to improve the learning performance of DNN. Thus, our study attempts to compensate for the error caused by non-ideal program time and standardize the pulse length (N) and pulse gap (t_gap) specifications for CTF-based RPUs for accurate system-level on-chip training.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.ET",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09797",
        "abstract url": "https://arxiv.org/abs/2402.09797",
        "title": "A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "In this work, we propose a novel cross-talk rejection framework for a multi-channel multi-talker setup for a live multiparty interactive show. Our far-field audio setup is required to be hands-free during live interaction and comprises four adjacent talkers with directional microphones in the same space. Such setups often introduce heavy cross-talk between channels, resulting in reduced automatic speech recognition (ASR) and natural language understanding (NLU) performance. To address this problem, we propose voice activity detection (VAD) model for all talkers using multichannel information, which is then used to filter audio for downstream tasks. We adopt a synthetic training data generation approach through playback and re-recording for such scenarios, simulating challenging speech overlap conditions. We train our models on this synthetic data and demonstrate that our approach outperforms single-channel VAD models and energy-based multi-channel VAD algorithm in various acoustic environments. In addition to VAD results, we also present multiparty ASR evaluation results to highlight the impact of using our VAD model for filtering audio in downstream tasks by significantly reducing the insertion error.",
        "subjects": [
            "cs.SD",
            "cs.HC",
            "eess.AS"
        ],
        "comment": "Accepted for presentation at the Hands-free Speech Communication and Microphone Arrays (HSCMA 2024)"
    },
    {
        "paper id": "2402.09808",
        "abstract url": "https://arxiv.org/abs/2402.09808",
        "title": "Knowledge of Pretrained Language Models on Surface Information of Tokens",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09841",
        "abstract url": "https://arxiv.org/abs/2402.09841",
        "title": "LAPDoc: Layout-Aware Prompting for Documents",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experiments we investigate the effects on the commercial ChatGPT model and the open-source LLM Solar. We demonstrate that using our approach both LLMs show improved performance on various standard document benchmarks. In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout. Our results indicate that layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15% compared to just using plain document text. In conclusion, this approach should be considered for the best model choice between text-based LLM or multi-modal document transformers.",
        "subjects": [
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Under review at ICDAR2024"
    },
    {
        "paper id": "2402.09865",
        "abstract url": "https://arxiv.org/abs/2402.09865",
        "title": "Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "29 pages"
    },
    {
        "paper id": "2402.09871",
        "abstract url": "https://arxiv.org/abs/2402.09871",
        "title": "MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset with multi-dimensional, high-precision music annotations, the Caichong Music Dataset (CaiMD), and carefully selected 1,000 high-quality entries to serve as the test set for MuChin. Based on MuChin, we analyzed the discrepancies between professionals and amateurs in terms of music description, and empirically demonstrated the effectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed MuChin to evaluate existing music understanding models on their ability to provide colloquial descriptions of music. All data related to the benchmark, along with the scoring code and detailed appendices, have been open-sourced (https://github.com/CarlWangChina/MuChin/).",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "Accepted by International Joint Conference on Artificial Intelligence 2024 (IJCAI 2024)"
    },
    {
        "paper id": "2402.09880",
        "abstract url": "https://arxiv.org/abs/2402.09880",
        "title": "Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09894",
        "abstract url": "https://arxiv.org/abs/2402.09894",
        "title": "Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived utility of AI is not just a novelty effect. The increase in benefits mainly comes from end-users' ability to customize prompts, and thus appropriate the system to their own needs. This points to a future where generative AI systems can allow us to design for appropriation.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CL",
            "cs.CY"
        ],
        "comment": "21 pages, 11 figures"
    },
    {
        "paper id": "2402.09906",
        "abstract url": "https://arxiv.org/abs/2402.09906",
        "title": "Generative Representational Instruction Tuning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "66 pages (16 main), 25 figures, 34 tables"
    },
    {
        "paper id": "2402.09910",
        "abstract url": "https://arxiv.org/abs/2402.09910",
        "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $\\approx$ 4% accuracy. Our code and datasets are available at https://github.com/avduarte333/DE-COP_Method",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09923",
        "abstract url": "https://arxiv.org/abs/2402.09923",
        "title": "A Dataset of Open-Domain Question Answering with Multiple-Span Answers",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions. Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEAN. Experimental results and analysis show the characteristics and challenge of the newly proposed CLEAN dataset for the community. Our dataset, CLEAN, will be publicly released at zhiyiluo.site/misc/clean_v1.0_ sample.json.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09926",
        "abstract url": "https://arxiv.org/abs/2402.09926",
        "title": "Predicting the Energy Demand of a Hardware Video Decoder with Unknown Design Using Software Profiling",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Energy efficiency for video communications and video-on-demand streaming is essential for mobile devices with a limited battery capacity. Therefore, hardware (HW) decoder implementations are commonly used to significantly reduce the energetic load of video playback. The energy consumption of such a HW implementation largely depends on a previously finalized standardization of a video codec that specifies which coding tools and methods are included in the new video codec. However, during the standardization, the true complexity of a HW implementation is unknown, and the adoption of coding tools relies solely on the expertise of experts in the industry. By using software (SW) decoder profiling, we are able to estimate the SW decoding energy demand with an average error of 1.25%. We propose a method that accurately models the energy demand of existing HW decoders with an average error of 1.79% by exploiting information from software (SW) decoder profiling. Motivated by the low estimation error, we propose a HW decoding energy metric that can predict and estimate the complexity of an unknown HW implementation using information from existing HW decoder implementations and available SW implementations of the future video decoder. By using multiple video codecs for model training, we can predict the complexity of a HW decoder with an error of less than 8% and a minimum error of 4.54% without using the corresponding HW decoder for training.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Submitted to IEEE Transactions on Circuits and Systems for Video Technology, 13 Pages"
    },
    {
        "paper id": "2402.09934",
        "abstract url": "https://arxiv.org/abs/2402.09934",
        "title": "Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "14 pages, 5 figures"
    },
    {
        "paper id": "2402.09939",
        "abstract url": "https://arxiv.org/abs/2402.09939",
        "title": "Generative AI in the Construction Industry: A State-of-the-art Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents. The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "74 pages, 11 figures, 20 tables"
    },
    {
        "paper id": "2402.09954",
        "abstract url": "https://arxiv.org/abs/2402.09954",
        "title": "Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos. Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09967",
        "abstract url": "https://arxiv.org/abs/2402.09967",
        "title": "Case Study: Testing Model Capabilities in Some Reasoning Tasks",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) excel in generating personalized content and facilitating interactive dialogues, showcasing their remarkable aptitude for a myriad of applications. However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in Progress"
    },
    {
        "paper id": "2402.09984",
        "abstract url": "https://arxiv.org/abs/2402.09984",
        "title": "Symmetry-Breaking Augmentations for Ad Hoc Teamwork",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies. While often simple for humans, this can be challenging for AI agents. For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry. To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation. By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates. We demonstrate this experimentally in two settings, and show that our approach improves upon previous ad hoc teamwork results in the challenging card game Hanabi. We also propose a general metric for estimating symmetry-dependency amongst a given set of policies.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Currently in review for ICML 2024. 16 pages (including references and appendix), 9 Figures, 11 tables"
    },
    {
        "paper id": "2402.09989",
        "abstract url": "https://arxiv.org/abs/2402.09989",
        "title": "LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods. 2) The introduction of entity expansion expression and Visual Entailment (VE) Module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding capabilities of any current or prospective multimodal pretraining models. Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09997",
        "abstract url": "https://arxiv.org/abs/2402.09997",
        "title": "LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing efficient batch inference to accommodate heterogeneous requests. Experimental results indicate that LoraRetriever consistently outperforms the baselines, highlighting its practical effectiveness and versatility.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10013",
        "abstract url": "https://arxiv.org/abs/2402.10013",
        "title": "Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures. Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives -- even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1, L2) or other meta-heuristics (early-stopping, dropout). However, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum.",
        "subjects": [
            "cs.CL",
            "cs.FL"
        ],
        "comment": "9 pages, 5 figures, 3 appendix pages"
    },
    {
        "paper id": "2402.10021",
        "abstract url": "https://arxiv.org/abs/2402.10021",
        "title": "SAWEC: Sensing-Assisted Wireless Edge Computing",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms. Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed. However, existing WEC methods require the transmission and processing of a high amount of video data which may ultimately saturate the wireless link. In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing (SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the physical environment to reduce the end-to-end latency and overall computational burden by transmitting to the edge server only the relevant data for the delivery of the service. Our intuition is that the transmission of the portion of the video frames where there are no changes with respect to previous frames can be avoided. Specifically, we leverage wireless sensing techniques to estimate the location of objects in the environment and obtain insights about the environment dynamics. Hence, only the part of the frames where any environmental change is detected is transmitted and processed. We evaluated SAWEC by using a 10K 360$^{\\circ}$ with a Wi-Fi 6 sensing system operating at 160 MHz and performing localization and tracking. We considered instance segmentation and object detection as benchmarking tasks for performance evaluation. We carried out experiments in an anechoic chamber and an entrance hall with two human subjects in six different setups. Experimental results show that SAWEC reduces both the channel occupation and end-to-end latency by more than 90% while improving the instance segmentation and object detection performance with respect to state-of-the-art WEC approaches.",
        "subjects": [
            "cs.CV",
            "cs.NI"
        ],
        "comment": "Submitted to ACM for possible publication"
    },
    {
        "paper id": "2402.10024",
        "abstract url": "https://arxiv.org/abs/2402.10024",
        "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "10 Pages, 3 Figures, 7 Tables"
    },
    {
        "paper id": "2402.10026",
        "abstract url": "https://arxiv.org/abs/2402.10026",
        "title": "Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Hyper spectral images have drawn the attention of the researchers for its complexity to classify. It has nonlinear relation between the materials and the spectral information provided by the HSI image. Deep learning methods have shown superiority in learning this nonlinearity in comparison to traditional machine learning methods. Use of 3-D CNN along with 2-D CNN have shown great success for learning spatial and spectral features. However, it uses comparatively large number of parameters. Moreover, it is not effective to learn inter layer information. Hence, this paper proposes a neural network combining 3-D CNN, 2-D CNN and Bi-LSTM. The performance of this model has been tested on Indian Pines(IP) University of Pavia(PU) and Salinas Scene(SA) data sets. The results are compared with the state of-the-art deep learning-based models. This model performed better in all three datasets. It could achieve 99.83, 99.98 and 100 percent accuracy using only 30 percent trainable parameters of the state-of-art model in IP, PU and SA datasets respectively.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10038",
        "abstract url": "https://arxiv.org/abs/2402.10038",
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "16 pages, 4 figures"
    },
    {
        "paper id": "2402.10039",
        "abstract url": "https://arxiv.org/abs/2402.10039",
        "title": "Feature Accentuation: Revealing 'What' Features Respond to in Natural Images",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Efforts to decode neural network vision models necessitate a comprehensive grasp of both the spatial and semantic facets governing feature responses within images. Most research has primarily centered around attribution methods, which provide explanations in the form of heatmaps, showing where the model directs its attention for a given feature. However, grasping 'where' alone falls short, as numerous studies have highlighted the limitations of those methods and the necessity to understand 'what' the model has recognized at the focal point of its attention. In parallel, 'Feature visualization' offers another avenue for interpreting neural network features. This approach synthesizes an optimal image through gradient ascent, providing clearer insights into 'what' features respond to. However, feature visualizations only provide one global explanation per feature; they do not explain why features activate for particular images. In this work, we introduce a new method to the interpretability tool-kit, 'feature accentuation', which is capable of conveying both where and what in arbitrary input images induces a feature's response. At its core, feature accentuation is image-seeded (rather than noise-seeded) feature visualization. We find a particular combination of parameterization, augmentation, and regularization yields naturalistic visualizations that resemble the seed image and target feature simultaneously. Furthermore, we validate these accentuations are processed along a natural circuit by the model. We make our precise implementation of feature accentuation available to the community as the Faccent library, an extension of Lucent.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10052",
        "abstract url": "https://arxiv.org/abs/2402.10052",
        "title": "Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination",
        "rating": "1",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "unlearning"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also with parameter-efficient fine-tuning, offering a novel pathway to addressing the challenges with private and sensitive data in LLM applications.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10073",
        "abstract url": "https://arxiv.org/abs/2402.10073",
        "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \\textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \\underline{\\textbf{Mo}}dular \\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement method (\\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10076",
        "abstract url": "https://arxiv.org/abs/2402.10076",
        "title": "QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of quantized Large Language Models (LLMs). QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels. Our method interleaves the quantized weight matrices of LLMs offline to skip the shared memory write-back after the dequantization. We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "9 pages, 8 figures"
    },
    {
        "paper id": "2402.10093",
        "abstract url": "https://arxiv.org/abs/2402.10093",
        "title": "MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters. The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: https://ml-jku.github.io/MIM-Refiner",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10099",
        "abstract url": "https://arxiv.org/abs/2402.10099",
        "title": "Any-Shift Prompting for Generalization over Distributions",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10104",
        "abstract url": "https://arxiv.org/abs/2402.10104",
        "title": "GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on the main subset but only a 6.00\\% accuracy on the challenging subset. This highlights the critical need for testing models against datasets on which they have not been pre-trained. Additionally, our findings indicate that GPT-series models perform more effectively on problems they have rephrased, suggesting a promising method for enhancing model capabilities.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10137",
        "abstract url": "https://arxiv.org/abs/2402.10137",
        "title": "TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In light of recent advances in large language models (LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs (TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks and the results show that modelling more verbose or responses without user expression mirroring is more challenging.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10151",
        "abstract url": "https://arxiv.org/abs/2402.10151",
        "title": "ControlLM: Crafting Diverse Personalities for Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning. This heightens the need to control model behaviors. We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met. Personality is a higher-level and more abstract behavioral representation for language models. We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference. This approach allows for the precise, real-time adjustment of model behavior. First, we demonstrate ControlLM's capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match average human values. Subsequently, we showcase improved reasoning and question answering through selective amplification of beneficial attributes like conscientiousness and friendliness. We hope that this work will inspire research on controlling human-like behaviors of language models and provide insights for future research. Our code is publicly available at: https://github.com/wengsyx/ControlLM.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2402.10168",
        "abstract url": "https://arxiv.org/abs/2402.10168",
        "title": "DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10171",
        "abstract url": "https://arxiv.org/abs/2402.10171",
        "title": "Data Engineering for Scaling Language Models to 128K Context",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Code at https://github.com/FranxYao/Long-Context-Data-Engineering"
    },
    {
        "paper id": "2402.10175",
        "abstract url": "https://arxiv.org/abs/2402.10175",
        "title": "Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by NAACL 2024 main conference"
    },
    {
        "paper id": "2402.10177",
        "abstract url": "https://arxiv.org/abs/2402.10177",
        "title": "Large Scale Constrained Clustering With Reinforcement Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage. In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance. While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances. We propose an approach to solve this constrained clustering problem via reinforcement learning. Our method involves training an agent to generate both feasible and (near) optimal solutions. The agent learns problem-specific heuristics, tailored to the instances encountered in this task. In the results section, we show that our algorithm finds near optimal solutions, even for large scale instances.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "LEANOPT-24 AAAI"
    },
    {
        "paper id": "2402.10178",
        "abstract url": "https://arxiv.org/abs/2402.10178",
        "title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents' abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10189",
        "abstract url": "https://arxiv.org/abs/2402.10189",
        "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted to the main conference of NAACL 2024"
    },
    {
        "paper id": "2402.10200",
        "abstract url": "https://arxiv.org/abs/2402.10200",
        "title": "Chain-of-Thought Reasoning Without Prompting",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10247",
        "abstract url": "https://arxiv.org/abs/2402.10247",
        "title": "Engraving Oriented Joint Estimation of Pitch Spelling and Local and Global Keys",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "We revisit the problems of pitch spelling and tonality guessing with a new algorithm for their joint estimation from a MIDI file including information about the measure boundaries. Our algorithm does not only identify a global key but also local ones all along the analyzed piece. It uses Dynamic Programming techniques to search for an optimal spelling in term, roughly, of the number of accidental symbols that would be displayed in the engraved score. The evaluation of this number is coupled with an estimation of the global key and some local keys, one for each measure. Each of the three informations is used for the estimation of the other, in a multi-steps procedure. An evaluation conducted on a monophonic and a piano dataset, comprising 216 464 notes in total, shows a high degree of accuracy, both for pitch spelling (99.5% on average on the Bach corpus and 98.2% on the whole dataset) and global key signature estimation (93.0% on average, 95.58% on the piano dataset). Designed originally as a backend tool in a music transcription framework, this method should also be useful in other tasks related to music notation processing.",
        "subjects": [
            "cs.SD",
            "cs.IR",
            "eess.AS"
        ],
        "comment": "International Conference on Technologies for Music Notation and Representation (TENOR), Apr 2024, Zurich (CH), Switzerland"
    },
    {
        "paper id": "2402.10257",
        "abstract url": "https://arxiv.org/abs/2402.10257",
        "title": "Analysis of Neural Video Compression Networks for 360-Degree Video Coding",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "With the increasing efforts of bringing high-quality virtual reality technologies into the market, efficient 360-degree video compression gains in importance. As such, the state-of-the-art H.266/VVC video coding standard integrates dedicated tools for 360-degree video, and considerable efforts have been put into designing 360-degree projection formats with improved compression efficiency. For the fast-evolving field of neural video compression networks (NVCs), the effects of different 360-degree projection formats on the overall compression performance have not yet been investigated. It is thus unclear, whether a resampling from the conventional equirectangular projection (ERP) to other projection formats yields similar gains for NVCs as for hybrid video codecs, and which formats perform best. In this paper, we analyze several generations of NVCs and an extensive set of 360-degree projection formats with respect to their compression performance for 360-degree video. Based on our analysis, we find that projection format resampling yields significant improvements in compression performance also for NVCs. The adjusted cubemap projection (ACP) and equatorial cylindrical projection (ECP) show to perform best and achieve rate savings of more than 55% compared to ERP based on WS-PSNR for the most recent NVC. Remarkably, the observed rate savings are higher than for H.266/VVC, emphasizing the importance of projection format resampling for NVCs.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "5 pages, 4 figures, 1 table, accepted for Picture Coding Symposium 2024 (PCS 2024)"
    },
    {
        "paper id": "2402.10260",
        "abstract url": "https://arxiv.org/abs/2402.10260",
        "title": "A StrongREJECT for Empty Jailbreaks",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The rise of large language models (LLMs) has drawn attention to the existence of \"jailbreaks\" that allow the models to be used maliciously. However, there is no standard benchmark for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these benchmarks often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the zero-shot performance of GPT-4 on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an \"uncensored\" open-source model. We present a new benchmark, StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality question set and a more accurate response grading algorithm. We show that our new grading scheme better accords with human judgment of response quality and overall jailbreak effectiveness, especially on the sort of low-quality responses that contribute the most to over-estimation of jailbreak performance on existing benchmarks. We release our code and data at https://github.com/alexandrasouly/strongreject.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CR"
        ],
        "comment": "Code and data at https://github.com/alexandrasouly/strongreject"
    },
    {
        "paper id": "2402.10302",
        "abstract url": "https://arxiv.org/abs/2402.10302",
        "title": "How to Discern Important Urgent News?",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We found that a simple property of clusters in a clustered dataset of news correlate strongly with importance and urgency of news (IUN) as assessed by LLM. We verified our finding across different news datasets, dataset sizes, clustering algorithms and embeddings. The found correlation should allow using clustering (as an alternative to LLM) for identifying the most important urgent news, or for filtering out unimportant articles.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 12 figures, 12 tables"
    },
    {
        "paper id": "2402.10311",
        "abstract url": "https://arxiv.org/abs/2402.10311",
        "title": "The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when it is composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in languages, the noun tends to be placed at one of the ends, confirming the theoretical prediction. We also show evidence of anti locality effects: syntactic dependency distances in preferred orders are longer than expected by chance.",
        "subjects": [
            "cs.CL",
            "physics.soc-ph"
        ],
        "comment": "Typos corrected"
    },
    {
        "paper id": "2402.10353",
        "abstract url": "https://arxiv.org/abs/2402.10353",
        "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to prompt pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\\%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10379",
        "abstract url": "https://arxiv.org/abs/2402.10379",
        "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at https://github.com/datadreamer-dev/DataDreamer .",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10400",
        "abstract url": "https://arxiv.org/abs/2402.10400",
        "title": "Chain of Logic: Rule-Based Reasoning with Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks involving three distinct compositional rules from the LegalBench benchmark and demonstrate it consistently outperforms other prompting methods, including chain of thought and self-ask, using open-source and commercial language models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10403",
        "abstract url": "https://arxiv.org/abs/2402.10403",
        "title": "Polyhedral Complex Derivation from Piecewise Trilinear Networks",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between the eikonal loss and the planarity of the hypersurfaces.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10412",
        "abstract url": "https://arxiv.org/abs/2402.10412",
        "title": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. \"best\" or \"correct\" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accurate hallucination measures than naively using reference LLMs. We also show how to leverage FEWL to reduce hallucination through both in-context learning and supervised finetuning. Last, we build a large-scale benchmark dataset to facilitate LLM hallucination research.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Paper Under Review"
    },
    {
        "paper id": "2402.10416",
        "abstract url": "https://arxiv.org/abs/2402.10416",
        "title": "Grounding Language about Belief in a Bayesian Theory-of-Mind",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others' beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by grounding the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent's actions, then evaluating statements about the agent's beliefs against these inferences via epistemic logic, our framework provides a conceptual role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and beliefs while watching an agent solve a doors-and-keys gridworld puzzle that requires instrumental reasoning about hidden objects. In contrast to pure logical deduction, non-mentalizing baselines, and mentalizing that ignores the role of instrumental plans, our model provides a much better fit to human goal and belief attributions, demonstrating the importance of theory-of-mind for a semantics of belief.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Under Review, 7 pages"
    },
    {
        "paper id": "2402.10422",
        "abstract url": "https://arxiv.org/abs/2402.10422",
        "title": "Pushing the Limits of Zero-shot End-to-End Speech Translation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Data scarcity and the modality gap between the speech and text modalities are two major obstacles of end-to-end Speech Translation (ST) systems, thus hindering their performance. Prior work has attempted to mitigate these challenges by leveraging external MT data and optimizing distance metrics that bring closer the speech-text representations. However, achieving competitive results typically requires some ST data. For this reason, we introduce ZeroSwot, a method for zero-shot ST that bridges the modality gap without any paired ST data. Leveraging a novel CTC compression and Optimal Transport, we train a speech encoder using only ASR data, to align with the representation space of a massively multilingual MT model. The speech encoder seamlessly integrates with the MT model at inference, enabling direct translation from speech to text, across all languages supported by the MT model. Our experiments show that we can effectively close the modality gap without ST data, while our results on MuST-C and CoVoST demonstrate our method's superiority over not only previous zero-shot models, but also supervised ones, achieving state-of-the-art results.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10424",
        "abstract url": "https://arxiv.org/abs/2402.10424",
        "title": "Understanding In-Context Learning with a Pelican Soup Framework",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Many existing theoretical analyses of in-context learning for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a $\\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of instruction tuning. An additional notion of \\textit{atom concepts} makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec, and a digit addition task that mimics types of distribution shifts a model needs to overcome to perform in-context learning. We also experiment with GPT2-Large on real-world NLP tasks. Our empirical results demonstrate the efficacy of our framework to explain in-context learning.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10426",
        "abstract url": "https://arxiv.org/abs/2402.10426",
        "title": "DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount. In this work, we propose DELL that identifies three key stages in misinformation detection where LLMs could be incorporated as part of the pipeline: 1) LLMs could \\emph{generate news reactions} to represent diverse perspectives and simulate user-news interaction networks; 2) LLMs could \\emph{generate explanations} for proxy tasks (e.g., sentiment, stance) to enrich the contexts of news articles and produce experts specializing in various aspects of news understanding; 3) LLMs could \\emph{merge task-specific experts} and provide an overall prediction by incorporating the predictions and confidence scores of varying experts. Extensive experiments on seven datasets with three LLMs demonstrate that DELL outperforms state-of-the-art baselines by up to 16.8\\% in macro f1-score. Further analysis reveals that the generated reactions and explanations are greatly helpful in misinformation detection, while our proposed LLM-guided expert merging helps produce better-calibrated predictions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10427",
        "abstract url": "https://arxiv.org/abs/2402.10427",
        "title": "Evaluating and Improving Continual Learning in Spoken Language Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Continual learning has emerged as an increasingly important challenge across various tasks, including Spoken Language Understanding (SLU). In SLU, its objective is to effectively handle the emergence of new concepts and evolving environments. The evaluation of continual learning algorithms typically involves assessing the model's stability, plasticity, and generalizability as fundamental aspects of standards. However, existing continual learning metrics primarily focus on only one or two of the properties. They neglect the overall performance across all tasks, and do not adequately disentangle the plasticity versus stability/generalizability trade-offs within the model. In this work, we propose an evaluation methodology that provides a unified evaluation on stability, plasticity, and generalizability in continual learning. By employing the proposed metric, we demonstrate how introducing various knowledge distillations can improve different aspects of these three properties of the SLU model. We further show that our proposed metric is more sensitive in capturing the impact of task ordering in continual learning, making it better suited for practical use-case scenarios.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10430",
        "abstract url": "https://arxiv.org/abs/2402.10430",
        "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics & humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10435",
        "abstract url": "https://arxiv.org/abs/2402.10435",
        "title": "Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Person re-identification (re-ID) continues to pose a significant challenge, particularly in scenarios involving occlusions. Prior approaches aimed at tackling occlusions have predominantly focused on aligning physical body features through the utilization of external semantic cues. However, these methods tend to be intricate and susceptible to noise. To address the aforementioned challenges, we present an innovative end-to-end solution known as the Dynamic Patch-aware Enrichment Transformer (DPEFormer). This model effectively distinguishes human body information from occlusions automatically and dynamically, eliminating the need for external detectors or precise image alignment. Specifically, we introduce a dynamic patch token selection module (DPSM). DPSM utilizes a label-guided proxy token as an intermediary to identify informative occlusion-free tokens. These tokens are then selected for deriving subsequent local part features. To facilitate the seamless integration of global classification features with the finely detailed local features selected by DPSM, we introduce a novel feature blending module (FBM). FBM enhances feature representation through the complementary nature of information and the exploitation of part diversity. Furthermore, to ensure that DPSM and the entire DPEFormer can effectively learn with only identity labels, we also propose a Realistic Occlusion Augmentation (ROA) strategy. This strategy leverages the recent advances in the Segment Anything Model (SAM). As a result, it generates occlusion images that closely resemble real-world occlusions, greatly enhancing the subsequent contrastive learning process. Experiments on occluded and holistic re-ID benchmarks signify a substantial advancement of DPEFormer over existing state-of-the-art approaches. The code will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 6 figures"
    },
    {
        "paper id": "2402.10436",
        "abstract url": "https://arxiv.org/abs/2402.10436",
        "title": "I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We explored cultural biases-individualism vs. collectivism-in ChatGPT across three Western languages (i.e., English, German, and French) and three Eastern languages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an individualistic persona in Western languages, its collectivism scores (i.e., out-group values) exhibited a more negative trend, surpassing their positive orientation towards individualism (i.e., in-group values). Conversely, when a collectivistic persona was assigned to ChatGPT in Eastern languages, a similar pattern emerged with more negative responses toward individualism (i.e., out-group values) as compared to collectivism (i.e., in-group values). The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group. The experiment was replicated in the political domain, and the results remained consistent. Furthermore, this replication unveiled an intrinsic Democratic bias in Large Language Models (LLMs), aligning with earlier findings and providing integral insights into mitigating such bias through prompt engineering. Extensive robustness checks were performed using varying hyperparameter and persona setup methods, with or without social identity labels, across other popular language models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10447",
        "abstract url": "https://arxiv.org/abs/2402.10447",
        "title": "Incremental Sequence Labeling: A Tale of Two Shifts",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, to tackle the O2E problem, we alleviate the model's bias towards new entities through debiased loss and optimization levels. Our experimental evaluation, conducted on three datasets with various incremental settings, demonstrates the superior performance of IS3 compared to the previous state-of-the-art method by a significant margin.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10978",
        "abstract url": "https://arxiv.org/abs/2402.10978",
        "title": "Language Models with Conformal Factuality Guarantees",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can provide 80-90% correctness guarantees while retaining the majority of the LM's original output.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10979",
        "abstract url": "https://arxiv.org/abs/2402.10979",
        "title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.08471",
        "abstract url": "https://arxiv.org/abs/2404.08471",
        "title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18934",
        "abstract url": "https://arxiv.org/abs/2404.18934",
        "title": "The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye Movement, Odometry, and Egocentric Video",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce the Visual Experience Dataset (VEDB), a compilation of over 240 hours of egocentric video combined with gaze- and head-tracking data that offers an unprecedented view of the visual world as experienced by human observers. The dataset consists of 717 sessions, recorded by 58 observers ranging from 6-49 years old. This paper outlines the data collection, processing, and labeling protocols undertaken to ensure a representative sample and discusses the potential sources of error or bias within the dataset. The VEDB's potential applications are vast, including improving gaze tracking methodologies, assessing spatiotemporal image statistics, and refining deep neural networks for scene and activity recognition. The VEDB is accessible through established open science platforms and is intended to be a living dataset with plans for expansion and community contributions. It is released with an emphasis on ethical considerations, such as participant privacy and the mitigation of potential biases. By providing a dataset grounded in real-world experiences and accompanied by extensive metadata and supporting code, the authors invite the research community to utilize and contribute to the VEDB, facilitating a richer understanding of visual perception and behavior in naturalistic settings.",
        "subjects": [
            "cs.CV",
            "cs.HC"
        ],
        "comment": "36 pages, 1 table, 7 figures"
    },
    {
        "paper id": "2402.09716",
        "abstract url": "https://arxiv.org/abs/2402.09716",
        "title": "User Privacy Harms and Risks in Conversational AI: A Proposed Framework",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "This study presents a unique framework that applies and extends Solove (2006)'s taxonomy to address privacy concerns in interactions with text-based AI chatbots. As chatbot prevalence grows, concerns about user privacy have heightened. While existing literature highlights design elements compromising privacy, a comprehensive framework is lacking. Through semi-structured interviews with 13 participants interacting with two AI chatbots, this study identifies 9 privacy harms and 9 privacy risks in text-based interactions. Using a grounded theory approach for interview and chatlog analysis, the framework examines privacy implications at various interaction stages. The aim is to offer developers, policymakers, and researchers a tool for responsible and secure implementation of conversational AI, filling the existing gap in addressing privacy issues associated with text-based AI chatbots.",
        "subjects": [
            "cs.HC",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09721",
        "abstract url": "https://arxiv.org/abs/2402.09721",
        "title": "Persuading a Learning Agent",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bounded by the agent's regret (swap-regret). If the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can do significantly better than the non-learning model. These conclusions hold not only for Bayesian persuasion, but also for any generalized principal-agent problem with complete information, including Stackelberg games and contract design.",
        "subjects": [
            "cs.GT",
            "cs.AI",
            "cs.LG",
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09734",
        "abstract url": "https://arxiv.org/abs/2402.09734",
        "title": "Agents Need Not Know Their Purpose",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Ensuring artificial intelligence behaves in such a way that is aligned with human values is commonly referred to as the alignment challenge. Prior work has shown that rational agents, behaving in such a way that maximizes a utility function, will inevitably behave in such a way that is not aligned with human values, especially as their level of intelligence goes up. Prior work has also shown that there is no \"one true utility function\"; solutions must include a more holistic approach to alignment. This paper describes oblivious agents: agents that are architected in such a way that their effective utility function is an aggregation of a known and hidden sub-functions. The hidden component, to be maximized, is internally implemented as a black box, preventing the agent from examining it. The known component, to be minimized, is knowledge of the hidden sub-function. Architectural constraints further influence how agent actions can evolve its internal environment model. We show that an oblivious agent, behaving rationally, constructs an internal approximation of designers' intentions (i.e., infers alignment), and, as a consequence of its architecture and effective utility function, behaves in such a way that maximizes alignment; i.e., maximizing the approximated intention function. We show that, paradoxically, it does this for whatever utility function is used as the hidden component and, in contrast with extant techniques, chances of alignment actually improve as agent intelligence grows.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09735",
        "abstract url": "https://arxiv.org/abs/2402.09735",
        "title": "DFORM: Diffeomorphic vector field alignment for assessing dynamics across learned models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Dynamical system models such as Recurrent Neural Networks (RNNs) have become increasingly popular as hypothesis-generating tools in scientific research. Evaluating the dynamics in such networks is key to understanding their learned generative mechanisms. However, comparison of learned dynamics across models is challenging due to their inherent nonlinearity and because a priori there is no enforced equivalence of their coordinate systems. Here, we propose the DFORM (Diffeomorphic vector field alignment for comparing dynamics across learned models) framework. DFORM learns a nonlinear coordinate transformation which provides a continuous, maximally one-to-one mapping between the trajectories of learned models, thus approximating a diffeomorphism between them. The mismatch between DFORM-transformed vector fields defines the orbital similarity between two models, thus providing a generalization of the concepts of smooth orbital and topological equivalence. As an example, we apply DFORM to models trained on a canonical neuroscience task, showing that learned dynamics may be functionally similar, despite overt differences in attractor landscapes.",
        "subjects": [
            "cs.LG",
            "eess.SY",
            "q-bio.NC"
        ],
        "comment": "12 pages, 8 figures"
    },
    {
        "paper id": "2402.09746",
        "abstract url": "https://arxiv.org/abs/2402.09746",
        "title": "Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recently, we introduced a new paradigm for alpha mining in the realm of quantitative investment, developing a new interactive alpha mining system framework, Alpha-GPT. This system is centered on iterative Human-AI interaction based on large language models, introducing a Human-in-the-Loop approach to alpha discovery. In this paper, we present the next-generation Alpha-GPT 2.0 \\footnote{Draft. Work in progress}, a quantitative investment framework that further encompasses crucial modeling and analysis phases in quantitative investment. This framework emphasizes the iterative, interactive research between humans and AI, embodying a Human-in-the-Loop strategy throughout the entire quantitative investment pipeline. By assimilating the insights of human researchers into the systematic alpha research process, we effectively leverage the Human-in-the-Loop approach, enhancing the efficiency and precision of quantitative investment research.",
        "subjects": [
            "q-fin.CP",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09750",
        "abstract url": "https://arxiv.org/abs/2402.09750",
        "title": "Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recently, the potential of large language models (LLMs) has been widely used in assisting programming. However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists' different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of LLM in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design suggestions for future work of AI-assisted creative tasks.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": "15 pages, 4 figures"
    },
    {
        "paper id": "2402.09754",
        "abstract url": "https://arxiv.org/abs/2402.09754",
        "title": "Robust SVD Made Easy: A fast and reliable algorithm for large-scale data analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The singular value decomposition (SVD) is a crucial tool in machine learning and statistical data analysis. However, it is highly susceptible to outliers in the data matrix. Existing robust SVD algorithms often sacrifice speed for robustness or fail in the presence of only a few outliers. This study introduces an efficient algorithm, called Spherically Normalized SVD, for robust SVD approximation that is highly insensitive to outliers, computationally scalable, and provides accurate approximations of singular vectors. The proposed algorithm achieves remarkable speed by utilizing only two applications of a standard reduced-rank SVD algorithm to appropriately scaled data, significantly outperforming competing algorithms in computation times. To assess the robustness of the approximated singular vectors and their subspaces against data contamination, we introduce new notions of breakdown points for matrix-valued input, including row-wise, column-wise, and block-wise breakdown points. Theoretical and empirical analyses demonstrate that our algorithm exhibits higher breakdown points compared to standard SVD and its modifications. We empirically validate the effectiveness of our approach in applications such as robust low-rank approximation and robust principal component analysis of high-dimensional microarray datasets. Overall, our study presents a highly efficient and robust solution for SVD approximation that overcomes the limitations of existing algorithms in the presence of outliers.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09764",
        "abstract url": "https://arxiv.org/abs/2402.09764",
        "title": "Aligning Crowd Feedback via Distributional Preference Reward Modeling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM policy to generate responses favoured by the population. Our experiments show that DPRM significantly enhances the alignment of LLMs with population preference, yielding more accurate, unbiased, and contextually appropriate responses.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09766",
        "abstract url": "https://arxiv.org/abs/2402.09766",
        "title": "From Variability to Stability: Advancing RecSys Benchmarking Practices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, we validate the reliability of our methodology under the variability of datasets, offering a benchmarking strategy that balances quality and computational demands. This methodology enables a fair yet effective means of evaluating RecSys algorithms, providing valuable guidance for future research endeavors.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "8 pages with 11 figures"
    },
    {
        "paper id": "2402.09769",
        "abstract url": "https://arxiv.org/abs/2402.09769",
        "title": "Representation Learning Using a Single Forward Pass",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm (SPELA). SPELA is a prime candidate for training and inference applications in Edge AI devices. At the same time, SPELA can optimally cater to the need for a framework to study perceptual representation learning and formation. SPELA has distinctive features such as neural priors (in the form of embedded vectors), no weight transport, no update locking of weights, complete local Hebbian learning, single forward pass with no storage of activations, and single weight update per sample. Juxtaposed with traditional approaches, SPELA operates without the need for backpropagation. We show that our algorithm can perform nonlinear classification on a noisy boolean operation dataset. Additionally, we exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST. Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on MNIST, KMNIST, and Fashion MNIST, where it consistently outperforms backpropagation.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2402.09796",
        "abstract url": "https://arxiv.org/abs/2402.09796",
        "title": "Closed-form Filtering for Non-linear Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Sequential Bayesian Filtering aims to estimate the current state distribution of a Hidden Markov Model, given the past observations. The problem is well-known to be intractable for most application domains, except in notable cases such as the tabular setting or for linear dynamical systems with gaussian noise. In this work, we propose a new class of filters based on Gaussian PSD Models, which offer several advantages in terms of density approximation and computational efficiency. We show that filtering can be efficiently performed in closed form when transitions and observations are Gaussian PSD Models. When the transition and observations are approximated by Gaussian PSD Models, we show that our proposed estimator enjoys strong theoretical guarantees, with estimation error that depends on the quality of the approximation and is adaptive to the regularity of the transition probabilities. In particular, we identify regimes in which our proposed filter attains a TV $\u03b5$-error with memory and computational complexity of $O(\u03b5^{-1})$ and $O(\u03b5^{-3/2})$ respectively, including the offline learning step, in contrast to the $O(\u03b5^{-2})$ complexity of sampling methods such as particle filtering.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "cs.RO"
        ],
        "comment": "38 pages"
    },
    {
        "paper id": "2402.09802",
        "abstract url": "https://arxiv.org/abs/2402.09802",
        "title": "Criterion collapse and loss distribution control",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we consider the notion of \"criterion collapse,\" in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD). We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09807",
        "abstract url": "https://arxiv.org/abs/2402.09807",
        "title": "Two trust region type algorithms for solving nonconvex-strongly concave minimax problems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we propose a Minimax Trust Region (MINIMAX-TR) algorithm and a Minimax Trust Region Algorithm with Contractions and Expansions(MINIMAX-TRACE) algorithm for solving nonconvex-strongly concave minimax problems. Both algorithms can find an $(\u03b5, \\sqrt\u03b5)$-second order stationary point(SSP) within $\\mathcal{O}(\u03b5^{-1.5})$ iterations, which matches the best well known iteration complexity.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09820",
        "abstract url": "https://arxiv.org/abs/2402.09820",
        "title": "Utilizing Deep Learning for Enhancing Network Resilience in Finance",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the age of the Internet, people's lives are increasingly dependent on today's network technology. Maintaining network integrity and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown risks causes the detection accuracy of the original model to decline. With this in mind, this paper uses deep learning for advanced threat detection to improve protective measures in the financial industry. Many network researchers have shifted their focus to exception-based intrusion detection techniques. The detection technology mainly uses statistical machine learning methods - collecting normal program and network behavior data, extracting multidimensional features, and training decision machine learning models on this basis (commonly used include naive Bayes, decision trees, support vector machines, random forests, etc.).",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.LG",
            "q-fin.GN"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09836",
        "abstract url": "https://arxiv.org/abs/2402.09836",
        "title": "Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning. Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between LLM reasoning and mechanistic gravity model. It leverages the step-by-step LLM reasoning to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model. Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time. Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications. The implementation of our approach is available in the paper.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09838",
        "abstract url": "https://arxiv.org/abs/2402.09838",
        "title": "Performative Reinforcement Learning in Gradually Shifting Environments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines samples from multiple deployments in its training. This makes MDRR particularly suitable for scenarios where the environment's response strongly depends on its previous dynamics, which are common in practice. We experimentally compare the algorithms using a simulation-based testbed and our results show that MDRR converges significantly faster than previous approaches.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09844",
        "abstract url": "https://arxiv.org/abs/2402.09844",
        "title": "Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat), including a pioneering general-purpose dataset.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2402.09849",
        "abstract url": "https://arxiv.org/abs/2402.09849",
        "title": "Recommendations for Baselines and Benchmarking Approximate Gaussian Processes",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Gaussian processes (GPs) are a mature and widely-used component of the ML toolbox. One of their desirable qualities is automatic hyperparameter selection, which allows for training without user intervention. However, in many realistic settings, approximations are typically needed, which typically do require tuning. We argue that this requirement for tuning complicates evaluation, which has led to a lack of a clear recommendations on which method should be used in which situation. To address this, we make recommendations for comparing GP approximations based on a specification of what a user should expect from a method. In addition, we develop a training procedure for the variational method of Titsias [2009] that leaves no choices to the user, and show that this is a strong baseline that meets our specification. We conclude that benchmarking according to our suggestions gives a clearer view of the current state of the field, and uncovers problems that are still open that future papers should address.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Preprint. 25 pages, 16 figures"
    },
    {
        "paper id": "2402.09872",
        "abstract url": "https://arxiv.org/abs/2402.09872",
        "title": "Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community",
        "rating": "0.5",
        "keywords": [
            [
                "synthesis",
                "text-to-image"
            ],
            [
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to engage and contribute with content. The recent progress of text-conditioned image synthesis has ushered in a collaborative era where AI empowers users to craft original visual artworks seeking community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Existing evaluation methods predominantly center on limited size user studies guided by image quality and prompt alignment. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from Picsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named Picsart Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art. Codes can be accessed at https://github.com/Picsart-AI-Research/Social-Reward",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages with 10 figures, accepted at ICLR 2024 as a spotlight, codes can be accessed at https://github.com/Picsart-AI-Research/Social-Reward"
    },
    {
        "paper id": "2402.09877",
        "abstract url": "https://arxiv.org/abs/2402.09877",
        "title": "On Computing Plans with Uniform Action Costs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09881",
        "abstract url": "https://arxiv.org/abs/2402.09881",
        "title": "Explaining Kernel Clustering via Decision Trees",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09900",
        "abstract url": "https://arxiv.org/abs/2402.09900",
        "title": "Revisiting Recurrent Reinforcement Learning with Memory Monoids",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Memory models such as Recurrent Neural Networks (RNNs) and Transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that we can model the recurrent update of these models using a monoid, leading us to reformulate existing models using a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. We leverage the properties of memory monoids to propose a batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09921",
        "abstract url": "https://arxiv.org/abs/2402.09921",
        "title": "Identifying and modelling cognitive biases in mobility choices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "This report presents results from an M1 internship dedicated to agent-based modelling and simulation of daily mobility choices. This simulation is intended to be realistic enough to serve as a basis for a serious game about the mobility transition. In order to ensure this level of realism, we conducted a survey to measure if real mobility choices are made rationally, or how biased they are. Results analysed here show that various biases could play a role in decisions. We then propose an implementation in a GAMA agent-based simulation.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.MA"
        ],
        "comment": "M1 internship report from Univ. Lyon 1 Claude Bernard. Internship was from October 2022 to June 2023"
    },
    {
        "paper id": "2402.09947",
        "abstract url": "https://arxiv.org/abs/2402.09947",
        "title": "Explaining Probabilistic Models with Distributional Values",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Code coming soon"
    },
    {
        "paper id": "2402.09961",
        "abstract url": "https://arxiv.org/abs/2402.09961",
        "title": "Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Crowdsourced delivery platforms face complex scheduling challenges to match couriers and customer orders. We consider two types of crowdsourced couriers, namely, committed and occasional couriers, each with different compensation schemes. Crowdsourced delivery platforms usually schedule committed courier shifts based on predicted demand. Therefore, platforms may devise an offline schedule for committed couriers before the planning period. However, due to the unpredictability of demand, there are instances where it becomes necessary to make online adjustments to the offline schedule. In this study, we focus on the problem of dynamically adjusting the offline schedule through shift extensions for committed couriers. This problem is modeled as a sequential decision process. The objective is to maximize platform profit by determining the shift extensions of couriers and the assignments of requests to couriers. To solve the model, a Deep Q-Network (DQN) learning approach is developed. Comparing this model with the baseline policy where no extensions are allowed demonstrates the benefits that platforms can gain from allowing shift extensions in terms of reward, reduced lost order costs, and lost requests. Additionally, sensitivity analysis showed that the total extension compensation increases in a nonlinear manner with the arrival rate of requests, and in a linear manner with the arrival rate of occasional couriers. On the compensation sensitivity, the results showed that the normal scenario exhibited the highest average number of shift extensions and, consequently, the fewest average number of lost requests. These findings serve as evidence of the successful learning of such dynamics by the DQN algorithm.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09963",
        "abstract url": "https://arxiv.org/abs/2402.09963",
        "title": "Why are Sensitive Functions Hard for Transformers?",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers' inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Fixed various errors"
    },
    {
        "paper id": "2402.09978",
        "abstract url": "https://arxiv.org/abs/2402.09978",
        "title": "Deep learning for the design of non-Hermitian topolectrical circuits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Non-Hermitian topological phases can produce some remarkable properties, compared with their Hermitian counterpart, such as the breakdown of conventional bulk-boundary correspondence and the non-Hermitian topological edge mode. Here, we introduce several algorithms with multi-layer perceptron (MLP), and convolutional neural network (CNN) in the field of deep learning, to predict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we use the smallest module of the periodic circuit as one unit to construct high-dimensional circuit data features. Further, we use the Dense Convolutional Network (DenseNet), a type of convolutional neural network that utilizes dense connections between layers to design a non-Hermitian topolectrical Chern circuit, as the DenseNet algorithm is more suitable for processing high-dimensional data. Our results demonstrate the effectiveness of the deep learning network in capturing the global topological characteristics of a non-Hermitian system based on training data.",
        "subjects": [
            "physics.app-ph",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09992",
        "abstract url": "https://arxiv.org/abs/2402.09992",
        "title": "Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-stage stochastic combinatorial optimization problems and perform numerical experiments to empirically validate our algorithm's robustness against realistic distribution shifts, without compromising performance on the training distribution. We show that our algorithm is superior to risk-neutral Soft Actor-Critic as well as to two benchmark approaches for robust deep reinforcement learning. Thereby, we provide the first structured analysis on the robustness of reinforcement learning under distribution shifts in the realm of contextual multi-stage stochastic combinatorial optimization problems.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": "11 pages, 8 figures"
    },
    {
        "paper id": "2402.10002",
        "abstract url": "https://arxiv.org/abs/2402.10002",
        "title": "MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding",
        "rating": "0.5",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks. For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully supervised methods. Additionally, we demonstrate its effectiveness in tasks such as few-shot classification, 3D part segmentation and 3D semantic segmentation.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "comment": "Accepted by AAAI 2024"
    },
    {
        "paper id": "2402.10036",
        "abstract url": "https://arxiv.org/abs/2402.10036",
        "title": "Predictive Linear Online Tracking for Unknown Targets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we study the problem of online tracking in linear control systems, where the objective is to follow a moving target. Unlike classical tracking control, the target is unknown, non-stationary, and its state is revealed sequentially, thus, fitting the framework of online non-stochastic control. We consider the case of quadratic costs and propose a new algorithm, called predictive linear online tracking (PLOT). The algorithm uses recursive least squares with exponential forgetting to learn a time-varying dynamic model of the target. The learned model is used in the optimal policy under the framework of receding horizon control. We show the dynamic regret of PLOT scales with $\\mathcal{O}(\\sqrt{TV_T})$, where $V_T$ is the total variation of the target dynamics and $T$ is the time horizon. Unlike prior work, our theoretical results hold for non-stationary targets. We implement PLOT on a real quadrotor and provide open-source software, thus, showcasing one of the first successful applications of online control methods on real hardware.",
        "subjects": [
            "eess.SY",
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10043",
        "abstract url": "https://arxiv.org/abs/2402.10043",
        "title": "Negative impact of heavy-tailed uncertainty and error distributions on the reliability of calibration statistics for machine learning regression tasks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Average calibration of the prediction uncertainties of machine learning regression tasks can be tested in two ways: one is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty; the alternative is to compare the mean squared z-scores (ZMS) or scaled errors to 1. The problem is that both approaches might lead to different conclusions, as illustrated in this study for an ensemble of datasets from the recent machine learning uncertainty quantification (ML-UQ) literature. It is shown that the estimation of MV, MSE and their confidence intervals can become unreliable for heavy-tailed uncertainty and error distributions, which seems to be a common issue for ML-UQ datasets. By contrast, the ZMS statistic is less sensitive and offers the most reliable approach in this context. Unfortunately, the same problem affects also conditional calibrations statistics, such as the popular ENCE, and very likely post-hoc calibration methods based on similar statistics. As not much can be done to relieve this issue, except for a change of paradigm to intervals- or distribution-based UQ metrics, robust tailedness metrics are proposed to detect the potentially problematic datasets.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10046",
        "abstract url": "https://arxiv.org/abs/2402.10046",
        "title": "How Flawed is ECE? An Analysis via Logit Smoothing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical pathologies of ECE may be avoidable in practice.",
        "subjects": [
            "cs.LG",
            "math.PR"
        ],
        "comment": "22 pages, 4 figures"
    },
    {
        "paper id": "2402.10061",
        "abstract url": "https://arxiv.org/abs/2402.10061",
        "title": "X-maps: Direct Depth Lookup for Event-based Structured Light Systems",
        "rating": "0.5",
        "keywords": [
            [
                "Depth",
                "event cameras"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras. These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach. Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search. Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process. Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy. Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events. This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial. We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results. Additional results and code are available on our project page: https://fraunhoferhhi.github.io/X-maps/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at the CVPR 2023 Workshop on Event-based Vision: https://tub-rip.github.io/eventvision2023/"
    },
    {
        "paper id": "2402.10063",
        "abstract url": "https://arxiv.org/abs/2402.10063",
        "title": "Balancing the Causal Effects in Class-Incremental Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence. Recently, Pre-Trained Models (PTMs) have led to breakthroughs in both visual and natural language processing tasks. Despite recent studies showing PTMs' potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we reveal that the crux lies in the imbalanced causal effects between new and old data. Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes. Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes. In other words, the adaptation process between new and old classes conflicts from the causal perspective. To alleviate this problem, we propose Balancing the Causal Effects (BaCE) in CIL. Concretely, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively. In this way, the model is encouraged to adapt to all classes with causal effects from both new and old data and thus alleviates the causal imbalance problem. We conduct extensive experiments on continual image classification, continual text classification, and continual named entity recognition. Empirical results show that BaCE outperforms a series of CIL methods on different tasks and settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10118",
        "abstract url": "https://arxiv.org/abs/2402.10118",
        "title": "Reusing Softmax Hardware Unit for GELU Computation in Transformers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) allows the reduction of the overall hardware area and power by 6.1% and 11.9%, respectively, on average.",
        "subjects": [
            "cs.AR",
            "cs.LG"
        ],
        "comment": "AICAS 2024"
    },
    {
        "paper id": "2402.10127",
        "abstract url": "https://arxiv.org/abs/2402.10127",
        "title": "Nonlinear spiked covariance matrices and signal propagation in deep neural networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network. However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the ''spike'' eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case. Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. As a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over training and characterize the alignment of the target function with the spike eigenvector of the CK on test data.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.PR",
            "math.ST"
        ],
        "comment": "55 pages"
    },
    {
        "paper id": "2402.10133",
        "abstract url": "https://arxiv.org/abs/2402.10133",
        "title": "Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models to propose levels based on the gameplay data continuously collected from individual players. We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques. Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "9 pages, 6 figures"
    },
    {
        "paper id": "2402.10142",
        "abstract url": "https://arxiv.org/abs/2402.10142",
        "title": "Tracking Changing Probabilities via Dynamic Learners",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The stream is unbounded and the predictor has finite limited space and we seek efficient prediction and update techniques: the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few recently frequent items may cease to occur again. The predictor, being space-bounded, need only provide probabilities for those items with (currently) sufficiently high frequency, i.e., the salient items. This problem is motivated in the setting of prediction games, a self-supervised learning regime where concepts serve as both the predictors and the predictands, and the set of concepts grows over time, resulting in non-stationarities as new concepts are generated and used. We develop sparse multiclass moving average techniques designed to respond to such non-stationarities in a timely manner. One technique is based on the exponentiated moving average (EMA) and another is based on queuing a few count snapshots. We show that the combination, and in particular supporting dynamic predictand-specific learning rates, offers advantages in terms of faster change detection and convergence.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "63 pages, 24 figures, 17 tables"
    },
    {
        "paper id": "2402.10150",
        "abstract url": "https://arxiv.org/abs/2402.10150",
        "title": "$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance. Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives. Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to TMLR in 2023"
    },
    {
        "paper id": "2402.10164",
        "abstract url": "https://arxiv.org/abs/2402.10164",
        "title": "Random features and polynomial rules",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experiments performed over many order of magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic limits where $D\\to \\infty$ and at least one between $P/D^K$, $N/D^L$ remains finite.",
        "subjects": [
            "cond-mat.dis-nn",
            "cs.LG"
        ],
        "comment": "11 pages + appendix, 4 figures. Comments are welcome"
    },
    {
        "paper id": "2402.10186",
        "abstract url": "https://arxiv.org/abs/2402.10186",
        "title": "Self-consistent Validation for Machine Learning Electronic Structure",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning has emerged as a significant approach to efficiently tackle electronic structure problems. Despite its potential, there is less guarantee for the model to generalize to unseen data that hinders its application in real-world scenarios. To address this issue, a technique has been proposed to estimate the accuracy of the predictions. This method integrates machine learning with self-consistent field methods to achieve both low validation cost and interpret-ability. This, in turn, enables exploration of the model's ability with active learning and instills confidence in its integration into real-world studies.",
        "subjects": [
            "cs.LG",
            "physics.chem-ph",
            "physics.comp-ph"
        ],
        "comment": "6 pages, 4 figures"
    },
    {
        "paper id": "2402.10202",
        "abstract url": "https://arxiv.org/abs/2402.10202",
        "title": "Bridging Associative Memory and Probabilistic Modeling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \\textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the evidence lower bound. Third, using tools from associative memory, we analytically and numerically characterize the memory capacity of Gaussian kernel density estimators, a widespread tool in probababilistic modeling. Fourth, we study a widespread implementation choice in transformers -- normalization followed by self attention -- to show it performs clustering on the hypersphere. Altogether, this work urges further exchange of useful ideas between these two continents of artificial intelligence.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10248",
        "abstract url": "https://arxiv.org/abs/2402.10248",
        "title": "A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Global ambient air pollution, a transboundary challenge, is typically addressed through interventions relying on data from spatially sparse and heterogeneously placed monitoring stations. These stations often encounter temporal data gaps due to issues such as power outages. In response, we have developed a scalable, data-driven, supervised machine learning framework. This model is designed to impute missing temporal and spatial measurements, thereby generating a comprehensive dataset for pollutants including NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of 0.25$^{\\circ}$ at hourly intervals and accompanied by prediction intervals for each estimate, caters to a wide range of stakeholders relying on outdoor air pollution data for downstream assessments. This enables more detailed studies. Additionally, the model's performance across various geographical locations is examined, providing insights and recommendations for strategic placement of future monitoring stations to further enhance the model's accuracy.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Main Paper: 25 pages, 15 figures, 5 tables. Supplementary: 4 pages, 3 figures"
    },
    {
        "paper id": "2402.10251",
        "abstract url": "https://arxiv.org/abs/2402.10251",
        "title": "Brant-2: Foundation Model for Brain Signals",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintain performance in scenarios with scarce labels.",
        "subjects": [
            "q-bio.NC",
            "cs.AI",
            "cs.LG",
            "eess.SP"
        ],
        "comment": "14 pages, 7 figures"
    },
    {
        "paper id": "2402.10252",
        "abstract url": "https://arxiv.org/abs/2402.10252",
        "title": "Online Control of Linear Systems with Unbounded and Degenerate Noise",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates the problem of controlling a linear system under possibly unbounded and degenerate noise with unknown cost functions, known as an online control problem. In contrast to the existing work, which assumes the boundedness of noise, we reveal that for convex costs, an $ \\widetilde{O}(\\sqrt{T}) $ regret bound can be achieved even for unbounded noise, where $ T $ denotes the time horizon. Moreover, when the costs are strongly convex, we establish an $ O({\\rm poly} (\\log T)) $ regret bound without the assumption that noise covariance is non-degenerate, which has been required in the literature. The key ingredient in removing the rank assumption on noise is a system transformation associated with the noise covariance. This simultaneously enables the parameter reduction of an online control algorithm.",
        "subjects": [
            "eess.SY",
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "comment": "26 pages"
    },
    {
        "paper id": "2402.10282",
        "abstract url": "https://arxiv.org/abs/2402.10282",
        "title": "Information Capacity Regret Bounds for Bandits with Mediator Feedback",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work addresses the mediator feedback problem, a bandit game where the decision set consists of a number of policies, each associated with a probability distribution over a common space of outcomes. Upon choosing a policy, the learner observes an outcome sampled from its distribution and incurs the loss assigned to this outcome in the present round. We introduce the policy set capacity as an information-theoretic measure for the complexity of the policy set. Adopting the classical EXP4 algorithm, we provide new regret bounds depending on the policy set capacity in both the adversarial and the stochastic settings. For a selection of policy set families, we prove nearly-matching lower bounds, scaling similarly with the capacity. We also consider the case when the policies' distributions can vary between rounds, thus addressing the related bandits with expert advice problem, which we improve upon its prior results. Additionally, we prove a lower bound showing that exploiting the similarity between the policies is not possible in general under linear bandit feedback. Finally, for a full-information variant, we provide a regret bound scaling with the information radius of the policy set.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10289",
        "abstract url": "https://arxiv.org/abs/2402.10289",
        "title": "Thompson Sampling in Partially Observable Contextual Bandits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Contextual bandits constitute a classical framework for decision-making under uncertainty. In this setting, the goal is to learn the arms of highest reward subject to contextual information, while the unknown reward parameters of each arm need to be learned by experimenting that specific arm. Accordingly, a fundamental problem is that of balancing exploration (i.e., pulling different arms to learn their parameters), versus exploitation (i.e., pulling the best arms to gain reward). To study this problem, the existing literature mostly considers perfectly observed contexts. However, the setting of partial context observations remains unexplored to date, despite being theoretically more general and practically more versatile. We study bandit policies for learning to select optimal arms based on the data of observations, which are noisy linear functions of the unobserved context vectors. Our theoretical analysis shows that the Thompson sampling policy successfully balances exploration and exploitation. Specifically, we establish the followings: (i) regret bounds that grow poly-logarithmically with time, (ii) square-root consistency of parameter estimation, and (iii) scaling of the regret with other quantities including dimensions and number of arms. Extensive numerical experiments with both real and synthetic data are presented as well, corroborating the efficacy of Thompson sampling. To establish the results, we introduce novel martingale techniques and concentration inequalities to address partially observed dependent random variables generated from unspecified distributions, and also leverage problem-dependent information to sharpen probabilistic bounds for time-varying suboptimality gaps. These techniques pave the road towards studying other decision-making problems with contextual information as well as partial observations.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "43 pages"
    },
    {
        "paper id": "2402.10290",
        "abstract url": "https://arxiv.org/abs/2402.10290",
        "title": "Experiments with Encoding Structured Data for Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The project's aim is to create an AI agent capable of selecting good actions in a game-playing domain called Battlespace. Sequential domains like Battlespace are important testbeds for planning problems, as such, the Department of Defense uses such domains for wargaming exercises. The agents we developed combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN) techniques in an effort to navigate the game environment, avoid obstacles, interact with adversaries, and capture the flag. This paper will focus on the encoding techniques we explored to present complex structured data stored in a Python class, a necessary precursor to an agent.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "18 pages, 8 figures, 2 tables"
    },
    {
        "paper id": "2402.10291",
        "abstract url": "https://arxiv.org/abs/2402.10291",
        "title": "An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic grounded in the Maximum Mean Discrepancy (MMD) non-parametric framework. This approach extends KCUSUM's pertinence to scenarios where only reference samples are available, such as atomic trajectories of proteins in vacuum, facilitating the detection of deviations from the reference sample without prior knowledge of the data's underlying distribution. Furthermore, by harnessing MMD's inherent random-walk structure, we can theoretically analyze KCUSUM's performance across various use cases, including metrics like expected delay and mean runtime to false alarms. Finally, we discuss real-world use cases from scientific simulations such as NWChem CODAR and protein folding data, demonstrating KCUSUM's practical effectiveness in online change point detection.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "16 pages. arXiv admin note: text overlap with arXiv:1903.01661"
    },
    {
        "paper id": "2402.10309",
        "abstract url": "https://arxiv.org/abs/2402.10309",
        "title": "Discrete Probabilistic Inference as Control in Multi-path Environments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL policy is proportional to the original reward, regardless of the structure of the underlying MDP. We also prove that some flow-matching objectives found in the GFlowNet literature are in fact equivalent to well-established MaxEnt RL algorithms with a corrected reward. Finally, we study empirically the performance of multiple MaxEnt RL and GFlowNet algorithms on multiple problems involving sampling from discrete distributions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10335",
        "abstract url": "https://arxiv.org/abs/2402.10335",
        "title": "Correlation Clustering with Vertex Splitting",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "We explore Cluster Editing and its generalization Correlation Clustering with a new operation called permissive vertex splitting which addresses finding overlapping clusters in the face of uncertain information. We determine that both problems are NP-hard, yet they exhibit significant differences in parameterized complexity and approximability. For Cluster Editing with Permissive Vertex Splitting, we show a polynomial kernel when parameterized by the solution size and develop a polynomial-time algorithm with approximation factor 7. In the case of Correlation Clustering, we establish para-NP-hardness when parameterized by solution size and demonstrate that computing an $n^{1-\u03b5}$-approximation is NP-hard for any constant $\u03b5> 0$. Additionally, we extend the established link between Correlation Clustering and Multicut to the setting with permissive vertex splitting.",
        "subjects": [
            "cs.DS",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10339",
        "abstract url": "https://arxiv.org/abs/2402.10339",
        "title": "What to Do When Your Discrete Optimization Is the Size of a Neural Network?",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Oftentimes, machine learning applications using neural networks involve solving discrete optimization problems, such as in pruning, parameter-isolation-based continual learning and training of binary networks. Still, these discrete problems are combinatorial in nature and are also not amenable to gradient-based optimization. Additionally, classical approaches used in discrete settings do not scale well to large neural networks, forcing scientists and empiricists to rely on alternative methods. Among these, two main distinct sources of top-down information can be used to lead the model to good solutions: (1) extrapolating gradient information from points outside of the solution set (2) comparing evaluations between members of a subset of the valid solutions. We take continuation path (CP) methods to represent using purely the former and Monte Carlo (MC) methods to represent the latter, while also noting that some hybrid methods combine the two. The main goal of this work is to compare both approaches. For that purpose, we first overview the two classes while also discussing some of their drawbacks analytically. Then, on the experimental section, we compare their performance, starting with smaller microworld experiments, which allow more fine-grained control of problem variables, and gradually moving towards larger problems, including neural network regression and neural network pruning for image classification, where we additionally compare against magnitude-based pruning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Submitted to JMLR"
    },
    {
        "paper id": "2402.10360",
        "abstract url": "https://arxiv.org/abs/2402.10360",
        "title": "Learnability is a Compact Property",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem. On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations? We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by examining its finite projections. For realizable and agnostic learning with respect to a wide class of proper loss functions, we demonstrate an exact compactness result: a class is learnable with a given sample complexity precisely when the same is true of all its finite projections. For realizable learning with improper loss functions, we show that exact compactness of sample complexity can fail, and provide matching upper and lower bounds of a factor of 2 on the extent to which such sample complexities can differ. We conjecture that larger gaps are possible for the agnostic case. At the heart of our technical work is a compactness result concerning assignments of variables that maintain a class of functions below a target value, which generalizes Hall's classic matching theorem and may be of independent interest.",
        "subjects": [
            "cs.LG",
            "cs.CC",
            "cs.DS",
            "cs.LO",
            "stat.ML"
        ],
        "comment": "23 pages, 1 figure"
    },
    {
        "paper id": "2402.10374",
        "abstract url": "https://arxiv.org/abs/2402.10374",
        "title": "Revisiting Experience Replayable Conditions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Experience replay (ER) used in (deep) reinforcement learning is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict \"experience replayable conditions\" (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical simulations that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning performance is comparable to that of a soft actor-critic, a state-of-the-art off-policy algorithm.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "22 pages, 8 figures"
    },
    {
        "paper id": "2402.10387",
        "abstract url": "https://arxiv.org/abs/2402.10387",
        "title": "MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Current generative models for drug discovery primarily use molecular docking to evaluate the quality of generated compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. We propose a multi-fidelity approach, Multi-Fidelity Bind (MFBind), to achieve the optimal trade-off between accuracy and computational cost. MFBind integrates docking and binding free energy simulators to train a multi-fidelity deep surrogate model with active learning. Our deep surrogate model utilizes a pretraining technique and linear prediction heads to efficiently fit small amounts of high-fidelity data. We perform extensive experiments and show that MFBind (1) outperforms other state-of-the-art single and multi-fidelity baselines in surrogate modeling, and (2) boosts the performance of generative models with markedly higher quality compounds.",
        "subjects": [
            "q-bio.BM",
            "cs.LG"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2402.10392",
        "abstract url": "https://arxiv.org/abs/2402.10392",
        "title": "Pretext Training Algorithms for Event Sequence Data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Pretext training followed by task-specific fine-tuning has been a successful approach in vision and language domains. This paper proposes a self-supervised pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and contrastive learning. Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public benchmarks demonstrate the potential of the proposed method across different tasks and data domains.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10423",
        "abstract url": "https://arxiv.org/abs/2402.10423",
        "title": "Connect the dots: Dataset Condensation, Differential Privacy, and Adversarial Uncertainty",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Our work focuses on understanding the underpinning mechanism of dataset condensation by drawing connections with ($\u03b5$, $\u03b4$)-differential privacy where the optimal noise, $\u03b5$, is chosen by adversarial uncertainty \\cite{Grining2017}. We can answer the question about the inner workings of the dataset condensation procedure. Previous work \\cite{dong2022} proved the link between dataset condensation (DC) and ($\u03b5$, $\u03b4$)-differential privacy. However, it is unclear from existing works on ablating DC to obtain a lower-bound estimate of $\u03b5$ that will suffice for creating high-fidelity synthetic data. We suggest that adversarial uncertainty is the most appropriate method to achieve an optimal noise level, $\u03b5$. As part of the internal dynamics of dataset condensation, we adopt a satisfactory scheme for noise estimation that guarantees high-fidelity data while providing privacy.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10429",
        "abstract url": "https://arxiv.org/abs/2402.10429",
        "title": "Fixed Confidence Best Arm Identification in the Bayesian Setting",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian Setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrary suboptimal performances in the Bayesian setting. We also prove a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10445",
        "abstract url": "https://arxiv.org/abs/2402.10445",
        "title": "Collaborative Learning with Different Labeling Functions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions. We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class. In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, we give learners that are both sample- and computationally-efficient.",
        "subjects": [
            "cs.LG",
            "cs.DS",
            "stat.ML"
        ],
        "comment": "v2 included additional discussion on related work"
    },
    {
        "paper id": "2402.10450",
        "abstract url": "https://arxiv.org/abs/2402.10450",
        "title": "PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https://github.com/FrankZheng2022/PRISE.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10981",
        "abstract url": "https://arxiv.org/abs/2402.10981",
        "title": "Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we study the inference accuracy of the Resistive Random Access Memory (ReRAM) neuromorphic circuit due to stuck-at faults (stuck-on, stuck-off, and stuck at a certain resistive value). A simulation framework using Python is used to perform supervised machine learning (neural network with 3 hidden layers, 1 input layer, and 1 output layer) of handwritten digits and construct a corresponding fully analog neuromorphic circuit (4 synaptic arrays) simulated by Spectre. A generic 45nm Process Development Kit (PDK) was used. We study the difference in the inference accuracy degradation due to stuck-on and stuck-off defects. Various defect patterns are studied including circular, ring, row, column, and circular-complement defects. It is found that stuck-on and stuck-off defects have a similar effect on inference accuracy. However, it is also found that if there is a spatial defect variation across the columns, the inference accuracy may be degraded significantly. We also propose a machine learning (ML) strategy to recover the inference accuracy degradation due to stuck-at faults. The inference accuracy is improved from 48% to 85% in a defective neuromorphic circuit.",
        "subjects": [
            "cs.AR",
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12260",
        "abstract url": "https://arxiv.org/abs/2402.12260",
        "title": "Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper considers minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles. We consider non-orthogonal multi-modal information dissemination, which is based on superposed message transmission from RSU and successive interference cancellation (SIC) at vehicles. The formulated problem is a multi-objective mixed-integer nonlinear programming problem; thus, a Pareto-optimal front is very challenging to obtain. First, we leverage the weighted-sum approach to decompose the multi-objective problem into a set of multiple single-objective sub-problems corresponding to each predefined objective preference weight. Then, we develop a hybrid deep Q-network (DQN)-deep deterministic policy gradient (DDPG) model to solve each optimization sub-problem respective to predefined objective-preference weight. The DQN optimizes the decoding order, while the DDPG solves the continuous power allocation. The model needs to be retrained for each sub-problem. We then present a two-stage meta-multi-objective reinforcement learning solution to estimate the Pareto front with a few fine-tuning update steps without retraining the model for each sub-problem. Simulation results illustrate the efficacy of the proposed solutions compared to the existing benchmarks and that the meta-multi-objective reinforcement learning model estimates a high-quality Pareto frontier with reduced training time.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.15521",
        "abstract url": "https://arxiv.org/abs/2402.15521",
        "title": "HKD-SHO: A hybrid smart home system based on knowledge-based and data-driven services",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "A smart home is realized by setting up various services. Several methods have been proposed to create smart home services, which can be divided into knowledge-based and data-driven approaches. However, knowledge-based approaches usually require manual input from the inhabitant, which can be complicated if the physical phenomena of the concerned environment states are complex, and the inhabitant does not know how to adjust related actuators to achieve the target values of the states monitored by services. Moreover, machine learning-based data-driven approaches that we are interested in are like black boxes and cannot show the inhabitant in which situations certain services proposed certain actuators' states. To solve these problems, we propose a hybrid system called HKD-SHO (Hybrid Knowledge-based and Data-driven services based Smart HOme system), where knowledge-based and machine learning-based data-driven services are profitably integrated. The principal advantage is that it inherits the explicability of knowledge-based services and the dynamism of data-driven services. We compare HKD-SHO with several systems for creating dynamic smart home services, and the results show the better performance of HKD-SHO.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "keywords: Hybrid System, Knowledge Representation, Reinforcement Learning, Services, Smart Home"
    },
    {
        "paper id": "2402.17771",
        "abstract url": "https://arxiv.org/abs/2402.17771",
        "title": "Utilizing Machine Learning for Signal Classification and Noise Reduction in Amateur Radio",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the realm of amateur radio, the effective classification of signals and the mitigation of noise play crucial roles in ensuring reliable communication. Traditional methods for signal classification and noise reduction often rely on manual intervention and predefined thresholds, which can be labor-intensive and less adaptable to dynamic radio environments. In this paper, we explore the application of machine learning techniques for signal classification and noise reduction in amateur radio operations. We investigate the feasibility and effectiveness of employing supervised and unsupervised learning algorithms to automatically differentiate between desired signals and unwanted interference, as well as to reduce the impact of noise on received transmissions. Experimental results demonstrate the potential of machine learning approaches to enhance the efficiency and robustness of amateur radio communication systems, paving the way for more intelligent and adaptive radio solutions in the amateur radio community.",
        "subjects": [
            "eess.SP",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.15394",
        "abstract url": "https://arxiv.org/abs/2403.15394",
        "title": "\"Model Cards for Model Reporting\" in 2024: Reclassifying Category of Ethical Considerations in Terms of Trustworthiness and Risk Management",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "In 2019, the paper entitled \"Model Cards for Model Reporting\" introduced a new tool for documenting model performance and encouraged the practice of transparent reporting for a defined list of categories. One of the categories detailed in that paper is ethical considerations, which includes the subcategories of data, human life, mitigations, risks and harms, and use cases. We propose to reclassify this category in the original model card due to the recent maturing of the field known as trustworthy AI, a term which analyzes whether the algorithmic properties of the model indicate that the AI system is deserving of trust from its stakeholders. In our examination of trustworthy AI, we highlight three respected organizations - the European Commission's High-Level Expert Group on AI, the OECD, and the U.S.-based NIST - that have written guidelines on various aspects of trustworthy AI. These recent publications converge on numerous characteristics of the term, including accountability, explainability, fairness, privacy, reliability, robustness, safety, security, and transparency, while recognizing that the implementation of trustworthy AI varies by context. Our reclassification of the original model-card category known as ethical considerations involves a two-step process: 1) adding a new category known as trustworthiness, where the subcategories will be derived from the discussion of trustworthy AI in our paper, and 2) maintaining the subcategories of ethical considerations under a renamed category known as risk environment and risk management, a title which we believe better captures today's understanding of the essence of these topics. We hope that this reclassification will further the goals of the original paper and continue to prompt those releasing trained models to accompany these models with documentation that will assist in the evaluation of their algorithmic properties.",
        "subjects": [
            "cs.CY",
            "cs.LG"
        ],
        "comment": "14 pages, 2 figures, submitted to ACM Conference on Fairness, Accountability, and Transparency 2024 (ACM FAccT '24)"
    },
    {
        "paper id": "2404.03662",
        "abstract url": "https://arxiv.org/abs/2404.03662",
        "title": "X-lifecycle Learning for Cloud Incident Management using LLMs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Incident management for large cloud services is a complex and tedious process and requires significant amount of manual efforts from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root causing and mitigating of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) created opportunities to automatically generate contextual recommendations to the OCEs assisting them to quickly identify and mitigate critical issues. However, existing research typically takes a silo-ed view for solving a certain task in incident management by leveraging data from a single stage of SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying ontology of service monitors used for automatically detecting incidents. By leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over State-of-The-Art methods.",
        "subjects": [
            "cs.NI",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09712",
        "abstract url": "https://arxiv.org/abs/2402.09712",
        "title": "Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09801",
        "abstract url": "https://arxiv.org/abs/2402.09801",
        "title": "EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09812",
        "abstract url": "https://arxiv.org/abs/2402.09812",
        "title": "DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The objective of text-to-image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods representing the reference concepts using unique text embeddings often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model. To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target prompts. Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios. Intensive analyses demonstrate the effectiveness of our approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page is available at https://ku-cvlab.github.io/DreamMatcher/"
    },
    {
        "paper id": "2402.09821",
        "abstract url": "https://arxiv.org/abs/2402.09821",
        "title": "Diffusion Models for Audio Restoration",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "With the development of audio playback devices and fast data transmission, the demand for high sound quality is rising, for both entertainment and communications. In this quest for better sound quality, challenges emerge from distortions and interferences originating at the recording side or caused by an imperfect transmission pipeline. To address this problem, audio restoration methods aim to recover clean sound signals from the corrupted input data. We present here audio restoration algorithms based on diffusion models, with a focus on speech enhancement and music restoration tasks. Traditional approaches, often grounded in handcrafted rules and statistical heuristics, have shaped our understanding of audio signals. In the past decades, there has been a notable shift towards data-driven methods that exploit the modeling capabilities of deep neural networks (DNNs). Deep generative models, and among them diffusion models, have emerged as powerful techniques for learning complex data distributions. However, relying solely on DNN-based learning approaches carries the risk of reducing interpretability, particularly when employing end-to-end models. Nonetheless, data-driven approaches allow more flexibility in comparison to statistical model-based frameworks whose performance depends on distributional and statistical assumptions that can be difficult to guarantee. Here, we aim to show that diffusion models can combine the best of both worlds and offer the opportunity to design audio restoration algorithms with a good degree of interpretability and a remarkable performance in terms of sound quality.",
        "subjects": [
            "eess.AS",
            "cs.LG",
            "cs.SD"
        ],
        "comment": "Full paper invited to the IEEE Signal Processing Magazine Special Issue \"Model-based and Data-Driven Audio Signal Processing\""
    },
    {
        "paper id": "2402.09874",
        "abstract url": "https://arxiv.org/abs/2402.09874",
        "title": "Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Adversarial attacks represent a substantial challenge in Natural Language Processing (NLP). This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of Transformer-based models under adversarial attacks. In the evaluation phase, we assess the susceptibility of three Transformer configurations, encoder-decoder, encoder-only, and decoder-only setups, to adversarial attacks of escalating complexity across datasets containing offensive language and misinformation. Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively. Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks. The resilience-enhancement phase employs adversarial training, integrating pre-camouflaged and dynamically altered data. This approach effectively reduces the performance drop in encoder-only models to an average of 5% in offensive language detection and 2% in misinformation detection tasks. Decoder-only models, occasionally exceeding original performance, limit the performance drop to 7% and 2% in the respective tasks. Although not surpassing the original performance, Encoder-decoder models can reduce the drop to an average of 6% and 2% respectively. Results suggest a trade-off between performance and robustness, with some models maintaining similar performance while gaining robustness. Our study and adversarial training techniques have been incorporated into an open-source tool for generating camouflaged datasets. However, methodology effectiveness depends on the specific camouflage technique and data encountered, emphasizing the need for continued exploration.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "19 pages, 8 figures, 5 tables"
    },
    {
        "paper id": "2402.09911",
        "abstract url": "https://arxiv.org/abs/2402.09911",
        "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions. For precise questions, we observe a minimum accuracy improvement of 7.5. Moreover, there is also demonstration that this framework exhibits generalizability across different KG sources. In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09941",
        "abstract url": "https://arxiv.org/abs/2402.09941",
        "title": "FedLion: Faster Adaptive Federated Optimization with Fewer Communication",
        "rating": "0",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training. To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into the FL framework. Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. Last but not least, this work also includes a novel theoretical analysis, showcasing that FedLion attains faster convergence rate than established FL algorithms like FedAvg.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "ICASSP 2024"
    },
    {
        "paper id": "2402.09966",
        "abstract url": "https://arxiv.org/abs/2402.09966",
        "title": "Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Subject-driven text-to-image diffusion models empower users to tailor the model to new concepts absent in the pre-training dataset using a few sample images. However, prevalent subject-driven models primarily rely on single-concept input images, facing challenges in specifying the target concept when dealing with multi-concept input images. To this end, we introduce a textual localized text-to-image model (Texual Localization) to handle multi-concept input images. During fine-tuning, our method incorporates a novel cross-attention guidance to decompose multiple concepts, establishing distinct connections between the visual representation of the target concept and the identifier token in the text prompt. Experimental results reveal that our method outperforms or performs comparably to the baseline models in terms of image fidelity and image-text alignment on multi-concept input images. In comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85% higher in single-concept and multi-concept generation, respectively. Notably, our method generates cross-attention maps consistent with the target concept in the generated images, a capability absent in existing models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10009",
        "abstract url": "https://arxiv.org/abs/2402.10009",
        "title": "Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples and code can be found on our examples page in https://hilamanor.github.io/AudioEditing/ .",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": "Updating consistent notations; Examples and code available in https://hilamanor.github.io/AudioEditing/"
    },
    {
        "paper id": "2402.10051",
        "abstract url": "https://arxiv.org/abs/2402.10051",
        "title": "SwissNYF: Tool Grounded LLM Agents for Black Box Setting",
        "rating": "0",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments. Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis. Therefore, we harness the program synthesis capabilities of LLMs to strategize tool usage in black-box settings, ensuring solutions are verified prior to implementation. We introduce TOPGUN, an ingeniously crafted approach leveraging program synthesis for black box tool planning. Accompanied by SwissNYF, a comprehensive suite that integrates black-box algorithms for planning and verification tasks, addressing the aforementioned challenges and enhancing the versatility and effectiveness of LLMs in complex API interactions. The public code for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10058",
        "abstract url": "https://arxiv.org/abs/2402.10058",
        "title": "Towards Safer Large Language Models through Machine Unlearning",
        "rating": "0",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages in total"
    },
    {
        "paper id": "2402.10107",
        "abstract url": "https://arxiv.org/abs/2402.10107",
        "title": "Quantized Embedding Vectors for Controllable Diffusion Language Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generation tasks, and more stable intermediate latent variables are obtained, which naturally brings in an accelerated convergence as well as better controllability. Additionally, the adaption fine-tuning method is employed to reduce tunable weights. Experimental results on five challenging fine-grained control tasks demonstrate that QE-CDLM compares favorably to existing methods in terms of quality and feasibility, achieving better perplexity and lightweight fine-tuning.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10110",
        "abstract url": "https://arxiv.org/abs/2402.10110",
        "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning",
        "rating": "0",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs. Our codes, models, and data will be released at https://github.com/tianyi-lab/Reflection_Tuning.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10176",
        "abstract url": "https://arxiv.org/abs/2402.10176",
        "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
        "rating": "0",
        "keywords": [
            [
                "synthesizing"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Data and models are available at https://huggingface.co/collections/nvidia/openmath-65c5619de2ba059be0775014"
    },
    {
        "paper id": "2402.10184",
        "abstract url": "https://arxiv.org/abs/2402.10184",
        "title": "Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. We mitigate such incompatibility through the design of dataset information structures during reward modeling, and introduce the Induced Bayesian Network (IBN), the first theory of reward generalization capable of generating substantial verified predictions on large language models (LLMs). Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and LLM behavior. Then, based on this framework, we introduce the IBN to analyze generalization in the reward modeling stage of RLHF. Drawing from random graph theory and causal analysis, it enables empirically grounded derivation of generalization error bounds, a key improvement over classical theories of generalization. Finally, an insight from our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines in conventional RLHF methods. With IBN, we derive that in complex contexts with limited data, the tree-based reward model (RM), trained on a tree-structured preference dataset, induces up to $\u0398(\\log n/\\log\\log n)$ times less variance than the baseline, where $n$ is the dataset size. As validation, we demonstrate that on three NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines. It shows that alignment performance can be gained for free via the design of dataset information structure, without the need for any other changes.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10207",
        "abstract url": "https://arxiv.org/abs/2402.10207",
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "20 pages, 12 figures, 6 tables"
    },
    {
        "paper id": "2402.10210",
        "abstract url": "https://arxiv.org/abs/2402.10210",
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\" and \"loser\" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "stat.ML"
        ],
        "comment": "28 pages, 8 figures, 10 tables"
    },
    {
        "paper id": "2402.10294",
        "abstract url": "https://arxiv.org/abs/2402.10294",
        "title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing",
        "rating": "0",
        "keywords": [
            [
                "Video Editing"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user's footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated LAVE's effectiveness. The results also shed light on user perceptions of the proposed LLM-assisted editing paradigm and its impact on users' creativity and sense of co-creation. Based on these findings, we propose design implications to inform the future development of agent-assisted content editing.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "comment": "Paper accepted to the ACM Conference on Intelligent User Interfaces (ACM IUI) 2024"
    },
    {
        "paper id": "2402.10342",
        "abstract url": "https://arxiv.org/abs/2402.10342",
        "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level elliptical potential analysis technique used to infer reward function parameters when comparison queries rather than reward observations are used. We provide and analyze algorithms in two settings: linear and neural function approximation, PG-RLHF and NN-PG-RLHF, respectively.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10357",
        "abstract url": "https://arxiv.org/abs/2402.10357",
        "title": "Efficient Sampling on Riemannian Manifolds via Langevin MCMC",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We study the task of efficiently sampling from a Gibbs distribution $d \u03c0^* = e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin MCMC; this algorithm involves computing exponential maps in random Gaussian directions and is efficiently implementable in practice. The key to our analysis of Langevin MCMC is a bound on the discretization error of the geometric Euler-Murayama scheme, assuming $\\nabla h$ is Lipschitz and $M$ has bounded sectional curvature. Our error bound matches the error of Euclidean Euler-Murayama in terms of its stepsize dependence. Combined with a contraction guarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling, we prove that the Langevin MCMC iterates lie within $\u03b5$-Wasserstein distance of $\u03c0^*$ after $\\tilde{O}(\u03b5^{-2})$ steps, which matches the iteration complexity for Euclidean Langevin MCMC. Our results apply in general settings where $h$ can be nonconvex and $M$ can have negative Ricci curvature. Under additional assumptions that the Riemannian curvature tensor has bounded derivatives, and that $\u03c0^*$ satisfies a $CD(\\cdot,\\infty)$ condition, we analyze the stochastic gradient version of Langevin MCMC, and bound its iteration complexity by $\\tilde{O}(\u03b5^{-2})$ as well.",
        "subjects": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.CO",
            "stat.ML"
        ],
        "comment": "This is an old paper from NeurIPS 2022. arXiv admin note: text overlap with arXiv:2204.13665"
    },
    {
        "paper id": "2402.10376",
        "abstract url": "https://arxiv.org/abs/2402.10376",
        "title": "Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)",
        "rating": "0",
        "keywords": [
            [
                "model editing"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representations, maintaining equivalent downstream performance while significantly improving their interpretability. We also demonstrate several use cases of SpLiCE representations including detecting spurious correlations, model editing, and quantifying semantic shifts in datasets.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "17 pages, 8 figures, Code is provided at https://github.com/AI4LIFE-GROUP/SpLiCE"
    },
    {
        "paper id": "2402.10380",
        "abstract url": "https://arxiv.org/abs/2402.10380",
        "title": "Subgraph-level Universal Prompt Tuning",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In the evolving landscape of machine learning, the adaptation of pre-trained models through prompt tuning has become increasingly prominent. This trend is particularly observable in the graph domain, where diverse pre-training strategies present unique challenges in developing effective prompt-based tuning methods for graph neural networks. Previous approaches have been limited, focusing on specialized prompting functions tailored to models with edge prediction pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space. This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple prompts to fully grasp the complex contexts found in graphs remains an open question, necessitating further investigation. Addressing this challenge, our work introduces the Subgraph-level Universal Prompt Tuning (SUPT) approach, focusing on the detailed context within subgraphs. In SUPT, prompt features are assigned at the subgraph-level, preserving the method's universal capability. This requires extremely fewer tuning parameters than fine-tuning-based methods, outperforming them in 42 out of 45 full-shot scenario experiments with an average improvement of over 2.5%. In few-shot scenarios, it excels in 41 out of 45 experiments, achieving an average performance increase of more than 6.6%.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10404",
        "abstract url": "https://arxiv.org/abs/2402.10404",
        "title": "Explaining generative diffusion models via visual analysis for interpretable decision-making process",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models have demonstrated remarkable performance in generation tasks. Nevertheless, explaining the diffusion process remains challenging due to it being a sequence of denoising noisy images that are difficult for experts to interpret. To address this issue, we propose the three research questions to interpret the diffusion process from the perspective of the visual concepts generated by the model and the region where the model attends in each time step. We devise tools for visualizing the diffusion process and answering the aforementioned research questions to render the diffusion process human-understandable. We show how the output is progressively generated in the diffusion process by explaining the level of denoising and highlighting relationships to foundational visual concepts at each time step through the results of experiments with various visual analyses using the tools. Throughout the training of the diffusion model, the model learns diverse visual concepts corresponding to each time-step, enabling the model to predict varying levels of visual concepts at different stages. We substantiate our tools using Area Under Cover (AUC) score, correlation quantification, and cross-attention mapping. Our findings provide insights into the diffusion process and pave the way for further research into explainable diffusion mechanisms.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "22 pages, published in Expert Systems with Applications"
    },
    {
        "paper id": "2404.03676",
        "abstract url": "https://arxiv.org/abs/2404.03676",
        "title": "Neural Information Organizing and Processing -- Neural Machines",
        "rating": "0",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The informational synthesis of neural structures, processes, parameters and characteristics that allow a unified description and modeling as neural machines of natural and artificial neural systems is presented. The general informational parameters as the global quantitative measure of the neural systems computing potential as absolute and relative neural power were proposed. Neural information organizing and processing follows the way in which nature manages neural information by developing functions, functionalities and circuits related to different internal or peripheral components and also to the whole system through a non-deterministic memorization, fragmentation and aggregation of afferent and efferent information, deep neural information processing representing multiple alternations of fragmentation and aggregation stages. The relevant neural characteristics were integrated into a neural machine type model that incorporates unitary also peripheral or interface components as the central ones. The proposed approach allows overcoming the technical constraints in artificial computational implementations of neural information processes and also provides a more relevant description of natural ones.",
        "subjects": [
            "cs.NE",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09715",
        "abstract url": "https://arxiv.org/abs/2402.09715",
        "title": "DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data. Cloud service providers further embrace Federated Learning as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data. Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines. Existing privacy budget scheduling studies prioritize either efficiency or fairness individually. In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and fairness. We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics. A sequential allocation mechanism is then designed using the Lagrange multiplier method and effective greedy heuristics. We theoretically prove that DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and Weak Strategy Proofness. We also theoretically prove the existence of a fairness-efficiency tradeoff in privacy budgeting. Extensive experiments demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an average efficiency improvement of $1.44\\times \\sim 3.49 \\times$, and an average fairness improvement of $1.37\\times \\sim 24.32 \\times$.",
        "subjects": [
            "cs.DC",
            "cs.CR",
            "cs.LG"
        ],
        "comment": "Accepted by IEEE International Conference on Computer Communications (INFOCOM '24)"
    },
    {
        "paper id": "2402.09728",
        "abstract url": "https://arxiv.org/abs/2402.09728",
        "title": "AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "SMS phishing, also known as \"smishing\", is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages. In recent past, we have also observed a rapid advancement of conversational generative AI chatbot services (e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large language models (LLMs). These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks. In this paper, we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns. To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing. Thus, we believe this study is the first of its kind to shed light on this emerging cybersecurity threat. We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing generative AI-based chatbot services by crafting prompt injection attacks to create newer smishing campaigns. We also discuss some future research directions and guidelines to protect the abuse of generative AI-based services and safeguard users from smishing attacks.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "6 pages, 12 figures, published in ISDFS 2024"
    },
    {
        "paper id": "2402.09729",
        "abstract url": "https://arxiv.org/abs/2402.09729",
        "title": "Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System",
        "rating": "-0.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system. We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions. Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers. To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT. Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training. The design of prompts integrating user-environment cues and user-preferred allocation improves the model's adaptability to various user environments during online execution.",
        "subjects": [
            "cs.AI",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09765",
        "abstract url": "https://arxiv.org/abs/2402.09765",
        "title": "Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows",
        "rating": "-0.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper introduces a reinforcement learning approach to optimize the Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on reducing travel costs in goods delivery. We develop a novel SVRP formulation that accounts for uncertain travel costs and demands, alongside specific customer time windows. An attention-based neural network trained through reinforcement learning is employed to minimize routing costs. Our approach addresses a gap in SVRP research, which traditionally relies on heuristic methods, by leveraging machine learning. The model outperforms the Ant-Colony Optimization algorithm, achieving a 1.73% reduction in travel costs. It uniquely integrates external information, demonstrating robustness in diverse environments, making it a valuable benchmark for future SVRP studies and industry application.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09780",
        "abstract url": "https://arxiv.org/abs/2402.09780",
        "title": "TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems",
        "rating": "-0.5",
        "keywords": [
            [
                "synthesize"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Continuous Learning (CL) paradigm consists of continuously evolving the parameters of the Deep Neural Network (DNN) model to progressively learn to perform new tasks without reducing the performance on previous tasks, i.e., avoiding the so-called catastrophic forgetting. However, the DNN parameter update in CL-based autonomous systems is extremely resource-hungry. The existing DNN accelerators cannot be directly employed in CL because they only support the execution of the forward propagation. Only a few prior architectures execute the backpropagation and weight update, but they lack the control and management for CL. Towards this, we design a hardware architecture, TinyCL, to perform CL on resource-constrained autonomous systems. It consists of a processing unit that executes both forward and backward propagation, and a control unit that manages memory-based CL workload. To minimize the memory accesses, the sliding window of the convolutional layer moves in a snake-like fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at runtime to execute different operations. As per our knowledge, our proposed TinyCL represents the first hardware accelerator that executes CL on autonomous systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS technology node with the conventional ASIC design flow. It executes 1 epoch of training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while 1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s, thus achieving a 58 x speedup, consuming 86 mW in a 4.74 mm2 die.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09830",
        "abstract url": "https://arxiv.org/abs/2402.09830",
        "title": "Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data",
        "rating": "-0.5",
        "keywords": [
            [
                "Anomaly detection"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods. GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection. The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative Adversarial network (GANs) algorithm to enhance the security of the transaction process.The study demonstrates the potential of GANs in enhancing transaction security through deep learning techniques.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09834",
        "abstract url": "https://arxiv.org/abs/2402.09834",
        "title": "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09916",
        "abstract url": "https://arxiv.org/abs/2402.09916",
        "title": "BUSTER: a \"BUSiness Transaction Entity Recognition\" dataset",
        "rating": "-0.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023), Industry Track"
    },
    {
        "paper id": "2402.09919",
        "abstract url": "https://arxiv.org/abs/2402.09919",
        "title": "Road Graph Generator: Mapping roads at construction sites from GPS data",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We propose a new method for inferring roads from GPS trajectories to map construction sites. This task presents a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which significantly diverge from typical vehicular traffic on established roads. Our proposed method first identifies intersections in the road network that serve as critical decision points, and then connects them with edges to produce a graph, which can subsequently be used for planning and task-allocation. We demonstrate the approach by mapping roads at a real-life construction site in Norway. The method is validated on four increasingly complex segments of the map. In our tests, the method achieved perfect accuracy in detecting intersections and inferring roads in data with no or low noise, while its performance was reduced in map areas with significant noise and consistently missing GPS updates.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "18 pages, 4 figures, 3 tables"
    },
    {
        "paper id": "2402.09949",
        "abstract url": "https://arxiv.org/abs/2402.09949",
        "title": "Multi-word Tokenization for Sequence Compression",
        "rating": "-0.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)"
    },
    {
        "paper id": "2402.09970",
        "abstract url": "https://arxiv.org/abs/2402.09970",
        "title": "Accelerating Parallel Sampling of Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampling algorithms such as DDIM and DDPM by a factor of 4~14 times. Notably, when applying ParaTAA with 100 steps DDIM for Stable Diffusion, a widely-used text-to-image diffusion model, it can produce the same images as the sequential sampling in only 7 inference steps.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10011",
        "abstract url": "https://arxiv.org/abs/2402.10011",
        "title": "Clifford Group Equivariant Simplicial Message Passing Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to outperform both equivariant and simplicial graph neural networks on a variety of geometric tasks.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10028",
        "abstract url": "https://arxiv.org/abs/2402.10028",
        "title": "Diffusion Models Meet Contextual Bandits with Large Action Spaces",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "26 pages, 5 figures"
    },
    {
        "paper id": "2402.10050",
        "abstract url": "https://arxiv.org/abs/2402.10050",
        "title": "On-Demand Myoelectric Control Using Wake Gestures to Eliminate False Activations During Activities of Daily Living",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "While myoelectric control has recently become a focus of increased research as a possible flexible hands-free input modality, current control approaches are prone to inadvertent false activations in real-world conditions. In this work, a novel myoelectric control paradigm -- on-demand myoelectric control -- is proposed, designed, and evaluated, to reduce the number of unrelated muscle movements that are incorrectly interpreted as input gestures . By leveraging the concept of wake gestures, users were able to switch between a dedicated control mode and a sleep mode, effectively eliminating inadvertent activations during activities of daily living (ADLs). The feasibility of wake gestures was demonstrated in this work through two online ubiquitous EMG control tasks with varying difficulty levels; dismissing an alarm and controlling a robot. The proposed control scheme was able to appropriately ignore almost all non-targeted muscular inputs during ADLs (>99.9%) while maintaining sufficient sensitivity for reliable mode switching during intentional wake gesture elicitation. These results highlight the potential of wake gestures as a critical step towards enabling ubiquitous myoelectric control-based on-demand input for a wide range of applications.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10053",
        "abstract url": "https://arxiv.org/abs/2402.10053",
        "title": "Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. Specifically, we show how the popular Friedkin--Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others. We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a $(1+\\varepsilon)$-approximate solution in time $\\tilde{O}(m\\sqrt{n} \\lg(1/\\varepsilon))$, where $m$ is the number of edges in the graph and $n$ is the number of vertices. We also present an algorithm that provably computes an $\\varepsilon$-approximation of our model in near-linear time. We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27\\,000 nodes (the previously largest publicly available dataset contains less than 550 nodes).",
        "subjects": [
            "cs.SI"
        ],
        "comment": "To appear at The WebConf 2024"
    },
    {
        "paper id": "2402.10065",
        "abstract url": "https://arxiv.org/abs/2402.10065",
        "title": "How Much Does Each Datapoint Leak Your Privacy? Quantifying the Per-datum Membership Leakage",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the per-datum Membership Inference Attacks (MIAs), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy. First, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it. Then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the Mahalanobis distance between the target datum and the data-generating distribution. We further assess the effect of two privacy defences, i.e. adding Gaussian noise and sub-sampling. We quantify exactly how both of them decrease the per-datum membership leakage. Our analysis builds on a novel proof technique that combines an Edgeworth expansion of the likelihood ratio test and a Lindeberg-Feller central limit theorem. Our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies different canary selection strategies used in the privacy auditing literature. Finally, our experiments demonstrate the impacts of the leakage score, the sub-sampling ratio and the noise scale on the per-datum membership leakage as indicated by the theory.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "math.ST",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10074",
        "abstract url": "https://arxiv.org/abs/2402.10074",
        "title": "Class-Balanced and Reinforced Active Learning on Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) have demonstrated significant success in various applications, such as node classification, link prediction, and graph classification. Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a lower cost. However, most existing algorithms for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. GNNs trained with class-imbalanced labeled data are susceptible to bias toward majority classes, and the lower performance of minority classes may lead to a decline in overall performance. To tackle this issue, we propose a novel class-balanced and reinforced active learning framework for GNNs, namely, GCBR. It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes. GCBR designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. The reinforcement learning algorithm Advantage Actor-Critic (A2C) is employed to learn an optimal policy stably and efficiently. We further upgrade GCBR to GCBR++ by introducing a punishment mechanism to obtain a more class-balanced labeled set. Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed approaches, achieving superior performance over state-of-the-art baselines.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10095",
        "abstract url": "https://arxiv.org/abs/2402.10095",
        "title": "Classification Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal denoiser for white Gaussian noise can be expressed in terms of the gradient of a cross-entropy-optimal classifier for predicting the noise level. As we illustrate, CDM achieves better denoising results compared to DDM, and leads to at least comparable FID in image generation. CDM is also capable of highly efficient one-step exact likelihood estimation, achieving state-of-the-art results among methods that use a single step. Code is available on the project's webpage in https://shaharYadin.github.io/CDM/ .",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10097",
        "abstract url": "https://arxiv.org/abs/2402.10097",
        "title": "Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while considering both the data and system heterogeneity. Experimental results under practical wireless network settings with real-world prototype demonstrate that the proposed independent sampling scheme substantially outperforms the current best sampling schemes under various training models and datasets.",
        "subjects": [
            "cs.LG",
            "cs.NI"
        ],
        "comment": "6 pages, 5 figures, accepted for publication in IEEE International Conference on Communications (ICC)"
    },
    {
        "paper id": "2402.10145",
        "abstract url": "https://arxiv.org/abs/2402.10145",
        "title": "A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data. In this research, the authors employ a secured Federated Learning method with an additional layer of privacy and proposes a method for addressing the non-IID challenge. Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy. The experimental approach assesses the performance of the federated deep learning model with differential privacy using both IID and non-IID data. In each experiment, the Federated Learning process improves the average performance metrics of the deep neural network, even in the case of non-IID data.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10191",
        "abstract url": "https://arxiv.org/abs/2402.10191",
        "title": "FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) is a distributed learning paradigm that facilitates collaborative training of a shared global model across devices while keeping data localized. The deployment of FL in numerous real-world applications faces delays, primarily due to the prevalent reliance on supervised tasks. Generating detailed labels at edge devices, if feasible, is demanding, given resource constraints and the imperative for continuous data updates. In addressing these challenges, solutions such as federated semi-supervised learning (FSSL), which relies on unlabeled clients' data and a limited amount of labeled data on the server, become pivotal. In this paper, we propose FedAnchor, an innovative FSSL method that introduces a unique double-head structure, called anchor head, paired with the classification head trained exclusively on labeled anchor data on the server. The anchor head is empowered with a newly designed label contrastive loss based on the cosine similarity metric. Our approach mitigates the confirmation bias and overfitting issues associated with pseudo-labeling techniques based on high-confidence model prediction samples. Extensive experiments on CIFAR10/100 and SVHN datasets demonstrate that our method outperforms the state-of-the-art method by a significant margin in terms of convergence rate and model accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10310",
        "abstract url": "https://arxiv.org/abs/2402.10310",
        "title": "Interpretable Generative Adversarial Imitation Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "GAN",
                "synthesis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Imitation learning methods have demonstrated considerable success in teaching autonomous systems complex tasks through expert demonstrations. However, a limitation of these methods is their lack of interpretability, particularly in understanding the specific task the learning agent aims to accomplish. In this paper, we propose a novel imitation learning method that combines Signal Temporal Logic (STL) inference and control synthesis, enabling the explicit representation of the task as an STL formula. This approach not only provides a clear understanding of the task but also allows for the incorporation of human knowledge and adaptation to new scenarios through manual adjustments of the STL formulae. Additionally, we employ a Generative Adversarial Network (GAN)-inspired training approach for both the inference and the control policy, effectively narrowing the gap between the expert and learned policies. The effectiveness of our algorithm is demonstrated through two case studies, showcasing its practical applicability and adaptability.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": "Submitted to L4DC 2024 (under review)"
    },
    {
        "paper id": "2402.10340",
        "abstract url": "https://arxiv.org/abs/2402.10340",
        "title": "On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities",
        "rating": "-0.5",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "Robotics",
                "robot",
                "navigation"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these attacks. Our empirical findings reveal a striking vulnerability of LLM/VLM-robot integrated systems: simple adversarial attacks can significantly undermine the effectiveness of LLM/VLM-robot integrated systems. Specifically, our data demonstrate an average performance deterioration of 21.2% under prompt attacks and a more alarming 30.2% under perception attacks. These results underscore the critical need for robust countermeasures to ensure the safe and reliable deployment of the advanced LLM/VLM-based robotic systems.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10397",
        "abstract url": "https://arxiv.org/abs/2402.10397",
        "title": "LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs",
        "rating": "-0.5",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "System logs are some of the most important information for the maintenance of software systems, which have become larger and more complex in recent years. The goal of log-based anomaly detection is to automatically detect system anomalies by analyzing the large number of logs generated in a short period of time, which is a critical challenge in the real world. Previous studies have used a log parser to extract templates from unstructured log data and detect anomalies on the basis of patterns of the template occurrences. These methods have limitations for logs with unknown templates. Furthermore, since most log anomalies are known to be point anomalies rather than contextual anomalies, detection methods based on occurrence patterns can cause unnecessary delays in detection. In this paper, we propose LogELECTRA, a new log anomaly detection model that analyzes a single line of log messages more deeply on the basis of self-supervised anomaly detection. LogELECTRA specializes in detecting log anomalies as point anomalies by applying ELECTRA, a natural language processing model, to analyze the semantics of a single line of log messages. LogELECTRA outperformed existing state-of-the-art methods in experiments on the public benchmark log datasets BGL, Sprit, and Thunderbird.",
        "subjects": [
            "cs.LG",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10974",
        "abstract url": "https://arxiv.org/abs/2402.10974",
        "title": "On the Cross-Dataset Generalization of Machine Learning for Network Intrusion Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Network Intrusion Detection Systems (NIDS) are a fundamental tool in cybersecurity. Their ability to generalize across diverse networks is a critical factor in their effectiveness and a prerequisite for real-world applications. In this study, we conduct a comprehensive analysis on the generalization of machine-learning-based NIDS through an extensive experimentation in a cross-dataset framework. We employ four machine learning classifiers and utilize four datasets acquired from different networks: CIC-IDS-2017, CSE-CIC-IDS2018, LycoS-IDS2017, and LycoS-Unicas-IDS2018. Notably, the last dataset is a novel contribution, where we apply corrections based on LycoS-IDS2017 to the well-known CSE-CIC-IDS2018 dataset. The results show nearly perfect classification performance when the models are trained and tested on the same dataset. However, when training and testing the models in a cross-dataset fashion, the classification accuracy is largely commensurate with random chance except for a few combinations of attacks and datasets. We employ data visualization techniques in order to provide valuable insights on the patterns in the data. Our analysis unveils the presence of anomalies in the data that directly hinder the classifiers capability to generalize the learned knowledge to new scenarios. This study enhances our comprehension of the generalization capabilities of machine-learning-based NIDS, highlighting the significance of acknowledging data heterogeneity.",
        "subjects": [
            "cs.CR",
            "cs.LG",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10977",
        "abstract url": "https://arxiv.org/abs/2402.10977",
        "title": "Generative AI and Process Systems Engineering: The Next Frontier",
        "rating": "-0.5",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could potentially advance PSE methodologies, providing insights and prospects for each area. Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and benchmarks, and trust and safety, thereby deepening the discourse on effective GenAI integration into systems analysis, design, optimization, operations, monitoring, and control. This paper provides a guide for future research focused on the applications of emerging GenAI in PSE.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SY",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10985",
        "abstract url": "https://arxiv.org/abs/2402.10985",
        "title": "Leveraging AI Planning For Detecting Cloud Security Vulnerabilities",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. Alongside their growing popularity, concerns related to their security vulnerabilities leading to data breaches and sophisticated attacks such as ransomware are growing. To address these, first, we propose a generic framework to express relations between different cloud objects such as users, datastores, security roles, to model access control policies in cloud systems. Access control misconfigurations are often the primary driver for cloud attacks. Second, we develop a PDDL model for detecting security vulnerabilities which can for example lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner can then generate attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations. Our system can identify a broad range of security vulnerabilities, which state-of-the-art industry tools cannot detect.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18923",
        "abstract url": "https://arxiv.org/abs/2403.18923",
        "title": "Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptively evolve to select relevant feature interactions within populations for different lake types and tasks. These models are not only capable of undergoing crossover and mutation mechanisms within intra-populations but also, albeit infrequently, engage in inter-population crossover. The second stage involves refining these models by retraining them with real observed labels. We have tested the performance of our NGCE strategy in predicting daily DO concentrations across a wide range of lakes in the Midwest, USA. These lakes, varying in size, depth, and trophic status, represent a broad spectrum of north temperate lakes. Our findings demonstrate that NGCE not only produces accurate predictions with few observed labels but also, through gene maps of models, reveals sophisticated phenological patterns of different lakes.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09722",
        "abstract url": "https://arxiv.org/abs/2402.09722",
        "title": "Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "6-DoF"
            ],
            [
                "robotics"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "Accepted to ICRA 2024. The first two authors contributed equally"
    },
    {
        "paper id": "2402.09742",
        "abstract url": "https://arxiv.org/abs/2402.09742",
        "title": "AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "healthcare",
                "Diagnosis",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. In our experiments, we validate the reliability of AI Hospital. The results not only explore the feasibility of apply LLMs in clinical consultation but also confirm the effectiveness of the dispute resolution focused collaboration method.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "https://github.com/LibertFan/AI_Hospital"
    },
    {
        "paper id": "2402.09747",
        "abstract url": "https://arxiv.org/abs/2402.09747",
        "title": "Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "health",
                "Disease",
                "Retinal"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Retinal optical coherence tomography (OCT) images provide crucial insights into the health of the posterior ocular segment. Therefore, the advancement of automated image analysis methods is imperative to equip clinicians and researchers with quantitative data, thereby facilitating informed decision-making. The application of deep learning (DL)-based approaches has gained extensive traction for executing these analysis tasks, demonstrating remarkable performance compared to labor-intensive manual analyses. However, the acquisition of Retinal OCT images often presents challenges stemming from privacy concerns and the resource-intensive labeling procedures, which contradicts the prevailing notion that DL models necessitate substantial data volumes for achieving superior performance. Moreover, limitations in available computational resources constrain the progress of high-performance medical artificial intelligence, particularly in less developed regions and countries. This paper introduces a novel ensemble learning mechanism designed for recognizing retinal diseases under limited resources (e.g., data, computation). The mechanism leverages insights from multiple pre-trained models, facilitating the transfer and adaptation of their knowledge to Retinal OCT images. This approach establishes a robust model even when confronted with limited labeled data, eliminating the need for an extensive array of parameters, as required in learning from scratch. Comprehensive experimentation on real-world datasets demonstrates that the proposed approach can achieve superior performance in recognizing Retinal OCT images, even when dealing with exceedingly restricted labeled datasets. Furthermore, this method obviates the necessity of learning extensive-scale parameters, making it well-suited for deployment in low-resource scenarios.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Ongoing work"
    },
    {
        "paper id": "2402.09755",
        "abstract url": "https://arxiv.org/abs/2402.09755",
        "title": "Data Smoothing Filling Method based on ScRNA-Seq Data Zero-Value Identification",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Single-cell RNA sequencing (scRNA-seq) determines RNA expression at single-cell resolution. It provides a powerful tool for studying immunity, regulation, and other life activities of cells. However, due to the limitations of the sequencing technique, the scRNA-seq data are represented with sparsity, whichcontains missing gene values, i.e., zero values, called dropout. Therefore, it is necessary to impute missing values before analyzing scRNA-seq data. However, existing imputation computation methods often only focus on the identification of technical zeros or imputing all zeros based on cell similarity. This study proposes a new method (SFAG) to reconstruct the gene expression relationship matrix by usinggraph regularization technology to preserve the high-dimensional manifold information of the data, andto mine the relationship between genes and cells in the data, and then uses a method of averaging the clustering results to fill in the identified technical zeros. Experimental results show that SFAGcan helpimprove downstream analysis and reconstruct cell trajectory",
        "subjects": [
            "q-bio.GN",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09756",
        "abstract url": "https://arxiv.org/abs/2402.09756",
        "title": "Mixture of Experts for Network Optimization: A Large Language Model-enabled Approach",
        "rating": "-1",
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "Optimizing various wireless user tasks poses a significant challenge for networking systems because of the expanding range of user requirements. Despite advancements in Deep Reinforcement Learning (DRL), the need for customized optimization tasks for individual users complicates developing and applying numerous DRL models, leading to substantial computation resource and energy consumption and can lead to inconsistent outcomes. To address this issue, we propose a novel approach utilizing a Mixture of Experts (MoE) framework, augmented with Large Language Models (LLMs), to analyze user objectives and constraints effectively, select specialized DRL experts, and weigh each decision from the participating experts. Specifically, we develop a gate network to oversee the expert models, allowing a collective of experts to tackle a wide array of new tasks. Furthermore, we innovatively substitute the traditional gate network with an LLM, leveraging its advanced reasoning capabilities to manage expert model selection for joint decisions. Our proposed method reduces the need to train new DRL models for each unique optimization problem, decreasing energy consumption and AI model implementation costs. The LLM-enabled MoE approach is validated through a general maze navigation task and a specific network service provider utility maximization task, demonstrating its effectiveness and practical applicability in optimizing complex networking systems.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09786",
        "abstract url": "https://arxiv.org/abs/2402.09786",
        "title": "Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model",
        "rating": "-1",
        "keywords": [
            [
                "Pathological"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY",
                "cs.CV"
            ]
        ],
        "abstract": "Generative adversarial networks (GANs) generate photorealistic faces that are often indistinguishable by humans from real faces. While biases in machine learning models are often assumed to be due to biases in training data, we find pathological internal color and luminance biases in the discriminator of a pre-trained StyleGAN3-r model that are not explicable by the training data. We also find that the discriminator systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine axes common in research on stereotyping in social psychology.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09804",
        "abstract url": "https://arxiv.org/abs/2402.09804",
        "title": "Coevolution of relationship and interaction in cooperative dynamical multiplex networks",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "While actors in a population can interact with anyone else freely, social relations significantly influence our inclination towards particular individuals. The consequence of such interactions, however, may also form the intensity of our relations established earlier. These dynamical processes are captured via a coevolutionary model staged in multiplex networks with two distinct layers. In a so-called relationship layer the weights of edges among players may change in time as a consequence of games played in the alternative interaction layer. As an reasonable assumption, bilateral cooperation confirms while mutual defection weakens these weight factors. Importantly, the fitness of a player, which basically determines the success of a strategy imitation, depends not only on the payoff collected from interactions, but also on the individual relationship index calculated from the mentioned weight factors of related edges. Within the framework of weak prisoner's dilemma situation we explore the potential outcomes of the mentioned coevolutionary process where we assume different topologies for relationship layer. We find that higher average degree of the relationship graph is more beneficial to maintain cooperation in regular graphs, but the randomness of links could be a decisive factor in harsh situations. Surprisingly, a stronger coupling between relationship index and fitness discourage the evolution of cooperation by weakening the direct consequence of a strategy change. To complete our study we also monitor how the distribution of relationship index vary and detect a strong relation between its polarization and the general cooperation level.",
        "subjects": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.GT",
            "nlin.PS"
        ],
        "comment": "11 two-column pages, 6 figures, to be published in Chaos"
    },
    {
        "paper id": "2402.09833",
        "abstract url": "https://arxiv.org/abs/2402.09833",
        "title": "R4: rapid reproducible robotics research open hardware control system",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "A key component of any robot is the interface between ROS2 software and physical motors. New robots often use arbitrary, messy mixtures of closed and open motor drivers and error-prone physical mountings, wiring, and connectors to interface them. There is a need for a standardizing OSH component to abstract this complexity, as Arduino did for interfacing to smaller components. We present a OSH printed circuit board to solve this problem once and for all. On the high-level side, it interfaces to Arduino Giga -- acting as an unusually large and robust shield -- and thus to existing open source ROS software stacks. On the lower-level side, it interfaces to existing emerging standard open hardware including OSH motor drivers and relays, which can already be used to drive fully open hardware wheeled and arm robots. This enables the creation of a family of standardized, fully open hardware, fully reproducible, research platforms.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09835",
        "abstract url": "https://arxiv.org/abs/2402.09835",
        "title": "Parameterized Algorithms for Steiner Forest in Bounded Width Graphs",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In this paper we reassess the parameterized complexity and approximability of the well-studied Steiner Forest problem in several graph classes of bounded width. The problem takes an edge-weighted graph and pairs of vertices as input, and the aim is to find a minimum cost subgraph in which each given vertex pair lies in the same connected component. It is known that this problem is APX-hard in general, and NP-hard on graphs of treewidth 3, treedepth 4, and feedback vertex set size 2. However, Bateni, Hajiaghayi and Marx [JACM, 2011] gave an approximation scheme with a runtime of $n^{O(\\frac{k^2}{\\varepsilon})}$ on graphs of treewidth $k$. Our main result is a much faster efficient parameterized approximation scheme (EPAS) with a runtime of $2^{O(\\frac{k^2}{\\varepsilon} \\log \\frac{k^2}{\\varepsilon})} \\cdot n^{O(1)}$. If $k$ instead is the vertex cover number of the input graph, we show how to compute the optimum solution in $2^{O(k \\log k)} \\cdot n^{O(1)}$ time, and we also prove that this runtime dependence on $k$ is asymptotically best possible, under ETH. Furthermore, if $k$ is the size of a feedback edge set, then we obtain a faster $2^{O(k)} \\cdot n^{O(1)}$ time algorithm, which again cannot be improved under ETH.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09845",
        "abstract url": "https://arxiv.org/abs/2402.09845",
        "title": "JustSTART: How to Find an RSA Authentication Bypass on Xilinx UltraScale(+) with Fuzzing",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Fuzzing is a well-established technique in the software domain to uncover bugs and vulnerabilities. Yet, applications of fuzzing for security vulnerabilities in hardware systems are scarce, as principal reasons are requirements for design information access (HDL source code). Moreover, observation of internal hardware state during runtime is typically an ineffective information source, as its documentation is often not publicly available. In addition, such observation during runtime is also inefficient due to bandwidth-limited analysis interfaces (JTAG, and minimal introspection of internal modules). In this work, we investigate fuzzing for 7-Series and UltraScale(+) FPGA configuration engines, the control plane governing the (secure) bitstream configuration within the FPGA. Our goal is to examine the effectiveness of fuzzing to analyze and document the opaque inner workings of FPGA configuration engines, with a primary emphasis on identifying security vulnerabilities. Using only the publicly available chip and dispersed documentation, we first design and implement ConFuzz, an advanced FPGA configuration engine fuzzing and rapid prototyping framework. Based on our detailed understanding of the bitstream file format, we then systematically define 3 novel key fuzzing strategies for Xilinx configuration engines. Moreover, our strategies are executed through mutational structure-aware fuzzers and incorporate various novel custom-tailored, FPGA-specific optimizations. Our evaluation reveals previously undocumented behavior within the configuration engine, including critical findings such as system crashes leading to unresponsive states of the FPGA. In addition, our investigations not only lead to the rediscovery of the starbleed attack but also uncover JustSTART (CVE-2023-20570), capable of circumventing RSA authentication for Xilinx UltraScale(+). Note that we also discuss countermeasures.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09867",
        "abstract url": "https://arxiv.org/abs/2402.09867",
        "title": "Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs",
        "rating": "-1",
        "keywords": [
            [
                "EEG"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders. These applications require long and continuous processing to generate feasible results. However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases. Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications. Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs. However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space. In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detection on the real-world embedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial evaluation of power-performance-accuracy trade-offs of EEG applications at different approximation, power, and performance levels to provide insights into the disciplined tuning of approximation in EEG applications on embedded platforms.",
        "subjects": [
            "eess.SP",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.PF"
        ],
        "comment": "7 pages, 10 figures"
    },
    {
        "paper id": "2402.09971",
        "abstract url": "https://arxiv.org/abs/2402.09971",
        "title": "Parameterized Vertex Integrity Revisited",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Vertex integrity is a graph parameter that measures the connectivity of a graph. Informally, its meaning is that a graph has small vertex integrity if it has a small separator whose removal disconnects the graph into connected components which are themselves also small. Graphs with low vertex integrity are extremely structured; this renders many hard problems tractable and has recently attracted interest in this notion from the parameterized complexity community. In this paper we revisit the NP-complete problem of computing the vertex integrity of a given graph from the point of view of structural parameterizations. We present a number of new results, which also answer some recently posed open questions from the literature. Specifically: We show that unweighted vertex integrity is W[1]-hard parameterized by treedepth; we show that the problem remains W[1]-hard if we parameterize by feedback edge set size (via a reduction from a Bin Packing variant which may be of independent interest); and complementing this we show that the problem is FPT by max-leaf number. Furthermore, for weighted vertex integrity, we show that the problem admits a single-exponential FPT algorithm parameterized by vertex cover or by modular width, the latter result improving upon a previous algorithm which required weights to be polynomially bounded.",
        "subjects": [
            "cs.DS",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09973",
        "abstract url": "https://arxiv.org/abs/2402.09973",
        "title": "TSTEM: A Cognitive Platform for Collecting Cyber Threat Intelligence in the Wild",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks. While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild. To address this gap, the study describes the implementation of an efficient and well-performing platform capable of processing compute-intensive data pipelines based on the cloud computing paradigm for real-time detection, collecting, and sharing CTI from different online sources. We developed a prototype platform (TSTEM), a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, ELK, Kafka, and MLOps to autonomously search, extract, and index IOCs in the wild. Moreover, the provisioning, monitoring, and management of the TSTEM platform are achieved through infrastructure as a code (IaC). Custom focus crawlers collect web content, which is then processed by a first-level classifier to identify potential indicators of compromise (IOCs). If deemed relevant, the content advances to a second level of extraction for further examination. Throughout this process, state-of-the-art NLP models are utilized for classification and entity extraction, enhancing the overall IOC extraction methodology. Our experimental results indicate that these models exhibit high accuracy (exceeding 98%) in the classification and extraction tasks, achieving this performance within a time frame of less than a minute. The effectiveness of our system can be attributed to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification of relevant information with low false positives.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09975",
        "abstract url": "https://arxiv.org/abs/2402.09975",
        "title": "Current and future roles of artificial intelligence in retinopathy of prematurity",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "disease",
                "clinical",
                "retinal"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness. While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting. Recent advancements in deep learning (DL), especially convolutional neural networks (CNNs), have significantly improved ROP detection and classification. The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential. This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain. Based on 89 original studies in this field (out of 1487 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions. AI holds great promise for improving ROP management. This review explores AI's potential in ROP detection, classification, diagnosis, and prognosis.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "28 pages, 8 figures, 2 tables, 235 references, 1 supplementary table"
    },
    {
        "paper id": "2402.09990",
        "abstract url": "https://arxiv.org/abs/2402.09990",
        "title": "TIAViz: A Browser-based Visualization Tool for Computational Pathology Models",
        "rating": "-1",
        "keywords": [
            [
                "healthcare",
                "diagnosis",
                "whole slide"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Digital pathology has gained significant traction in modern healthcare systems. This shift from optical microscopes to digital imagery brings with it the potential for improved diagnosis, efficiency, and the integration of AI tools into the pathologists workflow. A critical aspect of this is visualization. Throughout the development of a machine learning (ML) model in digital pathology, it is crucial to have flexible, openly available tools to visualize models, from their outputs and predictions to the underlying annotations and images used to train or test a model. We introduce TIAViz, a Python-based visualization tool built into TIAToolbox which allows flexible, interactive, fully zoomable overlay of a wide variety of information onto whole slide images, including graphs, heatmaps, segmentations, annotations and other WSIs. The UI is browser-based, allowing use either locally, on a remote machine, or on a server to provide publicly available demos. This tool is open source and is made available at: https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation (pip install tiatoolbox) and conda as part of TIAToolbox.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "Application note to be submitted to bioinformatics"
    },
    {
        "paper id": "2402.10010",
        "abstract url": "https://arxiv.org/abs/2402.10010",
        "title": "Enhancing signal detectability in learning-based CT reconstruction with a model observer inspired loss function",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "CT"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Deep neural networks used for reconstructing sparse-view CT data are typically trained by minimizing a pixel-wise mean-squared error or similar loss function over a set of training images. However, networks trained with such pixel-wise losses are prone to wipe out small, low-contrast features that are critical for screening and diagnosis. To remedy this issue, we introduce a novel training loss inspired by the model observer framework to enhance the detectability of weak signals in the reconstructions. We evaluate our approach on the reconstruction of synthetic sparse-view breast CT data, and demonstrate an improvement in signal detectability with the proposed loss.",
        "subjects": [
            "physics.med-ph",
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10014",
        "abstract url": "https://arxiv.org/abs/2402.10014",
        "title": "Trajectory Guidance: Enhanced Remote Driving of highly-automated Vehicles",
        "rating": "-1",
        "keywords": [
            [
                "Trajectory"
            ]
        ],
        "abstract": "Despite the rapid technological progress, autonomous vehicles still face a wide range of complex driving situations that require human intervention. Teleoperation technology offers a versatile and effective way to address these challenges. The following work puts existing ideas into a modern context and introduces a novel technical implementation of the trajectory guidance teleoperation concept. The presented system was developed within a high-fidelity simulation environment and experimentally validated, demonstrating a realistic ride-hailing mission with prototype autonomous vehicles and onboard passengers. The results indicate that the proposed concept can be a viable alternative to the existing remote driving options, offering a promising way to enhance teleoperation technology and improve overall operation safety.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to 2024 IEEE Intelligent Vehicles Symposium (IV)"
    },
    {
        "paper id": "2402.10022",
        "abstract url": "https://arxiv.org/abs/2402.10022",
        "title": "Reproducing, Extending, and Analyzing Naming Experiments",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "Naming is very important in software development, as names are often the only vehicle of meaning about what the code is intended to do. A recent study on how developers choose names collected the names given by different developers for the same objects. This enabled a study of these names' diversity and structure, and the construction of a model of how names are created. We reproduce different parts of this study in three independent experiments. Importantly, we employ methodological variations rather than striving of an exact replication. When the same results are obtained this then boosts our confidence in their validity by demonstrating that they do not depend on the methodology. Our results indeed corroborate those of the original study in terms of the diversity of names, the low probability of two developers choosing the same name, and the finding that experienced developers tend to use slightly longer names than inexperienced students. We explain name diversity by performing a new analysis of the names, classifying the concepts represented in them as universal (agreed upon), alternative (reflecting divergent views on a topic), or optional (reflecting divergent opinions on whether to include this concept at all). This classification enables new research directions concerning the considerations involved in naming decisions. We also show that explicitly using the model proposed in the original study to guide naming leads to the creation of better names, whereas the simpler approach of just asking participants to use longer and more detailed names does not.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "35 pages with 10 figures and 6 tables"
    },
    {
        "paper id": "2402.10030",
        "abstract url": "https://arxiv.org/abs/2402.10030",
        "title": "Systematic Literature Review of EM-SCA Attacks on Encryption",
        "rating": "-1",
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "Cryptography is vital for data security, but cryptographic algorithms can still be vulnerable to side-channel attacks (SCAs), physical assaults exploiting power consumption and EM radiation. SCAs pose a significant threat to cryptographic integrity, compromising device keys. While literature on SCAs focuses on real-world devices, the rise of sophisticated devices necessitates fresh approaches. Electromagnetic side-channel analysis (EM-SCA) gathers information by monitoring EM radiation, capable of retrieving encryption keys and detecting malicious activity. This study evaluates EM-SCA's impact on encryption across scenarios and explores its role in digital forensics and law enforcement. Addressing encryption susceptibility to EM-SCA can empower forensic investigators in overcoming encryption challenges, maintaining their crucial role in law enforcement. Additionally, the paper defines EM-SCA's current state in attacking encryption, highlighting vulnerable and resistant encryption algorithms and devices, and promising EM-SCA approaches. This study offers a comprehensive analysis of EM-SCA in law enforcement and digital forensics, suggesting avenues for further research.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10044",
        "abstract url": "https://arxiv.org/abs/2402.10044",
        "title": "On the Domain Generalizability of RF Fingerprints Through Multifractal Dimension Representation",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a possible method for enabling secure device identification and authentication. Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data collected under one domain performs badly when tested on data collected under a different domain. Some examples of a domain change include varying the location or environment of the device and varying the time or day of the data collection. In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable. We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ (in-phase and quadrature) signals, and we evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices. Our experimental results show that the proposed VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using IQ data samples.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "2023 IEEE Conference on Communications and Network Security (CNS). arXiv admin note: substantial text overlap with arXiv:2308.07925"
    },
    {
        "paper id": "2402.10055",
        "abstract url": "https://arxiv.org/abs/2402.10055",
        "title": "Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network",
        "rating": "-1",
        "keywords": [
            [
                "retinal"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism. In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH). Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed morphological quantification, and yet remains a challenging task. Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN). Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching. We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and dynamic probability map. We achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD) compared to literature, and outperformed baseline U-net. We have demonstrated tracing individual vessel trees from fundus images, and simultaneously retain the vessel hierarchy information. InSegNN paves a way for any subsequent morphological analysis of vascular morphology in relation to retinal diseases.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10066",
        "abstract url": "https://arxiv.org/abs/2402.10066",
        "title": "NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "CT",
                "cancer"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture. The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis. In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively. Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated. In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time. The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image. A decision is made once the total accumulated evidence surpasses a specific threshold. Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects. The results are noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even with approximately 60% less training data on this demanding and small dataset.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "cs.NE",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10071",
        "abstract url": "https://arxiv.org/abs/2402.10071",
        "title": "Approximate Message Passing-Enhanced Graph Neural Network for OTFS Data Detection",
        "rating": "-1",
        "keywords": [
            [
                "GNN",
                "Graph"
            ]
        ],
        "abstract": "Orthogonal time frequency space (OTFS) modulation has emerged as a promising solution to support high-mobility wireless communications, for which, cost-effective data detectors are critical. Although graph neural network (GNN)-based data detectors can achieve decent detection accuracy at reasonable computational cost, they fail to best harness prior information of transmitted data. To further minimize the data detection error of OTFS systems, this letter develops an AMP-GNN-based detector, leveraging the approximate message passing (AMP) algorithm to iteratively improve the symbol estimates of a GNN. Given the inter-Doppler interference (IDI) symbols incur substantial computational overhead to the constructed GNN, learning-based IDI approximation is implemented to sustain low detection complexity. Simulation results demonstrate a remarkable bit error rate (BER) performance achieved by the proposed AMP-GNN-based detector compared to existing baselines. Meanwhile, the proposed IDI approximation scheme avoids a large amount of computations with negligible BER degradation.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "8 pages, 7 figures, and 3 tables. Part of this article was submitted to IEEE for possible publication"
    },
    {
        "paper id": "2402.10109",
        "abstract url": "https://arxiv.org/abs/2402.10109",
        "title": "Towards Reducing Diagnostic Errors with Interpretable Risk Prediction",
        "rating": "-1",
        "keywords": [
            [
                "Health",
                "diagnosis"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual \"true\" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10124",
        "abstract url": "https://arxiv.org/abs/2402.10124",
        "title": "A Blob Method for Mean Field Control With Terminal Constraints",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "In the present work, we develop a novel particle method for a general class of mean field control problems, with source and terminal constraints. Specific examples of the problems we consider include the dynamic formulation of the p-Wasserstein metric, optimal transport around an obstacle, and measure transport subject to acceleration controls. Unlike existing numerical approaches, our particle method is meshfree and does not require global knowledge of an underlying cost function or of the terminal constraint. A key feature of our approach is a novel way of enforcing the terminal constraint via a soft, nonlocal approximation, inspired by recent work on blob methods for diffusion equations. We prove convergence of our particle approximation to solutions of the continuum mean-field control problem in the sense of Gamma-convergence. A byproduct of our result is an extension of existing discrete-to-continuum convergence results for mean field control problems to more general state and measure costs, as arise when modeling transport around obstacles, and more general constraint sets, including controllable linear time invariant systems. Finally, we conclude by implementing our method numerically and using it to compute solutions the example problems discussed above. We conduct a detailed numerical investigation of the convergence properties of our method, as well as its behavior in sampling applications and for approximation of optimal transport maps.",
        "subjects": [
            "math.OC",
            "eess.SY",
            "math.AP",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10128",
        "abstract url": "https://arxiv.org/abs/2402.10128",
        "title": "GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "synthesis"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes. It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website https://abdullahamdi.com/ges .",
        "subjects": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "comment": "preprint"
    },
    {
        "paper id": "2402.10130",
        "abstract url": "https://arxiv.org/abs/2402.10130",
        "title": "Is Continual Learning Ready for Real-world Challenges?",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "robotics"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS. We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in realistic settings. Our paper aims to initiate a paradigm shift, advocating for the adoption of continual learning methods through new experimental protocols that better emulate real-world conditions to facilitate breakthroughs in the field.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10138",
        "abstract url": "https://arxiv.org/abs/2402.10138",
        "title": "Transaction Capacity, Security and Latency in Blockchains",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "We analyze how secure a block is after the block becomes k-deep, i.e., security-latency, for Nakamoto consensus under an exponential network delay model. We give parameter regimes for which transactions are safe when sufficiently deep in the chain. We compare our results for Nakamoto consensus under bounded network delay models and obtain analogous bounds for safety violation threshold. Next, modeling the blockchain system as a batch service queue with exponential network delay, we connect the security-latency analysis to sustainable transaction rate of the queue system. As our model assumes exponential network delay, batch service queue models give a meaningful trade-off between transaction capacity, security and latency. As adversary can attack the queue service to hamper the service process, we consider two different attacks for adversary. In an extreme scenario, we modify the selfish-mining attack for this purpose and consider its effect on the sustainable transaction rate of the queue.",
        "subjects": [
            "cs.CR",
            "cs.DC",
            "cs.DM",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10153",
        "abstract url": "https://arxiv.org/abs/2402.10153",
        "title": "Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients",
        "rating": "-1",
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "4 pages, 3 figures, and 2 tables, conference paper"
    },
    {
        "paper id": "2402.10161",
        "abstract url": "https://arxiv.org/abs/2402.10161",
        "title": "Robotic Exploration using Generalized Behavioral Entropy",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This work presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term ``Behavioral entropy'', which builds on Prelec's probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.",
        "subjects": [
            "cs.RO",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10196",
        "abstract url": "https://arxiv.org/abs/2402.10196",
        "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Language agents powered by large language models (LLMs) have seen exploding development. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capability to connect LLMs to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful automation technology is emerging. However, new automation technologies come with new safety risks, especially for intricate systems like language agents. There is a surprisingly large gap between the speed and scale of their development and deployment and our understanding of their safety risks. Are we building a house of cards? In this position paper, we present the first systematic effort in mapping adversarial attacks against language agents. We first present a unified conceptual framework for agents with three major components: Perception, Brain, and Action. Under this framework, we present a comprehensive discussion and propose 12 potential attack scenarios against different components of an agent, covering different attack strategies (e.g., input manipulation, adversarial demonstrations, jailbreaking, backdoors). We also draw connections to successful attack strategies previously applied to LLMs. We emphasize the urgency to gain a thorough understanding of language agent risks before their widespread deployment.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10208",
        "abstract url": "https://arxiv.org/abs/2402.10208",
        "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CR",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10259",
        "abstract url": "https://arxiv.org/abs/2402.10259",
        "title": "GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from only 4 views and significantly outperforming previous state-of-the-art methods.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Project page: https://gaussianobject.github.io/"
    },
    {
        "paper id": "2402.10323",
        "abstract url": "https://arxiv.org/abs/2402.10323",
        "title": "A Computationally Efficient Learning-Based Model Predictive Control for Multirotors under Aerodynamic Disturbances",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Neglecting complex aerodynamic effects hinders high-speed yet high-precision multirotor autonomy. In this paper, we present a computationally efficient learning-based model predictive controller that simultaneously optimizes a trajectory that can be tracked within the physical limits (on thrust and orientation) of the multirotor system despite unknown aerodynamic forces and adapts the control input. To do this, we leverage the well-known differential flatness property of multirotors, which allows us to transform their nonlinear dynamics into a linear model. The main limitation of current flatness-based planning and control approaches is that they often neglect dynamic feasibility. This is because these constraints are nonlinear as a result of the mapping between the input, i.e., multirotor thrust, and the flat state. In our approach, we learn a novel representation of the drag forces by learning the mapping from the flat state to the multirotor thrust vector (in a world frame) as a Gaussian Process (GP). Our proposed approach leverages the properties of GPs to develop a convex optimal controller that can be iteratively solved as a second-order cone program (SOCP). In simulation experiments, our proposed approach outperforms related model predictive controllers that do not account for aerodynamic effects on trajectory feasibility, leading to a reduction of up to 55% in absolute tracking error.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10334",
        "abstract url": "https://arxiv.org/abs/2402.10334",
        "title": "HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting",
        "rating": "-1",
        "keywords": [
            [
                "RGBD",
                "Depth"
            ],
            [
                "GAN",
                "Inpainting"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Inpainting involves filling in missing pixels or areas in an image, a crucial technique employed in Mixed Reality environments for various applications, particularly in Diminished Reality (DR) where content is removed from a user's visual environment. Existing methods rely on digital replacement techniques which necessitate multiple cameras and incur high costs. AR devices and smartphones use ToF depth sensors to capture scene depth maps aligned with RGB images. Despite speed and affordability, ToF cameras create imperfect depth maps with missing pixels. To address the above challenges, we propose Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked edge and segmentation label images respectively, while CombinedRGBD-GAN combines their latent representation outputs and performs RGB and Depth inpainting. Edge images and particularly segmentation label images as auxiliary inputs significantly enhance inpainting performance by complementary context and hierarchical optimization. We believe we make the first attempt to incorporate label images into inpainting process.Unlike previous approaches requiring multiple sequential models and separate outputs, our work operates in an end-to-end manner, training all three models simultaneously and hierarchically. Specifically, EdgeGAN and LabelGAN are first optimized separately and further optimized inside CombinedRGBD-GAN to enhance inpainting quality. Experiments demonstrate that HI-GAN works seamlessly and achieves overall superior performance compared with existing approaches.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10343",
        "abstract url": "https://arxiv.org/abs/2402.10343",
        "title": "Non-adaptive Bellman-Ford: Yen's improvement is optimal",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The Bellman-Ford algorithm for single-source shortest paths repeatedly updates tentative distances in an operation called relaxing an edge. In several important applications a non-adaptive (oblivious) implementation is preferred, which means fixing the entire sequence of relaxations upfront, independent of the edge-weights. In a dense graph on $n$ vertices, the algorithm in its standard form performs $(1 + o(1))n^3$ relaxations. An improvement by Yen from 1970 reduces the number of relaxations by a factor of two. We show that no further constant-factor improvements are possible, and every non-adaptive deterministic algorithm based on relaxations must perform $(\\frac{1}{2} - o(1))n^3$ steps. This improves an earlier lower bound of Eppstein of $(\\frac{1}{6} - o(1))n^3$. Given that a non-adaptive randomized variant of Bellman-Ford with at most $(\\frac{1}{3} + o(1))n^3$ relaxations (with high probability) is known, our result implies a strict separation between deterministic and randomized strategies, answering an open question of Eppstein.",
        "subjects": [
            "cs.DS",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10372",
        "abstract url": "https://arxiv.org/abs/2402.10372",
        "title": "Sequential Manipulation of Deformable Linear Object Networks with Endpoint Pose Measurements using Adaptive Model Predictive Control",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Robotic manipulation of deformable linear objects (DLOs) is an active area of research, though emerging applications, like automotive wire harness installation, introduce constraints that have not been considered in prior work. Confined workspaces and limited visibility complicate prior assumptions of multi-robot manipulation and direct measurement of DLO configuration (state). This work focuses on single-arm manipulation of stiff DLOs (StDLOs) connected to form a DLO network (DLON), for which the measurements (output) are the endpoint poses of the DLON, which are subject to unknown dynamics during manipulation. To demonstrate feasibility of output-based control without state estimation, direct input-output dynamics are shown to exist by training neural network models on simulated trajectories. Output dynamics are then approximated with polynomials and found to contain well-known rigid body dynamics terms. A composite model consisting of a rigid body model and an online data-driven residual is developed, which predicts output dynamics more accurately than either model alone, and without prior experience with the system. An adaptive model predictive controller is developed with the composite model for DLON manipulation, which completes DLON installation tasks, both in simulation and with a physical automotive wire harness.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "Accepted to IEEE International Conference on Robotics and Automation - ICRA 2024. 7 pages. 4 figures"
    },
    {
        "paper id": "2402.10373",
        "abstract url": "https://arxiv.org/abs/2402.10373",
        "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
        "rating": "-1",
        "keywords": [
            [
                "BioMistral",
                "Medical",
                "health",
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10409",
        "abstract url": "https://arxiv.org/abs/2402.10409",
        "title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning",
        "rating": "-1",
        "keywords": [
            [
                "synthesize"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-strong generalization in the taxonomy classification task.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "TL;DR: We collected metadata about LLM surveys and developed a method for categorizing them into a taxonomy, indicating the superiority of graph representation learning over language models and revealing the efficacy of fine-tuning using weak labels"
    },
    {
        "paper id": "2402.10425",
        "abstract url": "https://arxiv.org/abs/2402.10425",
        "title": "DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision",
        "rating": "-1",
        "keywords": [
            [
                "CT",
                "clinical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Cochlear implants (CIs) are neural prosthetics used to treat patients with severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of the auditory nerve fiber (ANFs) can help audiologists improve the CI programming. These models require localization of the ANFs relative to surrounding anatomy and the CI. Localization is challenging because the ANFs are so small they are not directly visible in clinical imaging. In this work, we hypothesize the position of the ANFs can be accurately inferred from the location of the internal auditory canal (IAC), which has high contrast in CT, since the ANFs pass through this canal between the cochlea and the brain. Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC segmentation network. We create a single atlas in which the IAC and ANFs are pre-localized. Our network is trained to produce deformation fields (DFs) mapping coordinates from the atlas to new target volumes and that accurately segment the IAC. We hypothesize that DFs that accurately segment the IAC in target images will also facilitate accurate atlas-based localization of the ANFs. As opposed to VoxelMorph, which aims to produce DFs that accurately register the entire volume, our novel contribution is an entirely self-supervised training scheme that aims to produce DFs that accurately segment the target structure. This self-supervision is facilitated using a regional level set (LS) inspired loss function. We call our method Deep Atlas Based Segmentation using Level Sets (DABS-LS). Results show that DABS-LS outperforms VoxelMorph for IAC segmentation. Tests with publicly available datasets for trachea and kidney segmentation also show significant improvement in segmentation accuracy, demonstrating the generalizability of the method.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10433",
        "abstract url": "https://arxiv.org/abs/2402.10433",
        "title": "Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations",
        "rating": "-1",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "The protein dynamics are common and important for their biological functions and properties, the study of which usually involves time-consuming molecular dynamics (MD) simulations in silico. Recently, generative models has been leveraged as a surrogate sampler to obtain conformation ensembles with orders of magnitude faster and without requiring any simulation data (a \"zero-shot\" inference). However, being agnostic of the underlying energy landscape, the accuracy of such generative model may still be limited. In this work, we explore the few-shot setting of such pre-trained generative sampler which incorporates MD simulations in a tractable manner. Specifically, given a target protein of interest, we first acquire some seeding conformations from the pre-trained sampler followed by a number of physical simulations in parallel starting from these seeding samples. Then we fine-tuned the generative model using the simulation trajectories above to become a target-specific sampler. Experimental results demonstrated the superior performance of such few-shot conformation sampler at a tractable computational cost.",
        "subjects": [
            "q-bio.BM",
            "cs.LG",
            "q-bio.QM"
        ],
        "comment": "Published at the GEM workshop, ICLR 2024"
    },
    {
        "paper id": "2402.10441",
        "abstract url": "https://arxiv.org/abs/2402.10441",
        "title": "Barrier-Enhanced Homotopic Parallel Trajectory Optimization for Safety-Critical Autonomous Driving",
        "rating": "-1",
        "keywords": [
            [
                "Autonomous Driving",
                "Trajectory",
                "vehicle"
            ]
        ],
        "abstract": "Enforcing safety while preventing overly conservative behaviors is essential for autonomous vehicles to achieve high task performance. In this paper, we propose a barrier-enhanced homotopic parallel trajectory optimization (BHPTO) approach with over-relaxed alternating direction method of multipliers (ADMM) for real-time integrated decision-making and planning. To facilitate safety interactions between the ego vehicle (EV) and surrounding vehicles, a spatiotemporal safety module exhibiting bi-convexity is developed on the basis of barrier function. Varying barrier coefficients are adopted for different time steps in a planning horizon to account for the motion uncertainties of surrounding HVs and mitigate conservative behaviors. Additionally, we exploit the discrete characteristics of driving maneuvers to initialize nominal behavior-oriented free-end homotopic trajectories based on reachability analysis, and each trajectory is locally constrained to a specific driving maneuver while sharing the same task objectives. By leveraging the bi-convexity of the safety module and the kinematics of the EV, we formulate the BHPTO as a bi-convex optimization problem. Then constraint transcription and over-relaxed ADMM are employed to streamline the optimization process, such that multiple trajectories are generated in real time with feasibility guarantees. Through a series of experiments, the proposed development demonstrates improved task accuracy, stability, and consistency in various traffic scenarios using synthetic and real-world traffic datasets.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09710",
        "abstract url": "https://arxiv.org/abs/2402.09710",
        "title": "Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks",
        "rating": "-1.5",
        "keywords": [
            [
                "5G"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications. For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks. This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller. We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances. The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encrypt the data, following which, (ii) employ a custom Vision transformer (ViT) as the trained ML model that is capable of performing accurate inferences on such encrypted data. The paper offers a thorough analysis and comparisons with analogous convolutional neural networks (CNN) as well as deeper architectures (such as ResNet-50) as baselines. Our experiments showcase that the proposed approach significantly outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the percent accuracy and F1-Score respectively when operated on encrypted data. Though deeper ResNet-50 architecture is obtained as a slightly more accurate model, with an increase of 4.4%, the proposed approach boasts a reduction of parameters by 99.32%, and thus, offers a much-improved prediction time by nearly 60%.",
        "subjects": [
            "cs.CR",
            "cs.LG",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09730",
        "abstract url": "https://arxiv.org/abs/2402.09730",
        "title": "DOF: Accelerating High-order Differential Operators with Forward Propagation",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Solving partial differential equations (PDEs) efficiently is essential for analyzing complex physical systems. Recent advancements in leveraging deep learning for solving PDE have shown significant promise. However, machine learning methods, such as Physics-Informed Neural Networks (PINN), face challenges in handling high-order derivatives of neural network-parameterized functions. Inspired by Forward Laplacian, a recent method of accelerating Laplacian computation, we propose an efficient computational framework, Differential Operator with Forward-propagation (DOF), for calculating general second-order differential operators without losing any precision. We provide rigorous proof of the advantages of our method over existing methods, demonstrating two times improvement in efficiency and reduced memory consumption on any architectures. Empirical results illustrate that our method surpasses traditional automatic differentiation (AutoDiff) techniques, achieving 2x improvement on the MLP structure and nearly 20x improvement on the MLP with Jacobian sparsity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09761",
        "abstract url": "https://arxiv.org/abs/2402.09761",
        "title": "A Framework For Gait-Based User Demography Estimation Using Inertial Sensors",
        "rating": "-1.5",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Human gait has been shown to provide crucial motion cues for various applications. Recognizing patterns in human gait has been widely adopted in various application areas such as security, virtual reality gaming, medical rehabilitation, and ailment identification. Furthermore, wearable inertial sensors have been widely used for not only recording gait but also to predict users' demography. Machine Learning techniques such as deep learning, combined with inertial sensor signals, have shown promising results in recognizing patterns in human gait and estimate users' demography. However, the black-box nature of such deep learning models hinders the researchers from uncovering the reasons behind the model's predictions. Therefore, we propose leveraging deep learning and Layer-Wise Relevance Propagation (LRP) to identify the important variables that play a vital role in identifying the users' demography such as age and gender. To assess the efficacy of this approach we train a deep neural network model on a large sensor-based gait dataset consisting of 745 subjects to identify users' age and gender. Using LRP we identify the variables relevant for characterizing the gait patterns. Thus, we enable interpretation of non-linear ML models which are experts in identifying the users' demography based on inertial signals. We believe this approach can not only provide clinicians information about the gait parameters relevant to age and gender but also can be expanded to analyze and diagnose gait disorders.",
        "subjects": [
            "cs.HC",
            "cs.LG",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09782",
        "abstract url": "https://arxiv.org/abs/2402.09782",
        "title": "MC-DBN: A Deep Belief Network-Based Model for Modality Completion",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate monitoring domains. Comprehensive experiments showcase the model's capacity to bridge the semantic divide present in multi-modal data, subsequently enhancing its performance. The source code is available at: https://github.com/logan-0623/DBN-generate",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09784",
        "abstract url": "https://arxiv.org/abs/2402.09784",
        "title": "Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a month. To address these gaps, we propose a sequential recommendation model called TemProxRec, which includes contrastive learning and self-attention methods to consider temporal proximities both across and within user-item interactions. The proposed contrastive learning method learns representations of items selected in close temporal periods across different users to be close. Simultaneously, the proposed self-attention mechanism encodes temporal and positional contexts in a user sequence using both absolute and relative embeddings. This way, our TemProxRec accurately predicts the relevant items based on the user-item interactions within a specific timeframe. We validate this work through comprehensive experiments on TemProxRec, consistently outperforming existing models on benchmark datasets as well as showing the significance of considering the vertical and horizontal temporal proximities into sequential recommendation.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "10 pages, 9 figures"
    },
    {
        "paper id": "2402.09891",
        "abstract url": "https://arxiv.org/abs/2402.09891",
        "title": "Predictors from causal features do not generalize better to new domains",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. If the goal is to generalize to new domains, practitioners might as well train the best possible model on all available features.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "13 pages, 7 figures"
    },
    {
        "paper id": "2402.09897",
        "abstract url": "https://arxiv.org/abs/2402.09897",
        "title": "COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "The COVID-19 pandemic has had adverse effects on both physical and mental health. During this pandemic, numerous studies have focused on gaining insights into health-related perspectives from social media. In this study, our primary objective is to develop a machine learning-based web application for automatically classifying COVID-19-related discussions on social media. To achieve this, we label COVID-19-related Twitter data, provide benchmark classification results, and develop a web application. We collected data using the Twitter API and labeled a total of 6,667 tweets into five different classes: health risks, prevention, symptoms, transmission, and treatment. We extracted features using various feature extraction methods and applied them to seven different traditional machine learning algorithms, including Decision Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest Neighbour, Logistic Regression, and Linear SVC. Additionally, we used four deep learning algorithms: LSTM, CNN, RNN, and BERT, for classification. Overall, we achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning. The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing other traditional machine learning approaches. Our study not only contributes to the field of health-related data analysis but also provides a valuable resource in the form of a web-based tool for efficient data classification, which can aid in addressing public health challenges and increasing awareness during pandemics. We made the dataset and application publicly available, which can be downloaded from this link https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": "27 pages, 6 figures"
    },
    {
        "paper id": "2402.09948",
        "abstract url": "https://arxiv.org/abs/2402.09948",
        "title": "Neural 5G Indoor Localization with IMU Supervision",
        "rating": "-1.5",
        "keywords": [
            [
                "5G"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Radio signals are well suited for user localization because they are ubiquitous, can operate in the dark and maintain privacy. Many prior works learn mappings between channel state information (CSI) and position fully-supervised. However, that approach relies on position labels which are very expensive to acquire. In this work, this requirement is relaxed by using pseudo-labels during deployment, which are calculated from an inertial measurement unit (IMU). We propose practical algorithms for IMU double integration and training of the localization system. We show decimeter-level accuracy on simulated and challenging real data of 5G measurements. Our IMU-supervised method performs similarly to fully-supervised, but requires much less effort to deploy.",
        "subjects": [
            "eess.SP",
            "cs.LG"
        ],
        "comment": "IEEE GLOBECOM 2023"
    },
    {
        "paper id": "2402.09957",
        "abstract url": "https://arxiv.org/abs/2402.09957",
        "title": "On Designing Features for Condition Monitoring of Rotating Machines",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Various methods for designing input features have been proposed for fault recognition in rotating machines using one-dimensional raw sensor data. The available methods are complex, rely on empirical approaches, and may differ depending on the condition monitoring data used. Therefore, this article proposes a novel algorithm to design input features that unifies the feature extraction process for different time-series sensor data. This new insight for designing/extracting input features is obtained through the lens of histogram theory. The proposed algorithm extracts discriminative input features, which are suitable for a simple classifier to deep neural network-based classifiers. The designed input features are given as input to the classifier with end-to-end training in a single framework for machine conditions recognition. The proposed scheme has been validated through three real-time datasets: a) acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset. The real-time results and comparative study show the effectiveness of the proposed scheme for the prediction of the machine's health states.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10001",
        "abstract url": "https://arxiv.org/abs/2402.10001",
        "title": "Privacy Attacks in Decentralized Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some of the factors that affect the performance of the attack, namely the graph topology, the number of attackers, and their position in the graph.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10082",
        "abstract url": "https://arxiv.org/abs/2402.10082",
        "title": "FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments. Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence. The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks. Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handling sophisticated attacks without prior knowledge of the number of attackers. Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency. Consequently, malicious clients' weights are excluded. Our proposed approach was tested against various model poisoning attacks, demonstrating superior performance over state-of-the-art aggregation methods.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "14 pages, 9 figures, and 6 tables"
    },
    {
        "paper id": "2402.10083",
        "abstract url": "https://arxiv.org/abs/2402.10083",
        "title": "Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare",
                "clinical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest (87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%), LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4 evaluation demonstrated significant agreement with human clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80 respectively; while correlation based on Cohen Kappa was more modest at 0.50. Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the LLM-generated responses, which were appropriately identified by the GPT-4 evaluation. Conclusion: The notable clinical alignment of GPT-4 evaluation highlighted its potential to streamline the clinical evaluation of LLM chatbot responses to healthcare-related queries. By complementing the existing clinician-dependent manual grading, this efficient and automated evaluation could assist the validation of future developments in LLM applications for healthcare.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "13 Pages, 1 Figure, 8 Tables"
    },
    {
        "paper id": "2402.10172",
        "abstract url": "https://arxiv.org/abs/2402.10172",
        "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\\%$.",
        "subjects": [
            "cs.AI",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10198",
        "abstract url": "https://arxiv.org/abs/2402.10198",
        "title": "Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times fewer parameters. The code is available at https://github.com/romilbert/samformer.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "The first two authors contributed equally"
    },
    {
        "paper id": "2402.10206",
        "abstract url": "https://arxiv.org/abs/2402.10206",
        "title": "Ising on the Graph: Task-specific Graph Subsampling via the Ising Model",
        "rating": "-1.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10283",
        "abstract url": "https://arxiv.org/abs/2402.10283",
        "title": "Backdoor Attack against One-Class Sequential Anomaly Detection Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "Attack"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.IT"
        ],
        "comment": "This work is accepted by the PAKDD 2024. 12 pages"
    },
    {
        "paper id": "2402.10381",
        "abstract url": "https://arxiv.org/abs/2402.10381",
        "title": "UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledge that composes a variety of domain text pairs from multilingual mappings, entity relationships, and term explanation perspectives, respectively. In the multi-modal fusion phase, we novelly propose a user-aware multi-modal contribution measurement mechanism to weight multi-modal features dynamically according to user features at the interaction level and employ the DCN-V2 module to model bounded-degree multi-modal crosses effectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large real-world datasets, demonstrating substantial performance enhancements.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "Accepted by DASFAA 2024 Research track"
    },
    {
        "paper id": "2402.10388",
        "abstract url": "https://arxiv.org/abs/2402.10388",
        "title": "Improvising Age Verification Technologies in Canada: Technical, Regulatory and Social Dynamics",
        "rating": "-1.5",
        "keywords": [
            [
                "biometrics"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Age verification, which is a mandatory legal requirement for delivering certain age-appropriate services or products, has recently been emphasized around the globe to ensure online safety for children. The rapid advancement of artificial intelligence has facilitated the recent development of some cutting-edge age-verification technologies, particularly using biometrics. However, successful deployment and mass acceptance of these technologies are significantly dependent on the corresponding socio-economic and regulatory context. This paper reviews such key dynamics for improvising age-verification technologies in Canada. It is particularly essential for such technologies to be inclusive, transparent, adaptable, privacy-preserving, and secure. Effective collaboration between academia, government, and industry entities can help to meet the growing demands for age-verification services in Canada while maintaining a user-centric approach.",
        "subjects": [
            "cs.CY",
            "eess.SP"
        ],
        "comment": "Presented and accepted for publication in the 2023 IEEE International Humanitarian Technologies Conference (IEEE IHTC 2023), November 1 to 3, 2023, Cartagena, Colombia"
    },
    {
        "paper id": "2402.10393",
        "abstract url": "https://arxiv.org/abs/2402.10393",
        "title": "Darwin Turing Dawkins: Building a General Theory of Evolution",
        "rating": "-1.5",
        "keywords": [
            [
                "DNA"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Living things, computers, societies, and even books are part of a grand evolutionary struggle to survive. That struggle shapes nature, nations, religions, art, science, and you. What you think, feel, and do is determined by it. Darwinian evolution does not apply solely to the genes that are stored in DNA. Using the insights of Alan Turing and Richard Dawkins, we will see that it also applies to the memes we store in our brains and the information we store in our computers. The next time you run for president, fight a war, or just deal with the ordinary problems humans are heir to, perhaps this book will be of use. If you want to understand why and when you will die, or if you want to achieve greatness this book may help. If you are concerned about where the computer revolution is headed, this book may provide some answers.",
        "subjects": [
            "cs.GL",
            "cs.AI",
            "q-bio.PE"
        ],
        "comment": "247 pages"
    },
    {
        "paper id": "2402.10972",
        "abstract url": "https://arxiv.org/abs/2402.10972",
        "title": "Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "disease",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Prediction of symptomatic crises in chronic diseases allows to take decisions before the symptoms occur, such as the intake of drugs to avoid the symptoms or the activation of medical alarms. The prediction horizon is in this case an important parameter in order to fulfill the pharmacokinetics of medications, or the time response of medical services. This paper presents a study about the prediction limits of a chronic disease with symptomatic crises: the migraine. For that purpose, this work develops a methodology to build predictive migraine models and to improve these predictions beyond the limits of the initial models. The maximum prediction horizon is analyzed, and its dependency on the selected features is studied. A strategy for model selection is proposed to tackle the trade off between conservative but robust predictive models, with respect to less accurate predictions with higher horizons. The obtained results show a prediction horizon close to 40 minutes, which is in the time range of the drug pharmacokinetics. Experiments have been performed in a realistic scenario where input data have been acquired in an ambulatory clinical study by the deployment of a non-intrusive Wireless Body Sensor Network. Our results provide an effective methodology for the selection of the future horizon in the development of prediction algorithms for diseases experiencing symptomatic crises.",
        "subjects": [
            "q-bio.QM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10982",
        "abstract url": "https://arxiv.org/abs/2402.10982",
        "title": "mshw, a forecasting library to predict short-term electricity demand based on multiple seasonal Holt-Winters",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transmission system operators have a growing need for more accurate forecasting of electricity demand. Current electricity systems largely require demand forecasting so that the electricity market establishes electricity prices as well as the programming of production units. The companies that are part of the electrical system use exclusive software to obtain predictions, based on the use of time series and prediction tools, whether statistical or artificial intelligence. However, the most common form of prediction is based on hybrid models that use both technologies. In any case, it is software with a complicated structure, with a large number of associated variables and that requires a high computational load to make predictions. The predictions they can offer are not much better than those that simple models can offer. In this paper we present a MATLAB toolbox created for the prediction of electrical demand. The toolbox implements multiple seasonal Holt-Winters exponential smoothing models and neural network models. The models used include the use of discrete interval mobile seasonalities (DIMS) to improve forecasting on special days. Additionally, the results of its application in various electrical systems in Europe are shown, where the results obtained can be seen. The use of this library opens a new avenue of research for the use of models with discrete and complex seasonalities in other fields of application.",
        "subjects": [
            "cs.LG",
            "econ.EM",
            "stat.AP"
        ],
        "comment": "40 pages, 8 figures, 6 tables"
    },
    {
        "paper id": "2402.12391",
        "abstract url": "https://arxiv.org/abs/2402.12391",
        "title": "Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare",
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset to assess TAIS's effectiveness in gene identification, demonstrating our system's potential to significantly enhance the efficiency and scope of scientific exploration. Our findings represent a solid step towards automating scientific discovery through large language models.",
        "subjects": [
            "q-bio.GN",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "18 pages, 2 figures; added contact"
    },
    {
        "paper id": "2403.03223",
        "abstract url": "https://arxiv.org/abs/2403.03223",
        "title": "Exact Enforcement of Temporal Continuity in Sequential Physics-Informed Neural Networks",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The use of deep learning methods in scientific computing represents a potential paradigm shift in engineering problem solving. One of the most prominent developments is Physics-Informed Neural Networks (PINNs), in which neural networks are trained to satisfy partial differential equations (PDEs). While this method shows promise, the standard version has been shown to struggle in accurately predicting the dynamic behavior of time-dependent problems. To address this challenge, methods have been proposed that decompose the time domain into multiple segments, employing a distinct neural network in each segment and directly incorporating continuity between them in the loss function of the minimization problem. In this work we introduce a method to exactly enforce continuity between successive time segments via a solution ansatz. This hard constrained sequential PINN (HCS-PINN) method is simple to implement and eliminates the need for any loss terms associated with temporal continuity. The method is tested for a number of benchmark problems involving both linear and non-linear PDEs. Examples include various first order time dependent problems in which traditional PINNs struggle, namely advection, Allen-Cahn, and Korteweg-de Vries equations. Furthermore, second and third order time-dependent problems are demonstrated via wave and Jerky dynamics examples, respectively. Notably, the Jerky dynamics problem is chaotic, making the problem especially sensitive to temporal accuracy. The numerical experiments conducted with the proposed method demonstrated superior convergence and accuracy over both traditional PINNs and the soft-constrained counterparts.",
        "subjects": [
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": "30 pages, 13 figures"
    },
    {
        "paper id": "2403.15395",
        "abstract url": "https://arxiv.org/abs/2403.15395",
        "title": "An IoT system for a smart campus: Challenges and solutions illustrated over several real-world use cases",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "This article discusses the development of an IoT system for monitoring and controlling various devices and systems from different vendors. The authors considered key challenges in IoT projects, such as interoperability and integration, scalability, and data storage, processing, and visualization, during the design and deployment phases. In addition to these general challenges, the authors also delve into the specific integration challenges they encountered. Various devices and systems were integrated into the system and five real-world scenarios in a university campus environment are used to illustrate the challenges encountered. The scenarios involve monitoring various aspects of a university campus environment, including air quality, environmental parameters, energy efficiency, solar photovoltaic energy, and energy consumption. The authors analyzed data and CPU usage to ensure that the system could handle the large amount of data generated by the devices. The platform developed uses open source projects such as Home Assistant, InfluxDB, Grafana, and Node-RED. All developments have been published as open source in public repositories. In conclusion, this work highlights the potential and feasibility of IoT systems in various real-world applications, the importance of considering key challenges in IoT projects during the design and deployment phases, and the specific integration challenges that may be encountered.",
        "subjects": [
            "cs.CY",
            "cs.NI"
        ],
        "comment": "21 pages, 16 figures, Published in Internet of Things"
    },
    {
        "paper id": "2402.09720",
        "abstract url": "https://arxiv.org/abs/2402.09720",
        "title": "SpaceMeta: Global-Scale Massive Multi-User Virtual Interaction over LEO Satellite Constellations",
        "rating": "-2",
        "keywords": [
            [
                "Satellite"
            ]
        ],
        "abstract": "Low latency and high synchronization among users are critical for emerging multi-user virtual interaction applications. However, the existing ground-based cloud solutions are naturally limited by the complex ground topology and fiber speeds, making it difficult to pace with the requirement of multi-user virtual interaction. The growth of low earth orbit (LEO) satellite constellations becomes a promising alternative to ground solutions. To fully exploit the potential of the LEO satellite, in this paper, we study the satellite server selection problem for global-scale multi-user interaction applications over LEO constellations. We propose an effective server selection framework, called SpaceMeta, that jointly selects the ingress satellite servers and relay servers on the communication path to minimize latency and latency discrepancy among users. Extensive experiments using real-world Starlink topology demonstrate that SpaceMeta reduces the latency by 6.72% and the interquartile range (IQR) of user latency by 39.50% compared with state-of-the-art methods.",
        "subjects": [
            "cs.MM",
            "cs.NI"
        ],
        "comment": "Accepted by IEEE Satellite'23"
    },
    {
        "paper id": "2402.09743",
        "abstract url": "https://arxiv.org/abs/2402.09743",
        "title": "Quickest Detection of False Data Injection Attack in Distributed Process Tracking",
        "rating": "-2",
        "keywords": [
            [
                "graph"
            ],
            [
                "Attack"
            ]
        ],
        "abstract": "This paper addresses the problem of detecting false data injection (FDI) attacks in a distributed network without a fusion center, represented by a connected graph among multiple agent nodes. Each agent node is equipped with a sensor, and uses a Kalman consensus information filter (KCIF) to track a discrete time global process with linear dynamics and additive Gaussian noise. The state estimate of the global process at any sensor is computed from the local observation history and the information received by that agent node from its neighbors. At an unknown time, an attacker starts altering the local observation of one agent node. In the Bayesian setting where there is a known prior distribution of the attack beginning instant, we formulate a Bayesian quickest change detection (QCD) problem for FDI detection in order to minimize the mean detection delay subject to a false alarm probability constraint. While it is well-known that the optimal Bayesian QCD rule involves checking the Shriyaev's statistic against a threshold, we demonstrate how to compute the Shriyaev's statistic at each node in a recursive fashion given our non-i.i.d. observations. Next, we consider non-Bayesian QCD where the attack begins at an arbitrary and unknown time, and the detector seeks to minimize the worst case detection delay subject to a constraint on the mean time to false alarm and probability of misidentification. We use the multiple hypothesis sequential probability ratio test for attack detection and identification at each sensor. For unknown attack strategy, we use the window-limited generalized likelihood ratio (WL-GLR) algorithm to solve the QCD problem. Numerical results demonstrate the performances and trade-offs of the proposed algorithms.",
        "subjects": [
            "eess.SY",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09752",
        "abstract url": "https://arxiv.org/abs/2402.09752",
        "title": "Vector spectrometer with Hertz-level resolution and super-recognition capability",
        "rating": "-2",
        "keywords": [
            [
                "biosensing"
            ]
        ],
        "abstract": "High-resolution optical spectrometers are crucial in revealing intricate characteristics of signals, determining laser frequencies, measuring physical constants, identifying substances, and advancing biosensing applications. Conventional spectrometers, however, often grapple with inherent trade-offs among spectral resolution, wavelength range, and accuracy. Furthermore, even at high resolution, resolving overlapping spectral lines during spectroscopic analyses remains a huge challenge. Here, we propose a vector spectrometer with ultrahigh resolution, combining broadband optical frequency hopping, ultrafine microwave-photonic scanning, and vector detection. A programmable frequency-hopping laser was developed, facilitating a sub-Hz linewidth and Hz-level frequency stability, an improvement of four and six orders of magnitude, respectively, compared to those of state-of-the-art tunable lasers. We also designed an asymmetric optical transmitter and receiver to eliminate measurement errors arising from modulation nonlinearity and multi-channel crosstalk. The resultant vector spectrometer exhibits an unprecedented frequency resolution of 2 Hz, surpassing the state-of-the-art by four orders of magnitude, over a 33-nm range. Through high-resolution vector analysis, we observed that group delay information enhances the separation capability of overlapping spectral lines by over 47%, significantly streamlining the real-time identification of diverse substances. Our technique fills the gap in optical spectrometers with resolutions below 10 kHz and enables vector measurement to embrace revolution in functionality.",
        "subjects": [
            "physics.optics",
            "eess.SY",
            "physics.app-ph",
            "quant-ph"
        ],
        "comment": "21 pages, 6 figures"
    },
    {
        "paper id": "2402.09768",
        "abstract url": "https://arxiv.org/abs/2402.09768",
        "title": "Reeb Complements for Exploring Inclusions Between Isosurfaces From Two Scalar Fields",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "This article proposes to integrate two Reeb graphs with the information of their isosurfaces' inclusion relation. As computing power evolves, there arise numerical data that have small-scale physics inside larger ones -- for example, small clouds in a simulation can be contained inside an atmospheric layer, which is further contained in an enormous hurricane. Extracting such inclusions between isosurfaces is a challenge for isosurfacing: the user would have to explore the vast combinations of isosurfaces $(f_1^{-1}(l_1), f_2^{-1}(l_2))$ from scalar fields $f_i: M \\to \\mathbb{R}$, $i = 1, 2$, where $M$ is a domain manifold and $f_i$ are physical quantities, to find inclusion of one isosurface within another. For this, we propose the \\textit{Reeb complement}, a topological space that integrates two Reeb graphs with the inclusion relation. The Reeb complement has a natural partition that classifies equivalent containment of isosurfaces. This is a handy characteristic to let the Reeb complement serve as an overview of the inclusion relationship in the data. We also propose level-of-detail control of the inclusions through simplification of the Reeb complement.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "17 pages, 12 figures"
    },
    {
        "paper id": "2402.09790",
        "abstract url": "https://arxiv.org/abs/2402.09790",
        "title": "Multi-vertebral CT-based FE models implementing linear isotropic population-based material properties for the intervertebral discs cannot accurately predict strains",
        "rating": "-2",
        "keywords": [
            [
                "CT"
            ]
        ],
        "abstract": "Vertebral fractures prediction in clinics lacks of accuracy. The most used scores have limitations in distinguishing between subjects at risk or not. Finite element (FE) models generated from computed tomography (CT) of these patients may improve the predictive capability. Many models have already been proposed but the most of them considered the single vertebral body, excluding from the analysis the role of the inter-vertebral discs in the distribution of the load through the spine. Multi-vertebral models instead allow to examine more complex boundary condition. However, CT scans do not provide subject-specif information about the material properties of the disc. Consequently, the goal of the study was to validate a multi-vertebral FE model with subject specific modelling of the vertebral bone and population-based properties assigned to the disc, idealizing them with a linear isotropic material. Boundary condition were assigned in order to reproduce an experimental test performed on the same specimen and recorded using digital image correlation technique (DIC). FE and DIC strains on the vertebral surfaces are compared point-wise. Young's modulus values in the range 25-30 MPa allowed to achieve a comparable order of magnitude between experimental and computational data. However, the two distribution remained strongly different. To conclude, subject-specific material properties need to be assigned also to the discs as well as to the vertebrae to achieve acceptable accuracy in the assessment of the fracture risk.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "18 pages, 6 figures submitted to Biomechanics and Modeling in Mechanobiology"
    },
    {
        "paper id": "2402.09799",
        "abstract url": "https://arxiv.org/abs/2402.09799",
        "title": "Co-Designing a wiki-based community knowledge management system for personal science",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Personal science is the practice of addressing personally relevant health questions through self-research. Implementing personal science can be challenging, due to the need to develop and adopt research protocols, tools, and methods. While online communities can provide valuable peer support, tools for systematically accessing community knowledge are lacking. The objective of this study is to apply a participatory design process involving a community of personal science practitioners to develop a peer-produced knowledge base that supports the needs of practitioners as consumers and contributors of knowledge. The process led to the development of the Personal Science Wiki, an open repository for documenting and accessing individual self-tracking projects while facilitating the establishment of consensus knowledge. After initial design iterations and a field testing phase, we performed a user study with 21 participants to test and improve the platform, and to explore suitable information architectures. The study deepened our understanding of barriers to scaling the personal science community, established an infrastructure for knowledge management actively used by the community, and provided lessons on challenges, information needs, representations, and architectures to support individuals with their personal health inquiries",
        "subjects": [
            "cs.HC"
        ],
        "comment": "supplementary files are on Zenodo at https://zenodo.org/records/10659150"
    },
    {
        "paper id": "2402.09805",
        "abstract url": "https://arxiv.org/abs/2402.09805",
        "title": "Enabling Edge processing on LoRaWAN architecture",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "LoRaWAN is a wireless technology that enables high-density deployments of IoT devices. Designed for Low Power Wide Area Networks (LPWAN), LoRaWAN employs large cells to service a potentially extremely high number of devices. The technology enforces a centralized architecture, directing all data generated by the devices to a single network server for data processing. End-to-end encryption is used to guarantee the confidentiality and security of data. In this demo, we present \\edgelora, a system architecture designed to incorporate edge processing in LoRaWAN without compromising security and confidentiality of data. \\edgelora maintains backward compatibility and addresses scalability issues arising from handling large amounts of data sourced from a diverse range of devices. The demo provides evidence on the advantages in terms of reduced latency, lower network bandwidth requirements, higher scalability, and improved security and privacy resulting from the application of the Edge processing paradigm to LoRaWAN.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "This paper is a pre-print version of the conference paper that has been accepted to ACM MobiCom 2023: Proceedings of the 29th Annual International Conference on Mobile Computing and Networking"
    },
    {
        "paper id": "2402.09816",
        "abstract url": "https://arxiv.org/abs/2402.09816",
        "title": "Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment",
        "rating": "-2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "medical"
            ],
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained. Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09824",
        "abstract url": "https://arxiv.org/abs/2402.09824",
        "title": "On the discrete-time origins of the replicator dynamics: From convergence to instability and chaos",
        "rating": "-2",
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "We consider three distinct discrete-time models of learning and evolution in games: a biological model based on intra-species selective pressure, the dynamics induced by pairwise proportional imitation, and the exponential / multiplicative weights (EW) algorithm for online learning. Even though these models share the same continuous-time limit - the replicator dynamics - we show that second-order effects play a crucial role and may lead to drastically different behaviors in each model, even in very simple, symmetric $2\\times2$ games. Specifically, we study the resulting discrete-time dynamics in a class of parametrized congestion games, and we show that (i) in the biological model of intra-species competition, the dynamics remain convergent for any parameter value; (ii) the dynamics of pairwise proportional imitation exhibit an entire range of behaviors for larger time steps and different equilibrium configurations (stability, instability, and even Li-Yorke chaos); while (iii) in the EW algorithm, increasing the time step (almost) inevitably leads to chaos (again, in the formal, Li-Yorke sense). This divergence of behaviors comes in stark contrast to the globally convergent behavior of the replicator dynamics, and serves to delineate the extent to which the replicator dynamics provide a useful predictor for the long-run behavior of their discrete-time origins.",
        "subjects": [
            "math.DS",
            "cs.GT"
        ],
        "comment": "22 pages, 8 figures"
    },
    {
        "paper id": "2402.09828",
        "abstract url": "https://arxiv.org/abs/2402.09828",
        "title": "Validation of homogenized finite element models of human metastatic vertebrae using digital volume correlation",
        "rating": "-2",
        "keywords": [
            [
                "medical",
                "CT",
                "disease",
                "clinical"
            ]
        ],
        "abstract": "The incidence of vertebral fragility fracture is increased by the presence of preexisting pathologies such as metastatic disease. Computational tools could support the fracture prediction and consequently the decision of the best medical treatment. Anyway, validation is required to use these tools in clinical practice. To address this necessity, in this study subject-specific homogenized finite element models of single vertebrae were generated from micro CT images for both healthy and metastatic vertebrae and validated against experimental data. More in detail, spine segments were tested under compression and imaged with micro CT. The displacements field could be extracted for each vertebra singularly using the digital volume correlation full-field technique. Homogenized finite element models of each vertebra could hence be built from the micro CT images, applying boundary conditions consistent with the experimental displacements at the endplates. Numerical and experimental displacements and strains fields were eventually compared. In addition, the outcomes of a micro CT based homogenized model were compared to the ones of a clinical-CT based model. Good agreement between experimental and computational displacement fields, both for healthy and metastatic vertebrae, was found. Comparison between micro CT based and clinical-CT based outcomes showed strong correlations. Furthermore, models were able to qualitatively identify the regions which experimentally showed the highest strain concentration. In conclusion, the combination of experimental full-field technique and the in-silico modelling allowed the development of a promising pipeline for validation of fracture risk predictors, although further improvements in both fields are needed to better analyse quantitatively the post-yield behaviour of the vertebra.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09962",
        "abstract url": "https://arxiv.org/abs/2402.09962",
        "title": "ViGEO: an Assessment of Vision GNNs in Earth Observation",
        "rating": "-2",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "remote sensing",
                "Satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings. Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2. Thus, given the recent advances of machine learning, computer vision and the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks. Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area). Despite the recent advances in the field of computer vision, many works limit their analysis on Convolutional Neural Networks (CNNs) and, more recently, to vision transformers (ViTs). Given the recent successes of Graph Neural Networks (GNNs) on non-graph data, such as time-series and images, we investigate the performances of a recent Vision GNN architecture (ViG) applied to the task of land cover classification. The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale benchmarks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at SSTDM 2023 workshop, held in conjunction with ICDM 2023 conference"
    },
    {
        "paper id": "2402.09982",
        "abstract url": "https://arxiv.org/abs/2402.09982",
        "title": "Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "Facial"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\\% for the InceptionResNetV2 model.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "The 11th International Conference on Artificial Intelligence, Soft Computing and Applications (AIAA 2021)"
    },
    {
        "paper id": "2402.10035",
        "abstract url": "https://arxiv.org/abs/2402.10035",
        "title": "Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity",
        "rating": "-2",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "medical",
                "Retinal"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely. Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data. We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client. We evaluated two federated learning methods, FedAvg and FedProx for these settings. Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool. However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings. Conclusion: Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms. Notably, FedProx appears to be more robust to the increased heterogeneity.",
        "subjects": [
            "cs.CV",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10119",
        "abstract url": "https://arxiv.org/abs/2402.10119",
        "title": "Physics-Informed Neural Network Policy Iteration: Algorithms, Convergence, and Verification",
        "rating": "-2",
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "Solving nonlinear optimal control problems is a challenging task, particularly for high-dimensional problems. We propose algorithms for model-based policy iterations to solve nonlinear optimal control problems with convergence guarantees. The main component of our approach is an iterative procedure that utilizes neural approximations to solve linear partial differential equations (PDEs), ensuring convergence. We present two variants of the algorithms. The first variant formulates the optimization problem as a linear least square problem, drawing inspiration from extreme learning machine (ELM) for solving PDEs. This variant efficiently handles low-dimensional problems with high accuracy. The second variant is based on a physics-informed neural network (PINN) for solving PDEs and has the potential to address high-dimensional problems. We demonstrate that both algorithms outperform traditional approaches, such as Galerkin methods, by a significant margin. We provide a theoretical analysis of both algorithms in terms of convergence of neural approximations towards the true optimal solutions in a general setting. Furthermore, we employ formal verification techniques to demonstrate the verifiable stability of the resulting controllers.",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10204",
        "abstract url": "https://arxiv.org/abs/2402.10204",
        "title": "Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "astronomy"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models reconstruction, then used Photutils to determine source coordinates and fluxes, assessing the model's performance across different water vapor levels. Our method showed excellence in source localization, achieving more than 90% completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in the test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional DDPMs is a powerful tool for image-to-image translation, yielding accurate and robust characterisation of radio sources, and outperforming existing methodologies. While this study underscores its significant potential for applications in radio astronomy, we also acknowledge certain limitations that accompany its usage, suggesting directions for further refinement and research.",
        "subjects": [
            "astro-ph.IM",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "In production in Astronomy&Astrophyics"
    },
    {
        "paper id": "2402.10255",
        "abstract url": "https://arxiv.org/abs/2402.10255",
        "title": "Benchmarking the Operation of Quantum Heuristics and Ising Machines: Scoring Parameter Setting Strategies on Optimization Applications",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "We discuss guidelines for evaluating the performance of parameterized stochastic solvers for optimization problems, with particular attention to systems that employ novel hardware, such as digital quantum processors running variational algorithms, analog processors performing quantum annealing, or coherent Ising Machines. We illustrate through an example a benchmarking procedure grounded in the statistical analysis of the expectation of a given performance metric measured in a test environment. In particular, we discuss the necessity and cost of setting parameters that affect the algorithm's performance. The optimal value of these parameters could vary significantly between instances of the same target problem. We present an open-source software package that facilitates the design, evaluation, and visualization of practical parameter tuning strategies for complex use of the heterogeneous components of the solver. We examine in detail an example using parallel tempering and a simulator of a photonic Coherent Ising Machine computing and display the scoring of an illustrative baseline family of parameter-setting strategies that feature an exploration-exploitation trade-off.",
        "subjects": [
            "quant-ph",
            "cs.ET",
            "stat.CO",
            "stat.ME"
        ],
        "comment": "13 pages, 6 figures"
    },
    {
        "paper id": "2402.10307",
        "abstract url": "https://arxiv.org/abs/2402.10307",
        "title": "A New Radio to Overcome Critical Link Budgets",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "We propose Multi-Antenna (MA) Towards Inband Shift Keying (TISK): a new multi-carrier radio concept to cope with critical link budgets. In contrast to common proposals that rely on analog beamforming at both transmitter and receiver, MA-TISK does not require beam alignment. The transmitted signals have all constant envelope in continuous time, which allows for efficient, low-cost power amplification and up-conversion. The concept is compatible with any linear PSK-modulation as well as pulse position modulation. Each sub-carrier is sent over a separate antenna that is equipped with a voltage-controlled oscillator. The phases of these oscillators are controlled by digital baseband. Temporal signal combining makes up for the lack of beamforming gain at the transmitter. A common message may be broadcast to many receivers, simultaneously. Demodulation can be efficiently implemented by means of fast Fourier transform. MA-TISK does not suffer from spectral re-growth issues plaguing other constant envelope modulations like GMSK. Almost rectangular signal spectra similar to those for linear modulation with root-raised-cosine pulse shaping are possible. For the 100 MHz-wide spectral mask of 5G downlink, QPSK-modulation allows for 160 MBit/s with 5.74 MHz subcarrier spacing when using 16 transmit antennas. The wide carrier spacing makes the signals insensitive to Doppler effects. There is no loss in link budget gain compared to spatial beamforming at the transmitter.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "The paper is not correct. The paper calculates the beamforming gain of transmit beamforming for N antennas as N, but it should be N^2"
    },
    {
        "paper id": "2402.10312",
        "abstract url": "https://arxiv.org/abs/2402.10312",
        "title": "Towards Tight Convex Relaxations for Contact-Rich Manipulation",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "We present a method for global motion planning of robotic systems that interact with the environment through contacts. Our method directly handles the hybrid nature of such tasks using tools from convex optimization. We formulate the motion-planning problem as a shortest-path problem in a graph of convex sets, where a path in the graph corresponds to a contact sequence and a convex set models the quasi-static dynamics within a fixed contact mode. For each contact mode, we use semidefinite programming to relax the nonconvex dynamics that results from the simultaneous optimization of the object's pose, contact locations, and contact forces. The result is a tight convex relaxation of the overall planning problem, that can be efficiently solved and quickly rounded to find a feasible contact-rich trajectory. As a first application of this technique, we focus on the task of planar pushing. Exhaustive experiments show that our convex-optimization method generates plans that are consistently within a small percentage of the global optimum. We demonstrate the quality of these plans on a real robotic system.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10329",
        "abstract url": "https://arxiv.org/abs/2402.10329",
        "title": "Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "We present Universal Manipulation Interface (UMI) -- a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI's versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI's hardware and software system is open-sourced at https://umi-gripper.github.io.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Project website: https://umi-gripper.github.io"
    },
    {
        "paper id": "2402.10351",
        "abstract url": "https://arxiv.org/abs/2402.10351",
        "title": "Quantum Automating $\\mathbf{TC}^0$-Frege Is LWE-Hard",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "We prove the first hardness results against efficient proof search by quantum algorithms. We show that under Learning with Errors (LWE), the standard lattice-based cryptographic assumption, no quantum algorithm can weakly automate $\\mathbf{TC}^0$-Frege. This extends the line of results of Kraj\u00ed\u010dek and Pudl\u00e1k (Information and Computation, 1998), Bonet, Pitassi, and Ray (FOCS, 1997), and Bonet et al. (Computational Complexity, 2004), who showed that Extended Frege, $\\mathbf{TC}^0$-Frege and $\\mathbf{AC}^0$-Frege, respectively, cannot be weakly automated by classical algorithms if either the RSA cryptosystem or the Diffie-Hellman key exchange protocol are secure. To the best of our knowledge, this is the first interaction between quantum computation and propositional proof search.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10359",
        "abstract url": "https://arxiv.org/abs/2402.10359",
        "title": "Can we Soft Prompt LLMs for Graph Learning Tasks?",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "biological"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tasks demonstrate the effectiveness of our proposed method. The GraphPrompter framework unveils the substantial capabilities of LLMs as predictors in graph-related tasks, enabling researchers to utilize LLMs across a spectrum of real-world graph scenarios more effectively.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "Accepted by The Web Conference (WWW) 2024 Short Paper Track"
    },
    {
        "paper id": "2402.10368",
        "abstract url": "https://arxiv.org/abs/2402.10368",
        "title": "Beam Squinting Compensation: An NCR-Assisted Scenario",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Millimeter wave (mmWave) and sub-THz communications, foreseen for sixth generation (6G), suffer from high propagation losses which affect the network coverage. To address this point, smart entities such as network-controlled repeaters (NCRs) have been considered as cost-efficient solutions for coverage extension. NCRs, which have been standardized in 3rd generation partnership project Release 18, are radio frequency repeaters with beamforming capability controlled by the network through side control information. Another challenge raised by the adoption of high frequency bands is the use of large bandwidths. Here, a common configuration is to divide a large frequency band into multiple smaller subbands. In this context, we consider a scenario with NCRs where signaling related to measurements used for radio resource management is transmitted in one subband centered at frequency $f_c$ and data transmission is performed at a different frequency $f_c + \u0394f$ based on the measurements taken at $f_c$. Here, a challenge is that the array radiation pattern can be frequency dependent and, therefore, lead to beam misalignment, called beam squinting. We characterize beam squinting in the context of subband operation and propose a solution where the beam patterns to be employed at a given subband can be adjusted/compensated to mitigate beam squinting. Our results show that, without compensation, the perceived signal to interference-plus-noise ratio (SINR) and so the throughput can be substantially decreased due to beam squinting. However, with our proposed compensation method, the system is able to support NCR subband signaling operation with similar performance as if signaling and data were transmitted at the same frequency.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IEEE Transactions on Vehicular Technology"
    },
    {
        "paper id": "2402.10386",
        "abstract url": "https://arxiv.org/abs/2402.10386",
        "title": "Exploring RIS Coverage Enhancement in Factories: From Ray-Based Modeling to Use-Case Analysis",
        "rating": "-2",
        "keywords": [
            [
                "5G",
                "industrial"
            ]
        ],
        "abstract": "Reconfigurable Intelligent Surfaces (RISs) have risen to the forefront of wireless communications research due to their proactive ability to alter the wireless environment intelligently, promising improved wireless network capacity and coverage. Thus, RISs are a pivotal technology in evolving next-generation communication networks. This paper demonstrates a system-level modeling approach for RIS. The RIS model, integrated with the Volcano ray-tracing (RT) tool, is used to analyze the far-field (FF) RIS channel properties in a typical factory environment and explore coverage enhancement at sub-6 GHz and mmWave frequencies. The results obtained in non-line-of-sight (NLoS) scenarios confirm that RIS application is relevant for 5G industrial networks.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "copyright 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works"
    },
    {
        "paper id": "2402.10399",
        "abstract url": "https://arxiv.org/abs/2402.10399",
        "title": "Distributed Model Predictive Control for Cooperative Multirotor Landing on Uncrewed Surface Vessel in Waves",
        "rating": "-2",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Heterogeneous autonomous robot teams consisting of multirotor and uncrewed surface vessels (USVs) have the potential to enable various maritime applications, including advanced search-and-rescue operations. A critical requirement of these applications is the ability to land a multirotor on a USV for tasks such as recharging. This paper addresses the challenge of safely landing a multirotor on a cooperative USV in harsh open waters. To tackle this problem, we propose a novel sequential distributed model predictive control (MPC) scheme for cooperative multirotor-USV landing. Our approach combines standard tracking MPCs for the multirotor and USV with additional artificial intermediate goal locations. These artificial goals enable the robots to coordinate their cooperation without prior guidance. Each vehicle solves an individual optimization problem for both the artificial goal and an input that tracks it but only communicates the former to the other vehicle. The artificial goals are penalized by a suitable coupling cost. Furthermore, our proposed distributed MPC scheme utilizes a spatial-temporal wave model to coordinate in real-time a safer landing location and time the multirotor's landing to limit severe tilt of the USV.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10434",
        "abstract url": "https://arxiv.org/abs/2402.10434",
        "title": "Parametric Augmentation for Time Series Contrastive Learning",
        "rating": "-2",
        "keywords": [
            [
                "graph"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in contrastive learning approaches. Usually, preset human intuition directs the selection of relevant data augmentations. Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmentations on the fly. In this study, we address this gap by analyzing time series data augmentation using information theory and summarizing the most commonly adopted augmentations in a unified format. We then propose a contrastive learning framework with parametric augmentation, AutoTCL, which can be adaptively employed to support time series representation learning. The proposed approach is encoder-agnostic, allowing it to be seamlessly integrated with different backbone encoders. Experiments on univariate forecasting tasks demonstrate the highly competitive results of our method, with an average 6.5\\% reduction in MSE and 4.7\\% in MAE over the leading baselines. In classification tasks, AutoTCL achieves a $1.2\\%$ increase in average accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by International Conference on Learning Representations (ICLR 2024)"
    },
    {
        "paper id": "2404.02159",
        "abstract url": "https://arxiv.org/abs/2404.02159",
        "title": "Fairness-aware Age-of-Information Minimization in WPT-Assisted Short-Packet THz Communications for mURLLC",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "The technological landscape is swiftly advancing towards large-scale systems, creating significant opportunities, particularly in the domain of Terahertz (THz) communications. Networks designed for massive connectivity, comprising numerous Internet of Things (IoT) devices, are at the forefront of this advancement. In this paper, we consider Wireless Power Transfer (WPT)-enabled networks that support these IoT devices with massive Ultra-Reliable and Low-Latency Communication (mURLLC) services.The focus of such networks is information freshness, with the Age-of-Information (AoI) serving as the pivotal performance metric. In particular, we aim to minimize the maximum AoI among IoT devices by optimizing the scheduling policy. Our analytical findings establish the convexity property of the problem, which can be solved efficiently. Furthermore, we introduce the concept of AoI-oriented cluster capacity, examining the relationship between the number of supported devices and the AoI performance in the network. Numerical simulations validate the advantage of our proposed approach in enhancing AoI performance, indicating its potential to guide the design of future THz communication systems for IoT applications requiring mURLLC services.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09711",
        "abstract url": "https://arxiv.org/abs/2402.09711",
        "title": "Node Duplication Improves Cold-start Link Prediction",
        "rating": "-2.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "recommendation"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes. Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied to existing GNNs with very light computational cost. Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to GNNs and state-of-the-art cold-start methods.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09795",
        "abstract url": "https://arxiv.org/abs/2402.09795",
        "title": "An advanced data fabric architecture leveraging homomorphic encryption and federated learning",
        "rating": "-2.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "medical",
                "tumor"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning. This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture. With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features. The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data. The study demonstrates the method's effectiveness through a case study on pituitary tumor classification, achieving a significant level of accuracy. However, the primary focus of the study is on the development and evaluation of federated learning and partially homomorphic encryption as tools for secure medical image analysis. The results highlight the potential of these techniques to be applied to other privacy-sensitive domains and contribute to the growing body of research on secure and privacy-preserving machine learning.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09846",
        "abstract url": "https://arxiv.org/abs/2402.09846",
        "title": "A Deep Learning Approach to Radar-based QPE",
        "rating": "-2.5",
        "keywords": [
            [
                "Radar"
            ],
            [
                "forecast"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this study, we propose a volume-to-point framework for quantitative precipitation estimation (QPE) based on the Quantitative Precipitation Estimation and Segregation Using Multiple Sensor (QPESUMS) Mosaic Radar data set. With a data volume consisting of the time series of gridded radar reflectivities over the Taiwan area, we used machine learning algorithms to establish a statistical model for QPE in weather stations. The model extracts spatial and temporal features from the input data volume and then associates these features with the location-specific precipitations. In contrast to QPE methods based on the Z-R relation, we leverage the machine learning algorithms to automatically detect the evolution and movement of weather systems and associate these patterns to a location with specific topographic attributes. Specifically, we evaluated this framework with the hourly precipitation data of 45 weather stations in Taipei during 2013-2016. In comparison to the operational QPE scheme used by the Central Weather Bureau, the volume-to-point framework performed comparably well in general cases and excelled in detecting heavy-rainfall events. By using the current results as the reference benchmark, the proposed method can integrate the heterogeneous data sources and potentially improve the forecast in extreme precipitation scenarios.",
        "subjects": [
            "physics.ao-ph",
            "cs.LG",
            "eess.SP"
        ],
        "comment": "22 pages, 11 figures. Published in Earth and Space Science"
    },
    {
        "paper id": "2402.10102",
        "abstract url": "https://arxiv.org/abs/2402.10102",
        "title": "A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Cancer"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Distributed Artificial Intelligence is attracting interest day by day. In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way. The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation. This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature.",
        "subjects": [
            "cs.AI",
            "cs.DC"
        ],
        "comment": "Rough Sets: International Joint Conference, IJCRS 2020"
    },
    {
        "paper id": "2402.10115",
        "abstract url": "https://arxiv.org/abs/2402.10115",
        "title": "Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN",
        "rating": "-2.5",
        "keywords": [
            [
                "GAN",
                "synthesizing"
            ],
            [
                "EEG"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "eess.SP",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10135",
        "abstract url": "https://arxiv.org/abs/2402.10135",
        "title": "Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "biomedical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy. This research is focused on testing different federated strategies in a peer-to-peer environment. The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution. The strategies are tested with varying data sizes to identify the most robust ones. This research tests the strategies with several biomedical datasets and the results of the experiments show that the accuracy-based weighted average outperforms the classical federated averaging method.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10211",
        "abstract url": "https://arxiv.org/abs/2402.10211",
        "title": "Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling",
        "rating": "-2.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.",
        "subjects": [
            "cs.LG",
            "cs.RO",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10250",
        "abstract url": "https://arxiv.org/abs/2402.10250",
        "title": "Zastosowanie graf\u00f3w i sieci w systemach rekomendacji",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "The chapter aims to explore the application of graph theory and networks in the recommendation domain, encompassing the mathematical models that form the foundation for the algorithms and recommendation systems developed based on them. The initial section of the chapter provides a concise overview of the recommendation field, with a particular focus on the types of recommendation solutions and the mathematical description of the problem. Subsequently, the chapter delves into the models and techniques for utilizing graphs and networks, along with illustrative examples of algorithms constructed on their basis.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "in Polish language. Przedsi\u0119biorstwo w nowej rzeczywisto\u015bci gospodarczej. Relacje zmiany strategie; 2021"
    },
    {
        "paper id": "2402.10350",
        "abstract url": "https://arxiv.org/abs/2402.10350",
        "title": "Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review",
        "rating": "-2.5",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of LLMs in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact LLMs could have on forecasting and anomaly detection while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10983",
        "abstract url": "https://arxiv.org/abs/2402.10983",
        "title": "Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of Conjugate Variables in System Attacks",
        "rating": "-2.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "Quantum",
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural networks demonstrate inherent vulnerability to small, non-random perturbations, emerging as adversarial attacks. Such attacks, born from the gradient of the loss function relative to the input, are discerned as input conjugates, revealing a systemic fragility within the network structure. Intriguingly, a mathematical congruence manifests between this mechanism and the quantum physics' uncertainty principle, casting light on a hitherto unanticipated interdisciplinarity. This inherent susceptibility within neural network systems is generally intrinsic, highlighting not only the innate vulnerability of these networks but also suggesting potential advancements in the interdisciplinary area for understanding these black-box networks.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "quant-ph"
        ],
        "comment": "13 pages, 3 figures"
    },
    {
        "paper id": "2403.05559",
        "abstract url": "https://arxiv.org/abs/2403.05559",
        "title": "Improving Cognitive Diagnosis Models with Adaptive Relational Graph Neural Networks",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Diagnosis"
            ],
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Cognitive Diagnosis (CD) algorithms receive growing research interest in intelligent education. Typically, these CD algorithms assist students by inferring their abilities (i.e., their proficiency levels on various knowledge concepts). The proficiency levels can enable further targeted skill training and personalized exercise recommendations, thereby promoting students' learning efficiency in online education. Recently, researchers have found that building and incorporating a student-exercise bipartite graph is beneficial for enhancing diagnostic performance. However, there are still limitations in their studies. On one hand, researchers overlook the heterogeneity within edges, where there can be both correct and incorrect answers. On the other hand, they disregard the uncertainty within edges, e.g., a correct answer can indicate true mastery or fortunate guessing. To address the limitations, we propose Adaptive Semantic-aware Graph-based Cognitive Diagnosis model (ASG-CD), which introduces a novel and effective way to leverage bipartite graph information in CD. Specifically, we first map students, exercises, and knowledge concepts into a latent representation space and combine these latent representations to obtain student abilities and exercise difficulties. After that, we propose a Semantic-aware Graph Neural Network Layer to address edge heterogeneity. This layer splits the original bipartite graph into two subgraphs according to edge semantics, and aggregates information based on these two subgraphs separately. To mitigate the impact of edge uncertainties, we propose an Adaptive Edge Differentiation Layer that dynamically differentiates edges, followed by keeping reliable edges and filtering out uncertain edges. Extensive experiments on three real-world datasets have demonstrated the effectiveness of ASG-CD.",
        "subjects": [
            "cs.CY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09883",
        "abstract url": "https://arxiv.org/abs/2402.09883",
        "title": "Lester: rotoscope animation through video object segmentation and tracking",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09886",
        "abstract url": "https://arxiv.org/abs/2402.09886",
        "title": "Self-Supervised Learning of Visual Robot Localization Using LED State Prediction as a Pretext Task",
        "rating": "-3",
        "keywords": [
            [
                "robotics",
                "Robot"
            ],
            [
                "drone"
            ]
        ],
        "abstract": "We propose a novel self-supervised approach for learning to visually localize robots equipped with controllable LEDs. We rely on a few training samples labeled with position ground truth and many training samples in which only the LED state is known, whose collection is cheap. We show that using LED state prediction as a pretext task significantly helps to learn the visual localization end task. The resulting model does not require knowledge of LED states during inference. We instantiate the approach to visual relative localization of nano-quadrotors: experimental results show that using our pretext task significantly improves localization accuracy (from 68.3% to 76.2%) and outperforms alternative strategies, such as a supervised baseline, model pre-training, and an autoencoding pretext task. We deploy our model aboard a 27-g Crazyflie nano-drone, running at 21 fps, in a position-tracking task of a peer nano-drone. Our approach, relying on position labels for only 300 images, yields a mean tracking error of 4.2 cm versus 11.9 cm of a supervised baseline model trained without our pretext task. Videos and code of the proposed approach are available at https://github.com/idsia-robotics/leds-as-pretext",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09959",
        "abstract url": "https://arxiv.org/abs/2402.09959",
        "title": "LLM-based Federated Recommendation",
        "rating": "-3",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs. To address these challenges, we introduce a Privacy-Preserving LLM-based Recommendation (PPLR) framework. The PPLR framework employs two primary strategies. First, it implements a dynamic balance strategy, which involves the design of dynamic parameter aggregation and adjustment of learning speed for different clients during the training phase, to ensure relatively balanced performance across all clients. Second, PPLR adopts a flexible storage strategy, selectively retaining certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server. This approach aims to preserve user privacy while efficiently saving computational and storage resources. Experimental results demonstrate that PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner that is both computationally and storage-efficient, while effectively protecting user privacy.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10060",
        "abstract url": "https://arxiv.org/abs/2402.10060",
        "title": "Quantum Backtracking in Qrisp Applied to Sudoku Problems",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "The quantum backtracking algorithm proposed by Ashley Montanaro raised considerable interest, as it provides a quantum speed-up for a large class of classical optimization algorithms. It does not suffer from Barren-Plateaus and transfers well into the fault-tolerant era, as it requires only a limited number of arbitrary angle gates. Despite its potential, the algorithm has seen limited implementation efforts, presumably due to its abstract formulation. In this work, we provide a detailed instruction on implementing the quantum step operator for arbitrary backtracking instances. For a single controlled diffuser of a binary backtracking tree with depth n, our implementation requires only $6n+14$ CX gates. We detail the process of constructing accept and reject oracles for Sudoku problems using our interface to quantum backtracking. The presented code is written using Qrisp, a high-level quantum programming language, making it executable on most current physical backends and simulators. Subsequently, we perform several simulator based experiments and demonstrate solving 4x4 Sudoku instances with up to 9 empty fields. This is, to the best of our knowledge, the first instance of a compilable implementation of this generality, marking a significant and exciting step forward in quantum software engineering.",
        "subjects": [
            "quant-ph",
            "cs.DS",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10188",
        "abstract url": "https://arxiv.org/abs/2402.10188",
        "title": "Trainability Barriers in Low-Depth QAOA Landscapes",
        "rating": "-3",
        "keywords": [
            [
                "Depth"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "The Quantum Alternating Operator Ansatz (QAOA) is a prominent variational quantum algorithm for solving combinatorial optimization problems. Its effectiveness depends on identifying input parameters that yield high-quality solutions. However, understanding the complexity of training QAOA remains an under-explored area. Previous results have given analytical performance guarantees for a small, fixed number of parameters. At the opposite end of the spectrum, barren plateaus are likely to emerge at $\u03a9(n)$ parameters for $n$ qubits. In this work, we study the difficulty of training in the intermediate regime, which is the focus of most current numerical studies and near-term hardware implementations. Through extensive numerical analysis of the quality and quantity of local minima, we argue that QAOA landscapes can exhibit a superpolynomial growth in the number of low-quality local minima even when the number of parameters scales logarithmically with $n$. This means that the common technique of gradient descent from randomly initialized parameters is doomed to fail beyond small $n$, and emphasizes the need for good initial guesses of the optimal parameters.",
        "subjects": [
            "quant-ph",
            "cs.DS"
        ],
        "comment": "Accepted at the 21st ACM International Conference on Computing Frontiers CF'24"
    },
    {
        "paper id": "2402.10321",
        "abstract url": "https://arxiv.org/abs/2402.10321",
        "title": "LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of Spinning LiDAR",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "LiDAR",
                "vehicle"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "This paper presents an approach for applying camera perception techniques to spinning LiDAR data. To improve the robustness of long-term change detection from a 3D LiDAR, range and intensity information are rendered into virtual perspectives using a pinhole camera model. Hue-saturation-value image encoding is used to colourize the images by range and near-IR intensity. The LiDAR's active scene illumination makes it invariant to ambient brightness, which enables night-to-day change detection without additional processing. Using the range-colourized, perspective image allows existing foundation models to detect semantic regions. Specifically, the Segment Anything Model detects semantically similar regions in both a previously acquired map and live view from a path-repeating robot. By comparing the masks in both views, changes in the live scan are detected. Results indicate that the Segment Anything Model accurately captures the shape of arbitrary changes introduced into scenes. The proposed method achieves a segmentation intersection over union of 73.3% when evaluated in unstructured environments and 80.4% when evaluated within the planning corridor. Changes can be detected reliably through day-to-night illumination variations. After pixel-level masks are generated, the one-to-one correspondence with 3D points means that the 2D masks can be used directly to recover the 3D location of the changes. The detected 3D changes are avoided in a closed loop by treating them as obstacles in a local motion planner. Experiments on an unmanned ground vehicle demonstrate the performance of the method.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "9 pages (8 content, 1 references). 9 figures, Presented at 2024 Conference on Robots and Vision (CRV)"
    },
    {
        "paper id": "2402.10344",
        "abstract url": "https://arxiv.org/abs/2402.10344",
        "title": "Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field Conditions",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "point cloud",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "LiDAR"
            ],
            [
                "agricultural"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We evaluate different Neural Radiance Fields (NeRFs) techniques for reconstructing (3D) plants in varied environments, from indoor settings to outdoor fields. Traditional techniques often struggle to capture the complex details of plants, which is crucial for botanical and agricultural understanding. We evaluate three scenarios with increasing complexity and compare the results with the point cloud obtained using LiDAR as ground truth data. In the most realistic field scenario, the NeRF models achieve a 74.65% F1 score with 30 minutes of training on the GPU, highlighting the efficiency and accuracy of NeRFs in challenging environments. These findings not only demonstrate the potential of NeRF in detailed and realistic 3D plant modeling but also suggest practical approaches for enhancing the speed and efficiency of the 3D reconstruction process.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10280",
        "abstract url": "https://arxiv.org/abs/2402.10280",
        "title": "SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms",
        "rating": "-3.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attacks"
            ],
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a novel energy-aware federated learning (FL)-based system, namely SusFL, for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors. This system equips animals, such as cattle, with solar sensors with computational capabilities, including Raspberry Pis, to train a local deep-learning model on health data. These sensors periodically update Long Range (LoRa) gateways, forming a wireless sensor network (WSN) to detect diseases like mastitis. Our proposed SusFL system incorporates mechanism design, a game theory concept, for intelligent client selection to optimize monitoring quality while minimizing energy use. This strategy ensures the system's sustainability and resilience against adversarial attacks, including data poisoning and privacy threats, that could disrupt FL operations. Through extensive comparative analysis using real-time datasets, we demonstrate that our FL-based monitoring system significantly outperforms existing methods in prediction accuracy, operational efficiency, system reliability (i.e., mean time between failures or MTBF), and social welfare maximization by the mechanism designer. Our findings validate the superiority of our system for effective and sustainable animal health monitoring in smart farms. The experimental results show that SusFL significantly improves system performance, including a $10\\%$ reduction in energy consumption, a $15\\%$ increase in social welfare, and a $34\\%$ rise in Mean Time Between Failures (MTBF), alongside a marginal increase in the global model's prediction accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10389",
        "abstract url": "https://arxiv.org/abs/2402.10389",
        "title": "Enabling Zero Trust Security in IoMT Edge Network",
        "rating": "-3.5",
        "keywords": [
            [
                "Medical",
                "health"
            ],
            [
                "IoT"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Internet of Medical Things (IoMT) deals with a patient-data-rich segment, which makes security and privacy a severe concern for patients. Therefore, access control is a significant aspect of ensuring trust in the IoMT. However, deploying existing authentication and authorization solutions to the Internet of Medical Things (IoMT) is not straightforward because of highly dynamic and possibly unprotected environments and untrusted supply chain for the IoT devices. In this article, we propose Soter, a Zero-Trust based authentication system for the IoMT. Soter Incorporates trust negotiation mechanisms within the Zero Trust framework to enable dynamic trust establishment. When a user or device seeks access to a resource, initiate a trust negotiation process. During this process, credentials, attributes, and contextual information are exchanged between the requester and the resource owner. Soter defines access rules based on various factors, including user identity, device health, and location. Access is granted or denied based on these conditions.",
        "subjects": [
            "cs.CR",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10324",
        "abstract url": "https://arxiv.org/abs/2402.10324",
        "title": "Hacktivism Goes Orbital: Investigating NB65's Breach of ROSCOSMOS",
        "rating": "-4",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "attack"
            ],
            [
                "satellite"
            ]
        ],
        "abstract": "In March of 2022, Network battalion 65 (NB65), a hacktivist affiliate of Anonymous, publicly asserted its successful breach of ROSCOSMOS's satellite imaging capabilities in response to Russia's invasion of Ukraine. NB65 disseminated a series of primary sources as substantiation, proclaiming the incapacitation of ROSCOSMOS's space-based vehicle monitoring system and doxing of related proprietary documentation. Despite the profound implications of hacktivist incursions into the space sector, the event has garnered limited attention due to the obscurity of technical attack vectors and ROCOSMOS's denial of NB65's allegations. Through analysis of NB65's released primary sources of evidence, this paper uncovers the probable vulnerabilities and exploits that enabled the alleged breach into ROSCOSMOS's ground and space segment. Additionally, we highlight lessons learned and the consequences this event has for the global aerospace community.",
        "subjects": [
            "cs.CR",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10365",
        "abstract url": "https://arxiv.org/abs/2402.10365",
        "title": "Deep Spectral Meshes: Multi-Frequency Facial Mesh Processing with Graph Neural Networks",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "synthesis"
            ],
            [
                "Graph"
            ],
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the rising popularity of virtual worlds, the importance of data-driven parametric models of 3D meshes has grown rapidly. Numerous applications, such as computer vision, procedural generation, and mesh editing, vastly rely on these models. However, current approaches do not allow for independent editing of deformations at different frequency levels. They also do not benefit from representing deformations at different frequencies with dedicated representations, which would better expose their properties and improve the generated meshes' geometric and perceptual quality. In this work, spectral meshes are introduced as a method to decompose mesh deformations into low-frequency and high-frequency deformations. These features of low- and high-frequency deformations are used for representation learning with graph convolutional networks. A parametric model for 3D facial mesh synthesis is built upon the proposed framework, exposing user parameters that control disentangled high- and low-frequency deformations. Independent control of deformations at different frequencies and generation of plausible synthetic examples are mutually exclusive objectives. A Conditioning Factor is introduced to leverage these objectives. Our model takes further advantage of spectral partitioning by representing different frequency levels with disparate, more suitable representations. Low frequencies are represented with standardised Euclidean coordinates, and high frequencies with a normalised deformation representation (DR). This paper investigates applications of our proposed approach in mesh reconstruction, mesh interpolation, and multi-frequency editing. It is demonstrated that our method improves the overall quality of generated meshes on most datasets when considering both the $L_1$ norm and perceptual Dihedral Angle Mesh Error (DAME) metrics.",
        "subjects": [
            "cs.CV",
            "cs.CG",
            "cs.GR"
        ],
        "comment": "26 pages, 10 figures, journal article"
    },
    {
        "paper id": "2402.10192",
        "abstract url": "https://arxiv.org/abs/2402.10192",
        "title": "Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias",
        "rating": "-4.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "diagnosis"
            ],
            [
                "quantum",
                "Physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for a dynamic hypergraph is put forward to describe the agent's training history along with applications to AI and hypergraph visualization. An inductive bias inspired by the remarkably successful few-body interaction models used in quantum many-body physics is formalized for our classical mePS framework and employed to tackle the exponential complexity associated with naive implementations of hypergraphs. We prove that our inductive bias reduces the complexity from exponential to polynomial, with the exponent representing the cutoff on how many particles can interact. We numerically apply our method to two toy environments and a more complex scenario modelling the diagnosis of a broken computer. These environments demonstrate the resource savings provided by an appropriate choice of inductive bias, as well as showcasing aspects of interpretability. A quantum model for mePS is also briefly outlined and some future directions for it are discussed.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.DM",
            "quant-ph"
        ],
        "comment": "24 pages, 8 figures; Code repository at https://github.com/MariusKrumm/ManyBodyMEPS. Added figures and shortened computer maintenance section text for better readability"
    },
    {
        "paper id": "2402.10980",
        "abstract url": "https://arxiv.org/abs/2402.10980",
        "title": "ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback",
        "rating": "-5",
        "keywords": [
            [
                "3D"
            ],
            [
                "GNN",
                "graph"
            ],
            [
                "chemistry",
                "Chemical"
            ],
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided reasoning with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery.",
        "subjects": [
            "physics.chem-ph",
            "cs.AI",
            "cs.CE",
            "cs.LG"
        ],
        "comment": "8 pages, accepted by ICML 2024"
    },
    {
        "paper id": "2402.09810",
        "abstract url": "https://arxiv.org/abs/2402.09810",
        "title": "3D Cooperative Localization in UAV Systems: CRLB Analysis and Security Solutions",
        "rating": "-6",
        "keywords": [
            [
                "3D"
            ],
            [
                "vehicle"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "attacks"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "This paper presents a robust and secure framework for achieving accurate and reliable cooperative localization in multiple unmanned aerial vehicle (UAV) systems. The Cramer-Rao low bound (CRLB) for the three-dimensional (3D) cooperative localization network is derived, with particular attention given to the non-uniform spatial distribution of anchor nodes. Challenges of mobility and security threats are addressed, corresponding solutions are brought forth and numerically assessed . The proposed solution incorporates two key components: the Mobility Adaptive Gradient Descent (MAGD) and Time-evolving Anomaly Detection (TAD). The MAGD adapts the gradient descent algorithm to handle the configuration changes in cooperative localization systems, ensuring accurate localization in dynamic scenarios. The TAD cooperates with reputation propagation (RP) scheme to detect and mitigate potential attacks by identifying malicious data, enhancing the security and resilience of the cooperative localization.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IEEE Transactions on Wireless Communications"
    },
    {
        "paper id": "2402.09707",
        "abstract url": "https://arxiv.org/abs/2402.09707",
        "title": "On the adversarial robustness of Locality-Sensitive Hashing in Hamming space",
        "rating": "-10",
        "keywords": [],
        "abstract": "Locality-sensitive hashing~[Indyk,Motwani'98] is a classical data structure for approximate nearest neighbor search. It allows, after a close to linear time preprocessing of the input dataset, to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size. The resulting data structure is randomized and succeeds with high probability for every fixed query. In many modern applications of nearest neighbor search the queries are chosen adaptively. In this paper, we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space. We present a simple adversary that can, under mild assumptions on the initial point set, provably find a query to the approximate near neighbor search data structure that the data structure fails on. Crucially, our adaptive algorithm finds the hard query exponentially faster than random sampling.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09714",
        "abstract url": "https://arxiv.org/abs/2402.09714",
        "title": "An Accelerated Distributed Stochastic Gradient Method with Momentum",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we introduce an accelerated distributed stochastic gradient method with momentum for solving the distributed optimization problem, where a group of $n$ agents collaboratively minimize the average of the local objective functions over a connected network. The method, termed ``Distributed Stochastic Momentum Tracking (DSMT)'', is a single-loop algorithm that utilizes the momentum tracking technique as well as the Loopless Chebyshev Acceleration (LCA) method. We show that DSMT can asymptotically achieve comparable convergence rates as centralized stochastic gradient descent (SGD) method under a general variance condition regarding the stochastic gradients. Moreover, the number of iterations (transient times) required for DSMT to achieve such rates behaves as $\\mathcal{O}(n^{5/3}/(1-\u03bb))$ for minimizing general smooth objective functions, and $\\mathcal{O}(\\sqrt{n/(1-\u03bb)})$ under the Polyak-\u0141ojasiewicz (PL) condition. Here, the term $1-\u03bb$ denotes the spectral gap of the mixing matrix related to the underlying network topology. Notably, the obtained results do not rely on multiple inter-node communications or stochastic gradient accumulation per iteration, and the transient times are the shortest under the setting to the best of our knowledge.",
        "subjects": [
            "math.OC",
            "cs.DC",
            "cs.MA"
        ],
        "comment": "32 pages, 5 figures"
    },
    {
        "paper id": "2402.09736",
        "abstract url": "https://arxiv.org/abs/2402.09736",
        "title": "Federated Analytics-Empowered Frequent Pattern Mining for Decentralized Web 3.0 Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "The emerging Web 3.0 paradigm aims to decentralize existing web services, enabling desirable properties such as transparency, incentives, and privacy preservation. However, current Web 3.0 applications supported by blockchain infrastructure still cannot support complex data analytics tasks in a scalable and privacy-preserving way. This paper introduces the emerging federated analytics (FA) paradigm into the realm of Web 3.0 services, enabling data to stay local while still contributing to complex web analytics tasks in a privacy-preserving way. We propose FedWeb, a tailored FA design for important frequent pattern mining tasks in Web 3.0. FedWeb remarkably reduces the number of required participating data owners to support privacy-preserving Web 3.0 data analytics based on a novel distributed differential privacy technique. The correctness of mining results is guaranteed by a theoretically rigid candidate filtering scheme based on Hoeffding's inequality and Chebychev's inequality. Two response budget saving solutions are proposed to further reduce participating data owners. Experiments on three representative Web 3.0 scenarios show that FedWeb can improve data utility by ~25.3% and reduce the participating data owners by ~98.4%.",
        "subjects": [
            "cs.DC",
            "cs.CR"
        ],
        "comment": "Accepted by IEEE International Conference on Computer Communications (INFOCOM'24)"
    },
    {
        "paper id": "2402.09745",
        "abstract url": "https://arxiv.org/abs/2402.09745",
        "title": "WEFix: Intelligent Automatic Generation of Explicit Waits for Efficient Web End-to-End Flaky Tests",
        "rating": "-10",
        "keywords": [],
        "abstract": "Web end-to-end (e2e) testing evaluates the workflow of a web application. It simulates real-world user scenarios to ensure the application flows behave as expected. However, web e2e tests are notorious for being flaky, i.e., the tests can produce inconsistent results despite no changes to the code. One common type of flakiness is caused by nondeterministic execution orders between the test code and the client-side code under test. In particular, UI-based flakiness emerges as a notably prevalent and challenging issue to fix because the test code has limited knowledge about the client-side code execution. In this paper, we propose WEFix, a technique that can automatically generate fix code for UI-based flakiness in web e2e testing. The core of our approach is to leverage browser UI changes to predict the client-side code execution and generate proper wait oracles. We evaluate the effectiveness and efficiency of WEFix against 122 web e2e flaky tests from seven popular real-world projects. Our results show that WEFix dramatically reduces the overhead (from 3.7$\\times$ to 1.25$\\times$) while achieving a high correctness (98%).",
        "subjects": [
            "cs.SE"
        ],
        "comment": "8 pages. Accepted for publication in the proceedings of the ACM Web Conference 2024 (WWW 24)"
    },
    {
        "paper id": "2402.09757",
        "abstract url": "https://arxiv.org/abs/2402.09757",
        "title": "Construction of CCC and ZCCS Through Additive Characters Over Galois Field",
        "rating": "-10",
        "keywords": [],
        "abstract": "The rapid progression in wireless communication technologies, especially in multicarrier code-division multiple access (MC-CDMA), there is a need of advanced code construction methods. Traditional approaches, mainly based on generalized Boolean functions, have limitations in code length versatility. This paper introduces a novel approach to constructing complete complementary codes (CCC) and Z-complementary code sets (ZCCS), for reducing interference in MC-CDMA systems. The proposed construction, distinct from Boolean function-based approaches, employs additive characters over Galois fields GF($p^{r}$), where $p$ is prime and $r$ is a positive integer. First, we develop CCCs with lengths of $p^{r}$, which are then extended to construct ZCCS with both unreported lengths and sizes of $np^{r}$, where $n$ are arbitrary positive integers. The versatility of this method is further highlighted as it includes the lengths of ZCCS reported in prior studies as special cases, underscoring the method's comprehensive nature and superiority.",
        "subjects": [
            "cs.IT",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09776",
        "abstract url": "https://arxiv.org/abs/2402.09776",
        "title": "Strategic Vote Timing in Online Elections With Public Tallies",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the effect of public tallies on online elections, in a setting where voting is costly and voters are allowed to strategically time their votes. The strategic importance of choosing \\emph{when} to vote arises when votes are public, such as in online event scheduling polls (e.g., Doodle), or in blockchain governance mechanisms. In particular, there is a tension between voting early to influence future votes and waiting to observe interim results and avoid voting costs if the outcome has already been decided. Our study draws on empirical findings showing that \"temporal\" bandwagon effects occur when interim results are revealed to the electorate: late voters are more likely to vote for leading candidates. To capture this phenomenon, we analyze a novel model where the electorate consists of informed voters who have a preferred candidate, and uninformed swing voters who can be swayed according to the interim outcome at the time of voting. In our main results, we prove the existence of equilibria where both early and late voting occur with a positive probability, and we characterize conditions that lead to the appearance of \"last minute\" voting behavior, where all informed voters vote late.",
        "subjects": [
            "cs.GT",
            "cs.MA"
        ],
        "comment": "44 pages, 4 figures, 1 table"
    },
    {
        "paper id": "2402.09800",
        "abstract url": "https://arxiv.org/abs/2402.09800",
        "title": "Large-scale Benchmarking of Metaphor-based Optimization Heuristics",
        "rating": "-10",
        "keywords": [],
        "abstract": "The number of proposed iterative optimization heuristics is growing steadily, and with this growth, there have been many points of discussion within the wider community. One particular criticism that is raised towards many new algorithms is their focus on metaphors used to present the method, rather than emphasizing their potential algorithmic contributions. Several studies into popular metaphor-based algorithms have highlighted these problems, even showcasing algorithms that are functionally equivalent to older existing methods. Unfortunately, this detailed approach is not scalable to the whole set of metaphor-based algorithms. Because of this, we investigate ways in which benchmarking can shed light on these algorithms. To this end, we run a set of 294 algorithm implementations on the BBOB function suite. We investigate how the choice of the budget, the performance measure, or other aspects of experimental design impact the comparison of these algorithms. Our results emphasize why benchmarking is a key step in expanding our understanding of the algorithm space, and what challenges still need to be overcome to fully gauge the potential improvements to the state-of-the-art hiding behind the metaphors.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09809",
        "abstract url": "https://arxiv.org/abs/2402.09809",
        "title": "Effective and Scalable Math Support: Evidence on the Impact of an AI- Tutor on Math Achievement in Ghana",
        "rating": "-10",
        "keywords": [],
        "abstract": "This study evaluates the impact of Rori, an AI powered conversational math tutor accessible via WhatsApp, on the math performance of approximately 1,000 students in grades 3-9 across 11 schools in Ghana. Each school was assigned to a treatment group or control group; the students in the control group continued their regular math instruction, while students in the treatment group engaged with Rori, for two 30-minute sessions per week over 8 months in addition to regular math instruction. We find that the math growth scores were substantially higher for the treatment group with an effect size of 0.37, and that the results were statistically significant (p < 0.001). The fact that Rori works with basic mobile devices on low-bandwidth data networks gives the intervention strong potential to support personalized learning on other low-and-middle-income countries (LMICs), where laptop ownership and high-speed internet - prerequisite for many video-centered learning platforms - remain extremely limited. While the results should be interpreted judiciously, as they only report on year 1 of the intervention, and future research is necessary to better understand which conditions are necessary for successful implementation, they do suggest that chat-based tutoring solutions leveraging artificial intelligence could offer a costeffective approach to enhancing learning outcomes for millions of students globally.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09825",
        "abstract url": "https://arxiv.org/abs/2402.09825",
        "title": "Improved Lower Bounds for Approximating Parameterized Nearest Codeword and Related Problems under ETH",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper we present a new gap-creating randomized self-reduction for parameterized Maximum Likelihood Decoding problem over $\\mathbb{F}_p$ ($k$-MLD$_p$). The reduction takes a $k$-MLD$_p$ instance with $k\\cdot n$ vectors as input, runs in time $f(k)n^{O(1)}$ for some computable function $f$, outputs a $(3/2-\\varepsilon)$-Gap-$k'$-MLD$_p$ instance for any $\\varepsilon>0$, where $k'=O(k^2\\log k)$. Using this reduction, we show that assuming the randomized Exponential Time Hypothesis (ETH), no algorithms can approximate $k$-MLD$_p$ (and therefore its dual problem $k$-NCP$_p$) within factor $(3/2-\\varepsilon)$ in $f(k)\\cdot n^{o(\\sqrt{k/\\log k})}$ time for any $\\varepsilon>0$. We then use reduction by Bhattacharyya, Ghoshal, Karthik and Manurangsi (ICALP 2018) to amplify the $(3/2-\\varepsilon)$-gap to any constant. As a result, we show that assuming ETH, no algorithms can approximate $k$-NCP$_p$ and $k$-MDP$_p$ within $\u03b3$-factor in $f(k)n^{o(k^{\\varepsilon_\u03b3})}$ time for some constant $\\varepsilon_\u03b3>0$. Combining with the gap-preserving reduction by Bennett, Cheraghchi, Guruswami and Ribeiro (STOC 2023), we also obtain similar lower bounds for $k$-MDP$_p$, $k$-CVP$_p$ and $k$-SVP$_p$. These results improve upon the previous $f(k)n^{\u03a9(\\mathsf{poly} \\log k)}$ lower bounds for these problems under ETH using reductions by Bhattacharyya et al. (J.ACM 2021) and Bennett et al. (STOC 2023).",
        "subjects": [
            "cs.CC"
        ],
        "comment": "32 pages, 3 figures"
    },
    {
        "paper id": "2402.09854",
        "abstract url": "https://arxiv.org/abs/2402.09854",
        "title": "Improving the efficiency of GP-GOMEA for higher-arity operators",
        "rating": "-10",
        "keywords": [],
        "abstract": "Deploying machine learning models into sensitive domains in our society requires these models to be explainable. Genetic Programming (GP) can offer a way to evolve inherently interpretable expressions. GP-GOMEA is a form of GP that has been found particularly effective at evolving expressions that are accurate yet of limited size and, thus, promote interpretability. Despite this strength, a limitation of GP-GOMEA is template-based. This negatively affects its scalability regarding the arity of operators that can be used, since with increasing operator arity, an increasingly large part of the template tends to go unused. In this paper, we therefore propose two enhancements to GP-GOMEA: (i) semantic subtree inheritance, which performs additional variation steps that consider the semantic context of a subtree, and (ii) greedy child selection, which explicitly considers parts of the template that in standard GP-GOMEA remain unused. We compare different versions of GP-GOMEA regarding search enhancements on a set of continuous and discontinuous regression problems, with varying tree depths and operator sets. Experimental results show that both proposed search enhancements have a generally positive impact on the performance of GP-GOMEA, especially when the set of operators to choose from is large and contains higher-arity operators.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09860",
        "abstract url": "https://arxiv.org/abs/2402.09860",
        "title": "From Exploration to End of Life: Unpacking Sustainability in Physicalization Practices",
        "rating": "-10",
        "keywords": [],
        "abstract": "Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked. This work addresses this gap by investigating the interplay between sustainability and physicalization practices. We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability. Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle -- a framework that encompasses various stages in a physicalization's existence. Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices. We outline sustainability challenges and strategies based on participants' experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "To be published in CHI 2024"
    },
    {
        "paper id": "2402.09870",
        "abstract url": "https://arxiv.org/abs/2402.09870",
        "title": "Convex Equilibrium-Free Stability and Performance Analysis of Discrete-Time Nonlinear Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper considers the equilibrium-free stability and performance analysis of discrete-time nonlinear systems. We consider two types of equilibrium-free notions. Namely, the universal shifted concept, which considers stability and performance w.r.t. all equilibrium points of the system, and the incremental concept, which considers stability and performance between trajectories of the system. In this paper, we show how universal shifted stability and performance of discrete-time systems can be analyzed by making use of the time-difference dynamics. Moreover, we extend the existing results for incremental dissipativity for discrete-time systems based on dissipativity analysis of the differential dynamics to more general state-dependent storage functions for less conservative results. Finally, we show how both these equilibrium-free notions can be cast as a convex analysis problem by making use of the linear parameter-varying framework, which is also demonstrated by means of an example.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted to IET Control Theory & Applications"
    },
    {
        "paper id": "2402.09882",
        "abstract url": "https://arxiv.org/abs/2402.09882",
        "title": "Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cyber-Physical Production Systems (CPPSs), such as automated car manufacturing plants, execute a configurable sequence of production steps to manufacture products from a product portfolio. In CPPS engineering, domain experts start with manually determining feasible production step sequences and resources based on implicit knowledge. This process is hard to reproduce and highly inefficient. In this paper, we present the Extended Iterative Process Sequence Exploration (eIPSE) approach to derive variability models for products, processes, and resources from a domain-specific description. To automate the integrated exploration and configuration process for a CPPS, we provide a toolchain which automatically reduces the configuration space and allows to generate CPPS artifacts, such as control code for resources. We evaluate the approach with four real-world use cases, including the generation of control code artifacts, and an observational user study to collect feedback from engineers with different backgrounds. The results confirm the usefulness of the eIPSE approach and accompanying prototype to straightforwardly configure a desired CPPS.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "26 pages, 10 figures"
    },
    {
        "paper id": "2402.09896",
        "abstract url": "https://arxiv.org/abs/2402.09896",
        "title": "Two-Timescale Design for Active STAR-RIS Aided Massive MIMO Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Simultaneously transmitting and reflecting \\textcolor{black}{reconfigurable intelligent surface} (STAR-RIS) is a promising implementation of RIS-assisted systems that enables full-space coverage. However, STAR-RIS as well as conventional RIS suffer from the double-fading effect. Thus, in this paper, we propose the marriage of active RIS and STAR-RIS, denoted as ASTARS for massive multiple-input multiple-output (mMIMO) systems, and we focus on the energy splitting (ES) and mode switching (MS) protocols. Compared to prior literature, we consider the impact of correlated fading, and we rely our analysis on the two timescale protocol, being dependent on statistical channel state information (CSI). On this ground, we propose a channel estimation method for ASTARS with reduced overhead that accounts for its architecture. Next, we derive a \\textcolor{black}{closed-form expression} for the achievable sum-rate for both types of users in the transmission and reflection regions in a unified approach with significant practical advantages such as reduced complexity and overhead, which result in a lower number of required iterations for convergence compared to an alternating optimization (AO) approach. Notably, we maximize simultaneously the amplitudes, the phase shifts, and the active amplifying coefficients of the ASTARS by applying the projected gradient ascent method (PGAM). Remarkably, the proposed optimization can be executed at every several coherence intervals that reduces the processing burden considerably. Simulations corroborate the analytical results, provide insight into the effects of fundamental variables on the sum achievable SE, and present the superiority of 16 ASTARS compared to passive STAR-RIS for a practical number of surface elements.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "16 pages, accepted in IEEE TVT"
    },
    {
        "paper id": "2402.09898",
        "abstract url": "https://arxiv.org/abs/2402.09898",
        "title": "Asymptotic construction of locally repairable codes with multiple recovering sets",
        "rating": "-10",
        "keywords": [],
        "abstract": "Locally repairable codes have been extensively investigated due to practical applications in distributed and cloud storage systems in recent years. However, not much work on asymptotic behavior of locally repairable codes has been done. In particular, there is few result on constructive lower bound of asymptotic behavior of locally repairable codes with multiple recovering sets. In this paper, we construct some families of asymptotically good locally repairable codes with multiple recovering sets via automorphism groups of function fields of the Garcia-Stichtenoth towers. The main advantage of our construction is to allow more flexibility of localities.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09907",
        "abstract url": "https://arxiv.org/abs/2402.09907",
        "title": "On the convergence of Block Majorization-Minimization algorithms on the Grassmann Manifold",
        "rating": "-10",
        "keywords": [],
        "abstract": "The Majorization-Minimization (MM) framework is widely used to derive efficient algorithms for specific problems that require the optimization of a cost function (which can be convex or not). It is based on a sequential optimization of a surrogate function over closed convex sets. A natural extension of this framework incorporates ideas of Block Coordinate Descent (BCD) algorithms into the MM framework, also known as block MM. The rationale behind the block extension is to partition the optimization variables into several independent blocks, to obtain a surrogate for each block, and to optimize the surrogate of each block cyclically. The advantage of the block MM is that the construction and successive optimization of the surrogate functions is potentially easier than with the non-block alternative. The purpose of this letter is to exploit the geometrical properties of the Grassmann manifold (a non-convex set) for the purpose of extending classical convergence proofs of the block MM when at least one of the blocks is constrained in this manifold.",
        "subjects": [
            "math.OC",
            "eess.SP"
        ],
        "comment": "This work has been accepted to the IEEE Signal Processing letters. The IEEE is the copyright holder of this manuscript"
    },
    {
        "paper id": "2402.09912",
        "abstract url": "https://arxiv.org/abs/2402.09912",
        "title": "Efficient implementation of MPC for tracking using ADMM by decoupling its semi-banded structure",
        "rating": "-10",
        "keywords": [],
        "abstract": "Model Predictive Control (MPC) for tracking formulation presents numerous advantages compared to standard MPC, such as a larger domain of attraction and recursive feasibility even when abrupt changes in the reference are produced. As a drawback, it includes some extra decision variables in its related optimization problem, leading to a semi-banded structure that differs from the banded structure encountered in standard MPC. This semi-banded structure prevents the direct use of the efficient algorithms available for banded problems. To address this issue, we present an algorithm based on the alternating direction method of multipliers that explicitly takes advantage of the underlying semi-banded structure of the MPC for tracking.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09914",
        "abstract url": "https://arxiv.org/abs/2402.09914",
        "title": "Computing the EHZ capacity is NP-hard",
        "rating": "-10",
        "keywords": [],
        "abstract": "The Ekeland-Hofer-Zehnder capacity (EHZ capacity) is a fundamental symplectic invariant of convex bodies. We show that computing the EHZ capacity of polytopes is NP-hard. For this we reduce the feedback arc set problem in bipartite tournaments to computing the EHZ capacity of simplices.",
        "subjects": [
            "math.SG",
            "cs.CC",
            "math.CO"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2402.09925",
        "abstract url": "https://arxiv.org/abs/2402.09925",
        "title": "Characterizing Role Models in Software Practitioners' Career: An Interview Study",
        "rating": "-10",
        "keywords": [],
        "abstract": "A role model is a person who serves as an example for others to follow, especially in terms of values, behavior, achievements, and personal characteristics. In this paper, authors study how role models influence software practitioners careers, an aspect not studied in the literature before. By means of this study, authors aim to understand if there are any salient role model archetypes and what characteristics are valued by participants in their role models. To do so, authors use a thematic coding approach to analyze the data collected from interviewing ten Latin American software practitioners. Findings reveal that role models were perceived as sources of knowledge, yet the majority of participants, regardless of their career stage, displayed a stronger interest in the human side and the moral values that their role models embodied. This study also shows that any practitioner can be viewed as a role model.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "6 pages, 2 Tables. To appear in CHASE 2024: Proceedings of the 17th International Conference on Cooperative and Human Aspects of Software Engineering, April 14-15, 2024, Lisbon, Portugal"
    },
    {
        "paper id": "2402.09937",
        "abstract url": "https://arxiv.org/abs/2402.09937",
        "title": "A Systematic Evaluation of Evolving Highly Nonlinear Boolean Functions in Odd Sizes",
        "rating": "-10",
        "keywords": [],
        "abstract": "Boolean functions are mathematical objects used in diverse applications. Different applications also have different requirements, making the research on Boolean functions very active. In the last 30 years, evolutionary algorithms have been shown to be a strong option for evolving Boolean functions in different sizes and with different properties. Still, most of those works consider similar settings and provide results that are mostly interesting from the evolutionary algorithm's perspective. This work considers the problem of evolving highly nonlinear Boolean functions in odd sizes. While the problem formulation sounds simple, the problem is remarkably difficult, and the related work is extremely scarce. We consider three solutions encodings and four Boolean function sizes and run a detailed experimental analysis. Our results show that the problem is challenging, and finding optimal solutions is impossible except for the smallest tested size. However, once we added local search to the evolutionary algorithm, we managed to find a Boolean function in nine inputs with nonlinearity 241, which, to our knowledge, had never been accomplished before with evolutionary algorithms.",
        "subjects": [
            "cs.NE",
            "cs.CR"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2311.11881"
    },
    {
        "paper id": "2402.09951",
        "abstract url": "https://arxiv.org/abs/2402.09951",
        "title": "Strict width for Constraint Satisfaction Problems over homogeneous strucures of finite duality",
        "rating": "-10",
        "keywords": [],
        "abstract": "We investigate the `local consistency implies global consistency' principle of strict width among structures within the scope of the Bodirsky-Pinsker dichotomy conjecture for infinite-domain Constraint Satisfaction Problems (CSPs). Our main result implies that for certain CSP templates within the scope of that conjecture, having bounded strict width has a concrete consequence on the expressive power of the template called implicational simplicity. This in turn yields an explicit bound on the relational width of the CSP, i.e., the amount of local consistency needed to ensure the satisfiability of any instance. Our result applies to first-order expansions of any homogeneous $k$-uniform hypergraph, but more generally to any CSP template under the assumption of finite duality and general abstract conditions mainly on its automorphism group. In particular, it overcomes the restriction to binary signatures in the pioneering work of Wrona.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2402.09964",
        "abstract url": "https://arxiv.org/abs/2402.09964",
        "title": "Walsh-domain Neural Network for Power Amplifier Behavioral Modelling and Digital Predistortion",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper investigates the use of Neural Network (NN) nonlinear modelling for Power Amplifier (PA) linearization in the Walsh-Hadamard transceiver architecture. This novel architecture has recently been proposed for ultra-high bandwidth systems to reduce the transceiver power consumption by extensive parallelization of the digital baseband hardware. The parallelization is achieved by replacing two-dimensional quadrature modulation with multi-dimensional Walsh-Hadamard modulation. The open research question for this architecture is whether conventional baseband signal processing algorithms can be similarly parallelized while retaining their performance. A key baseband algorithm, digital predistortion using NN models for PA linearization, will be adapted to the parallel Walsh architecture. A straighforward parallelization of the state-of-the-art NN architecture is extended with a cross-domain Knowledge Distillation pre-training method to achieve linearization performance on par with the quadrature implementation. This result paves the way for the entire baseband processing chain to be adapted into ultra-high bandwidth, low-power Walsh transceivers.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted for presentation at the 2024 IEEE International Symposium on Circuits and Systems (ISCAS)"
    },
    {
        "paper id": "2402.09974",
        "abstract url": "https://arxiv.org/abs/2402.09974",
        "title": "Interference Mitigation for Network-Level ISAC: An Optimization Perspective",
        "rating": "-10",
        "keywords": [],
        "abstract": "Future wireless networks are envisioned to simultaneously provide high data-rate communication and ubiquitous environment-aware services for numerous users. One promising approach to meet this demand is to employ network-level integrated sensing and communications (ISAC) by jointly designing the signal processing and resource allocation over the entire network. However, to unleash the full potential of network-level ISAC, some critical challenges must be tackled. Among them, interference management is one of the most significant ones. In this article, we build up a bridge between interference mitigation techniques and the corresponding optimization methods, which facilitates efficient interference mitigation in network-level ISAC systems. In particular, we first identify several types of interference in network-level ISAC systems, including self-interference, mutual interference, crosstalk, clutter, and multiuser interference. Then, we present several promising techniques that can be utilized to suppress specific types of interference. For each type of interference, we discuss the corresponding problem formulation and identify the associated optimization methods. Moreover, to illustrate the effectiveness of the proposed interference mitigation techniques, two concrete network-level ISAC systems, namely coordinated cellular network-based and distributed antenna-based ISAC systems, are investigated from interference management perspective. Experiment results indicate that it is beneficial to collaboratively employ different interference mitigation techniques and leverage the network structure to achieve the full potential of network-level ISAC. Finally, we highlight several promising future research directions for the design of ISAC systems.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "7 pages, 6 figures, and the relevant simulation code can be found at https://dongfang-xu.github.io/homepage/code/Two_cases.zip"
    },
    {
        "paper id": "2402.09976",
        "abstract url": "https://arxiv.org/abs/2402.09976",
        "title": "Sensing-assisted Robust SWIPT for Mobile Energy Harvesting Receivers",
        "rating": "-10",
        "keywords": [],
        "abstract": "Simultaneous wireless information and power transfer (SWIPT) has been proposed to offer communication services and transfer power to the energy harvesting receiver (EHR) concurrently. However, existing works mainly focused on static EHRs, without considering the location uncertainty caused by the movement of EHRs and location estimation errors. To tackle this issue, this paper considers the sensing-assisted SWIPT design in a networked integrated sensing and communication (ISAC) system in the presence of location uncertainty. A two-phase robust design is proposed to reduce the location uncertainty and improve the power transfer efficiency. In particular, each time frame is divided into two phases, i.e., sensing and WPT phases, via time-splitting. The sensing phase performs collaborative sensing to localize the EHR, whose results are then utilized in the WPT phase for efficient WPT. To minimize the power consumption with given communication and power transfer requirements, a two-layer optimization framework is proposed to jointly optimize the time-splitting ratio, coordinated beamforming policy, and sensing node selection. Simulation results validate the effectiveness of the proposed design and demonstrate the existence of an optimal time-splitting ratio for given location uncertainty.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09983",
        "abstract url": "https://arxiv.org/abs/2402.09983",
        "title": "Optimistix: modular optimisation in JAX and Equinox",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce Optimistix: a nonlinear optimisation library built in JAX and Equinox. Optimistix introduces a novel, modular approach for its minimisers and least-squares solvers. This modularity relies on new practical abstractions for optimisation which we call search and descent, and which generalise classical notions of line search, trust-region, and learning-rate algorithms. It provides high-level APIs and solvers for minimisation, nonlinear least-squares, root-finding, and fixed-point iteration. Optimistix is available at https://github.com/patrick-kidger/optimistix.",
        "subjects": [
            "math.OC",
            "cs.MS"
        ],
        "comment": "8 pages, 4 figures, 2 tables"
    },
    {
        "paper id": "2402.09993",
        "abstract url": "https://arxiv.org/abs/2402.09993",
        "title": "Scalability limitations of Kademlia DHTs when enabling Data Availability Sampling in Ethereum",
        "rating": "-10",
        "keywords": [],
        "abstract": "Scalability in blockchain remains a significant challenge, especially when prioritizing decentralization and security. The Ethereum community has proposed comprehensive data-sharding techniques to overcome storage, computational, and network processing limitations. In this context, the propagation and availability of large blocks become the subject of research to achieve scalable data-sharding. This paper provides insights after exploring the usage of a Kademlia-based DHT to enable Data Availability Sampling (DAS) in Ethereum. It presents a DAS-DHT simulator to study this problem and validates the results of the simulator with experiments in a real DHT network, IPFS. Our results help us understand what parts of DAS can be achieved based on existing Kademlia DHT solutions and which ones cannot. We discuss the limitations of DHT solutions and discuss other alternatives.",
        "subjects": [
            "cs.NI",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09994",
        "abstract url": "https://arxiv.org/abs/2402.09994",
        "title": "Approximating Competitive Equilibrium by Nash Welfare",
        "rating": "-10",
        "keywords": [],
        "abstract": "We explore the relationship between two popular concepts on allocating divisible items: competitive equilibrium (CE) and allocations with maximum Nash welfare, i.e., allocations where the weighted geometric mean of the utilities is maximal. When agents have homogeneous concave utility functions, these two concepts coincide: the classical Eisenberg-Gale convex program that maximizes Nash welfare over feasible allocations yields a competitive equilibrium. However, these two concepts diverge for non-homogeneous utilities. From a computational perspective, maximizing Nash welfare amounts to solving a convex program for any concave utility functions, computing CE becomes PPAD-hard already for separable piecewise linear concave (SPLC) utilities. We introduce the concept of Gale-substitute utility functions, an analogue of the weak gross substitutes (WGS) property for the so-called Gale demand system. For Gale-substitutes utilities, we show that any allocation maximizing Nash welfare provides an approximate-CE with surprisingly strong guarantees, where every agent gets at least half the maximum utility they can get at any CE, and is approximately envy-free. Gale-substitutes include examples of utilities where computing CE is PPAD hard: in particular, all separable concave utilities, and the previously studied non-separable class of Leontief-free utilities. We introduce a new, general class of utility functions called generalized network utilities based on the generalized flow model; this class includes SPLC and Leontief-free utilities. We show that all such utilities are Gale-substitutes. Conversely, although some agents may get much higher utility at a Nash welfare maximizing allocation than at a CE, we show a price of anarchy type result: for general concave utilities, every CE achieves at least $(1/e)^{1/e} > 0.69$ fraction of the maximum Nash welfare, and this factor is tight.",
        "subjects": [
            "cs.GT",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.09995",
        "abstract url": "https://arxiv.org/abs/2402.09995",
        "title": "iJTyper: An Iterative Type Inference Framework for Java by Integrating Constraint- and Statistically-based Methods",
        "rating": "-10",
        "keywords": [],
        "abstract": "Inferring the types of API elements in incomplete code snippets (e.g., those on Q&A forums) is a prepositive step required to work with the code snippets. Existing type inference methods can be mainly categorized as constraint-based or statistically-based. The former imposes higher requirements on code syntax and often suffers from low recall due to the syntactic limitation of code snippets. The latter relies on the statistical regularities learned from a training corpus and does not take full advantage of the type constraints in code snippets, which may lead to low precision. In this paper, we propose an iterative type inference framework for Java, called iJTyper, by integrating the strengths of both constraint- and statistically-based methods. For a code snippet, iJTyper first applies a constraint-based method and augments the code context with the inferred types of API elements. iJTyper then applies a statistically-based method to the augmented code snippet. The predicted candidate types of API elements are further used to improve the constraint-based method by reducing its pre-built knowledge base. iJTyper iteratively executes both methods and performs code context augmentation and knowledge base reduction until a termination condition is satisfied. Finally, the final inference results are obtained by combining the results of both methods. We evaluated iJTyper on two open-source datasets. Results show that 1) iJTyper achieves high average precision/recall of 97.31% and 92.52% on both datasets; 2) iJTyper significantly improves the recall of two state-of-the-art baselines, SnR and MLMTyper, by at least 7.31% and 27.44%, respectively; and 3) iJTyper improves the average precision/recall of the popular language model, ChatGPT, by 3.25% and 0.51% on both datasets.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10012",
        "abstract url": "https://arxiv.org/abs/2402.10012",
        "title": "Countably Colorful Hyperplane Transversal",
        "rating": "-10",
        "keywords": [],
        "abstract": "Let $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ be an infinite sequence of families of compact connected sets in $\\mathbb{R}^{d}$. An infinite sequence of compact connected sets $\\left\\{ B_{n} \\right\\}_{n\\in \\mathbb{N}}$ is called heterochromatic sequence from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ if there exists an infinite sequence $\\left\\{ i_{n} \\right\\}_{n\\in \\mathbb{N}}$ of natural numbers satisfying the following two properties: (a) $\\{i_{n}\\}_{n\\in \\mathbb{N}}$ is a monotonically increasing sequence, and (b) for all $n \\in \\mathbb{N}$, we have $B_{n} \\in \\mathcal{F}_{i_n}$. We show that if every heterochromatic sequence from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ contains $d+1$ sets that can be pierced by a single hyperplane then there exists a finite collection $\\mathcal{H}$ of hyperplanes from $\\mathbb{R}^{d}$ that pierces all but finitely many families from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$. As a direct consequence of our result, we get that if every countable subcollection from an infinite family $\\mathcal{F}$ of compact connected sets in $\\mathbb{R}^{d}$ contains $d+1$ sets that can be pierced by a single hyperplane then $\\mathcal{F}$ can be pierced by finitely many hyperplanes. To establish the optimality of our result we show that, for all $d \\in \\mathbb{N}$, there exists an infinite sequence $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ of families of compact connected sets satisfying the following two conditions: (1) for all $n \\in \\mathbb{N}$, $\\mathcal{F}_{n}$ is not pierceable by finitely many hyperplanes, and (2) for any $m \\in \\mathbb{N}$ and every sequence $\\left\\{B_n\\right\\}_{n=m}^{\\infty}$ of compact connected sets in $\\mathbb{R}^d$, where $B_i\\in\\mathcal{F}_i$ for all $i \\geq m$, there exists a hyperplane in $\\mathbb{R}^d$ that pierces at least $d+1$ sets in the sequence.",
        "subjects": [
            "math.CO",
            "cs.CG"
        ],
        "comment": "23 pages, 7 figures"
    },
    {
        "paper id": "2402.10015",
        "abstract url": "https://arxiv.org/abs/2402.10015",
        "title": "A Piecewise Approach for the Analysis of Exact Algorithms",
        "rating": "-10",
        "keywords": [],
        "abstract": "To analyze the worst-case running time of branching algorithms, the majority of work in exponential time algorithms focuses on designing complicated branching rules over developing better analysis methods for simple algorithms. In the mid-$2000$s, Fomin et al. [2005] introduced measure & conquer, an advanced general analysis method, sparking widespread adoption for obtaining tighter worst-case running time upper bounds for many fundamental NP-complete problems. Yet, much potential in this direction remains untapped, as most subsequent work applied it without further advancement. Motivated by this, we present piecewise analysis, a new general method that analyzes the running time of branching algorithms. Our approach is to define a similarity ratio that divides instances into groups and then analyze the running time within each group separately. The similarity ratio is a scale between two parameters of an instance I. Instead of relying on a single measure and a single analysis for the whole instance space, our method allows to take advantage of different intrinsic properties of instances with different similarity ratios. To showcase its potential, we reanalyze two $17$-year-old algorithms from Fomin et al. [2007] that solve $4$-Coloring and #$3$-Coloring respectively. The original analysis in their paper gave running times of $O(1.7272^n)$ and $O(1.6262^n)$ respectively for these algorithms, our analysis improves these running times to $O(1.7215^n)$ and $O(1.6232^n)$. Among the two improvements, our new running time $O(1.7215^n)$ is the first improvement in the best known running time for the 4-Coloring problem since 2007.",
        "subjects": [
            "cs.DS",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10018",
        "abstract url": "https://arxiv.org/abs/2402.10018",
        "title": "Multi-Stage Algorithm for Group Testing with Prior Statistics",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we propose an efficient multi-stage algorithm for non-adaptive Group Testing (GT) with general correlated prior statistics. The proposed solution can be applied to any correlated statistical prior represented in trellis, e.g., finite state machines and Markov processes. We introduce a variation of List Viterbi Algorithm (LVA) to enable accurate recovery using much fewer tests than objectives, which efficiently gains from the correlated prior statistics structure. Our numerical results demonstrate that the proposed Multi-Stage GT (MSGT) algorithm can obtain the optimal Maximum A Posteriori (MAP) performance with feasible complexity in practical regimes, such as with COVID-19 and sparse signal recovery applications, and reduce in the scenarios tested the number of pooled tests by at least $25\\%$ compared to existing classical low complexity GT algorithms. Moreover, we analytically characterize the complexity of the proposed MSGT algorithm that guarantees its efficiency.",
        "subjects": [
            "cs.IT",
            "q-bio.QM",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10025",
        "abstract url": "https://arxiv.org/abs/2402.10025",
        "title": "An improved lower bound on the Shannon capacities of complements of odd cycles",
        "rating": "-10",
        "keywords": [],
        "abstract": "Improving a 2003 result of Bohman and Holzman, we show that for $n \\geq 1$, the Shannon capacity of the complement of the $2n+1$-cycle is at least $(2^{r_n} + 1)^{1/r_n} = 2 + \u03a9(2^{-r_n}/r_n)$, where $r_n = \\exp(O((\\log n)^2))$ is the number of partitions of $2(n-1)$ into powers of $2$.",
        "subjects": [
            "math.CO",
            "cs.IT"
        ],
        "comment": "5 pages, 1 figure"
    },
    {
        "paper id": "2402.10032",
        "abstract url": "https://arxiv.org/abs/2402.10032",
        "title": "Dimension-free Structured Covariance Estimation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Given a sample of i.i.d. high-dimensional centered random vectors, we consider a problem of estimation of their covariance matrix $\u03a3$ with an additional assumption that $\u03a3$ can be represented as a sum of a few Kronecker products of smaller matrices. Under mild conditions, we derive the first non-asymptotic dimension-free high-probability bound on the Frobenius distance between $\u03a3$ and a widely used penalized permuted least squares estimate. Because of the hidden structure, the established rate of convergence is faster than in the standard covariance estimation problem.",
        "subjects": [
            "math.ST",
            "eess.SP"
        ],
        "comment": "30 pages"
    },
    {
        "paper id": "2402.10059",
        "abstract url": "https://arxiv.org/abs/2402.10059",
        "title": "Partial Synchrony for Free? New Upper Bounds for Byzantine Agreement",
        "rating": "-10",
        "keywords": [],
        "abstract": "Byzantine agreement allows n processes to decide on a common value, in spite of arbitrary failures. The seminal Dolev-Reischuk bound states that any deterministic solution to Byzantine agreement exchanges Omega(n^2) bits. In synchronous networks, solutions with optimal O(n^2) bit complexity, optimal fault tolerance, and no cryptography have been established for over three decades. However, these solutions lack robustness under adverse network conditions. Therefore, research has increasingly focused on Byzantine agreement for partially synchronous networks. Numerous solutions have been proposed for the partially synchronous setting. However, these solutions are notoriously hard to prove correct, and the most efficient cryptography-free algorithms still require O(n^3) exchanged bits in the worst case. In this paper, we introduce Oper, the first generic transformation of deterministic Byzantine agreement algorithms from synchrony to partial synchrony. Oper requires no cryptography, is optimally resilient (n >= 3t+1, where t is the maximum number of failures), and preserves the worst-case per-process bit complexity of the transformed synchronous algorithm. Leveraging Oper, we present the first partially synchronous Byzantine agreement algorithm that (1) achieves optimal O(n^2) bit complexity, (2) requires no cryptography, and (3) is optimally resilient (n >= 3t+1), thus showing that the Dolev-Reischuk bound is tight even in partial synchrony. Moreover, we adapt Oper for long values and obtain several new partially synchronous algorithms with improved complexity and weaker (or completely absent) cryptographic assumptions.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10080",
        "abstract url": "https://arxiv.org/abs/2402.10080",
        "title": "Temporal hierarchies of regular languages",
        "rating": "-10",
        "keywords": [],
        "abstract": "We classify the regular languages using an operator $\\mathcal{C}\\mapsto TL(\\mathcal{C})$. For each input class of languages $\\mathcal{C}$, it builds a larger class $TL(\\mathcal{C})$ consisting of all languages definable in a variant of unary temporal logic whose future/past modalities depend on $\\mathcal{C}$. This defines the temporal hierarchy of basis $\\mathcal{C}$: level $n$ is built by applying this operator $n$ times to $\\mathcal{C}$. This hierarchy is closely related to another one, the concatenation hierarchy of basis $\\mathcal{C}$. In particular, the union of all levels in both hierarchies is the same. We focus on bases $\\mathcal{G}$ of group languages and natural extensions thereof, denoted $\\mathcal{G}^+$. We prove that the temporal hierarchies of bases $\\mathcal{G}$ and $\\mathcal{G}^+$ are strictly intertwined, and we compare them to the corresponding concatenation hierarchies. Furthermore, we look at two standard problems on classes of languages: membership (decide if an input language is in the class) and separation (decide, for two input regular languages $L_1,L_2$, if there is a language $K$ in the class with $L_1 \\subseteq K$ and $L_2 \\cap K = \\emptyset$). We prove that if separation is decidable for $\\mathcal{G}$, then so is membership for level two in the temporal hierarchies of bases $\\mathcal{G}$ and $\\mathcal{G}^+$. Moreover, we take a closer look at the case where $\\mathcal{G}$ is the trivial class $ST=\\{\\emptyset,A^*\\}$. The levels one in the hierarchies of bases $ST$ and $ST^+$ are the standard variants of unary temporal logic while the levels two were considered recently using alternate definitions. We prove that for these two bases, level two has decidable separation. Combined with earlier results about the operator $\\mathcal{G}\\mapsto TL(\\mathcal{G})$, this implies that the levels three have decidable membership.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10122",
        "abstract url": "https://arxiv.org/abs/2402.10122",
        "title": "Mitigating subjectivity and bias in AI development indices: A robust approach to redefining country rankings",
        "rating": "-10",
        "keywords": [],
        "abstract": "Countries worldwide have been implementing different actions national strategies for Artificial Intelligence (AI) to shape policy priorities and guide their development concerning AI. Several AI indices have emerged to assess countries' progress in AI development, aiding decision-making on investments and policy choices. Typically, these indices combine multiple indicators using linear additive methods such as weighted sums, although they are limited in their ability to account for interactions among indicators. Another limitation concerns the use of deterministic weights, which can be perceived as subjective and vulnerable to debate and scrutiny, especially by nations that feel disadvantaged. Aiming at mitigating these problems, we conduct a methodological analysis to derive AI indices based on multiple criteria decision analysis. Initially, we assess correlations between different AI dimensions and employ the Choquet integral to model them. Thus, we apply the Stochastic Multicriteria Acceptability Analysis (SMAA) to conduct a sensitivity analysis using both weighted sum and Choquet integral in order to evaluate the stability of the indices with regard the weights. Finally, we introduce a novel ranking methodology based on SMAA, which considers several sets of weights to derive the ranking of countries. As a result, instead of using predefined weights, in the proposed approach, the ranking is achieved based on the probabilities of countries in occupying a specific position. In the computational analysis, we utilize the data employed in The Global AI Index proposed by Tortoise. Results reveal correlations in the data, and our approach effectively mitigates bias. In the sensitivity analysis, we scrutinize changes in the ranking resulting from weight adjustments. We demonstrate that our proposal rankings closely align with those derived from weight variations, proving to be more robust.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10139",
        "abstract url": "https://arxiv.org/abs/2402.10139",
        "title": "Fast interpolation and multiplication of unbalanced polynomials",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the classical problems of interpolating a polynomial given a black box for evaluation, and of multiplying two polynomials, in the setting where the bit-lengths of the coefficients may vary widely, so-called unbalanced polynomials. Writing s for the total bit-length and D for the degree, our new algorithms have expected running time $\\tilde{O}(s \\log D)$, whereas previous methods for (resp.) dense or sparse arithmetic have at least $\\tilde{O}(sD)$ or $\\tilde{O}(s^2)$ bit complexity.",
        "subjects": [
            "cs.SC",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10157",
        "abstract url": "https://arxiv.org/abs/2402.10157",
        "title": "Revisiting Stochastic Realization Theory using Functional It\u00f4 Calculus",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper considers the problem of constructing finite-dimensional state space realizations for stochastic processes that can be represented as the outputs of a certain type of a causal system driven by a continuous semimartingale input process. The main assumption is that the output process is infinitely differentiable, where the notion of differentiability comes from the functional It\u00f4 calculus introduced by Dupire as a causal (nonanticipative) counterpart to Malliavin's stochastic calculus of variations. The proposed approach builds on the ideas of Hijab, who had considered the case of processes driven by a Brownian motion, and makes contact with the realization theory of deterministic systems based on formal power series and Chen-Fliess functional expansions.",
        "subjects": [
            "math.OC",
            "eess.SY",
            "math.PR"
        ],
        "comment": "16 pages; submitted to MTNS 2024"
    },
    {
        "paper id": "2402.10158",
        "abstract url": "https://arxiv.org/abs/2402.10158",
        "title": "InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10163",
        "abstract url": "https://arxiv.org/abs/2402.10163",
        "title": "Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage. In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory, study its properties, and its real world implications in AI. The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference. Instead, the model stores data as waves that is updated by the wave's boundary conditions. We rigorously examine the model's capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems. The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem. To understand the model's real-world applicability, we explore two cases: linear boundary condition (LBC) and non-linear, self-attention-driven boundary condition (SBC). The model with the linear boundary condition results in a shift matrix plus low-rank matrix currently used in H3 state space RNN. Further, our experiments with LBC reveal that this matrix is effectively learned by Recurrent Neural Networks (RNNs) through backpropagation when modeling history-dependent dynamical systems. Conversely, the SBC parallels the autoregressive loop of an attention-only transformer with the context vector representing the wave substrate. Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10174",
        "abstract url": "https://arxiv.org/abs/2402.10174",
        "title": "Overapproximation of Non-Linear Integer Arithmetic for Smart Contract Verification",
        "rating": "-10",
        "keywords": [],
        "abstract": "The need to solve non-linear arithmetic constraints presents a major obstacle to the automatic verification of smart contracts. In this case study we focus on the two overapproximation techniques used by the industry verification tool Certora Prover: overapproximation of non-linear integer arithmetic using linear integer arithmetic and using non-linear real arithmetic. We compare the performance of contemporary SMT solvers on verification conditions produced by the Certora Prover using these two approximations against the natural non-linear integer arithmetic encoding. Our evaluation shows that the use of the overapproximation methods leads to solving a significant number of new problems.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "13 pages, 2 figures, presented at The International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR) 2023"
    },
    {
        "paper id": "2402.10182",
        "abstract url": "https://arxiv.org/abs/2402.10182",
        "title": "Intent Demonstration in General-Sum Dynamic Games via Iterative Linear-Quadratic Approximations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Autonomous agents should be able to coordinate with other agents without knowing their intents ahead of time. While prior work has studied how agents can gather information about the intent of others, in this work we study the inverse problem: how agents can demonstrate their intent to others, within the framework of general-sum dynamic games. We first present a model of this intent demonstration problem and then propose an algorithm that enables an agent to trade off their task performance and intent demonstration to improve the overall system's performance. To scale to continuous states and action spaces as well as to nonlinear dynamics and costs, our algorithm leverages linear-quadratic approximations with an efficient intent teaching guarantee. Our empirical results show that intent demonstration accelerates other agents' learning and enables the demonstrating agent to balance task performance with intent expression.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Under review by L4DC 2024"
    },
    {
        "paper id": "2402.10293",
        "abstract url": "https://arxiv.org/abs/2402.10293",
        "title": "Parallel Play Saves Quantifiers",
        "rating": "-10",
        "keywords": [],
        "abstract": "The number of quantifiers needed to express first-order properties is captured by two-player combinatorial games called multi-structural (MS) games. We play these games on linear orders and strings, and introduce a technique we call \"parallel play\", that dramatically reduces the number of quantifiers needed in many cases. Linear orders and strings are the most basic representatives of ordered structures -- a class of structures that has historically been notoriously difficult to analyze. Yet, in this paper, we provide upper bounds on the number of quantifiers needed to characterize different-sized subsets of these structures, and prove that they are tight up to constant factors, including, in some cases, up to a factor of $1+\\varepsilon$, for arbitrarily small $\\varepsilon$.",
        "subjects": [
            "cs.LO",
            "cs.CC"
        ],
        "comment": "24 pages, 4 figures"
    },
    {
        "paper id": "2402.10298",
        "abstract url": "https://arxiv.org/abs/2402.10298",
        "title": "Streaming algorithm for balance gain and cost with cardinality constraint on the integer lattice",
        "rating": "-10",
        "keywords": [],
        "abstract": "Team formation problem is a very important problem in the labor market, and it is proved to be NP-hard. In this paper, we design an efficient bicriteria streaming algorithms to construct a balance between gain and cost in a team formation problem with cardinality constraint on the integer lattice. To solve this problem, we establish a model for maximizing the difference between a nonnegative normalized monotone submodule function and a nonnegative linear function. Further, we discuss the case where the first function of the object function is $\u03b1$--weakly submodular. Combining the lattice binary search with the threshold method, we present an online algorithm called bicriteria streaming algorithms. Meanwhile, we give detailed analysis for both of these models.",
        "subjects": [
            "cs.DS",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10305",
        "abstract url": "https://arxiv.org/abs/2402.10305",
        "title": "Effective module lattices and their shortest vectors",
        "rating": "-10",
        "keywords": [],
        "abstract": "We prove tight probabilistic bounds for the shortest vectors in module lattices over number fields using the results of arXiv:2308.15275. Moreover, establishing asymptotic formulae for counts of fixed rank matrices with algebraic integer entries and bounded Euclidean length, we prove an approximate Rogers integral formula for discrete sets of module lattices obtained from lifts of algebraic codes. This in turn implies that the moment estimates of arXiv:2308.15275 as well as the aforementioned bounds on the shortest vector also carry through for large enough discrete sets of module lattices.",
        "subjects": [
            "math.NT",
            "cs.IT"
        ],
        "comment": "21 pages"
    },
    {
        "paper id": "2402.10318",
        "abstract url": "https://arxiv.org/abs/2402.10318",
        "title": "Multi-Antenna Towards Inband Shift Keying",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose a new continuous phase frequency shift keying that is particularly suited for multi-antenna communications when the link budget is critical and beam alignment is problematic. It combines the constant envelope of frequency modulation with low-rate repetition coding in order to compensate for the absence of transmit beamforming. Although it is a frequency modulation, its transmit signal shows close to rectangular spectral shape. Similar to GSM's Gaussian minimum shift keying, it can be well approximated by linear modulation, when combined with differential precoding. This allows for easy coherent demodulation by means of a windowed fast Fourier transform.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "The initial version of this paper contains an error. It calculates the beamforming gain of transmit beamforming for N antennas as N, but it should be N^2. This error has been corrected in the latest version"
    },
    {
        "paper id": "2402.10352",
        "abstract url": "https://arxiv.org/abs/2402.10352",
        "title": "Subspace Tracking with Dynamical Models on the Grassmannian",
        "rating": "-10",
        "keywords": [],
        "abstract": "Tracking signals in dynamic environments presents difficulties in both analysis and implementation. In this work, we expand on a class of subspace tracking algorithms which utilize the Grassmann manifold -- the set of linear subspaces of a high-dimensional vector space. We design regularized least squares algorithms based on common manifold operations and intuitive dynamical models. We demonstrate the efficacy of the approach for a narrowband beamforming scenario, where the dynamics of multiple signals of interest are captured by motion on the Grassmannian.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IEEE SAM 2024. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2402.10382",
        "abstract url": "https://arxiv.org/abs/2402.10382",
        "title": "Making Short-Form Videos Accessible with Hierarchical Video Summaries",
        "rating": "-10",
        "keywords": [],
        "abstract": "Short videos on platforms such as TikTok, Instagram Reels, and YouTube Shorts (i.e. short-form videos) have become a primary source of information and entertainment. Many short-form videos are inaccessible to blind and low vision (BLV) viewers due to their rapid visual changes, on-screen text, and music or meme-audio overlays. In our formative study, 7 BLV viewers who regularly watched short-form videos reported frequently skipping such inaccessible content. We present ShortScribe, a system that provides hierarchical visual summaries of short-form videos at three levels of detail to support BLV viewers in selecting and understanding short-form videos. ShortScribe allows BLV users to navigate between video descriptions based on their level of interest. To evaluate ShortScribe, we assessed description accuracy and conducted a user study with 10 BLV participants comparing ShortScribe to a baseline interface. When using ShortScribe, participants reported higher comprehension and provided more accurate summaries of video content.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "To appear at CHI 2024"
    },
    {
        "paper id": "2402.10395",
        "abstract url": "https://arxiv.org/abs/2402.10395",
        "title": "Assessing the Performance of OpenTitan as Cryptographic Accelerator in Secure Open-Hardware System-on-Chips",
        "rating": "-10",
        "keywords": [],
        "abstract": "RISC-V open-source systems are emerging in deployment scenarios where safety and security are critical. OpenTitan is an open-source silicon root-of-trust designed to be deployed in a wide range of systems, from high-end to deeply embedded secure environments. Despite the availability of various cryptographic hardware accelerators that make OpenTitan suitable for offloading cryptographic workloads from the main processor, there has been no accurate and quantitative establishment of the benefits derived from using OpenTitan as a secure accelerator. This paper addresses this gap by thoroughly analysing strengths and inefficiencies when offloading cryptographic workloads to OpenTitan. The focus is on three key IPs - HMAC, AES, and OpenTitan Big Number accelerator (OTBN) - which can accelerate four security workloads: Secure Hash Functions, Message Authentication Codes, Symmetric cryptography, and Asymmetric cryptography. For every workload, we develop a bare-metal driver for the OpenTitan accelerator and analyze its efficiency when computation is offloaded from a RISC-V application core within a System-on-Chip designed for secure Cyber-Physical Systems applications. Finally, we assess it against a software implementation on the application core. The characterization was conducted on a cycle-accurate RTL simulator of the System-on-Chip (SoC). Our study demonstrates that OpenTitan significantly outperforms software implementations, with speedups ranging from 4.3x to 12.5x. However, there is potential for even greater gains as the current OpenTitan utilizes a fraction of the accelerator bandwidths, which ranges from 16% to 61%, depending on the memory being accessed and the accelerator used. Our results open the way to the optimization of OpenTitan-based secure platforms, providing design guidelines to unlock the full potential of its accelerators in secure applications.",
        "subjects": [
            "cs.CR",
            "cs.PF"
        ],
        "comment": "8 pages, 2 figures, accepted at CF'24 conference, pre camera-ready version"
    },
    {
        "paper id": "2402.10398",
        "abstract url": "https://arxiv.org/abs/2402.10398",
        "title": "Prompt Learning for Multi-Label Code Smell Detection: A Promising Approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "Code smells indicate the potential problems of software quality so that developers can identify refactoring opportunities by detecting code smells. State-of-the-art approaches leverage heuristics, machine learning, and deep learning to detect code smells. However, existing approaches have not fully explored the potential of large language models (LLMs). In this paper, we propose \\textit{PromptSmell}, a novel approach based on prompt learning for detecting multi-label code smell. Firstly, code snippets are acquired by traversing abstract syntax trees. Combined code snippets with natural language prompts and mask tokens, \\textit{PromptSmell} constructs the input of LLMs. Secondly, to detect multi-label code smell, we leverage a label combination approach by converting a multi-label problem into a multi-classification problem. A customized answer space is added to the word list of pre-trained language models, and the probability distribution of intermediate answers is obtained by predicting the words at the mask positions. Finally, the intermediate answers are mapped to the target class labels by a verbalizer as the final classification result. We evaluate the effectiveness of \\textit{PromptSmell} by answering six research questions. The experimental results demonstrate that \\textit{PromptSmell} obtains an improvement of 11.17\\% in $precision_{w}$ and 7.4\\% in $F1_{w}$ compared to existing approaches.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10402",
        "abstract url": "https://arxiv.org/abs/2402.10402",
        "title": "Non-convex optimization problems for maximum hands-off control",
        "rating": "-10",
        "keywords": [],
        "abstract": "The maximum hands-off control is the optimal solution to the L0 optimal control problem. It has the minimum support length among all feasible control inputs. To avoid computational difficulties arising from its combinatorial nature, the convex approximation method that replaces the L0 norm by the L1 norm in the cost function has been employed on standard. However, this approximation method does not necessarily obtain the maximum hands-off control. In response to this limitation, this paper newly introduces a non-convex approximation method and formulates a class of non-convex optimal control problems that are always equivalent to the maximum hands-off control problem. Based on the results, this paper describes the computation method that quotes algorithms designed for the difference of convex functions optimization. Finally, this paper confirms the effectiveness of the non-convex approximation method with a numerical example.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2402.10419",
        "abstract url": "https://arxiv.org/abs/2402.10419",
        "title": "Path Loss Modeling for RIS-Assisted Wireless System with Direct Link and Elevation Factors",
        "rating": "-10",
        "keywords": [],
        "abstract": "The present path loss models for wireless systems employing reconfigurable intelligent surfaces (RIS) do not account for the elevation of the transmitter, receiver, and RIS module. In this paper, we develop an analytical model for path loss of a wireless system utilizing an NxM-element RIS module positioned above the ground surface with elevated transmitter and receiver configurations. Furthermore, we integrate the direct link into the path loss model to enhance its applicability, a crucial aspect often neglected in previous research. We also present simplified analytical expressions for path loss under various configurations, including near-field and far-field scenarios. These expressions elucidate the impact of elevation factors on path loss, facilitating more accurate signal quality estimation at the receiver. Simulation results corroborate that accounting for elevated RIS modules and transceiver units can yield improved deployment strategies for RIS-based wireless systems.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This work has been submitted to IEEE for possible publcation"
    },
    {
        "paper id": "2402.10439",
        "abstract url": "https://arxiv.org/abs/2402.10439",
        "title": "Competitive Equilibrium for Chores: from Dual Eisenberg-Gale to a Fast, Greedy, LP-based Algorithm",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the computation of competitive equilibrium for Fisher markets with $n$ agents and $m$ divisible chores. Prior work showed that competitive equilibria correspond to the nonzero KKT points of a non-convex analogue of the Eisenberg-Gale convex program. We introduce an analogue of the Eisenberg-Gale dual for chores: we show that all KKT points of this dual correspond to competitive equilibria, and while it is not a dual of the non-convex primal program in a formal sense, the objectives touch at all KKT points. Similar to the primal, the dual has problems from an optimization perspective: there are many feasible directions where the objective tends to positive infinity. We then derive a new constraint for the dual, which restricts optimization to a hyperplane that avoids all these directions. We show that restriction to this hyperplane retains all KKT points, and surprisingly, does not introduce any new ones. This allows, for the first time ever, application of iterative optimization methods over a convex region for computing competitive equilibria for chores. We next introduce a greedy Frank-Wolfe algorithm for optimization over our program and show a state-of-the-art convergence rate to competitive equilibrium. In the case of equal incomes, we show a $\\mathcal{\\tilde O}(n/\u03b5^2)$ rate of convergence, which improves over the two prior state-of-the-art rates of $\\mathcal{\\tilde O}(n^3/\u03b5^2)$ for an exterior-point method and $\\mathcal{\\tilde O}(nm/\u03b5^2)$ for a combinatorial method. Moreover, our method is significantly simpler: each iteration of our method only requires solving a simple linear program. We show through numerical experiments on simulated data and a paper review bidding dataset that our method is extremely practical. This is the first highly practical method for solving competitive equilibrium for Fisher markets with chores.",
        "subjects": [
            "cs.GT",
            "math.OC"
        ],
        "comment": "25 pages, 17 figures"
    },
    {
        "paper id": "2402.10451",
        "abstract url": "https://arxiv.org/abs/2402.10451",
        "title": "Composition Orderings for Linear Functions and Matrix Multiplication Orderings",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider composition orderings for linear functions of one variable. Given $n$ linear functions $f_1,\\dots,f_n$ and a constant $c$, the objective is to find a permutation $\u03c3$ that minimizes/maximizes $f_{\u03c3(n)}\\circ\\dots\\circ f_{\u03c3(1)}(c)$. It was first studied in the area of time-dependent scheduling, and known to be solvable in $O(n\\log n)$ time if all functions are nondecreasing. In this paper, we present a complete characterization of optimal composition orderings for this case, by regarding linear functions as two-dimensional vectors. We also show several interesting properties on optimal composition orderings such as the equivalence between local and global optimality. Furthermore, by using the characterization above, we provide a fixed-parameter tractable (FPT) algorithm for the composition ordering problem for general linear functions, with respect to the number of decreasing linear functions. We next deal with matrix multiplication orderings as a generalization of composition of linear functions. Given $n$ matrices $M_1,\\dots,M_n\\in\\mathbb{R}^{m\\times m}$ and two vectors $w,y\\in\\mathbb{R}^m$, where $m$ denotes a positive integer, the objective is to find a permutation $\u03c3$ that minimizes/maximizes $w^\\top M_{\u03c3(n)}\\dots M_{\u03c3(1)} y$. The problem is also viewed as a generalization of flow shop scheduling through a limit. By this extension, we show that the multiplication ordering problem for $2\\times 2$ matrices is solvable in $O(n\\log n)$ time if all the matrices are simultaneously triangularizable and have nonnegative determinants, and FPT with respect to the number of matrices with negative determinants, if all the matrices are simultaneously triangularizable. As the negative side, we finally prove that three possible natural generalizations are NP-hard: 1) when $m=2$, 2) when $m\\geq 3$, and 3) the target version of the problem.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "38 pages"
    },
    {
        "paper id": "2402.10971",
        "abstract url": "https://arxiv.org/abs/2402.10971",
        "title": "Enabling data-driven and bidirectional model development in Verilog-A for photonic devices",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a method to model photonic components in Verilog-A by introducing bidirectional signaling through a single port. To achieve this, the concept of power waves and scattering parameters from electromagnetism are employed. As a consequence, one can simultaneously transmit forward and backward propagating waves on a single wire while also capturing realistic, measurement-backed response of photonic components in Verilog-A. We demonstrate examples to show the efficacy of the proposed technique in accounting for critical effects in photonic integrated circuits such as Fabry-Perot cavity resonance, reflections to lasers, etc. Our solution makes electronic-photonic co-simulation more intuitive and accurate.",
        "subjects": [
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2402.10975",
        "abstract url": "https://arxiv.org/abs/2402.10975",
        "title": "A System-Dynamic Based Simulation and Bayesian Optimization for Inventory Management",
        "rating": "-10",
        "keywords": [],
        "abstract": "Inventory management is a fundamental challenge in supply chain management. The challenge is compounded when the associated products have unpredictable demands. This study proposes an innovative optimization approach combining system-dynamic Monte-Carlo simulation and Bayesian optimization. The proposed algorithm is tested with a real-life, unpredictable demand dataset to find the optimal stock to meet the business objective. The findings show a considerable improvement in inventory policy. This information is helpful for supply chain analytics decision-making, which increases productivity and profitability. This study further adds sensitivity analysis, considering the variation in demand and expected output in profit percentage. This paper makes a substantial contribution by presenting a simple yet robust approach to addressing the fundamental difficulty of inventory management in a dynamic business environment.",
        "subjects": [
            "math.OC",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.19180",
        "abstract url": "https://arxiv.org/abs/2402.19180",
        "title": "ModZoo: A Large-Scale Study of Modded Android Apps and their Markets",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present the results of the first large-scale study into Android markets that offer modified or modded apps: apps whose features and functionality have been altered by a third-party. We analyse over 146k (thousand) apps obtained from 13 of the most popular modded app markets. Around 90% of apps we collect are altered in some way when compared to the official counterparts on Google Play. Modifications include games cheats, such as infinite coins or lives; mainstream apps with premium features provided for free; and apps with modified advertising identifiers or excluded ads. We find the original app developers lose significant potential revenue due to: the provision of paid for apps for free (around 5% of the apps across all markets); the free availability of premium features that require payment in the official app; and modified advertising identifiers. While some modded apps have all trackers and ads removed (3%), in general, the installation of these apps is significantly more risky for the user than the official version: modded apps are ten times more likely to be marked as malicious and often request additional permissions.",
        "subjects": [
            "cs.OH",
            "cs.CR",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.01544",
        "abstract url": "https://arxiv.org/abs/2405.01544",
        "title": "Transformational Outsourcing in IT Project Management",
        "rating": "-10",
        "keywords": [],
        "abstract": "Transformational outsourcing represents a strategic shift from traditional cost-focused outsourcing to a more profound and collaborative approach. It involves partnering with service providers to accomplish routine tasks and drive substantial organizational change and innovation. The report discusses the significance of pursuing transformational outsourcing for IT companies, highlighting its role in achieving strategic growth, competitive advantage, and cost-efficiency while enabling a focus on core competencies. It explores the pros and cons of IT outsourcing, emphasizing the benefits of cost savings, global talent access, scalability, and challenges related to quality, control, and data security. Additionally, the report identifies some critical reasons why outsourcing efforts may fail in achieving organizational goals, including poor vendor selection, communication issues, unclear objectives, resistance to change, and inadequate risk management. When carefully planned and executed, transformational outsourcing offers IT companies a pathway to enhance efficiency and foster innovation and competitiveness in a rapidly evolving technology landscape.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "17 pages, 4 Figures"
    },
    {
        "paper id": "2405.01545",
        "abstract url": "https://arxiv.org/abs/2405.01545",
        "title": "Analysing software failure using runtime verification and LTL",
        "rating": "-10",
        "keywords": [],
        "abstract": "A self-healing software system is an advanced computer program or system designed to detect, diagnose, and automatically recover from faults or errors without human intervention. These systems are typically employed in mission-critical applications where downtime can have significant financial or operational consequences. Failure detection is one of the important steps in the self-healing system. In this research, a method using runtime verification is proposed to diagnose four types of errors at the component level. The simulation on mRUBIS shows that the suggested method has the necessary efficiency in detecting the occurrence of failures.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    }
]