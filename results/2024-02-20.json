[
    {
        "paper id": "2402.12736",
        "abstract url": "https://arxiv.org/abs/2402.12736",
        "title": "CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning",
        "rating": 2,
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects. However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference. This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions. This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet. The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process. Furthermore, this paper has conducted an analysis on multiple fine-tuning strategies and have implemented their application within ResNet, thereby expanding the research on fine-tuning strategies for object detection networks. Besides, this paper carried out extensive experiments using five benchmark datasets. The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the finetune schemes is achieved.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12851",
        "abstract url": "https://arxiv.org/abs/2402.12851",
        "title": "MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models",
        "rating": 2,
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to learn distinct features. We conducted experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks. With the same number of parameters, our approach outperforms LoRA significantly. In math reasoning, MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on several benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12881",
        "abstract url": "https://arxiv.org/abs/2402.12881",
        "title": "GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models",
        "rating": 2,
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pre-trained VLMs do not necessarily capture object affordances effectively. Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and VLMs. Our research contributes a novel dataset for language grounding tasks, and presents insights into LM capabilities, advancing the understanding of object affordances. Codes and data are available at https://github.com/sayantan11995/Affordance",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13004",
        "abstract url": "https://arxiv.org/abs/2402.13004",
        "title": "Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition",
        "rating": 2,
        "keywords": [
            [
                "audio-visual"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)"
    },
    {
        "paper id": "2402.13022",
        "abstract url": "https://arxiv.org/abs/2402.13022",
        "title": "SoMeLVLM: A Large Vision Language Model for Social Media Processing",
        "rating": 2,
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge & comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows its significant advantages over baselines in terms of cognitive abilities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13088",
        "abstract url": "https://arxiv.org/abs/2402.13088",
        "title": "Slot-VLM: SlowFast Slots for Video-Language Modeling",
        "rating": 2,
        "keywords": [
            [
                "VLM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 10 figures"
    },
    {
        "paper id": "2402.13130",
        "abstract url": "https://arxiv.org/abs/2402.13130",
        "title": "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity",
        "rating": 2,
        "keywords": [
            [
                "parameter efficiency"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, using significantly fewer parameters and a substantially smaller embedding size. Finally, we observe further boosts by combining TMFT with a word similarity task or domain adaptive pre-training.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "7 pages, 9 figures, 2 tables"
    },
    {
        "paper id": "2402.13152",
        "abstract url": "https://arxiv.org/abs/2402.13152",
        "title": "AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies",
        "rating": 2,
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)"
    },
    {
        "paper id": "2402.13232",
        "abstract url": "https://arxiv.org/abs/2402.13232",
        "title": "A Touch, Vision, and Language Dataset for Multimodal Alignment",
        "rating": 2,
        "keywords": [
            [
                "vision-language",
                "vlm"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code and data: https://tactile-vlm.github.io.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12721",
        "abstract url": "https://arxiv.org/abs/2402.12721",
        "title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the original, downstream model. Moreover, the proposed PAC-FNO is ready to work with existing image recognition models. Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at ICLR 2024"
    },
    {
        "paper id": "2402.12846",
        "abstract url": "https://arxiv.org/abs/2402.12846",
        "title": "ConVQG: Contrastive Visual Question Generation with Multimodal Guidance",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems. Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of grounding. In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on both knowledge-aware and standard VQG benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions. Our human evaluation results also show preference for ConVQG questions compared to non-contrastive baselines.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "AAAI 2024. Project page at https://limirs.github.io/ConVQG"
    },
    {
        "paper id": "2402.13025",
        "abstract url": "https://arxiv.org/abs/2402.13025",
        "title": "CFEVER: A Chinese Fact Extraction and VERification Dataset",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification. CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia. Each claim in CFEVER is labeled as \"Supports\", \"Refutes\", or \"Not Enough Info\" to depict its degree of factualness. Similar to the FEVER dataset, claims in the \"Supports\" and \"Refutes\" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement. In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts. CFEVER is available at https://ikmlab.github.io/CFEVER.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "AAAI-24"
    },
    {
        "paper id": "2402.13122",
        "abstract url": "https://arxiv.org/abs/2402.13122",
        "title": "Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "workshop",
                "ICCV"
            ]
        ],
        "abstract": "Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware. Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data. Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons. In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to black-box source model predictions. Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the black-box source model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels. We benchmark CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using black-box models to transfer knowledge on lightweight models for a target data distribution.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "11 pages, 6 figures, ICCV2023 workshop"
    },
    {
        "paper id": "2402.13250",
        "abstract url": "https://arxiv.org/abs/2402.13250",
        "title": "Video ReCap: Recursive Captioning of Hour-Long Videos",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: https://sites.google.com/view/vidrecap",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2402.12730",
        "abstract url": "https://arxiv.org/abs/2402.12730",
        "title": "UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The aim of SemEval-2024 Task 1, \"Semantic Textual Relatedness for African and Asian Languages\" is to develop models for identifying semantic textual relatedness (STR) between two sentences using multiple languages (14 African and Asian languages) and settings (supervised, unsupervised, and cross-lingual). Large language models (LLMs) have shown impressive performance on several natural language understanding tasks such as multilingual machine translation (MMT), semantic similarity (STS), and encoding sentence embeddings. Using a combination of LLMs that perform well on these tasks, we developed two STR models, $\\textit{TranSem}$ and $\\textit{FineSem}$, for the supervised and cross-lingual settings. We explore the effectiveness of several training methods and the usefulness of machine translation. We find that direct fine-tuning on the task is comparable to using sentence embeddings and translating to English leads to better performance for some languages. In the supervised setting, our model performance is better than the official baseline for 3 languages with the remaining 4 performing on par. In the cross-lingual setting, our model performance is better than the baseline for 3 languages (leading to $1^{st}$ place for Africaans and $2^{nd}$ place for Indonesian), is on par for 2 languages and performs poorly on the remaining 7 languages. Our code is publicly available at https://github.com/dipta007/SemEval24-Task8.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at SemEval 2024 (Colocated with NAACL 2024)"
    },
    {
        "paper id": "2402.12741",
        "abstract url": "https://arxiv.org/abs/2402.12741",
        "title": "MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion",
        "rating": 1,
        "keywords": [
            [
                "vision-language",
                "VLM"
            ],
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on https://github.com/measure-infinity/mulan-code.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project website: https://measure-infinity.github.io/mulan"
    },
    {
        "paper id": "2402.12746",
        "abstract url": "https://arxiv.org/abs/2402.12746",
        "title": "Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "The expectation to deploy a universal neural network for speech enhancement, with the aim of improving noise robustness across diverse speech processing tasks, faces challenges due to the existing lack of awareness within static speech enhancement frameworks regarding the expected speech in downstream modules. These limitations impede the effectiveness of static speech enhancement approaches in achieving optimal performance for a range of speech processing tasks, thereby challenging the notion of universal applicability. The fundamental issue in achieving universal speech enhancement lies in effectively informing the speech enhancement module about the features of downstream modules. In this study, we present a novel weighting prediction approach, which explicitly learns the task relationships from downstream training information to address the core challenge of universal speech enhancement. We found the role of deciding whether to employ data augmentation techniques as crucial downstream training information. This decision significantly impacts the expected speech and the performance of the speech enhancement module. Moreover, we introduce a novel speech enhancement network, the Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural network that includes the speech enhancement module, gate module, and weight prediction module. Experimental results demonstrate that the proposed Plugin-SE approach is competitive or superior to other joint training methods across various downstream tasks.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12750",
        "abstract url": "https://arxiv.org/abs/2402.12750",
        "title": "Model Composition for Multimodal Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code will be available at https://github.com/THUNLP-MT/ModelCompose"
    },
    {
        "paper id": "2402.12765",
        "abstract url": "https://arxiv.org/abs/2402.12765",
        "title": "GOOD: Towards Domain Generalized Orientated Object Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Oriented object detection has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality. In this paper, we propose the task of domain generalized oriented object detection, which intends to explore the generalization of oriented object detectors on arbitrary unseen target domains. Learning domain generalized oriented object detectors is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions. To address these challenges, we propose a generalized oriented object detector (GOOD). After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC). The proposed RAC allows the oriented object detector to learn stable orientation representation from style-diversified samples. The proposed SEC further stabilizes the generalization ability of content representation from different image styles. Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD. Source code will be publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures"
    },
    {
        "paper id": "2402.12786",
        "abstract url": "https://arxiv.org/abs/2402.12786",
        "title": "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that \"even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different\". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12806",
        "abstract url": "https://arxiv.org/abs/2402.12806",
        "title": "SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable reasoning ability as in Chain-of-thought prompting, but faithful multi-step reasoning remains a challenge. We specifically focus on backward chaining, where the query is recursively decomposed using logical rules until proven. To address the limitations of current backward chaining implementations, we propose SymBa (Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls the entire proof process and the LLM is called to generate a single reasoning step only when the solver encounters a dead end. By this novel solver-LLM integration, while being able to produce an interpretable, structured proof, SymBa achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks (ProofWriter, Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward chaining baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "22 pages (8 pages for main text),9 figures"
    },
    {
        "paper id": "2402.12816",
        "abstract url": "https://arxiv.org/abs/2402.12816",
        "title": "OMRA: Online Motion Resolution Adaptation to Remedy Domain Shift in Learned Hierarchical B-frame Coding",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Learned hierarchical B-frame coding aims to leverage bi-directional reference frames for better coding efficiency. However, the domain shift between training and test scenarios due to dataset limitations poses a challenge. This issue arises from training the codec with small groups of pictures (GOP) but testing it on large GOPs. Specifically, the motion estimation network, when trained on small GOPs, is unable to handle large motion at test time, incurring a negative impact on compression performance. To mitigate the domain shift, we present an online motion resolution adaptation (OMRA) method. It adapts the spatial resolution of video frames on a per-frame basis to suit the capability of the motion estimation network in a pre-trained B-frame codec. Our OMRA is an online, inference technique. It need not re-train the codec and is readily applicable to existing B-frame codecs that adopt hierarchical bi-directional prediction. Experimental results show that OMRA significantly enhances the compression performance of two state-of-the-art learned B-frame codecs on commonly used datasets.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "7 pages, submitted to IEEE ICIP 2024"
    },
    {
        "paper id": "2402.12817",
        "abstract url": "https://arxiv.org/abs/2402.12817",
        "title": "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12819",
        "abstract url": "https://arxiv.org/abs/2402.12819",
        "title": "Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "When solving NLP tasks with limited labelled data, researchers can either use a general large language model without further update, or use a small number of labelled examples to tune a specialised smaller model. In this work, we address the research gap of how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of fine-tuning, instruction-tuning, prompting and in-context learning on 7 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average $10 - 1000$) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with this number being significantly lower on multi-class datasets (up to $100$) than on binary datasets (up to $5000$). When performance variance is taken into consideration, the number of required labels increases on average by $100 - 200\\%$ and even up to $1500\\%$ in specific cases.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12821",
        "abstract url": "https://arxiv.org/abs/2402.12821",
        "title": "Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero-shot approaches by much larger LLM, serving as an effective and efficient ready-to-use scorer.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12835",
        "abstract url": "https://arxiv.org/abs/2402.12835",
        "title": "PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12840",
        "abstract url": "https://arxiv.org/abs/2402.12840",
        "title": "ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA), and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Falcon struggle to achieve a score of 50%, while even the top-performing Arabic-centric model only achieves a score of 62.3%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12842",
        "abstract url": "https://arxiv.org/abs/2402.12842",
        "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets using the GPT-2 model family show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12847",
        "abstract url": "https://arxiv.org/abs/2402.12847",
        "title": "Instruction-tuned Language Models are Better Knowledge Learners",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12862",
        "abstract url": "https://arxiv.org/abs/2402.12862",
        "title": "Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12865",
        "abstract url": "https://arxiv.org/abs/2402.12865",
        "title": "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12872",
        "abstract url": "https://arxiv.org/abs/2402.12872",
        "title": "Deep, convergent, unrolled half-quadratic splitting for image deconvolution",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "In recent years, algorithm unrolling has emerged as a powerful technique for designing interpretable neural networks based on iterative algorithms. Imaging inverse problems have particularly benefited from unrolling-based deep network design since many traditional model-based approaches rely on iterative optimization. Despite exciting progress, typical unrolling approaches heuristically design layer-specific convolution weights to improve performance. Crucially, convergence properties of the underlying iterative algorithm are lost once layer-specific parameters are learned from training data. We propose an unrolling technique that breaks the trade-off between retaining algorithm properties while simultaneously enhancing performance. We focus on image deblurring and unrolling the widely-applied Half-Quadratic Splitting (HQS) algorithm. We develop a new parametrization scheme which enforces layer-specific parameters to asymptotically approach certain fixed points. Through extensive experimental studies, we verify that our approach achieves competitive performance with state-of-the-art unrolled layer-specific learning and significantly improves over the traditional HQS algorithm. We further establish convergence of the proposed unrolled network as the number of layers approaches infinity, and characterize its convergence rate. Our experimental verification involves simulations that validate the analytical results as well as comparison with state-of-the-art non-blind deblurring techniques on benchmark datasets. The merits of the proposed convergent unrolled network are established over competing alternatives, especially in the regime of limited training.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Accepted with mandatory minor revisions by Transactions on Computational Imaging"
    },
    {
        "paper id": "2402.12888",
        "abstract url": "https://arxiv.org/abs/2402.12888",
        "title": "Transformer-based Learned Image Compression for Joint Decoding and Denoising",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "This work introduces a Transformer-based image compression system. It has the flexibility to switch between the standard image reconstruction and the denoising reconstruction from a single compressed bitstream. Instead of training separate decoders for these tasks, we incorporate two add-on modules to adapt a pre-trained image decoder from performing the standard image reconstruction to joint decoding and denoising. Our scheme adopts a two-pronged approach. It features a latent refinement module to refine the latent representation of a noisy input image for reconstructing a noise-free image. Additionally, it incorporates an instance-specific prompt generator that adapts the decoding process to improve on the latent refinement. Experimental results show that our method achieves a similar level of denoising quality to training a separate decoder for joint decoding and denoising at the expense of only a modest increase in the decoder's model size and computational complexity.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Accepted to PCS 2024"
    },
    {
        "paper id": "2402.12913",
        "abstract url": "https://arxiv.org/abs/2402.12913",
        "title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12914",
        "abstract url": "https://arxiv.org/abs/2402.12914",
        "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12927",
        "abstract url": "https://arxiv.org/abs/2402.12927",
        "title": "CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection",
        "rating": 1,
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12940",
        "abstract url": "https://arxiv.org/abs/2402.12940",
        "title": "Normalized Orthography for Tunisian Arabic",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences. This research introduces the \"Normalized Orthography for Tunisian Arabic\" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency. The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology. This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Final Report for the Derja Association"
    },
    {
        "paper id": "2402.12968",
        "abstract url": "https://arxiv.org/abs/2402.12968",
        "title": "MapTrack: Tracking in the Map",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2402.12969",
        "abstract url": "https://arxiv.org/abs/2402.12969",
        "title": "Gl\u00f3rIA -- A Generative and Open Large Language Model for Portuguese",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\u00f3rIA, a robust European Portuguese decoder LLM. To pre-train Gl\u00f3rIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluation shows that Gl\u00f3rIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text. The model also exhibits strong potential for various downstream tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted for publication at PROPOR 2024"
    },
    {
        "paper id": "2402.12976",
        "abstract url": "https://arxiv.org/abs/2402.12976",
        "title": "The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12998",
        "abstract url": "https://arxiv.org/abs/2402.12998",
        "title": "Phonotactic Complexity across Dialects",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to COLING-LREC 2024"
    },
    {
        "paper id": "2402.13013",
        "abstract url": "https://arxiv.org/abs/2402.13013",
        "title": "Code Needs Comments: Enhancing Code LLMs with Comment Augmentation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13016",
        "abstract url": "https://arxiv.org/abs/2402.13016",
        "title": "Understanding the effects of language-specific class imbalance in multilingual fine-tuning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To be published in: Findings of the Association for Computational Linguistics: EACL 2024"
    },
    {
        "paper id": "2402.13018",
        "abstract url": "https://arxiv.org/abs/2402.13018",
        "title": "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Speech emotion recognition (SER) is a pivotal technology for human-computer interaction systems. However, 80.77% of SER papers yield results that cannot be reproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal PERformance Benchmark, which aims to enhance open-source initiatives for SER. EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art speech self-supervised learning models (SSLMs) for exhaustive evaluation across six open-source SER datasets. EMO-SUPERB streamlines result sharing via an online leaderboard, fostering collaboration within a community-driven benchmark and thereby enhancing the development of SER. On average, 2.58% of annotations are annotated using natural language. SER relies on classification models and is unable to process natural languages, leading to the discarding of these valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural language annotations, and subsequently re-label the data. By utilizing labels generated by ChatGPT, we consistently achieve an average relative gain of 3.08% across all settings.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "webpage: https://emosuperb.github.io/"
    },
    {
        "paper id": "2402.13035",
        "abstract url": "https://arxiv.org/abs/2402.13035",
        "title": "Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed \"Step CoT Check\". Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction. We compare the performance of models fine-tuned with the \"Step CoT Check\" prompt against those refined using other promps within the context of checking-correction data. The \"Step CoT Check\" outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness. For reproducibility, all the datasets and codes are provided in https://github.com/bammt/Learn-to-check.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13036",
        "abstract url": "https://arxiv.org/abs/2402.13036",
        "title": "SiLLM: Large Language Models for Simultaneous Machine Translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of LLM, generates translation using the partial source sentence. The two agents collaborate to accomplish SiMT. To facilitate the application of token-level policies determined by conventional SiMT models to LLM, we propose a word-level policy adapted for LLM. Experiments on two datasets demonstrate that, with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages, 6 tables, 7 figures"
    },
    {
        "paper id": "2402.13043",
        "abstract url": "https://arxiv.org/abs/2402.13043",
        "title": "Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NAACL 2024"
    },
    {
        "paper id": "2402.13055",
        "abstract url": "https://arxiv.org/abs/2402.13055",
        "title": "Identifying Semantic Induction Heads to Understand In-Context Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13064",
        "abstract url": "https://arxiv.org/abs/2402.13064",
        "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2402.13071",
        "abstract url": "https://arxiv.org/abs/2402.13071",
        "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Github: https://github.com/voidful/Codec-SUPERB"
    },
    {
        "paper id": "2402.13076",
        "abstract url": "https://arxiv.org/abs/2402.13076",
        "title": "Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience. This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time factor.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13094",
        "abstract url": "https://arxiv.org/abs/2402.13094",
        "title": "Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Text simplification refers to the process of increasing the comprehensibility of texts. Automatic text simplification models are most commonly evaluated by experts or crowdworkers instead of the primary target groups of simplified texts, such as persons with intellectual disabilities. We conducted an evaluation study of text comprehensibility including participants with and without intellectual disabilities reading unsimplified, automatically and manually simplified German texts on a tablet computer. We explored four different approaches to measuring comprehensibility: multiple-choice comprehension questions, perceived difficulty ratings, response time, and reading speed. The results revealed significant variations in these measurements, depending on the reader group and whether the text had undergone automatic or manual simplification. For the target group of persons with intellectual disabilities, comprehension questions emerged as the most reliable measure, while analyzing reading speed provided valuable insights into participants' reading behavior.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted for publication at the 2024 ACM Conference on Human Factors in Computing Systems (CHI'24)"
    },
    {
        "paper id": "2402.13098",
        "abstract url": "https://arxiv.org/abs/2402.13098",
        "title": "ELAD: Explanation-Guided Large Language Models Active Distillation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model's reasoning. Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLM knowledge distillation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13109",
        "abstract url": "https://arxiv.org/abs/2402.13109",
        "title": "CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts. This work aims to uncover the current limitations of LLMs in handling Chinese tasks, pushing towards the development of more culturally informed and linguistically diverse models with the released data and benchmark (https://yizhilll.github.io/CIF-Bench/).",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13113",
        "abstract url": "https://arxiv.org/abs/2402.13113",
        "title": "When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "work in progress"
    },
    {
        "paper id": "2402.13116",
        "abstract url": "https://arxiv.org/abs/2402.13116",
        "title": "A Survey on Knowledge Distillation of Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: \\textit{algorithm}, \\textit{skill}, and \\textit{verticalization} -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in KD and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "44 pages"
    },
    {
        "paper id": "2402.13125",
        "abstract url": "https://arxiv.org/abs/2402.13125",
        "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions. We also conduct more analysis to show the robustness and reliability of TreeEval. Our code can be accessed via the provided https://github.com/Ashura5/TreeEval.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13137",
        "abstract url": "https://arxiv.org/abs/2402.13137",
        "title": "The Hidden Space of Transformer Language Adapters",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2402.13145",
        "abstract url": "https://arxiv.org/abs/2402.13145",
        "title": "CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating \"ground\" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition. We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus. These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research. The code is available in https://github.com/JasonShao55/Chinese_Metaphor_Explanation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13146",
        "abstract url": "https://arxiv.org/abs/2402.13146",
        "title": "OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "COLING 2024"
    },
    {
        "paper id": "2402.13184",
        "abstract url": "https://arxiv.org/abs/2402.13184",
        "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this study, we introduce \"CosmoAgent,\" an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/agiresearch/AlienAgent.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13208",
        "abstract url": "https://arxiv.org/abs/2402.13208",
        "title": "How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%), which, in most cases, is not statistically significant.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2402.13211",
        "abstract url": "https://arxiv.org/abs/2402.13211",
        "title": "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2402.13212",
        "abstract url": "https://arxiv.org/abs/2402.13212",
        "title": "Soft Self-Consistency Improves Language Model Agents",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that Soft-SC can be applied to both open-source and black-box models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "14 pages, the first three authors contributed equally; Code: https://github.com/HanNight/soft_self_consistency"
    },
    {
        "paper id": "2402.13213",
        "abstract url": "https://arxiv.org/abs/2402.13213",
        "title": "Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task. For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of softmax probabilities and find similar (but not identical) results.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13217",
        "abstract url": "https://arxiv.org/abs/2402.13217",
        "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13220",
        "abstract url": "https://arxiv.org/abs/2402.13220",
        "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13222",
        "abstract url": "https://arxiv.org/abs/2402.13222",
        "title": "RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop code models for languages other than English.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2402.13228",
        "abstract url": "https://arxiv.org/abs/2402.13228",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \\textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \\textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. By fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly 2\\% better than any other open-source model on the HuggingFace Open LLM Leaderboard and becomes the first open-source LLM to surpass an average accuracy of 80\\%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13231",
        "abstract url": "https://arxiv.org/abs/2402.13231",
        "title": "Investigating Cultural Alignment of Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2402.13236",
        "abstract url": "https://arxiv.org/abs/2402.13236",
        "title": "Towards audio language modeling -- an overview",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13249",
        "abstract url": "https://arxiv.org/abs/2402.13249",
        "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NAACL 2024; Linguistic annotations available at https://github.com/amazon-science/tofueval"
    },
    {
        "paper id": "2402.13254",
        "abstract url": "https://arxiv.org/abs/2402.13254",
        "title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two critical under-explored problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using grounded image generation model GLIGEN to generate fine-tuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13 pages, 6 figures, 8 tables, Project Page: https://countercurate.github.io/"
    },
    {
        "paper id": "2402.13301",
        "abstract url": "https://arxiv.org/abs/2402.13301",
        "title": "Structure-informed Positional Encoding for Music Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers. We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13302",
        "abstract url": "https://arxiv.org/abs/2402.13302",
        "title": "Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic Lexical Resources",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Supervised models for Word Sense Disambiguation (WSD) currently yield to state-of-the-art results in the most popular benchmarks. Despite the recent introduction of Word Embeddings and Recurrent Neural Networks to design powerful context-related features, the interest in improving WSD models using Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based approaches. In this paper, we enhance \"modern\" supervised WSD models exploiting two popular SLRs: WordNet and WordNet Domains. We propose an effective way to introduce semantic features into the classifiers, and we consider using the SLR structure to augment the training data. We study the effect of different types of semantic features, investigating their interaction with local contexts encoded by means of mixtures of Word Embeddings or Recurrent Neural Networks, and we extend the proposed model into a novel multi-layer architecture for WSD. A detailed experimental comparison in the recent Unified Evaluation Framework (Raganato et al., 2017) shows that the proposed approach leads to supervised models that compare favourably with the state-of-the art.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "The 11th International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
        "paper id": "2402.13306",
        "abstract url": "https://arxiv.org/abs/2402.13306",
        "title": "Vision System Prototype for Inspection and Monitoring with a Smart Camera",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents the design of an artificial vision system prototype for automatic inspection and monitoring of objects over a conveyor belt and using a Smart camera 2D BOA-INS. The prototype consists of a conveyor belt and an embedded system based on an Arduino Mega card for system control, and it has as main peripherals the smart camera, a direct current motor, a photoelectric sensor, LED illumination and LEDs indicating the status (good or defect) of each evaluated object. The application of the prototype is for educational purposes, so that undergraduate, master and diploma students can simulate a continuous production line, controlled by an embedded system, and perform quality control by monitoring through a visual system and a personal computer. This allows implementing the topics of embedded systems, artificial vision, artificial intelligence, pattern recognition, automatic control, as well as automation of real processes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 16 figures, in Spanish language"
    },
    {
        "paper id": "2402.13331",
        "abstract url": "https://arxiv.org/abs/2402.13331",
        "title": "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Hallucinated translations pose significant threats and safety concerns when it comes to the practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13353",
        "abstract url": "https://arxiv.org/abs/2402.13353",
        "title": "Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting and analyzing various defect types in semiconductor materials is an important prerequisite for understanding the underlying mechanisms as well as tailoring the production processes. Analysis of microscopy images that reveal defects typically requires image analysis tasks such as segmentation and object detection. With the permanently increasing amount of data that is produced by experiments, handling these tasks manually becomes more and more impossible. In this work, we combine various image analysis and data mining techniques for creating a robust and accurate, automated image analysis pipeline. This allows for extracting the type and position of all defects in a microscopy image of a KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000 individual images.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13364",
        "abstract url": "https://arxiv.org/abs/2402.13364",
        "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "15 pages, 5 figures, 5 tables"
    },
    {
        "paper id": "2402.13374",
        "abstract url": "https://arxiv.org/abs/2402.13374",
        "title": "Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the realm of dialogue systems, user simulation techniques have emerged as a game-changer, redefining the evaluation and enhancement of task-oriented dialogue (TOD) systems. These methods are crucial for replicating real user interactions, enabling applications like synthetic data augmentation, error detection, and robust evaluation. However, existing approaches often rely on rigid rule-based methods or on annotated data. This paper introduces DAUS, a Domain-Aware User Simulator. Leveraging large language models, we fine-tune DAUS on real examples of task-oriented dialogues. Results on two relevant benchmarks showcase significant improvements in terms of user goal fulfillment. Notably, we have observed that fine-tuning enhances the simulator's coherence with user goals, effectively mitigating hallucinations -- a major source of inconsistencies in simulator responses.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13405",
        "abstract url": "https://arxiv.org/abs/2402.13405",
        "title": "A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across all three tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13426",
        "abstract url": "https://arxiv.org/abs/2402.13426",
        "title": "Explaining Relationships Among Research Papers",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Due to the rapid pace of research publications, keeping up to date with all the latest related papers is very time-consuming, even with daily feed tools. There is a need for automatically generated, short, customized literature reviews of sets of papers to help researchers decide what to read. While several works in the last decade have addressed the task of explaining a single research paper, usually in the context of another paper citing it, the relationship among multiple papers has been ignored; prior works have focused on generating a single citation sentence in isolation, without addressing the expository and transition sentences needed to connect multiple papers in a coherent story. In this work, we explore a feature-based, LLM-prompting approach to generate richer citation texts, as well as generating multiple citations at once to capture the complex relationships among research papers. We perform an expert evaluation to investigate the impact of our proposed features on the quality of the generated paragraphs and find a strong correlation between human preference and integrative writing style, suggesting that humans prefer high-level, abstract citations, with transition sentences between them to provide an overall story.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13433",
        "abstract url": "https://arxiv.org/abs/2402.13433",
        "title": "Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present the structured average intersection-over-union ratio (STRUCT-IOU), a similarity metric between constituency parse trees motivated by the problem of evaluating speech parsers. STRUCT-IOU enables comparison between a constituency parse tree (over automatically recognized spoken word boundaries) with the ground-truth parse (over written words). To compute the metric, we project the ground-truth parse tree to the speech domain by forced alignment, align the projected ground-truth constituents with the predicted ones under certain structured constraints, and calculate the average IOU score across all aligned constituent pairs. STRUCT-IOU takes word boundaries into account and overcomes the challenge that the predicted words and ground truth may not have perfect one-to-one correspondence. Extending to the evaluation of text constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to syntactically plausible parses than PARSEVAL (Black et al., 1991).",
        "subjects": [
            "cs.CL"
        ],
        "comment": "11 pages, 9 figures, 1 table"
    },
    {
        "paper id": "2402.13446",
        "abstract url": "https://arxiv.org/abs/2402.13446",
        "title": "Large Language Models for Data Annotation: A Survey",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at \\url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13449",
        "abstract url": "https://arxiv.org/abs/2402.13449",
        "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks. This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13463",
        "abstract url": "https://arxiv.org/abs/2402.13463",
        "title": "RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses. We further propose a recall-and-repeat prompts as a simple and effective way to enhance the model's responsiveness to feedback.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2402.13465",
        "abstract url": "https://arxiv.org/abs/2402.13465",
        "title": "Unsupervised learning based object detection using Contrastive Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Training image-based object detectors presents formidable challenges, as it entails not only the complexities of object detection but also the added intricacies of precisely localizing objects within potentially diverse and noisy environments. However, the collection of imagery itself can often be straightforward; for instance, cameras mounted in vehicles can effortlessly capture vast amounts of data in various real-world scenarios. In light of this, we introduce a groundbreaking method for training single-stage object detectors through unsupervised/self-supervised learning. Our state-of-the-art approach has the potential to revolutionize the labeling process, substantially reducing the time and cost associated with manual annotation. Furthermore, it paves the way for previously unattainable research opportunities, particularly for large, diverse, and challenging datasets lacking extensive labels. In contrast to prevalent unsupervised learning methods that primarily target classification tasks, our approach takes on the unique challenge of object detection. We pioneer the concept of intra-image contrastive learning alongside inter-image counterparts, enabling the acquisition of crucial location information essential for object detection. The method adeptly learns and represents this location information, yielding informative heatmaps. Our results showcase an outstanding accuracy of \\textbf{89.2\\%}, marking a significant breakthrough of approximately \\textbf{15x} over random initialization in the realm of unsupervised object detection within the field of computer vision.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2402.13482",
        "abstract url": "https://arxiv.org/abs/2402.13482",
        "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing LLM-powered data augmentation baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13492",
        "abstract url": "https://arxiv.org/abs/2402.13492",
        "title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity. Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NAACL2024 (main)"
    },
    {
        "paper id": "2402.13494",
        "abstract url": "https://arxiv.org/abs/2402.13494",
        "title": "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting unsafe prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13497",
        "abstract url": "https://arxiv.org/abs/2402.13497",
        "title": "Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing Quantization-Aware Training (QAT) methods intensively depend on the complete labeled dataset or knowledge distillation to guarantee the performances toward Full Precision (FP) accuracies. However, empirical results show that QAT still has inferior results compared to its FP counterpart. One question is how to push QAT toward or even surpass FP performances. In this paper, we address this issue from a new perspective by injecting the vicinal data distribution information to improve the generalization performances of QAT effectively. We present a simple, novel, yet powerful method introducing an Consistency Regularization (CR) for QAT. Concretely, CR assumes that augmented samples should be consistent in the latent feature space. Our method generalizes well to different network architectures and various QAT methods. Extensive experiments demonstrate that our approach significantly outperforms the current state-of-the-art QAT methods and even FP counterparts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "11 pages, 5 figures"
    },
    {
        "paper id": "2402.13505",
        "abstract url": "https://arxiv.org/abs/2402.13505",
        "title": "SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier. The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase. Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement. Moreover, we introduce two novel class distributions broadening the scope of the evaluation. Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios. Our code is available at https://github.com/LeapLabTHU/SimPro.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICML2024"
    },
    {
        "paper id": "2402.13511",
        "abstract url": "https://arxiv.org/abs/2402.13511",
        "title": "Mel-FullSubNet: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "In this work, we propose Mel-FullSubNet, a single-channel Mel-spectrogram denoising and dereverberation network for improving both speech quality and automatic speech recognition (ASR) performance. Mel-FullSubNet takes as input the noisy and reverberant Mel-spectrogram and predicts the corresponding clean Mel-spectrogram. The enhanced Mel-spectrogram can be either transformed to speech waveform with a neural vocoder or directly used for ASR. Mel-FullSubNet encapsulates interleaved full-band and sub-band networks, for learning the full-band spectral pattern of signals and the sub-band/narrow-band properties of signals, respectively. Compared to linear-frequency domain or time-domain speech enhancement, the major advantage of Mel-spectrogram enhancement is that Mel-frequency presents speech in a more compact way and thus is easier to learn, which will benefit both speech quality and ASR. Experimental results demonstrate a significant improvement in both speech quality and ASR performance achieved by the proposed model.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13514",
        "abstract url": "https://arxiv.org/abs/2402.13514",
        "title": "Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13524",
        "abstract url": "https://arxiv.org/abs/2402.13524",
        "title": "OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to automatically score different model outputs, which is shown closely related to human evaluation. We evaluate several representative multilingual LLMs on the proposed OMGEval, which we believe will provide a valuable reference for the community to further understand and improve the multilingual capability of LLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.14853",
        "abstract url": "https://arxiv.org/abs/2402.14853",
        "title": "NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To appear at EACL 2024"
    },
    {
        "paper id": "2402.14855",
        "abstract url": "https://arxiv.org/abs/2402.14855",
        "title": "An LLM Maturity Model for Reliable and Transparent Text-to-Query",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications. This maturity model seeks to fill the existing void in evaluating LLMs in such applications by incorporating dimensions beyond mere correctness or accuracy. Moreover, this work introduces a real-world use case from the law enforcement domain and showcases QueryIQ, an LLM-powered, domain-specific text-to-query assistant to expedite user workflows and reveal hidden relationship in data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2402.14856",
        "abstract url": "https://arxiv.org/abs/2402.14856",
        "title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\\textit{supposition following}$ or $\\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "31 pages, 19 figures"
    },
    {
        "paper id": "2402.14857",
        "abstract url": "https://arxiv.org/abs/2402.14857",
        "title": "Is the System Message Really Important to Jailbreaks in Large Language Models?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named \"jailbreak.\" This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the significant impact system messages can have on mitigating LLMs jailbreak. To generate system messages that are more resistant to jailbreak prompts, we propose System Messages Evolutionary Algorithms (SMEA). Through SMEA, we can get robust system messages population that demonstrate up to 98.9% resistance against jailbreak prompts. Our research not only bolsters LLMs security but also raises the bar for jailbreak, fostering advancements in this field of study.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages,3 figures"
    },
    {
        "paper id": "2402.14858",
        "abstract url": "https://arxiv.org/abs/2402.14858",
        "title": "ChatEL: Entity Linking with Chatbots",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these fine-tuned language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well. In the present work, we define ChatEL, which is a three-step framework to prompt LLMs to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough error analysis shows many instances with the ground truth labels were actually incorrect, and the labels predicted by ChatEL were actually correct. This indicates that the quantitative results presented in this paper may be a conservative estimate of the actual performance. All data and code are available as an open-source package on GitHub at https://github.com/yifding/In_Context_EL.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.14860",
        "abstract url": "https://arxiv.org/abs/2402.14860",
        "title": "Ranking Large Language Models without Ground Truth",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.14863",
        "abstract url": "https://arxiv.org/abs/2402.14863",
        "title": "Evaluation of a semi-autonomous attentive listening system with takeover prompting",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The handling of communication breakdowns and loss of engagement is an important aspect of spoken dialogue systems, particularly for chatting systems such as attentive listening, where the user is mostly speaking. We presume that a human is best equipped to handle this task and rescue the flow of conversation. To this end, we propose a semi-autonomous system, where a remote operator can take control of an autonomous attentive listening system in real-time. In order to make human intervention easy and consistent, we introduce automatic detection of low interest and engagement to provide explicit takeover prompts to the remote operator. We implement this semi-autonomous system which detects takeover points for the operator and compare it to fully tele-operated and fully autonomous attentive listening systems. We find that the semi-autonomous system is generally perceived more positively than the autonomous system. The results suggest that identifying points of conversation when the user starts to lose interest may help us improve a fully autonomous dialogue system.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.07214",
        "abstract url": "https://arxiv.org/abs/2404.07214",
        "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
        "rating": 1,
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The most extensive and up to date Survey on Visual Language Models covering 76 Visual Language Models"
    },
    {
        "paper id": "2402.12737",
        "abstract url": "https://arxiv.org/abs/2402.12737",
        "title": "Guarantee Regions for Local Explanations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Interpretability methods that utilise local surrogate models (e.g. LIME) are very good at describing the behaviour of the predictive model at a point of interest, but they are not guaranteed to extrapolate to the local region surrounding the point. However, overfitting to the local curvature of the predictive model and malicious tampering can significantly limit extrapolation. We propose an anchor-based algorithm for identifying regions in which local explanations are guaranteed to be correct by explicitly describing those intervals along which the input features can be trusted. Our method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model. We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines. We also show how our method can identify misleading local explanations with significantly poorer guarantee regions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12756",
        "abstract url": "https://arxiv.org/abs/2402.12756",
        "title": "Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Wi-Fi fingerprinting has emerged as the most popular approach to indoor localization. The use of ML algorithms has greatly improved the localization performance of Wi-Fi fingerprinting, but its success depends on the availability of fingerprint databases composed of a large number of RSSIs, the MAC addresses of access points, and the other measurement information. However, most fingerprint databases do not reflect well the time varying nature of electromagnetic interferences in complicated modern indoor environment. This could result in significant changes in statistical characteristics of training/validation and testing datasets, which are often constructed at different times, and even the characteristics of the testing datasets could be different from those of the data submitted by users during the operation of localization systems after their deployment. In this paper, we consider the implications of time-varying Wi-Fi fingerprints on indoor localization from a data-centric point of view and discuss the differences between static and dynamic databases. As a case study, we have constructed a dynamic database covering three floors of the IR building of XJTLU based on RSSI measurements, over 44 days, and investigated the differences between static and dynamic databases in terms of statistical characteristics and localization performance. The analyses based on variance calculations and Isolation Forest show the temporal shifts in RSSIs, which result in a noticeable trend of the increase in the localization error of a Gaussian process regression model with the maximum error of 6.65 m after 14 days of training without model adjustments. The results of the case study with the XJTLU dynamic database clearly demonstrate the limitations of static databases and the importance of the creation and adoption of dynamic databases for future indoor localization research and real-world deployment.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 5 figures, Invited paper with Excellent Paper Award to be presented at ICAIIC 2024, Osaka, Japan, Feb. 19--22, 2023"
    },
    {
        "paper id": "2402.12770",
        "abstract url": "https://arxiv.org/abs/2402.12770",
        "title": "Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "Workshop"
            ]
        ],
        "abstract": "In the realm of human-AI dialogue, the facilitation of empathetic responses is important. Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions. This study introduces the first framework designed to engender empathetic dialogue with validating responses. Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules. Further validation of our model's efficacy is confirmed in its application to the TUT Emotional Storytelling Corpus (TESC), a speech-based dialogue dataset, by surpassing both random baseline and the ChatGPT. This consistent performance across both textual and speech-based dialogues underscores the effectiveness of our framework in fostering empathetic human-AI communication.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "This paper has been accepted for presentation at International Workshop on Spoken Dialogue Systems Technology 2024 (IWSDS 2024)"
    },
    {
        "paper id": "2402.12789",
        "abstract url": "https://arxiv.org/abs/2402.12789",
        "title": "Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in sampling or used in training. Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12808",
        "abstract url": "https://arxiv.org/abs/2402.12808",
        "title": "Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications. Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods. In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem. We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited. We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "32 pages, 15 figures"
    },
    {
        "paper id": "2402.12845",
        "abstract url": "https://arxiv.org/abs/2402.12845",
        "title": "MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Drawing upon the intuition that aligning different modalities to the same semantic embedding space would allow models to understand states and actions more easily, we propose a new perspective to the offline reinforcement learning (RL) challenge. More concretely, we transform it into a supervised learning task by integrating multimodal and pre-trained language models. Our approach incorporates state information derived from images and action-related data obtained from text, thereby bolstering RL training performance and promoting long-term strategic thinking. We emphasize the contextual understanding of language and demonstrate how decision-making in RL can benefit from aligning states' and actions' representation with languages' representation. Our method significantly outperforms current baselines as evidenced by evaluations conducted on Atari and OpenAI Gym environments. This contributes to advancing offline RL performance and efficiency while providing a novel perspective on offline RL.Our code and data are available at https://github.com/Zheng0428/MORE_.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12852",
        "abstract url": "https://arxiv.org/abs/2402.12852",
        "title": "CCFC++: Enhancing Federated Clustering through Feature Decorrelation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In federated clustering, multiple data-holding clients collaboratively group data without exchanging raw data. This field has seen notable advancements through its marriage with contrastive learning, exemplified by Cluster-Contrastive Federated Clustering (CCFC). However, CCFC suffers from heterogeneous data across clients, leading to poor and unrobust performance. Our study conducts both empirical and theoretical analyses to understand the impact of heterogeneous data on CCFC. Findings indicate that increased data heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased correlations across multiple dimensions of the learned representations. To address this, we introduce a decorrelation regularizer to CCFC. Benefiting from the regularizer, the improved method effectively mitigates the detrimental effects of data heterogeneity, and achieves superior performance, as evidenced by a marked increase in NMI scores, with the gain reaching as high as 0.32 in the most pronounced case.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12868",
        "abstract url": "https://arxiv.org/abs/2402.12868",
        "title": "Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\u03c1\\log T)$ in stochastic environments. Here, $\u03c1> 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets. Additionally, it achieves an $O(\\sqrt{T})$ regret even in adversarial environments where FTL suffers an $\u03a9(T)$ regret, and attains an $O(\u03c1\\log T + \\sqrt{C \u03c1\\log T})$ regret bound in corrupted stochastic environments with corruption level $C$. Furthermore, by extending our analysis, we establish a regret upper bound of $O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for $q$-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and $\\ell_p$-balls for $p \\in [1,\\infty)$. This bound bridges the gap between the $O(\\log T)$ regret bound for strongly convex sets ($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$).",
        "subjects": [
            "cs.LG"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2402.12887",
        "abstract url": "https://arxiv.org/abs/2402.12887",
        "title": "The practice of qualitative parameterisation in the development of Bayesian networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The typical phases of Bayesian network (BN) structured development include specification of purpose and scope, structure development, parameterisation and validation. Structure development is typically focused on qualitative issues and parameterisation quantitative issues, however there are qualitative and quantitative issues that arise in both phases. A common step that occurs after the initial structure has been developed is to perform a rough parameterisation that only captures and illustrates the intended qualitative behaviour of the model. This is done prior to a more rigorous parameterisation, ensuring that the structure is fit for purpose, as well as supporting later development and validation. In our collective experience and in discussions with other modellers, this step is an important part of the development process, but is under-reported in the literature. Since the practice focuses on qualitative issues, despite being quantitative in nature, we call this step qualitative parameterisation and provide an outline of its role in the BN development process.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "6 pages, 2 figures, technical note"
    },
    {
        "paper id": "2402.12907",
        "abstract url": "https://arxiv.org/abs/2402.12907",
        "title": "Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "13 pages, 2 figures"
    },
    {
        "paper id": "2402.12916",
        "abstract url": "https://arxiv.org/abs/2402.12916",
        "title": "Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of Machine Learning Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products. With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems. This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks. By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape. This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more significant outcomes in increasingly intricate data domains. Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12930",
        "abstract url": "https://arxiv.org/abs/2402.12930",
        "title": "Learning Exceptional Subgroups by End-to-End Maximizing KL-divergence",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Finding and describing sub-populations that are exceptional regarding a target property has important applications in many scientific disciplines, from identifying disadvantaged demographic groups in census data to finding conductive molecules within gold nanoparticles. Current approaches to finding such subgroups require pre-discretized predictive variables, do not permit non-trivial target distributions, do not scale to large datasets, and struggle to find diverse results. To address these limitations, we propose Syflow, an end-to-end optimizable approach in which we leverage normalizing flows to model arbitrary target distributions, and introduce a novel neural layer that results in easily interpretable subgroup descriptions. We demonstrate on synthetic and real-world data, including a case study, that Syflow reliably finds highly exceptional subgroups accompanied by insightful descriptions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12991",
        "abstract url": "https://arxiv.org/abs/2402.12991",
        "title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13006",
        "abstract url": "https://arxiv.org/abs/2402.13006",
        "title": "Investigating the Impact of Model Instability on Explanations and Uncertainty",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during the training process. This suggests that noise-augmented models may be better at identifying salient tokens when uncertain. Furthermore, when predictive and epistemic uncertainty measures are over-confident, the robustness of a saliency map to perturbation can indicate model stability issues. Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance; however, this phenomenon is limited to smaller Transformer-based language models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13007",
        "abstract url": "https://arxiv.org/abs/2402.13007",
        "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed \"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13019",
        "abstract url": "https://arxiv.org/abs/2402.13019",
        "title": "Improving Neural-based Classification with Logical Background Knowledge",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditioning at inference can be used to build more accurate neural-based systems with fewer resources while guaranteeing the semantic consistency of outputs.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "9 pages, 3 figures, submitted to IJCAI 2024"
    },
    {
        "paper id": "2402.13028",
        "abstract url": "https://arxiv.org/abs/2402.13028",
        "title": "Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables",
        "rating": 0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces. It usually involves evidence retrieval and veracity reasoning. In this paper, we focus on the latter, reasoning over unstructured text and structured table information. Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational graph neural network, facilitating interactions between claims and evidence. An attention-based method is utilized to integrate information, combined with a language model for generating predictions. We introduce a multitask loss function to account for potential inaccuracies in evidence retrieval. Comprehensive experiments on the large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC. Code will be released at: https://github.com/Deno-V/HeterFC.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by 38th Association for the Advancement of Artificial Intelligence, AAAI"
    },
    {
        "paper id": "2402.13037",
        "abstract url": "https://arxiv.org/abs/2402.13037",
        "title": "Align Your Intents: Offline Imitation Learning via Optimal Transport",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and improves the performance of other offline RL algorithms in the sparse-reward tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13077",
        "abstract url": "https://arxiv.org/abs/2402.13077",
        "title": "Mechanistic Neural Networks for Scientific Machine Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences. It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling. Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs. This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing. Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling. We prove their comprehensive capabilities in analyzing and interpreting complex scientific data across various applications, showing significant performance against specialized state-of-the-art methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13087",
        "abstract url": "https://arxiv.org/abs/2402.13087",
        "title": "How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight? This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirical bound derived even under the strongest audit setup. The gap found is not a fluke. Our subsequent study provides an improved privacy result for private hyper-parameter tuning due to its distinct properties. Our privacy results are also more generalizable compared to prior analyses that are only easily applicable in specific setups.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13089",
        "abstract url": "https://arxiv.org/abs/2402.13089",
        "title": "Towards an empirical understanding of MoE design choices",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13147",
        "abstract url": "https://arxiv.org/abs/2402.13147",
        "title": "SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that the imitation policy is trained towards sub-optimal policies. In this paper, to address these issues, we propose a new approach based on inverse soft-Q learning, where a regularization term is added to the training objective, with the aim of aligning the learned rewards with a pre-assigned reward function that allocates higher weights to state-action pairs from expert demonstrations, and lower weights to those from lower expertise levels. On standard benchmarks, our inverse soft-Q learning significantly outperforms other offline IL baselines by a large margin.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13182",
        "abstract url": "https://arxiv.org/abs/2402.13182",
        "title": "Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13187",
        "abstract url": "https://arxiv.org/abs/2402.13187",
        "title": "Testing Calibration in Subquadratic Time",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$ is $\\varepsilon$-far from calibration. We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factors) in time $O(n^{1.5} \\log(n))$. This improves upon state-of-the-art black-box linear program solvers requiring $\u03a9(n^\u03c9)$ time, where $\u03c9> 2$ is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem, and give sample complexity lower bounds for alternative calibration distances to the one considered in this work. Finally, we present preliminary experiments showing that the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale to accommodate moderate sample sizes.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13188",
        "abstract url": "https://arxiv.org/abs/2402.13188",
        "title": "Question Calibration and Multi-Hop Modeling for Temporal Question Answering",
        "rating": 0.5,
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks. In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities. (II) They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering. To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach. Specifically, We first calibrate the question representation by fusing the question and the time-constrained concepts in KG. Then, we construct the GNN layer to complete multi-hop message passing. Finally, the question representation is combined with the embedding output by the GNN to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions dataset's complex questions are absolutely improved by 5.1% and 1.2% compared to the best-performing baseline. Moreover, QC-MHM can generate interpretable and trustworthy predictions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by AAAI 2024"
    },
    {
        "paper id": "2402.13196",
        "abstract url": "https://arxiv.org/abs/2402.13196",
        "title": "Practical Kernel Tests of Conditional Independence",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We describe a data-efficient, kernel-based approach to statistical testing of conditional independence. A major challenge of conditional independence testing, absent in tests of unconditional independence, is to obtain the correct test level (the specified upper bound on the rate of false positives), while still attaining competitive test power. Excess false positives arise due to bias in the test statistic, which is obtained using nonparametric kernel ridge regression. We propose three methods for bias control to correct the test level, based on data splitting, auxiliary data, and (where possible) simpler function classes. We show these combined strategies are effective both for synthetic and real-world data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13210",
        "abstract url": "https://arxiv.org/abs/2402.13210",
        "title": "Bayesian Reward Models for LLM Alignment",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimization in best-of-n sampling.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13244",
        "abstract url": "https://arxiv.org/abs/2402.13244",
        "title": "Are Fact-Checking Tools Reliable? An Evaluation of Google Fact Check",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Fact-checking is an effective approach to combat misinformation on social media, especially regarding significant social events such as the COVID-19 pandemic and the U.S. presidential elections. In this study, we evaluated the performance of Google Fact Check, a fact-checking search engine. By analyzing the search results regarding 1,000 COVID-19-related false claims, we found Google Fact Check not likely to provide sufficient fact-checking information for most false claims, even though the results obtained are generally reliable and helpful. We also found that the corresponding false claims of different fact-checking verdicts (i.e., \"False\", \"Partly False\", \"True\", and \"Unratable\") tend to reflect diverse emotional tones, and fact-checking sources are likely to check the claims in different lengths and using dictionary words to various extents. Claims addressing the same issue yet described differently are likely to obtain disparate fact-checking results. This research aims to shed light on the best practices for performing fact-checking searches for the general public.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13332",
        "abstract url": "https://arxiv.org/abs/2402.13332",
        "title": "Causal hybrid modeling with double machine learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hybrid modeling integrates machine learning with scientific knowledge to enhance interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emphasizes the necessity of explicitly defining causal graphs and relationships, advocating for this as a general best practice. We encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13366",
        "abstract url": "https://arxiv.org/abs/2402.13366",
        "title": "Statistical curriculum learning: An elimination algorithm achieving an oracle risk",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider a statistical version of curriculum learning (CL) in a parametric prediction setting. The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. We consider three types of learners, depending on the level of side-information they receive. The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. The third, a fully adaptive learner, estimates the target parameter vector without any prior information. In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners. We develop an adaptive multiple elimination-rounds CL algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner. We consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound. We derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13368",
        "abstract url": "https://arxiv.org/abs/2402.13368",
        "title": "Unsupervised Concept Discovery Mitigates Spurious Correlations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring sub-groups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for subpopulation shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13379",
        "abstract url": "https://arxiv.org/abs/2402.13379",
        "title": "Referee-Meta-Learning for Fast Adaptation of Locational Fairness",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model. Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step. We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13380",
        "abstract url": "https://arxiv.org/abs/2402.13380",
        "title": "Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time, optimal gap, and percent infeasibility over 240K benchmark CLSP instances tested. After the ML model is trained, conducting inference on the model, including post-processing, reduces the MIP into a linear program (LP). This transforms the ML-based algorithm, combined with an LP solver, into a polynomial-time approximation algorithm to solve a well-known NP-Hard problem, with almost perfect solution quality.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13388",
        "abstract url": "https://arxiv.org/abs/2402.13388",
        "title": "Transformer tricks: Precomputing the first layer",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This micro-paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, PaLM, and Gemma). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model is limited to 3% savings. See https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "5 pages, 2 figures"
    },
    {
        "paper id": "2402.13393",
        "abstract url": "https://arxiv.org/abs/2402.13393",
        "title": "Fairness Risks for Group-conditionally Missing Demographics",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13399",
        "abstract url": "https://arxiv.org/abs/2402.13399",
        "title": "Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants to rapidly learn those norms. We formalize this framework in the context of Markov games and demonstrate its operation in a multi-agent environment via approximately Bayesian rule induction of obligative and prohibitive norms. Using our approach, agents are able to rapidly learn and sustain a variety of cooperative institutions, including resource management norms and compensation for pro-social labor, promoting collective welfare while still allowing agents to act in their own interests.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Accepted to the 23rd International Conference on Autonomous Agents and Multi-Agent Systems, 8 pages (excl. references), 6 figures/tables, (Appendix: 7 pages, 6 figures/tables). Code available at: https://github.com/ninell-oldenburg/social-contracts"
    },
    {
        "paper id": "2402.13414",
        "abstract url": "https://arxiv.org/abs/2402.13414",
        "title": "Harnessing Large Language Models as Post-hoc Correctors",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions. Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13425",
        "abstract url": "https://arxiv.org/abs/2402.13425",
        "title": "Investigating the Histogram Loss in Regression",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. We then demonstrate the viability of the Histogram Loss in common deep learning applications without a need for costly hyperparameter tuning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "50 pages"
    },
    {
        "paper id": "2402.13440",
        "abstract url": "https://arxiv.org/abs/2402.13440",
        "title": "A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Multi-agent reinforcement learning (MARL) is well-suited for runtime decision-making in optimizing the performance of systems where multiple agents coexist and compete for shared resources. However, applying common deep learning-based MARL solutions to real-world problems suffers from issues of interpretability, sample efficiency, partial observability, etc. To address these challenges, we present an event-driven formulation, where decision-making is handled by distributed co-operative MARL agents using neuro-symbolic methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN) framework serves as a function approximator for the RL, to train a rules-based policy that is both logical and interpretable by construction. To enable decision-making under uncertainty and partial observability, we developed a novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural Networks (PLNN), which combines the capabilities of logical reasoning with probabilistic graphical models. In PLNN, the upward/downward inference strategy, inherited from LNN, is coupled with belief bounds by setting the activation function for the logical operator associated with each neural network node to a probability-respecting generalization of the Fr\u00e9chet inequalities. These PLNN nodes form the unifying element that combines probabilistic logic and Bayes Nets, permitting inference for variables with unobserved states. We demonstrate our contributions by addressing key MARL challenges for power sharing in a system-on-chip application.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13454",
        "abstract url": "https://arxiv.org/abs/2402.13454",
        "title": "Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "With increasing volume of data being used across machine learning tasks, the capability to target specific subsets of data becomes more important. To aid in this capability, the recently proposed Submodular Mutual Information (SMI) has been effectively applied across numerous tasks in literature to perform targeted subset selection with the aid of a exemplar query set. However, all such works are deficient in providing theoretical guarantees for SMI in terms of its sensitivity to a subset's relevance and coverage of the targeted data. For the first time, we provide such guarantees by deriving similarity-based bounds on quantities related to relevance and coverage of the targeted data. With these bounds, we show that the SMI functions, which have empirically shown success in multiple applications, are theoretically sound in achieving good query relevance and query coverage.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 10 figures"
    },
    {
        "paper id": "2402.13468",
        "abstract url": "https://arxiv.org/abs/2402.13468",
        "title": "STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\\%-24\\%$ and rare-class F-1 score by $17\\%-40\\%$ on multiple text classification datasets over common active learning methods within the class-imbalanced cold-start setting.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "11 pages, 1 figure"
    },
    {
        "paper id": "2402.13485",
        "abstract url": "https://arxiv.org/abs/2402.13485",
        "title": "ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the computation and parallelism of the verification phase in real-time and maximize the overall efficiency across different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD consistently outperforms existing decoding algorithms by 1.1-3.2x.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13516",
        "abstract url": "https://arxiv.org/abs/2402.13516",
        "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named \"ProSparse\" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along sine curves in multiple stages. This can enhance activation sparsity and alleviate performance degradation by avoiding radical shifts in activation distribution. With ProSparse, we obtain high sparsity of 89.32% and 88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable performance to their original Swish-activated versions. Our inference acceleration experiments further demonstrate the practical acceleration brought by higher activation sparsity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 3 figures, 7 tables"
    },
    {
        "paper id": "2402.13525",
        "abstract url": "https://arxiv.org/abs/2402.13525",
        "title": "MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight. Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data. However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive. Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models. In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices. Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms. MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-based DNNs.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13528",
        "abstract url": "https://arxiv.org/abs/2402.13528",
        "title": "Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.14035",
        "abstract url": "https://arxiv.org/abs/2402.14035",
        "title": "Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foundation model teachers and complementary teachers. Complementary teachers possess model characteristics akin to the student's, aiming to bridge the gap between the foundation model and specialized application models for a smoother knowledge transfer. Further, to accommodate the dissimilarity among the teachers in the committee, we introduce DiverseDistill, which allows the student to understand the expertise of each teacher and extract task knowledge. Our evaluations demonstrate that adding complementary teachers enhances student performance. Finally, DiverseDistill consistently outperforms baseline distillation methods, regardless of the teacher choices, resulting in significantly improved student performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.15526",
        "abstract url": "https://arxiv.org/abs/2402.15526",
        "title": "Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory. Existing approaches attempted to address this issue by decomposing or rewriting input instructions, yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g., programming within the context of software development). In response, this paper proposes a simple yet effective method named Chain-of-Specificity (CoS). Specifically, CoS iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within LLMs, and refines responses. Experiments conducted on publicly available and self-build complex datasets demonstrate that CoS outperforms existing methods in enhancing generated content especially for the specificity. Besides, as the number of specific constraints increase, other baselines falter, while CoS still performs well. Moreover, we show that distilling responses generated by CoS effectively enhances the ability of smaller models to follow the constrained instructions. Resources of this paper will be released for further research.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.05563",
        "abstract url": "https://arxiv.org/abs/2403.05563",
        "title": "The challenges of massification in higher education in Africa",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Like many developing countries, Togo faces the challenge of massification in higher education resulting from a large increase in the number of students enrolled in its public universities. Encouraged by the public authorities, with the support of the United Nations and Unesco, the number of students to be trained continues to grow to provide the country with qualified professionals and meet its socioeconomic needs. The number of students in large groups (over 3,000 in some courses) raises issues of training quality and equity (availability of resources, reproducibility of content, study conditions, access to digital solutions, etc.). Access to this type of training requires special training conditions and infrastructures that are not always available in developing countries. This article presents a qualitative study carried out with undergraduate students and teachers at the University of Lom{\u00e9} concerning teaching and learning conditions in large groups and a critical analysis of the solutions implemented by the university. This work can be transposed to other African countries with similar needs and will open the way to a solution analogous to intelligent classrooms for face-to-face courses.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.14638",
        "abstract url": "https://arxiv.org/abs/2403.14638",
        "title": "Personalized Programming Guidance based on Deep Programming Learning Style Capturing",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "With the rapid development of big data and AI technology, programming is in high demand and has become an essential skill for students. Meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. Previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. However, two significant challenges still need to be addressed in programming: C1) how to recognize complex programming behaviors; C2) how to capture intrinsic learning patterns that align with the actual learning process. To fill these gaps, in this paper, we propose a novel model called Programming Exercise Recommender with Learning Style (PERS), which simulates learners' intricate programming behaviors. Specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to capture the changes of consecutive code submissions (which addresses C1). To better profile programming behaviors, we extend the Felder-Silverman learning style model, a classical pedagogical theory, to perceive intrinsic programming patterns. Based on this, we align three latent vectors to record and update programming ability, processing style, and understanding style, respectively (which addresses C2). We perform extensive experiments on two real-world datasets to verify the rationality of modeling programming learning styles and the effectiveness of PERS for personalized programming guidance.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "18th International Conference on Computer Science & Education"
    },
    {
        "paper id": "2403.14639",
        "abstract url": "https://arxiv.org/abs/2403.14639",
        "title": "On Defining Smart Cities using Transformer Neural Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Cities worldwide are rapidly adopting smart technologies, transforming urban life. Despite this trend, a universally accepted definition of 'smart city' remains elusive. Past efforts to define it have not yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new 'compromise' definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing definitions of smart cities, generating a list of potential new composite definitions. Each of these new definitions was then tested against the pre-existing individual definitions we have gathered, using cosine similarity as our metric. This process identified smart city definitions with the highest average cosine similarity, semantically positioning them as the closest on average to all the 60 individual definitions selected.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "16 pages, 2 fugures"
    },
    {
        "paper id": "2402.12728",
        "abstract url": "https://arxiv.org/abs/2402.12728",
        "title": "Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering",
        "rating": 0,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two benchmark datasets show the superiority of MAIL with 24x less resources.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages,3 figures and 1 page appendix; The processed graphs and codes will be avalibale"
    },
    {
        "paper id": "2402.12754",
        "abstract url": "https://arxiv.org/abs/2402.12754",
        "title": "Fingerprint Presentation Attack Detector Using Global-Local Model",
        "rating": 0,
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The vulnerability of automated fingerprint recognition systems (AFRSs) to presentation attacks (PAs) promotes the vigorous development of PA detection (PAD) technology. However, PAD methods have been limited by information loss and poor generalization ability, resulting in new PA materials and fingerprint sensors. This paper thus proposes a global-local model-based PAD (RTK-PAD) method to overcome those limitations to some extent. The proposed method consists of three modules, called: 1) the global module; 2) the local module; and 3) the rethinking module. By adopting the cut-out-based global module, a global spoofness score predicted from nonlocal features of the entire fingerprint images can be achieved. While by using the texture in-painting-based local module, a local spoofness score predicted from fingerprint patches is obtained. The two modules are not independent but connected through our proposed rethinking module by localizing two discriminative patches for the local module based on the global spoofness score. Finally, the fusion spoofness score by averaging the global and local spoofness scores is used for PAD. Our experimental results evaluated on LivDet 2017 show that the proposed RTK-PAD can achieve an average classification error (ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false detection rate (FDR) equals 1.0%, which significantly outperformed the state-of-the-art methods by $\\sim$10% in terms of TDR (91.19% versus 80.74%).",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This paper was accepted by IEEE Transactions on Cybernetics. Current version is updated with minor revisions on introduction and related works"
    },
    {
        "paper id": "2402.12792",
        "abstract url": "https://arxiv.org/abs/2402.12792",
        "title": "OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "voxel",
                "depth",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12797",
        "abstract url": "https://arxiv.org/abs/2402.12797",
        "title": "A Geometric Algorithm for Tubular Shape Reconstruction from Skeletal Representation",
        "rating": 0,
        "keywords": [
            [
                "voxel"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce a novel approach for the reconstruction of tubular shapes from skeletal representations. Our method processes all skeletal points as a whole, eliminating the need for splitting input structure into multiple segments. We represent the tubular shape as a truncated signed distance function (TSDF) in a voxel hashing manner, in which the signed distance between a voxel center and the object is computed through a simple geometric algorithm. Our method does not involve any surface sampling scheme or solving large matrix equations, and therefore is a faster and more elegant solution for tubular shape reconstruction compared to other approaches. Experiments demonstrate the efficiency and effectiveness of the proposed method. Code is avaliable at https://github.com/wlsdzyzl/Dragon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages (without reference), 6 figures"
    },
    {
        "paper id": "2402.12874",
        "abstract url": "https://arxiv.org/abs/2402.12874",
        "title": "Skill or Luck? Return Decomposition via Advantage Functions",
        "rating": 0.0,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICLR 2024"
    },
    {
        "paper id": "2402.12875",
        "abstract url": "https://arxiv.org/abs/2402.12875",
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
        "rating": 0.0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "38 pages. Accepted by ICLR 2024"
    },
    {
        "paper id": "2402.12890",
        "abstract url": "https://arxiv.org/abs/2402.12890",
        "title": "More Discriminative Sentence Embeddings via Semantic Graph Smoothing",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion. Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks. Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted in EACL 2024"
    },
    {
        "paper id": "2402.12891",
        "abstract url": "https://arxiv.org/abs/2402.12891",
        "title": "Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard Plenoptic Camera",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Among the common applications of plenoptic cameras are depth reconstruction and post-shot refocusing. These require a calibration relating the camera-side light field to that of the scene. Numerous methods with this goal have been developed based on thin lens models for the plenoptic camera's main lens and microlenses. Our work addresses the often-overlooked role of the main lens exit pupil in these models and specifically in the decoding process of standard plenoptic camera (SPC) images. We formally deduce the connection between the refocusing distance and the resampling parameter for the decoded light field and provide an analysis of the errors that arise when the exit pupil is not considered. In addition, previous work is revisited with respect to the exit pupil's role and all theoretical results are validated through a ray-tracing-based simulation. With the public release of the evaluated SPC designs alongside our simulation and experimental data we aim to contribute to a more accurate and nuanced understanding of plenoptic camera optics.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "29 pages, 16 figures, Accepted for publication in MDPI Sensors, Special Issue 'Short-Range Optical 3D Scanning and 3D Data Processing '"
    },
    {
        "paper id": "2402.12908",
        "abstract url": "https://arxiv.org/abs/2402.12908",
        "title": "RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project: https://github.com/YangLing0818/RealCompo"
    },
    {
        "paper id": "2402.12974",
        "abstract url": "https://arxiv.org/abs/2402.12974",
        "title": "Visual Style Prompting with Swapping Self-Attention",
        "rating": 0,
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \\ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available https://curryjung.github.io/VisualStylePrompt/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13040",
        "abstract url": "https://arxiv.org/abs/2402.13040",
        "title": "Text-Guided Molecule Generation with Diffusion Language Model",
        "rating": 0.0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains. Code will be released at: https://github.com/Deno-V/tgm-dlm.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by 38th Association for the Advancement of Artificial Intelligence, AAAI"
    },
    {
        "paper id": "2402.13048",
        "abstract url": "https://arxiv.org/abs/2402.13048",
        "title": "Stable Knowledge Editing in Large Language Models",
        "rating": 0,
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale. However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. It introduces instability to the performance of the knowledge editing method. To transcend these assumptions, we introduce StableKE, a method adopts a novel perspective based on knowledge augmentation rather than knowledge localization. To overcome the expense of human labeling, StableKE integrates two automated knowledge augmentation strategies: Semantic Paraphrase Enhancement strategy, which diversifies knowledge descriptions to facilitate the teaching of new information to the model, and Contextual Description Enrichment strategy, expanding the surrounding knowledge to prevent the forgetting of related information. StableKE surpasses other knowledge editing methods, demonstrating stability both edited knowledge and multi-hop knowledge, while also preserving unrelated knowledge and general abilities. Moreover, StableKE can edit knowledge on ChatGPT.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13093",
        "abstract url": "https://arxiv.org/abs/2402.13093",
        "title": "Event-level Knowledge Editing",
        "rating": 0,
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies. We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses significant challenges to existing knowledge editing approaches. Our codes and dataset are publicly released to facilitate further research.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "18 pages, 2 figures"
    },
    {
        "paper id": "2402.13185",
        "abstract url": "https://arxiv.org/abs/2402.13185",
        "title": "UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing",
        "rating": 0,
        "keywords": [
            [
                "video editing",
                "text-to-video"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://jianhongbai.github.io/UniEdit/"
    },
    {
        "paper id": "2402.13241",
        "abstract url": "https://arxiv.org/abs/2402.13241",
        "title": "Federated Causal Discovery from Heterogeneous Data",
        "rating": 0.0,
        "keywords": [
            [
                "skeleton"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy. Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models. We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method. The code is available at https://github.com/lokali/FedCDH.git.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICLR 2024"
    },
    {
        "paper id": "2402.13243",
        "abstract url": "https://arxiv.org/abs/2402.13243",
        "title": "VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning",
        "rating": 0,
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://hgao-cv.github.io/VADv2"
    },
    {
        "paper id": "2402.13349",
        "abstract url": "https://arxiv.org/abs/2402.13349",
        "title": "Aria Everyday Activities Dataset",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses. AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains multimodal sensor data recorded through the Project Aria glasses. In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. AEA is an open source dataset that can be downloaded from https://www.projectaria.com/datasets/aea/. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools https://github.com/facebookresearch/projectaria_tools.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Dataset website: https://www.projectaria.com/datasets/aea/"
    },
    {
        "paper id": "2402.13372",
        "abstract url": "https://arxiv.org/abs/2402.13372",
        "title": "EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limitations and the value of dynamic datasets in uncovering them.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted for publication in main proceedings of LREC-COLING 2024, 16 pages, 3 figures"
    },
    {
        "paper id": "2402.13404",
        "abstract url": "https://arxiv.org/abs/2402.13404",
        "title": "Layout-to-Image Generation with Localized Descriptions using ControlNet with Cross-Attention Control",
        "rating": 0,
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "While text-to-image diffusion models can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images. Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout. Arguably the most popular among such methods, ControlNet, enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps). However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the prompt. In this work, we show the limitations of ControlNet for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation. We adapt and investigate several existing cross-attention control methods in the context of ControlNet and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions. To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control. Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13419",
        "abstract url": "https://arxiv.org/abs/2402.13419",
        "title": "Reward Bound for Behavioral Guarantee of Model-based Planning Agents",
        "rating": 0.0,
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.AI"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Recent years have seen an emerging interest in the trustworthiness of machine learning-based agents in the wild, especially in robotics, to provide safety assurance for the industry. Obtaining behavioral guarantees for these agents remains an important problem. In this work, we focus on guaranteeing a model-based planning agent reaches a goal state within a specific future time step. We show that there exists a lower bound for the reward at the goal state, such that if the said reward is below that bound, it is impossible to obtain such a guarantee. By extension, we show how to enforce preferences over multiple goals.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "To be published in ICLR 24 tiny paper track"
    },
    {
        "paper id": "2402.13448",
        "abstract url": "https://arxiv.org/abs/2402.13448",
        "title": "ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance",
        "rating": 0,
        "keywords": [
            [
                "time-efficient"
            ],
            [
                "bio-medical",
                "medical",
                "diagnosis"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learning to minimize ED wait time and maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours. Ablation studies demonstrate the importance of model scale and use of a bio-medical language model. Further analyses reveal the necessity of personalized laboratory test suggestions for diagnosing patients with severe cases, as well as the potential of ED-Copilot in providing ED clinicians with informative laboratory test recommendations. Our code is available at https://github.com/cxcscmu/ED-Copilot.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13462",
        "abstract url": "https://arxiv.org/abs/2402.13462",
        "title": "Potential and Challenges of Model Editing for Social Debiasing",
        "rating": 0,
        "keywords": [
            [
                "Model Editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2402.13488",
        "abstract url": "https://arxiv.org/abs/2402.13488",
        "title": "A Feature Matching Method Based on Multi-Level Refinement Strategy",
        "rating": 0,
        "keywords": [
            [
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Feature matching is a fundamental and crucial process in visual SLAM, and precision has always been a challenging issue in feature matching. In this paper, based on a multi-level fine matching strategy, we propose a new feature matching method called KTGP-ORB. This method utilizes the similarity of local appearance in the Hamming space generated by feature descriptors to establish initial correspondences. It combines the constraint of local image motion smoothness, uses the GMS algorithm to enhance the accuracy of initial matches, and finally employs the PROSAC algorithm to optimize matches, achieving precise matching based on global grayscale information in Euclidean space. Experimental results demonstrate that the KTGP-ORB method reduces the error by an average of 29.92% compared to the ORB algorithm in complex scenes with illumination variations and blur.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13490",
        "abstract url": "https://arxiv.org/abs/2402.13490",
        "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "synthesis",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a \"baseline\" that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13510",
        "abstract url": "https://arxiv.org/abs/2402.13510",
        "title": "SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The widespread adoption of implicit neural representations, especially Neural Radiance Fields (NeRF), highlights a growing need for editing capabilities in implicit 3D models, essential for tasks like scene post-processing and 3D content creation. Despite previous efforts in NeRF editing, challenges remain due to limitations in editing flexibility and quality. The key issue is developing a neural representation that supports local edits for real-time updates. Current NeRF editing methods, offering pixel-level adjustments or detailed geometry and color modifications, are mostly limited to static scenes. This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level editing in dynamic settings, specifically targeting the D-NeRF network. It allows for consistent edits across sequences by mapping editing actions to a specific timeframe, freezing the deformation network responsible for dynamic scene representation, and using a teacher-student approach to integrate changes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13517",
        "abstract url": "https://arxiv.org/abs/2402.13517",
        "title": "Round Trip Translation Defence against Large Language Model Jailbreaking Attacks",
        "rating": 0,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence",
        "subjects": [
            "cs.CL"
        ],
        "comment": "6 pages, 6 figures"
    },
    {
        "paper id": "2402.12729",
        "abstract url": "https://arxiv.org/abs/2402.12729",
        "title": "Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep transfer learning (DTL) is a fundamental method in the field of Intelligent Fault Detection (IFD). It aims to mitigate the degradation of method performance that arises from the discrepancies in data distribution between training set (source domain) and testing set (target domain). Considering the fact that fault data collection is challenging and certain faults are scarce, DTL-based methods face the limitation of available observable data, which reduces the detection performance of the methods in the target domain. Furthermore, DTL-based methods lack comprehensive uncertainty analysis that is essential for building reliable IFD systems. To address the aforementioned problems, this paper proposes a novel DTL-based method known as Neural Processes-based deep transfer learning with graph convolution network (GTNP). Feature-based transfer strategy of GTNP bridges the data distribution discrepancies of source domain and target domain in high-dimensional space. Both the joint modeling based on global and local latent variables and sparse sampling strategy reduce the demand of observable data in the target domain. The multi-scale uncertainty analysis is obtained by using the distribution characteristics of global and local latent variables. Global analysis of uncertainty enables GTNP to provide quantitative values that reflect the complexity of methods and the difficulty of tasks. Local analysis of uncertainty allows GTNP to model uncertainty (confidence of the fault detection result) at each sample affected by noise and bias. The validation of the proposed method is conducted across 3 IFD tasks, consistently showing the superior detection performance of GTNP compared to the other DTL-based methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12760",
        "abstract url": "https://arxiv.org/abs/2402.12760",
        "title": "A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis",
        "rating": -0.5,
        "keywords": [
            [
                "Synthesis",
                "Text-to-Image"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.",
        "subjects": [
            "cs.MM"
        ],
        "comment": "Accepted by The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)"
    },
    {
        "paper id": "2402.12780",
        "abstract url": "https://arxiv.org/abs/2402.12780",
        "title": "Tackling Byzantine Clients in Federated Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The possibility of adversarial (a.k.a., {\\em Byzantine}) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\\mathsf{FedAvg}$ algorithm by a \\emph{robust averaging rule}. While a significant amount of work has been devoted to studying the convergence of federated {\\em robust averaging} (which we denote by $\\mathsf{FedRo}$), prior work has largely ignored the impact of {\\em client subsampling} and {\\em local steps}, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis of $\\mathsf{FedRo}$ tightly analyzing the impact of client subsampling and local steps. Specifically, we present a sufficient condition on client subsampling for nearly-optimal convergence of $\\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show that the rate of improvement in learning accuracy {\\em diminishes} with respect to the number of clients subsampled, as soon as the sample size exceeds a threshold value. Interestingly, we also observe that under a careful choice of step-sizes, the learning error due to Byzantine clients decreases with the number of local steps. We validate our theory by experiments on the FEMNIST and CIFAR-$10$ image classification tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12812",
        "abstract url": "https://arxiv.org/abs/2402.12812",
        "title": "Scalable Decentralized Algorithms for Online Personalized Mean Estimation",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based approach, with complexity of O( r |A| log |A|) and O(r |A|), respectively. We establish conditions under which both algorithms yield asymptotically optimal estimates and offer a theoretical characterization of their performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12843",
        "abstract url": "https://arxiv.org/abs/2402.12843",
        "title": "SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets",
        "rating": -0.5,
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Published at ICLR Tiny Paper 2024"
    },
    {
        "paper id": "2402.12854",
        "abstract url": "https://arxiv.org/abs/2402.12854",
        "title": "Differentiable Mapper For Topological Optimization Of Data Representation",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itself. In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper graphs. In order to achieve this, we propose a relaxed and more general version of the Mapper graph, whose convergence properties are investigated. Finally, we demonstrate the usefulness of our approach by optimizing Mapper graph representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12861",
        "abstract url": "https://arxiv.org/abs/2402.12861",
        "title": "Bounding Reconstruction Attack Success of Adversaries Without Data Priors",
        "rating": -0.5,
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12876",
        "abstract url": "https://arxiv.org/abs/2402.12876",
        "title": "Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable insights into the strengths and limitations of existing baseline methods, contributing to the ongoing discourse on optimal FMTL application in practical scenarios. The source code can be found on https://github.com/youngfish42/FMTL-Benchmark .",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by ICMR'24"
    },
    {
        "paper id": "2402.12905",
        "abstract url": "https://arxiv.org/abs/2402.12905",
        "title": "Locally Rainbow Paths",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "We introduce the algorithmic problem of finding a locally rainbow path of length $\\ell$ connecting two distinguished vertices $s$ and $t$ in a vertex-colored directed graph. Herein, a path is locally rainbow if between any two visits of equally colored vertices, the path traverses consecutively at least $r$ differently colored vertices. This problem generalizes the well-known problem of finding a rainbow path. It finds natural applications whenever there are different types of resources that must be protected from overuse, such as crop sequence optimization or production process scheduling. We show that the problem is computationally intractable even if $r=2$ or if one looks for a locally rainbow among the shortest paths. On the positive side, if one looks for a path that takes only a short detour (i.e., it is slightly longer than the shortest path) and if $r$ is small, the problem can be solved efficiently. Indeed, the running time of the respective algorithm is near-optimal unless the ETH fails.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "Accepted at AAAI 2024"
    },
    {
        "paper id": "2402.12937",
        "abstract url": "https://arxiv.org/abs/2402.12937",
        "title": "GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks",
        "rating": -0.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We address the growing apprehension that GNNs, in the absence of fairness constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals. Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of fairness to be used within the GNN framework. Our proposal, GRAPHGINI, works with the two different goals of individual and group fairness in a single system, while maintaining high prediction accuracy. GRAPHGINI enforces individual fairness through learnable attention scores that help in aggregating more information through similar nodes. A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group fairness. Both the individual fairness constraint and the group fairness constraint are stated in terms of a differentiable approximation of the Gini coefficient. This approximation is a contribution that is likely to be of interest even beyond the scope of the problem studied in this paper. Unlike other state-of-the-art, GRAPHGINI automatically balances all three optimization objectives (utility, individual, and group fairness) of the GNN and is free from any manual tuning of weight parameters. Extensive experimentation on real-world datasets showcases the efficacy of GRAPHGINI in making significant improvements in individual fairness compared to all currently available state-of-the-art methods while maintaining utility and group equality.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12938",
        "abstract url": "https://arxiv.org/abs/2402.12938",
        "title": "UniCell: Universal Cell Nucleus Classification via Prompt Learning",
        "rating": -0.5,
        "keywords": [
            [
                "diagnosis",
                "pathological"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis. Numerous pathological datasets are currently available, but their annotations are inconsistent. Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition. In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains. In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets. Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features. The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources. Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks. Code and models are available at https://github.com/lhaof/UniCell",
        "subjects": [
            "cs.CV"
        ],
        "comment": "AAAI 2024, Code and models are available at https://github.com/lhaof/UniCell"
    },
    {
        "paper id": "2402.12939",
        "abstract url": "https://arxiv.org/abs/2402.12939",
        "title": "Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space",
        "rating": -0.5,
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability. However, the complexity of their policies often makes them challenging to understand. In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks. Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task. Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements. We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Submitted to the European Control Conference 2024"
    },
    {
        "paper id": "2402.12945",
        "abstract url": "https://arxiv.org/abs/2402.12945",
        "title": "Stochastic Approximation Approach to Federated Machine Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper examines Federated learning (FL) in a Stochastic Approximation (SA) framework. FL is a collaborative way to train neural network models across various participants or clients without centralizing their data. Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation. The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training. SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function. In this paper the clients use a stochastic approximation iterate to update the weights of its neural network. It is shown that the aggregated weights track an autonomous ODE. Numerical simulations are performed and the results are compared with standard algorithms like FedAvg and FedProx. It is observed that the proposed algorithm is robust and gives more reliable estimates of the weights, in particular when the clients data are not identically distributed.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12954",
        "abstract url": "https://arxiv.org/abs/2402.12954",
        "title": "Conditional Logical Message Passing Transformer for Complex Query Answering",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type. We empirically verified that this approach can reduce computational costs without affecting performance. Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding. Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements. Experimental results show that CLMPT is a new state-of-the-art neural CQA model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "13 pages, 3 figures, and 12 tables"
    },
    {
        "paper id": "2402.12987",
        "abstract url": "https://arxiv.org/abs/2402.12987",
        "title": "Towards Robust Graph Incremental Learning on Evolving Graphs",
        "rating": -0.5,
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once. This ability to learn incrementally from a stream of tasks is crucial for many real-world applications. However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL). This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added. In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks. We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the structural shift on catastrophic forgetting of the inductive NGIL problem. We show that the structural shift can lead to a shift in the input distribution for the existing tasks, and further lead to an increased risk of catastrophic forgetting. Through comprehensive empirical studies with several benchmark datasets, we demonstrate that our proposed method, Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to improve the performance of state-of-the-art GNN incremental learning frameworks in the inductive setting.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13000",
        "abstract url": "https://arxiv.org/abs/2402.13000",
        "title": "Generating Context-Aware Contrastive Explanations in Rule-based Systems",
        "rating": -0.5,
        "keywords": [
            [
                "Workshop"
            ]
        ],
        "abstract": "Human explanations are often contrastive, meaning that they do not answer the indeterminate \"Why?\" question, but instead \"Why P, rather than Q?\". Automatically generating contrastive explanations is challenging because the contrastive event (Q) represents the expectation of a user in contrast to what happened. We present an approach that predicts a potential contrastive event in situations where a user asks for an explanation in the context of rule-based systems. Our approach analyzes a situation that needs to be explained and then selects the most likely rule a user may have expected instead of what the user has observed. This contrastive event is then used to create a contrastive explanation that is presented to the user. We have implemented the approach as a plugin for a home automation system and demonstrate its feasibility in four test scenarios.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "2024 Workshop on Explainability Engineering (ExEn '24 )"
    },
    {
        "paper id": "2402.13058",
        "abstract url": "https://arxiv.org/abs/2402.13058",
        "title": "Random Graph Set and Evidence Pattern Reasoning Model",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Evidence theory is widely used in decision-making and reasoning systems. In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model. In order to better fit the decision making goals, the Evidence Pattern Reasoning Model (EPRM) is proposed. By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks. Random Permutation Set (RPS) expands order information for evidence theory. It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships. Therefore, Random Graph Set (RGS) were proposed to model complex relationships and represent more event types. In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated. The implementation of EPRM called Conflict Resolution Decision optimized 18.17\\% of the cases compared to Mean Velocity Decision, effectively improving the aircraft velocity ranking. EPRM provides a unified solution for evidence-based decision making.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13081",
        "abstract url": "https://arxiv.org/abs/2402.13081",
        "title": "IT Intrusion Detection Using Statistical Learning and Testbed Measurements",
        "rating": -0.5,
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as online scenarios, we find that both HMM and LSTM can be effective in predicting attack start time, attack type, and attack actions. If sufficient training data is available, LSTM achieves higher prediction accuracy than HMM. HMM, on the other hand, requires less computational resources and less training data for effective prediction. Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "A shortened version of this paper will appear in the conference proceedings of NOMS 2024 (IEEE/IFIP Network Operations and Management Symposium)"
    },
    {
        "paper id": "2402.13108",
        "abstract url": "https://arxiv.org/abs/2402.13108",
        "title": "On the Stability of Gradient Descent for Large Learning Rate",
        "rating": -0.5,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset in parameter space. Additionally, we prove that if the step-size is too big, then the set of initializations from which gradient descent converges to a critical point has measure zero.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13114",
        "abstract url": "https://arxiv.org/abs/2402.13114",
        "title": "BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes",
        "rating": -0.5,
        "keywords": [
            [
                "GNNs",
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs). To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced. However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes. To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation. Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings. Code is available at https://anonymous.4open.science/r/BuffGraph-730A.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13144",
        "abstract url": "https://arxiv.org/abs/2402.13144",
        "title": "Neural Network Diffusion",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion",
                "synthesize"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "We introduce a novel approach for parameter generation, named neural network diffusion (\\textbf{p-diff}, p stands for parameter), which employs a standard latent diffusion model to synthesize a new set of parameters"
    },
    {
        "paper id": "2402.13148",
        "abstract url": "https://arxiv.org/abs/2402.13148",
        "title": "Defending Jailbreak Prompts via In-Context Adversarial Game",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13252",
        "abstract url": "https://arxiv.org/abs/2402.13252",
        "title": "Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields",
        "rating": -0.5,
        "keywords": [
            [
                "3D",
                "voxel",
                "Radiance Fields"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "AAAI 2024. Project page: https://alex04072000.github.io/Joint-TensoRF/"
    },
    {
        "paper id": "2402.13369",
        "abstract url": "https://arxiv.org/abs/2402.13369",
        "title": "The Uncanny Valley: A Comprehensive Analysis of Diffusion Models",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Through Diffusion Models (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance predominantly reside in the diffusion process dynamics and the structural design of the model's network, rather than the specifics of configuration details. Our comparative analysis reveals that Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics consistently outperform the Noise Conditioned Score Network (NCSN)-based ones, not only when evaluated in their original forms but also when continuous through Stochastic Differential Equation (SDE)-based implementations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "28 pages, 21 figures"
    },
    {
        "paper id": "2402.13421",
        "abstract url": "https://arxiv.org/abs/2402.13421",
        "title": "Context-Aware Quantitative Risk Assessment Machine Learning Model for Drivers Distraction",
        "rating": -0.5,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Risk mitigation techniques are critical to avoiding accidents associated with driving behaviour. We provide a novel Multi-Class Driver Distraction Risk Assessment (MDDRA) model that considers the vehicle, driver, and environmental data during a journey. MDDRA categorises the driver on a risk matrix as safe, careless, or dangerous. It offers flexibility in adjusting the parameters and weights to consider each event on a specific severity level. We collect real-world data using the Field Operation Test (TeleFOT), covering drivers using the same routes in the East Midlands, United Kingdom (UK). The results show that reducing road accidents caused by driver distraction is possible. We also study the correlation between distraction (driver, vehicle, and environment) and the classification severity based on a continuous distraction severity score. Furthermore, we apply machine learning techniques to classify and predict driver distraction according to severity levels to aid the transition of control from the driver to the vehicle (vehicle takeover) when a situation is deemed risky. The Ensemble Bagged Trees algorithm performed best, with an accuracy of 96.2%.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13430",
        "abstract url": "https://arxiv.org/abs/2402.13430",
        "title": "LinkSAGE: Optimizing Job Matching Using Graph Neural Networks",
        "rating": -0.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIns extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Nets (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near realtime, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13441",
        "abstract url": "https://arxiv.org/abs/2402.13441",
        "title": "PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. We evaluate our approach on LSTM, MLP-Mixer, and ResNet models, as they exhibit diverse structures and are widely used for image classification tasks in order to test their effectiveness in four widely used graph applications. Compared to the teacher models with 5.406M parameters and an F1-score of 0.4626, our student models achieve a 552$\\times$ model size compression while maintaining an F1-score of 0.4538 (with a 1.92% performance drop). Our approach yields an 8.70% higher result compared to student models trained with standard knowledge distillation and an 8.88% higher result compared to student models trained without any form of knowledge distillation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "6 pages, 2 figures, HPEC '23"
    },
    {
        "paper id": "2402.13459",
        "abstract url": "https://arxiv.org/abs/2402.13459",
        "title": "Learning to Poison Large Language Models During Instruction Tuning",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\\%. Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding LLMs against these more sophisticated attacks. The source code can be found on this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13487",
        "abstract url": "https://arxiv.org/abs/2402.13487",
        "title": "Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits",
        "rating": -0.5,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\u03b5$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB algorithms.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13496",
        "abstract url": "https://arxiv.org/abs/2402.13496",
        "title": "HetTree: Heterogeneous Tree Graph Neural Network",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs. However, existing methods ignore a tree hierarchy among metapaths, which is naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel heterogeneous tree graph neural network that models both the graph structure and heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. Existing tree encoding techniques aggregate children nodes by weighting the contribution of children nodes based on similarity to the parent node. However, we find that this tree encoding fails to capture the entire parent-children hierarchy by only considering the parent node. Hence, HetTree uses a novel subtree attention mechanism to emphasize metapaths that are more helpful in encoding parent-children relationships. Moreover, instead of separating feature learning from label learning or treating features and labels equally by projecting them to the same latent space, HetTree proposes to match them carefully based on corresponding metapaths, which provides more accurate and richer information between node features and labels. Our evaluation of HetTree on a variety of real-world datasets demonstrates that it outperforms all existing baselines on open benchmarks and efficiently scales to large real-world graphs with millions of nodes and edges.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13512",
        "abstract url": "https://arxiv.org/abs/2402.13512",
        "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers",
        "rating": -0.5,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from an initial prompt. We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by self-attention collapses into sampling a limited subset of tokens due to its non-mixing nature. This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text. In summary, the equivalence to CCMC provides a simple but powerful framework to study self-attention and its properties.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "30 pages"
    },
    {
        "paper id": "2402.14033",
        "abstract url": "https://arxiv.org/abs/2402.14033",
        "title": "VN Network: Embedding Newly Emerging Entities with Virtual Neighbors",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Embedding entities and relations into continuous vector spaces has attracted a surge of interest in recent years. Most embedding methods assume that all test entities are available during training, which makes it time-consuming to retrain embeddings for newly emerging entities. To address this issue, recent works apply the graph neural network on the existing neighbors of the unseen entities. In this paper, we propose a novel framework, namely Virtual Neighbor (VN) network, to address three key challenges. Firstly, to reduce the neighbor sparsity problem, we introduce the concept of the virtual neighbors inferred by rules. And we assign soft labels to these neighbors by solving a rule-constrained problem, rather than simply regarding them as unquestionably true. Secondly, many existing methods only use one-hop or two-hop neighbors for aggregation and ignore the distant information that may be helpful. Instead, we identify both logic and symmetric path rules to capture complex patterns. Finally, instead of one-time injection of rules, we employ an iterative learning scheme between the embedding method and virtual neighbor prediction to capture the interactions within. Experimental results on two knowledge graph completion tasks demonstrate that our VN network significantly outperforms state-of-the-art baselines. Furthermore, results on Subject/Object-R show that our proposed VN network is highly robust to the neighbor sparsity problem.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2402.12716",
        "abstract url": "https://arxiv.org/abs/2402.12716",
        "title": "Off-Path TCP Hijacking in Wi-Fi Networks: A Packet-Size Side Channel Attack",
        "rating": -1,
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "In this paper, we unveil a fundamental side channel in Wi-Fi networks, specifically the observable frame size, which can be exploited by attackers to conduct TCP hijacking attacks. Despite the various security mechanisms (e.g., WEP and WPA2/WPA3) implemented to safeguard Wi-Fi networks, our study reveals that an off path attacker can still extract sufficient information from the frame size side channel to hijack the victim's TCP connection. Our side channel attack is based on two significant findings: (i) response packets (e.g., ACK and RST) generated by TCP receivers vary in size, and (ii) the encrypted frames containing these response packets have consistent and distinguishable sizes. By observing the size of the victim's encrypted frames, the attacker can detect and hijack the victim's TCP connections. We validate the effectiveness of this side channel attack through two case studies, i.e., SSH DoS and web traffic manipulation. Precisely, our attack can terminate the victim's SSH session in 19 seconds and inject malicious data into the victim's web traffic within 28 seconds. Furthermore, we conduct extensive measurements to evaluate the impact of our attack on real-world Wi-Fi networks. We test 30 popular wireless routers from 9 well-known vendors, and none of these routers can protect victims from our attack. Besides, we implement our attack in 80 real-world Wi-Fi networks and successfully hijack the victim's TCP connections in 75 (93.75%) evaluated Wi-Fi networks. We have responsibly disclosed the vulnerability to the Wi-Fi Alliance and proposed several mitigation strategies to address this issue.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12735",
        "abstract url": "https://arxiv.org/abs/2402.12735",
        "title": "Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference",
        "rating": -1,
        "keywords": [
            [
                "medical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "In Optical Coherence Tomography (OCT), speckle noise significantly hampers image quality, affecting diagnostic accuracy. Current methods, including traditional filtering and deep learning techniques, have limitations in noise reduction and detail preservation. Addressing these challenges, this study introduces a novel denoising algorithm, Block-Matching Steered-Mixture of Experts with Multi-Model Inference and Autoencoder (BM-SMoE-AE). This method combines block-matched implementation of the SMoE algorithm with an enhanced autoencoder architecture, offering efficient speckle noise reduction while retaining critical image details. Our method stands out by providing improved edge definition and reduced processing time. Comparative analysis with existing denoising techniques demonstrates the superior performance of BM-SMoE-AE in maintaining image integrity and enhancing OCT image usability for medical diagnostics.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "This submission contains 10 pages and 4 figures. It was presented at the 2024 SPIE Photonics West, held in San Francisco. The paper details advancements in photonics applications related to healthcare and includes supplementary material with additional datasets for review"
    },
    {
        "paper id": "2402.12738",
        "abstract url": "https://arxiv.org/abs/2402.12738",
        "title": "Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues",
        "rating": -1,
        "keywords": [
            [
                "health",
                "Psychological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human counselors.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted as a conference paper at IWSDS 2024"
    },
    {
        "paper id": "2402.12743",
        "abstract url": "https://arxiv.org/abs/2402.12743",
        "title": "APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Threat actor attribution is a crucial defense strategy for combating advanced persistent threats (APTs). Cyber threat intelligence (CTI), which involves analyzing multisource heterogeneous data from APTs, plays an important role in APT actor attribution. The current attribution methods extract features from different CTI perspectives and employ machine learning models to classify CTI reports according to their threat actors. However, these methods usually extract only one kind of feature and ignore heterogeneous information, especially the attributes and relations of indicators of compromise (IOCs), which form the core of CTI. To address these problems, we propose an APT actor attribution method based on multimodal and multilevel feature fusion (APT-MMF). First, we leverage a heterogeneous attributed graph to characterize APT reports and their IOC information. Then, we extract and fuse multimodal features, including attribute type features, natural language text features and topological relationship features, to construct comprehensive node representations. Furthermore, we design multilevel heterogeneous graph attention networks to learn the deep hidden features of APT report nodes; these networks integrate IOC type-level, metapath-based neighbor node-level, and metapath semantic-level attention. Utilizing multisource threat intelligence, we construct a heterogeneous attributed graph dataset for verification purposes. The experimental results show that our method not only outperforms the existing methods but also demonstrates its good interpretability for attribution analysis tasks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12747",
        "abstract url": "https://arxiv.org/abs/2402.12747",
        "title": "Enhanced Physical Layer Security for Full-duplex Symbiotic Radio with AN Generation and Forward Noise Suppression",
        "rating": -1,
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Due to the constraints on power supply and limited encryption capability, data security based on physical layer security (PLS) techniques in backscatter communications has attracted a lot of attention. In this work, we propose to enhance PLS in a full-duplex symbiotic radio (FDSR) system with a proactive eavesdropper, which may overhear the information and interfere legitimate communications simultaneously by emitting attack signals. To deal with the eavesdroppers, we propose a security strategy based on pseudo-decoding and artificial noise (AN) injection to ensure the performance of legitimate communications through forward noise suppression. A novel AN signal generation scheme is proposed using a pseudo-decoding method, where AN signal is superimposed on data signal to safeguard the legitimate channel. The phase control in the forward noise suppression scheme and the power allocation between AN and data signals are optimized to maximize security throughput. The formulated problem can be solved via problem decomposition and alternate optimization algorithms. Simulation results demonstrate the superiority of the proposed scheme in terms of security throughput and attack mitigation performance.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12749",
        "abstract url": "https://arxiv.org/abs/2402.12749",
        "title": "Me LLaMA: Foundation Large Language Models for Medical Applications",
        "rating": -1,
        "keywords": [
            [
                "biomedical",
                "Medical",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in large language models (LLMs) such as ChatGPT and LLaMA have hinted at their potential to revolutionize medical applications, yet their application in clinical settings often reveals limitations due to a lack of specialized training on medical-specific data. In response to this challenge, this study introduces Me-LLaMA, a novel medical LLM family that includes foundation models - Me-LLaMA 13/70B, along with their chat-enhanced versions - Me-LLaMA 13/70B-chat, developed through continual pre-training and instruction tuning of LLaMA2 using large medical datasets. Our methodology leverages a comprehensive domain-specific data suite, including a large-scale, continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a new medical evaluation benchmark (MIBE) across six critical medical tasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me-LLaMA models achieve overall better performance than existing open-source medical LLMs in zero-shot, few-shot and supervised learning abilities. With task-specific instruction tuning, Me-LLaMA models outperform ChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets. In addition, we investigated the catastrophic forgetting problem, and our results show that Me-LLaMA models outperform other open-source medical LLMs in mitigating this issue. Me-LLaMA is one of the largest open-source medical foundation LLMs that use both biomedical and clinical data. It exhibits superior performance across both general and medical tasks compared to other open-source medical LLMs, rendering it an attractive choice for medical AI applications. We release our models, datasets, and evaluation scripts at: https://github.com/BIDS-Xu-Lab/Me-LLaMA.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "21 pages, 3 figures, 8 tables"
    },
    {
        "paper id": "2402.12784",
        "abstract url": "https://arxiv.org/abs/2402.12784",
        "title": "Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems",
        "rating": -1,
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "The introduction of Vec2Text, a technique for inverting text embeddings, has raised serious privacy concerns within dense retrieval systems utilizing text embeddings, including those provided by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to text embeddings to reconstruct the original text. In this paper, we investigate various aspects of embedding models that could influence the recoverability of text using Vec2Text. Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions -- aspects not previously addressed in the original Vec2Text paper. Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between text recoverability and retrieval effectiveness in dense retrieval systems. This analysis provides valuable insights for practitioners involved in designing privacy-aware dense retrieval systems. Additionally, we propose a straightforward fix for embedding transformation that ensures equal ranking effectiveness while mitigating the risk of text recoverability. Furthermore, we extend the application of Vec2Text to the separate task of corpus poisoning, where, theoretically, Vec2Text presents a more potent threat compared to previous attack methods. Notably, Vec2Text does not require access to the dense retriever's model parameters and can efficiently generate numerous adversarial passages. In summary, this study highlights the potential threat posed by Vec2Text to existing dense retrieval systems, while also presenting effective methods to patch and strengthen such systems against such risks.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12788",
        "abstract url": "https://arxiv.org/abs/2402.12788",
        "title": "RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer",
        "rating": -1,
        "keywords": [
            [
                "healthcare",
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications such as healthcare, affective computing, anti-spoofing, etc. Due to the periodicity nature of rPPG, the long-range dependency capturing capacity of the Transformer was assumed to be advantageous for such signals. However, existing approaches have not conclusively demonstrated the superior performance of Transformer over traditional convolutional neural network methods, this gap may stem from a lack of thorough exploration of rPPG periodicity. In this paper, we propose RhythmFormer, a fully end-to-end transformer-based method for extracting rPPG signals by explicitly leveraging the quasi-periodic nature of rPPG. The core module, Hierarchical Temporal Periodic Transformer, hierarchically extracts periodic features from multiple temporal scales. It utilizes dynamic sparse attention based on periodicity in the temporal domain, allowing for fine-grained modeling of rPPG features. Furthermore, a fusion stem is proposed to guide self-attention to rPPG features effectively, and it can be easily transferred to existing methods to enhance their performance significantly. RhythmFormer achieves state-of-the-art performance with fewer parameters and reduced computational complexity in comprehensive experiments compared to previous approaches. The codes are available at https://github.com/zizheng-guo/RhythmFormer.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12801",
        "abstract url": "https://arxiv.org/abs/2402.12801",
        "title": "Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting",
        "rating": -1,
        "keywords": [
            [
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models are becoming the go-to solution for many natural language processing tasks, including in specialized domains where their few-shot capacities are expected to yield high performance in low-resource settings. Herein, we aim to assess the performance of Large Language Models for few shot clinical entity recognition in multiple languages. We evaluate named entity recognition in English, French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard corpora. We assess the performance of 10 auto-regressive language models using prompting and 16 masked language models used for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot set-up by limiting the amount of annotated data available to 100 sentences. Our experiments show that although larger prompt-based models tend to achieve competitive F-measure for named entity recognition outside the clinical domain, this level of performance does not carry over to the clinical domain where lighter supervised taggers relying on masked language models perform better, even with the performance drop incurred from the few-shot set-up. In all experiments, the CO2 impact of masked language models is inferior to that of auto-regressive models. Results are consistent over the three languages and suggest that few-shot learning using Large language models is not production ready for named entity recognition in the clinical domain. Instead, models could be used for speeding-up the production of gold standard annotated data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Submitted to Journal of Artificial Intelligence in Medicine"
    },
    {
        "paper id": "2402.12810",
        "abstract url": "https://arxiv.org/abs/2402.12810",
        "title": "PIP-Net: Pedestrian Intention Prediction in the Wild",
        "rating": -1,
        "keywords": [
            [
                "depth"
            ],
            [
                "automated driving",
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the model's contextual perception. Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance which is a breakthrough in current research studies in pedestrian intention prediction. Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12832",
        "abstract url": "https://arxiv.org/abs/2402.12832",
        "title": "Nearly Optimal Fault Tolerant Distance Oracle",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We present an $f$-fault tolerant distance oracle for an undirected weighted graph where each edge has an integral weight from $[1 \\dots W]$. Given a set $F$ of $f$ edges, as well as a source node $s$ and a destination node $t$, our oracle returns the \\emph{shortest path} from $s$ to $t$ avoiding $F$ in $O((cf \\log (nW))^{O(f^2)})$ time, where $c > 1$ is a constant. The space complexity of our oracle is $O(f^4n^2\\log^2 (nW))$. For a constant $f$, our oracle is nearly optimal both in terms of space and time (barring some logarithmic factor).",
        "subjects": [
            "cs.DS"
        ],
        "comment": "accepted in STOC, 2024"
    },
    {
        "paper id": "2402.12834",
        "abstract url": "https://arxiv.org/abs/2402.12834",
        "title": "SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Coarse-Grain Reconfigurable Arrays (CGRAs) represent emerging low-power architectures designed to accelerate Compute-Intensive Loops (CILs). The effectiveness of CGRAs in providing acceleration relies on the quality of mapping: how efficiently the CIL is compiled onto the platform. State of the Art (SoA) compilation techniques utilize modulo scheduling to minimize the Iteration Interval (II) and use graph algorithms like Max-Clique Enumeration to address mapping challenges. Our work approaches the mapping problem through a satisfiability (SAT) formulation. We introduce the Kernel Mobility Schedule (KMS), an ad-hoc schedule used with the Data Flow Graph and CGRA architectural information to generate Boolean statements that, when satisfied, yield a valid mapping. Experimental results demonstrate SAT-MapIt outperforming SoA alternatives in almost 50\\% of explored benchmarks. Additionally, we evaluated the mapping results in a synthesizable CGRA design and emphasized the run-time metrics trends, i.e. energy efficiency and latency, across different CILs and CGRA sizes. We show that a hardware-agnostic analysis performed on compiler-level metrics can optimally prune the architectural design space, while still retaining Pareto-optimal configurations. Moreover, by exploring how implementation details impact cost and performance on real hardware, we highlight the importance of holistic software-to-hardware mapping flows, as the one presented herein.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12844",
        "abstract url": "https://arxiv.org/abs/2402.12844",
        "title": "ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation",
        "rating": -1,
        "keywords": [
            [
                "X-ray",
                "clinical",
                "Radiology"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesions, our approach involves first extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mix-up augmentation technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, by linearly interpolating them during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12856",
        "abstract url": "https://arxiv.org/abs/2402.12856",
        "title": "A new simplified MOPSO based on Swarm Elitism and Swarm Memory: MO-ETPSO",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "This paper presents an algorithm based on Particle Swarm Optimization (PSO), adapted for multi-objective optimization problems: the Elitist PSO (MO-ETPSO). The proposed algorithm integrates core strategies from the well-established NSGA-II approach, such as the Crowding Distance Algorithm, while leveraging the advantages of Swarm Intelligence in terms of individual and social cognition. A novel aspect of the algorithm is the introduction of a swarm memory and swarm elitism, which may turn the adoption of NSGA-II strategies in PSO. These features enhance the algorithm's ability to retain and utilize high-quality solutions throughout optimization. Furthermore, all operators within the algorithm are intentionally designed for simplicity, ensuring ease of replication and implementation in various settings. Preliminary comparisons with the NSGA-II algorithm for the Green Vehicle Routing Problem, both in terms of solutions found and convergence, have yielded promising results in favor of MO-ETPSO.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12869",
        "abstract url": "https://arxiv.org/abs/2402.12869",
        "title": "Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data",
        "rating": -1,
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems. In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (DSFT and RAG frameworks) with four representative methods: Markdown format, Template serialization, TPLM-based method, and LLM-based method. Based on the experimental results, we draw some empirical findings and explore the underlying reasons behind the success of some methods. We hope the findings of this work will provide a valuable reference for the academic and industrial communities in developing robust QA systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024 Industry Track Paper"
    },
    {
        "paper id": "2402.12880",
        "abstract url": "https://arxiv.org/abs/2402.12880",
        "title": "Autism Detection in Speech -- A Survey",
        "rating": -1,
        "keywords": [
            [
                "biomedical",
                "psychological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers which could be beneficial in this context. Additionally, we were unable to find research combining both features from audio and transcripts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to EACL 2024 Findings"
    },
    {
        "paper id": "2402.12936",
        "abstract url": "https://arxiv.org/abs/2402.12936",
        "title": "Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Large language models (LLMs) have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. Backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. In this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. Specifically, we examine attention weights and biases, activation values, and context embeddings of the clean and poisoned CodeBERT models. Our results suggest noticeable patterns in activation values and context embeddings of poisoned samples for the poisoned CodeBERT model; however, attention weights and biases do not show any significant differences. This work contributes to ongoing efforts in white-box detection of backdoor signals in LLMs of code through the analysis of parameters and activations.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12948",
        "abstract url": "https://arxiv.org/abs/2402.12948",
        "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick",
        "rating": -1,
        "keywords": [
            [
                "Watermarking"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing other decoding-based watermarking methods by a minimum of 0.1.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12957",
        "abstract url": "https://arxiv.org/abs/2402.12957",
        "title": "Energy-Efficient Wireless Federated Learning via Doubly Adaptive Quantization",
        "rating": -1,
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "Federated learning (FL) has been recognized as a viable distributed learning paradigm for training a machine learning model across distributed clients without uploading raw data. However, FL in wireless networks still faces two major challenges, i.e., large communication overhead and high energy consumption, which are exacerbated by client heterogeneity in dataset sizes and wireless channels. While model quantization is effective for energy reduction, existing works ignore adapting quantization to heterogeneous clients and FL convergence. To address these challenges, this paper develops an energy optimization problem of jointly designing quantization levels, scheduling clients, allocating channels, and controlling computation frequencies (QCCF) in wireless FL. Specifically, we derive an upper bound identifying the influence of client scheduling and quantization errors on FL convergence. Under the longterm convergence constraints and wireless constraints, the problem is established and transformed into an instantaneous problem with Lyapunov optimization. Solving Karush-Kuhn-Tucker conditions, our closed-form solution indicates that the doubly adaptive quantization level rises with the training process and correlates negatively with dataset sizes. Experiment results validate our theoretical results, showing that QCCF consumes less energy with faster convergence compared with state-of-the-art baselines.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12959",
        "abstract url": "https://arxiv.org/abs/2402.12959",
        "title": "Prompt Stealing Attacks Against Large Language Models",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12970",
        "abstract url": "https://arxiv.org/abs/2402.12970",
        "title": "See Further Than CFAR: a Data-Driven Radar Detector Trained by Lidar",
        "rating": -1,
        "keywords": [
            [
                "Lidar",
                "Radar"
            ]
        ],
        "abstract": "In this paper, we address the limitations of traditional constant false alarm rate (CFAR) target detectors in automotive radars, particularly in complex urban environments with multiple objects that appear as extended targets. We propose a data-driven radar target detector exploiting a highly efficient 2D CNN backbone inspired by the computer vision domain. Our approach is distinguished by a unique cross sensor supervision pipeline, enabling it to learn exclusively from unlabeled synchronized radar and lidar data, thus eliminating the need for costly manual object annotations. Using a novel large-scale, real-life multi-sensor dataset recorded in various driving scenarios, we demonstrate that the proposed detector generates dense, lidar-like point clouds, achieving a lower Chamfer distance to the reference lidar point clouds than CFAR detectors. Overall, it significantly outperforms CFAR baselines detection accuracy.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted for lecture presentation at IEEE RadarConf'24, Denver, USA"
    },
    {
        "paper id": "2402.13061",
        "abstract url": "https://arxiv.org/abs/2402.13061",
        "title": "Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space",
        "rating": -1,
        "keywords": [
            [
                "healthcare",
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Fairness has become increasingly pivotal in machine learning for high-risk applications such as machine learning in healthcare and facial recognition. However, we see the deficiency in the previous logits space constraint methods. Therefore, we propose a novel framework, Logits-MMD, that achieves the fairness condition by imposing constraints on output logits with Maximum Mean Discrepancy. Moreover, quantitative analysis and experimental results show that our framework has a better property that outperforms previous methods and achieves state-of-the-art on two facial recognition datasets and one animal dataset. Finally, we show experimental results and demonstrate that our debias approach achieves the fairness condition effectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13075",
        "abstract url": "https://arxiv.org/abs/2402.13075",
        "title": "Formal Synthesis of Controllers for Safety-Critical Autonomous Systems: Developments and Challenges",
        "rating": -1,
        "keywords": [
            [
                "Synthesis"
            ]
        ],
        "abstract": "In recent years, formal methods have been extensively used in the design of autonomous systems. By employing mathematically rigorous techniques, formal methods can provide fully automated reasoning processes with provable safety guarantees for complex dynamic systems with intricate interactions between continuous dynamics and discrete logics. This paper provides a comprehensive review of formal controller synthesis techniques for safety-critical autonomous systems. Specifically, we categorize the formal control synthesis problem based on diverse system models, encompassing deterministic, non-deterministic, and stochastic, and various formal safety-critical specifications involving logic, real-time, and real-valued domains. The review covers fundamental formal control synthesis techniques, including abstraction-based approaches and abstraction-free methods. We explore the integration of data-driven synthesis approaches in formal control synthesis. Furthermore, we review formal techniques tailored for multi-agent systems (MAS), with a specific focus on various approaches to address the scalability challenges in large-scale systems. Finally, we discuss some recent trends and highlight research challenges in this area.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13104",
        "abstract url": "https://arxiv.org/abs/2402.13104",
        "title": "Self-Perception Versus Objective Driving Behavior: Subject Study of Lateral Vehicle Guidance",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "Advancements in technology are steering attention toward creating comfortable and acceptable driving characteristics in autonomous vehicles. Ensuring a safe and comfortable ride experience is vital for the widespread adoption of autonomous vehicles, as mismatches in driving styles between humans and autonomous systems can impact passenger confidence. Current driving functions have fixed parameters, and there is no universally agreed-upon driving style for autonomous vehicles. Integrating driving style preferences into automated vehicles may enhance acceptance and reduce uncertainty, expediting their adoption. A controlled vehicle study (N = 62) was conducted with a variety of German participants to identify the individual lateral driving behavior of human drivers, specifically emphasizing rural roads. We introduce novel indicators for assessing stationary and transient curve negotiation, directly applicable in developing personalized lateral driving functions. To assess the predictability of these indicators using self-reports, we introduce the MDSI-DE, the German version of the Multidimensional Driving Style Inventory. The correlation analysis between MDSI factor scores and proposed indicators showed modest but significant associations, primarily with acceleration and jerk statistics while the in-depth lateral driving behavior turned out to be highly driver-heterogeneous. The dataset including the anonymized socio-demographics and questionnaire responses, the raw vehicle measurements including labels, and the derived driving behavior indicators are publicly available at https://www.kaggle.com/datasets/jhaselberger/spodb-subject-study-of-lateral-vehicle-guidance.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "33 pages, 6 figures, under review"
    },
    {
        "paper id": "2402.13110",
        "abstract url": "https://arxiv.org/abs/2402.13110",
        "title": "HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for In-Air Acoustic Imaging",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "Airborne 3D imaging using ultrasound is a promising sensing modality for robotic applications in harsh environments. Over the last decade, several high-performance systems have been proposed in the literature. Most of these sensors use a reduced aperture microphone array, leading to artifacts in the resulting acoustic images. This paper presents a novel in-air ultrasound sensor that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array, in combination with a distributed embedded hardware design to perform the data acquisition. Using a broadband Minimum Variance Distortionless Response (MVDR) beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able to create both 2D and 3D ultrasound images of the full-frontal hemisphere with high angular accuracy with up to 70dB main lobe to side lobe ratio. This paper describes both the hardware infrastructure needed to obtain such highly detailed acoustical images, as well as the signal processing chain needed to convert the raw acoustic data into said images. Utilizing this novel high-resolution ultrasound imaging sensor, we wish to investigate the limits of both passive and active airborne ultrasound sensing by utilizing this virtually artifact-free imaging modality.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13117",
        "abstract url": "https://arxiv.org/abs/2402.13117",
        "title": "Faster and Deterministic Subtrajectory Clustering",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Given a trajectory $T$ and a distance $\u0394$, we wish to find a set $C$ of curves of complexity at most $\\ell$, such that we can cover $T$ with subcurves that each are within Fr\u00e9chet distance $\u0394$ to at least one curve in $C$. We call $C$ an $(\\ell,\u0394)$-clustering and aim to find an $(\\ell,\u0394)$-clustering of minimum cardinality. This problem was introduced by Akitaya $et$ $al.$ (2021) and shown to be NP-complete. The main focus has therefore been on bicriterial approximation algorithms, allowing for the clustering to be an $(\\ell, \u0398(\u0394))$-clustering of roughly optimal size. We present algorithms that construct $(\\ell,4\u0394)$-clusterings of $\\mathcal{O}(k \\log n)$ size, where $k$ is the size of the optimal $(\\ell, \u0394)$-clustering. For the discrete Fr\u00e9chet distance, we use $\\mathcal{O}(n \\ell \\log n)$ space and $\\mathcal{O}(k n^2 \\log^3 n)$ deterministic worst case time. For the continuous Fr\u00e9chet distance, we use $\\mathcal{O}(n^2 \\log n)$ space and $\\mathcal{O}(k n^3 \\log^3 n)$ time. Our algorithms significantly improve upon the clustering quality (improving the approximation factor in $\u0394$) and size (whenever $\\ell \\in \u03a9(\\log n)$). We offer deterministic running times comparable to known expected bounds. Additionally, in the continuous setting, we give a near-linear improvement upon the space usage. When compared only to deterministic results, we offer a near-linear speedup and a near-quadratic improvement in the space usage. When we may restrict ourselves to only considering clusters where all subtrajectories are vertex-to-vertex subcurves, we obtain even better results under the continuous Fr\u00e9chet distance. Our algorithm becomes near quadratic and uses space that is near linear in $n \\ell$.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "27 pages, 8 figures"
    },
    {
        "paper id": "2402.13118",
        "abstract url": "https://arxiv.org/abs/2402.13118",
        "title": "Multistatic OFDM Radar Fusion of MUSIC-based Angle Estimation",
        "rating": -1,
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "This study investigates the problem of angle-based localization of multiple targets using a multistatic OFDM radar. Although the maximum likelihood (ML) approach can be employed to merge data from different radar pairs, this method requires a high complexity multi-dimensional search process. The multiple signal classification (MUSIC) algorithm simplifies the complexity to a two-dimensional search, but no framework is derived for combining MUSIC pseudo-spectrums in a multistatic configuration. This paper exploits the relationship between MUSIC and ML estimators to approximate the multidimensional ML parameter estimation with a weighted combination of MUSIC pseudo-spectrum. This enables the computation of a likelihood map on which a peak selection is applied for target detection. In addition to reducing the computational complexity, the proposed method relies only on transmitting the estimated channel covariance matrices of each radar pair to the central processor. A numerical analysis is conducted to assess the benefits of the proposed fusion.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "5 pages, 3 figures, 1 table; accepted to EuCAP2024 conference"
    },
    {
        "paper id": "2402.13131",
        "abstract url": "https://arxiv.org/abs/2402.13131",
        "title": "exploreCOSMOS: Interactive Exploration of Conditional Statistical Shape Models in the Web-Browser",
        "rating": -1,
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Statistical Shape Models of faces and various body parts are heavily used in medical image analysis, computer vision and visualization. Whilst the field is well explored with many existing tools, all of them aim at experts, which limits their applicability. We demonstrate the first tool that enables the convenient exploration of statistical shape models in the browser, with the capability to manipulate the faces in a targeted manner. This manipulation is performed via a posterior model given partial observations. We release our code and application on GitHub https://github.com/maximilian-hahn/exploreCOSMOS",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Dies ist ein Vorabdruck des folgenden Beitrages, ver\u00f6ffentlicht in BVM 2024, herausgegeben von Maier, A. et al, 2024, Springer Nature, vervielf\u00e4ltigt mit Genehmigung von Springer Nature. Die finale authentifizierte Version ist online verf\u00fcgbar unter: https://doi.org/10.1007/978-3-658-44037-4_32"
    },
    {
        "paper id": "2402.13139",
        "abstract url": "https://arxiv.org/abs/2402.13139",
        "title": "Deterministic Dynamic Edge-Colouring",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Given a dynamic graph $G$ with $n$ vertices and $m$ edges subject to insertion an deletions of edges, we show how to maintain a $(1+\\varepsilon)\u0394$-edge-colouring of $G$ without the use of randomisation. More specifically, we show a deterministic dynamic algorithm with an amortised update time of $2^{\\tilde{O}_{\\log \\varepsilon^{-1}}(\\sqrt{\\log n})}$ using $(1+\\varepsilon)\u0394$ colours. If $\\varepsilon^{-1} \\in 2^{O(\\log^{0.49} n)}$, then our update time is sub-polynomial in $n$. While there exists randomised algorithms maintaining colourings with the same number of colours [Christiansen STOC'23, Duan, He, Zhang SODA'19, Bhattacarya, Costa, Panski, Solomon SODA'24] in polylogarithmic and even constant update time, this is the first deterministic algorithm to go below the greedy threshold of $2\u0394-1$ colours for all input graphs. On the way to our main result, we show how to dynamically maintain a shallow hierarchy of degree-splitters with both recourse and update time in $n^{o(1)}$. We believe that this algorithm might be of independent interest.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "53 pages"
    },
    {
        "paper id": "2402.13153",
        "abstract url": "https://arxiv.org/abs/2402.13153",
        "title": "Clustered Planarity Variants for Level Graphs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We consider variants of the clustered planarity problem for level-planar drawings. So far, only convex clusters have been studied in this setting. We introduce two new variants that both insist on a level-planar drawing of the input graph but relax the requirements on the shape of the clusters. In unrestricted Clustered Level Planarity (uCLP) we only require that they are bounded by simple closed curves that enclose exactly the vertices of the cluster and cross each edge of the graph at most once. The problem y-monotone Clustered Level Planarity (y-CLP) requires that additionally it must be possible to augment each cluster with edges that do not cross the cluster boundaries so that it becomes connected while the graph remains level-planar, thereby mimicking a classic characterization of clustered planarity in the level-planar setting. We give a polynomial-time algorithm for uCLP if the input graph is biconnected and has a single source. By contrast, we show that y-CLP is hard under the same restrictions and it remains NP-hard even if the number of levels is bounded by a constant and there is only a single non-trivial cluster.",
        "subjects": [
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13178",
        "abstract url": "https://arxiv.org/abs/2402.13178",
        "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
        "rating": -1,
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Homepage: https://teddy-xionggz.github.io/benchmark-medical-rag/"
    },
    {
        "paper id": "2402.13181",
        "abstract url": "https://arxiv.org/abs/2402.13181",
        "title": "DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "To appear at 2024 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
        "paper id": "2402.13224",
        "abstract url": "https://arxiv.org/abs/2402.13224",
        "title": "Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach demonstrates robustness against early disconnections by considering a wider range of uncertainty scenarios for optimization. The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline. Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13225",
        "abstract url": "https://arxiv.org/abs/2402.13225",
        "title": "AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning",
        "rating": -1,
        "keywords": [
            [
                "healthcare",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description. On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics. In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2402.13227",
        "abstract url": "https://arxiv.org/abs/2402.13227",
        "title": "Online Matching on $3$-Uniform Hypergraphs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The online matching problem was introduced by Karp, Vazirani and Vazirani (STOC 1990) on bipartite graphs with vertex arrivals. It is well-known that the optimal competitive ratio is $1-1/e$ for both integral and fractional versions of the problem. Since then, there has been considerable effort to find optimal competitive ratios for other related settings. In this work, we go beyond the graph case and study the online matching problem on $k$-uniform hypergraphs. For $k=3$, we provide an optimal primal-dual fractional algorithm, which achieves a competitive ratio of $(e-1)/(e+1)\\approx 0.4621$. As our main technical contribution, we present a carefully constructed adversarial instance, which shows that this ratio is in fact optimal. It combines ideas from known hard instances for bipartite graphs under the edge-arrival and vertex-arrival models. For $k\\geq 3$, we give a simple integral algorithm which performs better than greedy when the online nodes have bounded degree. As a corollary, it achieves the optimal competitive ratio of 1/2 on 3-uniform hypergraphs when every online node has degree at most 2. This is because the special case where every online node has degree 1 is equivalent to the edge-arrival model on graphs, for which an upper bound of 1/2 is known.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13246",
        "abstract url": "https://arxiv.org/abs/2402.13246",
        "title": "Game theory of undirected graphical models",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We model an $n$-player game $X$ in normal form via undirected discrete graphical models where the discrete random variables represent the players and their state spaces are the set of pure strategies. There exists an edge between the vertices of the graphical model $G$ whenever there is a dependency between the associated players. We study the Spohn conditional independence (CI) variety $\\mathcal{V}_{X,\\mathcal{C}}$, which is the intersection of the independence model $\\mathcal{M}_{\\text{global}(G)}$ with the Spohn variety of the game $X$. We prove a conjecture by the first author and Sturmfels that $\\mathcal{V}_{X,\\mathcal{C}}$ is of codimension $n$ in $\\mathcal{M}_{\\mathcal{C}}$ for a generic game $X$ with binary choices. In the case where the undirected graph is a disjoint union of cliques, we analyze certain algebro-geometric features of Spohn CI varieties and prove affine universality theorems.",
        "subjects": [
            "math.AG"
        ],
        "comment": "26 pages, 4 figures"
    },
    {
        "paper id": "2402.13253",
        "abstract url": "https://arxiv.org/abs/2402.13253",
        "title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
        "rating": -1,
        "keywords": [
            [
                "Medical",
                "healthcare"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic medical benchmark and 15% on bilingual evaluations across multiple datasets. Our project page with source code and trained model is available at https://github.com/mbzuai-oryx/BiMediX .",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13255",
        "abstract url": "https://arxiv.org/abs/2402.13255",
        "title": "How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "Radiance Fields"
            ],
            [
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13329",
        "abstract url": "https://arxiv.org/abs/2402.13329",
        "title": "A Disruptive Research Playbook for Studying Disruptive Innovations",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "As researchers, we are now witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative AI, Augmented Reality (AR) and Virtual Reality (VR). In particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its the socio-technical nature. In this paper, we reflect on the importance of formulating and addressing research in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. We propose a research playbook with the goal of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. We showcase how to apply the research playbook. Firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, Stack Overflow, and its impact on software development. Secondly, we show it can be used to question the impact of two current disruptive technologies: AI and AR/VR. Finally, we introduce a specialized GPT model to support the researcher in framing future investigations. We conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13350",
        "abstract url": "https://arxiv.org/abs/2402.13350",
        "title": "PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods",
        "rating": -1,
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present Polish Information Retrieval Benchmark (PIRB), a comprehensive evaluation framework encompassing 41 text information retrieval tasks for Polish. The benchmark incorporates existing datasets as well as 10 new, previously unpublished datasets covering diverse topics such as medicine, law, business, physics, and linguistics. We conduct an extensive evaluation of over 20 dense and sparse retrieval models, including the baseline models trained by us as well as other available Polish and multilingual methods. Finally, we introduce a three-step process for training highly effective language-specific retrievers, consisting of knowledge distillation, supervised fine-tuning, and building sparse-dense hybrid retrievers using a lightweight rescoring model. In order to validate our approach, we train new text encoders for Polish and compare their results with previously evaluated methods. Our dense models outperform the best solutions available to date, and the use of hybrid methods further improves their performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13351",
        "abstract url": "https://arxiv.org/abs/2402.13351",
        "title": "Malicious Reconfigurable Intelligent Surfaces: How Impactful can Destructive Beamforming be?",
        "rating": -1,
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Reconfigurable intelligent surfaces (RISs) have demonstrated significant potential for enhancing communication system performance if properly configured. However, a RIS might also pose a risk to the network security. In this letter, we explore the impact of a malicious RIS on a multi-user multiple-input single-output (MISO) system when the system is unaware of the RIS's malicious intentions. The objective of the malicious RIS is to degrade the \\ac{SNR} of a specific \\ac{UE}, with the option of preserving the SNR of the other UEs, making the attack harder to detect. To achieve this goal, we derive the optimal RIS phase-shift pattern, assuming perfect channel state information (CSI) at the hacker. We then relax this assumption by introducing CSI uncertainties and subsequently determine the RIS's phase-shift pattern using a robust optimization approach. Our simulations reveal a direct proportionality between the performance degradation caused by the malicious RIS and the number of reflective elements, along with resilience toward CSI uncertainties.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IEEE wireless communication letters"
    },
    {
        "paper id": "2402.13408",
        "abstract url": "https://arxiv.org/abs/2402.13408",
        "title": "Healthcare Copilot: Eliciting the Power of General LLMs for Medical Consultation",
        "rating": -1,
        "keywords": [
            [
                "Medical",
                "Healthcare"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The copilot framework, which aims to enhance and tailor large language models (LLMs) for specific complex tasks without requiring fine-tuning, is gaining increasing attention from the community. In this paper, we introduce the construction of a Healthcare Copilot designed for medical consultation. The proposed Healthcare Copilot comprises three main components: 1) the Dialogue component, responsible for effective and safe patient interactions; 2) the Memory component, storing both current conversation data and historical patient information; and 3) the Processing component, summarizing the entire dialogue and generating reports. To evaluate the proposed Healthcare Copilot, we implement an auto-evaluation scheme using ChatGPT for two roles: as a virtual patient engaging in dialogue with the copilot, and as an evaluator to assess the quality of the dialogue. Extensive results demonstrate that the proposed Healthcare Copilot significantly enhances the capabilities of general LLMs for medical consultations in terms of inquiry capability, conversational fluency, response accuracy, and safety. Furthermore, we conduct ablation studies to highlight the contribution of each individual module in the Healthcare Copilot. Code will be made publicly available on GitHub.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13412",
        "abstract url": "https://arxiv.org/abs/2402.13412",
        "title": "Scaling physics-informed hard constraints with mixture-of-experts",
        "rating": -1.0,
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an \"expert\" through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to the International Conference on Learning Representations (ICLR) 2024"
    },
    {
        "paper id": "2402.13415",
        "abstract url": "https://arxiv.org/abs/2402.13415",
        "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
        "rating": -1,
        "keywords": [
            [
                "navigation"
            ],
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13432",
        "abstract url": "https://arxiv.org/abs/2402.13432",
        "title": "DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain",
        "rating": -1,
        "keywords": [
            [
                "Biomedical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity recognition, part-of-speech tagging, question-answering, semantic textual similarity, and classification. We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-Coling 2024"
    },
    {
        "paper id": "2402.13443",
        "abstract url": "https://arxiv.org/abs/2402.13443",
        "title": "Autonomous Mapless Navigation on Uneven Terrains",
        "rating": -1,
        "keywords": [
            [
                "robot",
                "Navigation"
            ]
        ],
        "abstract": "We propose a new method for autonomous navigation in uneven terrains by utilizing a sparse Gaussian Process (SGP) based local perception model. The SGP local perception model is trained on local ranging observation (pointcloud) to learn the terrain elevation profile and extract the feasible navigation subgoals around the robot. Subsequently, a cost function, which prioritizes the safety of the robot in terms of keeping the robot's roll and pitch angles bounded within a specified range, is used to select a safety-aware subgoal that leads the robot to its final destination. The algorithm is designed to run in real-time and is intensively evaluated in simulation and real world experiments. The results compellingly demonstrate that our proposed algorithm consistently navigates uneven terrains with high efficiency and surpasses the performance of other planners. The code and video can be found here: https://rb.gy/3ov2r8",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This paper has 7 pages, 7 figures, 2 tables. It has been accepted for publication at the 2024 IEEE International Conference on Robotics and Automation (ICRA2024)"
    },
    {
        "paper id": "2402.13444",
        "abstract url": "https://arxiv.org/abs/2402.13444",
        "title": "The Effectiveness of Graph Contrastive Learning on Mathematical Information Retrieval",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "This paper details an empirical investigation into using Graph Contrastive Learning (GCL) to generate mathematical equation representations, a critical aspect of Mathematical Information Retrieval (MIR). Our findings reveal that this simple approach consistently exceeds the performance of the current leading formula retrieval model, TangentCFT. To support ongoing research and development in this field, we have made our source code accessible to the public at https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13457",
        "abstract url": "https://arxiv.org/abs/2402.13457",
        "title": "LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study",
        "rating": -1,
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of \"jailbreaking\", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "16 pages, 6 figures"
    },
    {
        "paper id": "2402.13470",
        "abstract url": "https://arxiv.org/abs/2402.13470",
        "title": "How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?",
        "rating": -1,
        "keywords": [
            [
                "Biomedical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13473",
        "abstract url": "https://arxiv.org/abs/2402.13473",
        "title": "Learning Highly Dynamic Behaviors for Quadrupedal Robots",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Learning highly dynamic behaviors for robots has been a longstanding challenge. Traditional approaches have demonstrated robust locomotion, but the exhibited behaviors lack diversity and agility. They employ approximate models, which lead to compromises in performance. Data-driven approaches have been shown to reproduce agile behaviors of animals, but typically have not been able to learn highly dynamic behaviors. In this paper, we propose a learning-based approach to enable robots to learn highly dynamic behaviors from animal motion data. The learned controller is deployed on a quadrupedal robot and the results show that the controller is able to reproduce highly dynamic behaviors including sprinting, jumping and sharp turning. Various behaviors can be activated through human interaction using a stick with markers attached to it. Based on the motion pattern of the stick, the robot exhibits walking, running, sitting and jumping, much like the way humans interact with a pet.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13481",
        "abstract url": "https://arxiv.org/abs/2402.13481",
        "title": "Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning",
        "rating": -1,
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ]
        ],
        "abstract": "Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL) have shown impressive results in many driving scenarios. However, the performance of these trained policies can be impacted when faced with diverse driving styles and personalities, particularly in highly interactive situations. This is because conventional MARL algorithms usually operate under the assumption of fully cooperative behavior among all agents and focus on maximizing team rewards during training. To address this issue, we introduce the Personality Modeling Network (PeMN), which includes a cooperation value function and personality parameters to model the varied interactions in high-interactive scenarios. The PeMN also enables the training of a background traffic flow with diverse behaviors, thereby improving the performance and generalization of the ego vehicle. Our extensive experimental studies, which incorporate different personality parameters in high-interactive driving scenarios, demonstrate that the personality parameters effectively model diverse driving styles and that policies trained with PeMN demonstrate better generalization compared to traditional MARL methods.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13498",
        "abstract url": "https://arxiv.org/abs/2402.13498",
        "title": "The Lay Person's Guide to Biomedicine: Orchestrating Large Language Models",
        "rating": -1,
        "keywords": [
            [
                "Biomedicine"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Automated lay summarisation (LS) aims to simplify complex technical documents into a more accessible format to non-experts. Existing approaches using pre-trained language models, possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation. Moreover, automated methods that can effectively assess the `layness' of generated summaries are lacking. Recently, large language models (LLMs) have demonstrated a remarkable capacity for text simplification, background information generation, and text evaluation. This has motivated our systematic exploration into using LLMs to generate and evaluate lay summaries of biomedical articles. We propose a novel \\textit{Explain-then-Summarise} LS framework, which leverages LLMs to generate high-quality background knowledge to improve supervised LS. We also evaluate the performance of LLMs for zero-shot LS and propose two novel LLM-based LS evaluation metrics, which assess layness from multiple perspectives. Finally, we conduct a human assessment of generated lay summaries. Our experiments reveal that LLM-generated background information can support improved supervised LS. Furthermore, our novel zero-shot LS evaluation metric demonstrates a high degree of alignment with human preferences. We conclude that LLMs have an important part to play in improving both the performance and evaluation of LS methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "18 pages, 4 figures"
    },
    {
        "paper id": "2402.13513",
        "abstract url": "https://arxiv.org/abs/2402.13513",
        "title": "Guac: Energy-Aware and SSA-Based Generation of Coarse-Grained Merged Accelerators from LLVM-IR",
        "rating": -1,
        "keywords": [
            [
                "Synthesis"
            ]
        ],
        "abstract": "Designing accelerators for resource- and power-constrained applications is a daunting task. High-level Synthesis (HLS) addresses these constraints through resource sharing, an optimization at the HLS binding stage that maps multiple operations to the same functional unit. However, resource sharing is often limited to reusing instructions within a basic block. Instead of searching globally for the best control and dataflow graphs (CDFGs) to combine, it is constrained by existing instruction mappings and schedules. Coarse-grained function merging (CGFM) at the intermediate representation (IR) level can reuse control and dataflow patterns without dealing with the post-scheduling complexity of mapping operations onto functional units, wires, and registers. The merged functions produced by CGFM can be translated to RTL by HLS, yielding Coarse Grained Merged Accelerators (CGMAs). CGMAs are especially profitable across applications with similar data- and control-flow patterns. Prior work has used CGFM to generate CGMAs without regard for which CGFM algorithms best optimize area, power, and energy costs. We propose Guac, an energy-aware and SSA-based (static single assignment) CGMA generation methodology. Guac implements a novel ensemble of cost models for efficient CGMA generation. We also show that CGFM algorithms using SSA form to merge control- and dataflow graphs outperform prior non-SSA CGFM designs. We demonstrate significant area, power, and energy savings with respect to the state of the art. In particular, Guac more than doubles energy savings with respect to the closest related work while using a strong resource-sharing baseline.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13522",
        "abstract url": "https://arxiv.org/abs/2402.13522",
        "title": "RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal State",
        "rating": -1,
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Humans pay careful attention to the interlocutor's internal state in dialogues. For example, in recommendation dialogues, we make recommendations while estimating the seeker's internal state, such as his/her level of knowledge and interest. Since there are no existing annotated resources for the analysis, we constructed RecMind, a Japanese movie recommendation dialogue dataset with annotations of the seeker's internal state at the entity level. Each entity has a subjective label annotated by the seeker and an objective label annotated by the recommender. RecMind also features engaging dialogues with long seeker's utterances, enabling a detailed analysis of the seeker's internal state. Our analysis based on RecMind reveals that entities that the seeker has no knowledge about but has an interest in contribute to recommendation success. We also propose a response generation framework that explicitly considers the seeker's internal state, utilizing the chain-of-thought prompting. The human evaluation results show that our proposed method outperforms the baseline method in both consistency and the success of recommendations.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.14854",
        "abstract url": "https://arxiv.org/abs/2402.14854",
        "title": "A Dual-Prompting for Interpretable Mental Health Language Models",
        "rating": -1,
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite the increasing demand for AI-based mental health monitoring tools, their practical utility for clinicians is limited by the lack of interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to enhance the interpretability of Large Language Models (LLMs), particularly in mental health analysis, by providing evidence of suicidality through linguistic content. We propose a dual-prompting approach: (i) Knowledge-aware evidence extraction by leveraging the expert identity and a suicide dictionary with a mental health-specific LLM; and (ii) Evidence summarization by employing an LLM-based consistency evaluator. Comprehensive experiments demonstrate the effectiveness of combining domain-specific information, revealing performance improvements and the approach's potential to aid clinicians in assessing mental state progression.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18946",
        "abstract url": "https://arxiv.org/abs/2403.18946",
        "title": "Random Aggregate Beamforming for Over-the-Air Federated Learning in Large-Scale Networks",
        "rating": -1,
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "At present, there is a trend to deploy ubiquitous artificial intelligence (AI) applications at the edge of the network. As a promising framework that enables secure edge intelligence, federated learning (FL) has received widespread attention, and over-the-air computing (AirComp) has been integrated to further improve the communication efficiency. In this paper, we consider a joint device selection and aggregate beamforming design with the objectives of minimizing the aggregate error and maximizing the number of selected devices. This yields a combinatorial problem, which is difficult to solve especially in large-scale networks. To tackle the problems in a cost-effective manner, we propose a random aggregate beamforming-based scheme, which generates the aggregator beamforming vector via random sampling rather than optimization. The implementation of the proposed scheme does not require the channel estimation. We additionally use asymptotic analysis to study the obtained aggregate error and the number of the selected devices when the number of devices becomes large. Furthermore, a refined method that runs with multiple randomizations is also proposed for performance improvement. Extensive simulation results are presented to demonstrate the effectiveness of the proposed random aggregate beamforming-based scheme as well as the refined method.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "30 pages, 11 figures"
    },
    {
        "paper id": "2404.07212",
        "abstract url": "https://arxiv.org/abs/2404.07212",
        "title": "Hybrid Training of Denoising Networks to Improve the Texture Acutance of Digital Cameras",
        "rating": -1,
        "keywords": [
            [
                "image restoration"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "In order to evaluate the capacity of a camera to render textures properly, the standard practice, used by classical scoring protocols, is to compute the frequential response to a dead leaves image target, from which is built a texture acutance metric. In this work, we propose a mixed training procedure for image restoration neural networks, relying on both natural and synthetic images, that yields a strong improvement of this acutance metric without impairing fidelity terms. The feasibility of the approach is demonstrated both on the denoising of RGB images and the full development of RAW images, opening the path to a systematic improvement of the texture acutance of real imaging devices.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12745",
        "abstract url": "https://arxiv.org/abs/2402.12745",
        "title": "Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss",
        "rating": -1.5,
        "keywords": [
            [
                "Quantum"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "The problem of minimizing the maximum of $N$ convex, Lipschitz functions plays significant roles in optimization and machine learning. It has a series of results, with the most recent one requiring $O(N\u03b5^{-2/3} + \u03b5^{-8/3})$ queries to a first-order oracle to compute an $\u03b5$-suboptimal point. On the other hand, quantum algorithms for optimization are rapidly advancing with speedups shown on many important optimization problems. In this paper, we conduct a systematic study for quantum algorithms and lower bounds for minimizing the maximum of $N$ convex, Lipschitz functions. On one hand, we develop quantum algorithms with an improved complexity bound of $\\tilde{O}(\\sqrt{N}\u03b5^{-5/3} + \u03b5^{-8/3})$. On the other hand, we prove that quantum algorithms must take $\\tilde\u03a9(\\sqrt{N}\u03b5^{-2/3})$ queries to a first order quantum oracle, showing that our dependence on $N$ is optimal up to poly-logarithmic factors.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "22 pages, 1 figure, To appear in The Twelfth International Conference on Learning Representations (ICLR 2024)"
    },
    {
        "paper id": "2402.12759",
        "abstract url": "https://arxiv.org/abs/2402.12759",
        "title": "Towards Fair Allocation in Social Commerce Platforms",
        "rating": -1.5,
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Social commerce platforms are emerging businesses where producers sell products through re-sellers who advertise the products to other customers in their social network. Due to the increasing popularity of this business model, thousands of small producers and re-sellers are starting to depend on these platforms for their livelihood; thus, it is important to provide fair earning opportunities to them. The enormous product space in such platforms prohibits manual search, and motivates the need for recommendation algorithms to effectively allocate product exposure and, consequently, earning opportunities. In this work, we focus on the fairness of such allocations in social commerce platforms and formulate the problem of assigning products to re-sellers as a fair division problem with indivisible items under two-sided cardinality constraints, wherein each product must be given to at least a certain number of re-sellers and each re-seller must get a certain number of products. Our work systematically explores various well-studied benchmarks of fairness -- including Nash social welfare, envy-freeness up to one item (EF1), and equitability up to one item (EQ1) -- from both theoretical and experimental perspectives. We find that the existential and computational guarantees of these concepts known from the unconstrained setting do not extend to our constrained model. To address this limitation, we develop a mixed-integer linear program and other scalable heuristics that provide near-optimal approximation of Nash social welfare in simulated and real social commerce datasets. Overall, our work takes the first step towards achieving provable fairness alongside reasonable revenue guarantees on social commerce platforms.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12767",
        "abstract url": "https://arxiv.org/abs/2402.12767",
        "title": "When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting",
        "rating": -1.5,
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated stationary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov model to estimate latent environments and modular prior networks to identify latent states. The IDEA model outperforms several latest nonstationary forecasting methods on various benchmark datasets, highlighting its advantages in real-world scenarios.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12921",
        "abstract url": "https://arxiv.org/abs/2402.12921",
        "title": "Right on Time: Revising Time Series Models by Constraining their Explanations",
        "rating": -1.5,
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT). Our method enables interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12946",
        "abstract url": "https://arxiv.org/abs/2402.12946",
        "title": "Cell Graph Transformer for Nuclei Classification",
        "rating": -1.5,
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "diagnosis"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Nuclei classification is a critical step in computer-aided diagnosis with histopathology images. In the past, various methods have employed graph neural networks (GNN) to analyze cell graphs that model inter-cell relationships by considering nuclei as vertices. However, they are limited by the GNN mechanism that only passes messages among local nodes via fixed edges. To address the issue, we develop a cell graph transformer (CGT) that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes. Nevertheless, training the transformer with a cell graph presents another challenge. Poorly initialized features can lead to noisy self-attention scores and inferior convergence, particularly when processing the cell graphs with numerous connections. Thus, we further propose a novel topology-aware pretraining method that leverages a graph convolutional network (GCN) to learn a feature extractor. The pre-trained features may suppress unreasonable correlations and hence ease the finetuning of CGT. Experimental results suggest that the proposed cell graph transformer with topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance. Code and models are available at https://github.com/lhaof/CGT",
        "subjects": [
            "cs.CV"
        ],
        "comment": "AAAI 2024, Code and models are available at https://github.com/lhaof/CGT"
    },
    {
        "paper id": "2402.13103",
        "abstract url": "https://arxiv.org/abs/2402.13103",
        "title": "Multivariate Functional Linear Discriminant Analysis for the Classification of Short Time Series with Missing Data",
        "rating": -1.5,
        "keywords": [
            [
                "medical",
                "psychological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Functional linear discriminant analysis (FLDA) is a powerful tool that extends LDA-mediated multiclass classification and dimension reduction to univariate time-series functions. However, in the age of large multivariate and incomplete data, statistical dependencies between features must be estimated in a computationally tractable way, while also dealing with missing data. There is a need for a computationally tractable approach that considers the statistical dependencies between features and can handle missing values. We here develop a multivariate version of FLDA (MUDRA) to tackle this issue and describe an efficient expectation/conditional-maximization (ECM) algorithm to infer its parameters. We assess its predictive power on the \"Articulary Word Recognition\" data set and show its improvement over the state-of-the-art, especially in the case of missing data. MUDRA allows interpretable classification of data sets with large proportions of missing data, which will be particularly useful for medical or psychological data sets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13219",
        "abstract url": "https://arxiv.org/abs/2402.13219",
        "title": "Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies",
        "rating": -1.5,
        "keywords": [
            [
                "industrial",
                "chemical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and efficiency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influence diagrams, a hidden Markov model, and deep reinforcement learning. The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye-tracking data, process logs, and responses from questionnaires. The results indicate interesting insights regarding the effectiveness of the approach in aiding decision-making, decreasing perceived workload, and increasing situational awareness for the scenarios considered. Additionally, the results provide valuable insights to compare differences between styles of information gathering when using the system by individual participants. These findings are particularly relevant when predicting the overall performance of the individual participant and their capacity to successfully handle a plant upset and the alarms connected to it using process and human-machine interaction logs in real-time. These predictions enable the development of more effective intervention strategies.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13233",
        "abstract url": "https://arxiv.org/abs/2402.13233",
        "title": "SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification",
        "rating": -1.5,
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices. In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate the negative impacts of domain shifts. Our evaluation on a variety of multi-sensor time series classification tasks shows that SMORE achieves on average 1.98% higher accuracy than state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and 4.63x faster inference.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2308.03295"
    },
    {
        "paper id": "2402.13338",
        "abstract url": "https://arxiv.org/abs/2402.13338",
        "title": "Incentivized Exploration via Filtered Posterior Sampling",
        "rating": -1.5,
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study \"incentivized exploration\" (IE) in social learning problems where the principal (a recommendation algorithm) can leverage information asymmetry to incentivize sequentially-arriving agents to take exploratory actions. We identify posterior sampling, an algorithmic approach that is well known in the multi-armed bandits literature, as a general-purpose solution for IE. In particular, we expand the existing scope of IE in several practically-relevant dimensions, from private agent types to informative recommendations to correlated Bayesian priors. We obtain a general analysis of posterior sampling in IE which allows us to subsume these extended settings as corollaries, while also recovering existing results as special cases.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13402",
        "abstract url": "https://arxiv.org/abs/2402.13402",
        "title": "Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities",
        "rating": -1.5,
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces. Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive. This resulted in strong interest towards active learning methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective. However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven. In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws and often varies exploration policies during the experiment. Here, we explore interactive workflows building on multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then structured (physics-driven) sMFBO, and extending it to allow human in the loop interactive iMFBO workflows for adaptive and domain expert aligned exploration. These approaches are demonstrated over highly non-smooth multi-fidelity simulation data generated from an Ising model, considering spin-spin interaction as parameter space, lattice sizes as fidelity spaces, and the objective as maximizing heat capacity. Detailed analysis and comparison show the impact of physics knowledge injection and on-the-fly human decisions for improved exploration, current challenges, and potential opportunities for algorithm development with combining data, physics and real time human decisions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Main text includes 29 pages and 10 figures, Supplementary mat. includes 4 pages and 4 figures"
    },
    {
        "paper id": "2402.13418",
        "abstract url": "https://arxiv.org/abs/2402.13418",
        "title": "EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding",
        "rating": -1.5,
        "keywords": [
            [
                "biological",
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNNcan capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than state-of-the-art methods and attains 36x inference speedup in comparison with large pre-trained models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13452",
        "abstract url": "https://arxiv.org/abs/2402.13452",
        "title": "LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data",
        "rating": -1.5,
        "keywords": [
            [
                "Health",
                "Disease"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\% improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods, achieving an F1-score of 0.7291. Our work suggests that Twitter data can be effectively leveraged to simulate neighborhood-level MH outcomes.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.05564",
        "abstract url": "https://arxiv.org/abs/2403.05564",
        "title": "Promoting Fair Vaccination Strategies Through Influence Maximization: A Case Study on COVID-19 Spread",
        "rating": -1.5,
        "keywords": [
            [
                "health",
                "healthcare",
                "disease"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "The aftermath of the Covid-19 pandemic saw more severe outcomes for racial minority groups and economically-deprived communities. Such disparities can be explained by several factors, including unequal access to healthcare, as well as the inability of low income groups to reduce their mobility due to work or social obligations. Moreover, senior citizens were found to be more susceptible to severe symptoms, largely due to age-related health reasons. Adapting vaccine distribution strategies to consider a range of demographics is therefore essential to address these disparities. In this study, we propose a novel approach that utilizes influence maximization (IM) on mobility networks to develop vaccination strategies which incorporate demographic fairness. By considering factors such as race, social status, age, and associated risk factors, we aim to optimize vaccine distribution to achieve various fairness definitions for one or more protected attributes at a time. Through extensive experiments conducted on Covid-19 spread in three major metropolitan areas across the United States, we demonstrate the effectiveness of our proposed approach in reducing disease transmission and promoting fairness in vaccination distribution.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "10 pages, 4 figures"
    },
    {
        "paper id": "2402.12733",
        "abstract url": "https://arxiv.org/abs/2402.12733",
        "title": "BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying. Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors. However, most multi-behavior approaches have limitations in learning the relationship between different behaviors. In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP). Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users' purchase intent. Compared with mainstream sequence models, MLP is competitive in terms of accuracy and has unique advantages in simplicity and efficiency. Extensive experiments show that BMLP achieves significant improvement over state-of-the-art algorithms on four public datasets. In addition, its pure MLP architecture leads to a linear time complexity.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12763",
        "abstract url": "https://arxiv.org/abs/2402.12763",
        "title": "BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization",
        "rating": -2,
        "keywords": [
            [
                "graph"
            ],
            [
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.To achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \\%, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12800",
        "abstract url": "https://arxiv.org/abs/2402.12800",
        "title": "Radar-Based Recognition of Static Hand Gestures in American Sign Language",
        "rating": -2,
        "keywords": [
            [
                "Radar"
            ],
            [
                "Sign Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the fast-paced field of human-computer interaction (HCI) and virtual reality (VR), automatic gesture recognition has become increasingly essential. This is particularly true for the recognition of hand signs, providing an intuitive way to effortlessly navigate and control VR and HCI applications. Considering increased privacy requirements, radar sensors emerge as a compelling alternative to cameras. They operate effectively in low-light conditions without capturing identifiable human details, thanks to their lower resolution and distinct wavelength compared to visible light. While previous works predominantly deploy radar sensors for dynamic hand gesture recognition based on Doppler information, our approach prioritizes classification using an imaging radar that operates on spatial information, e.g. image-like data. However, generating large training datasets required for neural networks (NN) is a time-consuming and challenging process, often falling short of covering all potential scenarios. Acknowledging these challenges, this study explores the efficacy of synthetic data generated by an advanced radar ray-tracing simulator. This simulator employs an intuitive material model that can be adjusted to introduce data diversity. Despite exclusively training the NN on synthetic data, it demonstrates promising performance when put to the test with real measurement data. This emphasizes the practicality of our methodology in overcoming data scarcity challenges and advancing the field of automatic gesture recognition in VR and HCI applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 6 figures. Accepted to IEEE Radarconf2024"
    },
    {
        "paper id": "2402.12867",
        "abstract url": "https://arxiv.org/abs/2402.12867",
        "title": "Towards MLOps: A DevOps Tools Recommender System for Machine Learning System",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Applying DevOps practices to machine learning system is termed as MLOps and machine learning systems evolve on new data unlike traditional systems on requirements. The objective of MLOps is to establish a connection between different open-source tools to construct a pipeline that can automatically perform steps to construct a dataset, train the machine learning model and deploy the model to the production as well as store different versions of model and dataset. Benefits of MLOps is to make sure the fast delivery of the new trained models to the production to have accurate results. Furthermore, MLOps practice impacts the overall quality of the software products and is completely dependent on open-source tools and selection of relevant open-source tools is considered as challenged while a generalized method to select an appropriate open-source tools is desirable. In this paper, we present a framework for recommendation system that processes the contextual information (e.g., nature of data, type of the data) of the machine learning project and recommends a relevant toolchain (tech-stack) for the operationalization of machine learning systems. To check the applicability of the proposed framework, four different approaches i.e., rule-based, random forest, decision trees and k-nearest neighbors were investigated where precision, recall and f-score is measured, the random forest out classed other approaches with highest f-score value of 0.66.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12886",
        "abstract url": "https://arxiv.org/abs/2402.12886",
        "title": "Real-time High-resolution View Synthesis of Complex Scenes with Explicit 3D Visibility Reasoning",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "Synthesis"
            ]
        ],
        "abstract": "Rendering photo-realistic novel-view images of complex scenes has been a long-standing challenge in computer graphics. In recent years, great research progress has been made on enhancing rendering quality and accelerating rendering speed in the realm of view synthesis. However, when rendering complex dynamic scenes with sparse views, the rendering quality remains limited due to occlusion problems. Besides, for rendering high-resolution images on dynamic scenes, the rendering speed is still far from real-time. In this work, we propose a generalizable view synthesis method that can render high-resolution novel-view images of complex static and dynamic scenes in real-time from sparse views. To address the occlusion problems arising from the sparsity of input views and the complexity of captured scenes, we introduce an explicit 3D visibility reasoning approach that can efficiently estimate the visibility of sampled 3D points to the input views. The proposed visibility reasoning approach is fully differentiable and can gracefully fit inside the volume rendering pipeline, allowing us to train our networks with only multi-view images as supervision while refining geometry and texture simultaneously. Besides, each module in our pipeline is carefully designed to bypass the time-consuming MLP querying process and enhance the rendering quality of high-resolution images, enabling us to render high-resolution novel-view images in real-time.Experimental results show that our method outperforms previous view synthesis methods in both rendering quality and speed, particularly when dealing with complex dynamic scenes with sparse views.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12889",
        "abstract url": "https://arxiv.org/abs/2402.12889",
        "title": "BFT-DSN: A Byzantine Fault Tolerant Decentralized Storage Network",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "With the rapid development of blockchain and its applications, the amount of data stored on decentralized storage networks (DSNs) has grown exponentially. DSNs bring together affordable storage resources from around the world to provide robust, decentralized storage services for tens of thousands of decentralized applications (dApps). However, existing DSNs do not offer verifiability when implementing erasure coding for redundant storage, making them vulnerable to Byzantine encoders. Additionally, there is a lack of Byzantine fault-tolerant consensus for optimal resilience in DSNs. This paper introduces BFT-DSN, a Byzantine fault-tolerant decentralized storage network designed to address these challenges. BFT-DSN combines storage-weighted BFT consensus with erasure coding and incorporates homomorphic fingerprints and weighted threshold signatures for decentralized verification. The implementation of BFT-DSN demonstrates its comparable performance in terms of storage cost and latency as well as superior performance in Byzantine resilience when compared to existing industrial decentralized storage networks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "11 pages, 8 figures"
    },
    {
        "paper id": "2402.12922",
        "abstract url": "https://arxiv.org/abs/2402.12922",
        "title": "Gimbal Actuator Modeling for a Spin-Stabilized Spacecraft Equipped with a 1DoF Gimbaled-Thruster and two Reaction Wheels",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Attitude control of spacecraft during an impulsive orbital maneuver is a vital task. Many spacecraft and launchers use the gimbaled thrust vector control (TVC) in their attitude control system during an orbital maneuver. Mathematical modeling of the gimbal actuator is an important task because we should show the applicability of the gimbaled-TVC in a spacecraft. In this paper, a spin-stabilized spacecraft equipped with one degree of freedom (DoF) gimbaled-thruster and two reaction wheels (RWs) is considered. The control goals are disturbance rejection and thrust vector (spin-axis) stabilization based on one DoF gimbal actuator and two RWs. The gimbal is assumed to be equipped with a gearbox and a DC electric motor. This actuator must supply the gimbal torque to rotate the spacecraft nozzle. The mathematical model of the mentioned spacecraft is extended with respect to the DC motor equations. In order to investigate the applicability of the proposed method, an industrial DC electric motor is considered for the gimbal actuator. The simulation results prove that an industrial DC electric motor is able to be used for attitude control of the mentioned spacecraft. The simulation results indicate the applicability of the proposed control method in an impulsive orbital maneuver.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted in 5th National Conference on Application of Novel Technologies in Engineering Sciences"
    },
    {
        "paper id": "2402.12950",
        "abstract url": "https://arxiv.org/abs/2402.12950",
        "title": "QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs. Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement sufficiency and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples. Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48%-96.05% more test samples than the random noise under the same perturbation size constraints). The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%).",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12971",
        "abstract url": "https://arxiv.org/abs/2402.12971",
        "title": "How Temporal Unrolling Supports Neural Physics Simulators",
        "rating": -2,
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does not utilize this solver. We also quantify a difference in the accuracy of models trained in a fully differentiable setup compared to their non-differentiable counterparts. While differentiable setups perform best, the accuracy of unrolling without temporal gradients comes comparatively close. Furthermore, we empirically show that these behaviors are invariant to changes in the underlying physical system, the network architecture and size, and the numerical scheme. These results motivate integrating non-differentiable numerical simulators into training setups even if full differentiability is unavailable. We also observe that the convergence rate of common neural architectures is low compared to numerical algorithms. This encourages the use of hybrid approaches combining neural and numerical algorithms to utilize the benefits of both.",
        "subjects": [
            "physics.comp-ph"
        ],
        "comment": "Project Page: https://ge.in.tum.de/publications/how-temporal-unrolling-supports-neural-physics-simulators/ , Github Page: https://github.com/tum-pbs/unrolling"
    },
    {
        "paper id": "2402.12984",
        "abstract url": "https://arxiv.org/abs/2402.12984",
        "title": "Can GNN be Good Adapter for LLMs?",
        "rating": -2,
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "recommendation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on node text (next token prediction). Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5\\% in terms of node classification. Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by WWW'24"
    },
    {
        "paper id": "2402.13005",
        "abstract url": "https://arxiv.org/abs/2402.13005",
        "title": "SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms",
        "rating": -2,
        "keywords": [
            [
                "EEG",
                "clinical"
            ]
        ],
        "abstract": "The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning benchmark based on public datasets converted to a standardized format. This benchmark defines the machine-learning task as well as reporting metrics. We illustrate the use of the benchmark by evaluating a set of existing seizure detection algorithms. The SzCORE (Seizure Community Open-source Research Evaluation) framework and benchmark are made publicly available along with an open-source software library to facilitate research use, while enabling rigorous evaluation of the clinical significance of the algorithms, fostering a collective effort to more optimally detect seizures to improve the lives of people with epilepsy.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13027",
        "abstract url": "https://arxiv.org/abs/2402.13027",
        "title": "Solving the decision-making differential equations from eye fixation data in Unity software by using Hermite Long-Short-Term Memory neural network",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Cognitive decision-making processes are crucial aspects of human behavior, influencing various personal and professional domains. This research delves into the application of differential equations in analyzing decision-making accuracy by leveraging eye-tracking data within a virtual industrial town setting. The study unveils a systematic approach to transforming raw data into a differential equation, essential for deciphering the relationship between eye movements during decision-making processes. Mathematical relationship extraction and variable-parameter definition pave the way for deriving a differential equation that encapsulates the growth of fixations on characters. The key factors in this equation encompass the fixation rate $(\u03bb)$ and separation rate $(\u03bc)$, reflecting user interaction dynamics and their impact on decision-making complexities tied to user engagement with virtual characters. For a comprehensive grasp of decision dynamics, solving this differential equation requires initial fixation counts, fixation rate, and separation rate. The formulation of differential equations incorporates various considerations such as engagement duration, character-player distance, relative speed, and character attributes, enabling the representation of fixation changes, speed dynamics, distance variations, and the effects of character attributes. This comprehensive analysis not only enhances our comprehension of decision-making processes but also provides a foundational framework for predictive modeling and data-driven insights for future research and applications in cognitive science and virtual reality environments.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13045",
        "abstract url": "https://arxiv.org/abs/2402.13045",
        "title": "A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction",
        "rating": -2,
        "keywords": [
            [
                "forecast"
            ]
        ],
        "abstract": "This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing. Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions. Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces. Then a Recurrent Neural Network (RNN) is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model. Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another RNN to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF. A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework. These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time. This adaption, driven by the uncertainties of the RNN models, addresses inaccuracies stemming from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions. Compared to the traditional RNN-based prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13049",
        "abstract url": "https://arxiv.org/abs/2402.13049",
        "title": "Two Quantum Paradigms, but Still No Signal",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "An overwhelming majority of quantum (pure and mixed) states, when undertaking a POVM measurement, will result in a classical probability with no algorithmic information. Thus most quantum states produce white noise when measured. Furthermore most non-pointer states, when undergoing the decoherence process, will produce white noise. These results can be seen as consequences of the vastness of Hilbert spaces.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13062",
        "abstract url": "https://arxiv.org/abs/2402.13062",
        "title": "3D high-resolution imaging algorithm using 1D MIMO array for autonomous driving application",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving",
                "trajectory",
                "radar",
                "vehicle"
            ]
        ],
        "abstract": "The problem of 3D high-resolution imaging in automotive multiple-input multiple-output (MIMO) side-looking radar using a 1D array is considered. The concept of motion-enhanced snapshots is introduced for generating larger apertures in the azimuth dimension. For the first time, 3D imaging capabilities can be achieved with high angular resolution using a 1D MIMO antenna array, which can alleviate the requirement for large radar systems in autonomous vehicles. The robustness to variations in the vehicle's movement trajectory is also considered and addressed with relevant compensations in the steering vector. The available degrees of freedom as well as the Signal to Noise Ratio (SNR) are shown to increase with the proposed method compared to conventional imaging approaches. The performance of the algorithm has been studied in simulations, and validated with experimental data collected in a realistic driving scenario.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IEEE Transactions. For citation, please use the IEEE DOI"
    },
    {
        "paper id": "2402.13096",
        "abstract url": "https://arxiv.org/abs/2402.13096",
        "title": "A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching in 6G HAPS Networks",
        "rating": -2,
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "This study investigates the integration of a high altitude platform station (HAPS), a non-terrestrial network (NTN) node, into the cell-switching paradigm for energy saving. By doing so, the sustainability and ubiquitous connectivity targets can be achieved. Besides, a delay-aware approach is also adopted, where the delay profiles of users are respected in such a way that we attempt to meet the latency requirements of users with a best-effort strategy. To this end, a novel, simple, and lightweight Q-learning algorithm is designed to address the cell-switching optimization problem. During the simulation campaigns, different interference scenarios and delay situations between base stations are examined in terms of energy consumption and quality-of-service (QoS), and the results confirm the efficacy of the proposed Q-learning algorithm.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13132",
        "abstract url": "https://arxiv.org/abs/2402.13132",
        "title": "Electric Field Evaluation of Reconfigurable Intelligent Surface in Wireless Networks",
        "rating": -2,
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Reconfigurable intelligent surface (RIS) used as infrastructure in wireless networks has been a trend, thanks to its low cost and high flexibility. Working in many ways including reflective mirrors and phase-shifted surfaces, RIS is able to enhance the coverage in communications and provide more degrees of freedom for sensing. However, the key issue lies in how to place RIS in accordance with the regulations for electromagnetic field (EMF) exposure, which requires refined evaluations. In this paper, we first investigate the regulations in terms of E-field. Then, relevant deployment characteristics are evaluated jointly: the minimum distance from the base station (BS) to the RIS, and the minimum height of the RIS are given for a given BS power limit and as function of the number of RIS elements. The ray-tracing simulations verify the correctness of our analysis. Besides, different frequency ranges (FRs) and radiation patterns of RIS elements are investigated. The results show that the EMF exposure risk is negligible when RIS works in the reflective-only (RO) mode. However, when it works in the beamforming (BO) mode, its placement should be well specified based on our analytical framework to comply with the regulations of E-field limit in general public scenarios. Finally, we provide an E-field measurement methodology and low-cost solutions in terms of general wireless networks and 5G standalone networks, which pave the way for real-world evaluation in future work.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "9 pages, 9 figures, 1 table, to be included in proc. IEEE DySPAN 2024"
    },
    {
        "paper id": "2402.13136",
        "abstract url": "https://arxiv.org/abs/2402.13136",
        "title": "Relaxing Trust Assumptions on Quantum Key Distribution Networks",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum security over long distances with untrusted relays is largely unfounded and is still an open question for active research. Nevertheless, quantum networks based on trusted relays are being built across the globe. However, standard QKD network architecture implores a complete trust requirement on QKD relays, which is too demanding and limits the use cases for QKD networks. In this work, we explore the possibility to securely relay a secret in a QKD network by relaxing the trust assumptions (if not completely) on the relay. We characterize QKD relays with different trust levels, namely, Full Access Trust (FAT), Partial Access Trust (PAT), and No Access Trust (NAT). As the name suggests, each level defines the degree with which a relay is required to be trusted with the secret provided by the key management system for end-to-end communication. We then review and propose multiple constructions of the QKD key management system based on the different trust levels. Main contribution of the paper is realized by evaluating key management systems with no access trust level. In principle, we review key management with centralized topology and propose a new decentralized key management system. These different topologies provide various advantages based on the QKD network requirements, allowing an operational flexibility in the architecture. We believe this work presents a new perspective to the open problem of providing a confiding and a practical solution for future long range secure communications",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13172",
        "abstract url": "https://arxiv.org/abs/2402.13172",
        "title": "3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "Biomechanical",
                "health",
                "diagnosis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate 3D kinematics estimation of human body is crucial in various applications for human health and mobility, such as rehabilitation, injury prevention, and diagnosis, as it helps to understand the biomechanical loading experienced during movement. Conventional marker-based motion capture is expensive in terms of financial investment, time, and the expertise required. Moreover, due to the scarcity of datasets with accurate annotations, existing markerless motion capture methods suffer from challenges including unreliable 2D keypoint detection, limited anatomic accuracy, and low generalization capability. In this work, we propose a novel biomechanics-aware network that directly outputs 3D kinematics from two input views with consideration of biomechanical prior and spatio-temporal information. To train the model, we create synthetic dataset ODAH with accurate kinematics annotations generated by aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal model. Our extensive experiments demonstrate that the proposed approach, only trained on synthetic data, outperforms previous state-of-the-art methods when evaluated across multiple datasets, revealing a promising direction for enhancing video-based human motion capture",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13194",
        "abstract url": "https://arxiv.org/abs/2402.13194",
        "title": "Quantum Wiretap Channel Coding Assisted by Noisy Correlation",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "We consider the private classical capacity of a quantum wiretap channel, where the users (sender Alice, receiver Bob, and eavesdropper Eve) have access to the resource of a shared quantum state, additionally to their channel inputs and outputs. An extreme case is maximal entanglement or a secret key between Alice and Bob, both of which would allow for onetime padding the message. But here both the wiretap channel and the shared state are general. In the other extreme case that the state is trivial, we recover the wiretap channel and its private capacity [N. Cai, A. Winter and R. W. Yeung, Probl. Inform. Transm. 40(4):318-336, 2004]. We show how to use the given resource state to build a code for secret classical communication. Our main result is a lower bound on the assisted private capacity, which asymptotically meets the multi-letter converse and which encompasses all sorts of previous results as special cases.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13195",
        "abstract url": "https://arxiv.org/abs/2402.13195",
        "title": "Design and Flight Demonstration of a Quadrotor for Urban Mapping and Target Tracking Research",
        "rating": -2,
        "keywords": [
            [
                "vehicle",
                "Flight"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "This paper describes the hardware design and flight demonstration of a small quadrotor with imaging sensors for urban mapping, hazard avoidance, and target tracking research. The vehicle is equipped with five cameras, including two pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a two-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running the Robot Operating System software is used for data collection. An autonomous tracking behavior was implemented to coordinate the motion of the quadrotor and gimbaled camera to track a moving GPS coordinate. The data collection system was demonstrated through a flight test that tracked a moving GPS-tagged vehicle through a series of roads and parking lots. A map of the environment was reconstructed from the collected images using the Direct Sparse Odometry (DSO) algorithm. The performance of the quadrotor was also characterized by acoustic noise, communication range, battery voltage in hover, and maximum speed tests.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 10 figures, To be presented at IEEE SoutheastCon 2024"
    },
    {
        "paper id": "2402.13251",
        "abstract url": "https://arxiv.org/abs/2402.13251",
        "title": "FlashTex: Fast Relightable Mesh Texturing with LightControlNet",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "text-to-image"
            ]
        ],
        "abstract": "Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting. Our algorithm is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "Project page: https://flashtex.github.io/"
    },
    {
        "paper id": "2402.13297",
        "abstract url": "https://arxiv.org/abs/2402.13297",
        "title": "Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences",
        "rating": -2,
        "keywords": [
            [
                "Biology"
            ]
        ],
        "abstract": "N-terminal coding sequence (NCS) influences gene expression by impacting the translation initiation rate. The NCS optimization problem is to find an NCS that maximizes gene expression. The problem is important in genetic engineering. However, current methods for NCS optimization such as rational design and statistics-guided approaches are labor-intensive yield only relatively small improvements. This paper introduces a deep learning/synthetic biology co-designed few-shot training workflow for NCS optimization. Our method utilizes k-nearest encoding followed by word2vec to encode the NCS, then performs feature extraction using attention mechanisms, before constructing a time-series network for predicting gene expression intensity, and finally a direct search algorithm identifies the optimal NCS with limited training data. We took green fluorescent protein (GFP) expressed by Bacillus subtilis as a reporting protein of NCSs, and employed the fluorescence enhancement factor as the metric of NCS optimization. Within just six iterative experiments, our model generated an NCS (MLD62) that increased average GFP expression by 5.41-fold, outperforming the state-of-the-art NCS designs. Extending our findings beyond GFP, we showed that our engineered NCS (MLD62) can effectively boost the production of N-acetylneuraminic acid by enhancing the expression of the crucial rate-limiting GNA1 gene, demonstrating its practical utility. We have open-sourced our NCS expression database and experimental procedures for public use.",
        "subjects": [
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13321",
        "abstract url": "https://arxiv.org/abs/2402.13321",
        "title": "Rigor with Machine Learning from Field Theory to the Poincar\u00e9 Conjecture",
        "rating": -2,
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Machine learning techniques are increasingly powerful, leading to many breakthroughs in the natural sciences, but they are often stochastic, error-prone, and blackbox. How, then, should they be utilized in fields such as theoretical physics and pure mathematics that place a premium on rigor and understanding? In this Perspective we discuss techniques for obtaining rigor in the natural sciences with machine learning. Non-rigorous methods may lead to rigorous results via conjecture generation or verification by reinforcement learning. We survey applications of these techniques-for-rigor ranging from string theory to the smooth $4$d Poincar\u00e9 conjecture in low-dimensional topology. One can also imagine building direct bridges between machine learning theory and either mathematics or theoretical physics. As examples, we describe a new approach to field theory motivated by neural network theory, and a theory of Riemannian metric flows induced by neural network gradient descent, which encompasses Perelman's formulation of the Ricci flow that was utilized to resolve the $3$d Poincar\u00e9 conjecture.",
        "subjects": [
            "hep-th"
        ],
        "comment": "13 pages. Preprint of edited version in Nature Reviews Physics. Please cite journal version"
    },
    {
        "paper id": "2402.13324",
        "abstract url": "https://arxiv.org/abs/2402.13324",
        "title": "Quantum Pseudorandomness Cannot Be Shrunk In a Black-Box Way",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Pseudorandom Quantum States (PRS) were introduced by Ji, Liu and Song as quantum analogous to Pseudorandom Generators. They are an ensemble of states efficiently computable but computationally indistinguishable from Haar random states. Subsequent works have shown that some cryptographic primitives can be constructed from PRSs. Moreover, recent classical and quantum oracle separations of PRS from One-Way Functions strengthen the interest in a purely quantum alternative building block for quantum cryptography, potentially weaker than OWFs. However, our lack of knowledge of extending or shrinking the number of qubits of the PRS output still makes it difficult to reproduce some of the classical proof techniques and results. Short-PRSs, that is PRSs with logarithmic size output, have been introduced in the literature along with cryptographic applications, but we still do not know how they relate to PRSs. Here we answer half of the question, by showing that it is not possible to shrink the output of a PRS from polynomial to logarithmic qubit length while still preserving the pseudorandomness property, in a relativized way. More precisely, we show that relative to Kretschmer's quantum oracle (TQC 2021) short-PRSs cannot exist (while PRSs exist, as shown by Kretschmer's work).",
        "subjects": [
            "quant-ph"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2402.13352",
        "abstract url": "https://arxiv.org/abs/2402.13352",
        "title": "KetGPT - Dataset Augmentation of Quantum Circuits using Transformers",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware. This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived from existing quantum algorithms and follows the typical patterns of human-written algorithm-based code (e.g., order of gates and qubits). Our three-fold verification process, involving manual inspection and Qiskit framework execution, transformer-based classification, and structural analysis, demonstrates the efficacy of KetGPT in producing large amounts of additional circuits that closely align with algorithm-based structures. Beyond benchmarking, we envision KetGPT contributing substantially to AI-driven quantum compilers and systems.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13378",
        "abstract url": "https://arxiv.org/abs/2402.13378",
        "title": "Stable matching as transportation",
        "rating": -2,
        "keywords": [
            [
                "organ"
            ]
        ],
        "abstract": "We study matching markets with aligned preferences and establish a connection between common design objectives -- stability, efficiency, and fairness -- and the theory of optimal transport. Optimal transport gives new insights into the structural properties of matchings obtained from pursuing these objectives, and into the trade-offs between different objectives. Matching markets with aligned preferences provide a tractable stylized model capturing supply-demand imbalances in a range of settings such as partnership formation, school choice, organ donor exchange, and markets with transferable utility where bargaining over transfers happens after a match is formed.",
        "subjects": [
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13417",
        "abstract url": "https://arxiv.org/abs/2402.13417",
        "title": "Unlocking the `Why' of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Explanations are crucial for enhancing user trust and understanding within modern recommendation systems. To build truly explainable systems, we need high-quality datasets that elucidate why users make choices. While previous efforts have focused on extracting users' post-purchase sentiment in reviews, they ignore the reasons behind the decision to buy. In our work, we propose a novel purchase reason explanation task. To this end, we introduce an LLM-based approach to generate a dataset that consists of textual explanations of why real users make certain purchase decisions. We induce LLMs to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review. An automated, LLM-driven evaluation, as well as a small scale human evaluation, confirms the effectiveness of our approach to obtaining high-quality, personalized explanations. We benchmark this dataset on two personalized explanation generation tasks. We release the code and prompts to spur further research.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13442",
        "abstract url": "https://arxiv.org/abs/2402.13442",
        "title": "CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting",
        "rating": -2,
        "keywords": [
            [
                "text-to-image"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages. To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator. To improve text-image alignment, FRIDA's major weakness, our system uses pre-trained text-to-image models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content. We propose a self-supervised fine-tuning procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art text-image alignment models with robots to enable co-painting in the physical world. Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text prompt more clearly than FRIDA, both from a blank canvas and one with human created work. More generally, our fine-tuning procedure successfully encodes the robot's constraints and abilities into a foundation model, showcasing promising results as an effective method for reducing sim-to-real gaps.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13464",
        "abstract url": "https://arxiv.org/abs/2402.13464",
        "title": "Investigating Why Clinicians Deviate from Standards of Care: Liberating Patients from Mechanical Ventilation in the ICU",
        "rating": -2,
        "keywords": [
            [
                "medical",
                "healthcare",
                "Clinical"
            ]
        ],
        "abstract": "Clinical practice guidelines, care pathways, and protocols are designed to support evidence-based practices for clinicians; however, their adoption remains a challenge. We set out to investigate why clinicians deviate from the ``Wake Up and Breathe'' protocol, an evidence-based guideline for liberating patients from mechanical ventilation in the intensive care unit (ICU). We conducted over 40 hours of direct observations of live clinical workflows, 17 interviews with frontline care providers, and 4 co-design workshops at three different medical intensive care units. Our findings indicate that unlike prior literature suggests, disagreement with the protocol is not a substantial barrier to adoption. Instead, the uncertainty surrounding the application of the protocol for individual patients leads clinicians to deprioritize adoption in favor of tasks where they have high certainty. Reflecting on these insights, we identify opportunities for technical systems to help clinicians in effectively executing the protocol and discuss future directions for HCI research to support the integration of protocols into clinical practice in complex, team-based healthcare settings.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "to appear at CHI 2024"
    },
    {
        "paper id": "2402.14862",
        "abstract url": "https://arxiv.org/abs/2402.14862",
        "title": "SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic",
        "rating": -2,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet communication standard protocol in the Automotive Open System Architecture (AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However, SOME/IP lacks a robust security architecture, making it susceptible to potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP communication. In this paper, we propose SISSA, a SOME/IP communication traffic-based approach for modeling and analyzing in-vehicle functional safety and cyber security. Specifically, SISSA models hardware failures with the Weibull distribution and addresses five potential attacks on SOME/IP communication, including Distributed Denial-of-Services, Man-in-the-Middle, and abnormal communication processes, assuming a malicious user accesses the in-vehicle network. Subsequently, SISSA designs a series of deep learning models with various backbones to extract features from SOME/IP sessions among ECUs. We adopt residual self-attention to accelerate the model's convergence and enhance detection accuracy, determining whether an ECU is under attack, facing functional failure, or operating normally. Additionally, we have created and annotated a dataset encompassing various classes, including indicators of attack, functionality, and normalcy. This contribution is noteworthy due to the scarcity of publicly accessible datasets with such characteristics.Extensive experimental results show the effectiveness and efficiency of SISSA.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.17775",
        "abstract url": "https://arxiv.org/abs/2402.17775",
        "title": "Wavelet Scattering Transform for Bioacustics: Application to Watkins Marine Mammal Sound Database",
        "rating": -2,
        "keywords": [
            [
                "Bioacustics"
            ]
        ],
        "abstract": "Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) is an extensive labeled dataset used in machine learning applications. However, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. This study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. Subsequently, we propose the application of the Wavelet Scattering Transform (WST) in place of standard methods based on the Short-Time Fourier Transform (STFT). The study also tackles a classification task using an ad-hoc deep architecture with residual layers. We outperform the existing classification architecture by $6\\%$ in accuracy using WST and $8\\%$ using Mel spectrogram preprocessing, effectively reducing by half the number of misclassified samples, and reaching a top accuracy of $96\\%$.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.05567",
        "abstract url": "https://arxiv.org/abs/2403.05567",
        "title": "A Unified Framework for Underwater Metaverse with Optical Perception",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "With the advancement of AI technology and increasing attention to deep-sea exploration, the underwater Metaverse is gradually emerging. This paper explores the concept of underwater Metaverse, emerging virtual reality systems and services aimed at simulating and enhancing virtual experience of marine environments. First, we discuss potential applications of underwater Metaverse in underwater scientific research and marine conservation. Next, we present the architecture and supporting technologies of the underwater Metaverse, including high-resolution underwater imageing technologies and image processing technologies for rendering a realistic virtual world. Based on this, we present a use case for building a realistic underwater virtual world using underwater quantum imaging-generated artificial intelligence (QI-GAI) technology. The results demonstrate the effectiveness of the underwater Metaverse framework in simulating complex underwater environments, thus validating its potential in providing high-quality, interactive underwater virtual experiences. Finally, the paper examines the future development directions of underwater Metaverse, and provides new perspectives for marine science and conservation.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12720",
        "abstract url": "https://arxiv.org/abs/2402.12720",
        "title": "Revisiting the Information Capacity of Neural Network Watermarks: Upper Bound Estimation and Beyond",
        "rating": -2.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "watermark"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "To trace the copyright of deep neural networks, an owner can embed its identity information into its model as a watermark. The capacity of the watermark quantify the maximal volume of information that can be verified from the watermarked model. Current studies on capacity focus on the ownership verification accuracy under ordinary removal attacks and fail to capture the relationship between robustness and fidelity. This paper studies the capacity of deep neural network watermarks from an information theoretical perspective. We propose a new definition of deep neural network watermark capacity analogous to channel capacity, analyze its properties, and design an algorithm that yields a tight estimation of its upper bound under adversarial overwriting. We also propose a universal non-invasive method to secure the transmission of the identity message beyond capacity by multiple rounds of ownership verification. Our observations provide evidence for neural network owners and defenders that are curious about the tradeoff between the integrity of their ownership and the performance degradation of their products.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted by AAAI 2024"
    },
    {
        "paper id": "2402.12722",
        "abstract url": "https://arxiv.org/abs/2402.12722",
        "title": "Structural Knowledge Informed Continual Multivariate Time Series Forecasting",
        "rating": -2.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative MTS samples from each regime for memory replay. Specifically, we develop a forecasting model based on graph structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the MTS data. As such, MTS representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the continual learning context. Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of MTS data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime. Thorough empirical studies on synthetic and real-world benchmarks validate SKI-CL's efficacy and advantages over the state-of-the-art for continual MTS forecasting tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12727",
        "abstract url": "https://arxiv.org/abs/2402.12727",
        "title": "Diffusion Posterior Sampling is Computationally Intractable",
        "rating": -2.5,
        "keywords": [
            [
                "Diffusion",
                "inpainting",
                "super-resolution"
            ],
            [
                "MRI"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \\mid x)$ and a measurement $y$, and would like to sample from $p(x \\mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time. In this paper we show that posterior sampling is \\emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \\emph{every} algorithm takes superpolynomial time, even though \\emph{unconditional} sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12761",
        "abstract url": "https://arxiv.org/abs/2402.12761",
        "title": "FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework",
        "rating": -2.5,
        "keywords": [
            [
                "federated learning"
            ],
            [
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph anomaly detection (GAD) aims to identify anomalous graphs that significantly deviate from other ones, which has raised growing attention due to the broad existence and complexity of graph-structured data in many real-world scenarios. However, existing GAD methods usually execute with centralized training, which may lead to privacy leakage risk in some sensitive cases, thereby impeding collaboration among organizations seeking to collectively develop robust GAD models. Although federated learning offers a promising solution, the prevalent non-IID problems and high communication costs present significant challenges, particularly pronounced in collaborations with graph data distributed among different participants. To tackle these challenges, we propose an effective federated graph anomaly detection framework (FGAD). We first introduce an anomaly generator to perturb the normal graphs to be anomalous, and train a powerful anomaly detector by distinguishing generated anomalous graphs from normal ones. Then, we leverage a student model to distill knowledge from the trained anomaly detector (teacher model), which aims to maintain the personality of local models and alleviate the adverse impact of non-IID problems. Moreover, we design an effective collaborative learning mechanism that facilitates the personalization preservation of local models and significantly reduces communication costs among clients. Empirical results of the GAD tasks on non-IID graphs compared with state-of-the-art baselines demonstrate the superiority and efficiency of the proposed FGAD method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12790",
        "abstract url": "https://arxiv.org/abs/2402.12790",
        "title": "From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition",
        "rating": -2.5,
        "keywords": [
            [
                "3D",
                "Skeleton"
            ],
            [
                "biomechanical",
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The advancement of deep learning in human activity recognition (HAR) using 3D skeleton data is critical for applications in healthcare, security, sports, and human-computer interaction. This paper tackles a well-known gap in the field, which is the lack of testing in the applicability and reliability of XAI evaluation metrics in the skeleton-based HAR domain. We have tested established XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this problem. The study also introduces a perturbation method that respects human biomechanical constraints to ensure realistic variations in human movement. Our findings indicate that \\textit{faithfulness} may not be a reliable metric in certain contexts, such as with the EfficientGCN model. Conversely, stability emerges as a more dependable metric when there is slight input data perturbations. CAM and Grad-CAM are also found to produce almost identical explanations, leading to very similar XAI metric performance. This calls for the need for more diversified metrics and new XAI methods applied in skeleton-based HAR.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13033",
        "abstract url": "https://arxiv.org/abs/2402.13033",
        "title": "Enhancing Real-World Complex Network Representations with Hyperedge Augmentation",
        "rating": -2.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "biology"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary node features by extracting from the virtual hyperedge information, which are used for enhancing GNN performances on downstream tasks. We design three diverse virtual hyperedge construction strategies to accompany the augmentation scheme: (1) via graph statistics, (2) from multiple data perspectives, and (3) utilising multi-modality. Furthermore, to facilitate HyperAug evaluation, we provide 23 novel real-world graph datasets across various domains including social media, biology, and e-commerce. Our empirical study shows that HyperAug consistently and significantly outperforms GNN baselines and other graph augmentation methods, across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into graph augmentation methods for real-world complex networks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Preprint. Under review. 17 pages, 4 figures, 14 tables. arXiv admin note: text overlap with arXiv:2306.05108"
    },
    {
        "paper id": "2402.13101",
        "abstract url": "https://arxiv.org/abs/2402.13101",
        "title": "A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations",
        "rating": -2.5,
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale simulations. However, the computational costs stand in the way of the practical application of this approach. The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point. A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models. In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step. Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest. We achieve this for an elasto-plastic material by predicting full-field microscopic strains using a graph neural network (GNN) while retaining the microscopic constitutive material model to obtain the stresses. This hybrid data-physics graph-based approach avoids the high dimensionality originating from predicting full-field responses while allowing non-locality to arise. By training the GNN on a variety of meshes, it learns to generalize to unseen meshes, allowing a single model to be used for a range of microstructures. The embedded microscopic constitutive model in the GNN implicitly tracks history-dependent variables and leads to improved accuracy. We demonstrate for several challenging scenarios that the surrogate can predict complex macroscopic stress-strain paths. As the computation time of our method scales favorably with the number of elements in the microstructure compared to the FE method, our method can significantly accelerate FE2 simulations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13221",
        "abstract url": "https://arxiv.org/abs/2402.13221",
        "title": "CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning",
        "rating": -2.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "chemistry"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text. We invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). We define 11 property prediction tasks and 6 structure prediction tasks, which are of special interest for nanomaterial research. We benchmark the performance of a wide array of baseline methods and use these benchmarking results to highlight areas which need future work. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale -- both on the individual graph level and of the dataset as a whole -- and the only nanomaterials datasets with high structural and elemental diversity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 15 figures, 8 tables. Dataset is available at https://github.com/UlrikFriisJensen/CHILI"
    },
    {
        "paper id": "2402.12779",
        "abstract url": "https://arxiv.org/abs/2402.12779",
        "title": "Two-stage Rainfall-Forecasting Diffusion Model",
        "rating": -3,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "radar"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \\href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12906",
        "abstract url": "https://arxiv.org/abs/2402.12906",
        "title": "Fog enabled distributed training architecture for federated learning",
        "rating": -3,
        "keywords": [
            [
                "federated learning"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "The amount of data being produced at every epoch of second is increasing every moment. Various sensors, cameras and smart gadgets produce continuous data throughout its installation. Processing and analyzing raw data at a cloud server faces several challenges such as bandwidth, congestion, latency, privacy and security. Fog computing brings computational resources closer to IoT that addresses some of these issues. These IoT devices have low computational capability, which is insufficient to train machine learning. Mining hidden patterns and inferential rules from continuously growing data is crucial for various applications. Due to growing privacy concerns, privacy preserving machine learning is another aspect that needs to be inculcated. In this paper, we have proposed a fog enabled distributed training architecture for machine learning tasks using resources constrained devices. The proposed architecture trains machine learning model on rapidly changing data using online learning. The network is inlined with privacy preserving federated learning training. Further, the learning capability of architecture is tested on a real world IIoT use case. We trained a neural network model for human position detection in IIoT setup on rapidly changing data.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Conference paper accepted at BDA 2021"
    },
    {
        "paper id": "2402.12994",
        "abstract url": "https://arxiv.org/abs/2402.12994",
        "title": "Distributionally Robust Graph-based Recommendation System",
        "rating": -3,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS). However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts. Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support. Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently. Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts. The code is available at https://github.com/WANGBohaO-jpg/DR-GNN.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by WWW2024"
    },
    {
        "paper id": "2402.13001",
        "abstract url": "https://arxiv.org/abs/2402.13001",
        "title": "A unifying primary framework for quantum graph neural networks from quantum graph states",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "Graph states are used to represent mathematical graphs as quantum states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum graph neural network model can be understood and realized based on graph states. We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "short version 6 pages, a few important typos are corrected"
    },
    {
        "paper id": "2402.13008",
        "abstract url": "https://arxiv.org/abs/2402.13008",
        "title": "Efficient Enumeration of Large Maximal k-Plexes",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "Finding cohesive subgraphs in a large graph has many important applications, such as community detection and biological network analysis. Clique is often a too strict cohesive structure since communities or biological modules rarely form as cliques for various reasons such as data noise. Therefore, $k$-plex is introduced as a popular clique relaxation, which is a graph where every vertex is adjacent to all but at most $k$ vertices. In this paper, we propose a fast branch-and-bound algorithm as well as its task-based parallel version to enumerate all maximal $k$-plexes with at least $q$ vertices. Our algorithm adopts an effective search space partitioning approach that provides a lower time complexity, a new pivot vertex selection method that reduces candidate vertex size, an effective upper-bounding technique to prune useless branches, and three novel pruning techniques by vertex pairs. Our parallel algorithm uses a timeout mechanism to eliminate straggler tasks, and maximizes cache locality while ensuring load balancing. Extensive experiments show that compared with the state-of-the-art algorithms, our sequential and parallel algorithms enumerate large maximal $k$-plexes with up to $5 \\times$ and $18.9 \\times$ speedup, respectively. Ablation results also demonstrate that our pruning techniques bring up to $7 \\times$ speedup compared with our basic algorithm.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2402.13029",
        "abstract url": "https://arxiv.org/abs/2402.13029",
        "title": "Federated Learning for Iot/Edge/Fog Computing Systems",
        "rating": -3,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Iot"
            ]
        ],
        "abstract": "With the help of a new architecture called Edge/Fog (E/F) computing, cloud computing services can now be extended nearer to data generator devices. E/F computing in combination with Deep Learning (DL) is a promisedtechnique that is vastly applied in numerous fields. To train their models, data producers in conventional DL architectures with E/F computing enable them to repeatedly transmit and communicate data with third-party servers, like Edge/Fog or cloud servers. Due to the extensive bandwidth needs, legal issues, and privacy risks, this architecture is frequently impractical. Through a centralized server, the models can be co-trained by FL through distributed clients, including cars, hospitals, and mobile phones, while preserving data localization. As it facilitates group learning and model optimization, FL can therefore be seen as a motivating element in the E/F computing paradigm. Although FL applications in E/F computing environments have been considered in previous studies, FL execution and hurdles in the E/F computing framework have not been thoroughly covered. In order to identify advanced solutions, this chapter will provide a review of the application of FL in E/F computing systems. We think that by doing this chapter, researchers will learn more about how E/F computing and FL enable related concepts and technologies. Some case studies about the implementation of federated learning in E/F computing are being investigated. The open issues and future research directions are introduced.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "21 pages, 4 figures, Book chapter"
    },
    {
        "paper id": "2402.13201",
        "abstract url": "https://arxiv.org/abs/2402.13201",
        "title": "Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers",
        "rating": -3,
        "keywords": [
            [
                "robotics",
                "robot"
            ],
            [
                "physics"
            ]
        ],
        "abstract": "Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning. We test our method in simulation using Isaac Gym, a realistic physics simulation environment designed for reinforcement learning. We empirically demonstrate that our method achieves natural looking gaits for Bittle, a resource-constrained quadruped robot. We also run multiple simulations to show the effects of pruning and quantization on the performance of the model. Our results show that quantization (down to 4 bits) and pruning reduce model size by around 30\\% while maintaining a competitive reward, making the model deployable in a resource-constrained system.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 4 figures"
    },
    {
        "paper id": "2402.13204",
        "abstract url": "https://arxiv.org/abs/2402.13204",
        "title": "SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search",
        "rating": -3,
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency. HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms. However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly. Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front. Addressing these issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware NAS. Our method leverages adaptive evolutionary operators guided by the learned importance of NN design parameters. Specifically, through tree-based surrogate models and a Reinforcement Learning agent, we aspire to gather knowledge on 'How' and 'When' to evolve NN architectures. Comprehensive evaluations across various NAS search spaces and hardware devices on the ImageNet-1k dataset have shown the merit of SONATA with up to 0.25% improvement in accuracy and up to 2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Pareto dominance over the native NSGA-II, further stipulating the importance of self-adaptive evolution operators in HW-aware NAS.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "13 pages, 9 figures"
    },
    {
        "paper id": "2402.13435",
        "abstract url": "https://arxiv.org/abs/2402.13435",
        "title": "Learning to Retrieve for Job Matching",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "Web-scale search systems typically tackle the scalability challenge with a two-step paradigm: retrieval and ranking. The retrieval step, also known as candidate selection, often involves extracting standardized entities, creating an inverted index, and performing term matching for retrieval. Such traditional methods require manual and time-consuming development of query models. In this paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns job search and recommendation systems. In the realm of promoted jobs, the key objective is to improve the quality of applicants, thereby delivering value to recruiter customers. To achieve this, we leverage confirmed hire data to construct a graph that evaluates a seeker's qualification for a job, and utilize learned links for retrieval. Our learned model is easy to explain, debug, and adjust. On the other hand, the focus for organic jobs is to optimize seeker engagement. We accomplished this by training embeddings for personalized retrieval, fortified by a set of rules derived from the categorization of member feedback. In addition to a solution based on a conventional inverted index, we developed an on-GPU solution capable of supporting both KNN and term matching efficiently.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13475",
        "abstract url": "https://arxiv.org/abs/2402.13475",
        "title": "Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images",
        "rating": -3,
        "keywords": [
            [
                "MRI",
                "disease"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals. Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease. It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future. However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches. To this end, we introduce the Multi-scale Spatio-temporal Transformer Network (MST-former) based on the transformer architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions. Specifically, we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image. Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data. Furthermore, we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue. Extensive experiments on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset demonstrate the superiority of the proposed MST-former method, achieving an AUC of 98.6% for glaucoma forecasting. Besides, our method shows excellent generalization capability on the Alzheimer's Disease Neuroimaging Initiative (ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and Alzheimer's disease prediction, outperforming the compared method by a large margin.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 7 figures"
    },
    {
        "paper id": "2402.13506",
        "abstract url": "https://arxiv.org/abs/2402.13506",
        "title": "Towards Efficient Verification of Constant-Time Cryptographic Implementations",
        "rating": -3,
        "keywords": [
            [
                "attacks"
            ],
            [
                "CT"
            ]
        ],
        "abstract": "Timing side-channel attacks exploit secret-dependent execution time to fully or partially recover secrets of cryptographic implementations, posing a severe threat to software security. Constant-time programming discipline is an effective software-based countermeasure against timing side-channel attacks, but developing constant-time implementations turns out to be challenging and error-prone. Current verification approaches/tools suffer from scalability and precision issues when applied to production software in practice. In this paper, we put forward practical verification approaches based on a novel synergy of taint analysis and safety verification of self-composed programs. Specifically, we first use an IFDS-based lightweight taint analysis to prove that a large number of potential (timing) side-channel sources do not actually leak secrets. We then resort to a precise taint analysis and a safety verification approach to determine whether the remaining potential side-channel sources can actually leak secrets. These include novel constructions of taint-directed semi-cross-product of the original program and its Boolean abstraction, and a taint-directed self-composition of the program. Our approach is implemented as a cross-platform and fully automated tool CT-Prover. The experiments confirm its efficiency and effectiveness in verifying real-world benchmarks from modern cryptographic and SSL/TLS libraries. In particular, CT-Prover identify new, confirmed vulnerabilities of open-source SSL libraries (e.g., Mbed SSL, BearSSL) and significantly outperforms the state-of-the-art tools.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted by ACM FSE 2024"
    },
    {
        "paper id": "2402.13523",
        "abstract url": "https://arxiv.org/abs/2402.13523",
        "title": "Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "EEG",
                "Disease"
            ]
        ],
        "abstract": "The prospect of future treatment warrants the development of cost-effective screening for Alzheimer's disease (AD). A promising candidate in this regard is electroencephalography (EEG), as it is one of the most economic imaging modalities. Recent efforts in EEG analysis have shifted towards leveraging spatial information, employing novel frameworks such as graph signal processing or graph neural networks. Here, we investigate the importance of spatial information relative to spectral or temporal information by varying the proportion of each dimension for AD classification. To do so, we systematically test various dimension resolution configurations on two routine EEG datasets. Our findings show that spatial information is more important than temporal information and equally valuable as spectral information. On the larger second dataset, substituting spectral with spatial information even led to an increase of 1.1% in accuracy, which emphasises the importance of spatial information for EEG-based AD classification. We argue that our resolution-based feature extraction has the potential to improve AD classification specifically, and multivariate signal classification generally.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "4 pages, 3 figures, conference paper"
    },
    {
        "paper id": "2403.00834",
        "abstract url": "https://arxiv.org/abs/2403.00834",
        "title": "Virtual Reality for Understanding Artificial-Intelligence-driven Scientific Discovery with an Application in Quantum Optics",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Generative Artificial Intelligence (AI) models can propose solutions to scientific problems beyond human capability. To truly make conceptual contributions, researchers need to be capable of understanding the AI-generated structures and extracting the underlying concepts and ideas. When algorithms provide little explanatory reasoning alongside the output, scientists have to reverse-engineer the fundamental insights behind proposals based solely on examples. This task can be challenging as the output is often highly complex and thus not immediately accessible to humans. In this work we show how transferring part of the analysis process into an immersive Virtual Reality (VR) environment can assist researchers in developing an understanding of AI-generated solutions. We demonstrate the usefulness of VR in finding interpretable configurations of abstract graphs, representing Quantum Optics experiments. Thereby, we can manually discover new generalizations of AI-discoveries as well as new understanding in experimental quantum optics. Furthermore, it allows us to customize the search space in an informed way - as a human-in-the-loop - to achieve significantly faster subsequent discovery iterations. As concrete examples, with this technology, we discover a new resource-efficient 3-dimensional entanglement swapping scheme, as well as a 3-dimensional 4-particle Greenberger-Horne-Zeilinger-state analyzer. Our results show the potential of VR for increasing a human researcher's ability to derive knowledge from graph-based generative AI that, which is a common abstract data representation used in diverse fields of science.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "12 pages, 6 figures, comments welcome"
    },
    {
        "paper id": "2403.05562",
        "abstract url": "https://arxiv.org/abs/2403.05562",
        "title": "SDXL Finetuned with LoRA for Coloring Therapy: Generating Graphic Templates Inspired by United Arab Emirates Culture",
        "rating": -3,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "biosignals",
                "health",
                "healthcare"
            ]
        ],
        "abstract": "A transformative approach to mental health therapy lies at the crossroads of cultural heritage and advanced technology. This paper introduces an innovative method that fuses machine learning techniques with traditional Emirati motifs, focusing on the United Arab Emirates (UAE). We utilize the Stable Diffusion XL (SDXL) model, enhanced with Low-Rank Adaptation (LoRA), to create culturally significant coloring templates featuring Al-Sadu weaving patterns. This novel approach leverages coloring therapy for its recognized stress-relieving benefits and embeds deep cultural resonance, making it a potent tool for therapeutic intervention and cultural preservation. Specifically targeting Generalized Anxiety Disorder (GAD), our method demonstrates significant potential in reducing associated symptoms. Additionally, the paper delves into the broader implications of color and music therapy, emphasizing the importance of culturally tailored content. The technical aspects of the SDXL model and its LoRA fine-tuning showcase its capability to generate high-quality, culturally specific images. This research stands at the forefront of integrating mental wellness practices with cultural heritage, providing a groundbreaking perspective on the synergy between technology, culture, and healthcare. In future work, we aim to employ biosignals to assess the level of engagement and effectiveness of color therapy. A key focus will be to examine the impact of the Emirati heritage Al Sadu art on Emirati individuals and compare their responses with those of other nationalities. This will provide deeper insights into the cultural specificity of therapeutic interventions and further the understanding of the unique interplay between cultural identity and mental health therapy.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.15405",
        "abstract url": "https://arxiv.org/abs/2403.15405",
        "title": "Predicting Parkinson's disease trajectory using clinical and functional MRI features: a reproduction and replication study",
        "rating": -3,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "biomarkers",
                "diagnosis",
                "MRI",
                "disease",
                "clinical"
            ]
        ],
        "abstract": "Parkinson's disease (PD) is a common neurodegenerative disorder with a poorly understood physiopathology. In clinical practice, challenges are encountered in the diagnosis of early stages and in the prediction of the disease progression due to the absence of established biomarkers. Several biomarkers obtained using neuroimaging techniques such as functional Magnetic Resonance Imaging (fMRI) have been studied recently. However, the reliability and generalizability of neuroimaging-based measurements are susceptible to several different sources of variability, including those introduced by different analysis methods or population sampling. In this context, an evaluation of the robustness of such biomarkers is essential. This study is part of a larger project investigating the replicability of potential neuroimaging biomarkers of PD. Here, we attempt to reproduce (same data, same method) and replicate (different data or method) the models described in Nguyen et al. 2021 to predict individual's PD current state and progression using demographic, clinical and neuroimaging features (fALFF and ReHo extracted from resting-state fMRI). We used the Parkinson's Progression Markers Initiative dataset (PPMI, ppmi-info.org), as in Nguyen et al. 2021 and tried to reproduce the original cohort, imaging features and machine learning models as closely as possible using the information available in the paper and the code. We also investigated methodological variations in cohort selection, feature extraction pipelines and sets of input features. Using the reproduction workflow, we managed to obtain better than chance performance for all our models (R2>0), but this performance remained very different from the ones reported in the original study. The challenges encountered while reproducing and replicating the original work are likely explained by the complexity of neuroimaging studies, in particular in clinical settings. We provide recommendations to facilitate the reproducibility of such studies in the future, for instance with the use of version control tools, standardization of pipelines and publication of analysis code and derived data.",
        "subjects": [
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13304",
        "abstract url": "https://arxiv.org/abs/2402.13304",
        "title": "Harmful algal bloom forecasting. A comparison between stream and batch learning",
        "rating": -3.5,
        "keywords": [
            [
                "biomass",
                "health"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates. The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry. High biomass of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs). Monitoring and forecasting systems are crucial for mitigating HABs impact. Predicting harmful algal blooms involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed. Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts. However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning. Historical data availability is a critical point in developing predictive systems. In oceanography, the available data collection can have some constrains and limitations, which has led to exploring new tools to obtain more exhaustive time series. In this study, a machine learning workflow for predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata, was developed with several key advancements. Seven machine learning algorithms were compared within two learning paradigms. Notably, the output data from CROCO, the ocean hydrodynamic model, was employed as the primary dataset, palliating the limitation of time-continuous historical data. This study highlights the value of models interpretability, fair models comparison methodology, and the incorporation of Stream Learning models. The model DoME, with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most effective and interpretable predictor, outperforming the other algorithms.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13371",
        "abstract url": "https://arxiv.org/abs/2402.13371",
        "title": "FIDLAR: Forecast-Informed Deep Learning Architecture for Flood Mitigation",
        "rating": -3.5,
        "keywords": [
            [
                "Forecast"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In coastal river systems, frequent floods, often occurring during major storms or king tides, pose a severe threat to lives and property. However, these floods can be mitigated or even prevented by strategically releasing water before extreme weather events with hydraulic structures such as dams, gates, pumps, and reservoirs. A standard approach used by local water management agencies is the \"rule-based\" method, which specifies predetermined pre-releases of water based on historical and time-tested human experience, but which tends to result in excess or inadequate water release. The model predictive control (MPC), a physics-based model for prediction, is an alternative approach, albeit involving computationally intensive calculations. In this paper, we propose a Forecast Informed Deep Learning Architecture, FIDLAR, to achieve rapid and optimal flood management with precise water pre-releases. FIDLAR seamlessly integrates two neural network modules: one called the Flood Manager, which is responsible for generating water pre-release schedules, and another called the Flood Evaluator, which assesses these generated schedules. The Evaluator module is pre-trained separately, and its gradient-based feedback is used to train the Manager model, ensuring optimal water pre-releases. We have conducted experiments using FIDLAR with data from a flood-prone coastal area in South Florida, particularly susceptible to frequent storms. Results show that FIDLAR is several orders of magnitude faster than currently used physics-based approaches while outperforming baseline methods with improved water pre-release schedules. Our code is at https://github.com/JimengShi/FIDLAR/.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13410",
        "abstract url": "https://arxiv.org/abs/2402.13410",
        "title": "Bayesian Neural Networks with Domain Knowledge Priors",
        "rating": -3.5,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, physics rules, and healthcare knowledge and achieving better predictive performance. We also present techniques for transferring the learned priors across different model architectures, demonstrating their broad utility across various settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "17 pages, 4 figures"
    },
    {
        "paper id": "2402.13427",
        "abstract url": "https://arxiv.org/abs/2402.13427",
        "title": "Quantitative causality, causality-guided scientific discovery, and causal machine learning",
        "rating": -3.5,
        "keywords": [
            [
                "forecasting"
            ],
            [
                "quantum"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "It has been said, arguably, that causality analysis should pave a promising way to interpretable deep learning and generalization. Incorporation of causality into artificial intelligence (AI) algorithms, however, is challenged with its vagueness, non-quantitiveness, computational inefficiency, etc. During the past 18 years, these challenges have been essentially resolved, with the establishment of a rigorous formalism of causality analysis initially motivated from atmospheric predictability. This not only opens a new field in the atmosphere-ocean science, namely, information flow, but also has led to scientific discoveries in other disciplines, such as quantum mechanics, neuroscience, financial economics, etc., through various applications. This note provides a brief review of the decade-long effort, including a list of major theoretical results, a sketch of the causal deep learning framework, and some representative real-world applications in geoscience pertaining to this journal, such as those on the anthropogenic cause of global warming, the decadal prediction of El Ni\u00f1o Modoki, the forecasting of an extreme drought in China, among others.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "10 pages, 3 figures. To appear in Ocean-Land-Atmosphere Research. arXiv admin note: substantial text overlap with arXiv:2112.14839"
    },
    {
        "paper id": "2402.14861",
        "abstract url": "https://arxiv.org/abs/2402.14861",
        "title": "CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks",
        "rating": -3.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "Graph"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The impact of meteorological observations on weather forecasting varies with sensor type, location, time, and other environmental factors. Thus, quantitative analysis of observation impacts is crucial for effective and efficient development of weather forecasting systems. However, the existing impact analysis methods are difficult to be widely applied due to their high dependencies on specific forecasting systems. Also, they cannot provide observation impacts at multiple spatio-temporal scales, only global impacts of observation types. To address these issues, we present a novel system called ``CloudNine,'' which allows analysis of individual observations' impacts on specific predictions based on explainable graph neural networks (XGNNs). Combining an XGNN-based atmospheric state estimation model with a numerical weather prediction model, we provide a web application to search for observations in the 3D space of the Earth system and to visualize the impact of individual observations on predictions in specific spatial regions and time periods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12777",
        "abstract url": "https://arxiv.org/abs/2402.12777",
        "title": "Application of Quantum Extreme Learning Machines for QoS Prediction of Elevators' Software in an Industrial Context",
        "rating": -4,
        "keywords": [
            [
                "Industrial"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum Extreme Learning Machine (QELM) is an emerging technique that utilizes quantum dynamics and an easy-training strategy to solve problems such as classification and regression efficiently. Although QELM has many potential benefits, its real-world applications remain limited. To this end, we present QELM's industrial application in the context of elevators, by proposing an approach called QUELL. In QUELL, we use QELM for the waiting time prediction related to the scheduling software of elevators, with applications for software regression testing, elevator digital twins, and real-time performance prediction. The scheduling software has been implemented by our industrial partner Orona, a globally recognized leader in elevator technology. We demonstrate that QUELL can efficiently predict waiting times, with prediction quality significantly better than that of classical ML models employed in a state-of-the-practice approach. Moreover, we show that the prediction quality of QUELL does not degrade when using fewer features. Based on our industrial application, we further provide insights into using QELM in other applications in Orona, and discuss how QELM could be applied to other industrial applications.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12993",
        "abstract url": "https://arxiv.org/abs/2402.12993",
        "title": "An Autonomous Large Language Model Agent for Chemical Literature Data Mining",
        "rating": -4,
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "synthesis"
            ],
            [
                "healthcare"
            ],
            [
                "chemistry",
                "Chemical"
            ]
        ],
        "abstract": "Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency. The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13038",
        "abstract url": "https://arxiv.org/abs/2402.13038",
        "title": "N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images",
        "rating": -4,
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "trajectory"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source along with the training images.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2024"
    },
    {
        "paper id": "2402.13056",
        "abstract url": "https://arxiv.org/abs/2402.13056",
        "title": "Edge Computing for IoT",
        "rating": -4,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "5G",
                "industrial",
                "IoT"
            ]
        ],
        "abstract": "Over the past few years, The idea of edge computing has seen substantial expansion in both academic and industrial circles. This computing approach has garnered attention due to its integrating role in advancing various state-of-the-art technologies such as Internet of Things (IoT) , 5G, artificial intelligence, and augmented reality. In this chapter, we introduce computing paradigms for IoT, offering an overview of the current cutting-edge computing approaches that can be used with IoT. Furthermore, we go deeper into edge computing paradigms, specifically focusing on cloudlet and mobile edge computing. After that, we investigate the architecture of edge computing-based IoT, its advantages, and the technologies that make Edge computing-based IoT possible, including artificial intelligence and lightweight virtualization. Additionally, we review real-life case studies of how edge computing is applied in IoT-based Intelligent Systems, including areas like healthcare, manufacturing, agriculture, and transportation. Finally, we discuss current research obstacles and outline potential future directions for further investigation in this domain.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "19 pages, 5 figures, Book Chapter In: Donta, P.K., Hazra, A., Lov\u00e9n, L. (eds) Learning Techniques for the Internet of Things. Springer, Cham"
    },
    {
        "paper id": "2402.13065",
        "abstract url": "https://arxiv.org/abs/2402.13065",
        "title": "Scalable Pattern Matching in Computation Graphs",
        "rating": -4,
        "keywords": [
            [
                "depth"
            ],
            [
                "Graph"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "Graph rewriting is a popular tool for the optimisation and modification of graph expressions in domains such as compilers, machine learning and quantum computing. The underlying data structures are often port graphs - graphs with labels at edge endpoints. These port labels greatly simplify pattern matching. A pre-requisite for graph rewriting is the ability to find subgraphs of the input that match known graph identities: the pattern matching problem. We propose a new solution to pattern matching in port graphs. Its novelty lies in the use of a pre-computed data structure that makes the pattern matching runtime complexity independent of the number of patterns. The runtime is bound by the maximum width $w$ and depth $d$ of the patterns, as well as the input graph size $|G|$ as $O(|G| \\cdot c^w / w^{1/2} \\cdot d)$ with $c = 6.75$. This offers a significant advantage over existing solutions for use cases where patterns have low width and the set of patterns is large and fixed ahead of time. In the context of quantum circuits, pattern width can be limited to qubit number. Quantum superoptimisers may use thousands of rewrite rules on circuits with less than 5 qubits, making them an ideal use case. We provide benchmarks showing that our algorithm offers a 20x speedup over current implementations on a dataset of 10'000 real world patterns describing quantum circuits.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13226",
        "abstract url": "https://arxiv.org/abs/2402.13226",
        "title": "NeRF Solves Undersampled MRI Reconstruction",
        "rating": -4,
        "keywords": [
            [
                "NeRF"
            ],
            [
                "MRI",
                "clinical"
            ],
            [
                "physics"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "This article presents a novel undersampled magnetic resonance imaging (MRI) technique that leverages the concept of Neural Radiance Field (NeRF). With radial undersampling, the corresponding imaging problem can be reformulated into an image modeling task from sparse-view rendered data; therefore, a high dimensional MR image is obtainable from undersampled k-space data by taking advantage of implicit neural representation. A multi-layer perceptron, which is designed to output an image intensity from a spatial coordinate, learns the MR physics-driven rendering relation between given measurement data and desired image. Effective undersampling strategies for high-quality neural representation are investigated. The proposed method serves two benefits: (i) The learning is based fully on single undersampled k-space data, not a bunch of measured data and target image sets. It can be used potentially for diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively rare or limited against diversity of clinical images while undersampled reconstruction is highly demanded. (ii) A reconstructed MR image is a scan-specific representation highly adaptive to the given k-space measurement. Numerous experiments validate the feasibility and capability of the proposed approach.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13466",
        "abstract url": "https://arxiv.org/abs/2402.13466",
        "title": "Leveraging Demonstrator-perceived Precision for Safe Interactive Imitation Learning of Clearance-limited Tasks",
        "rating": -4,
        "keywords": [
            [
                "6-DOF"
            ],
            [
                "robot"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "Interactive imitation learning is an efficient, model-free method through which a robot can learn a task by repetitively iterating an execution of a learning policy and a data collection by querying human demonstrations. However, deploying unmatured policies for clearance-limited tasks, like industrial insertion, poses significant collision risks. For such tasks, a robot should detect the collision risks and request intervention by ceding control to a human when collisions are imminent. The former requires an accurate model of the environment, a need that significantly limits the scope of IIL applications. In contrast, humans implicitly demonstrate environmental precision by adjusting their behavior to avoid collisions when performing tasks. Inspired by human behavior, this paper presents a novel interactive learning method that uses demonstrator-perceived precision as a criterion for human intervention called Demonstrator-perceived Precision-aware Interactive Imitation Learning (DPIIL). DPIIL captures precision by observing the speed-accuracy trade-off exhibited in human demonstrations and cedes control to a human to avoid collisions in states where high precision is estimated. DPIIL improves the safety of interactive policy learning and ensures efficiency without explicitly providing precise information of the environment. We assessed DPIIL's effectiveness through simulations and real-robot experiments that trained a UR5e 6-DOF robotic arm to perform assembly tasks. Our results significantly improved training safety, and our best performance compared favorably with other learning methods.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 5 figures, accepted by IEEE Robotics and Automation Letters (RA-L) 2024"
    },
    {
        "paper id": "2402.13469",
        "abstract url": "https://arxiv.org/abs/2402.13469",
        "title": "The Quantum Abstract Machine",
        "rating": -4,
        "keywords": [
            [
                "Chemical"
            ],
            [
                "Quantum",
                "physics"
            ]
        ],
        "abstract": "This paper develops a model of quantum behavior that is intended to support the abstract yet accurate design and functional verification of quantum communication protocols. The work is motivated by the need for conceptual tools for the development of quantum-communication systems that are usable by non-specialists in quantum physics while also correctly capturing at a useful abstraction the underlying quantum phenomena. Our approach involves defining a quantum abstract machine (QAM) whose operations correspond to well-known quantum circuits; these operations, however, are given direct abstract semantics in a style similar to that of Berry's and Boudol's Chemical Abstract Machine. This paper defines the QAM's semantics and shows via examples how it may be used to model and reason about existing quantum communication protocols.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2404.07213",
        "abstract url": "https://arxiv.org/abs/2404.07213",
        "title": "Evolving Genetic Programming Tree Models for Predicting the Mechanical Properties of Green Fibers for Better Biocomposite Materials",
        "rating": -4,
        "keywords": [
            [
                "Biocomposite"
            ],
            [
                "industrial",
                "chemical"
            ]
        ],
        "abstract": "Advanced modern technology and industrial sustainability theme have contributed implementing composite materials for various industrial applications. Green composites are among the desired alternatives for the green products. However, to properly control the performance of the green composites, predicting their constituents properties are of paramount importance. This work presents an innovative evolving genetic programming tree models for predicting the mechanical properties of natural fibers based upon several inherent chemical and physical properties. Cellulose, hemicellulose, lignin and moisture contents as well as the Microfibrillar angle of various natural fibers were considered to establish the prediction models. A one-hold-out methodology was applied for training/testing phases. Robust models were developed to predict the tensile strength, Young's modulus, and the elongation at break properties of the natural fibers. It was revealed that Microfibrillar angle was dominant and capable of determining the ultimate tensile strength of the natural fibers by 44.7% comparable to other considered properties, while the impact of cellulose content in the model was only 35.6%. This in order would facilitate utilizing artificial intelligence in predicting the overall mechanical properties of natural fibers without experimental efforts and cost to enhance developing better green composite materials for various industrial applications.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.15282",
        "abstract url": "https://arxiv.org/abs/2404.15282",
        "title": "Patent Value Characterization -- An Empirical Analysis of Elevator Industry Patents",
        "rating": -4,
        "keywords": [
            [
                "biotechnology",
                "healthcare"
            ],
            [
                "Patent"
            ]
        ],
        "abstract": "The global patent application count has steadily increased, achieving eight consecutive years of growth.The global patent industry has shown a general trend of expansion. This is attributed to the increasing innovation activities, particularly in the fields of technology, healthcare, and biotechnology. Some emerging market countries, such as China and India, have experienced significant growth in the patent domain, becoming important participants in global patent activities.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12794",
        "abstract url": "https://arxiv.org/abs/2402.12794",
        "title": "Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles",
        "rating": -5,
        "keywords": [
            [
                "3D"
            ],
            [
                "biomimetic"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH). In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site. To minimize human intervention, this paper introduces a novel methodology for autonomous 3D Reality Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors. These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach. The outcomes of this automated process may find applications in digital twin platforms, facilitating secure monitoring and management of cultural heritage sites and spaces, in both indoor and outdoor environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12923",
        "abstract url": "https://arxiv.org/abs/2402.12923",
        "title": "Advancements in Point Cloud-Based 3D Defect Detection and Classification for Industrial Systems: A Comprehensive Survey",
        "rating": -5,
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "synthesis"
            ],
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "Industrial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, 3D point clouds (PCs) have gained significant attention due to their diverse applications across various fields such as computer vision (CV), condition monitoring, virtual reality, robotics, autonomous driving etc. Deep learning (DL) has proven effective in leveraging 3D PCs to address various challenges previously encountered in 2D vision. However, the application of deep neural networks (DNN) to process 3D PCs presents its own set of challenges. To address these challenges, numerous methods have been proposed. This paper provides an in-depth review of recent advancements in DL-based condition monitoring (CM) using 3D PCs, with a specific focus on defect shape classification and segmentation within industrial applications for operational and maintenance purposes. Recognizing the crucial role of these aspects in industrial maintenance, the paper provides insightful observations that offer perspectives on the strengths and limitations of the reviewed DL-based PC processing methods. This synthesis of knowledge aims to contribute to the understanding and enhancement of CM processes, particularly within the framework of remaining useful life (RUL), in industrial systems.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "31 pages, 9 figures, 3 tables, review paper"
    },
    {
        "paper id": "2402.12785",
        "abstract url": "https://arxiv.org/abs/2402.12785",
        "title": "Stochastic Graph Heat Modelling for Diffusion-based Connectivity Retrieval",
        "rating": -6,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Graph"
            ],
            [
                "disease"
            ],
            [
                "thermal"
            ]
        ],
        "abstract": "Heat diffusion describes the process by which heat flows from areas with higher temperatures to ones with lower temperatures. This concept was previously adapted to graph structures, whereby heat flows between nodes of a graph depending on the graph topology. Here, we combine the graph heat equation with the stochastic heat equation, which ultimately yields a model for multivariate time signals on a graph. We show theoretically how the model can be used to directly compute the diffusion-based connectivity structure from multivariate signals. Unlike other connectivity measures, our heat model-based approach is inherently multivariate and yields an absolute scaling factor, namely the graph thermal diffusivity, which captures the extent of heat-like graph propagation in the data. On two datasets, we show how the graph thermal diffusivity can be used to characterise Alzheimer's disease (AD). We find that the graph thermal diffusivity is lower for AD patients than healthy controls and correlates with mini mental state examination (MMSE) scores, suggesting structural impairment in patients in line with previous findings.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "4 pages, 1 figure, conference paper"
    },
    {
        "paper id": "2402.12718",
        "abstract url": "https://arxiv.org/abs/2402.12718",
        "title": "Fostering Joint Innovation: A Global Online Platform for Ideas Sharing and Collaboration",
        "rating": -10,
        "keywords": [],
        "abstract": "In today's world, where moving forward hinges on innovation and working together, this article introduces a new global online platform that is all about sparking teamwork to come up with new ideas. This platform goes beyond borders and barriers between different fields, creating an exciting space where people from all over the world can swap ideas, get helpful feedback, and team up on exciting projects. What sets our platform apart is its ability to tap into the combined brainpower of a diverse bunch of users, giving people the power to come up with game-changing ideas that tackle big global problems. By making it easy for people to share ideas and promoting a culture of working together, our platform is like a buddy for innovation, boosting creativity and problem-solving on a global level. This article spills the details on what the platform aims to do, how it works, and what makes it special, emphasizing how it can kickstart creativity, ramp up problem-solving skills, and get different fields collaborating. It is not just a tool it is a whole new way of teaming up to make daily life better and build a global community of problem-solving pals.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "5 pages, 5 figures, ITNG 2024 21st International Conference on Information Technology. Las Vegas, Nevada, USA"
    },
    {
        "paper id": "2402.12734",
        "abstract url": "https://arxiv.org/abs/2402.12734",
        "title": "A Lower Bound on the Competitive Ratio of the Permutation Algorithm for Online Facility Assignment on a Line",
        "rating": -10,
        "keywords": [],
        "abstract": "In the online facility assignment on a line (OFAL) with a set $S$ of $k$ servers and a capacity $c:S\\to\\mathbb{N}$, each server $s\\in S$ with a capacity $c(s)$ is placed on a line and a request arrives on a line one-by-one. The task of an online algorithm is to irrevocably assign a current request to one of the servers with vacancies before the next request arrives. An algorithm can assign up to $c(s)$ requests to each server $s\\in S$. In this paper, we show that the competitive ratio of the permutation algorithm is at least $k+1$ for OFAL where the servers are evenly placed on a line. This disproves the result that the permutation algorithm is $k$-competitive by Ahmed et al..",
        "subjects": [
            "cs.DS"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2402.12742",
        "abstract url": "https://arxiv.org/abs/2402.12742",
        "title": "DDC: A Vision for a Disaggregated Datacenter",
        "rating": -10,
        "keywords": [],
        "abstract": "Datacenters of today have maintained the same architecture for decades using the server as the primary building block. However, this traditional approach suffers from under-utilization of its resources, often caused by over-allocating these resources when deploying applications to accommodate worst-case scenarios. Specifically, servers can quickly drain their over-allocated memory resources while their CPUs are not fully utilized. This problem gives rise to a different school of thought, where resources are disaggregated instead of tightly bound to servers. This can address the utilization problem by allowing each type of resource to be allocated, utilized and freed separately as required. New high performance communication protocols, like CXL, could pave the way for practical implementations of resource disaggregation. In this article, we argue it is time to reconsider the datacenter architecture as a whole. We present our vision for a disaggregated datacenter aided by well-established computer architecture design methodologies.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12762",
        "abstract url": "https://arxiv.org/abs/2402.12762",
        "title": "Learning under Singularity: An Information Criterion improving WBIC and sBIC",
        "rating": -10,
        "keywords": [],
        "abstract": "We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC). LS is effective without regularity constraints and demonstrates stability. Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite. In contrast, models not meeting these conditions are termed singular. Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients. Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood estimates. LS addresses these limitations by enhancing the utility of both WBIC and sBIC. It incorporates the empirical loss from the Widely Applicable Information Criterion (WAIC) to represent the goodness of fit to the statistical model, along with a penalty term similar to that of sBIC. This approach offers a flexible and robust method for model selection, free from regularity constraints.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12772",
        "abstract url": "https://arxiv.org/abs/2402.12772",
        "title": "GazePrompt: Enhancing Low Vision People's Reading Experience with Gaze-Aware Augmentations",
        "rating": -10,
        "keywords": [],
        "abstract": "Reading is a challenging task for low vision people. While conventional low vision aids (e.g., magnification) offer certain support, they cannot fully address the difficulties faced by low vision users, such as locating the next line and distinguishing similar words. To fill this gap, we present GazePrompt, a gaze-aware reading aid that provides timely and targeted visual and audio augmentations based on users' gaze behaviors. GazePrompt includes two key features: (1) a Line-Switching support that highlights the line a reader intends to read; and (2) a Difficult-Word support that magnifies or reads aloud a word that the reader hesitates with. Through a study with 13 low vision participants who performed well-controlled reading-aloud tasks with and without GazePrompt, we found that GazePrompt significantly reduced participants' line switching time, reduced word recognition errors, and improved their subjective reading experiences. A follow-up silent-reading study showed that GazePrompt can enhance users' concentration and perceived comprehension of the reading contents. We further derive design considerations for future gaze-based low vision aids.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12774",
        "abstract url": "https://arxiv.org/abs/2402.12774",
        "title": "Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding",
        "rating": -10,
        "keywords": [],
        "abstract": "Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents CONVINV, a simple yet effective approach to shed light on interpretable conversational dense retrieval models. CONVINV transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval. To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensive evaluations on three conversational search benchmarks demonstrate that CONVINV can yield more interpretable text and faithfully preserve original retrieval performance than baselines. Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12782",
        "abstract url": "https://arxiv.org/abs/2402.12782",
        "title": "Advancing GenAI Assisted Programming--A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4",
        "rating": -10,
        "keywords": [],
        "abstract": "This study aims to explore the best practices for utilizing GenAI as a programming tool, through a comparative analysis between GPT-4 and GLM-4. By evaluating prompting strategies at different levels of complexity, we identify that simplest and straightforward prompting strategy yields best code generation results. Additionally, adding a CoT-like preliminary confirmation step would further increase the success rate. Our results reveal that while GPT-4 marginally outperforms GLM-4, the difference is minimal for average users. In our simplified evaluation model, we see a remarkable 30 to 100-fold increase in code generation efficiency over traditional coding norms. Our GenAI Coding Workshop highlights the effectiveness and accessibility of the prompting methodology developed in this study. We observe that GenAI-assisted coding would trigger a paradigm shift in programming landscape, which necessitates developers to take on new roles revolving around supervising and guiding GenAI, and to focus more on setting high-level objectives and engaging more towards innovation.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "18 pages, 4 figures, 3 tables"
    },
    {
        "paper id": "2402.12804",
        "abstract url": "https://arxiv.org/abs/2402.12804",
        "title": "Modular Assurance of Complex Systems Using Contract-Based Design Principles",
        "rating": -10,
        "keywords": [],
        "abstract": "A growing number of safety-critical industries agree that building confidence in complex systems can be achieved through evidence and structured argumentation framed in assurance cases. Nevertheless, assurance cases can easily become too rigorous and difficult to develop and maintain when applied to complex systems. Therefore, we propose to use contract-based development (CBD), a method to manage complexity originally developed in computer science, to simplify assurance cases by modularizing them. This paper will not only summarize relevant previous work such as constructing consistent modular assurance cases using CBD, but more importantly also propose a novel approach to integrate CBD with the argumentation in assurance case modules. This approach will allow interdisciplinary subject-matter and domain experts to build assurance cases together without even knowing about CBD. This helps subject matter experts outside of computer science to reap benefits from CBD and helps with interdisciplinary co-development of assurance cases that cover all the required fields. This paper motivates four rules of thumb aimed to help practitioners developing high-quality modular assurance cases. It also explains how modularization of assurance is an enabler for multi-concern assurance that accounts for the inter-dependency of different concerns such as safety, security and performance.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "14 pages, 6 figures, manuscript submitted to the 43rd International Conference in Computer Safety, Reliability, and Security (SAFECOMP 2024)"
    },
    {
        "paper id": "2402.12813",
        "abstract url": "https://arxiv.org/abs/2402.12813",
        "title": "Scaling Laws Behind Code Understanding Model",
        "rating": -10,
        "keywords": [],
        "abstract": "The scaling law is becoming a fundamental law in many machine learning areas. That is, test error falls off with the power law when increasing training data, model size, and computing resource. However, whether this law is suitable for the task of code understanding is not well studied, and most current language models for code understanding are about 100M parameters, which are relatively \"small\" compared to large language models. In this paper, we conduct extensive experiments to investigate the scaling law for the code understanding task by varying training data, model size, and computing resource. We validate that the test error of code understanding models falls off with the power law when using larger models, indicating that the scaling law is suitable for the code understanding task. Besides, we apply different scales of models to two downstream code understanding tasks, and find that the performance increases with larger scale of models. Finally, we train a large-scale code understanding model named CoLSBERT with 1.5B parameters on a large dataset using more computing resource, which outperforms previous work by a large margin. We will release our code and the CoLSBERT model when our paper is published.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12814",
        "abstract url": "https://arxiv.org/abs/2402.12814",
        "title": "Exploring the Impact of AI Value Alignment in Collaborative Ideation: Effects on Perception, Ownership, and Output",
        "rating": -10,
        "keywords": [],
        "abstract": "AI-based virtual assistants are increasingly used to support daily ideation tasks. The values or bias present in these agents can influence output in hidden ways. They may also affect how people perceive the ideas produced with these AI agents and lead to implications for the design of AI-based tools. We explored the effects of AI agents with different values on the ideation process and user perception of idea quality, ownership, agent competence, and values present in the output. Our study tasked 180 participants with brainstorming practical solutions to a set of problems with AI agents of different values. Results show no significant difference in self-evaluation of idea quality and perception of the agent based on value alignment; however, ideas generated reflected the AI's values and feeling of ownership is affected. This highlights an intricate interplay between AI values and human ideation, suggesting careful design considerations for future AI-supported brainstorming tools.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12820",
        "abstract url": "https://arxiv.org/abs/2402.12820",
        "title": "ASCEND: Accurate yet Efficient End-to-End Stochastic Computing Acceleration of Vision Transformer",
        "rating": -10,
        "keywords": [],
        "abstract": "Stochastic computing (SC) has emerged as a promising computing paradigm for neural acceleration. However, how to accelerate the state-of-the-art Vision Transformer (ViT) with SC remains unclear. Unlike convolutional neural networks, ViTs introduce notable compatibility and efficiency challenges because of their nonlinear functions, e.g., softmax and Gaussian Error Linear Units (GELU). In this paper, for the first time, a ViT accelerator based on end-to-end SC, dubbed ASCEND, is proposed. ASCEND co-designs the SC circuits and ViT networks to enable accurate yet efficient acceleration. To overcome the compatibility challenges, ASCEND proposes a novel deterministic SC block for GELU and leverages an SC-friendly iterative approximate algorithm to design an accurate and efficient softmax circuit. To improve inference efficiency, ASCEND develops a two-stage training pipeline to produce accurate low-precision ViTs. With extensive experiments, we show the proposed GELU and softmax blocks achieve 56.3% and 22.6% error reduction compared to existing SC designs, respectively and reduce the area-delay product (ADP) by 5.29x and 12.6x, respectively. Moreover, compared to the baseline low-precision ViTs, ASCEND also achieves significant accuracy improvements on CIFAR10 and CIFAR100.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted in DATE 2024"
    },
    {
        "paper id": "2402.12828",
        "abstract url": "https://arxiv.org/abs/2402.12828",
        "title": "SGD with Clipping is Secretly Estimating the Median Gradient",
        "rating": -10,
        "keywords": [],
        "abstract": "There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study SGD with robust gradient estimators based on estimating the median. We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12863",
        "abstract url": "https://arxiv.org/abs/2402.12863",
        "title": "Finding Cross-rule Optimization Bugs in Datalog Engines",
        "rating": -10,
        "keywords": [],
        "abstract": "Datalog is a popular and widely-used declarative logic programming language. Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results. To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem. The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine. Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules. Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated-we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts. We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely Souffl\u00e9, CozoDB, $\u03bc$Z, and DDlog, and discovered a total of 30 bugs. Of these, 13 were logic bugs, while the remaining were crash and error bugs. Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach. Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5. Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17$\\times$ (for DDlog) to 31.02$\\times$ (for Souffl\u00e9) as many valid test cases with non-empty results as the naive random method. We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "The ACM SIGPLAN Conference on Object Oriented Programming, Systems, Languages, and Applications (2024), Pasadena, California, United States"
    },
    {
        "paper id": "2402.12864",
        "abstract url": "https://arxiv.org/abs/2402.12864",
        "title": "A Novel Protocol Using Captive Portals for FIDO2 Network Authentication",
        "rating": -10,
        "keywords": [],
        "abstract": "FIDO2 authentication is starting to be applied in numerous web authentication services, aiming to replace passwords and their known vulnerabilities. However, this new authentication method has not been integrated yet with network authentication systems. In this paper, we introduce FIDO2CAP: FIDO2 Captive-portal Authentication Protocol. Our proposal describes a novel protocol for captive-portal network authentication using FIDO2 authenticators, as security keys and passkeys. For validating our proposal, we have developed a prototype of FIDO2CAP authentication in a mock scenario. Using this prototype, we performed an usability experiment with 15 real users. This work makes the first systematic approach for adapting network authentication to the new authentication paradigm relying on FIDO2 authentication.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "This is an author version. It has not been peer reviewed"
    },
    {
        "paper id": "2402.12882",
        "abstract url": "https://arxiv.org/abs/2402.12882",
        "title": "Clifford Theory: A Geometrical Interpretation of Multivectorial Apparent Power",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, a generalization of the concept of electrical power for periodic current and voltage waveforms based on a new generalized complex geometric algebra (GCGA) is proposed. This powerful tool permits, in n-sinusoidal/nonlinear situations, representing and calculating the voltage, current, and apparent power in a single-port electrical network in terms of multivectors. The new expressions result in a novel representation of the apparent power, similar to the Steinmetz's phasor model, based on complex numbers, but limited to the purely sinusoidal case. The multivectorial approach presented is based on the frequency-domain decomposition of the apparent power into three components: the real part and the imaginary part of the complex-scalar associated to active and reactive power respectively, and distortion power, associated to the complex-bivector. A geometrical interpretation of the multivectorial components of apparent power is discussed. Numerical examples illustrate the clear advantages of the suggested approach.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "10 pages, 7 figures"
    },
    {
        "paper id": "2402.12885",
        "abstract url": "https://arxiv.org/abs/2402.12885",
        "title": "A Bound on the Maximal Marginal Degrees of Freedom",
        "rating": -10,
        "keywords": [],
        "abstract": "Common kernel ridge regression is expensive in memory allocation and computation time. This paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties. The fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable. The bound relates the effective dimension with the largest statistical leverage score. We characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel. This growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the Nystr\u00f6m method.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12928",
        "abstract url": "https://arxiv.org/abs/2402.12928",
        "title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
        "rating": -10,
        "keywords": [],
        "abstract": "By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, reading, conducting, or peer-reviewing review papers generally demands a significant investment of time and effort from researchers. To improve efficiency, this paper aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, this paper proposes several article-level, field-normalized, and large language model-empowered bibliometric indicators to evaluate reviews. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed. Second, based on these indicators, the study presents comparative analyses of representative reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in multiple aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this work offers insights into the current challenges of literature reviews and envisions future directions for their development.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "IEEE version v1. [February 19, 2024] IEEE version v2 with typos fixed. [February 23, 2024] IEEE version v3 with errors fixed. [February 29, 2024] IEEE version v4 with improved quaility. [February 29, 2024]"
    },
    {
        "paper id": "2402.12947",
        "abstract url": "https://arxiv.org/abs/2402.12947",
        "title": "Multigrid on unstructured meshes with regions of low quality cells",
        "rating": -10,
        "keywords": [],
        "abstract": "The convergence of multigrid methods degrades significantly if a small number of low quality cells are present in a finite element mesh, and this can be a barrier to the efficient and robust application of multigrid on complicated geometric domains. The degraded performance is observed also if intermediate levels in a non-nested geometric multigrid problem have low quality cells, even when the fine grid is high quality. It is demonstrated for geometric multigrid methods that the poor convergence is due to the local failure of smoothers to eliminate parts of error around cells of low quality. To overcome this, a global--local combined smoother is developed to maintain effective relaxation in the presence of a small number of poor quality cells. The smoother involves the application of a standard smoother on the whole domain, followed by local corrections for small subdomains with low quality cells. Two- and three-dimensional numerical experiments demonstrate that the degraded convergence of multigrid for low quality meshes can be restored to the high quality mesh reference case using the proposed smoother. The effect is particularly pronounced for higher-order finite elements. The results provide a basis for developing efficient, non-nested geometric multigrid methods for complicated engineering geometries.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12958",
        "abstract url": "https://arxiv.org/abs/2402.12958",
        "title": "Go Static: Contextualized Logging Statement Generation",
        "rating": -10,
        "keywords": [],
        "abstract": "Logging practices have been extensively investigated to assist developers in writing appropriate logging statements for documenting software behaviors. Although numerous automatic logging approaches have been proposed, their performance remains unsatisfactory due to the constraint of the single-method input, without informative programming context outside the method. Specifically, we identify three inherent limitations with single-method context: limited static scope of logging statements, inconsistent logging styles, and missing type information of logging variables. To tackle these limitations, we propose SCLogger, the first contextualized logging statement generation approach with inter-method static contexts. First, SCLogger extracts inter-method contexts with static analysis to construct the contextualized prompt for language models to generate a tentative logging statement. The contextualized prompt consists of an extended static scope and sampled similar methods, ordered by the chain-of-thought (COT) strategy. Second, SCLogger refines the access of logging variables by formulating a new refinement prompt for language models, which incorporates detailed type information of variables in the tentative logging statement. The evaluation results show that SCLogger surpasses the state-of-the-art approach by 8.7% in logging position accuracy, 32.1% in level accuracy, 19.6% in variable precision, and 138.4% in text BLEU-4 score. Furthermore, SCLogger consistently boosts the performance of logging statement generation across a range of large language models, thereby showcasing the generalizability of this approach.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "This paper was accepted by The ACM International Conference on the Foundations of Software Engineering (FSE 2024)"
    },
    {
        "paper id": "2402.12960",
        "abstract url": "https://arxiv.org/abs/2402.12960",
        "title": "Inferring Non-Failure Conditions for Declarative Programs",
        "rating": -10,
        "keywords": [],
        "abstract": "Unintended failures during a computation are painful but frequent during software development. Failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers. Programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct. This paper presents an approach to verify such assumptions. For this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations. In the positive case, the absence of such failures is ensured. In the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again. Our method is fully automatic and can be applied to larger declarative programs. The results of an implementation for functional logic Curry programs are presented.",
        "subjects": [
            "cs.PL"
        ],
        "comment": "Extended version of a paper presented at the 17th International Symposium on Functional and Logic Programming (FLOPS 2024)"
    },
    {
        "paper id": "2402.12962",
        "abstract url": "https://arxiv.org/abs/2402.12962",
        "title": "Multi-Level ML Based Burst-Aware Autoscaling for SLO Assurance and Cost Efficiency",
        "rating": -10,
        "keywords": [],
        "abstract": "Autoscaling is a technology to automatically scale the resources provided to their applications without human intervention to guarantee runtime Quality of Service (QoS) while saving costs. However, user-facing cloud applications serve dynamic workloads that often exhibit variable and contain bursts, posing challenges to autoscaling for maintaining QoS within Service-Level Objectives (SLOs). Conservative strategies risk over-provisioning, while aggressive ones may cause SLO violations, making it more challenging to design effective autoscaling. This paper introduces BAScaler, a Burst-Aware Autoscaling framework for containerized cloud services or applications under complex workloads, combining multi-level machine learning (ML) techniques to mitigate SLO violations while saving costs. BAScaler incorporates a novel prediction-based burst detection mechanism that distinguishes between predictable periodic workload spikes and actual bursts. When bursts are detected, BAScaler appropriately overestimates them and allocates resources accordingly to address the rapid growth in resource demand. On the other hand, BAScaler employs reinforcement learning to rectify potential inaccuracies in resource estimation, enabling more precise resource allocation during non-bursts. Experiments across ten real-world workloads demonstrate BAScaler's effectiveness, achieving a 57% average reduction in SLO violations and cutting resource costs by 10% compared to other prominent methods.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12967",
        "abstract url": "https://arxiv.org/abs/2402.12967",
        "title": "Quantifying Privacy via Information Density",
        "rating": -10,
        "keywords": [],
        "abstract": "We examine the relationship between privacy metrics that utilize information density to measure information leakage between a private and a disclosed random variable. Firstly, we prove that bounding the information density from above or below in turn implies a lower or upper bound on the information density, respectively. Using this result, we establish new relationships between local information privacy, asymmetric local information privacy, pointwise maximal leakage and local differential privacy. We further provide applications of these relations to privacy mechanism design. Furthermore, we provide statements showing the equivalence between a lower bound on information density and risk-averse adversaries. More specifically, we prove an equivalence between a guessing framework and a cost-function framework that result in the desired lower bound on the information density.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12973",
        "abstract url": "https://arxiv.org/abs/2402.12973",
        "title": "Between Green Hills and Green Bills: Unveiling the Green Shades of Sustainability and Burden Shifting through Multi-Objective Optimization in Swiss Energy System Planning",
        "rating": -10,
        "keywords": [],
        "abstract": "The Paris agreement is the first-ever universally accepted and legally binding agreement on global climate change. It is a bridge between today's and climate-neutrality policies and strategies before the end of the century. Critical to this endeavor is energy system modeling, which, while adept at devising cost-effective carbon-neutral strategies, often overlooks the broader environmental and social implications. This study introduces an innovative methodology that integrates life-cycle impact assessment indicators into energy system modeling, enabling a comprehensive assessment of both economic and environmental outcomes. Focusing on Switzerland's energy system as a case study, our model reveals that optimizing key environomic indicators can lead to significant economic advantages, with system costs potentially decreasing by 15% to 47% by minimizing potential impacts from operating fossil technologies to the indirect impact related to the construction of the renewable infrastructure. However, a system optimized solely for economic efficiency, despite achieving 63% reduction in carbon footprint compared to 2020, our results show a potential risk of burden shift to other environmental issues. The adoption of multi-objective optimization in our approach nuances the exploration of the complex interplay between environomic objectives and technological choices. Our results illuminate pathways towards more holistically optimized energy systems, effectively addressing trade-offs across environmental problems and enhancing societal acceptance of the solutions to this century's defining challenge.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12986",
        "abstract url": "https://arxiv.org/abs/2402.12986",
        "title": "Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory Manycore Clusters",
        "rating": -10,
        "keywords": [],
        "abstract": "Systolic arrays and shared-L1-memory manycore clusters are commonly used architectural paradigms that offer different trade-offs to accelerate parallel workloads. While the first excel with regular dataflow at the cost of rigid architectures and complex programming models, the second are versatile and easy to program but require explicit dataflow management and synchronization. This work aims at enabling efficient systolic execution on shared-L1-memory manycore clusters. We devise a flexible architecture where small and energy-efficient RISC-V cores act as the systolic array's processing elements (PEs) and can form diverse, reconfigurable systolic topologies through queues mapped in the cluster's shared memory. We introduce two low-overhead RISC-V ISA extensions for efficient systolic execution, namely Xqueue and Queue-linked registers (QLRs), which support queue management in hardware. The Xqueue extension enables single-instruction access to shared-memory-mapped queues, while QLRs allow implicit and autonomous access to them, relieving the cores of explicit communication instructions. We demonstrate Xqueue and QLRs in MemPool, an open-source shared-memory cluster with 256 PEs, and analyze the hybrid systolic-shared-memory architecture's trade-offs on several DSP kernels with diverse arithmetic intensity. For an area increase of just 6%, our hybrid architecture can double MemPool's compute unit utilization, reaching up to 73%. In typical conditions (TT/0.80V/25\u00b0C), in a 22 nm FDX technology, our hybrid architecture runs at 600 MHz with no frequency degradation and is up to 65% more energy efficient than the shared-memory baseline, achieving up to 208 GOPS/W, with up to 63% of power spent in the PEs.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12989",
        "abstract url": "https://arxiv.org/abs/2402.12989",
        "title": "Tactile Perception in Upper Limb Prostheses: Mechanical Characterization, Human Experiments, and Computational Findings",
        "rating": -10,
        "keywords": [],
        "abstract": "Our research investigates vibrotactile perception in four prosthetic hands with distinct kinematics and mechanical characteristics. We found that rigid and simple socket-based prosthetic devices can transmit tactile information and surprisingly enable users to identify the stimulated finger with high reliability. This ability decreases with more advanced prosthetic hands with additional articulations and softer mechanics. We conducted experiments to understand the underlying mechanisms. We assessed a prosthetic user's ability to discriminate finger contacts based on vibrations transmitted through the four prosthetic hands. We also performed numerical and mechanical vibration tests on the prostheses and used a machine learning classifier to identify the contacted finger. Our results show that simpler and rigid prosthetic hands facilitate contact discrimination (for instance, a user of a purely cosmetic hand can distinguish a contact on the index finger from other fingers with 83% accuracy), but all tested hands, including soft advanced ones, performed above chance level. Despite advanced hands reducing vibration transmission, a machine learning algorithm still exceeded human performance in discriminating finger contacts. These findings suggest the potential for enhancing vibrotactile feedback in advanced prosthetic hands and lay the groundwork for future integration of such feedback in prosthetic devices.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.12997",
        "abstract url": "https://arxiv.org/abs/2402.12997",
        "title": "Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism",
        "rating": -10,
        "keywords": [],
        "abstract": "Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13011",
        "abstract url": "https://arxiv.org/abs/2402.13011",
        "title": "An evolutionary game with reputation-based imitation-mutation dynamics",
        "rating": -10,
        "keywords": [],
        "abstract": "Reputation plays a crucial role in social interactions by affecting the fitness of individuals during an evolutionary process. Previous works have extensively studied the result of imitation dynamics without focusing on potential irrational choices in strategy updates. We now fill this gap and explore the consequence of such kind of randomness, or one may interpret it as an autonomous thinking. In particular, we study how this extended dynamics alters the evolution of cooperation when individual reputation is directly linked to collected payoff, hence providing a general fitness function. For a broadly valid conclusion, our spatial populations cover different types of interaction topologies, including lattices, small-world and scale-free graphs. By means of intensive simulations we can detect substantial increase in cooperation level that shows a reasonable stability in the presence of a notable strategy mutation.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": "13 pages, 8 figures, to be published in Applied Mathematics and Computation"
    },
    {
        "paper id": "2402.13024",
        "abstract url": "https://arxiv.org/abs/2402.13024",
        "title": "SmartEx: A Framework for Generating User-Centric Explanations in Smart Environments",
        "rating": -10,
        "keywords": [],
        "abstract": "Explainability is crucial for complex systems like pervasive smart environments, as they collect and analyze data from various sensors, follow multiple rules, and control different devices resulting in behavior that is not trivial and, thus, should be explained to the users. The current approaches, however, offer flat, static, and algorithm-focused explanations. User-centric explanations, on the other hand, consider the recipient and context, providing personalized and context-aware explanations. To address this gap, we propose an approach to incorporate user-centric explanations into smart environments. We introduce a conceptual model and a reference architecture for characterizing and generating such explanations. Our work is the first technical solution for generating context-aware and granular explanations in smart environments. Our architecture implementation demonstrates the feasibility of our approach through various scenarios.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "22nd International Conference on Pervasive Computing and Communications (PerCom 2024)"
    },
    {
        "paper id": "2402.13034",
        "abstract url": "https://arxiv.org/abs/2402.13034",
        "title": "Ray Tracing Algorithm for Reconfigurable Intelligent Surfaces",
        "rating": -10,
        "keywords": [],
        "abstract": "Ray tracing accelerated with graphics processing units (GPUs) is an accurate and efficient simulation technique of wireless communication channels. In this paper, we extend a GPU-accelerated ray tracer (RT) to support the effects of reconfigurable intelligent surfaces (RISs). To evaluate the electric field, we derived a RIS path loss model that can be integrated into a RT and enables further extensions for the implementation of additional features and incorporation into complex reflective scenarios. We verify the derivation and implementation of our model by comparison with empirical measurements in a lab environment. We demonstrate the capabilities of our model to support higher-order reflections from the RIS to the receiver. We find that such components have a significant effect on the received signal strength, concluding that the extensions of advanced functionality enabled by our model play an important role in the accurate modeling of radio wave propagation in an environment including RISs.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "submitted to IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), Valencia, Spain, September 2024"
    },
    {
        "paper id": "2402.13073",
        "abstract url": "https://arxiv.org/abs/2402.13073",
        "title": "Towards Intelligent Communications: Large Model Empowered Semantic Communications",
        "rating": -10,
        "keywords": [],
        "abstract": "Deep learning enabled semantic communications have shown great potential to significantly improve transmission efficiency and alleviate spectrum scarcity, by effectively exchanging the semantics behind the data. Recently, the emergence of large models, boasting billions of parameters, has unveiled remarkable human-like intelligence, offering a promising avenue for advancing semantic communication by enhancing semantic understanding and contextual understanding. This article systematically investigates the large model-empowered semantic communication systems from potential applications to system design. First, we propose a new semantic communication architecture that seamlessly integrates large models into semantic communication through the introduction of a memory module. Then, the typical applications are illustrated to show the benefits of the new architecture. Besides, we discuss the key designs in implementing the new semantic communication systems from module design to system training. Finally, the potential research directions are identified to boost the large model-empowered semantic communications.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "7 pages, 6 figures"
    },
    {
        "paper id": "2402.13079",
        "abstract url": "https://arxiv.org/abs/2402.13079",
        "title": "Mode Estimation with Partial Feedback",
        "rating": -10,
        "keywords": [],
        "abstract": "The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13085",
        "abstract url": "https://arxiv.org/abs/2402.13085",
        "title": "Kleene Theorems for Lasso Languages and $\u03c9$-Languages",
        "rating": -10,
        "keywords": [],
        "abstract": "Automata operating on pairs of words were introduced as an alternative way of capturing acceptance of regular $\u03c9$-languages. Families of DFAs and lasso automata operating on such pairs followed, giving rise to minimisation algorithms, a Myhill-Nerode theorem and language learning algorithms. Yet Kleene theorems for such a well-established class are still missing. We introduce rational lasso languages and expressions, show a Kleene theorem for lasso languages and explore the connection between rational lasso and $\u03c9$-expressions, which yields a Kleene theorem for $\u03c9$-languages with respect to saturated lasso automata. For one direction of the Kleene theorems, we also provide a Brzozowski construction for lasso automata from rational lasso expressions.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13086",
        "abstract url": "https://arxiv.org/abs/2402.13086",
        "title": "Profinite trees, through monads and the lambda-calculus",
        "rating": -10,
        "keywords": [],
        "abstract": "In its simplest form, the theory of regular languages is the study of sets of finite words recognized by finite monoids. The finiteness condition on monoids gives rise to a topological space whose points, called profinite words, encode the limiting behavior of words with respect to finite monoids. Yet, some aspects of the theory of regular languages are not particular to monoids and can be described in a general setting. On the one hand, Boja\u0144czyk has shown how to use monads to generalize the theory of regular languages and has given an abstract definition of the free profinite structure, defined by codensity, given a fixed monad and a notion of finite structure. On the other hand, Salvati has introduced the notion of language of $\u03bb$-terms, using denotational semantics, which generalizes the case of words and trees through the Church encoding. In recent work, the author and collaborators defined the notion of profinite $\u03bb$-term using semantics in finite sets and functions, which extend the Church encoding to profinite words. In this article, we prove that these two generalizations, based on monads and denotational semantics, coincide in the case of trees. To do so, we consider the monad of abstract clones which, when applied to a ranked alphabet, gives the associated clone of ranked trees. This induces a notion of free profinite clone, and hence of profinite trees. The main contribution is a categorical proof that the free profinite clone on a ranked alphabet is isomorphic, as a Stone-enriched clone, to the clone of profinite $\u03bb$-terms of Church type. Moreover, we also prove a parametricity theorem on families of semantic elements which provides another equivalent formulation of profinite trees in terms of Reynolds parametricity.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13106",
        "abstract url": "https://arxiv.org/abs/2402.13106",
        "title": "On Generalization Bounds for Deep Compound Gaussian Neural Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks. An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs. These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data. In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior. These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. The generalization error bound is formulated by bounding the Rademacher complexity of the class of compound Gaussian network estimates with Dudley's integral. Under realistic conditions, we show that, at worst, the generalization error scales $\\mathcal{O}(n\\sqrt{\\ln(n)})$ in the signal dimension and $\\mathcal{O}(($Network Size$)^{3/2})$ in network size.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "14 pages, 1 figure"
    },
    {
        "paper id": "2402.13107",
        "abstract url": "https://arxiv.org/abs/2402.13107",
        "title": "An Improved Lower Bound on the Number of Pseudoline Arrangements",
        "rating": -10,
        "keywords": [],
        "abstract": "Arrangements of pseudolines are classic objects in discrete and computational geometry. They have been studied with increasing intensity since their introduction almost 100 years ago. The study of the number $B_n$ of non-isomorphic simple arrangements of $n$ pseudolines goes back to Goodman and Pollack, Knuth, and others. It is known that $B_n$ is in the order of $2^{\u0398(n^2)}$ and finding asymptotic bounds on $b_n = \\frac{\\log_2(B_n)}{n^2}$ remains a challenging task. In 2011, Felsner and Valtr showed that $0.1887 \\leq b_n \\le 0.6571$ for sufficiently large $n$. The upper bound remains untouched but in 2020 Dumitrescu and Mandal improved the lower bound constant to $0.2083$. Their approach utilizes the known values of $B_n$ for up to $n=12$. We tackle the lower bound by utilizing dynamic programming and the Lindstr\u00f6m-Gessel-Viennot lemma. Our new bound is $b_n \\geq 0.2721$ for sufficiently large $n$. The result is based on a delicate interplay of theoretical ideas and computer assistance.",
        "subjects": [
            "math.CO"
        ],
        "comment": "This article is to appear in the proceedings of the 40th International Symposium on Computational Geometry (SoCG 2024). It is a merge of the following two independent submissions: 1) Justin Dallant -- Improved Lower Bound on the Number of Pseudoline Arrangements 2) Fernando Cort\u00e9s K\u00fchnast, Stefan Felsner, and Manfred Scheucher -- An Improved Lower Bound on the Number of Pseudoline Arrangements"
    },
    {
        "paper id": "2402.13120",
        "abstract url": "https://arxiv.org/abs/2402.13120",
        "title": "Tactile Weight Rendering: A Review for Researchers and Developers",
        "rating": -10,
        "keywords": [],
        "abstract": "Haptic rendering of weight plays an essential role in naturalistic object interaction in virtual environments. While kinesthetic devices have traditionally been used for this aim by applying forces on the limbs, tactile interfaces acting on the skin have recently offered potential solutions to enhance or substitute kinesthetic ones. Here, we aim to provide an in-depth overview and comparison of existing tactile weight rendering approaches. We categorized these approaches based on their type of stimulation into asymmetric vibration and skin stretch, further divided according to the working mechanism of the devices. Then, we compared these approaches using various criteria, including physical, mechanical, and perceptual characteristics of the reported devices and their potential applications. We found that asymmetric vibration devices have the smallest form factor, while skin stretch devices relying on the motion of flat surfaces, belts, or tactors present numerous mechanical and perceptual advantages for scenarios requiring more accurate weight rendering. Finally, we discussed the selection of the proposed categorization of devices and their application scopes, together with the limitations and opportunities for future research. We hope this study guides the development and use of tactile interfaces to achieve a more naturalistic object interaction and manipulation in virtual environments.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "15 pages, 2 tables, 3 figures, survey"
    },
    {
        "paper id": "2402.13123",
        "abstract url": "https://arxiv.org/abs/2402.13123",
        "title": "Exploring AI-assisted Ideation and Prototyping for Choreography",
        "rating": -10,
        "keywords": [],
        "abstract": "Choreography creation is a multimodal endeavor, demanding cognitive abilities to develop creative ideas and technical expertise to convert choreographic ideas into physical dance movements. Previous endeavors have sought to reduce the complexities in the choreography creation process in both dimensions. Among them, non-AI-based systems have focused on reinforcing cognitive activities by helping analyze and understand dance movements and augmenting physical capabilities by enhancing body expressivity. On the other hand, AI-based methods have helped the creation of novel choreographic materials with generative AI algorithms. The choreography creation process is constrained by time and requires a rich set of resources to stimulate novel ideas, but the need for iterative prototyping and reduced physical dependence have not been adequately addressed by prior research. Recognizing these challenges and the research gap, we present an innovative AI-based choreography-support system. Our goal is to facilitate rapid ideation by utilizing a generative AI model that can produce diverse and novel dance sequences. The system is designed to support iterative digital dance prototyping through an interactive web-based user interface that enables the editing and modification of generated motion. We evaluated our system by inviting six choreographers to analyze its limitations and benefits and present the evaluation results along with potential directions for future work.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13126",
        "abstract url": "https://arxiv.org/abs/2402.13126",
        "title": "VGMShield: Mitigating Misuse of Video Generative Models",
        "rating": -10,
        "keywords": [],
        "abstract": "With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information. In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \\textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \\textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy. Furthermore, anticipating future generative model improvements, we propose a {\\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal. Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "17 pages, 10 figures"
    },
    {
        "paper id": "2402.13135",
        "abstract url": "https://arxiv.org/abs/2402.13135",
        "title": "A Systematic Literature Review on Task Allocation and Performance Management Techniques in Cloud Data Center",
        "rating": -10,
        "keywords": [],
        "abstract": "As cloud computing usage grows, cloud data centers play an increasingly important role. To maximize resource utilization, ensure service quality, and enhance system performance, it is crucial to allocate tasks and manage performance effectively. The purpose of this study is to provide an extensive analysis of task allocation and performance management techniques employed in cloud data centers. The aim is to systematically categorize and organize previous research by identifying the cloud computing methodologies, categories, and gaps. A literature review was conducted, which included the analysis of 463 task allocations and 480 performance management papers. The review revealed three task allocation research topics and seven performance management methods. Task allocation research areas are resource allocation, load-Balancing, and scheduling. Performance management includes monitoring and control, power and energy management, resource utilization optimization, quality of service management, fault management, virtual machine management, and network management. The study proposes new techniques to enhance cloud computing work allocation and performance management. Short-comings in each approach can guide future research. The research's findings on cloud data center task allocation and performance management can assist academics, practitioners, and cloud service providers in optimizing their systems for dependability, cost-effectiveness, and scalability. Innovative methodologies can steer future research to fill gaps in the literature.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13143",
        "abstract url": "https://arxiv.org/abs/2402.13143",
        "title": "Systematic Mapping Protocol -- UX Design role in software development process",
        "rating": -10,
        "keywords": [],
        "abstract": "A systematic mapping protocol is a method for conducting a literature review in a rigorous and transparent way. It aims to provide an overview of the current state of research on a specific topic, identify gaps and opportunities, and guide future work. In this document, we present a systematic mapping protocol for investigating the role of the UX designer in the software development process. We define the research questions, scope, sources, search strategy, selection criteria, data extraction, and analysis methods that we will use to conduct the mapping study. Our goal is to understand how the UX designers collaborate with other stakeholders, what methods and tools they use, what challenges they face, and what outcomes they achieve in different contexts and domains.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2209.12004, arXiv:1702.02653"
    },
    {
        "paper id": "2402.13149",
        "abstract url": "https://arxiv.org/abs/2402.13149",
        "title": "Choosing a Suitable Requirement Prioritization Method: A Survey",
        "rating": -10,
        "keywords": [],
        "abstract": "Software requirements prioritization plays a crucial role in software development. It can be viewed as the process of ordering requirements by determining which requirements must be done first and which can be done later. Powerful requirements prioritization techniques are of paramount importance to finish the implementation on time and within budget. Many factors affect requirement prioritization such as stakeholder expectations, complexity, dependency, scalability, risk, and cost. Therefore, finding the proper order of requirements is a challenging process. Hence, different types of requirements prioritization techniques have been developed to support this task. In this survey, we propose a novel classification that can classify the prioritization techniques under two major classes: relative and exact prioritization techniques class, where each class is divided into two subclasses. We depend in our classification on the way the value of ranking is given to the requirement, either explicitly as a specific value in the case of the exact prioritization techniques class, or implicitly in the case of the Relative prioritization technique class. An overview of fifteen different requirements prioritization techniques are presented and organized according to the proposed classification criteria's. Moreover, we make a comparison between methods that are related to the same subclass to analyze their strengths and weaknesses. Based on the comparison results, the properties for each proposed subclass of techniques are identified. Depending on these properties, we present some recommendations to help project managers in the process of selecting the most suitable technique to prioritize requirements based on their project characteristics (number of requirements, time, cost, and accuracy).",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13151",
        "abstract url": "https://arxiv.org/abs/2402.13151",
        "title": "Almost-Tight Bounds on Preserving Cuts in Classes of Submodular Hypergraphs",
        "rating": -10,
        "keywords": [],
        "abstract": "Recently, a number of variants of the notion of cut-preserving hypergraph sparsification have been studied in the literature. These variants include directed hypergraph sparsification, submodular hypergraph sparsification, general notions of approximation including spectral approximations, and more general notions like sketching that can answer cut queries using more general data structures than just sparsifiers. In this work, we provide reductions between these different variants of hypergraph sparsification and establish new upper and lower bounds on the space complexity of preserving their cuts. At a high level, our results use the same general principle, namely, by showing that cuts in one class of hypergraphs can be simulated by cuts in a simpler class of hypergraphs, we can leverage sparsification results for the simpler class of hypergraphs.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13156",
        "abstract url": "https://arxiv.org/abs/2402.13156",
        "title": "Regret-Minimizing Contracts: Agency Under Uncertainty",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the fundamental problem of designing contracts in principal-agent problems under uncertainty. Previous works mostly addressed Bayesian settings in which principal's uncertainty is modeled as a probability distribution over agent's types. In this paper, we study a setting in which the principal has no distributional information about agent's type. In particular, in our setting, the principal only knows some uncertainty set defining possible agent's action costs. Thus, the principal takes a robust (adversarial) approach by trying to design contracts which minimize the (additive) regret: the maximum difference between what the principal could have obtained had them known agent's costs and what they actually get under the selected contract.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13159",
        "abstract url": "https://arxiv.org/abs/2402.13159",
        "title": "Barking dogs: A Fr\u00e9chet distance variant for detour detection",
        "rating": -10,
        "keywords": [],
        "abstract": "Imagine you are a dog behind a fence $Q$ and a hiker is passing by at constant speed along the hiking path $P$. In order to fulfil your duties as a watchdog, you desire to bark as long as possible at the human. However, your barks can only be heard in a fixed radius $\u03c1$ and, as a dog, you have bounded speed $s$. Can you optimize your route along the fence $Q$ in order to maximize the barking time with radius $\u03c1$, assuming you can run backwards and forward at speed at most $s$? We define the barking distance from a polyline $P$ on $n$ vertices to a polyline $Q$ on $m$ vertices as the time that the hiker stays in your barking radius if you run optimally along $Q$. This asymmetric similarity measure between two curves can be used to detect outliers in $Q$ compared to $P$ that other established measures like the Fr\u00e9chet distance and Dynamic Time Warping fail to capture at times. We consider this measure in three different settings. In the discrete setting, the traversals of $P$ and $Q$ are both discrete. For this case we show that the barking distance from $P$ to $Q$ can be computed in $O(nm\\log s)$ time. In the semi-discrete setting, the traversal of $Q$ is continuous while the one of $P$ is again discrete. Here, we show how to compute the barking distance in time $O(nm\\log (nm))$. Finally, in the continuous setting in which both traversals are continuous, we show that the problem can be solved in polynomial time. For all the settings we show that, assuming SETH, no truly subquadratic algorithm can exist.",
        "subjects": [
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13169",
        "abstract url": "https://arxiv.org/abs/2402.13169",
        "title": "Formal Verification for Blockchain-based Insurance Claims Processing",
        "rating": -10,
        "keywords": [],
        "abstract": "Insurance claims processing involves multi-domain entities and multi-source data, along with a number of human-agent interactions. Use of Blockchain technology-based platform can significantly improve scalability and response time for processing of claims which are otherwise manually-intensive and time-consuming. However, the chaincodes involved within the processes that issue claims, approve or deny them as required, need to be formally verified to ensure secure and reliable processing of transactions in Blockchain. In this paper, we use a formal modeling approach to verify various processes and their underlying chaincodes relating to different stages in insurance claims processing viz., issuance, approval, denial, and flagging for fraud investigation by using linear temporal logic (LTL). We simulate the formalism on the chaincodes and analyze the breach of chaincodes via model checking.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13170",
        "abstract url": "https://arxiv.org/abs/2402.13170",
        "title": "Improved Space Bounds for Subset Sum",
        "rating": -10,
        "keywords": [],
        "abstract": "More than 40 years ago, Schroeppel and Shamir presented an algorithm that solves the Subset Sum problem for $n$ integers in time $O^*(2^{0.5n})$ and space $O^*(2^{0.25n})$. The time upper bound remains unbeaten, but the space upper bound has been improved to $O^*(2^{0.249999n})$ in a recent breakthrough paper by Nederlof and W\u0119grzycki (STOC 2021). Their algorithm is a clever combination of a number of previously known techniques with a new reduction and a new algorithm for the Orthogonal Vectors problem. In this paper, we improve the space bound by Nederlof and W\u0119grzycki to $O^*(2^{0.246n})$ and also simplify their algorithm and its analysis. We achieve this by using an idea, due to Howgrave-Graham and Joux, of using a random prime number to filter the family of subsets. We incorporate it into the algorithm by Schroeppel and Shamir and then use this amalgam inside the representation technique. This allows us to reduce an instance of Subset Sum to a larger number of instances of weighted orthogonal vector.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13179",
        "abstract url": "https://arxiv.org/abs/2402.13179",
        "title": "homotopy.io: a proof assistant for finitely-presented globular $n$-categories",
        "rating": -10,
        "keywords": [],
        "abstract": "We present the proof assistant homotopy.io for working with finitely-presented semistrict higher categories. The tool runs in the browser with a point-and-click interface, allowing direct manipulation of proof objects via a graphical representation. We describe the user interface and explain how the tool can be used in practice. We also describe the essential subsystems of the tool, including collapse, contraction, expansion, typechecking, and layout, as well as key implementation details including data structure encoding, memoisation, and rendering. These technical innovations have been essential for achieving good performance in a resource-constrained setting.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13183",
        "abstract url": "https://arxiv.org/abs/2402.13183",
        "title": "Robust Model Predictive Control for nonlinear discrete-time systems using iterative time-varying constraint tightening",
        "rating": -10,
        "keywords": [],
        "abstract": "Robust Model Predictive Control (MPC) for nonlinear systems is a problem that poses significant challenges as highlighted by the diversity of approaches proposed in the last decades. Often compromises with respect to computational load, conservatism, generality, or implementation complexity have to be made, and finding an approach that provides the right balance is still a challenge to the research community. This work provides a contribution by proposing a novel shrinking-horizon robust MPC formulation for nonlinear discrete-time systems. By explicitly accounting for how disturbances and linearization errors are propagated through the nonlinear dynamics, a constraint tightening-based formulation is obtained, with guarantees of robust constraint satisfaction. The proposed controller relies on iteratively solving a Nonlinear Program (NLP) to simultaneously optimize system operation and the required constraint tightening. Numerical experiments show the effectiveness of the proposed controller with three different choices of NLP solvers as well as significantly improved computational speed, better scalability, and generally reduced conservatism when compared to an existing technique from the literature.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "22 pages, 12 figures. Submitted to the International Journal of Robust and Nonlinear Control"
    },
    {
        "paper id": "2402.13191",
        "abstract url": "https://arxiv.org/abs/2402.13191",
        "title": "Integrating Blockchain technology within an Information Ecosystem",
        "rating": -10,
        "keywords": [],
        "abstract": "Context: Blockchain-based Information Ecosystems (BBIEs) are a type of information ecosystem in which blockchain technology is used to provide a trust mechanism among parties and to manage shared business logic, breaking the traditional scheme of Information Ecosystems dominated by a leading company and leveraging the decentralization of data management, information flow, and business logic. Objective: In this paper, we propose architecture and technical aspects concerning the creation of a BBIE, underlining the advantages supplied and the logic decomposition among the business and storage components. Method: The requirements are derived from the current needs of the collaborative business and the data collected by surveying practitioners. To get these needs we followed the Grounded Theory research approach. We validate our architectural schema against a case study dealing with the management of a wine supply chain, also involving different companies and supervision authorities. Results: The proposed solution integrates blockchain-based applications with the existing information system as a module of the ecosystem, leveraging on the low costs, scalability, and high-level security because of the restricted access to the network. Conclusion: We must go a long way in deepening and refining the possibilities offered by technology in supporting innovative multi-organizational business models. BBIEs can contribute substantially to paving the way in such a direction.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "37 pages, 3 figures"
    },
    {
        "paper id": "2402.13192",
        "abstract url": "https://arxiv.org/abs/2402.13192",
        "title": "Spatial Queues with Nearest Neighbour Shifts",
        "rating": -10,
        "keywords": [],
        "abstract": "In this work we study multi-server queues on a Euclidean space. Consider $N$ servers that are distributed uniformly in $[0,1]^d$. Customers (users) arrive at the servers according to independent Poisson processes of intensity $\u03bb$. However, they probabilistically decide whether to join the queue they arrived at, or move to one of the nearest neighbours. The strategy followed by the customers affects the load on the servers in the long run. In this paper, we are interested in characterizing the fraction of servers that bear a larger load as compared to when the users do not follow any strategy, i.e., they join the queue they arrive at. These are called overloaded servers. In the one-dimensional case ($d=1$), we evaluate the expected fraction of overloaded servers for any finite $N$ when the users follow probabilistic nearest neighbour shift strategies. Additionally, for servers distributed in a $d$-dimensional space we provide expressions for the fraction of overloaded servers in the system as the total number of servers $N\\rightarrow \\infty$. Numerical experiments are provided to support our claims. Typical applications of our results include electric vehicles queueing at charging stations, and queues in airports or supermarkets.",
        "subjects": [
            "math.PR"
        ],
        "comment": "A part of this work was accepted to the conference International Teletraffic Congress (ITC 35) held between 3--5 October 2023 in Turin, Italy"
    },
    {
        "paper id": "2402.13234",
        "abstract url": "https://arxiv.org/abs/2402.13234",
        "title": "Unlocking Insights: Semantic Search in Jupyter Notebooks",
        "rating": -10,
        "keywords": [],
        "abstract": "Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher's intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in information retrieval. In this paper, we investigate the application of large language models to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks. Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent information. We demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook's contents, enabling it to effectively handle various types of user queries. Key components of this framework include: 1). A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative methodology is devised to address token size limitations that arise with code-type cells. We implement a finer-grained approach to data input, transitioning from the cell level to the function level, effectively resolving these issues.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13237",
        "abstract url": "https://arxiv.org/abs/2402.13237",
        "title": "Continuous Pushdown VASS in One Dimension are Easy",
        "rating": -10,
        "keywords": [],
        "abstract": "A pushdown vector addition system with states (PVASS) extends the model of vector addition systems with a pushdown stack. The algorithmic analysis of PVASS has applications such as static analysis of recursive programs manipulating integer variables. Unfortunately, reachability analysis, even for one-dimensional PVASS is not known to be decidable. We relax the model of one-dimensional PVASS to make the counter updates continuous and show that in this case reachability, coverability, and boundedness are decidable in polynomial time. In addition, for the extension of the model with lower-bound guards on the states, we show that coverability and reachability are in NP, and boundedness is in coNP.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "2 tables, 6 figures, 12 pages"
    },
    {
        "paper id": "2402.13326",
        "abstract url": "https://arxiv.org/abs/2402.13326",
        "title": "Deep Hedging with Market Impact",
        "rating": -10,
        "keywords": [],
        "abstract": "Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability. Dynamic hedging optimization can be framed as a sequential decision problem; thus, Reinforcement Learning (RL) models were recently proposed to tackle this task. However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments. Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity. In this paper, we propose a novel general market impact dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time. The optimal policy obtained from the DRL model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging. Results show our DRL model behaves better in contexts of low liquidity by, among others: 1) learning the extent to which portfolio rebalancing actions should be dampened or delayed to avoid high costs, 2) factoring in the impact of features not considered by conventional approaches, such as previous hedging errors through the portfolio value, and the underlying asset's drift (i.e. the magnitude of its expected return).",
        "subjects": [
            "q-fin.CP"
        ],
        "comment": "13 pages, 5 figures"
    },
    {
        "paper id": "2402.13340",
        "abstract url": "https://arxiv.org/abs/2402.13340",
        "title": "Greedy Monochromatic Island Partitions",
        "rating": -10,
        "keywords": [],
        "abstract": "Constructing partitions of colored points is a well-studied problem in discrete and computational geometry. We study the problem of creating a minimum-cardinality partition into monochromatic islands. Our input is a set $S$ of $n$ points in the plane where each point has one of $k \\geq 2$ colors. A set of points is monochromatic if it contains points of only one color. An island $I$ is a subset of $S$ such that $\\mathcal{CH}(I) \\cap S = I$, where $\\mathcal{CH}(I)$ denotes the convex hull of $I$. We identify an island with its convex hull; therefore, a partition into islands has the additional requirement that the convex hulls of the islands are pairwise-disjoint. We present three greedy algorithms for constructing island partitions and analyze their approximation ratios.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "13 pages, 12 figures"
    },
    {
        "paper id": "2402.13343",
        "abstract url": "https://arxiv.org/abs/2402.13343",
        "title": "EOSC CZ: Towards the development of Czech national ecosystem for FAIR research data",
        "rating": -10,
        "keywords": [],
        "abstract": "This short paper presents a compact overview of the Czech approach to implementing the European Open Science Cloud and plans for developing a Czech national infrastructure for FAIR research data. Its purpose is to provide an all-encompassing summary of the near future of research data management in Czechia. As such, we deliberately attempt to explain complicated concepts in minimum words, sacrificing the precision of expression for compactness.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "2 pages, 1 image"
    },
    {
        "paper id": "2402.13357",
        "abstract url": "https://arxiv.org/abs/2402.13357",
        "title": "Minimizing Tardy Processing Time on a Single Machine in Near-Linear Time",
        "rating": -10,
        "keywords": [],
        "abstract": "In this work we revisit the elementary scheduling problem $1||\\sum p_j U_j$. The goal is to select, among $n$ jobs with processing times and due dates, a subset of jobs with maximum total processing time that can be scheduled in sequence without violating their due dates. This problem is NP-hard, but a classical algorithm by Lawler and Moore from the 60s solves this problem in pseudo-polynomial time $O(nP)$, where $P$ is the total processing time of all jobs. With the aim to develop best-possible pseudo-polynomial-time algorithms, a recent wave of results has improved Lawler and Moore's algorithm for $1||\\sum p_j U_j$: First to time $\\tilde O(P^{7/4})$ [Bringmann, Fischer, Hermelin, Shabtay, Wellnitz; ICALP'20], then to time $\\tilde O(P^{5/3})$ [Klein, Polak, Rohwedder; SODA'23], and finally to time $\\tilde O(P^{7/5})$ [Schieber, Sitaraman; WADS'23]. It remained an exciting open question whether these works can be improved further. In this work we develop an algorithm in near-linear time $\\tilde O(P)$ for the $1||\\sum p_j U_j$ problem. This running time not only significantly improves upon the previous results, but also matches conditional lower bounds based on the Strong Exponential Time Hypothesis or the Set Cover Hypothesis and is therefore likely optimal (up to subpolynomial factors). Our new algorithm also extends to the case of $m$ machines in time $\\tilde O(P^m)$. In contrast to the previous improvements, we take a different, more direct approach inspired by the recent reductions from Modular Subset Sum to dynamic string problems. We thereby arrive at a satisfyingly simple algorithm.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13376",
        "abstract url": "https://arxiv.org/abs/2402.13376",
        "title": "Probabilistic automatic complexity of finite strings",
        "rating": -10,
        "keywords": [],
        "abstract": "We introduce a new complexity measure for finite strings using probabilistic finite-state automata (PFAs), in the same spirit as existing notions employing DFAs and NFAs, and explore its properties. The PFA complexity $A_P(x)$ is the least number of states of a PFA for which $x$ is the most likely string of its length to be accepted. The variant $A_{P,\u03b4}(x)$ adds a real-valued parameter $\u03b4$ specifying a lower bound on the gap in acceptance probabilities between $x$ and other strings. We relate $A_P$ to the DFA and NFA complexities, obtain a complete classification of binary strings with $A_P=2$, and prove $A_{P,\u03b4}(x)$ is computable for every $x$ and for cofinitely many $\u03b4$ (depending on $x$). Finally, we discuss several other variations on $A_P$ with a view to obtaining additional desirable properties.",
        "subjects": [
            "cs.FL"
        ],
        "comment": "45 pages, 5 figures. This work extends Chapter 2 of the author's PhD dissertation at Penn State"
    },
    {
        "paper id": "2402.13385",
        "abstract url": "https://arxiv.org/abs/2402.13385",
        "title": "Regular Languages in the Sliding Window Model",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the space complexity of the following problem: For a fixed regular language $L$, test membership of a sliding window of length $n$ to $L$ over a given stream of symbols. For deterministic streaming algorithms we prove a trichotomy theorem, namely that the (optimal) space complexity is either constant, logarithmic or linear, measured in the window length $n$. Additionally, we provide natural language-theoretic characterizations of the space classes. We then extend the results to randomized streaming algorithms and we show that in this setting, the space complexity of any regular language is either constant, doubly logarithmic, logarithmic or linear. Finally, we introduce sliding window testers, which can distinguish whether a window belongs to the language $L$ or has Hamming distance $> \u03b5n$ to $L$. We prove that every regular language has a deterministic (resp., randomized) sliding window tester that requires only logarithmic (resp., constant) space.",
        "subjects": [
            "cs.FL"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:1909.10261"
    },
    {
        "paper id": "2402.13387",
        "abstract url": "https://arxiv.org/abs/2402.13387",
        "title": "DistriFS: A Platform and User Agnostic Approach to File Distribution",
        "rating": -10,
        "keywords": [],
        "abstract": "In an age where the distribution of information is crucial, current file sharing solutions suffer significant deficiencies. Popular systems such as Google Drive, torrenting and IPFS suffer issues with compatibility, accessibility and censorship. This paper introduces DistriFS, a novel decentralized approach tailored for efficient and large-scale distribution of files. The architecture of DistriFS is grounded in three foundational pillars: scalability, security, and seamless integration. The proposed server implementation harnesses the power of Golang, ensuring near-universal interoperability across operating systems and hardware. Moreover, the use of the HTTP protocol eliminates the need for additional software to access the network, ensuring compatibility across all major operating systems and facilitating effortless downloads. The design and efficacy of DistriFS represent a significant advancement in the realm of file distribution systems, offering a scalable and secure alternative to current centralized and decentralized models.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13397",
        "abstract url": "https://arxiv.org/abs/2402.13397",
        "title": "Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join",
        "rating": -10,
        "keywords": [],
        "abstract": "Similarity join finds all pairs of close points within a given distance threshold. Many similarity join methods have been proposed, but they are usually not efficient on high-dimensional space due to the curse of dimensionality and data-unawareness. We investigate the possibility of using metric space Bloom filter (MSBF), a family of data structures checking if a query point has neighbors in a multi-dimensional space, to speed up similarity join. However, there are several challenges when applying MSBF to similarity join, including excessive information loss, data-unawareness and hard constraint on the distance metric. In this paper, we propose Xling, a generic framework to build a learning-based metric space filter with any existing regression model, aiming at accurately predicting whether a query point has enough number of neighbors. The framework provides a suite of optimization strategies to further improve the prediction quality based on the learning model, which has demonstrated significantly higher prediction quality than existing MSBF. We also propose XJoin, one of the first filter-based similarity join methods, based on Xling. By predicting and skipping those queries without enough neighbors, XJoin can effectively reduce unnecessary neighbor searching and therefore it achieves a remarkable acceleration. Benefiting from the generalization capability of deep learning models, XJoin can be easily transferred onto new dataset (in similar distribution) without re-training. Furthermore, Xling is not limited to being applied in XJoin, instead, it acts as a flexible plugin that can be inserted to any loop-based similarity join methods for a speedup.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13400",
        "abstract url": "https://arxiv.org/abs/2402.13400",
        "title": "The Dimension of Self-Directed Learning",
        "rating": -10,
        "keywords": [],
        "abstract": "Understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s. Within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning. In this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $SDdim$, that exactly characterizes the self-directed learning mistake-bound for any concept class. The intuition behind $SDdim$ can be understood as a two-player game called the \"labelling game\". Armed with this two-player game, we calculate $SDdim$ on a whole host of examples with notable results on axis-aligned rectangles, VC dimension $1$ classes, and linear separators. We demonstrate several learnability gaps with a central focus on self-directed learning and offline sequence learning models that include either the best or worst ordering. Finally, we extend our analysis to the self-directed binary agnostic setting where we derive upper and lower bounds.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "ALT 2024 Camera ready version"
    },
    {
        "paper id": "2402.13429",
        "abstract url": "https://arxiv.org/abs/2402.13429",
        "title": "Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask",
        "rating": -10,
        "keywords": [],
        "abstract": "As the number of pre-trained machine learning (ML) models is growing exponentially, data reduction tools are not catching up. Existing data reduction techniques are not specifically designed for pre-trained model (PTM) dataset files. This is largely due to a lack of understanding of the patterns and characteristics of these datasets, especially those relevant to data reduction and compressibility. This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Our analysis spans different types of data reduction and compression techniques, from hash-based data deduplication, data similarity detection, to dictionary-coding compression. Our analysis explores these techniques at three data granularity levels, from model layers, model chunks, to model parameters. We draw new observations that indicate that modern data reduction tools are not effective when handling PTM datasets. There is a pressing need for new compression methods that take into account PTMs' data characteristics for effective storage reduction. Motivated by our findings, we design ELF, a simple yet effective, error-bounded, lossy floating-point compression method. ELF transforms floating-point parameters in such a way that the common exponent field of the transformed parameters can be completely eliminated to save storage space. We develop Elves, a compression framework that integrates ELF along with several other data reduction methods. Elves uses the most effective method to compress PTMs that exhibit different patterns. Evaluation shows that Elves achieves an overall compression ratio of $1.52\\times$, which is $1.31\\times$, $1.32\\times$ and $1.29\\times$ higher than a general-purpose compressor (zstd), an error-bounded lossy compressor (SZ3), and the uniform model quantization, respectively, with negligible model accuracy loss.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Motivated by our findings, we design ELF, a simple yet effective, error-bounded, lossy floating-point compression method"
    },
    {
        "paper id": "2402.13436",
        "abstract url": "https://arxiv.org/abs/2402.13436",
        "title": "Optimisation of design parameters to improve performance of a planar electromagnetic actuator",
        "rating": -10,
        "keywords": [],
        "abstract": "Planar electromagnetic actuators based on the principle of linear motors are widely employed for micro and nano positioning applications. These actuators usually employ a planar magnetic platform driven by a co-planar electromagnetic coil. While these actuators offer a large motion range and high positioning resolution, their actuation bandwidth is limited due to relatively small electromagnetic stiffness. We report optimization of the design parameters of the electromagnetic coil and the magnetic assembly to maximize the electromagnetic force and stiffness. Firstly, we derive closed-form expressions for the electromagnetic forces and stiffness, which enable us to express these quantities in terms of the design parameters of the actuator. Secondly, based on these derived expressions, we estimate the optimum values of the design parameters to maximize force and stiffness. Notably, for the optimum design parameters, the force and stiffness per unit volume can be increased by two and three orders of magnitude, respectively by reducing the pitch of the electromagnetic coil by a factor of 10. Lastly, we develop an electromagnetic actuator and evaluate its performance using a Microelectromechanical system (MEMS) based force sensor. By operating the force sensor in a feedback loop, we precisely measure the generated electromagnetic forces for different design parameters of the actuator. The experimental results obtained align closely with the analytical values, with an error of less than 15%.",
        "subjects": [
            "physics.app-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13437",
        "abstract url": "https://arxiv.org/abs/2402.13437",
        "title": "Sketching AI Concepts with Capabilities and Examples: AI Innovation in the Intensive Care Unit",
        "rating": -10,
        "keywords": [],
        "abstract": "Advances in artificial intelligence (AI) have enabled unprecedented capabilities, yet innovation teams struggle when envisioning AI concepts. Data science teams think of innovations users do not want, while domain experts think of innovations that cannot be built. A lack of effective ideation seems to be a breakdown point. How might multidisciplinary teams identify buildable and desirable use cases? This paper presents a first hand account of ideating AI concepts to improve critical care medicine. As a team of data scientists, clinicians, and HCI researchers, we conducted a series of design workshops to explore more effective approaches to AI concept ideation and problem formulation. We detail our process, the challenges we encountered, and practices and artifacts that proved effective. We discuss the research implications for improved collaboration and stakeholder engagement, and discuss the role HCI might play in reducing the high failure rate experienced in AI innovation.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "to appear at CHI 2024"
    },
    {
        "paper id": "2402.13453",
        "abstract url": "https://arxiv.org/abs/2402.13453",
        "title": "A rational logit dynamic for decision-making under uncertainty: well-posedness, vanishing-noise limit, and numerical approximation",
        "rating": -10,
        "keywords": [],
        "abstract": "The classical logit dynamic on a continuous action space for decision-making un-der uncertainty is generalized to the dynamic where the exponential function for the softmax part has been replaced by a rational one that includes the former as a special case. We call the new dynamic as the rational logit dynamic. The use of the rational logit function implies that the uncertainties have a longer tail than that assumed in the classical one. We show that the rational logit dynamic admits a unique measure-valued solution and the solution can be approximated using a fi-nite difference discretization. We also show that the vanishing-noise limit of the rational logit dynamic exists and is different from the best-response one, demon-strating that influences of the uncertainty tail persist in the rational logit dynamic. We finally apply the rational logit dynamic to a unique fishing competition data that has been recently acquired by the authors.",
        "subjects": [
            "math.DS"
        ],
        "comment": "The first draft under review at some international conference"
    },
    {
        "paper id": "2402.13495",
        "abstract url": "https://arxiv.org/abs/2402.13495",
        "title": "Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness",
        "rating": -10,
        "keywords": [],
        "abstract": "Recommender systems (RSs) have gained widespread applications across various domains owing to the superior ability to capture users' interests. However, the complexity and nuanced nature of users' interests, which span a wide range of diversity, pose a significant challenge in delivering fair recommendations. In practice, user preferences vary significantly; some users show a clear preference toward certain item categories, while others have a broad interest in diverse ones. Even though it is expected that all users should receive high-quality recommendations, the effectiveness of RSs in catering to this disparate interest diversity remains under-explored. In this work, we investigate whether users with varied levels of interest diversity are treated fairly. Our empirical experiments reveal an inherent disparity: users with broader interests often receive lower-quality recommendations. To mitigate this, we propose a multi-interest framework that uses multiple (virtual) interest embeddings rather than single ones to represent users. Specifically, the framework consists of stacked multi-interest representation layers, which include an interest embedding generator that derives virtual interests from shared parameters, and a center embedding aggregator that facilitates multi-hop aggregation. Experiments demonstrate the effectiveness of the framework in achieving better trade-off between fairness and utility across various datasets and backbones.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by WWW'24"
    },
    {
        "paper id": "2402.13499",
        "abstract url": "https://arxiv.org/abs/2402.13499",
        "title": "Benchmarking and Dissecting the Nvidia Hopper GPU Architecture",
        "rating": -10,
        "keywords": [],
        "abstract": "Graphics processing units (GPUs) are continually evolving to cater to the computational demands of contemporary general-purpose workloads, particularly those driven by artificial intelligence (AI) utilizing deep learning techniques. A substantial body of studies have been dedicated to dissecting the microarchitectural metrics characterizing diverse GPU generations, which helps researchers understand the hardware details and leverage them to optimize the GPU programs. However, the latest Hopper GPUs present a set of novel attributes, including new tensor cores supporting FP8, DPX, and distributed shared memory. Their details still remain mysterious in terms of performance and operational characteristics. In this research, we propose an extensive benchmarking study focused on the Hopper GPU. The objective is to unveil its microarchitectural intricacies through an examination of the new instruction-set architecture (ISA) of Nvidia GPUs and the utilization of new CUDA APIs. Our approach involves two main aspects. Firstly, we conduct conventional latency and throughput comparison benchmarks across the three most recent GPU architectures, namely Hopper, Ada, and Ampere. Secondly, we delve into a comprehensive discussion and benchmarking of the latest Hopper features, encompassing the Hopper DPX dynamic programming (DP) instruction set, distributed shared memory, and the availability of FP8 tensor cores. The microbenchmarking results we present offer a deeper understanding of the novel GPU AI function units and programming features introduced by the Hopper architecture. This newfound understanding is expected to greatly facilitate software optimization and modeling efforts for GPU architectures. To the best of our knowledge, this study makes the first attempt to demystify the tensor core performance and programming instruction sets unique to Hopper GPUs.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13500",
        "abstract url": "https://arxiv.org/abs/2402.13500",
        "title": "Leveraging Translation For Optimal Recall: Tailoring LLM Personalization With User Profiles",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper explores a novel technique for improving recall in cross-language information retrieval (CLIR) systems using iterative query refinement grounded in the user's lexical-semantic space. The proposed methodology combines multi-level translation, semantic embedding-based expansion, and user profile-centered augmentation to address the challenge of matching variance between user queries and relevant documents. Through an initial BM25 retrieval, translation into intermediate languages, embedding lookup of similar terms, and iterative re-ranking, the technique aims to expand the scope of potentially relevant results personalized to the individual user. Comparative experiments on news and Twitter datasets demonstrate superior performance over baseline BM25 ranking for the proposed approach across ROUGE metrics. The translation methodology also showed maintained semantic accuracy through the multi-step process. This personalized CLIR framework paves the path for improved context-aware retrieval attentive to the nuances of user language.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "This is just an initial idea and it's implementation. The results are computed for the first 100 data points. Detailed results will be published with the actual paper"
    },
    {
        "paper id": "2402.13503",
        "abstract url": "https://arxiv.org/abs/2402.13503",
        "title": "Multiple-Error-Correcting Codes for Analog Computing on Resistive Crossbars",
        "rating": -10,
        "keywords": [],
        "abstract": "Error-correcting codes over the real field are studied which can locate outlying computational errors when performing approximate computing of real vector--matrix multiplication on resistive crossbars. Prior work has concentrated on locating a single outlying error and, in this work, several classes of codes are presented which can handle multiple errors. It is first shown that one of the known constructions, which is based on spherical codes, can in fact handle multiple outlying errors. A second family of codes is then presented with $\\zeroone$~parity-check matrices which are sparse and disjunct; such matrices have been used in other applications as well, especially in combinatorial group testing. In addition, a certain class of the codes that are obtained through this construction is shown to be efficiently decodable. As part of the study of sparse disjunct matrices, this work also contains improved lower and upper bounds on the maximum Hamming weight of the rows in such matrices.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13518",
        "abstract url": "https://arxiv.org/abs/2402.13518",
        "title": "RITFIS: Robust input testing framework for LLMs-based intelligent software",
        "rating": -10,
        "keywords": [],
        "abstract": "The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing. Current testing methods focus solely on the robustness of LLM-based software to prompts. Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance. To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software. To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs. This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem. Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints. RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software. RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the LLM-based software testing scenario. It demonstrates the effectiveness of RITFIS in evaluating LLM-based intelligent software through empirical validation. However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models. Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13521",
        "abstract url": "https://arxiv.org/abs/2402.13521",
        "title": "Test-Driven Development for Code Generation",
        "rating": -10,
        "keywords": [],
        "abstract": "Large language models (LLMs) like GPT4, have shown proficiency in generating code snippets from problem statements. Traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the code for the functionality is written. In the context of LLM-based code generation, one obvious benefit of TDD is that the developer then knows for sure if the generated code has passed all the given tests or not. Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to GPT4 is better than just giving the problem statement as input. To test our hypothesis, we build a framework TGen. In our experiments on the MBPP, HumanEval and CodeChef datasets, we consistently find that including tests solves more programming problems than not including them. Thus we show that TDD is a better development model than just using a problem statement when using GPT4 for code generation tasks.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.13529",
        "abstract url": "https://arxiv.org/abs/2402.13529",
        "title": "Multitier Service Migration Framework Based on Mobility Prediction in Mobile Edge Computing",
        "rating": -10,
        "keywords": [],
        "abstract": "Mobile edge computing (MEC) pushes computing resources to the edge of the network and distributes them at the edge of the mobile network. Offloading computing tasks to the edge instead of the cloud can reduce computing latency and backhaul load simultaneously. However, new challenges incurred by user mobility and limited coverage of MEC server service arise. Services should be dynamically migrated between multiple MEC servers to maintain service performance due to user movement. Tackling this problem is nontrivial because it is arduous to predict user movement, and service migration will generate service interruptions and redundant network traffic. Service interruption time must be minimized, and redundant network traffic should be reduced to ensure service quality. In this paper, the container lives migration technology based on prediction is studied, and an online prediction method based on map data that does not rely on prior knowledge such as user trajectories is proposed to address this challenge in terms of mobility prediction accuracy.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "13 pages, 9 figures"
    },
    {
        "paper id": "2402.14031",
        "abstract url": "https://arxiv.org/abs/2402.14031",
        "title": "Autoencoder with Ordered Variance for Nonlinear Model Identification",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper presents a novel autoencoder with ordered variance (AEO) in which the loss function is modified with a variance regularization term to enforce order in the latent space. Further, the autoencoder is modified using ResNets, which results in a ResNet AEO (RAEO). The paper also illustrates the effectiveness of AEO and RAEO in extracting nonlinear relationships among input variables in an unsupervised setting.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "14 pages, 8 figures"
    },
    {
        "paper id": "2402.14034",
        "abstract url": "https://arxiv.org/abs/2402.14034",
        "title": "AgentScope: A Flexible yet Robust Multi-Agent Platform",
        "rating": -10,
        "keywords": [],
        "abstract": "With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications. However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. Together with abundant syntactic tools, built-in resources, and user-friendly interactions, our communication mechanism significantly reduces the barriers to both development and understanding. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms while it is also armed with system-level supports for multi-modal data generation, storage and transmission. Additionally, we design an actor-based distribution framework, enabling easy conversion between local and distributed deployments and automatic parallel optimization without extra effort. With these features, AgentScope empowers developers to build applications that fully realize the potential of intelligent agents. We have released AgentScope at https://github.com/modelscope/agentscope, and hope AgentScope invites wider participation and innovation in this fast-moving field.",
        "subjects": [
            "cs.MA"
        ],
        "comment": "We have released code on https://github.com/modelscope/agentscope"
    },
    {
        "paper id": "2402.14859",
        "abstract url": "https://arxiv.org/abs/2402.14859",
        "title": "The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative",
        "rating": -10,
        "keywords": [],
        "abstract": "Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when manipulated to produce specific prompts or instructions, can effectively ``infect'' other agents within a society of MLLMs. This infection leads to the generation and circulation of harmful outputs, such as dangerous instructions or misinformation, across the society. We also show the transferability of these indirectly generated prompts, highlighting their possibility in propagating malice through inter-agent communication. This research provides a critical insight into a new dimension of threat posed by MLLMs, where a single agent can act as a catalyst for widespread malevolent influence. Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications. Our implementation is released at \\url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.05565",
        "abstract url": "https://arxiv.org/abs/2403.05565",
        "title": "OpenHEXAI: An Open-Source Framework for Human-Centered Evaluation of Explainable Machine Learning",
        "rating": -10,
        "keywords": [],
        "abstract": "Recently, there has been a surge of explainable AI (XAI) methods driven by the need for understanding machine learning model behaviors in high-stakes scenarios. However, properly evaluating the effectiveness of the XAI methods inevitably requires the involvement of human subjects, and conducting human-centered benchmarks is challenging in a number of ways: designing and implementing user studies is complex; numerous design choices in the design space of user study lead to problems of reproducibility; and running user studies can be challenging and even daunting for machine learning researchers. To address these challenges, this paper presents OpenHEXAI, an open-source framework for human-centered evaluation of XAI methods. OpenHEXAI features (1) a collection of diverse benchmark datasets, pre-trained models, and post hoc explanation methods; (2) an easy-to-use web application for user study; (3) comprehensive evaluation metrics for the effectiveness of post hoc explanation methods in the context of human-AI decision making tasks; (4) best practice recommendations of experiment documentation; and (5) convenient tools for power analysis and cost estimation. OpenHEAXI is the first large-scale infrastructural effort to facilitate human-centered benchmarks of XAI methods. It simplifies the design and implementation of user studies for XAI methods, thus allowing researchers and practitioners to focus on the scientific questions. Additionally, it enhances reproducibility through standardized designs. Based on OpenHEXAI, we further conduct a systematic benchmark of four state-of-the-art post hoc explanation methods and compare their impacts on human-AI decision making tasks in terms of accuracy, fairness, as well as users' trust and understanding of the machine learning model.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.07215",
        "abstract url": "https://arxiv.org/abs/2404.07215",
        "title": "Computation Offloading for Multi-server Multi-access Edge Vehicular Networks: A DDQN-based Method",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we investigate a multi-user offloading problem in the overlapping domain of a multi-server mobile edge computing system. We divide the original problem into two stages: the offloading decision making stage and the request scheduling stage. To prevent the terminal from going out of service area during offloading, we consider the mobility parameter of the terminal according to the human behaviour model when making the offloading decision, and then introduce a server evaluation mechanism based on both the mobility parameter and the server load to select the optimal offloading server. In order to fully utilise the server resources, we design a double deep Q-network (DDQN)-based reward evaluation algorithm that considers the priority of tasks when scheduling offload requests. Finally, numerical simulations are conducted to verify that our proposed method outperforms traditional mathematical computation methods as well as the DQN algorithm.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.01549",
        "abstract url": "https://arxiv.org/abs/2405.01549",
        "title": "Exploring Conceptual Modeling Metaphysics: Existence Containers, Leibniz's Monads and Avicenna's Essence",
        "rating": -10,
        "keywords": [],
        "abstract": "Requirement specifications in software engineering involve developing a conceptual model of a target domain. The model is based on ontological exploration of things in reality. Many things in such a process closely tie to problems in metaphysics, the field of inquiry of what reality fundamentally is. According to some researchers, metaphysicians are trying to develop an account of the world that properly conceptualizes the way it is, and software design is similar. Notions such as classes, object orientation, properties, instantiation, algorithms, etc. are metaphysical concepts developed many years ago. Exploring the metaphysics of such notions aims to establish quality assurance though some objective foundation not subject to misapprehensions and conventions. Much metaphysical work might best be understood as a model-building process. Here, a model is viewed as a hypothetical structure that we describe and investigate to understand more complex, real-world systems. The purpose of this paper is to enhance understanding of the metaphysical origins of conceptual modeling as exemplified by a specific proposed high-level model called thinging machines (TMs). The focus is on thimacs (things/machine) as a single category of TM modeling in the context of a two-phase world of staticity and dynamics. The general idea of this reality has been inspired by Deleuze s the virtual and related to the classical notions of Leibniz's monads and Avicenna's essence. The analysis of TMs leads to several interesting results about a thimac s nature at the static and existence levels.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "11 pages, 27 Figures"
    }
]