[
    {
        "paper id": "2402.16141",
        "abstract url": "https://arxiv.org/abs/2402.16141",
        "title": "PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Supervised fine-tuning is the most common method to adapt large language models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive computational resources. Recently, parameter-efficient fine-tuning (PEFT) methods have been widely studied due to its cost-effectiveness. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional. Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices. In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank. PLoRA has multiple training stages. During each stage, we still update only the LoRA weights. However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then reinitialize the LoRA states. Experimental results show that PLoRA has stronger learning ability, approximately 1.8 times that of LoRA's learning ability at most, but it does not increase memory usage. Further, we introduce a momentum-based unloading strategy for PLoRA to mitigate the training instability.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00812",
        "abstract url": "https://arxiv.org/abs/2403.00812",
        "title": "LoRA Meets Dropout under a Unified Framework",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "efficient finetuning"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which instantiates these methods based on dropping position, structural pattern and compensation measure. Through this framework, we reveal the new preferences and performance comparisons of them when involved with limited trainable parameters. This framework also allows us to amalgamate the most favorable aspects into a novel dropout method named HiddenKey. Extensive experiments verify the remarkable superiority and sufficiency of HiddenKey across multiple models and tasks, which highlights it as the preferred approach for high-performance and parameter-efficient finetuning of LLMs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16041",
        "abstract url": "https://arxiv.org/abs/2402.16041",
        "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \\textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \\textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the difference between two samples. To tackle this, we propose a novel \\textit{multi-population} aware optimization method for MMD called MMD-MP, which can \\textit{avoid variance increases} and thus improve the stability to measure the distributional discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and sentence-based detection, respectively. Extensive experiments on various LLMs, \\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The source code is available at \\url{https://github.com/ZSHsh98/MMD-MP}.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted at ICLR 2024"
    },
    {
        "paper id": "2402.15991",
        "abstract url": "https://arxiv.org/abs/2402.15991",
        "title": "$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.15992",
        "abstract url": "https://arxiv.org/abs/2402.15992",
        "title": "A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Since internet technologies have advanced, one of the primary factors in company development is customer happiness. Online platforms have become prominent places for sharing reviews. Twitter is one of these platforms where customers frequently post their thoughts. Reviews of flights on these platforms have become a concern for the airline business. A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation. So it's vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive. But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task. This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels. Some work has already been done on this strategy to automate the procedure using machine learning and deep learning techniques. However, they are all purely concerned with assessing the text's sentiment. In addition to the text, the tweet also includes the time, location, username, airline name, and so on. This additional information can be crucial for improving the model's outcome. To provide a machine learning based solution, this work has broadened its perspective to include these qualities. And it has come as no surprise that the additional features beyond text sentiment analysis produce better outcomes in machine learning based models.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16003",
        "abstract url": "https://arxiv.org/abs/2402.16003",
        "title": "Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter Estimation",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Dynamic parameterization of acoustic environments has drawn widespread attention in the field of audio processing. Precise representation of local room acoustic characteristics is crucial when designing audio filters for various audio rendering applications. Key parameters in this context include reverberation time (RT60) and geometric room volume. In recent years, neural networks have been extensively applied in the task of blind room parameter estimation. However, there remains a question of whether pure attention mechanisms can achieve superior performance in this task. To address this issue, this study employs blind room parameter estimation based on monaural noisy speech signals. Various model architectures are investigated, including a proposed attention-based model. This model is a convolution-free Audio Spectrogram Transformer, utilizing patch splitting, attention mechanisms, and cross-modality transfer learning from a pretrained Vision Transformer. Experimental results suggest that the proposed attention mechanism-based model, relying purely on attention mechanisms without using convolution, exhibits significantly improved performance across various room parameter estimation tasks, especially with the help of dedicated pretraining and data augmentation schemes. Additionally, the model demonstrates more advantageous adaptability and robustness when handling variable-length audio inputs compared to existing methods.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "28 pages, 9 figures, accepted for publishing to EURASIP Journal On Audio Speech And Music Processing"
    },
    {
        "paper id": "2402.16009",
        "abstract url": "https://arxiv.org/abs/2402.16009",
        "title": "PST-Bench: Tracing and Benchmarking the Source of Publications",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Tracing the source of research papers is a fundamental yet challenging task for researchers. The billion-scale citation relations between papers hinder researchers from understanding the evolution of science efficiently. To date, there is still a lack of an accurate and scalable dataset constructed by professional researchers to identify the direct source of their studied papers, based on which automatic algorithms can be developed to expand the evolutionary knowledge of science. In this paper, we study the problem of paper source tracing (PST) and construct a high-quality and ever-increasing dataset PST-Bench in computer science. Based on PST-Bench, we reveal several intriguing discoveries, such as the differing evolution patterns across various topics. An exploration of various methods underscores the hardness of PST-Bench, pinpointing potential directions on this topic. The dataset and codes have been available at https://github.com/THUDM/paper-source-trace.",
        "subjects": [
            "cs.DL",
            "cs.CL"
        ],
        "comment": "8 pages, 3 appendix pages"
    },
    {
        "paper id": "2402.16017",
        "abstract url": "https://arxiv.org/abs/2402.16017",
        "title": "Spectrum Extraction and Clipping for Implicitly Linear Layers",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers. We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at https://github.com/Ali-E/FastClip.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16021",
        "abstract url": "https://arxiv.org/abs/2402.16021",
        "title": "TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL",
                "eess.AS"
            ]
        ],
        "abstract": "The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16030",
        "abstract url": "https://arxiv.org/abs/2402.16030",
        "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "19 pages, Under review"
    },
    {
        "paper id": "2402.16034",
        "abstract url": "https://arxiv.org/abs/2402.16034",
        "title": "Emotion Classification in Short English Texts using Deep Learning Techniques",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts. Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy. To evaluate these methods, we introduce the \"SmallEnglishEmotions\" dataset, comprising 6372 varied short English texts annotated with five primary emotion categories. Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16038",
        "abstract url": "https://arxiv.org/abs/2402.16038",
        "title": "Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further exploration and research in the realm of large-scale NLP.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16039",
        "abstract url": "https://arxiv.org/abs/2402.16039",
        "title": "Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Conversational Agents (CAs) have increasingly been integrated into everyday life, sparking significant discussions on social media. While previous research has examined public perceptions of AI in general, there is a notable lack in research focused on CAs, with fewer investigations into cultural variations in CA perceptions. To address this gap, this study used computational methods to analyze about one million social media discussions surrounding CAs and compared people's discourses and perceptions of CAs in the US and China. We find Chinese participants tended to view CAs hedonically, perceived voice-based and physically embodied CAs as warmer and more competent, and generally expressed positive emotions. In contrast, US participants saw CAs more functionally, with an ambivalent attitude. Warm perception was a key driver of positive emotions toward CAs in both countries. We discussed practical implications for designing contextually sensitive and user-centric CAs to resonate with various users' preferences and needs.",
        "subjects": [
            "cs.HC",
            "cs.CL"
        ],
        "comment": "17 pages, 4 figures, 7 tables"
    },
    {
        "paper id": "2402.16048",
        "abstract url": "https://arxiv.org/abs/2402.16048",
        "title": "LLMs with Chain-of-Thought Are Non-Causal Reasoners",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "8 pages, 6 figures, 16 tables"
    },
    {
        "paper id": "2402.16050",
        "abstract url": "https://arxiv.org/abs/2402.16050",
        "title": "LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16058",
        "abstract url": "https://arxiv.org/abs/2402.16058",
        "title": "Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both passage and instruction compression tasks. Further analysis on gist verbalization results suggests that our gist prompts serve different functions in aiding language models. They may directly provide potential answers, generate the chain-of-thought, or simply repeat the inputs. All data and codes are available at https://github.com/OpenMatch/Gist-COCO .",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16061",
        "abstract url": "https://arxiv.org/abs/2402.16061",
        "title": "How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ $\\mathcal V$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers; and (3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence. Code is publicly available at https://github.com/Jometeorie/probing_llama.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024 (Long Paper)"
    },
    {
        "paper id": "2402.16063",
        "abstract url": "https://arxiv.org/abs/2402.16063",
        "title": "Citation-Enhanced Generation for LLM-based Chatbots",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16065",
        "abstract url": "https://arxiv.org/abs/2402.16065",
        "title": "Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script. Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on machine translation which requires cross-lingual knowledge. The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step. Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in machine translation across both translation directions.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16090",
        "abstract url": "https://arxiv.org/abs/2402.16090",
        "title": "Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This study provides a comprehensive benchmark framework for Source-Free Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to achieve a rigorous empirical understanding of the complex relationships between multiple key design factors in SF-UDA methods. The study empirically examines a diverse set of SF-UDA techniques, assessing their consistency across datasets, sensitivity to specific hyperparameters, and applicability across different families of backbone architectures. Moreover, it exhaustively evaluates pre-training datasets and strategies, particularly focusing on both supervised and self-supervised methods, as well as the impact of fine-tuning on the source domain. Our analysis also highlights gaps in existing benchmark practices, guiding SF-UDA research towards more effective and general approaches. It emphasizes the importance of backbone architecture and pre-training dataset selection on SF-UDA performance, serving as an essential reference and providing key insights. Lastly, we release the source code of our experimental framework. This facilitates the construction, training, and testing of SF-UDA methods, enabling systematic large-scale experimental analysis and supporting further research efforts in this field.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16092",
        "abstract url": "https://arxiv.org/abs/2402.16092",
        "title": "StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Utilizing large-scale pretrained models is a well-known strategy to enhance performance on various target tasks. It is typically achieved through fine-tuning pretrained models on target tasks. However, na\u00efve fine-tuning may not fully leverage knowledge embedded in pretrained models. In this study, we introduce a novel fine-tuning method, called stochastic cross-attention (StochCA), specific to Transformer architectures. This method modifies the Transformer's self-attention mechanism to selectively utilize knowledge from pretrained models during fine-tuning. Specifically, in each block, instead of self-attention, cross-attention is performed stochastically according to the predefined probability, where keys and values are extracted from the corresponding block of a pretrained model. By doing so, queries and channel-mixing multi-layer perceptron layers of a target model are fine-tuned to target tasks to learn how to effectively exploit rich representations of pretrained models. To verify the effectiveness of StochCA, extensive experiments are conducted on benchmarks in the areas of transfer learning and domain generalization, where the exploitation of pretrained models is critical. Our experimental results show the superiority of StochCA over state-of-the-art approaches in both areas. Furthermore, we demonstrate that StochCA is complementary to existing approaches, i.e., it can be combined with them to further improve performance. Our code is available at https://github.com/daintlab/stochastic_cross_attention",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The first two authors contributed equally"
    },
    {
        "paper id": "2402.16102",
        "abstract url": "https://arxiv.org/abs/2402.16102",
        "title": "Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the rise of increasingly powerful and user-facing NLP systems, there is growing interest in assessing whether they have a good representation of uncertainty by evaluating the quality of their predictive distribution over outcomes. We identify two main perspectives that drive starkly different evaluation protocols. The first treats predictive probability as an indication of model confidence; the second as an indication of human label variation. We discuss their merits and limitations, and take the position that both are crucial for trustworthy and fair NLP systems, but that exploiting a single predictive distribution is limiting. We recommend tools and highlight exciting directions towards models with disentangled representations of uncertainty about predictions and uncertainty about human labels.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EACL 2024 main"
    },
    {
        "paper id": "2402.16107",
        "abstract url": "https://arxiv.org/abs/2402.16107",
        "title": "FuseChat: Knowledge Fusion of Chat Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \\textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \\textsc{FuseChat}. \\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs to derive multiple target LLMs of identical structure and size via lightweight fine-tuning. Then, these target LLMs are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning. We validate our approach using three prominent chat LLMs with diverse architectures and scales, namely \\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and \\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains demonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad spectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5 (March)} and approaching \\texttt{Mixtral-8x7B-Instruct}. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/FuseLLM}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Technical Report, work in progress"
    },
    {
        "paper id": "2402.16121",
        "abstract url": "https://arxiv.org/abs/2402.16121",
        "title": "Towards Accurate Post-training Quantization for Reparameterized Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Model reparameterization is a widely accepted technique for improving inference speed without compromising performance. However, current Post-training Quantization (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models. This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of quantization parameters. To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of quantized reparameterization models. Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters. Our framework comprises two main components: Quantization Protecting Reparameterization and Across-block Calibration. For effective calibration, Quantization Protecting Reparameterization combines multiple branches into a single convolution with an affine layer. During training, the affine layer accelerates convergence and amplifies the output of the convolution to better accommodate samples with outliers. Additionally, Across-block Calibration leverages the measurement of stage output as supervision to address the gradient problem introduced by MAE and enhance the interlayer correlation with quantization parameters. Comprehensive experiments demonstrate the effectiveness of RepAPQ across various models and tasks. Our framework outperforms previous methods by approximately 1\\% for 8-bit PTQ and 2\\% for 6-bit PTQ, showcasing its superior performance. The code is available at \\url{https://github.com/ilur98/DLMC-QUANT}.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16139",
        "abstract url": "https://arxiv.org/abs/2402.16139",
        "title": "What Generative Artificial Intelligence Means for Terminological Definitions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This paper examines the impact of Generative Artificial Intelligence (GenAI) tools like ChatGPT on the creation and consumption of terminological definitions. From the terminologist's point of view, the strategic use of GenAI tools can streamline the process of crafting definitions, reducing both time and effort, while potentially enhancing quality. GenAI tools enable AI-assisted terminography, notably post-editing terminography, where the machine produces a definition that the terminologist then corrects or refines. However, the potential of GenAI tools to fulfill all the terminological needs of a user, including term definitions, challenges the very existence of terminological definitions and resources as we know them. Unlike terminological definitions, GenAI tools can describe the knowledge activated by a term in a specific context. However, a main drawback of these tools is that their output can contain errors. For this reason, users requiring reliability will likely still resort to terminological resources for definitions. Nevertheless, with the inevitable integration of AI into terminology work, the distinction between human-created and AI-created content will become increasingly blurred.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "37 pages, 1 figure"
    },
    {
        "paper id": "2402.16153",
        "abstract url": "https://arxiv.org/abs/2402.16153",
        "title": "ChatMusician: Understanding and Generating Music Intrinsically with LLM",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "GitHub: https://shanghaicannon.github.io/ChatMusician/"
    },
    {
        "paper id": "2402.16168",
        "abstract url": "https://arxiv.org/abs/2402.16168",
        "title": "Hitting \"Probe\"rty with Non-Linearity, and More",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Structural probes learn a linear transformation to find how dependency trees are embedded in the hidden states of language models. This simple design may not allow for full exploitation of the structure of the encoded information. Hence, to investigate the structure of the encoded information to its full extent, we incorporate non-linear structural probes. We reformulate the design of non-linear structural probes introduced by White et al. making its design simpler yet effective. We also design a visualization framework that lets us qualitatively assess how strongly two words in a sentence are connected in the predicted dependency tree. We use this technique to understand which non-linear probe variant is good at encoding syntactical information. Additionally, we also use it to qualitatively investigate the structure of dependency trees that BERT encodes in each of its layers. We find that the radial basis function (RBF) is an effective non-linear probe for the BERT model than the linear probe.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16184",
        "abstract url": "https://arxiv.org/abs/2402.16184",
        "title": "Deep Neural Network Initialization with Sparsity Inducing Activations",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\u03c6(x)=\\max(0, x-\u03c4)$ for $\u03c4\\ge 0$) and soft thresholding ($\u03c6(x)=0$ for $|x|\\le\u03c4$ and $x-\\text{sign}(x)\u03c4$ for $|x|>\u03c4$). We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map. Numerical experiments verify the theory and show that the proposed magnitude clipped sparsifying activations can be trained with training and test fractional sparsity as high as 85\\% while retaining close to full accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Published in the International Conference on Learning Representations (ICLR) 2024"
    },
    {
        "paper id": "2402.16189",
        "abstract url": "https://arxiv.org/abs/2402.16189",
        "title": "One-stage Prompt-based Continual Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop < 1%. We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power. The QR loss is only applied during training time, so there is no computational overhead at inference from the QR loss. With the QR loss, our approach maintains ~ 50% computational cost reduction during inference as well as outperforms the prior two-stage PCL methods by ~1.4% on public class-incremental continual learning benchmarks including CIFAR-100, ImageNet-R, and DomainNet.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16194",
        "abstract url": "https://arxiv.org/abs/2402.16194",
        "title": "ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Effective feature representations play a critical role in enhancing the performance of text generation models that rely on deep neural networks. However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated text. In this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user's utterance while simultaneously enhancing performance. We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of sentiment analysis for open-domain chatbots, enabling the generation of empathetic responses that are fluent and relevant. In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely zeroes in on sentiment and emotion nuances within the user's utterance. This ensures the generation of context-rich representations tailored to the underlying emotional tone and sentiment intricacies of the text. Our approach outperforms existing methods for generating empathetic embeddings, providing empathetic and diverse responses. The performance of our proposed model significantly exceeds that of existing models, enhancing emotion detection accuracy by 6.2% and lexical diversity by 1.4%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to the LREC-COLING 2024"
    },
    {
        "paper id": "2402.16200",
        "abstract url": "https://arxiv.org/abs/2402.16200",
        "title": "IR2: Information Regularization for Information Retrieval",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regularization is applied. This provides a systematic approach for optimizing synthetic data generation in data-limited, complex-query IR scenarios. All code, prompts and synthetic data are available at https://github.com/Info-Regularization/Information-Regularization.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted by LREC-COLING 2024 - The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation"
    },
    {
        "paper id": "2402.16247",
        "abstract url": "https://arxiv.org/abs/2402.16247",
        "title": "Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learning (ECTL), in which an agent is trained in self-play with EC and then learns from the data to translate between the emergent protocol and the target community's protocol.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16248",
        "abstract url": "https://arxiv.org/abs/2402.16248",
        "title": "Topic-to-essay generation with knowledge-based content selection",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16261",
        "abstract url": "https://arxiv.org/abs/2402.16261",
        "title": "UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture the subtle relationship between dialogue context and different candidates by regarding historically selected candidates as hard negatives. Extensive experiments and analysis establish state-of-the-art retrieval quality both within and outside its training domain, revealing the promising potential and generalization capability of our model to serve as a universal retriever for different candidate selection tasks simultaneously.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16278",
        "abstract url": "https://arxiv.org/abs/2402.16278",
        "title": "A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that the self-matching training method with InME outperforms the existing ontology embeddings for the GO and FoodOn ontologies and that the method with the concatenation of CoME and OWL2Vec* outperforms them for the HeLiS ontology.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "21 pages, 6 figures"
    },
    {
        "paper id": "2402.16280",
        "abstract url": "https://arxiv.org/abs/2402.16280",
        "title": "Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances. As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative adversarial learning, etc. In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of few-shot learning (FSL). Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation. To achieve this goal, we adopt the meta-learning based FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task. First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of few-shot instance segmentation (FSIS) to generalized few-shot instance segmentation (GFSIS). Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized Few-Shot Instance Segmentation (SGFSIS) framework. Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including semi-supervised learning, simple transfer learning, etc., with comparable performance to fully supervised learning with less than 5% annotations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16288",
        "abstract url": "https://arxiv.org/abs/2402.16288",
        "title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and three retrievers. Experimental results demonstrate that BERT-based classification models significantly outperform LLMs such as ChatGLM3 and ChatGPT in the memory classification task. Furthermore, our study highlights the importance of effective memory integration in the QA task.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16918",
        "abstract url": "https://arxiv.org/abs/2402.16918",
        "title": "m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources. Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules. Our approach involves teacher modules split from a pretrained monolithic model, and student modules of a modular model. m2mKD separately combines these modules with a shared meta model and encourages the student module to mimic the behaviour of the teacher module. We evaluate the effectiveness of m2mKD on two distinct modular neural architectures: Neural Attentive Circuits (NACs) and Vision Mixture-of-Experts (V-MoE). By applying m2mKD to NACs, we achieve significant improvements in IID accuracy on Tiny-ImageNet (up to 5.6%) and OOD robustness on Tiny-ImageNet-R (up to 4.2%). On average, we observe a 1% gain in both ImageNet and ImageNet-R. The V-MoE-Base model trained using m2mKD also achieves 3.5% higher accuracy than end-to-end training on ImageNet. The experimental results demonstrate that our method offers a promising solution for connecting modular networks with pretrained monolithic models. Code is available at https://github.com/kamanphoebe/m2mKD.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00816",
        "abstract url": "https://arxiv.org/abs/2403.00816",
        "title": "CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demonstrate that our methodology achieved state-of-the-art or competitive results with both single-page and multi-page documents in various fields.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03224",
        "abstract url": "https://arxiv.org/abs/2403.03224",
        "title": "Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Live performances of music are always charming, with the unpredictability of improvisation due to the dynamic between musicians and interactions with the audience. Jazz improvisation is a particularly noteworthy example for further investigation from a theoretical perspective. Here, we introduce a novel mathematical game theory model for jazz improvisation, providing a framework for studying music theory and improvisational methodologies. We use computational modeling, mainly reinforcement learning, to explore diverse stochastic improvisational strategies and their paired performance on improvisation. We find that the most effective strategy pair is a strategy that reacts to the most recent payoff (Stepwise Changes) with a reinforcement learning strategy limited to notes in the given chord (Chord-Following Reinforcement Learning). Conversely, a strategy that reacts to the partner's last note and attempts to harmonize with it (Harmony Prediction) strategy pair yields the lowest non-control payoff and highest standard deviation, indicating that picking notes based on immediate reactions to the partner player can yield inconsistent outcomes. On average, the Chord-Following Reinforcement Learning strategy demonstrates the highest mean payoff, while Harmony Prediction exhibits the lowest. Our work lays the foundation for promising applications beyond jazz: including the use of artificial intelligence (AI) models to extract data from audio clips to refine musical reward systems, and training machine learning (ML) models on existing jazz solos to further refine strategies within the game.",
        "subjects": [
            "physics.soc-ph",
            "cs.AI",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "16 pages, 4 figures"
    },
    {
        "paper id": "2403.04770",
        "abstract url": "https://arxiv.org/abs/2403.04770",
        "title": "Social Orientation: A New Feature for Dialogue Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "There are many settings where it is useful to predict and explain the success or failure of a dialogue. Circumplex theory from psychology models the social orientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation participants and can be used to predict and explain the outcome of social interactions. Our work is novel in its systematic application of social orientation tags to modeling conversation outcomes. In this paper, we introduce a new data set of dialogue utterances machine-labeled with social orientation tags. We show that social orientation tags improve task performance, especially in low-resource settings, on both English and Chinese language benchmarks. We also demonstrate how social orientation tags help explain the outcomes of social interactions when used in neural models. Based on these results showing the utility of social orientation tags for dialogue outcome prediction tasks, we release our data sets, code, and models that are fine-tuned to predict social orientation tags on dialogue utterances.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2403.12077",
        "abstract url": "https://arxiv.org/abs/2403.12077",
        "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval. These findings highlight the potential security risks of these systems and emphasize the need for rigorous evaluation before deployment.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": "21 pages, 7 figures, 4 tables"
    },
    {
        "paper id": "2404.10777",
        "abstract url": "https://arxiv.org/abs/2404.10777",
        "title": "Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic Displays",
        "rating": "1",
        "keywords": [
            [
                "Time-Efficient",
                "GPU memory"
            ],
            [
                "3D"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Recently, deep learning-based computer-generated holography (CGH) has demonstrated tremendous potential in three-dimensional (3D) displays and yielded impressive display quality. However, most existing deep learning-based CGH techniques can only generate holograms of 1080p resolution, which is far from the ultra-high resolution (16K+) required for practical virtual reality (VR) and augmented reality (AR) applications to support a wide field of view and large eye box. One of the major obstacles in current CGH frameworks lies in the limited memory available on consumer-grade GPUs which could not facilitate the generation of higher-definition holograms. To overcome the aforementioned challenge, we proposed a divide-conquer-and-merge strategy to address the memory and computational capacity scarcity in ultra-high-definition CGH generation. This algorithm empowers existing CGH frameworks to synthesize higher-definition holograms at a faster speed while maintaining high-fidelity image display quality. Both simulations and experiments were conducted to demonstrate the capabilities of the proposed framework. By integrating our strategy into HoloNet and CCNNs, we achieved significant reductions in GPU memory usage during the training period by 64.3\\% and 12.9\\%, respectively. Furthermore, we observed substantial speed improvements in hologram generation, with an acceleration of up to 3$\\times$ and 2 $\\times$, respectively. Particularly, we successfully trained and inferred 8K definition holograms on an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore, we conducted full-color optical experiments to verify the effectiveness of our method. We believe our strategy can provide a novel approach for memory- and time-efficient holographic displays.",
        "subjects": [
            "eess.IV",
            "cs.GR",
            "physics.optics"
        ],
        "comment": "This paper has been accepted as conference paper in IEEE VR 2024"
    },
    {
        "paper id": "2402.15989",
        "abstract url": "https://arxiv.org/abs/2402.15989",
        "title": "PIDformer: Transformer Meets Control Theory",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this work, we address two main shortcomings of transformer architectures: input corruption and rank collapse in their output representation. We unveil self-attention as an autonomous state-space model that inherently promotes smoothness in its solutions, leading to lower-rank outputs and diminished representation capacity. Moreover, the steady-state solution of the model is sensitive to input perturbations. We incorporate a Proportional-Integral-Derivative (PID) closed-loop feedback control system with a reference point into the model to improve robustness and representation capacity. This integration aims to preserve high-frequency details while bolstering model stability, rendering it more noise-resilient. The resulting controlled state-space model is theoretically proven robust and adept at addressing the rank collapse. Motivated by this control framework, we derive a novel class of transformers, PID-controlled Transformer (PIDformer), aimed at improving robustness and mitigating the rank-collapse issue inherent in softmax transformers. We empirically evaluate the model for advantages and robustness against baseline transformers across various practical tasks, including object classification, image segmentation, and language modeling.",
        "subjects": [
            "cs.AI",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.15990",
        "abstract url": "https://arxiv.org/abs/2402.15990",
        "title": "An Empirical Study of Challenges in Machine Learning Asset Management",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle. This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs. Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects. Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic clustering. We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the most discussed. We also find 79 solution topics, categorized under 18 macro-topics, highlighting software dependency, feature development, and file management as key solutions. This research underscores the need for further exploration of identified pain points and the importance of collaborative efforts across academia, industry, and the research community.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.15993",
        "abstract url": "https://arxiv.org/abs/2402.15993",
        "title": "Learning Method for S4 with Diagonal State Space Layers using Balanced Truncation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation, a prevalent model reduction technique in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPO initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggesting that our approach effectively leverages the strengths of the original model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.15994",
        "abstract url": "https://arxiv.org/abs/2402.15994",
        "title": "Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Portfolio management issues have been extensively studied in the field of artificial intelligence in recent years, but existing deep learning-based quantitative trading methods have some areas where they could be improved. First of all, the prediction mode of stocks is singular; often, only one trading expert is trained by a model, and the trading decision is solely based on the prediction results of the model. Secondly, the data source used by the model is relatively simple, and only considers the data of the stock itself, ignoring the impact of the whole market risk on the stock. In this paper, the DQN algorithm is introduced into asset management portfolios in a novel and straightforward way, and the performance greatly exceeds the benchmark, which fully proves the effectiveness of the DRL algorithm in portfolio management. This also inspires us to consider the complexity of financial problems, and the use of algorithms should be fully combined with the problems to adapt. Finally, in this paper, the strategy is implemented by selecting the assets and actions with the largest Q value. Since different assets are trained separately as environments, there may be a phenomenon of Q value drift among different assets (different assets have different Q value distribution areas), which may easily lead to incorrect asset selection. Consider adding constraints so that the Q values of different assets share a Q value distribution to improve results.",
        "subjects": [
            "q-fin.CP",
            "cs.CE",
            "cs.LG"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2402.15995",
        "abstract url": "https://arxiv.org/abs/2402.15995",
        "title": "Improved Hardness Results for Learning Intersections of Halfspaces",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting. Strikingly little is known about this problem. For instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces. On the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of Feige's 3SAT hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With intersections of fewer halfspaces being only ruled out under less standard assumptions [DV21] (such as the existence of local pseudo-random generators with large stretch). We significantly narrow this gap by showing that even learning $\u03c9(\\log \\log N)$ halfspaces in dimension $N$ takes super-polynomial time under standard assumptions on worst-case lattice problems (namely that SVP and SIVP are hard to approximate within polynomial factors). Further, we give unconditional hardness results in the statistical query framework. Specifically, we show that for any $k$ (even constant), learning $k$ halfspaces in dimension $N$ requires accuracy $N^{-\u03a9(k)}$, or exponentially many queries -- in particular ruling out SQ algorithms with polynomial accuracy for $\u03c9(1)$ halfspaces. To the best of our knowledge this is the first unconditional hardness result for learning a super-constant number of halfspaces. Our lower bounds are obtained in a unified way via a novel connection we make between intersections of halfspaces and the so-called parallel pancakes distribution [DKS17,BLPR19,BRST21] that has been at the heart of many lower bound constructions in (robust) high-dimensional statistics in the past few years.",
        "subjects": [
            "cs.CC",
            "cs.LG",
            "math.ST",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.15997",
        "abstract url": "https://arxiv.org/abs/2402.15997",
        "title": "Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks \"just right\" for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preference learning for supporting efficient visualization design optimization.",
        "subjects": [
            "cs.HC",
            "cs.GR",
            "cs.LG"
        ],
        "comment": "CHI 2024. 12 pages/9 figures"
    },
    {
        "paper id": "2402.16026",
        "abstract url": "https://arxiv.org/abs/2402.16026",
        "title": "Feature Selection Based on Orthogonal Constraints and Polygon Area",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The goal of feature selection is to choose the optimal subset of features for a recognition task by evaluating the importance of each feature, thereby achieving effective dimensionality reduction. Currently, proposed feature selection methods often overlook the discriminative dependencies between features and labels. To address this problem, this paper introduces a novel orthogonal regression model incorporating the area of a polygon. The model can intuitively capture the discriminative dependencies between features and labels. Additionally, this paper employs a hybrid non-monotone linear search method to efficiently tackle the non-convex optimization challenge posed by orthogonal constraints. Experimental results demonstrate that our approach not only effectively captures discriminative dependency information but also surpasses traditional methods in reducing feature dimensions and enhancing classification performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16077",
        "abstract url": "https://arxiv.org/abs/2402.16077",
        "title": "Equivariant Frames and the Impossibility of Continuous Canonicalization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures. Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements. In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged. In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function. To address this fundamental robustness problem, we formally define and construct \\emph{weighted} frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the actions of $SO(2)$, $SO(3)$, and $S_n$ on point clouds.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16086",
        "abstract url": "https://arxiv.org/abs/2402.16086",
        "title": "Deep Homography Estimation for Visual Place Recognition",
        "rating": "0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without additional homography labels, which can also be jointly trained with the backbone network to help it extract the features that are more suitable for local matching. Extensive experiments on benchmark datasets show that our method can outperform several state-of-the-art methods. And it is more than one order of magnitude faster than the mainstream hierarchical VPR methods using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted by AAAI2024"
    },
    {
        "paper id": "2402.16105",
        "abstract url": "https://arxiv.org/abs/2402.16105",
        "title": "Informed Meta-Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational noise, task distribution shifts, and heterogeneity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16119",
        "abstract url": "https://arxiv.org/abs/2402.16119",
        "title": "DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge. DeepForge uses an architecture that combines 1D convolutional neural networks and gated recurrent units. It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging. The paper also details DeepForge's architecture and the finite element simulation model used to generate the data set, using a three-stroke forging process. The results demonstrate DeepForge's ability to predict microstructure with a mean absolute error of 0.4$\\pm$0.3%. In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece. These results are then verified experimentally, demonstrating a significant step towards improved control and quality in forging processes where temperature can be used as an additional degree of freedom in the process.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16131",
        "abstract url": "https://arxiv.org/abs/2402.16131",
        "title": "A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results.",
        "subjects": [
            "cs.LG",
            "stat.ME",
            "stat.ML"
        ],
        "comment": "Accepted by Transactions on Machine Learning Research"
    },
    {
        "paper id": "2402.16181",
        "abstract url": "https://arxiv.org/abs/2402.16181",
        "title": "How Can LLM Guide RL? A Value-Based Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the LLM-informed policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration. Additionally, we present a practical algorithm SLINVIT that simplifies the construction of the value function and employs subgoals to reduce the search complexity. Our experiments across three interactive environments ALFWorld, InterCode, and BlocksWorld demonstrate that our method achieves state-of-the-art success rates and also surpasses previous RL and LLM approaches in terms of sample efficiency. Our code is available at https://github.com/agentification/Language-Integrated-VI.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16196",
        "abstract url": "https://arxiv.org/abs/2402.16196",
        "title": "Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems. However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging. We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim. SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients. We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. We additionally provide an OpenFOAM sub-module with examples that can be used as starting points for real-world applications in CFD+ML.",
        "subjects": [
            "cs.LG",
            "physics.flu-dyn"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16197",
        "abstract url": "https://arxiv.org/abs/2402.16197",
        "title": "Language Models for Code Completion: A Practical Evaluation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the model's predictions, we found that 66.3% of failures were due to the models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.",
        "subjects": [
            "cs.SE",
            "cs.LG",
            "cs.PL"
        ],
        "comment": "To be published in the proceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE 2024)"
    },
    {
        "paper id": "2402.16235",
        "abstract url": "https://arxiv.org/abs/2402.16235",
        "title": "Human-AI Co-Creation of Worked Examples for Programming Classes",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes. Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code. However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class. In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming. We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary.We also present a study that assesses the quality of explanations created with this approach",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2312.02105"
    },
    {
        "paper id": "2402.16237",
        "abstract url": "https://arxiv.org/abs/2402.16237",
        "title": "Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold. A theoretical analysis for the convergence of the algorithm to an accurate solution is provided. On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16268",
        "abstract url": "https://arxiv.org/abs/2402.16268",
        "title": "Foundation Model Transparency Reports",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g., the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16269",
        "abstract url": "https://arxiv.org/abs/2402.16269",
        "title": "From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using ChatGPT. We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed. We also propose possible research directions to overcome these gaps. We also see this work as a call to action to bring together the LLM and optimization communities to pursue our vision, thereby enabling much more widespread improved decision-making.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16285",
        "abstract url": "https://arxiv.org/abs/2402.16285",
        "title": "A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector onboard the International Space Station containing six different subdetectors. The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are used to separate electrons/positrons from the abundant cosmic-ray proton background. The positron flux measured in space by AMS falls with a power law which unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several theoretical models try to explain these phenomena, and a purer measurement of positrons at higher energies is needed to help test them. The currently used methods to reject the proton background at high energies involve extrapolating shower features from the ECAL to use as inputs for boosted decision tree and likelihood classifiers. We present a new approach for particle identification with the AMS ECAL using deep learning (DL). By taking the energy deposition within all the ECAL cells as an input and treating them as pixels in an image-like format, we train an MLP, a CNN, and multiple ResNets and Convolutional vision Transformers (CvTs) as shower classifiers. Proton rejection performance is evaluated using Monte Carlo (MC) events and ISS data separately. For MC, using events with a reconstructed energy between 0.2 - 2 TeV, at 90% electron accuracy, the proton rejection power of our CvT model is more than 5 times that of the other DL models. Similarly, for ISS data with a reconstructed energy between 50 - 70 GeV, the proton rejection power of our CvT model is more than 2.5 times that of the other DL models.",
        "subjects": [
            "hep-ex",
            "cs.LG"
        ],
        "comment": "19 pages, 11 Figures"
    },
    {
        "paper id": "2402.16297",
        "abstract url": "https://arxiv.org/abs/2402.16297",
        "title": "Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation. Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16300",
        "abstract url": "https://arxiv.org/abs/2402.16300",
        "title": "Conformalized Selective Regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the \"reject option,\" allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper comparison of selective regression approaches. Via an extensive experimental approach, we demonstrate how our proposed approach, conformalized selective regression, demonstrates an advantage over multiple state-of-the-art baselines.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16910",
        "abstract url": "https://arxiv.org/abs/2402.16910",
        "title": "NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We present a neuro-symbolic (NeSy) workflow combining a symbolic-based learning technique with a large language model (LLM) agent to generate synthetic data for code comment classification in the C programming language. We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation and increases the performance of classical machine learning models on the code comment classification task. Our best model, a Neural Network, achieves a Macro-F1 score of 91.412% with an increase of 1.033% after data augmentation.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": "18 pages, 4 figures, forthcoming chapter in the book Generative Artificial Intelligence for Code - Impact of Large Language Models on Code Generation and Summarization in the book series Transactions on Computer Systems and Networks, Springer"
    },
    {
        "paper id": "2402.18595",
        "abstract url": "https://arxiv.org/abs/2402.18595",
        "title": "EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks (DNNs) have achieved great breakthroughs in many fields such as image classification and natural language processing. However, the execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC) operations on hardware and thus incurs a large power consumption. To address this challenge, we propose a novel digital MAC design based on encoding. In this new design, the multipliers are replaced by simple logic gates to project the results onto a wide bit representation. These bits carry individual position weights, which can be trained for specific neural networks to enhance inference accuracy. The outputs of the new multipliers are added by bit-wise weighted accumulation and the accumulation results are compatible with existing computing platforms accelerating neural networks with either uniform or non-uniform quantization. Since the multiplication function is replaced by simple logic projection, the critical paths in the resulting circuits become much shorter. Correspondingly, pipelining stages in the MAC array can be reduced, leading to a significantly smaller area as well as a better power efficiency. The proposed design has been synthesized and verified by ResNet18-Cifar10, ResNet20-Cifar100 and ResNet50-ImageNet. The experimental results confirmed the reduction of circuit area by up to 79.63% and the reduction of power consumption of executing DNNs by up to 70.18%, while the accuracy of the neural networks can still be well maintained.",
        "subjects": [
            "cs.AR",
            "cs.CE",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00814",
        "abstract url": "https://arxiv.org/abs/2403.00814",
        "title": "Gender Biased Legal Case Retrieval System on Users' Decision Process",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "In the last decade, legal case search has become an important part of a legal practitioner's work. During legal case search, search engines retrieval a number of relevant cases from huge amounts of data and serve them to users. However, it is uncertain whether these cases are gender-biased and whether such bias has impact on user perceptions. We designed a new user experiment framework to simulate the judges' reading of relevant cases. 72 participants with backgrounds in legal affairs invited to conduct the experiment. Participants were asked to simulate the role of the judge in conducting a legal case search on 3 assigned cases and determine the sentences of the defendants in these cases. Gender of the defendants in both the task and relevant cases was edited to statistically measure the effect of gender bias in the legal case search results on participants' perceptions. The results showed that gender bias in the legal case search results did not have a significant effect on judges' perceptions.",
        "subjects": [
            "cs.IR",
            "cs.CY",
            "cs.HC"
        ],
        "comment": "10pages, in Chinese language. Accepted by CCIR 2023"
    },
    {
        "paper id": "2404.16847",
        "abstract url": "https://arxiv.org/abs/2404.16847",
        "title": "State-of-the-Art Approaches to Enhancing Privacy Preservation of Machine Learning Datasets: A Survey",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper examines the evolving landscape of machine learning (ML) and its profound impact across various sectors, with a special focus on the emerging field of Privacy-preserving Machine Learning (PPML). As ML applications become increasingly integral to industries like telecommunications, financial technology, and surveillance, they raise significant privacy concerns, necessitating the development of PPML strategies. The paper highlights the unique challenges in safeguarding privacy within ML frameworks, which stem from the diverse capabilities of potential adversaries, including their ability to infer sensitive information from model outputs or training data. We delve into the spectrum of threat models that characterize adversarial intentions, ranging from membership and attribute inference to data reconstruction. The paper emphasizes the importance of maintaining the confidentiality and integrity of training data, outlining current research efforts that focus on refining training data to minimize privacy-sensitive information and enhancing data processing techniques to uphold privacy. Through a comprehensive analysis of privacy leakage risks and countermeasures in both centralized and collaborative learning settings, this paper aims to provide a thorough understanding of effective strategies for protecting ML training data against privacy intrusions. It explores the balance between data privacy and model utility, shedding light on privacy-preserving techniques that leverage cryptographic methods, Differential Privacy, and Trusted Execution Environments. The discussion extends to the application of these techniques in sensitive domains, underscoring the critical role of PPML in ensuring the privacy and security of ML systems.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16006",
        "abstract url": "https://arxiv.org/abs/2402.16006",
        "title": "From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models. We conducted experiments on LLMs such as LLaMa2, Vicuna and using the Advbench dataset's harmful instructions. The results indicate that our method achieves a much better attack success rate to existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini. As a result, the prompts generated through our method exhibit enriched semantic diversity, which potentially provides more adversarial examples for LLM defense methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16012",
        "abstract url": "https://arxiv.org/abs/2402.16012",
        "title": "Deep Contrastive Graph Learning with Clustering-Oriented Guidance",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering. To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN. Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph. To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering. Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features. On this basis, feature-level contrastive learning is introduced to enhance the discriminative capacity, and the relationship between samples and centroids is employed as the clustering-oriented guidance. Afterward, a two-branch graph learning mechanism is designed to extract the local and global structural relationships, which are further embedded into a unified graph under the cluster-level contrastive guidance. Experimental results on several benchmark datasets demonstrate the superiority of DCGL against state-of-the-art algorithms.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accept at AAAI24"
    },
    {
        "paper id": "2402.16024",
        "abstract url": "https://arxiv.org/abs/2402.16024",
        "title": "HiGPT: Heterogeneous Graph Language Model",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the \"pre-train\" and \"fine-tune\" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: \"Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we propose HiGPT, a general large graph model with Heterogeneous graph instruction-tuning paradigm. Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets. To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation. We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions. Through comprehensive evaluations, our proposed framework demonstrates exceptional performance in terms of generalization performance.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16029",
        "abstract url": "https://arxiv.org/abs/2402.16029",
        "title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "27pages, 15 tables"
    },
    {
        "paper id": "2402.16033",
        "abstract url": "https://arxiv.org/abs/2402.16033",
        "title": "Diving Deep into Regions: Exploiting Regional Information Transformer for Single Image Deraining",
        "rating": "0",
        "keywords": [
            [
                "Deraining"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transformer-based Single Image Deraining (SID) methods have achieved remarkable success, primarily attributed to their robust capability in capturing long-range interactions. However, we've noticed that current methods handle rain-affected and unaffected regions concurrently, overlooking the disparities between these areas, resulting in confusion between rain streaks and background parts, and inabilities to obtain effective interactions, ultimately resulting in suboptimal deraining outcomes. To address the above issue, we introduce the Region Transformer (Regformer), a novel SID method that underlines the importance of independently processing rain-affected and unaffected regions while considering their combined impact for high-quality image reconstruction. The crux of our method is the innovative Region Transformer Block (RTB), which integrates a Region Masked Attention (RMA) mechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention selection of rain-affected and unaffected regions and local modeling of mixed scales. The RMA generates attention maps tailored to these two regions and their interactions, enabling our model to capture comprehensive features essential for rain removal. To better recover high-frequency textures and capture more local details, we develop the MGFB as a compensation module to complete local mixed scale modeling. Extensive experiments demonstrate that our model reaches state-of-the-art performance, significantly improving the image deraining quality. Our code and trained models are publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16036",
        "abstract url": "https://arxiv.org/abs/2402.16036",
        "title": "Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving",
        "rating": "0",
        "keywords": [
            [
                "Autonomous Driving",
                "Trajectory",
                "Vehicle"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, the expansion of internet technology and advancements in automation have brought significant attention to autonomous driving technology. Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have progressively introduced products ranging from assisted-driving vehicles to semi-autonomous vehicles. However, this period has also witnessed several traffic safety incidents involving self-driving vehicles. For instance, in March 2016, a Google self-driving car was involved in a minor collision with a bus. At the time of the accident, the autonomous vehicle was attempting to merge into the right lane but failed to dynamically respond to the real-time environmental information during the lane change. It incorrectly assumed that the approaching bus would slow down to avoid it, leading to a low-speed collision with the bus. This incident highlights the current technological shortcomings and safety concerns associated with autonomous lane-changing behavior, despite the rapid advancements in autonomous driving technology. Lane-changing is among the most common and hazardous behaviors in highway driving, significantly impacting traffic safety and flow. Therefore, lane-changing is crucial for traffic safety, and accurately predicting drivers' lane change intentions can markedly enhance driving safety. This paper introduces a deep learning-based prediction method for autonomous driving lane change behavior, aiming to facilitate safe lane changes and thereby improve road safety.",
        "subjects": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16117",
        "abstract url": "https://arxiv.org/abs/2402.16117",
        "title": "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis",
        "rating": "0",
        "keywords": [
            [
                "robotics",
                "navigation"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16123",
        "abstract url": "https://arxiv.org/abs/2402.16123",
        "title": "InstructEdit: Instruction-based Knowledge Editing for Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approaches encounter issues with limited generalizability across tasks, necessitating one distinct editor for each task, significantly hindering the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdit consistently surpass previous strong baselines. To further investigate the underlying mechanisms of instruction-based knowledge editing, we analyze the principal components of the editing gradient directions, which unveils that instructions can help control optimization direction with stronger OOD generalization. Code and datasets are available in https://github.com/zjunlp/EasyEdit.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "IJCAI 2024; the project website is at https://www.zjukg.org/project/InstructEdit/"
    },
    {
        "paper id": "2402.16192",
        "abstract url": "https://arxiv.org/abs/2402.16192",
        "title": "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "37 pages"
    },
    {
        "paper id": "2402.16255",
        "abstract url": "https://arxiv.org/abs/2402.16255",
        "title": "Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models",
        "rating": "0",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: \\textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the \"Assembled Projection Heads\" (APH) method for enhancing the reliability of federated models. By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted fine-tuning on locally available data under varying learning rates. Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging. We evaluate the effectiveness of the proposed APH method across three prominent federated benchmarks. Experimental results validate the efficacy of APH in model calibration and uncertainty estimation. Notably, APH can be seamlessly integrated into various federated approaches but only requires less than 30\\% additional computation cost for 100$\\times$ inferences within large models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted in AAAI-24"
    },
    {
        "paper id": "2402.16267",
        "abstract url": "https://arxiv.org/abs/2402.16267",
        "title": "Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space",
        "rating": "0",
        "keywords": [
            [
                "Infrared"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities. Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically. As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited. In this paper, we first propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance. For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the multi-modal embedding space using CLIP. A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors to represent the fusion objective and input image modalities. Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via supervised training. Experiments show that our method can obtain much better fusion results than existing techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16911",
        "abstract url": "https://arxiv.org/abs/2402.16911",
        "title": "Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune",
        "rating": "0",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Performance degradation owing to data heterogeneity and low output interpretability are the most significant challenges faced by federated learning in practical applications. Personalized federated learning diverges from traditional approaches, as it no longer seeks to train a single model, but instead tailors a unique personalized model for each client. However, previous work focused only on personalization from the perspective of neural network parameters and lack of robustness and interpretability. In this work, we establish a novel framework for personalized federated learning, incorporating Bayesian methodology which enhances the algorithm's ability to quantify uncertainty. Furthermore, we introduce normalizing flow to achieve personalization from the parameter posterior perspective and theoretically analyze the impact of normalizing flow on out-of-distribution (OOD) detection for Bayesian neural networks. Finally, we evaluated our approach on heterogeneous datasets, and the experimental results indicate that the new algorithm not only improves accuracy but also outperforms the baseline significantly in OOD detection due to the reliable output of the Bayesian approach.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16914",
        "abstract url": "https://arxiv.org/abs/2402.16914",
        "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \\textbf{D}ecomposition and \\textbf{R}econstruction framework for jailbreak \\textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts' synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0\\% on GPT-4 with merely 15 queries surpassed previous art by 33.1\\%. The project is available at https://github.com/xirui-li/DrAttack.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.07901",
        "abstract url": "https://arxiv.org/abs/2403.07901",
        "title": "MIP: CLIP-based Image Reconstruction from PEFT Gradients",
        "rating": "0",
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "Federated Learning"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Contrastive Language-Image Pre-training (CLIP) model, as an effective pre-trained multimodal neural network, has been widely used in distributed machine learning tasks, especially Federated Learning (FL). Typically, CLIP-based FL adopts Parameter-Efficient Fine-Tuning (PEFT) for model training, which only fine-tunes adapter parameters or soft prompts rather than the full parameters. Although PEFT is different from the traditional training mode, in this paper, we theoretically analyze that the gradients of adapters or soft prompts can still be used to perform image reconstruction attacks. Based on our theoretical analysis, we propose Multm-In-Parvo (MIP), a proprietary reconstruction attack method targeting CLIP-based distributed machine learning architecture. Specifically, MIP can reconstruct CLIP training images according to the gradients of soft prompts or an adapter. In addition, MIP includes a label prediction strategy to accelerate convergence and an inverse gradient estimation mechanism to avoid the vanishing gradient problem on the text encoder. Experimental results show that MIP can effectively reconstruct training images according to the gradients of soft prompts or adapters of CLIP models.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16013",
        "abstract url": "https://arxiv.org/abs/2402.16013",
        "title": "Semi-supervised Open-World Object Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Conventional open-world object detection (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown objects when introduced with labels in the subsequent tasks. However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages. Such reliance on run-time makes this formulation less realistic in a real-world deployment. To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner. We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting. Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the object query representations between the original and augmented images to leverage the large unlabeled and few labeled data. We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder object queries to capture object-specific information. We demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing object detection, proposing carefully curated splits and baseline performance evaluations. Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach. Our source code, models and splits are available here - https://github.com/sahalshajim/SS-OWFormer",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to AAAI 2024 (Main Track)"
    },
    {
        "paper id": "2402.16020",
        "abstract url": "https://arxiv.org/abs/2402.16020",
        "title": "A Step-by-step Introduction to the Implementation of Automatic Differentiation",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Automatic differentiation is a key component in deep learning. This topic is well studied and excellent surveys such as Baydin et al. (2018) have been available to clearly describe the basic concepts. Further, sophisticated implementations of automatic differentiation are now an important part of popular deep learning frameworks. However, it is difficult, if not impossible, to directly teach students the implementation of existing systems due to the complexity. On the other hand, if the teaching stops at the basic concept, students fail to sense the realization of an implementation. For example, we often mention the computational graph in teaching automatic differentiation, but students wonder how to implement and use it. In this document, we partially fill the gap by giving a step by step introduction of implementing a simple automatic differentiation system. We streamline the mathematical concepts and the implementation. Further, we give the motivation behind each implementation detail, so the whole setting becomes very natural.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "17 pages, 15 figures"
    },
    {
        "paper id": "2402.16059",
        "abstract url": "https://arxiv.org/abs/2402.16059",
        "title": "Gradient-enhanced deep Gaussian processes for multifidelity modelling",
        "rating": "-0.5",
        "keywords": [
            [
                "vehicle",
                "flight"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. Principally, this work extends deep GPs to incorporate gradient data. We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aerodynamic coefficients of a hypersonic flight vehicle over a range of flight conditions and geometries. In both examples, the gradient-enhanced deep GP outperforms a gradient-enhanced linear GP model and their non-gradient-enhanced counterparts.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16068",
        "abstract url": "https://arxiv.org/abs/2402.16068",
        "title": "ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics",
                "Robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects. Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions. However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics. To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions. An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection. ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Accepted by the \"Causal-HRI: Causal Learning for Human-Robot Interaction\" workshop at the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
        "paper id": "2402.16075",
        "abstract url": "https://arxiv.org/abs/2402.16075",
        "title": "Don't Start from Scratch: Behavioral Refinement via Interpolant-based Policy Diffusion",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to mitigate the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging simulation benchmarks and on real robots, BRIDGER outperforms state-of-the-art diffusion policies. We provide further analysis on design considerations when applying BRIDGER.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16078",
        "abstract url": "https://arxiv.org/abs/2402.16078",
        "title": "Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted as a full conference paper in the International Conference on Learning Representations 2024"
    },
    {
        "paper id": "2402.16091",
        "abstract url": "https://arxiv.org/abs/2402.16091",
        "title": "Bayesian Neural Network For Personalized Federated Learning Parameter Selection",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field. Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data. One of such approach involves personalizing specific layers of neural networks. However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting. In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization. To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters. Finally, we validate our algorithm's efficacy on several real-world datasets, demonstrating that our proposed approach outperforms existing baselines.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16157",
        "abstract url": "https://arxiv.org/abs/2402.16157",
        "title": "Consensus learning: A novel decentralised ensemble learning paradigm",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability. This work introduces a novel distributed machine learning paradigm -- \\emph{consensus learning} -- which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems. These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol. Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism. We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithms. The discussion is supplemented by various numerical simulations, which describe the robustness of the algorithms against Byzantine participants.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "27 pages plus appendices, 8 figures"
    },
    {
        "paper id": "2402.16158",
        "abstract url": "https://arxiv.org/abs/2402.16158",
        "title": "Distribution-Free Fair Federated Learning with Small Samples",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for both fairness and accuracy, and our experimental results further provide robust empirical validation for our proposed method.",
        "subjects": [
            "stat.ML",
            "cs.CY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16240",
        "abstract url": "https://arxiv.org/abs/2402.16240",
        "title": "High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model. Different from previous \"cascaded architectures\" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual enhancement between network and text signals in diverse granularities. Moreover, we show that existing contrastive objective learns the low-frequency component of the augmentation graph and propose a high-frequency component (HFC)-aware contrastive learning objective that makes the learned embeddings more distinctive. Extensive experiments on six real-world benchmarks substantiate the efficacy of our proposed approach. In addition, theoretical analysis and item embedding visualization provide insights into our model interoperability.",
        "subjects": [
            "cs.IR",
            "cs.SI"
        ],
        "comment": "Accepted by WWW 2024"
    },
    {
        "paper id": "2402.16912",
        "abstract url": "https://arxiv.org/abs/2402.16912",
        "title": "An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As cyber-attacks become more sophisticated, improving the robustness of Machine Learning (ML) models must be a priority for enterprises of all sizes. To reliably compare the robustness of different ML models for cyber-attack detection in enterprise computer networks, they must be evaluated in standardized conditions. This work presents a methodical adversarial robustness benchmark of multiple decision tree ensembles with constrained adversarial examples generated from standard datasets. The robustness of regularly and adversarially trained RF, XGB, LGBM, and EBM models was evaluated on the original CICIDS2017 dataset, a corrected version of it designated as NewCICIDS, and the HIKARI dataset, which contains more recent network traffic. NewCICIDS led to models with a better performance, especially XGB and EBM, but RF and LGBM were less robust against the more recent cyber-attacks of HIKARI. Overall, the robustness of the models to adversarial cyber-attack examples was improved without their generalization to regular traffic being affected, enabling a reliable detection of suspicious activity without costly increases of false alarms.",
        "subjects": [
            "cs.CR",
            "cs.LG",
            "cs.NI"
        ],
        "comment": "15 pages, 8 tables, 2 figures, FPS 2023 conference"
    },
    {
        "paper id": "2402.16915",
        "abstract url": "https://arxiv.org/abs/2402.16915",
        "title": "More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Trajectory representation learning plays a pivotal role in supporting various downstream tasks. Traditional methods in order to filter the noise in GPS trajectories tend to focus on routing-based methods used to simplify the trajectories. However, this approach ignores the motion details contained in the GPS data, limiting the representation capability of trajectory representation learning. To fill this gap, we propose a novel representation learning framework that Joint GPS and Route Modelling based on self-supervised technology, namely JGRM. We consider GPS trajectory and route as the two modes of a single movement observation and fuse information through inter-modal information interaction. Specifically, we develop two encoders, each tailored to capture representations of route and GPS trajectories respectively. The representations from the two modalities are fed into a shared transformer for inter-modal information interaction. Eventually, we design three self-supervised tasks to train the model. We validate the effectiveness of the proposed method on two real datasets based on extensive experiments. The experimental results demonstrate that JGRM outperforms existing methods in both road segment representation and trajectory representation tasks. Our source code is available at Anonymous Github.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16001",
        "abstract url": "https://arxiv.org/abs/2402.16001",
        "title": "Cross-Resolution Land Cover Classification Using Outdated Products and Transformers",
        "rating": "-1",
        "keywords": [
            [
                "Remote sensing",
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues. Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas. Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods. In this paper, we propose a Transformerbased weakly supervised method for cross-resolution land cover classification using outdated data. First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like Transformer based on a reverse difference mechanism (RDM) using dynamic sparse attention. Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT). Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products. By introducing a weakly supervised loss with weights and employing unsupervised loss, the RDM-based U-Net-like Transformer was trained. Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method. The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017. The results show the superiority of the proposed method compared to state-of-the-art methods. The code is available at https://github.com/yu-ni1989/ANLC-Former.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16008",
        "abstract url": "https://arxiv.org/abs/2402.16008",
        "title": "Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "disease"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes. However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well. While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data. Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples. In this paper, we approach XAI with an innovative {\\em model debugging} methodology realized through Jacobian Saliency Map (JSM). To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemming from the intricate nature of its progression. We introduce an interpretable, multimodal model for AD classification over its multi-stage progression, incorporating JSM as a modality-agnostic tool that provides insights into volumetric changes indicative of brain abnormalities. Our extensive evaluation including ablation study manifests the efficacy of using JSM for model debugging and interpretation, while significantly enhancing model accuracy as well.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), May 2024, Taiwan"
    },
    {
        "paper id": "2402.16016",
        "abstract url": "https://arxiv.org/abs/2402.16016",
        "title": "Complexity of Manipulation and Bribery in Premise-Based Judgment Aggregation with Simple Formulas",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Judgment aggregation is a framework to aggregate individual opinions on multiple, logically connected issues into a collective outcome. These opinions are cast by judges, which can be for example referees, experts, advisors or jurors, depending on the application and context. It is open to manipulative attacks such as \\textsc{Manipulation} where judges cast their judgments strategically. Previous works have shown that most computational problems corresponding to these manipulative attacks are \\NP-hard. This desired computational barrier, however, often relies on formulas that are either of unbounded size or of complex structure. We revisit the computational complexity for various \\textsc{Manipulation} and \\textsc{Bribery} problems in premise-based judgment aggregation, now focusing on simple and realistic formulas. We restrict all formulas to be clauses that are (positive) monotone, Horn-clauses, or have bounded length. For basic variants of \\textsc{Manipulation}, we show that these restrictions make several variants, which were in general known to be \\NP-hard, polynomial-time solvable. Moreover, we provide a P vs.\\ NP dichotomy for a large class of clause restrictions (generalizing monotone and Horn clauses) by showing a close relationship between variants of \\textsc{Manipulation} and variants of \\textsc{Satisfiability}. For Hamming distance based \\textsc{Manipulation}, we show that \\NP-hardness even holds for positive monotone clauses of length three, but the problem becomes polynomial-time solvable for positive monotone clauses of length two. For \\textsc{Bribery}, we show that \\NP-hardness even holds for positive monotone clauses of length two, but it becomes polynomial-time solvable for the same clause set if there is a constant budget.",
        "subjects": [
            "cs.GT",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16035",
        "abstract url": "https://arxiv.org/abs/2402.16035",
        "title": "Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations",
        "rating": "-1",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks. In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations. Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advantages of pre-trained models in understanding complex user intentions and improving the quality of recommendations. In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy. Ultimately, the paper points out that the application of Transformer structural pre-training models in e-commerce has not only driven technological innovation, but also brought substantial benefits to merchants and consumers, and looking forward, these models will continue to play a key role in e-commerce and beyond.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16040",
        "abstract url": "https://arxiv.org/abs/2402.16040",
        "title": "EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "Health",
                "healthcare",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models showed that their scores on EHRNoteQA correlate more closely with their performance in addressing real-world medical questions evaluated by clinicians than their scores from other LLM benchmarks. This underscores the significance of EHRNoteQA in evaluating LLMs for medical applications and highlights its crucial role in facilitating the integration of LLMs into healthcare systems. The dataset will be made available to the public under PhysioNet credential access, promoting further research in this vital field.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2402.16045",
        "abstract url": "https://arxiv.org/abs/2402.16045",
        "title": "Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios",
        "rating": "-1",
        "keywords": [
            [
                "robot",
                "robotic manipulation"
            ]
        ],
        "abstract": "In this work, we delve into the intricate synergy among non-prehensile actions like pushing, and prehensile actions such as grasping and throwing, within the domain of robotic manipulation. We introduce an innovative approach to learning these synergies by leveraging model-free deep reinforcement learning. The robot's workflow involves detecting the pose of the target object and the basket at each time step, predicting the optimal push configuration to isolate the target object, determining the appropriate grasp configuration, and inferring the necessary parameters for an accurate throw into the basket. This empowers robots to skillfully reconfigure cluttered scenarios through pushing, creating space for collision-free grasping actions. Simultaneously, we integrate throwing behavior, showcasing how this action significantly extends the robot's operational reach. Ensuring safety, we developed a simulation environment in Gazebo for robot training, applying the learned policy directly to our real robot. Notably, this work represents a pioneering effort to learn the synergy between pushing, grasping, and throwing actions. Extensive experimentation in both simulated and real-robot scenarios substantiates the effectiveness of our approach across diverse settings. Our approach achieves a success rate exceeding 80\\% in both simulated and real-world scenarios. A video showcasing our experiments is available online at: https://youtu.be/q1l4BJVDbRw",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This paper has been accepted at the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)"
    },
    {
        "paper id": "2402.16082",
        "abstract url": "https://arxiv.org/abs/2402.16082",
        "title": "Modeling Point Uncertainty in Radar SLAM",
        "rating": "-1",
        "keywords": [
            [
                "Radar",
                "SLAM"
            ]
        ],
        "abstract": "While visual and laser-based simultaneous localization and mapping (SLAM) techniques have gained significant attention, radar SLAM remains a robust option for challenging conditions. This paper aims to improve the performance of radar SLAM by modeling point uncertainty. The basic SLAM system is a radar-inertial odometry (RIO) system that leverages velocity-aided radar points and high-frequency inertial measurements. We first propose to model the uncertainty of radar points in polar coordinates by considering the nature of radar sensing. Then in the SLAM system, the uncertainty model is designed into the data association module and is incorporated to weight the motion estimation. Real-world experiments on public and self-collected datasets validate the effectiveness of the proposed models and approaches. The findings highlight the potential of incorporating radar point uncertainty modeling to improve the radar SLAM system in adverse environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 8 figures"
    },
    {
        "paper id": "2402.16087",
        "abstract url": "https://arxiv.org/abs/2402.16087",
        "title": "How to Privately Tune Hyperparameters in Federated Learning? Insights from a Benchmark Study",
        "rating": "-1",
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "In this paper, we address the problem of privacy-preserving hyperparameter (HP) tuning for cross-silo federated learning (FL). We first perform a comprehensive measurement study that benchmarks various HP strategies suitable for FL. Our benchmarks show that the optimal parameters of the FL server, e.g., the learning rate, can be accurately and efficiently tuned based on the HPs found by each client on its local data. We demonstrate that HP averaging is suitable for iid settings, while density-based clustering can uncover the optimal set of parameters in non-iid ones. Then, to prevent information leakage from the exchange of the clients' local HPs, we design and implement PrivTuna, a novel framework for privacy-preserving HP tuning using multiparty homomorphic encryption. We use PrivTuna to implement privacy-preserving federated averaging and density-based clustering, and we experimentally evaluate its performance demonstrating its computation/communication efficiency and its precision in tuning hyperparameters.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16114",
        "abstract url": "https://arxiv.org/abs/2402.16114",
        "title": "Model-based Manipulation of Deformable Objects with Non-negligible Dynamics as Shape Regulation",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Model-based manipulation of deformable objects has traditionally dealt with objects in the quasi-static regimes, either because they are extremely lightweight/small or constrained to move very slowly. On the contrary, soft robotic research has made considerable strides toward general modeling and control - despite soft robots and deformable linear objects being very similar from a mechanical standpoint. In this work, we leverage these recent results to develop a fully dynamic framework of slender deformable objects grasped at one of their ends by a robotic manipulator. We introduce a dynamic model of this system using functional strain parameterizations and describe the manipulation challenge as a regulation control problem. This enables us to define a fully model-based control architecture, for which we can prove analytically closed-loop stability and provide sufficient conditions for steady state convergence to the desired manipulation state. The nature of this work is intended to be markedly experimental. We propose an extensive experimental validation of the proposed ideas. For that, we use a 7-DoF robot tasked with the goal of positioning the distal end of six different electric cables, moving on a plane, in a given position and orientation in space.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16132",
        "abstract url": "https://arxiv.org/abs/2402.16132",
        "title": "LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting",
        "rating": "-1",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "9 pages, 4 figures, 3 tables, 2 page references, 2 page appendix"
    },
    {
        "paper id": "2402.16142",
        "abstract url": "https://arxiv.org/abs/2402.16142",
        "title": "From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This groundbreaking study explores the expanse of Large Language Models (LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) across varied domains ranging from technology, finance, healthcare to education. Despite their established prowess in Natural Language Processing (NLP), these LLMs have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management. This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of LLMs' utility in diverse domains, recognizes the research gaps and realms where the potential of LLMs is yet to be harnessed. This study uncovers innovative ways in which LLMs can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire future researches and applications in the said avenues.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16150",
        "abstract url": "https://arxiv.org/abs/2402.16150",
        "title": "Effective MSO-Definability for Tree-width Bounded Models of an Inductive Separation Logic of Relations",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A class of graph languages is definable in Monadic Second-Order logic (MSO) if and only if it consists of sets of models of MSO formul\u00e6. If, moreover, there is a computable bound on the tree-widths of the graphs in each such set, the satisfiability and entailment problems are decidable, by Courcelle's Theorem. This motivates the comparison of other graph logics to MSO. In this paper, we consider the MSO definability of a Separation Logic of Relations (SLR) that describes simple hyper-graphs, in which each sequence of vertices is attached to at most one edge with a given label. Our logic SLR uses inductive predicates whose recursive definitions consist of existentially quantified separated conjunctions of relation and predicate atoms. The main contribution of this paper is an expressive fragment of SLR that describes bounded tree-width sets of graphs which can, moreover, be effectively translated into MSO.",
        "subjects": [
            "cs.LO",
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16164",
        "abstract url": "https://arxiv.org/abs/2402.16164",
        "title": "Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, self-supervision has drawn a lot of attention in remote sensing society due to its ability to reduce the demand of exact labels in supervised deep learning model training. Self-supervision methods generally utilize image-level information to pretrain models in an unsupervised fashion. Though these pretrained encoders show effectiveness in many downstream tasks, their performance on segmentation tasks is often not as good as that on classification tasks. On the other hand, many easily available label sources (e.g., automatic labeling tools and land cover land use products) exist, which can provide a large amount of noisy labels for segmentation model training. In this work, we propose to explore the under-exploited potential of noisy labels for segmentation task specific pretraining, and exam its robustness when confronted with mismatched categories and different decoders during fine-tuning. Specifically, we inspect the impacts of noisy labels on different layers in supervised model training to serve as the basis of our work. Experiments on two datasets indicate the effectiveness of task specific supervised pretraining with noisy labels. The findings are expected to shed light on new avenues for improving the accuracy and versatility of pretraining strategies for remote sensing image segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16182",
        "abstract url": "https://arxiv.org/abs/2402.16182",
        "title": "MoodCapture: Depression Detection Using In-the-Wild Smartphone Images",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "MoodCapture presents a novel approach that assesses depression based on images automatically captured from the front-facing camera of smartphones as people go about their daily lives. We collect over 125,000 photos in the wild from N=177 participants diagnosed with major depressive disorder for 90 days. Images are captured naturalistically while participants respond to the PHQ-8 depression survey question: \\textit{``I have felt down, depressed, or hopeless''}. Our analysis explores important image attributes, such as angle, dominant colors, location, objects, and lighting. We show that a random forest trained with face landmarks can classify samples as depressed or non-depressed and predict raw PHQ-8 scores effectively. Our post-hoc analysis provides several insights through an ablation study, feature importance analysis, and bias assessment. Importantly, we evaluate user concerns about using MoodCapture to detect depression based on sharing photos, providing critical insights into privacy concerns that inform the future design of in-the-wild image-based mental health assessment tools.",
        "subjects": [
            "cs.HC",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16201",
        "abstract url": "https://arxiv.org/abs/2402.16201",
        "title": "Honeybee: Decentralized Peer Sampling with Verifiable Random Walks for Blockchain Data Sharding",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Data sharding - in which block data is sharded without sharding compute - is at the present the favored approach for scaling Ethereum. A key challenge toward implementing data sharding is verifying whether the entirety of a block's data is available in the network (across its shards). A central technique proposed to conduct this verification uses erasure coded blocks and is called data availability sampling (DAS). While the high-level protocol details of DAS has been well discussed in the community, discussions around how such a protocol will be implemented at the peer-to-peer layer are lacking. We identify random sampling of nodes as a fundamental primitive necessary to carry out DAS and present Honeybee, a decentralized algorithm for sampling node that uses verifiable random walks. Honeybee is secure against attacks even in the presence of a large number of Byzantine nodes (e.g., 50% of the network). We evaluate Honeybee through experiments and show that the quality of sampling achieved by Honeybee is significantly better compared to the state-of-the-art. Our proposed algorithm has implications for DAS functions in both full nodes and light nodes.",
        "subjects": [
            "cs.NI",
            "cs.CR",
            "cs.DC",
            "cs.DS",
            "cs.MA"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2402.16205",
        "abstract url": "https://arxiv.org/abs/2402.16205",
        "title": "Enhanced Graph Pattern Matching",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Pattern matching queries on strings can be solved in linear time by Knuth-Morris-Pratt (KMP) algorithm. In 1973, Weiner introduced the suffix tree of a string [FOCS 1973] and showed that the seemingly more difficult problem of computing matching statistics can also be solved in liner time. Pattern matching queries on graphs are inherently more difficult: under the Orthogonal Vector hypothesis, the graph pattern matching problem cannot be solved in subquadratic time [TALG 2023]. The complexity of graph pattern matching can be parameterized by the topological complexity of the considered graph, which is captured by a parameter $ p $ [JACM 2023]. In this paper, we show that, as in the string setting, computing matching statistics on graph is as difficult as solving standard pattern matching queries. To this end, we introduce a notion of longest common prefix (LCP) array for arbitrary graphs.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16211",
        "abstract url": "https://arxiv.org/abs/2402.16211",
        "title": "HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs' hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models' performance ranged between 3% and 11%, and evaluator agents demonstrated a 6% error rate in hallucination prediction. The proposed framework provides opportunities to test and improve LLMs. Additionally, it has the potential to generate benchmarking datasets tailored to specific domains, such as law, health, and finance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EACL SRW 2024 Camera Ready"
    },
    {
        "paper id": "2402.16215",
        "abstract url": "https://arxiv.org/abs/2402.16215",
        "title": "Branch-depth is minor closure of contraction-deletion-depth",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "The notion of branch-depth for matroids was introduced by DeVos, Kwon and Oum as the matroid analogue of the tree-depth of graphs. The contraction-deletion-depth, another tree-depth like parameter of matroids, is the number of recursive steps needed to decompose a matroid by contractions and deletions to single elements. Any matroid with contraction-deletion-depth at most d has branch-depth at most d. However, the two notions are not functionally equivalent as contraction-deletion-depth of matroids with branch-depth two can be arbitrarily large. We show that the two notions are functionally equivalent for representable matroids when minor closures are considered. Namely, an F-representable matroid has small branch-depth if and only if it is a minor of an F-representable matroid with small contraction-deletion-depth. This implies that any class of F-representable matroids has bounded branch-depth if and only if it is a subclass of the minor closure of a class of F-representable matroids with bounded contraction-deletion-depth.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16221",
        "abstract url": "https://arxiv.org/abs/2402.16221",
        "title": "Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "healthcare",
                "Tumor"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "This research presents a machine-learning approach for tumor detection in medical images using convolutional neural networks (CNNs). The study focuses on preprocessing techniques to enhance image features relevant to tumor detection, followed by developing and training a CNN model for accurate classification. Various image processing techniques, including Gaussian smoothing, bilateral filtering, and K-means clustering, are employed to preprocess the input images and highlight tumor regions. The CNN model is trained and evaluated on a dataset of medical images, with augmentation and data generators utilized to enhance model generalization. Experimental results demonstrate the effectiveness of the proposed approach in accurately detecting tumors in medical images, paving the way for improved diagnostic tools in healthcare.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "5 pages, 5 figures, utilizing convolutional neural networks and preprocessing methods for tumor detection in MRI images, featuring a detailed methodology section on image preprocessing, segmentation, and model training, with a comprehensive evaluation of model performance on the Figshare dataset using IEEE template"
    },
    {
        "paper id": "2402.16227",
        "abstract url": "https://arxiv.org/abs/2402.16227",
        "title": "Scaling Robust Optimization for Multi-Agent Robotic Systems: A Distributed Perspective",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "This paper presents a novel distributed robust optimization scheme for steering distributions of multi-agent systems under stochastic and deterministic uncertainty. Robust optimization is a subfield of optimization which aims in discovering an optimal solution that remains robustly feasible for all possible realizations of the problem parameters within a given uncertainty set. Such approaches would naturally constitute an ideal candidate for multi-robot control, where in addition to stochastic noise, there might be exogenous deterministic disturbances. Nevertheless, as these methods are usually associated with significantly high computational demands, their application to multi-agent robotics has remained limited. The scope of this work is to propose a scalable robust optimization framework that effectively addresses both types of uncertainties, while retaining computational efficiency and scalability. In this direction, we provide tractable approximations for robust constraints that are relevant in multi-robot settings. Subsequently, we demonstrate how computations can be distributed through an Alternating Direction Method of Multipliers (ADMM) approach towards achieving scalability and communication efficiency. Simulation results highlight the performance of the proposed algorithm in effectively handling both stochastic and deterministic uncertainty in multi-robot systems. The scalability of the method is also emphasized by showcasing tasks with up to 100 agents. The results of this work indicate the promise of blending robust optimization, distribution steering and distributed optimization towards achieving scalable, safe and robust multi-robot control.",
        "subjects": [
            "cs.RO",
            "eess.SY",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16242",
        "abstract url": "https://arxiv.org/abs/2402.16242",
        "title": "HSONet:A Siamese foreground association-driven hard case sample optimization network for high-resolution remote sensing image change detection",
        "rating": "-1",
        "keywords": [
            [
                "remote sensing"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the later training stages, further improvement of the models ability to determine changes relies on how well the change detection (CD) model learns hard cases; however, there are two additional challenges to learning hard case samples: (1) change labels are limited and tend to pointer only to foreground targets, yet hard case samples are prevalent in the background, which leads to optimizing the loss function focusing on the foreground targets and ignoring the background hard cases, which we call imbalance. (2) Complex situations, such as light shadows, target occlusion, and seasonal changes, induce hard case samples, and in the absence of both supervisory and scene information, it is difficult for the model to learn hard case samples directly to accurately obtain the feature representations of the change information, which we call missingness. We propose a Siamese foreground association-driven hard case sample optimization network (HSONet). To deal with this imbalance, we propose an equilibrium optimization loss function to regulate the optimization focus of the foreground and background, determine the hard case samples through the distribution of the loss values, and introduce dynamic weights in the loss term to gradually shift the optimization focus of the loss from the foreground to the background hard cases as the training progresses. To address this missingness, we understand hard case samples with the help of the scene context, propose the scene-foreground association module, use potential remote sensing spatial scene information to model the association between the target of interest in the foreground and the related context to obtain scene embedding, and apply this information to the feature reinforcement of hard cases. Experiments on four public datasets show that HSONet outperforms current state-of-the-art CD methods, particularly in detecting hard case samples.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "17 figures, 8 tables, 18 pages"
    },
    {
        "paper id": "2402.16260",
        "abstract url": "https://arxiv.org/abs/2402.16260",
        "title": "Distributed Finite-time Differentiator for Multi-agent Systems Under Directed Graph",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "This paper proposes a new distributed finite-time differentiator (DFD) for multi-agent systems (MAS) under directed graph, which extends the differentiator algorithm from the centralized case to the distributed case by only using relative/absolute position information. By skillfully constructing a Lyapunov function, the finite-time stability of the closed-loop system under DFD is proved. Inspired by the duality principle of control theory, a distributed continuous finite-time output consensus algorithm extended from DFD for a class of leader-follower MAS is provided, which not only completely suppresses disturbance, but also avoids chattering. Finally, several simulation examples are given to verify the effectiveness of the DFD.",
        "subjects": [
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16274",
        "abstract url": "https://arxiv.org/abs/2402.16274",
        "title": "Radar Anti-jamming Strategy Learning via Domain-knowledge Enhanced Online Convex Optimization",
        "rating": "-1",
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "The dynamic competition between radar and jammer systems presents a significant challenge for modern Electronic Warfare (EW), as current active learning approaches still lack sample efficiency and fail to exploit jammer's characteristics. In this paper, the competition between a frequency agile radar and a Digital Radio Frequency Memory (DRFM)-based intelligent jammer is considered. We introduce an Online Convex Optimization (OCO) framework designed to illustrate this adversarial interaction. Notably, traditional OCO algorithms exhibit suboptimal sample efficiency due to the limited information obtained per round. To address the limitations, two refined algorithms are proposed, utilizing unbiased gradient estimators that leverage the unique attributes of the jammer system. Sub-linear theoretical results on both static regret and universal regret are provided, marking a significant improvement in OCO performance. Furthermore, simulation results reveal that the proposed algorithms outperform common OCO baselines, suggesting the potential for effective deployment in real-world scenarios.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16298",
        "abstract url": "https://arxiv.org/abs/2402.16298",
        "title": "MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer",
        "rating": "-1",
        "keywords": [
            [
                "cancer",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis. In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors. Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct convolutional branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations. In this paper, we propose an innovative multi-view network exclusively based on transformers to address challenges in mammographic image classification. Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between views at the spatial feature map level. Furthermore, we conduct a comprehensive comparative analysis of the performance and effectiveness of transformer-based models under diverse settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is publicly available at https://github.com/prithuls/MV-Swin-T",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "4 pages, 2 figures"
    },
    {
        "paper id": "2402.17587",
        "abstract url": "https://arxiv.org/abs/2402.17587",
        "title": "Instance-aware Exploration-Verification-Exploitation for Instance ImageGoal Navigation",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to navigate to a specified object depicted by a goal image in an unexplored environment. The main challenge of this task lies in identifying the target object from different viewpoints while rejecting similar distractors. Existing ImageGoal Navigation methods usually adopt the simple Exploration-Exploitation framework and ignore the identification of specific instance during navigation. In this work, we propose to imitate the human behaviour of ``getting closer to confirm\" when distinguishing objects from a distance. Specifically, we design a new modular navigation framework named Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level image goal navigation. Our method allows for active switching among the exploration, verification, and exploitation actions, thereby facilitating the agent in making reasonable decisions under different situations. On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our method surpasses previous state-of-the-art work, with a classical segmentation model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00815",
        "abstract url": "https://arxiv.org/abs/2403.00815",
        "title": "RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "Health",
                "Clinical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \\url{https://github.com/ritaranx/RAM-EHR}.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "q-bio.OT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.00683",
        "abstract url": "https://arxiv.org/abs/2405.00683",
        "title": "Frequency-Guided U-Net: Leveraging Attention Filter Gates and Fast Fourier Transformation for Enhanced Medical Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "healthcare",
                "diagnosis"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Purpose Medical imaging diagnosis faces challenges, including low-resolution images due to machine artifacts and patient movement. This paper presents the Frequency-Guided U-Net (GFNet), a novel approach for medical image segmentation that addresses challenges associated with low-resolution images and inefficient feature extraction. Approach In response to challenges related to computational cost and complexity in feature extraction, our approach introduces the Attention Filter Gate. Departing from traditional spatial domain learning, our model operates in the frequency domain using FFT. A strategically placed weighted learnable matrix filters feature, reducing computational costs. FFT is integrated between up-sampling and down-sampling, mitigating issues of throughput, latency, FLOP, and enhancing feature extraction. Results Experimental outcomes shed light on model performance. The Attention Filter Gate, a pivotal component of GFNet, achieves competitive segmentation accuracy (Mean Dice: 0.8366, Mean IoU: 0.7962). Comparatively, the Attention Gate model surpasses others, with a Mean Dice of 0.9107 and a Mean IoU of 0.8685. The widely-used U-Net baseline demonstrates satisfactory performance (Mean Dice: 0.8680, Mean IoU: 0.8268). Conclusion his work introduces GFNet as an efficient and accurate method for medical image segmentation. By leveraging the frequency domain and attention filter gates, GFNet addresses key challenges of information loss, computational cost, and feature extraction limitations. This novel approach offers potential advancements for computer-aided diagnosis and other healthcare applications. Keywords: Medical Segmentation, Neural Networks,",
        "subjects": [
            "eess.IV"
        ],
        "comment": "29 pagges , 12 Figures"
    },
    {
        "paper id": "2402.15988",
        "abstract url": "https://arxiv.org/abs/2402.15988",
        "title": "Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation",
        "rating": "-1.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels. We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used currently by the research community. These new datasets offer significant values for FairGAD by providing realistic data that captures the intricacies of social networks. Using our datasets, we investigate the performance-fairness trade-off in eleven existing GAD and non-graph AD methods on five state-of-the-art fairness methods, which sheds light on their effectiveness and limitations in addressing the FairGAD problem.",
        "subjects": [
            "cs.SI",
            "cs.LG"
        ],
        "comment": "Our code and datasets are available at https://github.com/nigelnnk/FairGAD"
    },
    {
        "paper id": "2402.16073",
        "abstract url": "https://arxiv.org/abs/2402.16073",
        "title": "Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "9 pages, 8 figures"
    },
    {
        "paper id": "2402.16281",
        "abstract url": "https://arxiv.org/abs/2402.16281",
        "title": "Towards Agile Robots: Intuitive Robot Position Speculation with Neural Networks",
        "rating": "-1.5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "robotics",
                "Robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The robot position speculation, which determines where the chassis should move, is one key step to control the mobile manipulators. The target position must ensure the feasibility of chassis movement and manipulability, which is guaranteed by randomized sampling and kinematic checking in traditional methods. Addressing the demands of agile robotics, this paper proposes a robot position speculation network(RPSN), a learning-based approach to enhance the agility of mobile manipulators. The RPSN incorporates a differentiable inverse kinematic algorithm and a neural network. Through end-to-end training, the RPSN can speculate positions with a high success rate. We apply the RPSN to mobile manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs). Extensive experiments on various simulated environments and physical mobile manipulators demonstrate that the probability of the initial position provided by RPSN being the ideal position is 96.67%. From the kinematic constraint perspective, it achieves 100% generation of the ideal position on average within 1.28 attempts. Much lower than that of random sampling, 31.04. Moreover, the proposed method demonstrates superior data efficiency over pure neural network approaches. The proposed RPSN enables the robot to quickly infer feasible target positions by intuition. This work moves towards building agile robots that can act swiftly like humans.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16299",
        "abstract url": "https://arxiv.org/abs/2402.16299",
        "title": "Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a \"filter bubble\". Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern. To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users' latent preferences, a hypergraph-based random walk embedding method is applied to the constructed hypergraph. In our investigation, accuracy is gauged by the alignment between the user and the track, whereas the array of recommended track types measures diversity. We rigorously compared DWHRec against seven state-of-the-art recommendation algorithms using two real-world music datasets. The experimental results validate DWHRec as a solution that adeptly harmonizes accuracy and diversity, delivering a more enriched musical experience. Beyond music recommendation, DWHRec can be extended to cater to other scenarios with similar data structures.",
        "subjects": [
            "cs.IR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16909",
        "abstract url": "https://arxiv.org/abs/2402.16909",
        "title": "Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "health",
                "psychological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The concept of Quality of Life (QoL) refers to a holistic measurement of an individual's well-being, incorporating psychological and social aspects. Pregnant women, especially those with obesity and stress, often experience lower QoL. Physical activity (PA) has shown the potential to enhance the QoL. However, pregnant women who are overweight and obese rarely meet the recommended level of PA. Studies have investigated the relationship between PA and QoL during pregnancy using correlation-based approaches. These methods aim to discover spurious correlations between variables rather than causal relationships. Besides, the existing methods mainly rely on physical activity parameters and neglect the use of different factors such as maternal (medical) history and context data, leading to biased estimates. Furthermore, the estimations lack an understanding of mediators and counterfactual scenarios that might affect them. In this paper, we investigate the causal relationship between being physically active (treatment variable) and the QoL (outcome) during pregnancy and postpartum. To estimate the causal effect, we develop a Causal Machine Learning method, integrating causal discovery and causal inference components. The data for our investigation is derived from a long-term wearable-based health monitoring study focusing on overweight and obese pregnant women. The machine learning (meta-learner) estimation technique is used to estimate the causal effect. Our result shows that performing adequate physical activity during pregnancy and postpartum improves the QoL by units of 7.3 and 3.4 on average in physical health and psychological domains, respectively. In the final step, four refutation analysis techniques are employed to validate our estimation.",
        "subjects": [
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16913",
        "abstract url": "https://arxiv.org/abs/2402.16913",
        "title": "PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in deep learning have led to the development of various models for long-term multivariate time-series forecasting (LMTF), many of which have shown promising results. Generally, the focus has been on historical-value-based models, which rely on past observations to predict future series. Notably, a new trend has emerged with time-index-based models, offering a more nuanced understanding of the continuous dynamics underlying time series. Unlike these two types of models that aggregate the information of spatial domains or temporal domains, in this paper, we consider multivariate time series as spatiotemporal data regularly sampled from a continuous dynamical system, which can be represented by partial differential equations (PDEs), with the spatial domain being fixed. Building on this perspective, we present PDETime, a novel LMTF model inspired by the principles of Neural PDE solvers, following the encoding-integration-decoding operations. Our extensive experimentation across seven diverse real-world LMTF datasets reveals that PDETime not only adapts effectively to the intrinsic spatiotemporal nature of the data but also sets new benchmarks, achieving state-of-the-art results",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.14654",
        "abstract url": "https://arxiv.org/abs/2403.14654",
        "title": "ChatGPT in Veterinary Medicine: A Practical Guidance of Generative Artificial Intelligence in Clinics, Education, and Research",
        "rating": "-1.5",
        "keywords": [
            [
                "diagnosing",
                "clinical"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "ChatGPT, the most accessible generative artificial intelligence (AI) tool, offers considerable potential for veterinary medicine, yet a dedicated review of its specific applications is lacking. This review concisely synthesizes the latest research and practical applications of ChatGPT within the clinical, educational, and research domains of veterinary medicine. It intends to provide specific guidance and actionable examples of how generative AI can be directly utilized by veterinary professionals without a programming background. For practitioners, ChatGPT can extract patient data, generate progress notes, and potentially assist in diagnosing complex cases. Veterinary educators can create custom GPTs for student support, while students can utilize ChatGPT for exam preparation. ChatGPT can aid in academic writing tasks in research, but veterinary publishers have set specific requirements for authors to follow. Despite its transformative potential, careful use is essential to avoid pitfalls like hallucination. This review addresses ethical considerations, provides learning resources, and offers tangible examples to guide responsible implementation. Carefully selected, up-to-date links to platforms that host large language models are provided for advanced readers with programming capability. A table of key takeaways was provided to summarize this review. By highlighting potential benefits and limitations, this review equips veterinarians, educators, and researchers to harness the power of ChatGPT effectively.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16005",
        "abstract url": "https://arxiv.org/abs/2402.16005",
        "title": "Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation",
        "rating": "-2",
        "keywords": [
            [
                "attacks"
            ],
            [
                "biomedical",
                "Medical",
                "diagnosis"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients. Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images. Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues. The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes. Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning. However, a significant {\\em domain discrepancy} exists between natural and medical images, which causes AI models resulting from transfer learning to exhibit heightened {\\em vulnerability} to adversarial attacks. This paper proposes a {\\em domain assimilation} approach that introduces texture and color adaptation into transfer learning, followed by a texture preservation component to suppress undesired distortion. We systematically analyze the performance of transfer learning in the face of various adversarial attacks under different data modalities, with the overarching goal of fortifying the model's robustness and security in medical imaging tasks. The results demonstrate high effectiveness in reducing attack efficacy, contributing toward more trustworthy transfer learning in biomedical applications.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), May 2024, Taiwan"
    },
    {
        "paper id": "2402.16025",
        "abstract url": "https://arxiv.org/abs/2402.16025",
        "title": "Learning with Semantics: Towards a Semantics-Aware Routing Anomaly Detection System",
        "rating": "-2",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "BGP is the de facto inter-domain routing protocol to ensure global connectivity of the Internet. However, various reasons, such as deliberate attacks or misconfigurations, could cause BGP routing anomalies. Traditional methods for BGP routing anomaly detection require significant manual investigation of routes by network operators. Although machine learning has been applied to automate the process, prior arts typically impose significant training overhead (such as large-scale data labeling and feature crafting), and only produce uninterpretable results. To address these limitations, this paper presents a routing anomaly detection system centering around a novel network representation learning model named BEAM. The core design of BEAM is to accurately learn the unique properties (defined as \\emph{routing role}) of each Autonomous System (AS) in the Internet by incorporating BGP semantics. As a result, routing anomaly detection, given BEAM, is reduced to a matter of discovering unexpected routing role churns upon observing new route announcements. We implement a prototype of our routing anomaly detection system and extensively evaluate its performance. The experimental results, based on 18 real-world RouteViews datasets containing over 11 billion route announcement records, demonstrate that our system can detect all previously-confirmed routing anomalies, while only introducing at most five false alarms every 180 million route announcements. We also deploy our system at a large ISP to perform real-world detection for one month. During the course of deployment, our system detects 497 true anomalies in the wild with an average of only 1.65 false alarms per day.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "To be published in USENIX Security 2024"
    },
    {
        "paper id": "2402.16028",
        "abstract url": "https://arxiv.org/abs/2402.16028",
        "title": "FedFDP: Federated Learning with Fairness and Differential Privacy",
        "rating": "-2",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "Federated learning (FL) is a new machine learning paradigm to overcome the challenge of data silos and has garnered significant attention. However, through our observations, a globally effective trained model may performance disparities in different clients. This implies that the jointly trained models by clients may lead to unfair outcomes. On the other hand, relevant studies indicate that the transmission of gradients or models in federated learning can also give rise to privacy leakage issues, such as membership inference attacks. To address the first issue mentioned above, we propose a federated algorithm with fairness, termed FedFair. Building upon FedFair, we introduce privacy protection to form the FedFDP algorithm to address the second issue mentioned above. In FedFDP, we devise a fairness-aware clipping strategy to achieve differential privacy while adjusting fairness. Additionally, for the extra uploaded loss values, we present an adaptive clipping approach to maximize utility. Furthermore, we theoretically prove that our algorithm converges and ensures differential privacy. Lastly, Extensive experimental results demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art solutions in terms of model performance and fairness. The code is accessible at https://anonymous.4open.science/r/FedFDP-E754.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16085",
        "abstract url": "https://arxiv.org/abs/2402.16085",
        "title": "Online Drone Scheduling for Last-mile Delivery",
        "rating": "-2",
        "keywords": [
            [
                "Drone"
            ]
        ],
        "abstract": "Delivering a parcel from the distribution hub to the customer's doorstep is called the \\textit{last-mile delivery} step in delivery logistics. In this paper, we study a hybrid {\\it truck-drones} model for the last-mile delivery step, in which a truck moves on a predefined path carrying parcels and drones deliver the parcels. We define the \\textsc{online drone scheduling} problem, where the truck moves in a predefined path, and the customer's requests appear online during the truck's movement. The objective is to schedule a drone associated with every request to minimize the number of drones used subject to the battery budget of the drones and compatibility of the schedules. We propose a 3-competitive deterministic algorithm using the next-fit strategy and 2.7-competitive algorithms using the first-fit strategy for the problem with $O(\\log n)$ worst-case time complexity per request, where $n$ is the maximum number of active requests at any time. We also introduce \\textsc{online variable-size drone scheduling} problem (OVDS). Here, we know all the customer's requests in advance; however, the drones with different battery capacities appear online. The objective is to schedule customers' requests for drones to minimize the number of drones used. We propose a $(2\u03b1+ 1)$-competitive algorithm for the OVDS problem with total running time $O(n \\log n)$ for $n$ customer requests, where $\u03b1$ is the ratio of the maximum battery capacity to the minimum battery capacity of the drones. Finally, we address how to generate intervals corresponding to each customer request when there are discrete stopping points on the truck's route, from where the drone can fly and meet with the truck.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16097",
        "abstract url": "https://arxiv.org/abs/2402.16097",
        "title": "Molecular Code-Division Multiple-Access: Signaling, Detection, and Performance",
        "rating": "-2",
        "keywords": [
            [
                "Bio-Nano"
            ]
        ],
        "abstract": "To accomplish relatively complex tasks, in Internet of Bio-Nano Things (IoBNT), information collected by different nano-machines (NMs) is usually sent via multiple-access channels to fusion centers (FCs) for further processing. Relying on two types of molecules, in this paper, a molecular code-division multiple-access (MoCDMA) scheme is designed for multiple NMs to simultaneously send information to an access-point (AP) in a diffusive molecular communications (DMC) environment. We assume that different NMs may have different distances from AP, which generates `near-far' effect. Correspondingly, the uniform and channel-inverse based molecular emission schemes are proposed for NMs to emit information molecules. To facilitate the design of different signal detection schemes, the received signals by AP are represented in different forms. Specifically, by considering the limited computational power of nano-machines, three low-complexity detectors are designed in the principles of matched-filtering (MF), zero-forcing (ZF), and minimum mean-square error (MMSE). The noise characteristics in MoCDMA systems and the complexity of various detection schemes are analyzed. The error performance of the MoCDMA systems with various molecular emission and detection schemes is demonstrated and compared. Our studies and performance results demonstrate that MoCDMA constitutes a promising scheme for supporting multiple-access transmission in DMC, while the channel-inverse based transmission can ensure the fairness of communication qualities (FoCQ) among different NMs. Furthermore, different detection schemes may be implemented to attain a good trade-off between implementation complexity and communication reliability.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16124",
        "abstract url": "https://arxiv.org/abs/2402.16124",
        "title": "AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation",
        "rating": "-2",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "While considerable progress has been made in achieving accurate lip synchronization for 3D speech-driven talking face generation, the task of incorporating expressive facial detail synthesis aligned with the speaker's speaking status remains challenging. Our goal is to directly leverage the inherent style information conveyed by human speech for generating an expressive talking face that aligns with the speaking status. In this paper, we propose AVI-Talking, an Audio-Visual Instruction system for expressive Talking face generation. This system harnesses the robust contextual reasoning and hallucination capability offered by Large Language Models (LLMs) to instruct the realistic synthesis of 3D talking faces. Instead of directly learning facial movements from human speech, our two-stage strategy involves the LLMs first comprehending audio information and generating instructions implying expressive facial details seamlessly corresponding to the speech. Subsequently, a diffusion-based generative network executes these instructions. This two-stage process, coupled with the incorporation of LLMs, enhances model interpretability and provides users with flexibility to comprehend instructions and specify desired operations or modifications. Extensive experiments showcase the effectiveness of our approach in producing vivid talking faces with expressive facial movements and consistent emotional status.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16126",
        "abstract url": "https://arxiv.org/abs/2402.16126",
        "title": "A statistical method for crack detection in 3D concrete images",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "CT"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In practical applications, effectively segmenting cracks in large-scale computed tomography (CT) images holds significant importance for understanding the structural integrity of materials. However, classical methods and Machine Learning algorithms often incur high computational costs when dealing with the substantial size of input images. Hence, a robust algorithm is needed to pre-detect crack regions, enabling focused analysis and reducing computational overhead. The proposed approach addresses this challenge by offering a streamlined method for identifying crack regions in CT images with high probability. By efficiently identifying areas of interest, our algorithm allows for a more focused examination of potential anomalies within the material structure. Through comprehensive testing on both semi-synthetic and real 3D CT images, we validate the efficiency of our approach in enhancing crack segmentation while reducing computational resource requirements.",
        "subjects": [
            "cs.CV",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16140",
        "abstract url": "https://arxiv.org/abs/2402.16140",
        "title": "Multi-access Distributed Computing Models from Map-Reduce Arrays",
        "rating": "-2",
        "keywords": [
            [
                "CT"
            ]
        ],
        "abstract": "A novel distributed computing model called \"Multi-access Distributed Computing (MADC)\" was recently introduced in http://www.arXiv:2206.12851. In this paper, we represent MADC models via 2-layered bipartite graphs called Map-Reduce Graphs (MRGs) and a set of arrays called Map-Reduce Arrays (MRAs) inspired from the Placement Delivery Arrays (PDAs) used in the coded caching literature. The connection between MRAs and MRGs is established, thereby exploring new topologies and providing coded shuffling schemes for the MADC models with MRGs using the structure of MRAs. A novel \\textit{Nearest Neighbor Connect-MRG (NNC-MRG)} is explored and a coding scheme is provided for MADC models with NNC-MRG, exploiting the connections between MRAs and PDAs. Moreover, CT is generalized to Generalized Combinatorial-MRG (GC-MRG). A set of $g-$regular MRAs is provided which corresponds to the existing scheme for MADC models with CT and extended those to generate another set of MRAs to represent MADC models with GC-MRG. A lower bound on the computation-communication curve for MADC model with GC-MRG under homogeneous setting is derived and certain cases are explored where the existing scheme is optimal under CT. One of the major limitations of the existing scheme for CT is that it requires an exponentially large number of reducer nodes and input files for large $\u039b$. This can be overcome by representing CT by MRAs, where coding schemes can be derived even if some of the reducer nodes are not present. Another way of tackling this is by using a different MRG, specifically NNC-MRG, where the number of reducer nodes and files required are significantly smaller compared to CT. Hence, the advantages are two-fold, which is achievable at the expense of a slight increase in the communication load.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "18 pages, 5 figures and 2 tables"
    },
    {
        "paper id": "2402.16165",
        "abstract url": "https://arxiv.org/abs/2402.16165",
        "title": "On the Feasibility of Deep Learning Classification from Raw Signal Data in Radiology, Ultrasonography and Electrophysiology",
        "rating": "-2",
        "keywords": [
            [
                "Medical",
                "healthcare",
                "Radiology"
            ]
        ],
        "abstract": "Medical imaging is a very useful tool in healthcare, various technologies being employed to non-invasively peek inside the human body. Deep learning with neural networks in radiology was welcome - albeit cautiously - by the radiologist community. Most of the currently deployed or researched deep learning solutions are applied on already generated images of medical scans, use the neural networks to aid in the generation of such images, or use them for identifying specific substance markers in spectrographs. This paper's author posits that if the neural networks were trained directly on the raw signals from the scanning machines, they would gain access to more nuanced information than from the already processed images, hence the training - and later, the inferences - would become more accurate. The paper presents the main current applications of deep learning in radiography, ultrasonography, and electrophysiology, and discusses whether the proposed neural network training directly on raw signals is feasible.",
        "subjects": [
            "eess.SY",
            "cs.NE",
            "q-bio.QM"
        ],
        "comment": "Updated 2024.04.14 17:49+3: accepted to the conference, so added IEEE copyright notice to the first page, as per \"IEEE Publication Services and Products Board Operations Manual 2024\"'s subsection 8.1.9. point D. 6 pages, 5 figures, 1 table, 56 references, submitted to AQTR 2024"
    },
    {
        "paper id": "2402.16174",
        "abstract url": "https://arxiv.org/abs/2402.16174",
        "title": "GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "drone"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16175",
        "abstract url": "https://arxiv.org/abs/2402.16175",
        "title": "XAI-based gait analysis of patients walking with Knee-Ankle-Foot orthosis using video cameras",
        "rating": "-2",
        "keywords": [
            [
                "super-resolution"
            ],
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent technological advancements in artificial intelligence and computer vision have enabled gait analysis on portable devices such as cell phones. However, most state-of-the-art vision-based systems still impose numerous constraints for capturing a patient's video, such as using a static camera and maintaining a specific distance from it. While these constraints are manageable under professional observation, they pose challenges in home settings. Another issue with most vision-based systems is their output, typically a classification label and confidence value, whose reliability is often questioned by medical professionals. This paper addresses these challenges by presenting a novel system for gait analysis robust to camera movements and providing explanations for its output. The study utilizes a dataset comprising videos of subjects wearing two types of Knee Ankle Foot Orthosis (KAFO), namely \"Locked Knee\" and \"Semi-flexion,\" for mobility, along with metadata and ground truth for explanations. The ground truth highlights the statistical significance of seven features captured using motion capture systems to differentiate between the two gaits. To address camera movement challenges, the proposed system employs super-resolution and pose estimation during pre-processing. It then identifies the seven features - Stride Length, Step Length and Duration of single support of orthotic and non-orthotic leg, Cadence, and Speed - using the skeletal output of pose estimation. These features train a multi-layer perceptron, with its output explained by highlighting the features' contribution to classification. While most state-of-the-art systems struggle with processing the video or training on the proposed dataset, our system achieves an average accuracy of 94%. The model's explainability is validated using ground truth and can be considered reliable.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages 6 figures 3 tables Dataset Link : http://tinyurl.com/5ds5f33c"
    },
    {
        "paper id": "2402.16187",
        "abstract url": "https://arxiv.org/abs/2402.16187",
        "title": "Attacking LLM Watermarks by Exploiting Their Strengths",
        "rating": "-2",
        "keywords": [
            [
                "attack"
            ],
            [
                "Watermarking"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.",
        "subjects": [
            "cs.CR",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16188",
        "abstract url": "https://arxiv.org/abs/2402.16188",
        "title": "ARIN: Adaptive Resampling and Instance Normalization for Robust Blind Inpainting of Dunhuang Cave Paintings",
        "rating": "-2",
        "keywords": [
            [
                "Inpainting",
                "super resolution"
            ],
            [
                "Image enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image enhancement algorithms are very useful for real world computer vision tasks where image resolution is often physically limited by the sensor size. While state-of-the-art deep neural networks show impressive results for image enhancement, they often struggle to enhance real-world images. In this work, we tackle a real-world setting: inpainting of images from Dunhuang caves. The Dunhuang dataset consists of murals, half of which suffer from corrosion and aging. These murals feature a range of rich content, such as Buddha statues, bodhisattvas, sponsors, architecture, dance, music, and decorative patterns designed by different artists spanning ten centuries, which makes manual restoration challenging. We modify two different existing methods (CAR, HINet) that are based upon state-of-the-art (SOTA) super resolution and deblurring networks. We show that those can successfully inpaint and enhance these deteriorated cave paintings. We further show that a novel combination of CAR and HINet, resulting in our proposed inpainting network (ARIN), is very robust to external noise, especially Gaussian noise. To this end, we present a quantitative and qualitative comparison of our proposed approach with existing SOTA networks and winners of the Dunhuang challenge. One of the proposed methods HINet) represents the new state of the art and outperforms the 1st place of the Dunhuang Challenge, while our combination ARIN, which is robust to noise, is comparable to the 1st place. We also present and discuss qualitative results showing the impact of our method for inpainting on Dunhuang cave images.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16212",
        "abstract url": "https://arxiv.org/abs/2402.16212",
        "title": "Photon-counting CT using a Conditional Diffusion Model for Super-resolution and Texture-preservation",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion",
                "Super-resolution"
            ],
            [
                "CT",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Ultra-high resolution images are desirable in photon counting CT (PCCT), but resolution is physically limited by interactions such as charge sharing. Deep learning is a possible method for super-resolution (SR), but sourcing paired training data that adequately models the target task is difficult. Additionally, SR algorithms can distort noise texture, which is an important in many clinical diagnostic scenarios. Here, we train conditional denoising diffusion probabilistic models (DDPMs) for PCCT super-resolution, with the objective to retain textural characteristics of local noise. PCCT simulation methods are used to synthesize realistic resolution degradation. To preserve noise texture, we explore decoupling the noise and signal image inputs and outputs via deep denoisers, explicitly mapping to each during the SR process. Our experimental results indicate that our DDPM trained on simulated data can improve sharpness in real PCCT images. Additionally, the disentanglement of noise from the original image allows our model more faithfully preserve noise texture.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "5 pages, 4 figures"
    },
    {
        "paper id": "2402.16246",
        "abstract url": "https://arxiv.org/abs/2402.16246",
        "title": "Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices",
        "rating": "-2",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "UAV",
                "drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "28 pages,26 figures"
    },
    {
        "paper id": "2402.16249",
        "abstract url": "https://arxiv.org/abs/2402.16249",
        "title": "SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce Sequence-to-Sequence tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ICRA2024"
    },
    {
        "paper id": "2402.16291",
        "abstract url": "https://arxiv.org/abs/2402.16291",
        "title": "mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection",
        "rating": "-2",
        "keywords": [
            [
                "NAS"
            ],
            [
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting objects across various scales remains a significant challenge in computer vision, particularly in tasks such as Rice Leaf Disease (RLD) detection, where objects exhibit considerable scale variations. Traditional object detection methods often struggle to address these variations, resulting in missed detections or reduced accuracy. In this study, we propose the multi-scale Attention Pyramid module (mAPm), a novel approach that integrates dilated convolutions into the Feature Pyramid Network (FPN) to enhance multi-scale information ex-traction. Additionally, we incorporate a global Multi-Head Self-Attention (MHSA) mechanism and a deconvolutional layer to refine the up-sampling process. We evaluate mAPm on YOLOv7 using the MRLD and COCO datasets. Compared to vanilla FPN, BiFPN, NAS-FPN, PANET, and ACFPN, mAPm achieved a significant improvement in Average Precision (AP), with a +2.61% increase on the MRLD dataset compared to the baseline FPN method in YOLOv7. This demonstrates its effectiveness in handling scale variations. Furthermore, the versatility of mAPm allows its integration into various FPN-based object detection models, showcasing its potential to advance object detection techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18949",
        "abstract url": "https://arxiv.org/abs/2403.18949",
        "title": "An IoT Based Water-Logging Detection System: A Case Study of Dhaka",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "With a large number of populations, many problems are rising rapidly in Dhaka, the capital city of Bangladesh. Water-logging is one of the major issues among them. Heavy rainfall, lack of awareness and poor maintenance causes bad sewerage system in the city. As a result, water is overflowed on the roads and sometimes it gets mixed with the drinking water. To overcome this problem, this paper realizes the potential of using Internet of Things to combat water-logging in drainage pipes which are used to move wastes as well as rainwater away from the city. The proposed system will continuously monitor real time water level, water flow and gas level inside the drainage pipe. Moreover, all the monitoring data will be stored in the central database for graphical representation and further analysis. In addition to that if any emergency arises in the drainage system, an alert will be sent directly to the nearest maintenance office.",
        "subjects": [
            "cs.OH"
        ],
        "comment": "Global Conference on Technology and Information Management"
    },
    {
        "paper id": "2402.16002",
        "abstract url": "https://arxiv.org/abs/2402.16002",
        "title": "Post-Quantum Cryptography Neural Network",
        "rating": "-2.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, quantum computers and Shor quantum algorithm have posed a threat to current mainstream asymmetric cryptography methods (e.g. RSA and Elliptic Curve Cryptography (ECC)). Therefore, it is necessary to construct a Post-Quantum Cryptography (PQC) method to resist quantum computing attacks. Therefore, this study proposes a PQC-based neural network that maps a code-based PQC method to a neural network structure and enhances the security of ciphertexts with non-linear activation functions, random perturbation of ciphertexts, and uniform distribution of ciphertexts. In practical experiments, this study uses cellular network signals as a case study to demonstrate that encryption and decryption can be performed by the proposed PQC-based neural network with the uniform distribution of ciphertexts. In the future, the proposed PQC-based neural network could be applied to various applications.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "2023 International Conference on Smart Systems for applications in Electrical Sciences (ICSSES) 7-8 July 2023. The manuscript was written in Chinese and submitted on 10 March 2023, but it was rejected on 22 April 2023. The appeal was accepted on 24 February 2024"
    },
    {
        "paper id": "2402.16014",
        "abstract url": "https://arxiv.org/abs/2402.16014",
        "title": "Building Flexible Machine Learning Models for Scientific Computing at Scale",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering applications and physics discovery.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Work in Progress"
    },
    {
        "paper id": "2402.16173",
        "abstract url": "https://arxiv.org/abs/2402.16173",
        "title": "Communication Traffic Characteristics Reveal an IoT Devices Identity",
        "rating": "-2.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "IoT"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Internet of Things (IoT) is one of the technological advancements of the twenty-first century which can improve living standards. However, it also imposes new types of security challenges, including device authentication, traffic types classification, and malicious traffic identification, in the network domain. Traditionally, internet protocol (IP) and media access control (MAC) addresses are utilized for identifying network-connected devices in a network, whilst these addressing schemes are prone to be compromised, including spoofing attacks and MAC randomization. Therefore, device identification using only explicit identifiers is a challenging task. Accurate device identification plays a key role in securing a network. In this paper, a supervised machine learning-based device fingerprinting (DFP) model has been proposed for identifying network-connected IoT devices using only communication traffic characteristics (or implicit identifiers). A single transmission control protocol/internet protocol (TCP/IP) packet header features have been utilized for generating unique fingerprints, with the fingerprints represented as a vector of 22 features. Experimental results have shown that the proposed DFP method achieves over 98% in classifying individual IoT devices using the UNSW dataset with 22 smart-home IoT devices. This signifies that the proposed approach is invaluable to network operators in making their networks more secure.",
        "subjects": [
            "cs.NI",
            "cs.AI"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2402.16230",
        "abstract url": "https://arxiv.org/abs/2402.16230",
        "title": "GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "healthcare",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life. The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS). However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients. In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets, representing diverse clinical scenarios. Upon comparison with twelve well-established baseline methods, GARNNs not only achieve the best prediction accuracy but also provide high-quality temporal interpretability, in particular for postprandial glucose levels as a result of corresponding meal intake and insulin injection. These findings underline the potential of GARNN as a robust tool for improving diabetes care, bridging the gap between deep learning technology and real-world healthcare solutions.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16294",
        "abstract url": "https://arxiv.org/abs/2402.16294",
        "title": "Decentralized Federated Unlearning on Blockchain",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Unlearning"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conducted in parallel or serial. We conduct a comprehensive study of two typical unlearning methods, gradient ascent and re-training, demonstrating the efficient unlearning workflow in these two categories with minimal CH and block update operations. Additionally, we compare the computation and communication costs of these methods.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16043",
        "abstract url": "https://arxiv.org/abs/2402.16043",
        "title": "LuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "IoT devices are currently facing continuous malicious attacks due to their widespread use. Among these IoT devices, web vulnerabilities are also widely exploited because of their inherent characteristics, such as improper permission controls and insecure interfaces. Recently, the embedded system web interface framework has become highly diverse, and specific vulnerabilities can arise if developers forget to detect user input parameters or if the detection process is not strict enough. Therefore, discovering vulnerabilities in the web interfaces of IoT devices accurately and comprehensively through an automated method is a major challenge. This paper aims to work out the challenge. We have developed an automated vulnerability detection system called LuaTaint for the typical web interface framework, LuCI. The system employs static taint analysis to address web security issues on mobile terminal platforms to ensure detection coverage. It integrates rules pertaining to page handler control logic within the taint detection process to improve its extensibility. We also implemented a post-processing step with the assistance of large language models to enhance accuracy and reduce the need for manual analysis. We have created a prototype of LuaTaint and tested it on 92 IoT firmwares from 8 well-known vendors. LuaTaint has discovered 68 unknown vulnerabilities.",
        "subjects": [
            "cs.CR",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16046",
        "abstract url": "https://arxiv.org/abs/2402.16046",
        "title": "Decoding Driver Takeover Behaviour in Conditional Automation with Immersive Virtual Reality",
        "rating": "-3",
        "keywords": [
            [
                "automated driving",
                "vehicle"
            ],
            [
                "survival"
            ]
        ],
        "abstract": "The safe transition from conditional automation to manual driving control is significantly intertwined with the vehicle's lateral and longitudinal dynamics. The transition may occur as a result of a system-initiated mandatory takeover (MTOR) or as a driver-initiated discretionary takeover (DTOR). In either condition, the takeover process entails differing cognitive demands and may affect the driving behaviour differently. This study analyzes driving stability and perceived mental workload in 304 takeover attempts recorded from 104 participants within virtual and immersive reality environments. Adopting an exploratory approach, this dynamic simulator study employs a mixed factorial design. Utilizing a deep neural network-based survival analysis with SHAP interpretability, the study investigated the influence of covariates on perception-reaction time (PRT), distinguishing between safe and unsafe control transition and offering insights into the temporal dynamics of these shifts. The distributions of key parameters in experimental groups were analyzed and factors influencing the perceived mental workload were estimated using multivariate linear regression. The findings indicate a notable decrease in the risk of unsafe takeovers (described by a longer PRT) when drivers have prior control-transition experience and familiarity with Automated Vehicles (AVs). However, driver's prior familiarity and experience with AVs only decreased the perceived mental workload associated with DTOR, with an insignificant impact on the cognitive demand of MTOR. Furthermore, multitasking during automated driving significantly elevated the cognitive demand linked to DTOR and led to longer PRT in MTOR situations.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16101",
        "abstract url": "https://arxiv.org/abs/2402.16101",
        "title": "Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern",
        "rating": "-3",
        "keywords": [
            [
                "Robot"
            ],
            [
                "medical",
                "Surgical",
                "surgery"
            ]
        ],
        "abstract": "In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot base is crucial for successful surgery. Improper placement can hinder performance because of manipulator limitations and inaccessible workspaces. Conventional base placement relies on the experience of trained medical staff. This study proposes a novel method for determining the optimal base pose based on the surgeon's working pattern. The proposed method analyzes recorded end-effector poses using a machine learning-based clustering technique to identify key positions and orientations preferred by the surgeon. We introduce two scoring metrics to address the joint limit and singularity issues: joint margin and manipulability scores. We then train a multi-layer perceptron regressor to predict the optimal base pose based on these scores. Evaluation in a simulated environment using the da Vinci Research Kit shows unique base pose score maps for four volunteers, highlighting the individuality of the working patterns. Results comparing with 20,000 randomly selected base poses suggest that the score obtained using the proposed method is 28.2% higher than that obtained by random base placement. These results emphasize the need for operator-specific optimization during base placement in RAMIS.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures, 2 tables"
    },
    {
        "paper id": "2402.16110",
        "abstract url": "https://arxiv.org/abs/2402.16110",
        "title": "Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability",
        "rating": "-3",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Multimodal recommender systems amalgamate multimodal information (e.g., textual descriptions, images) into a collaborative filtering framework to provide more accurate recommendations. While the incorporation of multimodal information could enhance the interpretability of these systems, current multimodal models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret. To address this, we propose a Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both model and recommendation interpretability. DGVAE initially projects multimodal information into textual contents, such as converting images to text, by harnessing state-of-the-art multimodal pre-training technologies. It then constructs a frozen item-item graph and encodes the contents and interactions into two sets of disentangled representations utilizing a simplified residual graph convolutional network. DGVAE further regularizes these disentangled representations through mutual information maximization, aligning the representations derived from the interactions between users and items with those learned from textual content. This alignment facilitates the interpretation of user binary interactions via text. Our empirical analysis conducted on three real-world datasets demonstrates that DGVAE significantly surpasses the performance of state-of-the-art baselines by a margin of 10.02%. We also furnish a case study from a real-world dataset to illustrate the interpretability of DGVAE. Code is available at: \\url{https://github.com/enoche/DGVAE}.",
        "subjects": [
            "cs.IR",
            "cs.MM"
        ],
        "comment": "12 pages, 7 figures"
    },
    {
        "paper id": "2402.16154",
        "abstract url": "https://arxiv.org/abs/2402.16154",
        "title": "IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations",
        "rating": "-3",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "robot"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Many applications require a robot to accurately track reference end-effector trajectories. Certain trajectories may not be tracked as single, continuous paths due to the robot's kinematic constraints or obstacles elsewhere in the environment. In this situation, it becomes necessary to divide the trajectory into shorter segments. Each such division introduces a reconfiguration, in which the robot deviates from the reference trajectory, repositions itself in configuration space, and then resumes task execution. The occurrence of reconfigurations should be minimized because they increase the time and energy usage. In this paper, we present IKLink, a method for finding joint motions to track reference end-effector trajectories while executing minimal reconfigurations. Our graph-based method generates a diverse set of Inverse Kinematics (IK) solutions for every waypoint on the reference trajectory and utilizes a dynamic programming algorithm to find the globally optimal motion by linking the IK solutions. We demonstrate the effectiveness of IKLink through a simulation experiment and an illustrative demonstration using a physical robot.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted as a contributed paper at the 2024 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
        "paper id": "2402.16159",
        "abstract url": "https://arxiv.org/abs/2402.16159",
        "title": "DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem",
        "rating": "-3",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "Named Entity Recognition"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the AI revolution in place, the trend for building automated systems to support professionals in different domains such as the open source software systems, healthcare systems, banking systems, transportation systems and many others have become increasingly prominent. A crucial requirement in the automation of support tools for such systems is the early identification of named entities, which serves as a foundation for developing specialized functionalities. However, due to the specific nature of each domain, different technical terminologies and specialized languages, expert annotation of available data becomes expensive and challenging. In light of these challenges, this paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our model significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2403.00813",
        "abstract url": "https://arxiv.org/abs/2403.00813",
        "title": "UrbanGPT: Spatio-Temporal Large Language Models",
        "rating": "-3",
        "keywords": [
            [
                "forecast"
            ],
            [
                "crime"
            ],
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks. To achieve this objective, we present the UrbanGPT, which seamlessly integrates a spatio-temporal dependency encoder with the instruction-tuning paradigm. This integration enables LLMs to comprehend the complex inter-dependencies across time and space, facilitating more comprehensive and accurate predictions under data scarcity. To validate the effectiveness of our approach, we conduct extensive experiments on various public datasets, covering different spatio-temporal prediction tasks. The results consistently demonstrate that our UrbanGPT, with its carefully designed architecture, consistently outperforms state-of-the-art baselines. These findings highlight the potential of building large language models for spatio-temporal learning, particularly in zero-shot scenarios where labeled data is scarce.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2402.16143",
        "abstract url": "https://arxiv.org/abs/2402.16143",
        "title": "Cinematographic Camera Diffusion Model",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "grammar"
            ]
        ],
        "abstract": "Designing effective camera trajectories in virtual 3D environments is a challenging task even for experienced animators. Despite an elaborate film grammar, forged through years of experience, that enables the specification of camera motions through cinematographic properties (framing, shots sizes, angles, motions), there are endless possibilities in deciding how to place and move cameras with characters. Dealing with these possibilities is part of the complexity of the problem. While numerous techniques have been proposed in the literature (optimization-based solving, encoding of empirical rules, learning from real examples,...), the results either lack variety or ease of control. In this paper, we propose a cinematographic camera diffusion model using a transformer-based architecture to handle temporality and exploit the stochasticity of diffusion models to generate diverse and qualitative trajectories conditioned by high-level textual descriptions. We extend the work by integrating keyframing constraints and the ability to blend naturally between motions using latent interpolation, in a way to augment the degree of control of the designers. We demonstrate the strengths of this text-to-camera motion approach through qualitative and quantitative experiments and gather feedback from professional artists. The code and data are available at \\URL{https://github.com/jianghd1996/Camera-control}.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16245",
        "abstract url": "https://arxiv.org/abs/2402.16245",
        "title": "Random Staircase Generator Matrix Codes",
        "rating": "-4",
        "keywords": [
            [
                "BIOS"
            ],
            [
                "5G"
            ]
        ],
        "abstract": "In this paper, we propose a class of codes, referred to as random staircase generator matrix codes (SGMCs), which have staircase-like generator matrices. In the infinite-length region, we prove that the random SGMC is capacity-achieving over binary-input output-symmetric (BIOS) channels. In the finite-length region, we present the representative ordered statistics decoding with local constraints (LC-ROSD) algorithm for the SGMCs. The most distinguished feature of the SGMCs with LC-ROSD is that the staircase-like matrices enable parallel implementation of the Gaussian elimination (GE), avoiding the serial GE of conventional OSD and supporting a potential low decoding latency, as implied from simulations. To analyze the performance of random SGMCs in the finite-length region, we derive the ensemble weight spectrum and invoke the conventional union bound. We also derive a partially random coding union (RCU) bound, which is tighter than the conventional one and is used as a criterion to design the SGMCs. Staircase-like generator matrices allow us to derive a series of (tighter and tighter) lower bounds based on the second-order Bonferroni inequality with the incremental number of codewords. The numerical results show that the decoding performance can match well with the proposed partially RCU bound for different code rates and different profiles. The numerical results also show that the tailored SGMCs with the LC-ROSD algorithm can approach the finite-length performance bound, outperforming the 5G low-density parity-check (LDPC) codes, 5G polar codes, and Reed-Muller (RM) codes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16908",
        "abstract url": "https://arxiv.org/abs/2402.16908",
        "title": "Lightweight, error-tolerant edge detection using memristor-enabled stochastic logics",
        "rating": "-4",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "medical",
                "diagnosis"
            ],
            [
                "industrial"
            ],
            [
                "cs.LG",
                "eess.IV"
            ]
        ],
        "abstract": "The demand for efficient edge vision has spurred the interest in developing stochastic computing approaches for performing image processing tasks. Memristors with inherent stochasticity readily introduce probability into the computations and thus enable stochastic image processing computations. Here, we present a stochastic computing approach for edge detection, a fundamental image processing technique, facilitated with memristor-enabled stochastic logics. Specifically, we integrate the memristors with logic circuits and harness the stochasticity from the memristors to realize compact stochastic logics for stochastic number encoding and processing. The stochastic numbers, exhibiting well-regulated probabilities and correlations, can be processed to perform logic operations with statistical probabilities. This can facilitate lightweight stochastic edge detection for edge visual scenarios characterized with high-level noise errors. As a practical demonstration, we implement a hardware stochastic Roberts cross operator using the stochastic logics, and prove its exceptional edge detection performance, remarkably, with 95% less computational cost while withstanding 50% bit-flip errors. The results underscore the great potential of our stochastic edge detection approach in developing lightweight, error-tolerant edge vision hardware and systems for autonomous driving, virtual/augmented reality, medical imaging diagnosis, industrial automation, and beyond.",
        "subjects": [
            "cs.ET",
            "cond-mat.mtrl-sci",
            "cs.LG",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16190",
        "abstract url": "https://arxiv.org/abs/2402.16190",
        "title": "Accurate predictions of keyhole depths using machine learning-aided simulations",
        "rating": "-5",
        "keywords": [
            [
                "depth"
            ],
            [
                "X-ray"
            ],
            [
                "alloys"
            ]
        ],
        "abstract": "The keyhole phenomenon is widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing. Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies. The formation of these pores is typically associated with the dynamic behavior of the keyhole. So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time has been a challenging task. In situ characterization of keyhole dynamic behavior using a synchrotron X-ray is complicated and expensive. Current simulations are hindered by their poor accuracies in predicting keyhole depths due to the lack of real-time laser absorptance data. Here, we develop a machine learning-aided simulation method that allows us to accurately predict keyhole depth over a wide range of processing parameters. Based on titanium and aluminum alloys, two commonly used engineering materials as examples, we achieve an accuracy with an error margin of 10 %, surpassing those simulated using other existing models (with an error margin in a range of 50-200 %). Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques.",
        "subjects": [
            "cs.CE",
            "cond-mat.mtrl-sci"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16052",
        "abstract url": "https://arxiv.org/abs/2402.16052",
        "title": "Maximizing UAV Fog Deployment Efficiency for Critical Rescue Operations",
        "rating": "-5.5",
        "keywords": [
            [
                "health"
            ],
            [
                "IoT"
            ],
            [
                "UAV"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In disaster scenarios and high-stakes rescue operations, integrating Unmanned Aerial Vehicles (UAVs) as fog nodes has become crucial. This integration ensures a smooth connection between affected populations and essential health monitoring devices, supported by the Internet of Things (IoT). Integrating UAVs in such environments is inherently challenging, where the primary objectives involve maximizing network connectivity and coverage while extending the network's lifetime through energy-efficient strategies to serve the maximum number of affected individuals. In this paper, We propose a novel model centred around dynamic UAV-based fog deployment that optimizes the system's adaptability and operational efficacy within the afflicted areas. First, we decomposed the problem into two subproblems. Connectivity and coverage subproblem, and network lifespan optimization subproblem. We shape our UAV fog deployment problem as a uni-objective optimization and introduce a specialized UAV fog deployment algorithm tailored specifically for UAV fog nodes deployed in rescue missions. While the network lifespan optimization subproblem is efficiently solved via a one-dimensional swapping method. Following that, We introduce a novel optimization strategy for UAV fog node placement in dynamic networks during evacuation scenarios, with a primary focus on ensuring robust connectivity and maximal coverage for mobile users, while extending the network's lifespan. Finally, we introduce Adaptive Whale Optimization Algorithm (WOA) for fog node deployment in a dynamic network. Its agility, rapid convergence, and low computational demands make it an ideal fit for high-pressure environments.",
        "subjects": [
            "cs.NI",
            "cs.AI",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16010",
        "abstract url": "https://arxiv.org/abs/2402.16010",
        "title": "Energy-conserving intermittent-contact motion in complex models",
        "rating": "-10",
        "keywords": [],
        "abstract": "Some mechanical systems, that are modeled to have inelastic collisions, nonetheless possess energy-conserving intermittent-contact solutions, known as collisionless solutions. Such a solution, representing a persistent hopping or walking across a level ground, may be important for understanding animal locomotion or for designing efficient walking machines. So far, collisionless motion has been analytically studied in simple two degrees of freedom (DOF) systems, or in a system that decouples into 2-DOF subsystems in the harmonic approximation. In this paper we extend the consideration to a N-DOF system, recovering the known solutions as a special N = 2 case of the general formulation. We show that in the harmonic approximation the collisionless solution is determined by the spectrum of the system. We formulate a solution existence condition, which requires the presence of at least one oscillating normal mode in the most constrained phase of the motion. An application of the developed general framework is illustrated by finding a collisionless solution for a rocking motion of a biped with an armed standing torso.",
        "subjects": [
            "cs.RO",
            "physics.bio-ph",
            "physics.class-ph"
        ],
        "comment": "27 pages, 6 figures"
    },
    {
        "paper id": "2402.16027",
        "abstract url": "https://arxiv.org/abs/2402.16027",
        "title": "Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance Analysis and Optimization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Massive interconnection has sparked people's envisioning for next-generation ultra-reliable and low-latency communications (xURLLC), prompting the design of customized next-generation advanced transceivers (NGAT). Rate-splitting multiple access (RSMA) has emerged as a pivotal technology for NGAT design, given its robustness to imperfect channel state information (CSI) and resilience to quality of service (QoS). Additionally, xURLLC urgently appeals to large-scale access techniques, thus massive multiple-input multiple-output (mMIMO) is anticipated to integrate with RSMA to enhance xURLLC. In this paper, we develop an innovative RSMA-assisted massive-MIMO xURLLC (RSMA-mMIMO-xURLLC) network architecture tailored to accommodate xURLLC's critical QoS constraints in finite blocklength (FBL) regimes. Leveraging uplink pilot training under imperfect CSI at the transmitter, we estimate channel gains and customize linear precoders for efficient downlink short-packet data transmission. Subsequently, we formulate a joint rate-splitting, beamforming, and transmit antenna selection optimization problem to maximize the total effective transmission rate (ETR). Addressing this multi-variable coupled non-convex problem, we decompose it into three corresponding subproblems and propose a low-complexity joint iterative algorithm for efficient optimization. Extensive simulations substantiate that compared with non-orthogonal multiple access (NOMA) and space division multiple access (SDMA), the developed architecture improves the total ETR by 15.3% and 41.91%, respectively, as well as accommodates larger-scale access.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "14 pages, 11 figures, Submitted to IEEE for potential publication"
    },
    {
        "paper id": "2402.16047",
        "abstract url": "https://arxiv.org/abs/2402.16047",
        "title": "List Coloring of some Cayley graphs using Kernel perfections",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we try to determine exact or bounds on the choosability, or list chromatic numbers of some Cayley graphs, typically some Unitary Cayley graphs and Cayley graphs on Dihedral groups.",
        "subjects": [
            "math.CO",
            "cs.DM",
            "cs.DS"
        ],
        "comment": "3 pages, short paper"
    },
    {
        "paper id": "2402.16057",
        "abstract url": "https://arxiv.org/abs/2402.16057",
        "title": "Fractal Gripper: Adaptive manipulator with mode switching",
        "rating": "-10",
        "keywords": [],
        "abstract": "Although the multi-jointed underactuated manipulator is highly dexterous, its grasping capacity does not match that of the parallel jaw gripper. This work introduces a fractal gripper to enhance the grasping capacity of multi-joint underactuated manipulators, preserving their passive clamping features. We describe in detail the working principle and manufacturing process of the fractal gripper. This work, inspired by the 'Fractal Vise' structure, resulted in the invention of a fractal gripper with mode switching capabilities. The fractal gripper inherits the inherent adaptive properties of the fractal structure and realizes the self-resetting function by integrating spring into the original design, thereby enhancing the efficiency of object grasping tasks. The fractal gripper prevents object damage by distributing pressure evenly and applying it at multiple points through its fractal structure during closure. Objects of various shapes are effectively grasped by the fractal gripper, which ensures a safe and secure grasp. The superior performance was provided by the force distribution characteristics of the fractal gripper. By applying the flexible polymer PDMS, which possesses superior elasticity, to the fractal structure's wrapping surface, potential scratching during grasping is effectively prevented, thus protecting the object's geometric surface. Grab experiments with objects of diverse shapes and sizes confirm fractal gripper multi-scale adaptability and superior grasping stability.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16072",
        "abstract url": "https://arxiv.org/abs/2402.16072",
        "title": "Demonstration of 3 V Programmable Josephson Junction Arrays Using Non-Integer-Multiple Logic",
        "rating": "-10",
        "keywords": [],
        "abstract": "This article demonstrates a new kind of programmable logic for the representation of an integer that can be used for the programmable Josephson voltage standard. It can enable the numbers of junctions in most bits to be variable integer values, which is different from normal binary logic or ternary logic. Consequently, missing junctions due to superconducting short circuits can be tolerated under this logic. This logic can also have nearly the same segmentation efficiency as ternary logic. The completeness of the sequences using this logic is proven by the recursive method in mathematics in this paper. After that, a new algorithm for the representation of integers is presented according to the proven process, and an analysis of the number of fault-tolerant junctions for each bit is provided. Although the first and second bits are not tolerant to missing junctions, bits beyond these can tolerate one to hundreds of missing junctions. Due to the non-fixed multiples between the bits of the sequence, this logic is called non-integer-multiple logic. Finally, the design and fabrication of a 3 V programmable Josephson junction array using this logic are described, and the measurements and analysis of the characteristic parameters are presented.",
        "subjects": [
            "cs.ET",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16081",
        "abstract url": "https://arxiv.org/abs/2402.16081",
        "title": "HPE Transformer: Learning to Optimize Multi-Group Multicast Beamforming Under Nonconvex QoS Constraints",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper studies the quality-of-service (QoS) constrained multi-group multicast beamforming design problem, where each multicast group is composed of a number of users requiring the same content. Due to the nonconvex QoS constraints, this problem is nonconvex and NP-hard. While existing optimization-based iterative algorithms can obtain a suboptimal solution, their iterative nature results in large computational complexity and delay. To facilitate real-time implementations, this paper proposes a deep learning-based approach, which consists of a beamforming structure assisted problem transformation and a customized neural network architecture named hierarchical permutation equivariance (HPE) transformer. The proposed HPE transformer is proved to be permutation equivariant with respect to the users within each multicast group, and also permutation equivariant with respect to different multicast groups. Simulation results demonstrate that the proposed HPE transformer outperforms state-of-the-art optimization-based and deep learning-based approaches for multi-group multicast beamforming design in terms of the total transmit power, the constraint violation, and the computational time. In addition, the proposed HPE transformer achieves pretty good generalization performance on different numbers of users, different numbers of multicast groups, and different signal-to-interference-plus-noise ratio targets.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16084",
        "abstract url": "https://arxiv.org/abs/2402.16084",
        "title": "From Concept to Implementation: Streamlining Sensor and Actuator Selection for Collaborative Design and Engineering of Interactive Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Selecting appropriate sensors and actuators is a pivotal aspect of design and engineering, particularly in projects involving interactive systems. This article introduces the Design Thinking Based Iterative Sensor and Actuator Selection Flow, a structured decision-making approach aimed at streamlining this essential, yet often complex task. Created to accommodate individuals with diverse levels of technical expertise, our approach is uniquely suited for interdisciplinary teams of designers and engineers. Through the application of the flow to four real-world case studies, we highlight its broad applicability and demonstrate its efficacy in expediting project timelines and enhancing resource utilization. Our work lays a foundation for a more streamlined and user-centered process in selecting sensors and actuators, significantly benefiting the practice of interactive system design. This contribution serves as a seminal foundation for future research, offering significant contributions to both academic inquiry and practical applications across various industries. While the focus of the flow is on streamlining the selection process rather than on in-depth technical considerations, which are beyond the scope of this study, it provides a comprehensive guide for efficient and informed decision-making in the realm of interactive system design.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "21 pages, 14 figures, journal"
    },
    {
        "paper id": "2402.16094",
        "abstract url": "https://arxiv.org/abs/2402.16094",
        "title": "Bistochastically private release of data streams with zero delay",
        "rating": "-10",
        "keywords": [],
        "abstract": "Although the bulk of the research in privacy and statistical disclosure control is designed for static data, more and more data are often collected as continuous streams, and extensions of popular privacy tools and models have been proposed for this scenario. However, most of these proposals require buffers, where incoming individuals are momentarily stored, anonymized, and then released following a delay, thus considering a data stream as a succession of batches while it is by nature continuous. Having a delay unavoidably alters data freshness but also, more critically, inordinately exerts constraints on what can be achieved in terms of protection and information preservation. By considering randomized response, and specifically its recent bistochastic extension, in the context of dynamic data, this paper proposes a protocol for the anonymization of data streams that achieves zero delay while exhibiting formal privacy guarantees. Using a new tool in the privacy literature that introduces the concept of elementary plausible deniability, we show that it is feasible to achieve an atomic processing of individuals entering a stream, in-stead of proceeding by batches. We illustrate the application of the proposed approach by an empirical example.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16095",
        "abstract url": "https://arxiv.org/abs/2402.16095",
        "title": "chainBoost: A Secure Performance Booster for Blockchain-based Resource Markets",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cryptocurrencies and blockchain technology provide an innovative model for reshaping digital services. Driven by the movement toward Web 3.0, recent systems started to provide distributed services, such as computation outsourcing or file storage, on top of the currency exchange medium. By allowing anyone to join and collect cryptocurrency payments for serving others, these systems create decentralized markets for trading digital resources. Yet, there is still a big gap between the promise of these markets and their practical viability. Existing initiatives are still early-stage and have already encountered security and efficiency obstacles. At the same time, existing work around promising ideas, specifically sidechains, fall short in exploiting their full potential in addressing these problems. To bridge this gap, we propose chainBoost, a secure performance booster for decentralized resource markets. It expedites service related operations, reduces the blockchain size, and supports flexible service-payment exchange modalities at low overhead. At its core, chainBoost employs a sidechain, that has a (security and semantic) mutual-dependence with the mainchain, to which the system offloads heavy/frequent operations. To enable it, we develop a novel sidechain architecture composed of temporary and permanent blocks, a block suppression mechanism to prune the sidechain, a syncing protocol to permit arbitrary data exchange between the two chains, and an autorecovery protocol to support robustness and resilience. We analyze the security of chainBoost, and implement a proof-of-concept prototype for a distributed file storage market as a use case. For a market handling around 2000 transactions per round, our experiments show up to 11x improvement in throughput and 94\\% reduction in confirmation time. They also show that chainBoost can reduce the main blockchain size by around 90%.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16116",
        "abstract url": "https://arxiv.org/abs/2402.16116",
        "title": "On Performance of RIS-Aided Fluid Antenna Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This letter studies the performance of reconfigurable intelligent surface (RIS)-aided communications for a fluid antenna system (FAS) enabled receiver. Specifically, a fixed singleantenna base station (BS) transmits information through a RIS to a mobile user (MU) which is equipped with a planar fluid antenna in the absence of a direct link.We first analyze the spatial correlation structures among the positions (or ports) in the planar FAS, and then derive the joint distribution of the equivalent channel gain at the user by exploiting the central limit theorem. Furthermore, we obtain compact analytical expressions for the outage probability (OP) and delay outage rate (DOR). Numerical results illustrate that using FAS with only one activated port into the RIS-aided communication network can greatly enhance the performance, when compared to traditional antenna systems (TAS).",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16129",
        "abstract url": "https://arxiv.org/abs/2402.16129",
        "title": "Localization in Reconfigurable Intelligent Surface Aided mmWave Systems: A Multiple Measurement Vector Based Channel Estimation Method",
        "rating": "-10",
        "keywords": [],
        "abstract": "The sparsity of millimeter wave (mmWave) channels in the angular and temporal domains is beneficial to channel estimation, while the associated channel parameters can be utilized for localization. However, line-of-sight (LoS) blockage poses a significant challenge on the localization in mmWave systems, potentially leading to substantial positioning errors. A promising solution is to employ reconfigurable intelligent surface (RIS) to generate the virtual line-of-sight (VLoS) paths to aid localization. Consequently, wireless localization in the RIS-assisted mmWave systems has become the essential research issue. In this paper, a multiple measurement vector (MMV) model is constructed and a two-stage channel estimation based localization scheme is proposed. During the first stage, by exploiting the beamspace sparsity and employing a random RIS phase shift matrix, the channel parameters are estimated, based on which the precoder at base station and combiner at user equipment (UE) are designed. Then, in the second stage, based on the designed precoding and combining matrices, the optimal phase shift matrix for RIS is designed using the proposed modified temporally correlated multiple sparse Bayesian learning (TMSBL) algorithm. Afterwards, the channel parameters, such as angle of reflection, time-of-arrival, etc., embedding location information are estimated for finally deriving the location of UE. We demonstrate the achievable performance of the proposed algorithm and compare it with the state-of-the-art algorithms. Our studies show that the proposed localization scheme is capable of achieving centimeter level localization accuracy, when LoS path is blocked. Furthermore, the proposed algorithm has a low computational complexity and outperforms the legacy algorithms in different perspectives.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16144",
        "abstract url": "https://arxiv.org/abs/2402.16144",
        "title": "100 Gbps Indoor Access and 4.8 Gbps Outdoor Point-to-Point LiFi Transmission Systems using Laser-based Light Sources",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we demonstrate the communication capabilities of light-fidelity (LiFi) systems based on highbrightness and high-bandwidth integrated laser-based sources in a surface mount device (SMD) packaging platform. The laserbased source is able to deliver 450 lumens of white light illumination and the resultant light brightness is over 1000 cd mm2. It is demonstrated that a wavelength division multiplexing (WDM) LiFi system with ten parallel channels is able to deliver over 100 Gbps data rate with the assistance of Volterra filter-based nonlinear equalisers. In addition, an aggregated transmission data rate of 4.8 Gbps has been achieved over a link distance of 500 m with the same type of SMD light source. This work demonstrates the scalability of LiFi systems that employ laserbased light sources, particularly in their capacity to enable highspeed short range, as well as long-range data transmission.",
        "subjects": [
            "eess.SY",
            "physics.optics"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16145",
        "abstract url": "https://arxiv.org/abs/2402.16145",
        "title": "Egalitarian Price of Fairness for Indivisible Goods",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the context of fair division, the concept of price of fairness has been introduced to quantify the loss of welfare when we have to satisfy some fairness condition. In other words, it is the price we have to pay to guarantee fairness. Various settings of fair division have been considered previously; we extend to the setting of indivisible goods by using egalitarian welfare as the welfare measure, instead of the commonly used utilitarian welfare. We provide lower and upper bounds for various fairness and efficiency conditions such as envy-freeness up to one good (EF1) and maximum Nash welfare (MNW).",
        "subjects": [
            "cs.GT",
            "cs.MA"
        ],
        "comment": "A shorter version appears in the 20th Pacific Rim International Conference on Artificial Intelligence (PRICAI), 2023"
    },
    {
        "paper id": "2402.16156",
        "abstract url": "https://arxiv.org/abs/2402.16156",
        "title": "Radio Maps for Beam Alignment in mmWave Communications with Location Uncertainty",
        "rating": "-10",
        "keywords": [],
        "abstract": "Next generation communication systems require accurate beam alignment to counteract the impairments that characterize propagation in high-frequency bands. The overhead of the pilot sequences required to select the best beam pair is prohibitive when codebooks contain a large number of beams, as is the case in practice. To remedy this issue, some schemes exploit information about the user location to predict the best beam pair. However, these schemes (i) involve no measurements whatsoever, which generally results in a highly suboptimal predicted beam, and (ii) are not robust to localization errors. To address these limitations, this paper builds upon the notion of radio map to develop two algorithms that attain a balance between the quality of the obtained beam pair and measurement overhead. The proposed algorithms predict the received power corresponding to each pair and measure just the Q pairs with highest prediction. While the first algorithm targets simplicity, the second one relies on a Bayesian approach to endow the prediction process with robustness to localization error. The performance of both algorithms is shown to widely outperform existing methods using ray-tracing data.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16162",
        "abstract url": "https://arxiv.org/abs/2402.16162",
        "title": "Catch Me If You Can: Combatting Fraud in Artificial Currency Based Government Benefits Programs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Artificial currencies have grown in popularity in many real-world resource allocation settings, gaining traction in government benefits programs like food assistance and transit benefits programs. However, such programs are susceptible to misreporting fraud, wherein users can misreport their private attributes to gain access to more artificial currency (credits) than they are entitled to. To address the problem of misreporting fraud in artificial currency based benefits programs, we introduce an audit mechanism that induces a two-stage game between an administrator and users. In our proposed mechanism, the administrator running the benefits program can audit users at some cost and levy fines against them for misreporting their information. For this audit game, we study the natural solution concept of a signaling game equilibrium and investigate conditions on the administrator budget to establish the existence of equilibria. The computation of equilibria can be done via linear programming in our problem setting through an appropriate design of the audit rules. Our analysis also provides upper bounds that hold in any signaling game equilibrium on the expected excess payments made by the administrator and the probability that users misreport their information. We further show that the decrease in misreporting fraud corresponding to our audit mechanism far outweighs the administrator spending to run it by establishing that its total costs are lower than that of the status quo with no audits. Finally, to highlight the practical viability of our audit mechanism in mitigating misreporting fraud, we present a case study based on the Washington D.C. federal transit benefits program. In this case study, the proposed audit mechanism achieves several orders of magnitude improvement in total cost compared to a no-audit strategy for some parameter ranges.",
        "subjects": [
            "eess.SY",
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16170",
        "abstract url": "https://arxiv.org/abs/2402.16170",
        "title": "Nonparametric Steady-state Learning for Robust Output Regulation of Nonlinear Output Feedback Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This article addresses the nonadaptive and robust output regulation problem of the general nonlinear output feedback system with error output. The global robust output regulation problem for a class of general output feedback nonlinear systems with an uncertain exosystem and high relative degree can be tackled by constructing a linear generic internal model provided that a continuous nonlinear mapping exists. Leveraging the presented nonadaptive framework facilitates the conversion of the nonlinear robust output regulation problem into a robust nonadaptive stabilization endeavour for the augmented system endowed with Input-to-State Stable dynamics, removing the need for constructing a specific Lyapunov function with positive semidefinite derivatives. To ensure the feasibility of the nonlinear mapping, the approach is extended by incorporating the nonparametric learning framework. Moreover, the introduced nonparametric learning framework provides the ability to learn the dynamics of the steady-state/input behaviour from the signal generated from the internal model only using the output error feedback. As a result, the nonadaptive/nonparametric approach can be advantageous by guaranteeing convergence of the estimation and tracking error even when the underlying controlled system dynamics are complex or poorly understood. The effectiveness of the theoretical results is illustrated for a controlled duffing system and a continuously stirred tank reactor",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": "11 pages, 10 figures"
    },
    {
        "paper id": "2402.16171",
        "abstract url": "https://arxiv.org/abs/2402.16171",
        "title": "How to avoid the commuting conversions of IPC",
        "rating": "-10",
        "keywords": [],
        "abstract": "Since the observation in 2006 that it is possible to embed IPC into the atomic polymorphic lambda-calculus (a predicative fragment of system F with universal instantiations restricted to atomic formulas) different such embeddings appeared in the literature. All of them comprise the Russell-Prawitz translation of formulas, but have different strategies for the translation of proofs. Although these embeddings preserve proof identity, all fail in delivering preservation of reduction steps. In fact, they translate the commuting conversions of IPC to beta-equality, or to other kinds of reduction or equality generated by new principles added to system F. The cause for this is the generation of redexes by the translation itself. In this paper, we present an embedding of IPC into atomic system F, still based on the same translation of formulas, but which maps commuting conversions to syntactic identity, while simulating the other kinds of reduction steps present in IPC beta\u03b7-reduction. In this sense the translation achieves a truly commuting conversion-free image of IPC in atomic system F.",
        "subjects": [
            "cs.LO",
            "math.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16186",
        "abstract url": "https://arxiv.org/abs/2402.16186",
        "title": "An Execution-time-certified Riccati-based IPM Algorithm for RTI-based Input-constrained NMPC",
        "rating": "-10",
        "keywords": [],
        "abstract": "Establishing an execution time certificate in deploying model predictive control (MPC) is a pressing and challenging requirement. As nonlinear MPC (NMPC) results in nonlinear programs, differing from quadratic programs encountered in linear MPC, deriving an execution time certificate for NMPC seems an impossible task. Our prior work \\cite{wu2023direct} introduced an input-constrained MPC algorithm with the exact and only \\textit{dimension-dependent} (\\textit{data-independent}) number of floating-point operations ([flops]). This paper extends it to input-constrained NMPC problems via the real-time iteration (RTI) scheme, which results in \\textit{data-varying} (but \\textit{dimension-invariant}) input-constrained MPC problems. Therefore, applying our previous algorithm can certify the execution time based on the assumption that processors perform fixed [flops] in constant time. As the RTI-based scheme generally results in MPC with a long prediction horizon, this paper employs the efficient factorized Riccati recursion, whose computational cost scales linearly with the prediction horizon, to solve the Newton system at each iteration. The execution-time certified capability of the algorithm is theoretically and numerically validated through a case study involving nonlinear control of the chaotic Lorenz system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "7 pages"
    },
    {
        "paper id": "2402.16225",
        "abstract url": "https://arxiv.org/abs/2402.16225",
        "title": "Discrete Fourier Transform Approximations Based on the Cooley-Tukey Radix-2 Algorithm",
        "rating": "-10",
        "keywords": [],
        "abstract": "This report elaborates on approximations for the discrete Fourier transform by means of replacing the exact Cooley-Tukey algorithm twiddle-factors by low-complexity integers, such as $0, \\pm \\frac{1}{2}, \\pm 1$.",
        "subjects": [
            "eess.SP",
            "math.NA",
            "stat.CO",
            "stat.ME"
        ],
        "comment": "Originally written in Oct 2016. Contains 55 pages, 17 figures, 4 tables"
    },
    {
        "paper id": "2402.16259",
        "abstract url": "https://arxiv.org/abs/2402.16259",
        "title": "Problems on Group-labeled Matroid Bases",
        "rating": "-10",
        "keywords": [],
        "abstract": "Consider a matroid equipped with a labeling of its ground set to an abelian group. We define the label of a subset of the ground set as the sum of the labels of its elements. We study a collection of problems on finding bases and common bases of matroids with restrictions on their labels. For zero bases and zero common bases, the results are mostly negative. While finding a non-zero basis of a matroid is not difficult, it turns out that the complexity of finding a non-zero common basis depends on the group. Namely, we show that the problem is hard for a fixed group if it contains an element of order two, otherwise it is polynomially solvable. As a generalization of both zero and non-zero constraints, we further study $F$-avoiding constraints where we seek a basis or common basis whose label is not in a given set $F$ of forbidden labels. Using algebraic techniques, we give a randomized algorithm for finding an $F$-avoiding common basis of two matroids represented over the same field for finite groups given as operation tables. The study of $F$-avoiding bases with groups given as oracles leads to a conjecture stating that whenever an $F$-avoiding basis exists, an $F$-avoiding basis can be obtained from an arbitrary basis by exchanging at most $|F|$ elements. We prove the conjecture for the special cases when $|F|\\le 2$ or the group is ordered. By relying on structural observations on matroids representable over fixed, finite fields, we verify a relaxed version of the conjecture for these matroids. As a consequence, we obtain a polynomial-time algorithm in these special cases for finding an $F$-avoiding basis when $|F|$ is fixed.",
        "subjects": [
            "cs.DM",
            "cs.DS",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16262",
        "abstract url": "https://arxiv.org/abs/2402.16262",
        "title": "CoGenT: A Content-oriented Generative-hit Framework for Content Delivery Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "The service provided by content delivery networks (CDNs) may overlook content locality, leaving room for potential performance improvement. In this study, we explore the feasibility of leveraging generated data as a replacement for fetching data in missing scenarios based on content locality. Due to sufficient local computing resources and reliable generation efficiency, we propose a content-oriented generative-hit framework (CoGenT) for CDNs. CoGenT utilizes idle computing resources on edge nodes to generate requested data based on similar or related cached data to achieve hits. Our implementation in a real-world system demonstrates that CoGenT reduces the average access latency by half. Additionally, experiments conducted on a simulator also confirm that CoGenT can enhance existing caching algorithms, resulting in reduced latency and bandwidth usage.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16279",
        "abstract url": "https://arxiv.org/abs/2402.16279",
        "title": "Quadratic Message Passing for Generalized Quadratic Equations Model",
        "rating": "-10",
        "keywords": [],
        "abstract": "For approximate inference in the generalized quadratic equations model, many state-of-the-art algorithms lack any prior knowledge of the target signal structure, exhibits slow convergence, and can not handle any analytic prior knowledge of the target signal structure. So, this paper proposes a new algorithm, Quadratic Message passing (QMP). QMP has a complexity as low as $O(N^{3})$. The SE derived for QMP can capture precisely the per-iteration behavior of the simulated algorithm. Simulation results confirm QMP outperforms many state-of-the-art algorithms.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16284",
        "abstract url": "https://arxiv.org/abs/2402.16284",
        "title": "Self-Assembly of Patterns in the abstract Tile Assembly Model",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the abstract Tile Assembly Model, self-assembling systems consisting of tiles of different colors can form structures on which colored patterns are ``painted.'' We explore the complexity, in terms of the numbers of unique tile types required, of assembling various patterns. We first demonstrate how to efficiently self-assemble a set of simple patterns, then show tight bounds on the tile type complexity of self-assembling 2-colored patterns on the surfaces of square assemblies. Finally, we demonstrate an exponential gap in tile type complexity of self-assembling an infinite series of patterns between systems restricted to one plane versus those allowed two planes.",
        "subjects": [
            "cs.ET",
            "cs.CC",
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.16290",
        "abstract url": "https://arxiv.org/abs/2402.16290",
        "title": "Card-Based Overwriting Protocol for Equality Function and Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "Research in the area of secure multi-party computation with an unconventional method of using a physical deck of playing cards began in 1989 when den Boar proposed a protocol to compute the logical AND function using five cards. Since then, the area has gained interest from many researchers and several card-based protocols to compute various functions have been developed. In this paper, we propose a card-based protocol called the overwriting protocol that can securely compute the $k$-candidate $n$-variable equality function $f: \\{0,1,\\ldots ,k-1\\}^n \\rightarrow \\{0,1\\}$. We also apply the technique used in this protocol to compute other similar functions.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    }
]