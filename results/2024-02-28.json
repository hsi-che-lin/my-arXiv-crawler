[
    {
        "paper id": "2402.18476",
        "abstract url": "https://arxiv.org/abs/2402.18476",
        "title": "IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding",
        "rating": 2,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18086",
        "abstract url": "https://arxiv.org/abs/2402.18086",
        "title": "Generalizable Two-Branch Framework for Image Class-Incremental Learning",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Deep neural networks often severely forget previously learned knowledge when learning new knowledge. Various continual learning (CL) methods have been proposed to handle such a catastrophic forgetting issue from different perspectives and achieved substantial improvements. In this paper, a novel two-branch continual learning framework is proposed to further enhance most existing CL methods. Specifically, the main branch can be any existing CL model and the newly introduced side branch is a lightweight convolutional network. The output of each main branch block is modulated by the output of the corresponding side branch block. Such a simple two-branch model can then be easily implemented and learned with the vanilla optimization setting without whistles and bells. Extensive experiments with various settings on multiple image datasets show that the proposed framework yields consistent improvements over state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages,3 figures,accepted by ICASSP 2024"
    },
    {
        "paper id": "2402.18091",
        "abstract url": "https://arxiv.org/abs/2402.18091",
        "title": "Polos: Multimodal Metric Learning from Human Feedback for Image Captioning",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets. Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2402.18115",
        "abstract url": "https://arxiv.org/abs/2402.18115",
        "title": "UniVS: Unified and Universal Video Segmentation with Prompts as Queries",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \\url{https://github.com/MinghanLi/UniVS}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "21 pages, 11 figures, 10 tabels, CVPR2024"
    },
    {
        "paper id": "2402.18144",
        "abstract url": "https://arxiv.org/abs/2402.18144",
        "title": "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
        "rating": 1.5,
        "keywords": [
            [
                "social biases"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose \"random silicon sampling,\" a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "25 pages, 4 figures, 19 Tables"
    },
    {
        "paper id": "2402.18201",
        "abstract url": "https://arxiv.org/abs/2402.18201",
        "title": "Learning Invariant Inter-pixel Correlations for Superpixel Generation",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones. Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset. Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios. To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise. Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations. Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations. Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles. The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency. Code and pre-trained model are available at https://github.com/rookiie/CDSpixel.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI24"
    },
    {
        "paper id": "2402.18217",
        "abstract url": "https://arxiv.org/abs/2402.18217",
        "title": "Region-Aware Exposure Consistency Network for Mixed Exposure Correction",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects. Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions. The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process. In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations. Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space. Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information. To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity. Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method. The code is released at: https://github.com/kravrolens/RECNet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI 2024"
    },
    {
        "paper id": "2402.18277",
        "abstract url": "https://arxiv.org/abs/2402.18277",
        "title": "Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene's actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2402.18407",
        "abstract url": "https://arxiv.org/abs/2402.18407",
        "title": "Why does music source separation benefit from cacophony?",
        "rating": 1.5,
        "keywords": [
            [
                "eess.AS"
            ],
            [
                "Workshop",
                "ICASSP"
            ]
        ],
        "abstract": "In music source separation, a standard training data augmentation procedure is to create new training samples by randomly combining instrument stems from different songs. These random mixes have mismatched characteristics compared to real music, e.g., the different stems do not have consistent beat or tonality, resulting in a cacophony. In this work, we investigate why random mixing is effective when training a state-of-the-art music source separation model in spite of the apparent distribution shift it creates. Additionally, we examine why performance levels off despite potentially limitless combinations, and examine the sensitivity of music source separation performance to differences in beat and tonality of the instrumental sources in a mixture.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "ICASSP 2024 Workshop on Explainable AI for Speech and Audio"
    },
    {
        "paper id": "2402.18409",
        "abstract url": "https://arxiv.org/abs/2402.18409",
        "title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
        "rating": 1.5,
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the \"Cookie Theft\" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18411",
        "abstract url": "https://arxiv.org/abs/2402.18411",
        "title": "Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data. Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment. However, these segregated strategies overlook the potential synergies between these tasks. This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework. ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR. By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios. Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning. This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness. ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets. Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is available at https://github.com/HCVLAB/ProtoOT.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI2024"
    },
    {
        "paper id": "2402.18449",
        "abstract url": "https://arxiv.org/abs/2402.18449",
        "title": "HOP to the Next Tasks and Domains for Continual Learning in NLP",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of our HOP.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "AAAI 2024. Main + supplmentary"
    },
    {
        "paper id": "2402.18467",
        "abstract url": "https://arxiv.org/abs/2402.18467",
        "title": "Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Weakly supervised semantic segmentation (WSSS) with image-level labels aims to achieve segmentation tasks without dense annotations. However, attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in WSSS. In this work, we devise a 'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to 'separate' the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to 'conquer' the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted, which guarantee the correctness of knowledge and further facilitate the discrepancy among co-contexts. We streamline the multi-staged WSSS pipeline end-to-end and tackle this issue without external supervision. Extensive experiments are conducted, validating the efficiency of our method and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code is available at https://github.com/zwyang6/SeCo.git.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2402.18528",
        "abstract url": "https://arxiv.org/abs/2402.18528",
        "title": "Gradient Reweighting: Towards Imbalanced Class-Incremental Learning",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge. A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024"
    },
    {
        "paper id": "2402.18811",
        "abstract url": "https://arxiv.org/abs/2402.18811",
        "title": "BFRFormer: Transformer-based generator for Real-World Blind Face Restoration",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Blind face restoration is a challenging task due to the unknown and complex degradation. Although face prior-based methods and reference-based methods have recently demonstrated high-quality results, the restored images tend to contain over-smoothed results and lose identity-preserved details when the degradation is severe. It is observed that this is attributed to short-range dependencies, the intrinsic limitation of convolutional neural networks. To model long-range dependencies, we propose a Transformer-based blind face restoration method, named BFRFormer, to reconstruct images with more identity-preserved details in an end-to-end manner. In BFRFormer, to remove blocking artifacts, the wavelet discriminator and aggregated attention module are developed, and spectral normalization and balanced consistency regulation are adaptively applied to address the training instability and over-fitting problem, respectively. Extensive experiments show that our method outperforms state-of-the-art methods on a synthetic dataset and four real-world datasets. The source code, Casia-Test dataset, and pre-trained models are released at https://github.com/s8Znk/BFRFormer.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ICASSP 2024"
    },
    {
        "paper id": "2402.18821",
        "abstract url": "https://arxiv.org/abs/2402.18821",
        "title": "Debiased Novel Category Discovering and Localization",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "In recent years, object detection in deep learning has experienced rapid development. However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set. These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery. We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI 2024"
    },
    {
        "paper id": "2403.08822",
        "abstract url": "https://arxiv.org/abs/2403.08822",
        "title": "LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models",
        "rating": 1.5,
        "keywords": [
            [
                "parameter-efficient",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In addressing the computational and memory demands of fine-tuning Large Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter Adaptation), a novel approach utilizing randomized half-selective parameter freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently balances pre-trained knowledge retention and adaptability for task-specific optimizations. Through a randomized mechanism, LoRA-SP determines which parameters to update or freeze, significantly reducing computational and memory requirements without compromising model performance. We evaluated LoRA-SP across several benchmark NLP tasks, demonstrating its ability to achieve competitive performance with substantially lower resource consumption compared to traditional full-parameter fine-tuning and other parameter-efficient techniques. LoRA-SP innovative approach not only facilitates the deployment of advanced NLP models in resource-limited settings but also opens new research avenues into effective and efficient model adaptation strategies.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18048",
        "abstract url": "https://arxiv.org/abs/2402.18048",
        "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "preprint, 9 pages, 5 figures"
    },
    {
        "paper id": "2402.18050",
        "abstract url": "https://arxiv.org/abs/2402.18050",
        "title": "MEGAnno+: A Human-LLM Collaborative Annotation System",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EACL 2024 Demo"
    },
    {
        "paper id": "2402.18054",
        "abstract url": "https://arxiv.org/abs/2402.18054",
        "title": "Contextualizing Generated Citation Texts",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Abstractive citation text generation is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context. However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference papers main contribution, ignoring the citation contexts focus on a different topic. To address this problem, we propose a simple modification to the citation text generation task: the generation target is not only the citation itself, but the entire context window, including the target citation. This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and allows the generation model to make use of contextual clues about what topic to discuss and what stance to take.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18056",
        "abstract url": "https://arxiv.org/abs/2402.18056",
        "title": "Improvement Of Audiovisual Quality Estimation Using A Nonlinear Autoregressive Exogenous Neural Network And Bitstream Parameters",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "With the increasing demand for audiovisual services, telecom service providers and application developers are compelled to ensure that their services provide the best possible user experience. Particularly, services such as videoconferencing are very sensitive to network conditions. Therefore, their performance should be monitored in real time in order to adjust parameters to any network perturbation. In this paper, we developed a parametric model for estimating the perceived audiovisual quality in videoconference services. Our model is developed with the nonlinear autoregressive exogenous (NARX) recurrent neural network and estimates the perceived quality in terms of mean opinion score (MOS). We validate our model using the publicly available INRS bitstream audiovisual quality dataset. This dataset contains bitstream parameters such as loss per frame, bit rate and video duration. We compare the proposed model against state-of-the-art methods based on machine learning and show our model to outperform these methods in terms of mean square error (MSE=0.150) and Pearson correlation coefficient (R=0.931)",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18061",
        "abstract url": "https://arxiv.org/abs/2402.18061",
        "title": "On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clean data; (4) Inference on the test data. The experimental results show that Clean-LaVe can outperform the baseline by 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation classification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot cross-lingual relation classification task, and by 8% on ACE05-E+ in the zero-shot event argument classification task. The code is share in https://github.com/wjw136/Clean_LaVe.git.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "accepted by coling2024. arXiv:2211.13883 is our first edition"
    },
    {
        "paper id": "2402.18068",
        "abstract url": "https://arxiv.org/abs/2402.18068",
        "title": "SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model",
        "rating": 1,
        "keywords": [
            [
                "Vision-Language",
                "VLM"
            ],
            [
                "diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18109",
        "abstract url": "https://arxiv.org/abs/2402.18109",
        "title": "Dual-Context Aggregation for Universal Image Matting",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Natural image matting aims to estimate the alpha matte of the foreground from a given image. Various approaches have been explored to address this problem, such as interactive matting methods that use guidance such as click or trimap, and automatic matting methods tailored to specific objects. However, existing matting methods are designed for specific objects or guidance, neglecting the common requirement of aggregating global and local contexts in image matting. As a result, these methods often encounter challenges in accurately identifying the foreground and generating precise boundaries, which limits their effectiveness in unforeseen scenarios. In this paper, we propose a simple and universal matting framework, named Dual-Context Aggregation Matting (DCAM), which enables robust image matting with arbitrary guidance or without guidance. Specifically, DCAM first adopts a semantic backbone network to extract low-level features and context features from the input image and guidance. Then, we introduce a dual-context aggregation network that incorporates global object aggregators and local appearance aggregators to iteratively refine the extracted context features. By performing both global contour segmentation and local boundary refinement, DCAM exhibits robustness to diverse types of guidance and objects. Finally, we adopt a matting decoder network to fuse the low-level features and the refined context features for alpha matte estimation. Experimental results on five matting datasets demonstrate that the proposed DCAM outperforms state-of-the-art matting methods in both automatic matting and interactive matting tasks, which highlights the strong universality and high performance of DCAM. The source code is available at \\url{https://github.com/Windaway/DCAM}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18113",
        "abstract url": "https://arxiv.org/abs/2402.18113",
        "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a \"teacher\" generating data, as well as a \"critic\" evaluating the student's performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18117",
        "abstract url": "https://arxiv.org/abs/2402.18117",
        "title": "PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations. In addition, we generate Virtual Negatives (VNs) based on GDP to involve the contrastive learning process. Extensive experiments on two public benchmarks demonstrate the superiority of our PRCL framework.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "19 pages, 11 figures"
    },
    {
        "paper id": "2402.18120",
        "abstract url": "https://arxiv.org/abs/2402.18120",
        "title": "Exploring Multilingual Concepts of Human Value in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source language. Drawing from our findings on multilingual value alignment, we prudently provide suggestions on the composition of multilingual data for LLMs pre-training: including a limited number of dominant languages for cross-lingual alignment transfer while avoiding their excessive prevalence, and keeping a balanced distribution of non-dominant languages. We aspire that our findings would contribute to enhancing the safety and utility of multilingual AI.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18121",
        "abstract url": "https://arxiv.org/abs/2402.18121",
        "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "5 pages, 10 figures"
    },
    {
        "paper id": "2402.18128",
        "abstract url": "https://arxiv.org/abs/2402.18128",
        "title": "Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at: https://github.com/Alexiland/MLOMAE",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18133",
        "abstract url": "https://arxiv.org/abs/2402.18133",
        "title": "Classes Are Not Equal: An Empirical Study on Image Recognition Fairness",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further, we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification. The Code is available at https://github.com/dvlab-research/Parametric-Contrastive-Learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2402.18139",
        "abstract url": "https://arxiv.org/abs/2402.18139",
        "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multiple causal reasoning tasks such as causal discovery, causal identification and counterfactual reasoning. The counterfactual sentences add explicit knowledge of the not caused by scenarios. By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability. Evaluation of benchmark datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores. We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18145",
        "abstract url": "https://arxiv.org/abs/2402.18145",
        "title": "Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Gradient-based explanation methods are increasingly used to interpret neural models in natural language processing (NLP) due to their high fidelity. Such methods determine word-level importance using dimension-level gradient values through a norm function, often presuming equal significance for all gradient dimensions. However, in the context of Aspect-based Sentiment Analysis (ABSA), our preliminary research suggests that only specific dimensions are pertinent. To address this, we propose the Information Bottleneck-based Gradient (\\texttt{IBG}) explanation framework for ABSA. This framework leverages an information bottleneck to refine word embeddings into a concise intrinsic dimension, maintaining essential features and omitting unrelated information. Comprehensive tests show that our \\texttt{IBG} approach considerably improves both the models' performance and interpretability by identifying sentiment-aware features.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by COLING 2024"
    },
    {
        "paper id": "2402.18150",
        "abstract url": "https://arxiv.org/abs/2402.18150",
        "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18154",
        "abstract url": "https://arxiv.org/abs/2402.18154",
        "title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by pruning conflicting attention heads without updating model parameters. PH3 can flexibly control eight LMs to use internal memory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3 can also improve the performance of LMs on open-domain QA tasks. We also conduct extensive experiments to demonstrate the cross-model, cross-relation, and cross-format generalization of our method.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "21 pages, 42 figures, 4 tables"
    },
    {
        "paper id": "2402.18158",
        "abstract url": "https://arxiv.org/abs/2402.18158",
        "title": "Evaluating Quantized Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their applicability. Based on the extensive experiments, we systematically summarize the effect of quantization, provide recommendations to apply quantization techniques, and point out future directions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18162",
        "abstract url": "https://arxiv.org/abs/2402.18162",
        "title": "Out-of-Distribution Detection using Neural Activation Prior",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Out-of-distribution detection (OOD) is a crucial technique for deploying machine learning models in the real world to handle the unseen scenarios. In this paper, we first propose a simple yet effective Neural Activation Prior (NAP) for OOD detection. Our neural activation prior is based on a key observation that, for a channel before the global pooling layer of a fully trained neural network, the probability of a few neurons being activated with a large response by an in-distribution (ID) sample is significantly higher than that by an OOD sample. An intuitive explanation is that for a model fully trained on ID dataset, each channel would play a role in detecting a certain pattern in the ID dataset, and a few neurons can be activated with a large response when the pattern is detected in an input sample. Then, a new scoring function based on this prior is proposed to highlight the role of these strongly activated neurons in OOD detection. Our approach is plug-and-play and does not lead to any performance degradation on ID data classification and requires no extra training or statistics from training or external datasets. Notice that previous methods primarily rely on post-global-pooling features of the neural networks, while the within-channel distribution information we leverage would be discarded by the global pooling operator. Consequently, our method is orthogonal to existing approaches and can be effectively combined with them in various applications. Experimental results show that our method achieves the state-of-the-art performance on CIFAR benchmark and ImageNet dataset, which demonstrates the power of the proposed prior. Finally, we extend our method to Transformers and the experimental findings indicate that NAP can also significantly enhance the performance of OOD detection on Transformers, thereby demonstrating the broad applicability of this prior knowledge.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18163",
        "abstract url": "https://arxiv.org/abs/2402.18163",
        "title": "Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, model quantization for face recognition has gained prominence. Traditionally, compressing models involved vast datasets like the 5.8 million-image MS1M dataset as well as extensive training times, raising the question of whether such data enormity is essential. This paper addresses this by introducing an efficiency-driven approach, fine-tuning the model with just up to 14,000 images, 440 times smaller than MS1M. We demonstrate that effective quantization is achievable with a smaller dataset, presenting a new paradigm. Moreover, we incorporate an evaluation-based metric loss and achieve an outstanding 96.15% accuracy on the IJB-C dataset, establishing a new state-of-the-art compressed model training for face recognition. The subsequent analysis delves into potential applications, emphasizing the transformative power of this approach. This paper advances model quantization by highlighting the efficiency and optimal results with small data and training time.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18169",
        "abstract url": "https://arxiv.org/abs/2402.18169",
        "title": "MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention knowledge base featuring 1,372K intentions rooted in 137,287 posts. We conduct a two-stage annotation to verify the quality of the generated knowledge and benchmark the performance of widely used LLMs for intention generation. We further apply MIKO to a sarcasm detection dataset and distill a student model to demonstrate the downstream benefits of applying intention knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "11 pages, 5 figures"
    },
    {
        "paper id": "2402.18171",
        "abstract url": "https://arxiv.org/abs/2402.18171",
        "title": "Digging Into Normal Incorporated Stereo Matching",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, disparity estimation in low-texture, occluded, and bordered regions still remains a bottleneck that limits the performance. To tackle these challenges, geometric guidance like plane information is necessary as it provides intuitive guidance about disparity consistency and affinity similarity. In this paper, we propose a normal incorporated joint learning framework consisting of two specific modules named non-local disparity propagation(NDP) and affinity-aware residual learning(ARL). The estimated normal map is first utilized for calculating a non-local affinity matrix and a non-local offset to perform spatial propagation at the disparity level. To enhance geometric consistency, especially in low-texture regions, the estimated normal map is then leveraged to calculate a local affinity matrix, providing the residual learning with information about where the correction should refer and thus improving the residual learning efficiency. Extensive experiments on several public datasets including Scene Flow, KITTI 2015, and Middlebury 2014 validate the effectiveness of our proposed method. By the time we finished this work, our approach ranked 1st for stereo matching across foreground pixels on the KITTI 2015 dataset and 3rd on the Scene Flow dataset among all the published works.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18178",
        "abstract url": "https://arxiv.org/abs/2402.18178",
        "title": "Reflection Removal Using Recurrent Polarization-to-Polarization Network",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper addresses reflection removal, which is the task of separating reflection components from a captured image and deriving the image with only transmission components. Considering that the existence of the reflection changes the polarization state of a scene, some existing methods have exploited polarized images for reflection removal. While these methods apply polarized images as the inputs, they predict the reflection and the transmission directly as non-polarized intensity images. In contrast, we propose a polarization-to-polarization approach that applies polarized images as the inputs and predicts \"polarized\" reflection and transmission images using two sequential networks to facilitate the separation task by utilizing the interrelated polarization information between the reflection and the transmission. We further adopt a recurrent framework, where the predicted reflection and transmission images are used to iteratively refine each other. Experimental results on a public dataset demonstrate that our method outperforms other state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18204",
        "abstract url": "https://arxiv.org/abs/2402.18204",
        "title": "ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "This paper proposes a method for Acoustic Constrained Segmentation (ACS) in audio recordings of vehicles driven through a production test track, delimiting the boundaries of surface types in the track. ACS is a variant of classical acoustic segmentation where the sequence of labels is known, contiguous and invariable, which is especially useful in this work as the test track has a standard configuration of surface types. The proposed ConvDTW-ACS method utilizes a Convolutional Neural Network for classifying overlapping image chunks extracted from the full audio spectrogram. Then, our custom Dynamic Time Warping algorithm aligns the sequence of predicted probabilities to the sequence of surface types in the track, from which timestamps of the surface type boundaries can be extracted. The method was evaluated on a real-world dataset collected from the Ford Manufacturing Plant in Valencia (Spain), achieving a mean error of 166 milliseconds when delimiting, within the audio, the boundaries of the surfaces in the track. The results demonstrate the effectiveness of the proposed method in accurately segmenting different surface types, which could enable the development of more specialized AI systems to improve the quality inspection process.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "12 pages, 2 figures"
    },
    {
        "paper id": "2402.18209",
        "abstract url": "https://arxiv.org/abs/2402.18209",
        "title": "DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Named entity recognition is one of the cornerstones of Danish NLP, essential for language technology applications within both industry and research. However, Danish NER is inhibited by a lack of available datasets. As a consequence, no current models are capable of fine-grained named entity recognition, nor have they been evaluated for potential generalizability issues across datasets and domains. To alleviate these limitations, this paper introduces: 1) DANSK: a named entity dataset providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) DaCy 2.6.0 that includes three generalizable models with fine-grained annotation; and 3) an evaluation of current state-of-the-art models' ability to generalize across domains. The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field. Shortcomings of the annotation quality of the dataset and its impact on model training and evaluation are also discussed. Despite these limitations, we advocate for the use of the new dataset DANSK alongside further work on the generalizability within Danish NER.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18216",
        "abstract url": "https://arxiv.org/abs/2402.18216",
        "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that many of the task-switches can lead to significant performance degradation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "16 pages, 11 figures, 10 tables"
    },
    {
        "paper id": "2402.18223",
        "abstract url": "https://arxiv.org/abs/2402.18223",
        "title": "Improving Open-Ended Text Generation via Adaptive Decoding",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.com/zwhong714/adaptive_decoding.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18225",
        "abstract url": "https://arxiv.org/abs/2402.18225",
        "title": "CogBench: a large language model walks into a psychology lab",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18233",
        "abstract url": "https://arxiv.org/abs/2402.18233",
        "title": "Zero-Shot Aerial Object Detection with Visual Description Regularization",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing object detection models are mainly trained on large-scale labeled datasets. However, annotating data for novel aerial object classes is expensive since it is time-consuming and may require expert knowledge. Thus, it is desirable to study label-efficient object detection methods on aerial images. In this work, we propose a zero-shot method for aerial object detection named visual Description Regularization, or DescReg. Concretely, we identify the weak semantic-visual correlation of the aerial objects and aim to address the challenge with prior descriptions of their visual appearance. Instead of directly encoding the descriptions into class embedding space which suffers from the representation gap problem, we propose to infuse the prior inter-class visual similarity conveyed in the descriptions into the embedding learning. The infusion process is accomplished with a newly designed similarity-aware triplet loss which incorporates structured regularization on the representation space. We conduct extensive experiments with three challenging aerial object detection datasets, including DIOR, xView, and DOTA. The results demonstrate that DescReg significantly outperforms the state-of-the-art ZSD methods with complex projection designs and generative frameworks, e.g., DescReg outperforms best reported ZSD method on DIOR by 4.5 mAP on unseen classes and 8.1 in HM. We further show the generalizability of DescReg by integrating it into generative ZSD methods as well as varying the detection architecture.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13 pages, 3 figures"
    },
    {
        "paper id": "2402.18243",
        "abstract url": "https://arxiv.org/abs/2402.18243",
        "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18252",
        "abstract url": "https://arxiv.org/abs/2402.18252",
        "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select the most suitable mental models for the problem, achieving or being near to the state-of-the-art results on diverse tasks such as STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We hope that the insights presented herein will stimulate further exploration of generalist prompting methods for LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18262",
        "abstract url": "https://arxiv.org/abs/2402.18262",
        "title": "Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. The pre-trained models and code are available at https://github.com/X-LANCE/weblm.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18264",
        "abstract url": "https://arxiv.org/abs/2402.18264",
        "title": "Retrieval-based Full-length Wikipedia Generation for Emergent Events",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In today's fast-paced world, the growing demand to quickly generate comprehensive and accurate Wikipedia documents for emerging events is both crucial and challenging. However, previous efforts in Wikipedia generation have often fallen short of meeting real-world requirements. Some approaches focus solely on generating segments of a complete Wikipedia document, while others overlook the importance of faithfulness in generation or fail to consider the influence of the pre-training corpus. In this paper, we simulate a real-world scenario where structured full-length Wikipedia documents are generated for emergent events using input retrieved from web sources. To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events, we select events that have taken place recently and introduce a new benchmark Wiki-GenBen, which consists of 309 events paired with their corresponding retrieved web pages for generating evidence. Additionally, we design a comprehensive set of systematic evaluation metrics and baseline methods, to evaluate the capability of LLMs in generating factual full-length Wikipedia documents. The data and code are open-sourced at WikiGenBench.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18272",
        "abstract url": "https://arxiv.org/abs/2402.18272",
        "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "22 pages, 5 figures, 10 tables"
    },
    {
        "paper id": "2402.18275",
        "abstract url": "https://arxiv.org/abs/2402.18275",
        "title": "Investigation of Adapter for Automatic Speech Recognition in Noisy Environment",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "Adapting an automatic speech recognition (ASR) system to unseen noise environments is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This study thoroughly investigates adapter-based ASR adaptation in noisy environments. We conducted experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. The simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training is still useful for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18281",
        "abstract url": "https://arxiv.org/abs/2402.18281",
        "title": "Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in SRL.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "work in progress"
    },
    {
        "paper id": "2402.18284",
        "abstract url": "https://arxiv.org/abs/2402.18284",
        "title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 2 figures"
    },
    {
        "paper id": "2402.18286",
        "abstract url": "https://arxiv.org/abs/2402.18286",
        "title": "Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis",
        "rating": 1,
        "keywords": [
            [
                "efficient fine-tuning"
            ],
            [
                "super-resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18302",
        "abstract url": "https://arxiv.org/abs/2402.18302",
        "title": "EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving",
        "rating": 1,
        "keywords": [
            [
                "Audio-visual"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack model and its components. The source code and datasets will be made publicly available at https://github.com/lab206/EchoTrack.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The source code and datasets will be made publicly available at https://github.com/lab206/EchoTrack"
    },
    {
        "paper id": "2402.18305",
        "abstract url": "https://arxiv.org/abs/2402.18305",
        "title": "NERV++: An Enhanced Implicit Neural Video Representation",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Neural fields, also known as implicit neural representations (INRs), have shown a remarkable capability of representing, generating, and manipulating various data types, allowing for continuous data reconstruction at a low memory footprint. Though promising, INRs applied to video compression still need to improve their rate-distortion performance by a large margin, and require a huge number of parameters and long training iterations to capture high-frequency details, limiting their wider applicability. Resolving this problem remains a quite challenging task, which would make INRs more accessible in compression tasks. We take a step towards resolving these shortcomings by introducing neural representations for videos NeRV++, an enhanced implicit neural video representation, as more straightforward yet effective enhancement over the original NeRV decoder architecture, featuring separable conv2d residual blocks (SCRBs) that sandwiches the upsampling block (UB), and a bilinear interpolation skip layer for improved feature representation. NeRV++ allows videos to be directly represented as a function approximated by a neural network, and significantly enhance the representation capacity beyond current INR-based video codecs. We evaluate our method on UVG, MCL JVC, and Bunny datasets, achieving competitive results for video compression with INRs. This achievement narrows the gap to autoencoder-based video coding, marking a significant stride in INR-based video compression research.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18307",
        "abstract url": "https://arxiv.org/abs/2402.18307",
        "title": "Feature Denoising For Low-Light Instance Segmentation Using Weighted Non-Local Blocks",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Instance segmentation for low-light imagery remains largely unexplored due to the challenges imposed by such conditions, for example shot noise due to low photon count, color distortions and reduced contrast. In this paper, we propose an end-to-end solution to address this challenging task. Based on Mask R-CNN, our proposed method implements weighted non-local (NL) blocks in the feature extractor. This integration enables an inherent denoising process at the feature level. As a result, our method eliminates the need for aligned ground truth images during training, thus supporting training on real-world low-light datasets. We introduce additional learnable weights at each layer in order to enhance the network's adaptability to real-world noise characteristics, which affect different feature scales in different ways. Experimental results show that the proposed method outperforms the pretrained Mask R-CNN with an Average Precision (AP) improvement of +10.0, with the introduction of weighted NL Blocks further enhancing AP by +1.0.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18312",
        "abstract url": "https://arxiv.org/abs/2402.18312",
        "title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of Llama-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token appear in the later half, attention heads that move information along ontological relationships appear in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18320",
        "abstract url": "https://arxiv.org/abs/2402.18320",
        "title": "Location-guided Head Pose Estimation for Fisheye Image",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye lens distortion in the peripheral region of the image leads to degraded performance of the existing head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created a fisheye-distorted version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Revised Introduction and Related Work; Submitted to lEEE Transactions on Cognitive and Developmental Systems for review"
    },
    {
        "paper id": "2402.18331",
        "abstract url": "https://arxiv.org/abs/2402.18331",
        "title": "FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes",
        "rating": 1,
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers' parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: https://finediffusion.github.io/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18334",
        "abstract url": "https://arxiv.org/abs/2402.18334",
        "title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18344",
        "abstract url": "https://arxiv.org/abs/2402.18344",
        "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18376",
        "abstract url": "https://arxiv.org/abs/2402.18376",
        "title": "Tokenization Is More Than Compression",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18385",
        "abstract url": "https://arxiv.org/abs/2402.18385",
        "title": "The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the \"Conversational Multi-Doc QA\" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "1st solution for WSDM Cup 2024"
    },
    {
        "paper id": "2402.18402",
        "abstract url": "https://arxiv.org/abs/2402.18402",
        "title": "A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation. In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies. All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\\ie, when upstream and downstream task/network are the same). In this paper, we propose SyMPIE to solve these shortcomings. To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost. Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples. Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels. We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\% relative accuracy gain across the board. The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at ACM MMSys'24. 10 pages, 7 figures, 8 tables"
    },
    {
        "paper id": "2402.18424",
        "abstract url": "https://arxiv.org/abs/2402.18424",
        "title": "Emotion Classification in Low and Moderate Resource Languages",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \\textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano, Odia, and Azerbaijani. Our results indicate that our approaches outperform random baselines and transfer emotions across languages successfully. For all languages, the direct cross-lingual transfer of emotion yields better results. We also create annotated emotion-labeled resources for four languages: Farsi, Azerbaijani, Ilocano and Odia.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18428",
        "abstract url": "https://arxiv.org/abs/2402.18428",
        "title": "Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for Neural Machine Translation (NMT). AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data's complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead of teachers and students. To hierarchically leverage the bilateral contextual information, token-level mutual learning and sequence-level contrastive learning are adopted between AR and NAR models. Extensive experiments on four widely used benchmarks show that the proposed DCMCL method can simultaneously improve both AR and NAR models with up to 1.38 and 2.98 BLEU scores respectively, and can also outperform the current best-unified model with up to 0.97 BLEU scores for both AR and NAR decoding.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 6 figures"
    },
    {
        "paper id": "2402.18439",
        "abstract url": "https://arxiv.org/abs/2402.18439",
        "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \\url{https://github.com/thunlp/AutoForm}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18447",
        "abstract url": "https://arxiv.org/abs/2402.18447",
        "title": "Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18458",
        "abstract url": "https://arxiv.org/abs/2402.18458",
        "title": "Meta-Task Prompting Elicits Embedding from Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18479",
        "abstract url": "https://arxiv.org/abs/2402.18479",
        "title": "NewsQs: Multi-Source Question Generation for the Inquiring Mind",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present NewsQs (news-cues), a dataset that provides question-answer pairs for multiple news documents. To create NewsQs, we augment a traditional multi-document summarization dataset with questions automatically generated by a T5-Large model fine-tuned on FAQ-style news articles from the News On the Web corpus. We show that fine-tuning a model with control codes produces questions that are judged acceptable more often than the same model without them as measured through human evaluation. We use a QNLI model with high correlation with human annotations to filter our data. We release our final dataset of high-quality questions, answers, and document clusters as a resource for future work in query-based multi-document summarization.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "in submission"
    },
    {
        "paper id": "2402.18502",
        "abstract url": "https://arxiv.org/abs/2402.18502",
        "title": "Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for fine-tuning a model. Fairness in LLMs helps ensure inclusivity, equal representation based on factors such as race, gender and promotes responsible AI deployment. As the use of LLMs has become increasingly prevalent, it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness. In this study, we introduce a framework outlining fairness regulations aligned with various fairness definitions, with each definition being modulated by varying degrees of abstraction. We explore the configuration for in-context learning and the procedure for selecting in-context demonstrations using RAG, while incorporating fairness rules into the process. Experiments conducted with different LLMs indicate that GPT-4 delivers superior results in terms of both accuracy and fairness compared to other models. This work is one of the early attempts to achieve fairness in prediction tasks by utilizing LLMs through in-context learning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2402.18503",
        "abstract url": "https://arxiv.org/abs/2402.18503",
        "title": "Detection of Micromobility Vehicles in Urban Traffic Videos",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Urban traffic environments present unique challenges for object detection, particularly with the increasing presence of micromobility vehicles like e-scooters and bikes. To address this object detection problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame object detection with the richer features offered by video object detection frameworks. This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture. This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability. Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin objects. Our approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18553",
        "abstract url": "https://arxiv.org/abs/2402.18553",
        "title": "Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Radiometric accuracy of data is crucial in quantitative precision agriculture, to produce reliable and repeatable data for modeling and decision making. The effect of exposure time and gain settings on the radiometric accuracy of multispectral images was not explored enough. The goal of this study was to determine if having a fixed exposure (FE) time during image acquisition improved radiometric accuracy of images, compared to the default auto-exposure (AE) settings. This involved quantifying the errors from auto-exposure and determining ideal exposure values within which radiometric mean absolute percentage error (MAPE) were minimal (< 5%). The results showed that FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than AE orthomosaic. An ideal exposure range was determined for capturing canopy and soil objects, without loss of information from under-exposure or saturation from over-exposure. A simulation of errors from AE showed that MAPE < 5% for the blue, green, red, and NIR bands and < 7% for the red edge band for exposure settings within the determined ideal ranges and increased exponentially beyond the ideal exposure upper limit. Further, prediction of total plant nitrogen uptake (g/plant) using vegetation indices (VIs) from two different growing seasons were closer to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to 14%, p < 0.05) when FE was used, compared to the prediction from AE images (mostly, R2 < 0.13, MAPE = 15 to 18%, p >= 0.05).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18614",
        "abstract url": "https://arxiv.org/abs/2402.18614",
        "title": "Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "The recently discovered Neural collapse (NC) phenomenon states that the last-layer weights of Deep Neural Networks (DNN), converge to the so-called Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training. This ETF geometry is equivalent to vanishing within-class variability of the last layer activations. Inspired by NC properties, we explore in this paper the transferability of DNN models trained with their last layer weight fixed according to ETF. This enforces class separation by eliminating class covariance information, effectively providing implicit regularization. We show that DNN models trained with such a fixed classifier significantly improve transfer performance, particularly on out-of-domain datasets. On a broad range of fine-grained image classification datasets, our approach outperforms i) baseline methods that do not perform any covariance regularization (up to 22%), as well as ii) methods that explicitly whiten covariance of activations throughout training (up to 19%). Our findings suggest that DNNs trained with fixed ETF classifiers offer a powerful mechanism for improving transfer learning across domains.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICASSP 2024. Copyright 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other"
    },
    {
        "paper id": "2402.18659",
        "abstract url": "https://arxiv.org/abs/2402.18659",
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages, 4 figures"
    },
    {
        "paper id": "2402.18667",
        "abstract url": "https://arxiv.org/abs/2402.18667",
        "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's role in guiding the selection of domain-specific AI agents. FoFo is released here at https://github.com/SalesforceAIResearch/FoFo.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "The first two authors contributed equally"
    },
    {
        "paper id": "2402.18668",
        "abstract url": "https://arxiv.org/abs/2402.18668",
        "title": "Simple linear attention language models balance the recall-throughput tradeoff",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18673",
        "abstract url": "https://arxiv.org/abs/2402.18673",
        "title": "Trends, Applications, and Challenges in Human Attention Modelling",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human attention modelling has proven, in recent years, to be particularly useful not only for understanding the cognitive processes underlying visual exploration, but also for providing support to artificial intelligence models that aim to solve problems in various domains, including image and video processing, vision-and-language applications, and language modelling. This survey offers a reasoned overview of recent efforts to integrate human attention mechanisms into contemporary deep learning models and discusses future research directions and challenges. For a comprehensive overview on the ongoing research refer to our dedicated repository available at https://github.com/aimagelab/awesome-human-visual-attention.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at IJCAI 2024 Survey Track"
    },
    {
        "paper id": "2402.18678",
        "abstract url": "https://arxiv.org/abs/2402.18678",
        "title": "RORA: Robust Free-Text Rationale Evaluation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \\citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that RORA aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18695",
        "abstract url": "https://arxiv.org/abs/2402.18695",
        "title": "Grounding Language Models for Visual Entity Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial double-digit margin.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18698",
        "abstract url": "https://arxiv.org/abs/2402.18698",
        "title": "Spatial Coherence Loss for Salient and Camouflaged Object Detection and Beyond",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generic object detection is a category-independent task that relies on accurate modeling of objectness. Most relevant CNN-based models of objectness utilize loss functions (e.g., binary cross entropy) that focus on the single-response, i.e., the loss response of a single pixel. Inspired by the human visual system, which first discerns the boundaries of ambiguous regions (i.e., hard regions) before delving into the semantic meaning, we propose a novel loss function, Spatial Coherence Loss (SCLoss), that uses the mutual response between adjacent pixels to suppress or emphasize the single-response of pixels. We demonstrate that the proposed SCLoss can gradually learn the hard regions by detecting and emphasizing their boundaries. Through comprehensive experiments, we demonstrate that replacing popular loss functions with SCLoss can improve the performance of current state-of-the-art (SOTA) salient or camouflaged object detection (SOD or COD) models. We also demonstrate that combining SCLoss with other loss functions can further improve performance and result in the SOTA outcomes for different applications. Finally, as a demonstrative example of the potential uses for other related tasks, we show an application of SCLoss for semantic segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18700",
        "abstract url": "https://arxiv.org/abs/2402.18700",
        "title": "Learning to Compress Prompt in Natural Language Formats",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framework compressing original prompts into NL formatted Capsule Prompt while maintaining the prompt utility and transferability. Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss. To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints. Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of the original length, decrease inference latency up to 4.5x, and save 80.1% of budget overheads while providing transferability across diverse LLMs and different datasets.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18756",
        "abstract url": "https://arxiv.org/abs/2402.18756",
        "title": "How Much Annotation is Needed to Compare Summarization Models?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates according to human preference.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2402.18761",
        "abstract url": "https://arxiv.org/abs/2402.18761",
        "title": "Exploration of Learned Lifting-Based Transform Structures for Fully Scalable and Accessible Wavelet-Like Image Compression",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "This paper provides a comprehensive study on features and performance of different ways to incorporate neural networks into lifting-based wavelet-like transforms, within the context of fully scalable and accessible image compression. Specifically, we explore different arrangements of lifting steps, as well as various network architectures for learned lifting operators. Moreover, we examine the impact of the number of learned lifting steps, the number of channels, the number of layers and the support of kernels in each learned lifting operator. To facilitate the study, we investigate two generic training methodologies that are simultaneously appropriate to a wide variety of lifting structures considered. Experimental results ultimately suggest that retaining fixed lifting steps from the base wavelet transform is highly beneficial. Moreover, we demonstrate that employing more learned lifting steps and more layers in each learned lifting operator do not contribute strongly to the compression performance. However, benefits can be obtained by utilizing more channels in each learned lifting operator. Ultimately, the learned wavelet-like transform proposed in this paper achieves over 25% bit-rate savings compared to JPEG 2000 with compact spatial support.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2402.18766",
        "abstract url": "https://arxiv.org/abs/2402.18766",
        "title": "Advancing Generative AI for Portuguese with Open Decoder Gerv\u00e1sio PT*",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gerv\u00e1sio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gerv\u00e1sio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18807",
        "abstract url": "https://arxiv.org/abs/2402.18807",
        "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\\&$exploitation trade-off ability, reasoning ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4. Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs. These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18815",
        "abstract url": "https://arxiv.org/abs/2402.18815",
        "title": "How do Large Language Models Handle Multilingualism?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs. By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose. Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18817",
        "abstract url": "https://arxiv.org/abs/2402.18817",
        "title": "Gradient Alignment for Cross-Domain Face Anti-Spoofing",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in domain generalization (DG) for face anti-spoofing (FAS) have garnered considerable attention. Traditional methods have focused on designing learning objectives and additional modules to isolate domain-specific features while retaining domain-invariant characteristics in their representations. However, such approaches often lack guarantees of consistent maintenance of domain-invariant features or the complete removal of domain-specific features. Furthermore, most prior works of DG for FAS do not ensure convergence to a local flat minimum, which has been shown to be advantageous for DG. In this paper, we introduce GAC-FAS, a novel learning objective that encourages the model to converge towards an optimal flat minimum without necessitating additional learning modules. Unlike conventional sharpness-aware minimizers, GAC-FAS identifies ascending points for each domain and regulates the generalization gradient updates at these points to align coherently with empirical risk minimization (ERM) gradient updates. This unique approach specifically guides the model to be robust against domain shifts. We demonstrate the efficacy of GAC-FAS through rigorous testing on challenging cross-domain FAS datasets, where it establishes state-of-the-art performance. The code is available at https://github.com/leminhbinh0209/CVPR24-FAS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18825",
        "abstract url": "https://arxiv.org/abs/2402.18825",
        "title": "Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure. Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input text which contains fruitful label co-occurrence information. In this work, we introduce this local hierarchy with an adversarial framework. We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information. We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies. Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by LREC-COLING 2024"
    },
    {
        "paper id": "2402.18838",
        "abstract url": "https://arxiv.org/abs/2402.18838",
        "title": "When does word order matter and when doesn't it?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while for others, like RTE, the consistency is near random when the PMI gets lower, i.e., word order is really important.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2402.18849",
        "abstract url": "https://arxiv.org/abs/2402.18849",
        "title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.14414",
        "abstract url": "https://arxiv.org/abs/2404.14414",
        "title": "Removing Reflections from RAW Photos",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We describe a system to remove real-world reflections from images for consumer photography. Our system operates on linear (RAW) photos, with the (optional) addition of a contextual photo looking in the opposite direction, e.g., using the selfie camera on a mobile device, which helps disambiguate what should be considered the reflection. The system is trained using synthetic mixtures of real-world RAW images, which are combined using a reflection simulation that is photometrically and geometrically accurate. Our system consists of a base model that accepts the captured photo and optional contextual photo as input, and runs at 256p, followed by an up-sampling model that transforms output 256p images to full resolution. The system can produce images for review at 1K in 4.5 to 6.5 seconds on a MacBook or iPhone 14 Pro. We test on RAW photos that were captured in the field and embody typical consumer photographs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages plus 22 pages of supplemental material"
    },
    {
        "paper id": "2405.00029",
        "abstract url": "https://arxiv.org/abs/2405.00029",
        "title": "Automatic Creative Selection with Cross-Modal Matching",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Application developers advertise their Apps by creating product pages with App images, and bidding on search terms. It is then crucial for App images to be highly relevant with the search terms. Solutions to this problem require an image-text matching model to predict the quality of the match between the chosen image and the search terms. In this work, we present a novel approach to matching an App image to search terms based on fine-tuning a pre-trained LXMERT model. We show that compared to the CLIP model and a baseline using a Transformer model for search terms, and a ResNet model for images, we significantly improve the matching accuracy. We evaluate our approach using two sets of labels: advertiser associated (image, search term) pairs for a given application, and human ratings for the relevance between (image, search term) pairs. Our approach achieves 0.96 AUC score for advertiser associated ground truth, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 8% and 14%. For human labeled ground truth, our approach achieves 0.95 AUC score, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 16% and 17%.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18078",
        "abstract url": "https://arxiv.org/abs/2402.18078",
        "title": "Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion",
                "Synthesis",
                "text-to-image"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of a pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at https://github.com/YanzuoLu/CFLD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024 (Highlight)"
    },
    {
        "paper id": "2402.18096",
        "abstract url": "https://arxiv.org/abs/2402.18096",
        "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18129",
        "abstract url": "https://arxiv.org/abs/2402.18129",
        "title": "On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method improving robustness against the marginal distribution of the sensitive attribute. Finally, we present several numerical results on the application of DP-based learning methods to standard centralized and distributed learning problems. The empirical findings support our theoretical results on the inductive biases in DP-based fair learning algorithms and the debiasing effects of the proposed SA-DRO method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18149",
        "abstract url": "https://arxiv.org/abs/2402.18149",
        "title": "Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\\tilde{O}\\left(\\frac{e^{|\u03b3|H}-1}{|\u03b3|H}H^2\\sqrt{KHS^2OA}\\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interest to the theoretical study of reinforcement learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "38 pages"
    },
    {
        "paper id": "2402.18152",
        "abstract url": "https://arxiv.org/abs/2402.18152",
        "title": "Boosting Neural Representations for Videos with a Conditional Decoder",
        "rating": 0.5,
        "keywords": [
            [
                "inpainting"
            ],
            [
                "eess.IV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts multiple baseline INRs in the reconstruction quality and convergence speed for video regression, and exhibits superior inpainting and interpolation results. Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs. Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs. Code is available at https://github.com/Xinjie-Q/Boosting-NeRV.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Accept by CVPR 2024"
    },
    {
        "paper id": "2402.18157",
        "abstract url": "https://arxiv.org/abs/2402.18157",
        "title": "From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing complicated real-life user queries. At each step, we guide LLMs to summarize the achieved results and determine the next course of action. We term this pipeline `from Summary to action', Sum2Act for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench benchmark show significant performance improvements, outperforming established methods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18159",
        "abstract url": "https://arxiv.org/abs/2402.18159",
        "title": "Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: \\texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \\texttt{RS-DisRL-V}, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Markov Decision Process (MDP), we derive the first $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ dependency of the regret upper bound for RSRL with static LRM, marking a pioneering contribution towards statistically efficient algorithms in this domain.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18164",
        "abstract url": "https://arxiv.org/abs/2402.18164",
        "title": "Autoencoder-based General Purpose Representation Learning for Customer Embedding",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, exploiting the domain-specific underlying structure of data and its generative factors for representation learning has shown success in various use-case agnostic applications. However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors. We design an autoencoder-based framework for building general purpose embeddings, we assess the performance of different autoencoder architectures, and show simpler models outperform complex ones in embedding highly complex tabular data. We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models. Moreover, we propose a significant improvement to the calculation of reconstruction loss for multi-layer contractive autoencoders (CAE) by calculating the Jacobian of the entire encoder leading to a 15% improvement in reconstruction quality when compared to a stacked CAE.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 2 figures"
    },
    {
        "paper id": "2402.18180",
        "abstract url": "https://arxiv.org/abs/2402.18180",
        "title": "Human Simulacra: A Step toward the Personification of Large Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18198",
        "abstract url": "https://arxiv.org/abs/2402.18198",
        "title": "Automated Machine Learning for Multi-Label Classification",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Automated machine learning (AutoML) aims to select and configure machine learning algorithms and combine them into machine learning pipelines tailored to a dataset at hand. For supervised learning tasks, most notably binary and multinomial classification, aka single-label classification (SLC), such AutoML approaches have shown promising results. However, the task of multi-label classification (MLC), where data points are associated with a set of class labels instead of a single class label, has received much less attention so far. In the context of multi-label classification, the data-specific selection and configuration of multi-label classifiers are challenging even for experts in the field, as it is a high-dimensional optimization problem with multi-level hierarchical dependencies. While for SLC, the space of machine learning pipelines is already huge, the size of the MLC search space outnumbers the one of SLC by several orders. In the first part of this thesis, we devise a novel AutoML approach for single-label classification tasks optimizing pipelines of machine learning algorithms, consisting of two algorithms at most. This approach is then extended first to optimize pipelines of unlimited length and eventually configure the complex hierarchical structures of multi-label classification methods. Furthermore, we investigate how well AutoML approaches that form the state of the art for single-label classification tasks scale with the increased problem complexity of AutoML for multi-label classification. In the second part, we explore how methods for SLC and MLC could be configured more flexibly to achieve better generalization performance and how to increase the efficiency of execution-based AutoML systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18206",
        "abstract url": "https://arxiv.org/abs/2402.18206",
        "title": "Balancing Act: Distribution-Guided Debiasing in Diffusion Models",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024. Project Page : https://ab-34.github.io/balancing_act/"
    },
    {
        "paper id": "2402.18208",
        "abstract url": "https://arxiv.org/abs/2402.18208",
        "title": "Shorts on the Rise: Assessing the Effects of YouTube Shorts on Long-Form Video Content",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Short form content has permeated into the video creator space over the past few years, led by industry leading products such as TikTok, YouTube Shorts and Instagram Reels. YouTube in particular was previously synonymous with being the main hub for long form video content consumption. The monetization of long form videos was easier as it allowed multiple advertisement placements during the course of the video. This model also facilitated thematic brand partnerships. However, since the introduction of short form content, creators have found it more difficult to generate revenue as advertisement placements have decreased. This leads to a unique situation where people are spending more time watching shorter videos, and yet they generate less revenue for the creators. In this paper, we perform a study of 250 creators with significant audiences to see if the introduction of short form content has affected the view counts and engagement of long form content. Our findings reveal a noteworthy trend: since the advent of short-form content, there has been a significant decrease in both view counts and engagement in long-form videos on these channels.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18235",
        "abstract url": "https://arxiv.org/abs/2402.18235",
        "title": "On the Joint Effect of Culture and Discussion Topics on X (Twitter) Signed Ego Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM). These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups. Until recently, the main focus of research was the structural characteristics of this model. The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion. A total of 26 datasets were collected for this work. It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion. However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures. These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses. Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "Funding: H2020 SoBigData++ (Grant Agreement n.871042), PNRR SoBigData.it (Prot. IR0000013), PNRR ICSC (CN00000013), PNRR FAIR (PE00000013)"
    },
    {
        "paper id": "2402.18260",
        "abstract url": "https://arxiv.org/abs/2402.18260",
        "title": "Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Active learning of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space. Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose. In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed. This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles. We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP. Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed. The effectiveness of our safe active learning approach is demonstrated through extensive simulations and validated using a real-world engine example.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "AISTATS 2024"
    },
    {
        "paper id": "2402.18296",
        "abstract url": "https://arxiv.org/abs/2402.18296",
        "title": "Comparative Analysis of XGBoost and Minirocket Algortihms for Human Activity Recognition",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Human Activity Recognition (HAR) has been extensively studied, with recent emphasis on the implementation of advanced Machine Learning (ML) and Deep Learning (DL) algorithms for accurate classification. This study investigates the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and MiniRocket, in the realm of HAR using data collected from smartphone sensors. The experiments are conducted on a dataset obtained from the UCI repository, comprising accelerometer and gyroscope signals captured from 30 volunteers performing various activities while wearing a smartphone. The dataset undergoes preprocessing, including noise filtering and feature extraction, before being utilized for training and testing the classifiers. Monte Carlo cross-validation is employed to evaluate the models' robustness. The findings reveal that both XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as 0.99 in activity classification. XGBoost exhibits a slightly superior performance compared to MiniRocket. Notably, both algorithms surpass the performance of other ML and DL algorithms reported in the literature for HAR tasks. Additionally, the study compares the computational efficiency of the two algorithms, revealing XGBoost's advantage in terms of training time. Furthermore, the performance of MiniRocket, which achieves accuracy and F1 values of 0.94, and an AUC value of 0.96 using raw data and utilizing only one channel from the sensors, highlights the potential of directly leveraging unprocessed signals. It also suggests potential advantages that could be gained by utilizing sensor fusion or channel fusion techniques. Overall, this research sheds light on the effectiveness and computational characteristics of XGBoost and MiniRocket in HAR tasks, providing insights for future studies in activity recognition using smartphone sensor data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 1 figure, 1st Bilsel International Sur Scientific Researches Congress, 10-11 February, 2024, Diyarbakir, T\u00fcrk\u0130ye"
    },
    {
        "paper id": "2402.18311",
        "abstract url": "https://arxiv.org/abs/2402.18311",
        "title": "Escaping Local Optima in Global Placement",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Placement is crucial in the physical design, as it greatly affects power, performance, and area metrics. Recent advancements in analytical methods, such as DREAMPlace, have demonstrated impressive performance in global placement. However, DREAMPlace has some limitations, e.g., may not guarantee legalizable placements under the same settings, leading to fragile and unpredictable results. This paper highlights the main issue as being stuck in local optima, and proposes a hybrid optimization framework to efficiently escape the local optima, by perturbing the placement result iteratively. The proposed framework achieves significant improvements compared to state-of-the-art methods on two popular benchmarks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Work-in-Progress (WIP) poster of DAC 2024"
    },
    {
        "paper id": "2402.18321",
        "abstract url": "https://arxiv.org/abs/2402.18321",
        "title": "Privacy Policies and Consent Management Platforms: Growth and Users' Interactions over Time",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "In response to growing concerns about user privacy, legislators have introduced new regulations and laws such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force websites to obtain user consent before activating personal data collection, fundamental to providing targeted advertising. The cornerstone of this consent-seeking process involves the use of Privacy Banners, the technical mechanism to collect users' approval for data collection practices. Consent management platforms (CMPs) have emerged as practical solutions to make it easier for website administrators to properly manage consent, allowing them to outsource the complexities of managing user consent and activating advertising features. This paper presents a detailed and longitudinal analysis of the evolution of CMPs spanning nine years. We take a twofold perspective: Firstly, thanks to the HTTP Archive dataset, we provide insights into the growth, market share, and geographical spread of CMPs. Noteworthy observations include the substantial impact of GDPR on the proliferation of CMPs in Europe. Secondly, we analyse millions of user interactions with a medium-sized CMP present in thousands of websites worldwide. We observe how even small changes in the design of Privacy Banners have a critical impact on the user's giving or denying their consent to data collection. For instance, over 60% of users do not consent when offered a simple \"one-click reject-all\" option. Conversely, when opting out requires more than one click, about 90% of users prefer to simply give their consent. The main objective is in fact to eliminate the annoying privacy banner rather the make an informed decision. Curiously, we observe iOS users exhibit a higher tendency to accept cookies compared to Android users, possibly indicating greater confidence in the privacy offered by Apple devices.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18326",
        "abstract url": "https://arxiv.org/abs/2402.18326",
        "title": "When Should Algorithms Resign?",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "This paper discusses algorithmic resignation, a strategic approach for managing the use of AI systems within organizations. Algorithmic resignation involves the deliberate and informed disengagement from AI assistance in certain scenarios, by embedding governance mechanisms directly into AI systems. Our proposal is not merely about disuse of AI but includes guiding when and how these systems should be used or avoided. We discuss the multifaceted benefits of algorithmic resignation, spanning economic efficiency, reputational gains, and legal compliance. Further, we outline the operationalization of resignation through various methods such as positive and negative nudges, stakeholder incentive alignment, and careful consideration of the level of AI engagement. Using techniques like barring access to AI outputs selectively or providing explicit disclaimers on system performance, algorithmic resignation not only mitigates risks associated with AI but also leverages its benefits, ensuring the responsible and effective use of AI systems.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18330",
        "abstract url": "https://arxiv.org/abs/2402.18330",
        "title": "Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting",
        "rating": 0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation. Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem. To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process. We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network. The Grid ViT Encoder summarizes joint heatmaps into effective feature embedding using self-attention. Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints. Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9\\% reduction of error in an MPJPE metric. Our source code is available in GitHub.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 9 figures, to be published as CVPR 2024 paper"
    },
    {
        "paper id": "2402.18335",
        "abstract url": "https://arxiv.org/abs/2402.18335",
        "title": "Detecting Anti-vaccine Content on Twitter using Multiple Message-Based Network Representations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Social media platforms such as Twitter have a fundamental role in facilitating the spread and discussion of ideas online through the concept of retweeting and replying. However, these features also contribute to the spread of mis/disinformation during the vaccine rollout of the COVID-19 pandemic. Using COVID-19 vaccines as a case study, we analyse multiple social network representation derived from three message-based interactions on Twitter (quote retweets, mentions and replies) based upon a set of known anti-vax hashtags and keywords. Each network represents a certain hashtag or keyword which were labelled as \"controversial\" and \"non-controversial\" according to a small group of participants. For each network, we extract a combination of global and local network-based metrics which are used as feature vectors for binary classification. Our results suggest that it is possible to detect controversial from non-controversial terms with high accuracy using simple network-based metrics. Furthermore, these results demonstrate the potential of network representations as language-agnostic models for detecting mis/disinformation at scale, irrespective of content and across multiple social media platforms.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18377",
        "abstract url": "https://arxiv.org/abs/2402.18377",
        "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18381",
        "abstract url": "https://arxiv.org/abs/2402.18381",
        "title": "Large Language Models As Evolution Strategies",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLLM', that robustly outperforms baseline algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB functions as well as small neuroevolution tasks. Hence, LLMs can act as `plug-in' in-context recombination operators. We provide several comparative studies of the LLM's model size, prompt strategy, and context construction. Finally, we show that one can flexibly improve EvoLLM's performance by providing teacher algorithm information via instruction fine-tuning on previously collected teacher optimization trajectories.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "11 pages, 14 figures"
    },
    {
        "paper id": "2402.18392",
        "abstract url": "https://arxiv.org/abs/2402.18392",
        "title": "Unveiling the Potential of Robustness in Evaluating Causal Inference Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for CATE estimator selection. The proposed DRM not only eliminates the need to fit additional models but also excels at selecting a robust CATE estimator. Experimental studies demonstrate the efficacy of the DRM method, showcasing its consistent effectiveness in identifying superior estimators while mitigating the risk of selecting inferior ones.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18426",
        "abstract url": "https://arxiv.org/abs/2402.18426",
        "title": "A Relational Inductive Bias for Dimensional Abstraction in Neural Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment. In contrast, standard neural network architectures often struggle with abstract reasoning tasks, overfitting, and requiring extensive data for training. This paper investigates the impact of the relational bottleneck -- a mechanism that focuses processing on relations among inputs -- on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing. We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases. Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to underlie human cognitive flexibility. Moreover, the relational network mimics human biases towards regularity without pre-specified symbolic primitives, suggesting that the bottleneck fosters the emergence of abstract representations that confer flexibility akin to symbols.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18463",
        "abstract url": "https://arxiv.org/abs/2402.18463",
        "title": "Understanding the Impact of AI Generated Content on Social Media: The Pixiv Case",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "In the last two years, Artificial Intelligence Generated Content (AIGC) has received significant attention, leading to an anecdotal rise in the amount of AIGC being shared via social media platforms. The impact of AIGC and its implications are of key importance to social platforms, e.g., regarding the implementation of policies, community formation, and algorithmic design. Yet, to date, we know little about how the arrival of AIGC has impacted the social media ecosystem. To fill this gap, we present a comprehensive study of Pixiv, an online community for artists who wish to share and receive feedback on their illustrations. Pixiv hosts over 100 million artistic submissions and receives more than 1 billion page views per month (as of 2023). Importantly, it allows both human and AI generated content to be uploaded. Exploiting this, we perform the first analysis of the impact that AIGC has had on the social media ecosystem, through the lens of Pixiv. Based on a dataset of 15.2 million posts (including 2.4 million AI-generated images), we measure the impact of AIGC on the Pixiv community, as well as the differences between AIGC and human-generated content in terms of content creation and consumption patterns. Our results offer key insight to how AIGC is changing the dynamics of social media platforms like Pixiv.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18470",
        "abstract url": "https://arxiv.org/abs/2402.18470",
        "title": "A Higher-Order Lens for Social Systems",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Despite the widespread adoption of higher-order mathematical structures such as hypergraphs, methodological tools for their analysis lag behind those for traditional graphs. This work addresses a critical gap in this context by proposing two micro-canonical random null models for directed hypergraphs: the Directed Hypergraph Configuration Model (DHCM) and the Directed Hypergraph JOINT Model (DHJM). These models preserve essential structural properties of directed hypergraphs such as node in- and out-degree sequences and hyperedge head and tail size sequences, or their joint tensor. We also describe two efficient MCMC algorithms, NuDHy-Degs and NuDHy-JOINT, to sample random hypergraphs from these ensembles. To showcase the interdisciplinary applicability of the proposed null models, we present three distinct use cases in sociology, epidemiology, and economics. First, we reveal the oscillatory behavior of increased homophily in opposition parties in the US Congress over a 40-year span, emphasizing the role of higher-order structures in quantifying political group homophily. Second, we investigate non-linear contagion in contact hyper-networks, demonstrating that disparities between simulations and theoretical predictions can be explained by considering higher-order joint degree distributions. Last, we examine the economic complexity of countries in the global trade network, showing that local network properties preserved by NuDHy explain the main structural economic complexity indexes. This work pioneers the development of null models for directed hypergraphs, addressing the intricate challenges posed by their complex entity relations, and providing a versatile suite of tools for researchers across various domains.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18490",
        "abstract url": "https://arxiv.org/abs/2402.18490",
        "title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding",
        "rating": 0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes. However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods. This is attributed to the domain shift in the 2D images and the distinct focus of each modality. To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergistic adapters. First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs. Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training. Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks. Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8\\% to 50.7\\%, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1\\% to 99.0\\%. Project page: https://alanzhangcs.github.io/tamm-page.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This paper is accepted by CVPR 2024"
    },
    {
        "paper id": "2402.18496",
        "abstract url": "https://arxiv.org/abs/2402.18496",
        "title": "Language Models Represent Beliefs of Self and Others",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "project page: https://walter0807.github.io/RepBelief/"
    },
    {
        "paper id": "2402.18508",
        "abstract url": "https://arxiv.org/abs/2402.18508",
        "title": "Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical. This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism. Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning. At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation. The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while maintaining efficiency and quasilinear scalability for long sequences. We rigorously evaluate Orchid across multiple domains, including language modeling and image classification, to showcase its performance and generality. Our experiments demonstrate that Orchid architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers. This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18512",
        "abstract url": "https://arxiv.org/abs/2402.18512",
        "title": "Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurrent unit.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2402.18540",
        "abstract url": "https://arxiv.org/abs/2402.18540",
        "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the \"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost eliminates them in some cases.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "20 pages"
    },
    {
        "paper id": "2402.18546",
        "abstract url": "https://arxiv.org/abs/2402.18546",
        "title": "Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A major goal in neuroscience is to discover neural data representations that generalize. This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others. Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments. In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024). EEGNet is a widely used convolutional neural network, while TOTEM is a discrete time series tokenizer and transformer model. We find that TOTEM outperforms or matches EEGNet across all generalizability cases. Finally through analysis of TOTEM's latent codebook we observe that tokenization enables generalization.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages, 8 figures"
    },
    {
        "paper id": "2402.18551",
        "abstract url": "https://arxiv.org/abs/2402.18551",
        "title": "Implicit Bias of Next-Token Prediction",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that the parameters of GD projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits' difference of in-support tokens to be equal to the log-ratio of their respective probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of parameters satisfying the \\NTP-separability conditions. Akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with NTP.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18571",
        "abstract url": "https://arxiv.org/abs/2402.18571",
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).",
        "subjects": [
            "cs.LG"
        ],
        "comment": "The code and model are released at https://github.com/Haoxiang-Wang/directional-preference-alignment"
    },
    {
        "paper id": "2402.18605",
        "abstract url": "https://arxiv.org/abs/2402.18605",
        "title": "FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbone network, strengthens the representation reuse of the gradient-based meta-learning methods. Our experimental results across various few-shot learning datasets, demonstrate the superiority of our proposed method compared to the state-of-the-art methods, especially MAML, its Euclidean counterpart.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18621",
        "abstract url": "https://arxiv.org/abs/2402.18621",
        "title": "Unveiling News Publishers Trustworthiness Through Social Interactions",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "With the primary goal of raising readers' awareness of misinformation phenomena, extensive efforts have been made by both academic institutions and independent organizations to develop methodologies for assessing the trustworthiness of online news publishers. Unfortunately, existing approaches are costly and face critical scalability challenges. This study presents a novel framework for assessing the trustworthiness of online news publishers using user interactions on social media platforms. The proposed methodology provides a versatile solution that serves the dual purpose of i) identifying verifiable online publishers and ii) automatically performing an initial estimation of the trustworthiness of previously unclassified online news outlets.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "A pre-final version of the paper accepted at WebSci'24"
    },
    {
        "paper id": "2402.18664",
        "abstract url": "https://arxiv.org/abs/2402.18664",
        "title": "Online disinformation in the 2020 U.S. Election: swing vs. safe states",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "For U.S. presidential elections, most states use the so-called winner-take-all system, in which the state's presidential electors are awarded to the winning political party in the state after a popular vote phase, regardless of the actual margin of victory. Therefore, election campaigns are especially intense in states where there is no clear direction on which party will be the winning party. These states are often referred to as swing states. To measure the impact of such an election law on the campaigns, we analyze the Twitter activity surrounding the 2020 US preelection debate, with a particular focus on the spread of disinformation. We find that about 88% of the online traffic was associated with swing states. In addition, the sharing of links to unreliable news sources is significantly more prevalent in tweets associated with swing states: in this case, untrustworthy tweets are predominantly generated by automated accounts. Furthermore, we observe that the debate is mostly led by two main communities, one with a predominantly Republican affiliation and the other with accounts of different political orientations. Most of the disinformation comes from the former.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2303.12474"
    },
    {
        "paper id": "2402.18715",
        "abstract url": "https://arxiv.org/abs/2402.18715",
        "title": "Commonsense Ontology Micropatterns",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The previously introduced Modular Ontology Modeling methodology (MOMo) attempts to mimic the human analogical process by using modular patterns to assemble more complex concepts. To support this, MOMo organizes organizes ontology design patterns into design libraries, which are programmatically queryable, to support accelerated ontology development, for both human and automated processes. However, a major bottleneck to large-scale deployment of MOMo is the (to-date) limited availability of ready-to-use ontology design patterns. At the same time, Large Language Models have quickly become a source of common knowledge and, in some cases, replacing search engines for questions. In this paper, we thus present a collection of 104 ontology design patterns representing often occurring nouns, curated from the common-sense knowledge available in LLMs, organized into a fully-annotated modular ontology design library ready for use with MOMo.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18724",
        "abstract url": "https://arxiv.org/abs/2402.18724",
        "title": "Learning Associative Memories with Gradient Descent",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.'' Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small Transformer models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18726",
        "abstract url": "https://arxiv.org/abs/2402.18726",
        "title": "Unveiling Privacy, Memorization, and Input Curvature Links",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. First, we derive an upper bound on memorization characterized by both differential privacy and input loss curvature. Second, we present a novel insight showing that input loss curvature is upper-bounded by the differential privacy parameter. Our theoretical findings are further empirically validated using deep models on CIFAR and ImageNet datasets, showing a strong correlation between our theoretical predictions and results observed in practice.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18732",
        "abstract url": "https://arxiv.org/abs/2402.18732",
        "title": "GAIA: Categorical Foundations of Generative AI",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this paper, we propose GAIA, a generative AI architecture based on category theory. GAIA is based on a hierarchical model where modules are organized as a simplicial complex. Each simplicial complex updates its internal parameters biased on information it receives from its superior simplices and in turn relays updates to its subordinate sub-simplices. Parameter updates are formulated in terms of lifting diagrams over simplicial sets, where inner and outer horn extensions correspond to different types of learning problems. Backpropagation is modeled as an endofunctor over the category of parameters, leading to a coalgebraic formulation of deep learning.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "65 pages. arXiv admin note: text overlap with arXiv:2212.08981"
    },
    {
        "paper id": "2402.18734",
        "abstract url": "https://arxiv.org/abs/2402.18734",
        "title": "Priority Sampling of Large Language Models for Compilers",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the autotuner used for the generation of labels for the training of the original model in just 30 samples.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18752",
        "abstract url": "https://arxiv.org/abs/2402.18752",
        "title": "Pre-training Differentially Private Models with Limited Public Data",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process. In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, using only 10\\% of public data, our strategy can achieve DP accuracy of 41.5\\% on ImageNet-21k (with $\u03b5=8$), as well as non-DP accuracy of 55.7\\% and and 60.0\\% on downstream tasks Places365 and iNaturalist-2021, respectively, on par with state-of-the-art standard pre-training and substantially outperforming existing DP pre-trained models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18762",
        "abstract url": "https://arxiv.org/abs/2402.18762",
        "title": "Disentangling the Causes of Plasticity Loss in Neural Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \\textit{stationary} data distribution. In settings where this assumption is violated, e.g.\\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18771",
        "abstract url": "https://arxiv.org/abs/2402.18771",
        "title": "NARUTO: Neural Active Reconstruction from Uncertain Target Observations",
        "rating": 0.5,
        "keywords": [
            [
                "SLAM"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction. Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment. By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning. Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity. We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy. Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on benchmark datasets like Replica and MP3D.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR2024. Project page: https://oppo-us-research.github.io/NARUTO-website/. Code: https://github.com/oppo-us-research/NARUTO"
    },
    {
        "paper id": "2402.18784",
        "abstract url": "https://arxiv.org/abs/2402.18784",
        "title": "Brain-inspired and Self-based Artificial Intelligence",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The question \"Can machines think?\" and the Turing Test to assess whether machines could achieve human-level intelligence is one of the roots of AI. With the philosophical argument \"I think, therefore I am\", this paper challenge the idea of a \"thinking machine\" supported by current AIs since there is no sense of self in them. Current artificial intelligence is only seemingly intelligent information processing and does not truly understand or be subjectively aware of oneself and perceive the world with the self as human intelligence does. In this paper, we introduce a Brain-inspired and Self-based Artificial Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to coordinating various cognitive functions and learning strategies in a self-organized manner to build human-level AI models and robotic applications. Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the future AI, rooted with a practical hierarchical Self framework, including Perception and Learning, Bodily Self, Autonomous Self, Social Self, and Conceptual Self. The hierarchical framework of the Self highlights self-based environment perception, self-bodily modeling, autonomous interaction with the environment, social interaction and collaboration with others, and even more abstract understanding of the Self. Furthermore, the positive mutual promotion and support among multiple levels of Self, as well as between Self and learning, enhance the BriSe AI's conscious understanding of information and flexible adaptation to complex environments, serving as a driving force propelling BriSe AI towards real Artificial General Intelligence.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18803",
        "abstract url": "https://arxiv.org/abs/2402.18803",
        "title": "To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In fair machine learning, one source of performance disparities between groups is over-fitting to groups with relatively few training samples. We derive group-specific bounds on the generalization error of welfare-centric fair machine learning that benefit from the larger sample size of the majority group. We do this by considering group-specific Rademacher averages over a restricted hypothesis class, which contains the family of models likely to perform well with respect to a fair learning objective (e.g., a power-mean). Our simulations demonstrate these bounds improve over a naive method, as expected by theory, with particularly significant improvement for smaller group sizes.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18805",
        "abstract url": "https://arxiv.org/abs/2402.18805",
        "title": "VEC-SBM: Optimal Community Detection with Vectorial Edges Covariates",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Social networks are often associated with rich side information, such as texts and images. While numerous methods have been developed to identify communities from pairwise interactions, they usually ignore such side information. In this work, we study an extension of the Stochastic Block Model (SBM), a widely used statistical framework for community detection, that integrates vectorial edges covariates: the Vectorial Edges Covariates Stochastic Block Model (VEC-SBM). We propose a novel algorithm based on iterative refinement techniques and show that it optimally recovers the latent communities under the VEC-SBM. Furthermore, we rigorously assess the added value of leveraging edge's side information in the community detection process. We complement our theoretical results with numerical experiments on synthetic and semi-synthetic data.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18819",
        "abstract url": "https://arxiv.org/abs/2402.18819",
        "title": "Dual Operating Modes of In-Context Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution. With the closed-form expression, we obtain a quantitative understanding of the two operating modes of ICL. Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the ICL risk initially increases and then decreases with more in-context examples. Our model offers a plausible explanation for this \"early ascent\" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels. Lastly, we validate our findings and predictions via experiments involving Transformers and large language models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "53 pages, 20 figures"
    },
    {
        "paper id": "2402.18824",
        "abstract url": "https://arxiv.org/abs/2402.18824",
        "title": "Batch size invariant Adam",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a batch size invariant version of Adam, for use in large-scale, distributed settings, in which the mini-batch is divided into micro-batches which are distributed among worker nodes. For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average. Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient. In contrast, the approach proposed here gives batch size invariance without this assumption. We confirm that in practice our scheme gives batch size invariance in a much larger range of scenarios than the previous approach.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18836",
        "abstract url": "https://arxiv.org/abs/2402.18836",
        "title": "A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep reinforcement learning setting to improve sample efficiency. First, we formulate an augmented policy loss combining a maximum entropy reinforcement learning objective with a behavioral cloning loss that leverages a forward dynamics model. Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function. Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various benchmarks by effectively utilizing available expert observations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18842",
        "abstract url": "https://arxiv.org/abs/2402.18842",
        "title": "ViewFusion: Towards Multi-View Consistency via Interpolated Denoising",
        "rating": 0.5,
        "keywords": [
            [
                "diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024,homepage:https://wi-sc.github.io/ViewFusion.github.io/"
    },
    {
        "paper id": "2402.18846",
        "abstract url": "https://arxiv.org/abs/2402.18846",
        "title": "Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the highest fidelity level by combining data from multiple sources. Traditional methods relying on Gaussian processes can hardly scale to high-dimensional data. Deep learning approaches utilize neural network based encoders and decoders to improve scalability. These approaches share encoded representations across fidelities without including corresponding decoder parameters. At the highest fidelity, the representations are decoded with different parameters, making the shared information inherently inaccurate. This hinders inference performance, especially in out-of-distribution scenarios when the highest fidelity data has limited domain coverage. To address these limitations, we propose Multi-fidelity Residual Neural Processes (MFRNP), a novel multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity decoders for accurate information sharing by aggregating lower fidelity surrogate outputs and models residual between the aggregation and ground truth on the highest fidelity. We show that MFRNP significantly outperforms current state-of-the-art in learning partial differential equations and a real-world climate modeling task.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "A novel probabilistic inference approach for scalable multi-fidelity surrogate modeling"
    },
    {
        "paper id": "2403.00032",
        "abstract url": "https://arxiv.org/abs/2403.00032",
        "title": "Time to Cite: Modeling Citation Networks using the Dynamic Impact Single-Event Embedding Model",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Understanding the structure and dynamics of scientific research, i.e., the science of science (SciSci), has become an important area of research in order to address imminent questions including how scholars interact to advance science, how disciplines are related and evolve, and how research impact can be quantified and predicted. Central to the study of SciSci has been the analysis of citation networks. Here, two prominent modeling methodologies have been employed: one is to assess the citation impact dynamics of papers using parametric distributions, and the other is to embed the citation networks in a latent space optimal for characterizing the static relations between papers in terms of their citations. Interestingly, citation networks are a prominent example of single-event dynamic networks, i.e., networks for which each dyad only has a single event (i.e., the point in time of citation). We presently propose a novel likelihood function for the characterization of such single-event networks. Using this likelihood, we propose the Dynamic Impact Single-Event Embedding model (DISEE). The \\textsc{\\modelabbrev} model characterizes the scientific interactions in terms of a latent distance model in which random effects account for citation heterogeneity while the time-varying impact is characterized using existing parametric representations for assessment of dynamic impact. We highlight the proposed approach on several real citation networks finding that the DISEE well reconciles static latent distance network embedding approaches with classical dynamic impact assessments.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "Accepted for AISTATS 2024"
    },
    {
        "paper id": "2402.18066",
        "abstract url": "https://arxiv.org/abs/2402.18066",
        "title": "Six-Point Method for Multi-Camera Systems with Reduced Solution Space",
        "rating": 0,
        "keywords": [
            [
                "6DOF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Relative pose estimation using point correspondences (PC) is a widely used technique. A minimal configuration of six PCs is required for generalized cameras. In this paper, we present several minimal solvers that use six PCs to compute the 6DOF relative pose of a multi-camera system, including a minimal solver for the generalized camera and two minimal solvers for the practical configuration of two-camera rigs. The equation construction is based on the decoupling of rotation and translation. Rotation is represented by Cayley or quaternion parametrization, and translation can be eliminated by using the hidden variable technique. Ray bundle constraints are found and proven when a subset of PCs relate the same cameras across two views. This is the key to reducing the number of solutions and generating numerically stable solvers. Moreover, all configurations of six-point problems for multi-camera systems are enumerated. Extensive experiments demonstrate that our solvers are more accurate than the state-of-the-art six-point methods, while achieving better performance in efficiency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2102.11996"
    },
    {
        "paper id": "2402.18085",
        "abstract url": "https://arxiv.org/abs/2402.18085",
        "title": "AI-assisted Tagging of Deepfake Audio Calls using Challenge-Response",
        "rating": 0,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.SD"
            ]
        ],
        "abstract": "Scammers are aggressively leveraging AI voice-cloning technology for social engineering attacks, a situation significantly worsened by the advent of audio Real-time Deepfakes (RTDFs). RTDFs can clone a target's voice in real-time over phone calls, making these interactions highly interactive and thus far more convincing. Our research confidently addresses the gap in the existing literature on deepfake detection, which has largely been ineffective against RTDF threats. We introduce a robust challenge-response-based method to detect deepfake audio calls, pioneering a comprehensive taxonomy of audio challenges. Our evaluation pitches 20 prospective challenges against a leading voice-cloning system. We have compiled a novel open-source challenge dataset with contributions from 100 smartphone and desktop users, yielding 18,600 original and 1.6 million deepfake samples. Through rigorous machine and human evaluations of this dataset, we achieved a deepfake detection rate of 86% and an 80% AUC score, respectively. Notably, utilizing a set of 11 challenges significantly enhances detection capabilities. Our findings reveal that combining human intuition with machine precision offers complementary advantages. Consequently, we have developed an innovative human-AI collaborative system that melds human discernment with algorithmic accuracy, boosting final joint accuracy to 82.9%. This system highlights the significant advantage of AI-assisted pre-screening in call verification processes. Samples can be heard at https://mittalgovind.github.io/autch-samples/",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Dataset will be made public by end of March 2024"
    },
    {
        "paper id": "2402.18172",
        "abstract url": "https://arxiv.org/abs/2402.18172",
        "title": "NiteDR: Nighttime Image De-Raining with Cross-View Sensor Cooperative Learning for Dynamic Driving Scenes",
        "rating": 0,
        "keywords": [
            [
                "autonomous driving",
                "infrared"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In real-world environments, outdoor imaging systems are often affected by disturbances such as rain degradation. Especially, in nighttime driving scenes, insufficient and uneven lighting shrouds the scenes in darkness, resulting degradation of both the image quality and visibility. Particularly, in the field of autonomous driving, the visual perception ability of RGB sensors experiences a sharp decline in such harsh scenarios. Additionally, driving assistance systems suffer from reduced capabilities in capturing and discerning the surrounding environment, posing a threat to driving safety. Single-view information captured by single-modal sensors cannot comprehensively depict the entire scene. To address these challenges, we developed an image de-raining framework tailored for rainy nighttime driving scenes. It aims to remove rain artifacts, enrich scene representation, and restore useful information. Specifically, we introduce cooperative learning between visible and infrared images captured by different sensors. By cross-view fusion of these multi-source data, the scene within the images gains richer texture details and enhanced contrast. We constructed an information cleaning module called CleanNet as the first stage of our framework. Moreover, we designed an information fusion module called FusionNet as the second stage to fuse the clean visible images with infrared images. Using this stage-by-stage learning strategy, we obtain de-rained fusion images with higher quality and better visual perception. Extensive experiments demonstrate the effectiveness of our proposed Cross-View Cooperative Learning (CVCL) in adverse driving scenarios in low-light rainy environments. The proposed approach addresses the gap in the utilization of existing rain removal algorithms in specific low-light conditions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18175",
        "abstract url": "https://arxiv.org/abs/2402.18175",
        "title": "Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus",
        "rating": 0,
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera. To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel self-supervised learning method that leverages the pair of real sharp and blurred images, which can be easily captured by changing the aperture setting of the camera. In our PSF estimation, we assume rotationally symmetric PSFs and introduce the polar coordinate system to more accurately learn the PSF estimation network. We also handle the focus breathing phenomenon that occurs in real DfD situations. Experimental results on synthetic and real data demonstrate the effectiveness of our method regarding both the PSF estimation and the depth estimation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18179",
        "abstract url": "https://arxiv.org/abs/2402.18179",
        "title": "Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations",
        "rating": 0,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision. At the same time, advances around the detection of fake news were mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form graph-like structures that hold contextual information apart from the news article to classify. We propose to merge these two developments by applying pre-training of Graph Neural Networks (GNNs) in the domain of context-based fake news detection. Our experiments provide an evaluation of different pre-training strategies for graph-based misinformation detection and demonstrate that transfer learning does currently not lead to significant improvements over training a model from scratch in the domain. We argue that a major current issue is the lack of suitable large-scale resources that can be used for pre-training.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Preprint accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2402.18196",
        "abstract url": "https://arxiv.org/abs/2402.18196",
        "title": "NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human pose estimation (HPE) in the top-view using fisheye cameras presents a promising and innovative application domain. However, the availability of datasets capturing this viewpoint is extremely limited, especially those with high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage the capabilities of Neural Radiance Fields (NeRF) technique to establish a comprehensive pipeline for generating human pose datasets from existing 2D and 3D datasets, specifically tailored for the top-view fisheye perspective. Through this pipeline, we create a novel dataset NToP570K (NeRF-powered Top-view human Pose dataset for fisheye cameras with over 570 thousand images), and conduct an extensive evaluation of its efficacy in enhancing neural networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE after finetuning on our training set. A similarly finetuned HybrIK-Transformer model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18251",
        "abstract url": "https://arxiv.org/abs/2402.18251",
        "title": "On the Accuracy of Edge Detectors in Number Plate Extraction",
        "rating": 0,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Edge detection as a pre-processing stage is a fundamental and important aspect of the number plate extraction system. This is due to the fact that the identification of a particular vehicle is achievable using the number plate because each number plate is unique to a vehicle. As such, the characters of a number plate system that differ in lines and shapes can be extracted using the principle of edge detection. This paper presents a method of number plate extraction using edge detection technique. Edges in number plates are identified with changes in the intensity of pixel values. Therefore, these edges are identified using a single based pixel or collection of pixel-based approach. The efficiency of these approaches of edge detection algorithms in number plate extraction in both noisy and clean environment are experimented. Experimental results are achieved in MATLAB 2017b using the Pratt Figure of Merit (PFOM) as a performance metric",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18267",
        "abstract url": "https://arxiv.org/abs/2402.18267",
        "title": "A Survey on Neural Question Generation: Methods, Applications, and Prospects",
        "rating": 0,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths. Accompanying this survey is a curated collection of related research papers, datasets and codes, systematically organized on Github, providing an extensive reference for those delving into NQG.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by IJCAI 2024"
    },
    {
        "paper id": "2402.18278",
        "abstract url": "https://arxiv.org/abs/2402.18278",
        "title": "EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods",
        "rating": 0,
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "High-definition (HD) map is crucial for autonomous driving systems. Most existing works design map elements detection heads based on the DETR decoder. However, the initial queries lack explicit incorporation of physical positional information, and vanilla self-attention entails high computational complexity. Therefore, we propose EAN-MapNet for Efficiently constructing HD map using Anchor Neighborhoods. Firstly, we design query units based on the anchor neighborhoods, allowing non-neighborhood central anchors to effectively assist in fitting the neighborhood central anchors to the target points representing map elements. Then, we propose grouped local self-attention (GL-SA) by leveraging the relative instance relationship among the queries. This facilitates direct feature interaction among queries of the same instances, while innovatively employing local queries as intermediaries for interaction among queries from different instances. Consequently, GL-SA significantly reduces the computational complexity of self-attention while ensuring ample feature interaction among queries. On the nuScenes dataset, EAN-MapNet achieves a state-of-the-art performance with 63.0 mAP after training for 24 epochs, surpassing MapTR by 12.7 mAP. Furthermore, it considerably reduces memory consumption by 8198M compared to MapTRv2.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18293",
        "abstract url": "https://arxiv.org/abs/2402.18293",
        "title": "Continuous Memory Representation for Anomaly Detection",
        "rating": 0,
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "There have been significant advancements in anomaly detection in an unsupervised manner, where only normal images are available for training. Several recent methods aim to detect anomalies based on a memory, comparing or reconstructing the input with directly stored normal features (or trained features with normal images). However, such memory-based approaches operate on a discrete feature space implemented by the nearest neighbor or attention mechanism, suffering from poor generalization or an identity shortcut issue outputting the same as input, respectively. Furthermore, the majority of existing methods are designed to detect single-class anomalies, resulting in unsatisfactory performance when presented with multiple classes of objects. To tackle all of the above challenges, we propose CRAD, a novel anomaly detection method for representing normal features within a \"continuous\" memory, enabled by transforming spatial features into coordinates and mapping them to continuous grids. Furthermore, we carefully design the grids tailored for anomaly detection, representing both local and global normal features and fusing them effectively. Our extensive experiments demonstrate that CRAD successfully generalizes the normal features and mitigates the identity shortcut, furthermore, CRAD effectively handles diverse classes in a single model thanks to the high-granularity continuous representation. In an evaluation using the MVTec AD dataset, CRAD significantly outperforms the previous state-of-the-art method by reducing 65.0% of the error for multi-class unified anomaly detection. The project page is available at https://tae-mo.github.io/crad/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://tae-mo.github.io/crad/"
    },
    {
        "paper id": "2402.18397",
        "abstract url": "https://arxiv.org/abs/2402.18397",
        "title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the use of instructions in prompts. Our multilingual investigation shows that English-centric language models perform better on average than multilingual models. Our study offers insights into the multilingual transferability of English-centric LLMs, contributing to the understanding of their multilingual linguistic knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "18 pages, 7 figures"
    },
    {
        "paper id": "2402.18777",
        "abstract url": "https://arxiv.org/abs/2402.18777",
        "title": "GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning",
        "rating": 0,
        "keywords": [
            [
                "voxel"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Functional magnetic resonance imaging techniques benefit from echo-planar imaging's fast image acquisition but are susceptible to inhomogeneities in the main magnetic field, resulting in geometric distortion and signal loss artifacts in the images. Traditional methods leverage a field map or voxel displacement map for distortion correction. However, voxel displacement map estimation requires additional sequence acquisitions, and the accuracy of the estimation influences correction performance. This work implements a novel approach called GDCNet, which estimates a geometric distortion map by non-linear registration to T1-weighted anatomical images and applies it for distortion correction. GDCNet demonstrated fast distortion correction of functional images in retrospectively and prospectively acquired datasets. Among the compared models, the 2D self-supervised configuration resulted in a statistically significant improvement to normalized mutual information between distortion-corrected functional and T1-weighted images compared to the benchmark methods FUGUE and TOPUP. Furthermore, GDCNet models achieved processing speeds 14 times faster than TOPUP in the prospective dataset.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "30 pages, 9 figures, 3 tables"
    },
    {
        "paper id": "2402.18789",
        "abstract url": "https://arxiv.org/abs/2402.18789",
        "title": "FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning",
        "rating": 0,
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Finetuning",
                "GPU memory"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization and graph pruning, two static compilation optimizations, to minimize the memory overhead and latency for co-serving. Compared to existing systems, FlexLLM's co-serving approach reduces the activation GPU memory overhead by up to 8x, and the end-to-end GPU memory requirement of finetuning by up to 36% while maintaining a low inference latency and improving finetuning throughput. For example, under a heavy inference workload, FlexLLM can still preserve more than 80% of the peak finetuning throughput, whereas existing systems cannot make any progress with finetuning. The source code of FlexLLM is publicly available at https://github.com/flexflow/FlexFlow.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18835",
        "abstract url": "https://arxiv.org/abs/2402.18835",
        "title": "Envisioning the Applications and Implications of Generative AI for News Media",
        "rating": 0.0,
        "keywords": [
            [
                "cs.CY"
            ],
            [
                "Workshop"
            ]
        ],
        "abstract": "This article considers the increasing use of algorithmic decision-support systems and synthetic media in the newsroom, and explores how generative models can help reporters and editors across a range of tasks from the conception of a news story to its distribution. Specifically, we draw from a taxonomy of tasks associated with news production, and discuss where generative models could appropriately support reporters, the journalistic and ethical values that must be preserved within these interactions, and the resulting implications for design contributions in this area in the future. Our essay is relevant to practitioners and researchers as they consider using generative AI systems to support different tasks and workflows.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Accepted to CHI 2023 Workshop on Generative AI and HCI; 8 pages"
    },
    {
        "paper id": "2403.00835",
        "abstract url": "https://arxiv.org/abs/2403.00835",
        "title": "CLLMs: Consistency Large Language Models",
        "rating": 0,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18127",
        "abstract url": "https://arxiv.org/abs/2402.18127",
        "title": "Hierarchical Multi-Relational Graph Representation Learning for Large-Scale Prediction of Drug-Drug Interactions",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Most existing methods for predicting drug-drug interactions (DDI) predominantly concentrate on capturing the explicit relationships among drugs, overlooking the valuable implicit correlations present between drug pairs (DPs), which leads to weak predictions. To address this issue, this paper introduces a hierarchical multi-relational graph representation learning (HMGRL) approach. Within the framework of HMGRL, we leverage a wealth of drug-related heterogeneous data sources to construct heterogeneous graphs, where nodes represent drugs and edges denote clear and various associations. The relational graph convolutional network (RGCN) is employed to capture diverse explicit relationships between drugs from these heterogeneous graphs. Additionally, a multi-view differentiable spectral clustering (MVDSC) module is developed to capture multiple valuable implicit correlations between DPs. Within the MVDSC, we utilize multiple DP features to construct graphs, where nodes represent DPs and edges denote different implicit correlations. Subsequently, multiple DP representations are generated through graph cutting, each emphasizing distinct implicit correlations. The graph-cutting strategy enables our HMGRL to identify strongly connected communities of graphs, thereby reducing the fusion of irrelevant features. By combining every representation view of a DP, we create high-level DP representations for predicting DDIs. Two genuine datasets spanning three distinct tasks are adopted to gauge the efficacy of our HMGRL. Experimental outcomes unequivocally indicate that HMGRL surpasses several leading-edge methods in performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages,10 figures"
    },
    {
        "paper id": "2402.18140",
        "abstract url": "https://arxiv.org/abs/2402.18140",
        "title": "OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction",
        "rating": -0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "This technical report presents our solution, \"occTransformer\" for the 3D occupancy prediction track in the autonomous driving challenge at CVPR 2023. Our method builds upon the strong baseline BEVFormer and improves its performance through several simple yet effective techniques. Firstly, we employed data augmentation to increase the diversity of the training data and improve the model's generalization ability. Secondly, we used a strong image backbone to extract more informative features from the input data. Thirdly, we incorporated a 3D unet head to better capture the spatial information of the scene. Fourthly, we added more loss functions to better optimize the model. Additionally, we used an ensemble approach with the occ model BevDet and SurroundOcc to further improve the performance. Most importantly, we integrated 3D detection model StreamPETR to enhance the model's ability to detect objects in the scene. Using these methods, our solution achieved 49.23 miou on the 3D occupancy prediction track in the autonomous driving challenge.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Innovation Award in the 3D Occupancy Prediction Challenge (CVPR23)"
    },
    {
        "paper id": "2402.18153",
        "abstract url": "https://arxiv.org/abs/2402.18153",
        "title": "Diffusion-based Neural Network Weights Generation",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks. While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets. Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task. To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling. Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets. By learning the distribution of a neural network on a variety pretrained models, our approach enables adaptive sampling weights for unseen datasets achieving faster convergence and reaching competitive performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2402.18167",
        "abstract url": "https://arxiv.org/abs/2402.18167",
        "title": "Decentralised Traffic Incident Detection via Network Lasso",
        "rating": -0.5,
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Traffic incident detection plays a key role in intelligent transportation systems, which has gained great attention in transport engineering. In the past, traditional machine learning (ML) based detection methods achieved good performance under a centralised computing paradigm, where all data are transmitted to a central server for building ML models therein. Nowadays, deep neural networks based federated learning (FL) has become a mainstream detection approach to enable the model training in a decentralised manner while warranting local data governance. Such neural networks-centred techniques, however, have overshadowed the utility of well-established ML-based detection methods. In this work, we aim to explore the potential of potent conventional ML-based detection models in modern traffic scenarios featured by distributed data. We leverage an elegant but less explored distributed optimisation framework named Network Lasso, with guaranteed global convergence for convex problem formulations, integrate the potent convex ML model with it, and compare it with centralised learning, local learning, and federated learning methods atop a well-known traffic incident detection dataset. Experimental results show that the proposed network lasso-based approach provides a promising alternative to the FL-based approach in data-decentralised traffic scenarios, with a strong convergence guarantee while rekindling the significance of conventional ML-based detection methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18211",
        "abstract url": "https://arxiv.org/abs/2402.18211",
        "title": "Catastrophic Overfitting: A Potential Blessing in Disguise",
        "rating": -0.5,
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fast Adversarial Training (FAT) has gained increasing attention within the research community owing to its efficacy in improving adversarial robustness. Particularly noteworthy is the challenge posed by catastrophic overfitting (CO) in this field. Although existing FAT approaches have made strides in mitigating CO, the ascent of adversarial robustness occurs with a non-negligible decline in classification accuracy on clean samples. To tackle this issue, we initially employ the feature activation differences between clean and adversarial examples to analyze the underlying causes of CO. Intriguingly, our findings reveal that CO can be attributed to the feature coverage induced by a few specific pathways. By intentionally manipulating feature activation differences in these pathways with well-designed regularization terms, we can effectively mitigate and induce CO, providing further evidence for this observation. Notably, models trained stably with these terms exhibit superior performance compared to prior FAT work. On this basis, we harness CO to achieve `attack obfuscation', aiming to bolster model performance. Consequently, the models suffering from CO can attain optimal classification accuracy on both clean and adversarial data when adding random noise to inputs during evaluation. We also validate their robustness against transferred adversarial examples and the necessity of inducing CO to improve robustness. Hence, CO may not be a problem that has to be solved.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18213",
        "abstract url": "https://arxiv.org/abs/2402.18213",
        "title": "Multi-objective Differentiable Neural Architecture Search",
        "rating": -0.5,
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a Transformer space on machine translation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "31 pages, 22 figures"
    },
    {
        "paper id": "2402.18285",
        "abstract url": "https://arxiv.org/abs/2402.18285",
        "title": "PiShield: A NeSy Framework for Learning with Requirements",
        "rating": -0.5,
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Demo paper"
    },
    {
        "paper id": "2402.18393",
        "abstract url": "https://arxiv.org/abs/2402.18393",
        "title": "Evaluating Decision Optimality of Autonomous Driving via Metamorphic Testing",
        "rating": -0.5,
        "keywords": [
            [
                "Autonomous Driving"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety. However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is equally vital to ensure the intelligence and reduce risks of AVs. Currently, there is little work dedicated to assessing ADSs' optimal decision-making performance due to the lack of corresponding oracles and the difficulty in generating scenarios with non-optimal decisions. In this paper, we focus on evaluating the decision-making quality of an ADS and propose the first method for detecting non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal paths for AVs. Firstly, to deal with the oracle problem, we propose a novel metamorphic relation (MR) aimed at exposing violations of optimal decisions. The MR identifies the property that the ADS should retain optimal decisions when the optimal path remains unaffected by non-invasive changes. Subsequently, we develop a new framework, Decictor, designed to generate NoDSs efficiently. Decictor comprises three main components: Non-invasive Mutation, MR Check, and Feedback. The Non-invasive Mutation ensures that the original optimal path in the mutated scenarios is not affected, while the MR Check is responsible for determining whether non-optimal decisions are made. To enhance the effectiveness of identifying NoDSs, we design a feedback metric that combines both spatial and temporal aspects of the AV's movement. We evaluate Decictor on Baidu Apollo, an open-source and production-grade ADS. The experimental results validate the effectiveness of Decictor in detecting non-optimal decisions of ADSs. Our work provides valuable and original insights into evaluating the non-safety-critical performance of ADSs.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18443",
        "abstract url": "https://arxiv.org/abs/2402.18443",
        "title": "LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs",
        "rating": -0.5,
        "keywords": [
            [
                "architecture search"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge. This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions. In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters. We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component. We observe that the proposed framework can rapidly (within hours) discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "19 pages, 5 figures, 10 tables and 3 algorithms"
    },
    {
        "paper id": "2402.18493",
        "abstract url": "https://arxiv.org/abs/2402.18493",
        "title": "Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection",
        "rating": -0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "LiDAR-based 3D object detection models have traditionally struggled under rainy conditions due to the degraded and noisy scanning signals. Previous research has attempted to address this by simulating the noise from rain to improve the robustness of detection models. However, significant disparities exist between simulated and actual rain-impacted data points. In this work, we propose a novel rain simulation method, termed DRET, that unifies Dynamics and Rainy Environment Theory to provide a cost-effective means of expanding the available realistic rain data for 3D detection training. Furthermore, we present a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D detection under rainy conditions. Extensive experiments on the WaymoOpenDataset large-scale dataset show that, when combined with the state-of-the-art DSVT model and other classical 3D detectors, our proposed framework demonstrates significant detection accuracy improvements, without losing efficiency. Remarkably, our framework also improves detection capabilities under sunny conditions, therefore offering a robust solution for 3D detection regardless of whether the weather is rainy or sunny",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AAAI2024"
    },
    {
        "paper id": "2402.18495",
        "abstract url": "https://arxiv.org/abs/2402.18495",
        "title": "ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Open-set graph learning is a practical task that aims to classify the known class nodes and to identify unknown class samples as unknowns. Conventional node classification methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as out-of-distribution (OOD) data and in-distribution (IND) noise. OOD data are samples that do not belong to any known classes. They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing. IND noise are training samples which are assigned incorrect labels. The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem. Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID graph data.To this end, we propose a unified framework named ROG$_{PL}$ to achieve robust open-set learning on complex noisy graph data, by introducing prototype learning. In specific, ROG$_{PL}$ consists of two modules, i.e., denoising via label propagation and open-set prototype learning via regions. The first module corrects noisy labels through similarity-based label propagation and removes low-confidence samples, to solve the intra-class variety problem caused by noise. The second module learns open-set prototypes for each known class via non-overlapped regions and remains both interior and border prototypes to remedy the inter-class confusion problem.The two modules are iteratively updated under the constraints of classification loss and prototype diversity loss. To the best of our knowledge, the proposed ROG$_{PL}$ is the first robust open-set node classification method for graph data with complex noise.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages, 5 figures"
    },
    {
        "paper id": "2402.18510",
        "abstract url": "https://arxiv.org/abs/2402.18510",
        "title": "RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "42 pages, 5 figures"
    },
    {
        "paper id": "2402.18567",
        "abstract url": "https://arxiv.org/abs/2402.18567",
        "title": "Diffusion Language Models Are Versatile Protein Learners",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18603",
        "abstract url": "https://arxiv.org/abs/2402.18603",
        "title": "MMSR: Symbolic Regression is a Multimodal Task",
        "rating": -0.5,
        "keywords": [
            [
                "skeletons"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, we proposed MMSR. The SR problem is solved as a pure multimodal problem, and contrastive learning is also introduced in the training process for modal alignment to facilitate later modal feature fusion. It is worth noting that in order to better promote the modal feature fusion, we adopt the strategy of training contrastive learning loss and other losses at the same time, which only needs one-step training, instead of training contrastive learning loss first and then training other losses. Because our experiments prove training together can make the feature extraction module and feature fusion module running-in better. Experimental results show that compared with multiple large-scale pre-training baselines, MMSR achieves the most advanced results on multiple mainstream datasets including SRBench.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 page"
    },
    {
        "paper id": "2402.18606",
        "abstract url": "https://arxiv.org/abs/2402.18606",
        "title": "Impact of network topology on the performance of Decentralized Federated Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fully decentralized learning is gaining momentum for training AI models at the Internet's edge, addressing infrastructure challenges and privacy concerns. In a decentralized machine learning system, data is distributed across multiple nodes, with each node training a local model based on its respective dataset. The local models are then shared and combined to form a global model capable of making accurate predictions on new data. Our exploration focuses on how different types of network structures influence the spreading of knowledge - the process by which nodes incorporate insights gained from learning patterns in data available on other nodes across the network. Specifically, this study investigates the intricate interplay between network structure and learning performance using three network topologies and six data distribution methods. These methods consider different vertex properties, including degree centrality, betweenness centrality, and clustering coefficient, along with whether nodes exhibit high or low values of these metrics. Our findings underscore the significance of global centrality metrics (degree, betweenness) in correlating with learning performance, while local clustering proves less predictive. We highlight the challenges in transferring knowledge from peripheral to central nodes, attributed to a dilution effect during model aggregation. Additionally, we observe that central nodes exert a pull effect, facilitating the spread of knowledge. In examining degree distribution, hubs in Barabasi-Albert networks positively impact learning for central nodes but exacerbate dilution when knowledge originates from peripheral nodes. Finally, we demonstrate the formidable challenge of knowledge circulation outside of segregated communities.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Funding: H2020 HumaneAI Net (Grant N. 952026), CHIST-ERA SAI (CHIST-ERA-19-XAI010), PNRR FAIR (PE00000013), PNRR RESTART (PE00000001). arXiv admin note: text overlap with arXiv:2307.15947"
    },
    {
        "paper id": "2402.18610",
        "abstract url": "https://arxiv.org/abs/2402.18610",
        "title": "Why Attention Graphs Are All We Need: Pioneering Hierarchical Classification of Hematologic Cell Populations with LeukoGraph",
        "rating": -0.5,
        "keywords": [
            [
                "GNNs",
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the complex landscape of hematologic samples such as peripheral blood or bone marrow, cell classification, delineating diverse populations into a hierarchical structure, presents profound challenges. This study presents LeukoGraph, a recently developed framework designed explicitly for this purpose employing graph attention networks (GATs) to navigate hierarchical classification (HC) complexities. Notably, LeukoGraph stands as a pioneering effort, marking the application of graph neural networks (GNNs) for hierarchical inference on graphs, accommodating up to one million nodes and millions of edges, all derived from flow cytometry data. LeukoGraph intricately addresses a classification paradigm where for example four different cell populations undergo flat categorization, while a fifth diverges into two distinct child branches, exemplifying the nuanced hierarchical structure inherent in complex datasets. The technique is more general than this example. A hallmark achievement of LeukoGraph is its F-score of 98%, significantly outclassing prevailing state-of-the-art methodologies. Crucially, LeukoGraph's prowess extends beyond theoretical innovation, showcasing remarkable precision in predicting both flat and hierarchical cell types across flow cytometry datasets from 30 distinct patients. This precision is further underscored by LeukoGraph's ability to maintain a correct label ratio, despite the inherent challenges posed by hierarchical classifications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18679",
        "abstract url": "https://arxiv.org/abs/2402.18679",
        "title": "Data Interpreter: An LLM Agent For Data Science",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18689",
        "abstract url": "https://arxiv.org/abs/2402.18689",
        "title": "The VOROS: Lifting ROC curves to 3D",
        "rating": -0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The area under the ROC curve is a common measure that is often used to rank the relative performance of different binary classifiers. However, as has been also previously noted, it can be a measure that ill-captures the benefits of different classifiers when either the true class values or misclassification costs are highly unbalanced between the two classes. We introduce a third dimension to capture these costs, and lift the ROC curve to a ROC surface in a natural way. We study both this surface and introduce the VOROS, the volume over this ROC surface, as a 3D generalization of the 2D area under the ROC curve. For problems where there are only bounds on the expected costs or class imbalances, we restrict consideration to the volume of the appropriate subregion of the ROC surface. We show how the VOROS can better capture the costs of different classifiers on both a classical and a modern example dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "38 pages, 19 figures"
    },
    {
        "paper id": "2402.18759",
        "abstract url": "https://arxiv.org/abs/2402.18759",
        "title": "Learning with Language-Guided State Abstractions",
        "rating": -0.5,
        "keywords": [
            [
                "robot"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications. We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "ICLR 2024"
    },
    {
        "paper id": "2402.18786",
        "abstract url": "https://arxiv.org/abs/2402.18786",
        "title": "OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition",
        "rating": -0.5,
        "keywords": [
            [
                "diagnosis",
                "disease",
                "facial"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2402.18787",
        "abstract url": "https://arxiv.org/abs/2402.18787",
        "title": "Enhancing the \"Immunity\" of Mixture-of-Experts Networks for Adversarial Defense",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs) to adversarial examples, which can easily fool DNNs into making incorrect predictions. To mitigate this deficiency, we propose a novel adversarial defense method called \"Immunity\" (Innovative MoE with MUtual information \\& positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture in this work. The key enhancements to the standard MoE are two-fold: 1) integrating of Random Switch Gates (RSGs) to obtain diverse network structures via random permutation of RSG parameters at evaluation time, despite of RSGs being determined after one-time training; 2) devising innovative Mutual Information (MI)-based and Position Stability-based loss functions by capitalizing on Grad-CAM's explanatory power to increase the diversity and the causality of expert networks. Notably, our MI-based loss operates directly on the heatmaps, thereby inducing subtler negative impacts on the classification performance when compared to other losses of the same type, theoretically. Extensive evaluation validates the efficacy of the proposed approach in improving adversarial robustness against a wide range of attacks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18792",
        "abstract url": "https://arxiv.org/abs/2402.18792",
        "title": "MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks",
        "rating": -0.5,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks have been proven to be vulnerable to adversarial examples and various methods have been proposed to defend against adversarial attacks for natural language processing tasks. However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task. In this paper, we propose a malicious perturbation based adversarial training method (MPAT) for building robust deep neural networks against textual adversarial attacks. Specifically, we construct a multi-level malicious example generation strategy to generate adversarial examples with malicious perturbations, which are used instead of original inputs for model training. Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task. We conduct comprehensive experiments to evaluate our defense method by attacking five victim models on three benchmark datasets. The result demonstrates that our method is more effective against malicious adversarial attacks compared with previous defense methods while maintaining or further improving the performance on the original task.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18800",
        "abstract url": "https://arxiv.org/abs/2402.18800",
        "title": "BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise Missing Data",
        "rating": -0.5,
        "keywords": [
            [
                "GAN"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Block-wise missing data poses significant challenges in real-world data imputation tasks. Compared to scattered missing data, block-wise gaps exacerbate adverse effects on subsequent analytic and machine learning tasks, as the lack of local neighboring elements significantly reduces the interpolation capability and predictive power. However, this issue has not received adequate attention. Most SOTA matrix completion methods appeared less effective, primarily due to overreliance on neighboring elements for predictions. We systematically analyze the issue and propose a novel matrix completion method ``BlockEcho\" for a more comprehensive solution. This method creatively integrates Matrix Factorization (MF) within Generative Adversarial Networks (GAN) to explicitly retain long-distance inter-element relationships in the original matrix. Besides, we incorporate an additional discriminator for GAN, comparing the generator's intermediate progress with pre-trained MF results to constrain high-order feature distributions. Subsequently, we evaluate BlockEcho on public datasets across three domains. Results demonstrate superior performance over both traditional and SOTA methods when imputing block-wise missing data, especially at higher missing rates. The advantage also holds for scattered missing data at high missing rates. We also contribute on the analyses in providing theoretical justification on the optimality and convergence of fusing MF and GAN for missing block data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18839",
        "abstract url": "https://arxiv.org/abs/2402.18839",
        "title": "Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation",
        "rating": -0.5,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the \"guidance strength,\" but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method that aims to match the matrix field as opposed to the vector field. Our framework ensures the continuity of the generated conditional distribution through the existence of flow between conditional distributions. We will present our theory through experiments and mathematical results.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "15 pages, 4 figures"
    },
    {
        "paper id": "2402.18848",
        "abstract url": "https://arxiv.org/abs/2402.18848",
        "title": "SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting",
        "rating": -0.5,
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a self-supervised pre-training strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new benchmark in relighting realism.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024. Live demos available at https://www.beeble.ai/"
    },
    {
        "paper id": "2403.00024",
        "abstract url": "https://arxiv.org/abs/2403.00024",
        "title": "FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class Classification in Flow Cytometry Benchmarking",
        "rating": -0.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents FlowCyt, the first comprehensive benchmark for multi-class single-cell classification in flow cytometry data. The dataset comprises bone marrow samples from 30 patients, with each cell characterized by twelve markers. Ground truth labels identify five hematological cell types: T lymphocytes, B lymphocytes, Monocytes, Mast cells, and Hematopoietic Stem/Progenitor Cells (HSPCs). Experiments utilize supervised inductive learning and semi-supervised transductive learning on up to 1 million cells per patient. Baseline methods include Gaussian Mixture Models, XGBoost, Random Forests, Deep Neural Networks, and Graph Neural Networks (GNNs). GNNs demonstrate superior performance by exploiting spatial relationships in graph-encoded data. The benchmark allows standardized evaluation of clinically relevant classification tasks, along with exploratory analyses to gain insights into hematological cell phenotypes. This represents the first public flow cytometry benchmark with a richly annotated, heterogeneous dataset. It will empower the development and rigorous assessment of novel methodologies for single-cell analysis.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2402.18611"
    },
    {
        "paper id": "2403.00025",
        "abstract url": "https://arxiv.org/abs/2403.00025",
        "title": "On the Challenges and Opportunities in Generative AI",
        "rating": -0.5,
        "keywords": [
            [
                "synthesizing"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI solutions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00027",
        "abstract url": "https://arxiv.org/abs/2403.00027",
        "title": "A Quick Framework for Evaluating Worst Robustness of Complex Networks",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Robustness is pivotal for comprehending, designing, optimizing, and rehabilitating networks, with simulation attacks being the prevailing evaluation method. Simulation attacks are often time-consuming or even impractical, however, a more crucial yet persistently overlooked drawback is that any attack strategy merely provides a potential paradigm of disintegration. The key concern is: in the worst-case scenario or facing the most severe attacks, what is the limit of robustness, referred to as ``Worst Robustness'', for a given system? Understanding a system's worst robustness is imperative for grasping its reliability limits, accurately evaluating protective capabilities, and determining associated design and security maintenance costs. To address these challenges, we introduce the concept of Most Destruction Attack (MDA) based on the idea of knowledge stacking. MDA is employed to assess the worst robustness of networks, followed by the application of an adapted CNN algorithm for rapid worst robustness prediction. We establish the logical validity of MDA and highlight the exceptional performance of the adapted CNN algorithm in predicting the worst robustness across diverse network topologies, encompassing both model and empirical networks.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "30 pages, 8figures, 4tables,journal"
    },
    {
        "paper id": "2403.00030",
        "abstract url": "https://arxiv.org/abs/2403.00030",
        "title": "GraphPub: Generation of Differential Privacy Graph with High Availability",
        "rating": -0.5,
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "In recent years, with the rapid development of graph neural networks (GNN), more and more graph datasets have been published for GNN tasks. However, when an upstream data owner publishes graph data, there are often many privacy concerns, because many real-world graph data contain sensitive information like person's friend list. Differential privacy (DP) is a common method to protect privacy, but due to the complex topological structure of graph data, applying DP on graphs often affects the message passing and aggregation of GNN models, leading to a decrease in model accuracy. In this paper, we propose a novel graph edge protection framework, graph publisher (GraphPub), which can protect graph topology while ensuring that the availability of data is basically unchanged. Through reverse learning and the encoder-decoder mechanism, we search for some false edges that do not have a large negative impact on the aggregation of node features, and use them to replace some real edges. The modified graph will be published, which is difficult to distinguish between real and false data. Sufficient experiments prove that our framework achieves model accuracy close to the original graph with an extremely low privacy budget.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00839",
        "abstract url": "https://arxiv.org/abs/2403.00839",
        "title": "ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "While achieving remarkable progress in a broad range of tasks, large language models (LLMs) remain significantly limited in properly using massive external tools. Existing in-context learning approaches simply format tools into a list of plain text descriptions and input them to LLMs, from which, LLMs generate a sequence of tool calls to solve problems step by step. Such a paradigm ignores the intrinsic dependency between tools and offloads all reasoning loads to LLMs, making them restricted to a limited number of specifically designed tools. It thus remains challenging for LLMs to operate on a library of massive tools, casting a great limitation when confronted with real-world scenarios. This paper proposes ToolNet, a plug-and-play framework that scales up the number of tools to thousands with a moderate increase in token consumption. ToolNet organizes tools into a directed graph. Each node represents a tool, and weighted edges denote tool transition. Starting from an initial tool node, an LLM navigates in the graph by iteratively choosing the next one from its successors until the task is resolved. Extensive experiments show that ToolNet can achieve impressive results in challenging multi-hop tool learning datasets and is resilient to tool failures.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18060",
        "abstract url": "https://arxiv.org/abs/2402.18060",
        "title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions",
        "rating": -1,
        "keywords": [
            [
                "Medical",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "LLMs have demonstrated impressive performance in answering medical questions, such as passing scores on medical licensing examinations. However, medical board exam questions or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. The inconsistency between automatic and human evaluations of model-generated explanations highlights the need to develop new metrics to support future research on explainable medical QA.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18065",
        "abstract url": "https://arxiv.org/abs/2402.18065",
        "title": "A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains",
        "rating": -1,
        "keywords": [
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a benchmark real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted for publication at IEEE ICRA 2024"
    },
    {
        "paper id": "2402.18076",
        "abstract url": "https://arxiv.org/abs/2402.18076",
        "title": "Online Ecological Gearshift Strategy via Neural Network with Soft-Argmax Operator",
        "rating": -1,
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "This paper presents a neural network optimizer with soft-argmax operator to achieve an ecological gearshift strategy in real-time. The strategy is reformulated as the mixed-integer model predictive control (MIMPC) problem to minimize energy consumption. Then the outer convexification is introduced to transform integer variables into relaxed binary controls. To approximate binary solutions properly within training, the soft-argmax operator is applied to the neural network with the fact that all the operations of this scheme are differentiable. Moreover, this operator can help push the relaxed binary variables close to 0 or 1. To evaluate the strategy effect, we deployed it to a 2-speed electric vehicle (EV). In contrast to the mature solver Bonmin, our proposed method not only achieves similar energy-saving effects but also significantly reduces the solution time to meet real-time requirements. This results in a notable energy savings of 6.02% compared to the rule-based method.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 5 figures, submitted to 8th IFAC Conference on Nonlinear Model Predictive Control"
    },
    {
        "paper id": "2402.18101",
        "abstract url": "https://arxiv.org/abs/2402.18101",
        "title": "Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context",
        "rating": -1,
        "keywords": [
            [
                "Grammar"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and articles, especially the latter, were predominant. Specifically, in terms of context-independent errors, the model occasionally overlooked basic ones and faced challenges with overly erroneous or complex structures. Meanwhile, context-dependent errors, notably those related to tense and noun number, as well as those possibly influenced by the students' first language (L1), remained particularly challenging.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
        "paper id": "2402.18104",
        "abstract url": "https://arxiv.org/abs/2402.18104",
        "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
        "rating": -1,
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 90\\% attack success rate on LLM chatbots GPT-4.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18112",
        "abstract url": "https://arxiv.org/abs/2402.18112",
        "title": "Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input",
        "rating": -1,
        "keywords": [
            [
                "infrared"
            ]
        ],
        "abstract": "Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for monitoring brain activity. To better understand the brain, researchers often use deep learning to address the classification challenges of fNIRS data. Our study shows that while current networks in fNIRS are highly accurate for predictions within their training distribution, they falter at identifying and excluding abnormal data which is out-of-distribution, affecting their reliability. We propose integrating metric learning and supervised methods into fNIRS research to improve networks capability in identifying and excluding out-of-distribution outliers. This method is simple yet effective. In our experiments, it significantly enhances the performance of various networks in fNIRS, particularly transformer-based one, which shows the great improvement in reliability. We will make our experiment data available on GitHub.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18114",
        "abstract url": "https://arxiv.org/abs/2402.18114",
        "title": "PIMSYN: Synthesizing Processing-in-memory CNN Accelerators",
        "rating": -1,
        "keywords": [
            [
                "Synthesizing"
            ]
        ],
        "abstract": "Processing-in-memory architectures have been regarded as a promising solution for CNN acceleration. Existing PIM accelerator designs rely heavily on the experience of experts and require significant manual design overhead. Manual design cannot effectively optimize and explore architecture implementations. In this work, we develop an automatic framework PIMSYN for synthesizing PIM-based CNN accelerators, which greatly facilitates architecture design and helps generate energyefficient accelerators. PIMSYN can automatically transform CNN applications into execution workflows and hardware construction of PIM accelerators. To systematically optimize the architecture, we embed an architectural exploration flow into the synthesis framework, providing a more comprehensive design space. Experiments demonstrate that PIMSYN improves the power efficiency by several times compared with existing works. PIMSYN can be obtained from https://github.com/lixixi-jook/PIMSYN-NN.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18122",
        "abstract url": "https://arxiv.org/abs/2402.18122",
        "title": "G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment",
        "rating": -1,
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4G's success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18123",
        "abstract url": "https://arxiv.org/abs/2402.18123",
        "title": "Fixture calibration with guaranteed bounds from a few correspondence-free surface points",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Calibration of fixtures in robotic work cells is essential but also time consuming and error-prone, and poor calibration can easily lead to wasted debugging time in downstream tasks. Contact-based calibration methods let the user measure points on the fixture's surface with a tool tip attached to the robot's end effector. Most such methods require the user to manually annotate correspondences on the CAD model, however, this is error-prone and a cumbersome user experience. We propose a correspondence-free alternative: The user simply measures a few points from the fixture's surface, and our method provides a tight superset of the poses which could explain the measured points. This naturally detects ambiguities related to symmetry and uninformative points and conveys this uncertainty to the user. Perhaps more importantly, it provides guaranteed bounds on the pose. The computation of such bounds is made tractable by the use of a hierarchical grid on SE(3). Our method is evaluated both in simulation and on a real collaborative robot, showing great potential for easier and less error-prone fixture calibration. Project page at https://sites.google.com/view/ttpose",
        "subjects": [
            "cs.RO"
        ],
        "comment": "ICRA 2024"
    },
    {
        "paper id": "2402.18132",
        "abstract url": "https://arxiv.org/abs/2402.18132",
        "title": "Understanding the Role of Pathways in a Deep Neural Network",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep neural networks have demonstrated superior performance in artificial intelligence applications, but the opaqueness of their inner working mechanism is one major drawback in their application. The prevailing unit-based interpretation is a statistical observation of stimulus-response data, which fails to show a detailed internal process of inherent mechanisms of neural networks. In this work, we analyze a convolutional neural network (CNN) trained in the classification task and present an algorithm to extract the diffusion pathways of individual pixels to identify the locations of pixels in an input image associated with object classes. The pathways allow us to test the causal components which are important for classification and the pathway-based representations are clearly distinguishable between categories. We find that the few largest pathways of an individual pixel from an image tend to cross the feature maps in each layer that is important for classification. And the large pathways of images of the same category are more consistent in their trends than those of different categories. We also apply the pathways to understanding adversarial attacks, object completion, and movement perception. Further, the total number of pathways on feature maps in all layers can clearly discriminate the original, deformed, and target samples.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18134",
        "abstract url": "https://arxiv.org/abs/2402.18134",
        "title": "Learning to Deblur Polarized Images",
        "rating": -1,
        "keywords": [
            [
                "dehazing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "A polarization camera can capture four polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of polarization (DoP) and the angle of polarization (AoP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoP and AoP. Deblurring methods for conventional images often show degenerated performance when handling the polarized images since they only focus on deblurring without considering the polarization constrains. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18137",
        "abstract url": "https://arxiv.org/abs/2402.18137",
        "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively tailored for decision-making tasks, providing an embodied representation learning framework that elegantly extracts both local and global task progression features, with temporal consistency enforced through implicit time contrastive learning, while ensuring trajectory-level instruction grounding via multimodal joint encoding. Evaluation on both simulated and real robots demonstrates that DecisionNCE effectively facilitates diverse downstream policy learning tasks, offering a versatile solution for unified representation and reward learning. Project Page: https://2toinf.github.io/DecisionNCE/",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Preprint. 27 pages, 19 figures"
    },
    {
        "paper id": "2402.18147",
        "abstract url": "https://arxiv.org/abs/2402.18147",
        "title": "A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction",
        "rating": -1,
        "keywords": [
            [
                "Image Enhancement"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Human vision relies heavily on available ambient light to perceive objects. Low-light scenes pose two distinct challenges: information loss due to insufficient illumination and undesirable brightness shifts. Low-light image enhancement (LLIE) refers to image enhancement technology tailored to handle this scenario. We introduce CPGA-Net, an innovative LLIE network that combines dark/bright channel priors and gamma correction via deep learning and integrates features inspired by the Atmospheric Scattering Model and the Retinex Theory. This approach combines the use of traditional and deep learning methodologies, designed within a simple yet efficient architectural framework that focuses on essential feature extraction. The resulting CPGA-Net is a lightweight network with only 0.025 million parameters and 0.030 seconds for inference time, yet it achieves superior performance over existing LLIE methods on both objective and subjective evaluation criteria. Furthermore, we utilized knowledge distillation with explainable factors and proposed an efficient version that achieves 0.018 million parameters and 0.006 seconds for inference time. The proposed approaches inject new solution ideas into LLIE, providing practical applications in challenging low-light scenarios.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Preprint of an article submitted for consideration in [International Journal of Pattern Recognition and Artificial Intelligence] \\c{opyright} [2024] [copyright World Scientific Publishing Company] [https://www.worldscientific.com/worldscinet/ijprai]"
    },
    {
        "paper id": "2402.18191",
        "abstract url": "https://arxiv.org/abs/2402.18191",
        "title": "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation",
        "rating": -1,
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process.In our experiment, CaR selected a subset containing only 1.96% of Alpaca's IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1% in GPT-4 evaluations. Furthermore, our method utilizes small models (355M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18202",
        "abstract url": "https://arxiv.org/abs/2402.18202",
        "title": "Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments",
        "rating": -1,
        "keywords": [
            [
                "Drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The high incidence of oil spills in port areas poses a serious threat to the environment, prompting the need for efficient detection mechanisms. Utilizing automated drones for this purpose can significantly improve the speed and accuracy of oil spill detection. Such advancements not only expedite cleanup operations, reducing environmental harm but also enhance polluter accountability, potentially deterring future incidents. Currently, there's a scarcity of datasets employing RGB images for oil spill detection in maritime settings. This paper presents a unique, annotated dataset aimed at addressing this gap, leveraging a neural network for analysis on both desktop and edge computing platforms. The dataset, captured via drone, comprises 1268 images categorized into oil, water, and other, with a convolutional neural network trained using an Unet model architecture achieving an F1 score of 0.71 for oil detection. This underscores the dataset's practicality for real-world applications, offering crucial resources for environmental conservation in port environments.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18203",
        "abstract url": "https://arxiv.org/abs/2402.18203",
        "title": "Exploring the space of graphs with fixed discrete curvatures",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Discrete curvatures are quantities associated to the nodes and edges of a graph that reflect the local geometry around them. These curvatures have a rich mathematical theory and they have recently found success as a tool to analyze networks across a wide range of domains. In this work, we consider the problem of constructing graphs with a prescribed set of discrete edge curvatures, and explore the space of such graphs. We address this problem in two ways: first, we develop an evolutionary algorithm to sample graphs with discrete curvatures close to a given set. We use this algorithm to explore how other network statistics vary when constrained by the discrete curvatures in the network. Second, we solve the exact reconstruction problem for the specific case of Forman-Ricci curvature. By leveraging the theory of Markov bases, we obtain a finite set of rewiring moves that connects the space of all graphs with a fixed discrete curvature.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": "21 pages, 8 figures"
    },
    {
        "paper id": "2402.18222",
        "abstract url": "https://arxiv.org/abs/2402.18222",
        "title": "HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Considerable efforts are currently underway to mitigate the negative impacts of echo chambers, such as increased susceptibility to fake news and resistance towards accepting scientific evidence. Prior research has presented the development of computer systems that support the consumption of news information from diverse political perspectives to mitigate the echo chamber effect. However, existing studies still lack the ability to effectively support the key processes of news information consumption and quantitatively identify a political stance towards the information. In this paper, we present HearHere, an AI-based web system designed to help users accommodate information and opinions from diverse perspectives. HearHere facilitates the key processes of news information consumption through two visualizations. Visualization 1 provides political news with quantitative political stance information, derived from our graph-based political classification model, and users can experience diverse perspectives (Hear). Visualization 2 allows users to express their opinions on specific political issues in a comment form and observe the position of their own opinions relative to pro-liberal and pro-conservative comments presented on a map interface (Here). Through a user study with 94 participants, we demonstrate the feasibility of HearHere in supporting the consumption of information from various perspectives. Our findings highlight the importance of providing political stance information and quantifying users' political status as a means to mitigate political polarization. In addition, we propose design implications for system development, including the consideration of demographics such as political interest and providing users with initiatives.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "34 pages, 6 figures, 6 tables, CSCW 2024"
    },
    {
        "paper id": "2402.18245",
        "abstract url": "https://arxiv.org/abs/2402.18245",
        "title": "Lower Bounds for Leaf Rank of Leaf Powers",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Leaf powers and $k$-leaf powers have been studied for over 20 years, but there are still several aspects of this graph class that are poorly understood. One such aspect is the leaf rank of leaf powers, i.e. the smallest number $k$ such that a graph $G$ is a $k$-leaf power. Computing the leaf rank of leaf powers has proved a hard task, and furthermore, results about the asymptotic growth of the leaf rank as a function of the number of vertices in the graph have been few and far between. We present an infinite family of rooted directed path graphs that are leaf powers, and prove that they have leaf rank exponential in the number of vertices (utilizing a type of subtree model first presented by Rautenbach [Some remarks about leaf roots. Discrete mathematics, 2006]). This answers an open question by Brandst\u00e4dt et al. [Rooted directed path graphs are leaf powers. Discrete mathematics, 2010].",
        "subjects": [
            "cs.DM"
        ],
        "comment": "Submitted to IWOCA 2024. 14 pages, 5 figures"
    },
    {
        "paper id": "2402.18246",
        "abstract url": "https://arxiv.org/abs/2402.18246",
        "title": "Reinforcement Learning and Graph Neural Networks for Probabilistic Risk Assessment",
        "rating": -1,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ]
        ],
        "abstract": "This paper presents a new approach to the solution of Probabilistic Risk Assessment (PRA) models using the combination of Reinforcement Learning (RL) and Graph Neural Networks (GNNs). The paper introduces and demonstrates the concept using one of the most popular PRA models - Fault Trees. This paper's original idea is to apply RL algorithms to solve a PRA model represented with a graph model. Given enough training data, or through RL, such an approach helps train generic PRA solvers that can optimize and partially substitute classical PRA solvers that are based on existing formal methods. Such an approach helps to solve the problem of the fast-growing complexity of PRA models of modern technical systems.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18258",
        "abstract url": "https://arxiv.org/abs/2402.18258",
        "title": "A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18265",
        "abstract url": "https://arxiv.org/abs/2402.18265",
        "title": "Output-Sensitive Enumeration of Potential Maximal Cliques in Polynomial Space",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A set of vertices in a graph forms a potential maximal clique if there exists a minimal chordal completion in which it is a maximal clique. Potential maximal cliques were first introduced as a key tool to obtain an efficient, though exponential-time algorithm to compute the treewidth of a graph. As a byproduct, this allowed to compute the treewidth of various graph classes in polynomial time. In recent years, the concept of potential maximal cliques regained interest as it proved to be useful for a handful of graph algorithmic problems. In particular, it turned out to be a key tool to obtain a polynomial time algorithm for computing maximum weight independent sets in $P_5$-free and $P_6$-free graphs (Lokshtanov et al., SODA `14 and Grzeskik et al., SODA `19. In most of their applications, obtaining all the potential maximal cliques constitutes an algorithmic bottleneck, thus motivating the question of how to efficiently enumerate all the potential maximal cliques in a graph $G$. The state-of-the-art algorithm by Bouchitt\u00e9 \\& Todinca can enumerate potential maximal cliques in output-polynomial time by using exponential space, a significant limitation for the size of feasible instances. In this paper, we revisit this algorithm and design an enumeration algorithm that preserves an output-polynomial time complexity while only requiring polynomial space.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18287",
        "abstract url": "https://arxiv.org/abs/2402.18287",
        "title": "Windowed-FourierMixer: Enhancing Clutter-Free Room Modeling with Fourier Transform",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the growing demand for immersive digital applications, the need to understand and reconstruct 3D scenes has significantly increased. In this context, inpainting indoor environments from a single image plays a crucial role in modeling the internal structure of interior spaces as it enables the creation of textured and clutter-free reconstructions. While recent methods have shown significant progress in room modeling, they rely on constraining layout estimators to guide the reconstruction process. These methods are highly dependent on the performance of the structure estimator and its generative ability in heavily occluded environments. In response to these issues, we propose an innovative approach based on a U-Former architecture and a new Windowed-FourierMixer block, resulting in a unified, single-phase network capable of effectively handle human-made periodic structures such as indoor spaces. This new architecture proves advantageous for tasks involving indoor scenes where symmetry is prevalent, allowing the model to effectively capture features such as horizon/ceiling height lines and cuboid-shaped rooms. Experiments show the proposed approach outperforms current state-of-the-art methods on the Structured3D dataset demonstrating superior performance in both quantitative metrics and qualitative results. Code and models will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18292",
        "abstract url": "https://arxiv.org/abs/2402.18292",
        "title": "FSL Model can Score Higher as It Is",
        "rating": -1,
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample. It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample. Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset. According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs. By augmenting both the support set and the queries, we can achieve even more performance improvement. Our Github Repository is publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18294",
        "abstract url": "https://arxiv.org/abs/2402.18294",
        "title": "Whole-body Humanoid Robot Locomotion with Human Reference",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, \"Adam\", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7pages, 7 figures"
    },
    {
        "paper id": "2402.18304",
        "abstract url": "https://arxiv.org/abs/2402.18304",
        "title": "Play like a Vertex: A Stackelberg Game Approach for Streaming Graph Partitioning",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "In the realm of distributed systems tasked with managing and processing large-scale graph-structured data, optimizing graph partitioning stands as a pivotal challenge. The primary goal is to minimize communication overhead and runtime cost. However, alongside the computational complexity associated with optimal graph partitioning, a critical factor to consider is memory overhead. Real-world graphs often reach colossal sizes, making it impractical and economically unviable to load the entire graph into memory for partitioning. This is also a fundamental premise in distributed graph processing, where accommodating a graph with non-distributed systems is unattainable. Currently, existing streaming partitioning algorithms exhibit a skew-oblivious nature, yielding satisfactory partitioning results exclusively for specific graph types. In this paper, we propose a novel streaming partitioning algorithm, the Skewness-aware Vertex-cut Partitioner S5P, designed to leverage the skewness characteristics of real graphs for achieving high-quality partitioning. S5P offers high partitioning quality by segregating the graph's edge set into two subsets, head and tail sets. Following processing by a skewness-aware clustering algorithm, these two subsets subsequently undergo a Stackelberg graph game. Our extensive evaluations conducted on substantial real-world and synthetic graphs demonstrate that, in all instances, the partitioning quality of S5P surpasses that of existing streaming partitioning algorithms, operating within the same load balance constraints. For example, S5P can bring up to a 51% improvement in partitioning quality compared to the top partitioner among the baselines. Lastly, we showcase that the implementation of S5P results in up to an 81% reduction in communication cost and a 130% increase in runtime efficiency for distributed graph processing tasks on PowerGraph.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "This paper has been accepted by SIGMOD 2024"
    },
    {
        "paper id": "2402.18309",
        "abstract url": "https://arxiv.org/abs/2402.18309",
        "title": "Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis",
        "rating": -1,
        "keywords": [
            [
                "point cloud",
                "depth"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance. Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants. Accurately estimating this space from simple images proves challenging due to a lack of depth information. This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective. Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving. These scans, however, also open up possibilities in urban management. In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed. Our system uses semantic segmentation to filter relevant points and downstream processing steps to create the required volume to be kept clear above the road. Challenges include obscured stretches of road, the noisy unstructured nature of LiDAR point clouds, and the assessment of the road shape. The identified points of non-compliant trees can be projected from the point cloud onto images, providing municipalities with a visual aid for dealing with such occurrences. By automating this process, municipalities can address potential road space constraints, enhancing safety for all. They may also save valuable time by carrying out the inspections more systematically. Our open-source code gives communities inspiration on how to automate the process themselves.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18323",
        "abstract url": "https://arxiv.org/abs/2402.18323",
        "title": "Equivalent Environments and Covering Spaces for Robots",
        "rating": -1,
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "This paper formally defines a robot system, including its sensing and actuation components, as a general, topological dynamical system. The focus is on determining general conditions under which various environments in which the robot can be placed are indistinguishable. A key result is that, under very general conditions, covering maps witness such indistinguishability. This formalizes the intuition behind the well studied loop closure problem in robotics. An important special case is where the sensor mapping reports an invariant of the local topological (metric) structure of an environment because such structure is preserved by (metric) covering maps. Whereas coverings provide a sufficient condition for the equivalence of environments, we also give a necessary condition using bisimulation. The overall framework is applied to unify previously identified phenomena in robotics and related fields, in which moving agents with sensors must make inferences about their environments based on limited data. Many open problems are identified.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "34 pages, 8 figures"
    },
    {
        "paper id": "2402.18329",
        "abstract url": "https://arxiv.org/abs/2402.18329",
        "title": "Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation",
        "rating": -1,
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs. LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning. To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs. Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors. We conduct an extensive ablation study to understand which models better handle our augmented dataset, also manipulated to mimic the presence of model-agnostic evasion and poisoning attacks. Our results suggest that augmentation is needed to maintain high-predictive capabilities, robustness to attack is achieved through specific hardening techniques like adversarial training, and it is possible to deploy near-real-time models with almost-zero false alarms.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18332",
        "abstract url": "https://arxiv.org/abs/2402.18332",
        "title": "Recursive GNNs for Learning Precoding Policies with Size-Generalizability",
        "rating": -1,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) have been shown promising in optimizing power allocation and link scheduling with good size generalizability and low training complexity. These merits are important for learning wireless policies under dynamic environments, which partially come from the matched permutation equivariance (PE) properties of the GNNs to the policies to be learned. Nonetheless, it has been noticed in literature that only satisfying the PE property of a precoding policy in multi-antenna systems cannot ensure a GNN for learning precoding to be generalizable to the unseen number of users. Incorporating models with GNNs helps improve size generalizability, which however is only applicable to specific problems, settings, and algorithms. In this paper, we propose a framework of size generalizable GNNs for learning precoding policies that are purely data-driven and can learn wireless policies including but not limited to baseband and hybrid precoding in multi-user multi-antenna systems. To this end, we first find a special structure of each iteration of two numerical algorithms for optimizing precoding, from which we identify the key characteristics of a GNN that affect its size generalizability. Then, we design size-generalizable GNNs that are with these key characteristics and satisfy the PE properties of precoding policies in a recursive manner. Simulation results show that the proposed GNNs can be well-generalized to the number of users for learning baseband and hybrid precoding policies and require much fewer samples than existing counterparts to achieve the same performance.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "37 pages, 8 figures"
    },
    {
        "paper id": "2402.18339",
        "abstract url": "https://arxiv.org/abs/2402.18339",
        "title": "Online Edge Coloring is (Nearly) as Easy as Offline",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The classic theorem of Vizing (Diskret. Analiz.'64) asserts that any graph of maximum degree $\u0394$ can be edge colored (offline) using no more than $\u0394+1$ colors (with $\u0394$ being a trivial lower bound). In the online setting, Bar-Noy, Motwani and Naor (IPL'92) conjectured that a $(1+o(1))\u0394$-edge-coloring can be computed online in $n$-vertex graphs of maximum degree $\u0394=\u03c9(\\log n)$. Numerous algorithms made progress on this question, using a higher number of colors or assuming restricted arrival models, such as random-order edge arrivals or vertex arrivals (e.g., AGKM FOCS'03, BMM SODA'10, CPW FOCS'19, BGW SODA'21, KLSST STOC'22). In this work, we resolve this longstanding conjecture in the affirmative in the most general setting of adversarial edge arrivals. We further generalize this result to obtain online counterparts of the list edge coloring result of Kahn (J. Comb. Theory. A'96) and of the recent \"local\" edge coloring result of Christiansen (STOC'23).",
        "subjects": [
            "cs.DS"
        ],
        "comment": "38 pages, to appear in STOC'24"
    },
    {
        "paper id": "2402.18345",
        "abstract url": "https://arxiv.org/abs/2402.18345",
        "title": "Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural Networks",
        "rating": -1,
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "Challenges in real-world robotic applications often stem from managing multiple, dynamically varying entities such as neighboring robots, manipulable objects, and navigation goals. Existing multi-agent control strategies face scalability limitations, struggling to handle arbitrary numbers of entities. Additionally, they often rely on engineered heuristics for assigning entities among agents. We propose a data driven approach to address these limitations by introducing a decentralized control system using neural network policies trained in simulation. Leveraging permutation invariant neural network architectures and model-free reinforcement learning, our approach allows control agents to autonomously determine the relative importance of different entities without being biased by ordering or limited by a fixed capacity. We validate our approach through both simulations and real-world experiments involving multiple wheeled-legged quadrupedal robots, demonstrating their collaborative control capabilities. We prove the effectiveness of our architectural choice through experiments with three exemplary multi-entity problems. Our analysis underscores the pivotal role of the end-to-end trained permutation invariant encoders in achieving scalability and improving the task performance in multi-object manipulation or multi-goal navigation problems. The adaptability of our policy is further evidenced by its ability to manage varying numbers of entities in a zero-shot manner, showcasing near-optimal autonomous task distribution and collision avoidance behaviors.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18351",
        "abstract url": "https://arxiv.org/abs/2402.18351",
        "title": "LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "GAN"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose LatentSwap, a simple face swapping framework generating a face swap latent code of a given generator. Utilizing randomly sampled latent codes, our framework is light and does not require datasets besides employing the pre-trained models, with the training procedure also being fast and straightforward. The loss objective consists of only three terms, and can effectively control the face swap results between source and target images. By attaching a pre-trained GAN inversion model independent to the model and using the StyleGAN2 generator, our model produces photorealistic and high-resolution images comparable to other competitive face swap models. We show that our framework is applicable to other generators such as StyleNeRF, paving a way to 3D-aware face swapping and is also compatible with other downstream StyleGAN2 generator tasks. The source code and models can be found at \\url{https://github.com/usingcolor/LatentSwap}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 11 figures"
    },
    {
        "paper id": "2402.18352",
        "abstract url": "https://arxiv.org/abs/2402.18352",
        "title": "Polynomial-time approximation schemes for induced subgraph problems on fractionally tree-independence-number-fragile graphs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We investigate a relaxation of the notion of fractional treewidth-fragility, namely fractional tree-independence-number-fragility. In particular, we obtain polynomial-time approximation schemes for meta-problems such as finding a maximum-weight sparse induced subgraph satisfying a given $\\mathsf{CMSO}_2$ formula on fractionally tree-independence-number-fragile graph classes. Our approach unifies and extends several known polynomial-time approximation schemes on seemingly unrelated graph classes, such as classes of intersection graphs of fat objects in a fixed dimension or proper minor-closed classes. We also study the related notion of layered tree-independence number, a relaxation of layered treewidth, and its applications to exact subexponential-time algorithms.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "48 pages, 8 figures. arXiv admin note: substantial text overlap with arXiv:2303.07444"
    },
    {
        "paper id": "2402.18365",
        "abstract url": "https://arxiv.org/abs/2402.18365",
        "title": "Token-based Vehicular Security System (TVSS): Scalable, Secure, Low-latency Public Key Infrastructure for Connected Vehicles",
        "rating": -1,
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "Connected and Autonomous vehicles stand to drastically improve the safety and efficiency of the transportation system in the near future while also reducing pollution. These systems leverage communication to coordinate among vehicles and infrastructure in service of a number of safety and efficiency driver assist and even fully autonomous applications. Attackers can compromise these systems in a number of ways including by falsifying communication messages, making it critical to support security mechanisms that can operate and scale in dynamic scenarios. Towards this end, we present TVSS, a new VPKI system which improves drastically over prior work in the area (including over SCMS; the US department of transportation standard for VPKI). TVSS leverages the idea of unforgeable tokens to enable rapid verification at the road side units (RSUs), which are part of the road infrastructure at the edge of the network. This edge based solution enables agile authentication by avoiding the need for back-end servers during the potentially short contact time between a moving vehicle and the infrastructure. It also results in several security advantages: (1) Scalable Revocation: it greatly simplifies the revocation problem, a difficult problem in large scale certificate systems; and (2) Faster Refresh: Vehicles interact more frequently with the system to refresh their credentials, improving the privacy of the system. We provide a construction of the system and formally prove its security. Field experiments on a test-bed we develop consisting of on-board units (OBUs) and RSUs shows substantial reduction in the latency of refreshing credentials compared to SCMS, allowing the system to work even with smaller window of connectivity when vehicles are moving at higher speeds. Notably, we are able to execute the bottleneck operation of our scheme with a stationary RSU while traveling at highway speeds .",
        "subjects": [
            "cs.CR"
        ],
        "comment": "14 pages, 14 figures"
    },
    {
        "paper id": "2402.18374",
        "abstract url": "https://arxiv.org/abs/2402.18374",
        "title": "VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models",
        "rating": -1,
        "keywords": [
            [
                "biomedical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can successfully verify errors from existing models as a model-agnostic approach. Further analyses on out-of-domain and low-resource settings show the usefulness of VerifiNER on real-world applications.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "19 pages, 9 figures"
    },
    {
        "paper id": "2402.18380",
        "abstract url": "https://arxiv.org/abs/2402.18380",
        "title": "UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers multimodal measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the underlying robot sensor suit. To validate the approach, we show how the proposed sensor fusion can be integrated into a twolevel torque control architecture aiming at task-space torquecontrol. The performances of the proposed approach are shown through extensive tests on the new humanoid robot ergoCub, currently being developed at Istituto Italiano di Tecnologia. We also compare our strategy with the existing state-of-theart approach based on the recursive Newton-Euler algorithm. Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18383",
        "abstract url": "https://arxiv.org/abs/2402.18383",
        "title": "Robust Quantification of Percent Emphysema on CT via Domain Attention: the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study",
        "rating": -1,
        "keywords": [
            [
                "CT",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Robust quantification of pulmonary emphysema on computed tomography (CT) remains challenging for large-scale research studies that involve scans from different scanner types and for translation to clinical scans. Existing studies have explored several directions to tackle this challenge, including density correction, noise filtering, regression, hidden Markov measure field (HMMF) model-based segmentation, and volume-adjusted lung density. Despite some promising results, previous studies either required a tedious workflow or limited opportunities for downstream emphysema subtyping, limiting efficient adaptation on a large-scale study. To alleviate this dilemma, we developed an end-to-end deep learning framework based on an existing HMMF segmentation framework. We first demonstrate that a regular UNet cannot replicate the existing HMMF results because of the lack of scanner priors. We then design a novel domain attention block to fuse image feature with quantitative scanner priors which significantly improves the results.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 5 figures. Accepted to IEEE International Symposium on Biomedical Imaging 2024 (ISBI 2024). Camera-ready version"
    },
    {
        "paper id": "2402.18390",
        "abstract url": "https://arxiv.org/abs/2402.18390",
        "title": "Neuromorphic Event-Driven Semantic Communication in Microgrids",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids. On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer. This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system's control performance. Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner simply using the power exchanges between the nodes. As opposed to the conventional neuromorphic sensors that operate with spiking signals, we employ an event-driven selective process to collect sparse data for training of SNNs. Finally, its multi-fold effectiveness and reliable performance is validated under simulation conditions with different microgrid topologies and components to establish a new direction in the sense-actuate-compute cycle for power electronic dominated grids and microgrids.",
        "subjects": [
            "cs.ET"
        ],
        "comment": "The manuscript has been accepted for publication in IEEE Transactions on Smart Grid"
    },
    {
        "paper id": "2402.18394",
        "abstract url": "https://arxiv.org/abs/2402.18394",
        "title": "Dual-IMU State Estimation for Relative Localization of Two Mobile Agents",
        "rating": -1,
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "In this paper, we address the problem of relative localization of two mobile agents. Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them. Previous works, however, typically assumed known ego motion and ignored biases of the IMUs. Instead, we study the most general case of unknown biases for both IMUs. Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions. Through numerical simulations, we validate our key observability findings and examine their impact on the estimation accuracy and consistency. Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18400",
        "abstract url": "https://arxiv.org/abs/2402.18400",
        "title": "Balanced Similarity with Auxiliary Prompts: Towards Alleviating Text-to-Image Retrieval Bias for CLIP in Zero-shot Learning",
        "rating": -1,
        "keywords": [
            [
                "Text-to-Image"
            ]
        ],
        "abstract": "CLIP has the ability to align texts and images and is nearly the most frequently used foundation model in cross-modal zero-shot learning. However, our experimental findings reveal that CLIP suffers from a bias in text-to-image retrieval, resulting in a decrease in CLIP's zero-shot learning performance. We analytically discover that the bias partly arises from the imbalanced range of similarity scores obtained by CLIP. Accordingly, we propose a Balanced Similarity with Auxiliary Prompts (BSAP) to mitigate the text-to-image retrieval bias of CLIP. Specifically, our BSAP designs auxiliary prompts for CLIP to calculate multiple similarity scores for the retrieval images and then normalizes the scores between each image and the given query text as well as our auxiliary prompts to obtain balanced similarity scores. The balanced similarity score of the given query text is used for the final retrieval. In addition, we attempt to adopt a hybrid similarity that combines our BSAP with the original similarity of CLIP to obtain a more robust outcome. Extensive experiments on two typical zero-shot learning tasks,i.e., Referring Expression Comprehension (REC) and Referring Image Segmentation (RIS), are conducted to demonstrate the effectiveness of our BSAP. Specifically, when using the val dataset of RefCOCO in REC, BSAP increases CLIP's performance by 20.6%.",
        "subjects": [
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18401",
        "abstract url": "https://arxiv.org/abs/2402.18401",
        "title": "DevPhish: Exploring Social Engineering in Software Supply Chain Attacks on Developers",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "The Software Supply Chain (SSC) has captured considerable attention from attackers seeking to infiltrate systems and undermine organizations. There is evidence indicating that adversaries utilize Social Engineering (SocE) techniques specifically aimed at software developers. That is, they interact with developers at critical steps in the Software Development Life Cycle (SDLC), such as accessing Github repositories, incorporating code dependencies, and obtaining approval for Pull Requests (PR) to introduce malicious code. This paper aims to comprehensively explore the existing and emerging SocE tactics employed by adversaries to trick Software Engineers (SWEs) into delivering malicious software. By analyzing a diverse range of resources, which encompass established academic literature and real-world incidents, the paper systematically presents an overview of these manipulative strategies within the realm of the SSC. Such insights prove highly beneficial for threat modeling and security gap analysis.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "7 pages, 2 figures"
    },
    {
        "paper id": "2402.18417",
        "abstract url": "https://arxiv.org/abs/2402.18417",
        "title": "Prediction of recurrence free survival of head and neck cancer using PET/CT radiomics and clinical information",
        "rating": -1,
        "keywords": [
            [
                "survival",
                "CT",
                "cancer",
                "clinical",
                "TumOR"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The 5-year survival rate of Head and Neck Cancer (HNC) has not improved over the past decade and one common cause of treatment failure is recurrence. In this paper, we built Cox proportional hazard (CoxPH) models that predict the recurrence free survival (RFS) of oropharyngeal HNC patients. Our models utilise both clinical information and multimodal radiomics features extracted from tumour regions in Computed Tomography (CT) and Positron Emission Tomography (PET). Furthermore, we were one of the first studies to explore the impact of segmentation accuracy on the predictive power of the extracted radiomics features, through under- and over-segmentation study. Our models were trained using the HEad and neCK TumOR (HECKTOR) challenge data, and the best performing model achieved a concordance index (C-index) of 0.74 for the model utilising clinical information and multimodal CT and PET radiomics features, which compares favourably with the model that only used clinical information (C-index of 0.67). Our under- and over-segmentation study confirms that segmentation accuracy affects radiomics extraction, however, it affects PET and CT differently.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18419",
        "abstract url": "https://arxiv.org/abs/2402.18419",
        "title": "Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "Health",
                "healthcare"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Moreover, we report qualitative assessment by humans on the natural language generation outputs from our approach. Results show that our method achieves superior performance with the mean weighted F1 score of 0.61 as compared to its standard counterparts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18430",
        "abstract url": "https://arxiv.org/abs/2402.18430",
        "title": "Smishing Dataset I: Phishing SMS Dataset from Smishtank.com",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "While smishing (SMS Phishing) attacks have risen to become one of the most common types of social engineering attacks, there is a lack of relevant smishing datasets. One of the biggest challenges in the domain of smishing prevention is the availability of fresh smishing datasets. Additionally, as time persists, smishing campaigns are shut down and the crucial information related to the attack are lost. With the changing nature of smishing attacks, a consistent flow of new smishing examples is needed by both researchers and engineers to create effective defenses. In this paper, we present the community-sourced smishing datasets from the smishtank.com. It provides a wealth of information relevant to combating smishing attacks through the breakdown and analysis of smishing samples at the point of submission. In the contribution of our work, we provide a corpus of 1090 smishing samples that have been publicly submitted through the site. Each message includes information relating to the sender, message body, and any brands referenced in the message. Additionally, when a URL is found, we provide additional information on the domain, VirusTotal results, and a characterization of the URL. Through the open access of fresh smishing data, we empower academia and industries to create robust defenses against this evolving threat.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18507",
        "abstract url": "https://arxiv.org/abs/2402.18507",
        "title": "Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images",
        "rating": -1,
        "keywords": [
            [
                "clinical",
                "Cardiac"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents a multimodal deep learning framework that utilizes advanced image techniques to improve the performance of clinical analysis heavily dependent on routinely acquired standard images. More specifically, we develop a joint learning network that for the first time leverages the accuracy and reproducibility of myocardial strains obtained from Displacement Encoding with Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic resonance (CMR) imaging in late mechanical activation (LMA) detection. An image registration network is utilized to acquire the knowledge of cardiac motions, an important feature estimator of strain values, from standard cine CMRs. Our framework consists of two major components: (i) a DENSE-supervised strain network leveraging latent motion features learned from a registration network to predict myocardial strains; and (ii) a LMA network taking advantage of the predicted strain for effective LMA detection. Experimental results show that our proposed work substantially improves the performance of strain analysis and LMA detection from cine CMR images, aligning more closely with the achievements of DENSE.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18516",
        "abstract url": "https://arxiv.org/abs/2402.18516",
        "title": "Model Predictive Control with adaptive resilience for Denial-of-Service Attacks mitigation on a Regulated Dam",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "In recent years, SCADA (Supervisory Control and Data Acquisition) systems have increasingly become the target of cyber attacks. SCADAs are no longer isolated, as web-based applications expose strategic infrastructures to the outside world connection. In a cyber-warfare context, we propose a Model Predictive Control (MPC) architecture with adaptive resilience, capable of guaranteeing control performance in normal operating conditions and driving towards resilience against DoS (controller-actuator) attacks when needed. Since the attackers' goal is typically to maximize the system damage, we assume they solve an adversarial optimal control problem. An adaptive resilience factor is then designed as a function of the intensity function of a Hawkes process, a point process model estimating the occurrence of random events in time, trained on a moving window to estimate the return time of the next attack. We demonstrate the resulting MPC strategy's effectiveness in 2 attack scenarios on a real system with actual data, the regulated Olginate dam of Lake Como.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18527",
        "abstract url": "https://arxiv.org/abs/2402.18527",
        "title": "Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures",
        "rating": -1,
        "keywords": [
            [
                "X-Ray"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems. By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only benchmarks the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches. The experimental results demonstrate that these traditional features, when fine-tuned and combined with machine learning models, can significantly improve the accuracy and reliability of tire defect detection, aiming to set a new standard in automated quality assurance in tire manufacturing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "7 pages, 2 figures, 3 tables, submitted to ICIP2024"
    },
    {
        "paper id": "2402.18541",
        "abstract url": "https://arxiv.org/abs/2402.18541",
        "title": "Dynamic Deterministic Constant-Approximate Distance Oracles with $n^\u03b5$ Worst-Case Update Time",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We present a new distance oracle in the fully dynamic setting: given a weighted undirected graph $G=(V,E)$ with $n$ vertices undergoing both edge insertions and deletions, and an arbitrary parameter $\u03b5$ where $\u03b5\\in[1/\\log^{c} n,1]$ and $c>0$ is a small constant, we can deterministically maintain a data structure with $n^\u03b5$ worst-case update time that, given any pair of vertices $(u,v)$, returns a $2^{{\\rm poly}(1/\u03b5)}$-approximate distance between $u$ and $v$ in ${\\rm poly}(1/\u03b5)\\log\\log n$ query time. Our algorithm significantly advances the state-of-the-art in two aspects, both for fully dynamic algorithms and even decremental algorithms. First, no existing algorithm with worst-case update time guarantees a $o(n)$-approximation while also achieving an $n^{2-\u03a9(1)}$ update and $n^{o(1)}$ query time, while our algorithm offers a constant $O_\u03b5(1)$-approximation with $n^\u03b5$ update time and $O_\u03b5(\\log \\log n)$ query time. Second, even if amortized update time is allowed, it is the first deterministic constant-approximation algorithm with $n^{1-\u03a9(1)}$ update and query time. The best result in this direction is the recent deterministic distance oracle by Chuzhoy and Zhang [STOC 2023] which achieves an approximation of $(\\log\\log n)^{2^{O(1/\u03b5^{3})}}$ with amortized update time of $n^\u03b5$ and query time of $2^{{\\rm poly}(1/\u03b5)}\\log n\\log\\log n$. We obtain the result by dynamizing tools related to length-constrained expanders [Haeupler-R\u00e4cke-Ghaffari, STOC 2022; Haeupler-Hershkowitz-Tan, 2023; Haeupler-Huebotter-Ghaffari, 2022]. Our technique completely bypasses the 40-year-old Even-Shiloach tree, which has remained the most pervasive tool in the area but is inherently amortized.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "137 pages"
    },
    {
        "paper id": "2402.18558",
        "abstract url": "https://arxiv.org/abs/2402.18558",
        "title": "Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks",
        "rating": -1,
        "keywords": [
            [
                "autonomous driving",
                "trajectory"
            ]
        ],
        "abstract": "The F1TENTH autonomous driving platform, consisting of 1:10-scale remote-controlled cars, has evolved into a well-established education and research platform. The many publications and real-world competitions span many domains, from classical path planning to novel learning-based algorithms. Consequently, the field is wide and disjointed, hindering direct comparison of developed methods and making it difficult to assess the state-of-the-art. Therefore, we aim to unify the field by surveying current approaches, describing common methods, and providing benchmark results to facilitate clear comparisons and establish a baseline for future work. This research aims to survey past and current work with F1TENTH vehicles in the classical and learning categories and explain the different solution approaches. We describe particle filter localisation, trajectory optimisation and tracking, model predictive contouring control, follow-the-gap, and end-to-end reinforcement learning. We provide an open-source evaluation of benchmark methods and investigate overlooked factors of control frequency and localisation accuracy for classical methods as well as reward signal and training map for learning methods. The evaluation shows that the optimisation and tracking method achieves the fastest lap times, followed by the online planning approach. Finally, our work identifies and outlines the relevant research aspects to help motivate future work in the F1TENTH domain.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "12 pages, 18 figures. Sumbitted for publication"
    },
    {
        "paper id": "2402.18566",
        "abstract url": "https://arxiv.org/abs/2402.18566",
        "title": "A Categorization of Complexity Classes for Information Retrieval and Synthesis Using Natural Logic",
        "rating": -1,
        "keywords": [
            [
                "Synthesis"
            ]
        ],
        "abstract": "Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. But, different kinds of questions have different answers, and different answers have different complexities. In this paper, we introduce a novel framework for analyzing the complexity of a question answer based on the natural deduction calculus as presented in Prawitz (1965). Our framework is novel both in that no one to our knowledge has used this logic as a basis for complexity classes, and also in that no other existing complexity classes to these have been delineated using any analogous methods either. We identify three decidable fragments in particular called the forward, query and planning fragments, and we compare this to what would be needed to do proofs for the complete first-order calculus, for which theorem-proving is long known to be undecidable.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18569",
        "abstract url": "https://arxiv.org/abs/2402.18569",
        "title": "Energy-Aware Heterogeneous Federated Learning via Approximate Systolic DNN Accelerators",
        "rating": -1,
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "In Federated Learning (FL), devices that participate in the training usually have heterogeneous resources, i.e., energy availability. In current deployments of FL, devices that do not fulfill certain hardware requirements are often dropped from the collaborative training. However, dropping devices in FL can degrade training accuracy and introduce bias or unfairness. Several works have tacked this problem on an algorithmic level, e.g., by letting constrained devices train a subset of the server neural network (NN) model. However, it has been observed that these techniques are not effective w.r.t. accuracy. Importantly, they make simplistic assumptions about devices' resources via indirect metrics such as multiply accumulate (MAC) operations or peak memory requirements. In this work, for the first time, we consider on-device accelerator design for FL with heterogeneous devices. We utilize compressed arithmetic formats and approximate computing, targeting to satisfy limited energy budgets. Using a hardware-aware energy model, we observe that, contrary to the state of the art's moderate energy reduction, our technique allows for lowering the energy requirements (by 4x) while maintaining higher accuracy.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18611",
        "abstract url": "https://arxiv.org/abs/2402.18611",
        "title": "HemaGraph: Breaking Barriers in Hematologic Single Cell Classification with Graph Attention",
        "rating": -1,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ]
        ],
        "abstract": "In the realm of hematologic cell populations classification, the intricate patterns within flow cytometry data necessitate advanced analytical tools. This paper presents 'HemaGraph', a novel framework based on Graph Attention Networks (GATs) for single-cell multi-class classification of hematological cells from flow cytometry data. Harnessing the power of GATs, our method captures subtle cell relationships, offering highly accurate patient profiling. Based on evaluation of data from 30 patients, HemaGraph demonstrates classification performance across five different cell classes, outperforming traditional methodologies and state-of-the-art methods. Moreover, the uniqueness of this framework lies in the training and testing phase of HemaGraph, where it has been applied for extremely large graphs, containing up to hundreds of thousands of nodes and two million edges, to detect low frequency cell populations (e.g. 0.01% for one population), with accuracies reaching 98%. Our findings underscore the potential of HemaGraph in improving hematoligic multi-class classification, paving the way for patient-personalized interventions. To the best of our knowledge, this is the first effort to use GATs, and Graph Neural Networks (GNNs) in general, to classify cell populations from single-cell flow cytometry data. We envision applying this method to single-cell data from larger cohort of patients and on other hematologic diseases.",
        "subjects": [
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18617",
        "abstract url": "https://arxiv.org/abs/2402.18617",
        "title": "ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Offline learning has become widely used due to its ability to derive effective policies from offline datasets gathered by expert demonstrators without interacting with the environment directly. Recent research has explored various ways to enhance offline learning efficiency by considering the characteristics (e.g., expertise level or multiple demonstrators) of the dataset. However, a different approach is necessary in the context of zero-sum games, where outcomes vary significantly based on the strategy of the opponent. In this study, we introduce a novel approach that uses unsupervised learning techniques to estimate the exploited level of each trajectory from the offline dataset of zero-sum games made by diverse demonstrators. Subsequently, we incorporate the estimated exploited level into the offline learning to maximize the influence of the dominant strategy. Our method enables interpretable exploited level estimation in multiple zero-sum games and effectively identifies dominant strategy data. Also, our exploited level augmented offline learning significantly enhances the original offline learning algorithms including imitation learning and offline reinforcement learning for zero-sum games.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "12 pages, 8 figures"
    },
    {
        "paper id": "2402.18649",
        "abstract url": "https://arxiv.org/abs/2402.18649",
        "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems",
        "rating": -1,
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4. Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user's chat history, all without the need to manipulate the user's input or gain direct access to OpenAI GPT4. Our demo is in the link: https://fzwark.github.io/LLM-System-Attack-Demo/",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18650",
        "abstract url": "https://arxiv.org/abs/2402.18650",
        "title": "The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps. Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials. The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation. It also collects data and swaps between multiple objects enabling robust dataset collection with no human intervention. We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort. In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps. The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM. The dataset includes ranges of grasps conducted across four objects and a variety of orientations. Manipulator states, object pose, video, and grasp success data are provided for every trial.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA2024)"
    },
    {
        "paper id": "2402.18651",
        "abstract url": "https://arxiv.org/abs/2402.18651",
        "title": "Quantifying Human Priors over Social and Navigation Networks",
        "rating": -1.0,
        "keywords": [
            [
                "Navigation"
            ],
            [
                "graph"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Human knowledge is largely implicit and relational -- do we have a friend in common? can I walk from here to there? In this work, we leverage the combinatorial structure of graphs to quantify human priors over such relational data. Our experiments focus on two domains that have been continuously relevant over evolutionary timescales: social interaction and spatial navigation. We find that some features of the inferred priors are remarkably consistent, such as the tendency for sparsity as a function of graph size. Other features are domain-specific, such as the propensity for triadic closure in social interactions. More broadly, our work demonstrates how nonclassical statistical analysis of indirect behavioral experiments can be used to efficiently model latent biases in the data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Published on Proceedings of the 40th International Conference on Machine Learning (ICML), PMLR 202:3063-3105, 2023"
    },
    {
        "paper id": "2402.18675",
        "abstract url": "https://arxiv.org/abs/2402.18675",
        "title": "Robot Body Schema Learning from Full-body Extero/Proprioception Sensors",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "For a robot, its body structure is an a-prior knowledge when it is designed. However, when such information is not available, can a robot recognize it by itself? In this paper, we aim to grant a robot such ability to learn its body structure from exteroception and proprioception data collected from on-body sensors. By a novel machine learning method, the robot can learn a binary Heterogeneous Dependency Matrix from its sensor readings. We showed such matrix is equivalent to a Heterogeneous out-tree structure which can uniquely represent the robot body topology. We explored the properties of such matrix and the out-tree, and proposed a remedy to fix them when they are contaminated by partial observability or data noise. We ran our algorithm on 6 different robots with different body structures in simulation and 1 real robot. Our algorithm correctly recognized their body structures with only on-body sensor readings but no topology prior knowledge.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18682",
        "abstract url": "https://arxiv.org/abs/2402.18682",
        "title": "Acoustic tactile sensing for mobile robot wheels",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Tactile sensing in mobile robots remains under-explored, mainly due to challenges related to sensor integration and the complexities of distributed sensing. In this work, we present a tactile sensing architecture for mobile robots based on wheel-mounted acoustic waveguides. Our sensor architecture enables tactile sensing along the entire circumference of a wheel with a single active component: an off-the-shelf acoustic rangefinder. We present findings showing that our sensor, mounted on the wheel of a mobile robot, is capable of discriminating between different terrains, detecting and classifying obstacles with different geometries, and performing collision detection via contact localization. We also present a comparison between our sensor and sensors traditionally used in mobile robots, and point to the potential for sensor fusion approaches that leverage the unique capabilities of our tactile sensing architecture. Our findings demonstrate that autonomous mobile robots can further leverage our sensor architecture for diverse mapping tasks requiring knowledge of terrain material, surface topology, and underlying structure.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "12 pages, 12 figures"
    },
    {
        "paper id": "2402.18699",
        "abstract url": "https://arxiv.org/abs/2402.18699",
        "title": "Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ]
        ],
        "abstract": "3D articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects.Point-level affordance, which predicts the per-point actionable score and thus proposes the best point to interact with, has demonstrated excellent performance and generalization capabilities in articulated object manipulation. However, a significant challenge remains: while previous works use perfect point cloud generated in simulation, the models cannot directly apply to the noisy point cloud in the real-world. To tackle this challenge, we leverage the property of real-world scanned point cloud that, the point cloud becomes less noisy when the camera is closer to the object. Therefore, we propose a novel coarse-to-fine affordance learning pipeline to mitigate the effect of point cloud noise in two stages. In the first stage, we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate. Then, we move the camera in front of the approximated place, scan a less noisy point cloud containing precise local geometries for manipulation, and learn affordance on such point cloud to propose fine-grained final actions. The proposed method is thoroughly evaluated both using large-scale simulated noisy point clouds mimicking real-world scans, and in the real world scenarios, with superiority over existing methods, demonstrating the effectiveness in tackling the noisy real-world point cloud problem.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "ICRA 2024"
    },
    {
        "paper id": "2402.18742",
        "abstract url": "https://arxiv.org/abs/2402.18742",
        "title": "Comparing Importance Sampling Based Methods for Mitigating the Effect of Class Imbalance",
        "rating": -1,
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Most state-of-the-art computer vision models heavily depend on data. However, many datasets exhibit extreme class imbalance which has been shown to negatively impact model performance. Among the training-time and data-generation solutions that have been explored, one subset that leverages existing data is importance sampling. A good deal of this work focuses primarily on the CIFAR-10 and CIFAR-100 datasets which fail to be representative of the scale, composition, and complexity of current state-of-the-art datasets. In this work, we explore and compare three techniques that derive from importance sampling: loss reweighting, undersampling, and oversampling. Specifically, we compare the effect of these techniques on the performance of two encoders on an impactful satellite imagery dataset, Planet's Amazon Rainforest dataset, in preparation for another work. Furthermore, we perform supplemental experimentation on a scene classification dataset, ADE20K, to test on a contrasting domain and clarify our results. Across both types of encoders, we find that up-weighting the loss for and undersampling has a negigible effect on the performance on underrepresented classes. Additionally, our results suggest oversampling generally improves performance for the same underrepresented classes. Interestingly, our findings also indicate that there may exist some redundancy in data in the Planet dataset. Our work aims to provide a foundation for further work on the Planet dataset and similar domain-specific datasets. We open-source our code at https://github.com/RichardZhu123/514-class-imbalance for future work on other satellite imagery datasets as well.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2402.18747",
        "abstract url": "https://arxiv.org/abs/2402.18747",
        "title": "Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains",
        "rating": -1,
        "keywords": [
            [
                "biomedical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18754",
        "abstract url": "https://arxiv.org/abs/2402.18754",
        "title": "Extending QGroundControl for Automated Mission Planning of UAVs",
        "rating": -1,
        "keywords": [
            [
                "Vehicle",
                "flight"
            ]
        ],
        "abstract": "Unmanned Aerial Vehicle (UAVs) have become very popular in the last decade due to some advantages such as strong terrain adaptation, low cost, zero casualties, and so on. One of the most interesting advances in this field is the automation of mission planning (task allocation) and real-time replanning, which are highly useful to increase the autonomy of the vehicle and reduce the operator workload. These automated mission planning and replanning systems require a Human Computer Interface (HCI) that facilitates the visualization and selection of plans that will be executed by the vehicles. In addition, most missions should be assessed before their real-life execution. This paper extends QGroundControl, an open-source simulation environment for flight control of multiple vehicles, by adding a mission designer that permits the operator to build complex missions with tasks and other scenario items; an interface for automated mission planning and replanning, which works as a test bed for different algorithms, and a Decision Support System (DSS) that helps the operator in the selection of the plan. In this work, a complete guide of these systems and some practical use cases are provided.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Preprint submitted and accepted in Sensors"
    },
    {
        "paper id": "2402.18780",
        "abstract url": "https://arxiv.org/abs/2402.18780",
        "title": "A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The development of generative models that create 3D content from a text prompt has made considerable strides thanks to the use of the score distillation sampling (SDS) method on pre-trained diffusion models for image generation. However, the SDS method is also the source of several artifacts, such as the Janus problem, the misalignment between the text prompt and the generated 3D model, and 3D model inaccuracies. While existing methods heavily rely on the qualitative assessment of these artifacts through visual inspection of a limited set of samples, in this work we propose more objective quantitative evaluation metrics, which we cross-validate via human ratings, and show analysis of the failure cases of the SDS technique. We demonstrate the effectiveness of this analysis by designing a novel computationally efficient baseline model that achieves state-of-the-art performance on the proposed metrics while addressing all the above-mentioned artifacts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18781",
        "abstract url": "https://arxiv.org/abs/2402.18781",
        "title": "Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Asymmetric information stochastic games (\\textsc{aisg}s) arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures. Existing computational methods for \\textsc{aisg}s are primarily offline and can not adapt to equilibrium deviations. Further, current methods are limited to special classes of \\textsc{aisg}s to avoid belief hierarchies. To address these limitations, we propose conjectural online learning (\\textsc{col}), an online method for generic \\textsc{aisg}s. \\textsc{col} uses a forecaster-actor-critic (\\textsc{fac}) architecture where subjective forecasts are used to conjecture the opponents' strategies within a lookahead horizon, and Bayesian learning is used to calibrate the conjectures. To adapt strategies to nonstationary environments, \\textsc{col} uses online rollout with cost function approximation (actor-critic). We prove that the conjectures produced by \\textsc{col} are asymptotically consistent with the information feedback in the sense of a relaxed Bayesian consistency. We also prove that the empirical strategy profile induced by \\textsc{col} converges to the Berk-Nash equilibrium, a solution concept characterizing rationality under subjectivity. Experimental results from an intrusion response use case demonstrate \\textsc{col}'s superiority over state-of-the-art reinforcement learning methods against nonstationary attacks.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00023",
        "abstract url": "https://arxiv.org/abs/2403.00023",
        "title": "Auditable Homomorphic-based Decentralized Collaborative AI with Attribute-based Differential Privacy",
        "rating": -1,
        "keywords": [
            [
                "federated learning"
            ]
        ],
        "abstract": "In recent years, the notion of federated learning (FL) has led to the new paradigm of distributed artificial intelligence (AI) with privacy preservation. However, most current FL systems suffer from data privacy issues due to the requirement of a trusted third party. Although some previous works introduce differential privacy to protect the data, however, it may also significantly deteriorate the model performance. To address these issues, we propose a novel decentralized collaborative AI framework, named Auditable Homomorphic-based Decentralised Collaborative AI (AerisAI), to improve security with homomorphic encryption and fine-grained differential privacy. Our proposed AerisAI directly aggregates the encrypted parameters with a blockchain-based smart contract to get rid of the need of a trusted third party. We also propose a brand-new concept for eliminating the negative impacts of differential privacy for model performance. Moreover, the proposed AerisAI also provides the broadcast-aware group key management based on ciphertext-policy attribute-based encryption (CPABE) to achieve fine-grained access control based on different service-level agreements. We provide a formal theoretical analysis of the proposed AerisAI as well as the functionality comparison with the other baselines. We also conduct extensive experiments on real datasets to evaluate the proposed approach. The experimental results indicate that our proposed AerisAI significantly outperforms the other state-of-the-art baselines.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "12 pages, 9 figures"
    },
    {
        "paper id": "2403.05578",
        "abstract url": "https://arxiv.org/abs/2403.05578",
        "title": "Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners",
        "rating": -1,
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ]
        ],
        "abstract": "Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-quality personalized banners for users.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2403.06993",
        "abstract url": "https://arxiv.org/abs/2403.06993",
        "title": "Automatic driving lane change safety prediction model based on LSTM",
        "rating": -1,
        "keywords": [
            [
                "Autonomous driving",
                "trajectory"
            ]
        ],
        "abstract": "Autonomous driving technology can improve traffic safety and reduce traffic accidents. In addition, it improves traffic flow, reduces congestion, saves energy and increases travel efficiency. In the relatively mature automatic driving technology, the automatic driving function is divided into several modules: perception, decision-making, planning and control, and a reasonable division of labor can improve the stability of the system. Therefore, autonomous vehicles need to have the ability to predict the trajectory of surrounding vehicles in order to make reasonable decision planning and safety measures to improve driving safety. By using deep learning method, a safety-sensitive deep learning model based on short term memory (LSTM) network is proposed. This model can alleviate the shortcomings of current automatic driving trajectory planning, and the output trajectory not only ensures high accuracy but also improves safety. The cell state simulation algorithm simulates the trackability of the trajectory generated by this model. The research results show that compared with the traditional model-based method, the trajectory prediction method based on LSTM network has obvious advantages in predicting the trajectory in the long time domain. The intention recognition module considering interactive information has higher prediction and accuracy, and the algorithm results show that the trajectory is very smooth based on the premise of safe prediction and efficient lane change. And autonomous vehicles can efficiently and safely complete lane changes.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18059",
        "abstract url": "https://arxiv.org/abs/2402.18059",
        "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
        "rating": -1.5,
        "keywords": [
            [
                "Watermarking"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 9 figures, 2 tables"
    },
    {
        "paper id": "2402.18146",
        "abstract url": "https://arxiv.org/abs/2402.18146",
        "title": "3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling",
        "rating": -1.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "synthesizing"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds. We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds. Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios. By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene. Furthermore, we developed a novel 3D scene flow data augmentation method for global and local motion. By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios. On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous supervised and unsupervised methods without requiring manual labelling. Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR2024! 10 pages, 6 figures"
    },
    {
        "paper id": "2402.18192",
        "abstract url": "https://arxiv.org/abs/2402.18192",
        "title": "Misalignment-Robust Frequency Distribution Loss for Image Transformation",
        "rating": -1.5,
        "keywords": [
            [
                "super-resolution"
            ],
            [
                "image enhancement"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments. However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data. To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain. Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT). Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function. Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain. Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions. Furthermore, we explore the potential of our FDL for image style transfer that relies solely on completely misaligned data. Our code is available at: https://github.com/eezkni/FDL",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to Computer Vision and Pattern Recognition Conference (CVPR) 2024"
    },
    {
        "paper id": "2402.18337",
        "abstract url": "https://arxiv.org/abs/2402.18337",
        "title": "Probabilistic Bayesian optimal experimental design using conditional normalizing flows",
        "rating": -1.5,
        "keywords": [
            [
                "MRI"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework. Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs. To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach. This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution. We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\\times$ 320) parameters at high image resolution, high-dimensional (640 $\\times$ 386) observations, and binary mask designs to select the most informative observations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18505",
        "abstract url": "https://arxiv.org/abs/2402.18505",
        "title": "Evolving machine learning workflows through interactive AutoML",
        "rating": -1.5,
        "keywords": [
            [
                "grammar"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Automatic workflow composition (AWC) is a relevant problem in automated machine learning (AutoML) that allows finding suitable sequences of preprocessing and prediction models together with their optimal hyperparameters. This problem can be solved using evolutionary algorithms and, in particular, grammar-guided genetic programming (G3P). Current G3P approaches to AWC define a fixed grammar that formally specifies how workflow elements can be combined and which algorithms can be included. In this paper we present \\ourmethod, an interactive G3P algorithm that allows users to dynamically modify the grammar to prune the search space and focus on their regions of interest. Our proposal is the first to combine the advantages of a G3P method with ideas from interactive optimisation and human-guided machine learning, an area little explored in the context of AutoML. To evaluate our approach, we present an experimental study in which 20 participants interact with \\ourmethod to evolve workflows according to their preferences. Our results confirm that the collaboration between \\ourmethod and humans allows us to find high-performance workflows in terms of accuracy that require less tuning time than those found without human intervention.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "40 pages, 5 figures, 3 tables. Paper submitted to journal. Under review"
    },
    {
        "paper id": "2402.18545",
        "abstract url": "https://arxiv.org/abs/2402.18545",
        "title": "Crowdsourcing Dermatology Images with Google Search Ads: Creating a Real-World Skin Condition Dataset",
        "rating": -1.5,
        "keywords": [
            [
                "medical",
                "Health",
                "diagnosis",
                "disease",
                "clinical"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Background: Health datasets from clinical sources do not reflect the breadth and diversity of disease in the real world, impacting research, medical education, and artificial intelligence (AI) tool development. Dermatology is a suitable area to develop and test a new and scalable method to create representative health datasets. Methods: We used Google Search advertisements to invite contributions to an open access dataset of images of dermatology conditions, demographic and symptom information. With informed contributor consent, we describe and release this dataset containing 10,408 images from 5,033 contributions from internet users in the United States over 8 months starting March 2023. The dataset includes dermatologist condition labels as well as estimated Fitzpatrick Skin Type (eFST) and Monk Skin Tone (eMST) labels for the images. Results: We received a median of 22 submissions/day (IQR 14-30). Female (66.72%) and younger (52% < age 40) contributors had a higher representation in the dataset compared to the US population, and 32.6% of contributors reported a non-White racial or ethnic identity. Over 97.5% of contributions were genuine images of skin conditions. Dermatologist confidence in assigning a differential diagnosis increased with the number of available variables, and showed a weaker correlation with image sharpness (Spearman's P values <0.001 and 0.01 respectively). Most contributions were short-duration (54% with onset < 7 days ago ) and 89% were allergic, infectious, or inflammatory conditions. eFST and eMST distributions reflected the geographical origin of the dataset. The dataset is available at github.com/google-research-datasets/scin . Conclusion: Search ads are effective at crowdsourcing images of health conditions. The SCIN dataset bridges important gaps in the availability of representative images of common skin conditions.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18563",
        "abstract url": "https://arxiv.org/abs/2402.18563",
        "title": "Approaching Human-Level Forecasting with Language Models",
        "rating": -1.5,
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18607",
        "abstract url": "https://arxiv.org/abs/2402.18607",
        "title": "Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective",
        "rating": -1.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined. In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execute fairness poisoning attacks to undermine the receiver's downstream models by manipulating the training data distribution of the diffusion model. Meanwhile, the receiver can perform property inference attacks to reveal the distribution of sensitive features in the sharer's dataset. Our experiments conducted on real-world datasets demonstrate remarkable attack performance on different types of diffusion models, which highlights the critical importance of robust data auditing and privacy protection protocols in pertinent applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18609",
        "abstract url": "https://arxiv.org/abs/2402.18609",
        "title": "ICE-SEARCH: A Language Model-Driven Feature Selection Approach",
        "rating": -1.5,
        "keywords": [
            [
                "Medical",
                "disease"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, which is among the first works that melds large language models (LLMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LLMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. The study emphasizes the critical role of incorporating domain-specific insights, illustrating ICE-SEARCH's robustness, generalizability, and convergence. This opens avenues for further research into comprehensive and intricate FS landscapes, marking a significant stride in the application of artificial intelligence in medical predictive analytics.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18743",
        "abstract url": "https://arxiv.org/abs/2402.18743",
        "title": "A revision on Multi-Criteria Decision Making methods for Multi-UAV Mission Planning Support",
        "rating": -1.5,
        "keywords": [
            [
                "UAV"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively used in many commercial applications due to their manageability and risk avoidance. One of the main problems considered is the Mission Planning for multiple UAVs, where a solution plan must be found satisfying the different constraints of the problem. This problem has multiple variables that must be optimized simultaneously, such as the makespan, the cost of the mission or the risk. Therefore, the problem has a lot of possible optimal solutions, and the operator must select the final solution to be executed among them. In order to reduce the workload of the operator in this decision process, a Decision Support System (DSS) becomes necessary. In this work, a DSS consisting of ranking and filtering systems, which order and reduce the optimal solutions, has been designed. With regard to the ranking system, a wide range of Multi-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are compared on a multi-UAV mission planning scenario, in order to study which method could fit better in a multi-UAV decision support system. Expert operators have evaluated the solutions returned, and the results show, on the one hand, that fuzzy methods generally achieve better average scores, and on the other, that all of the tested methods perform better when the preferences of the operators are biased towards a specific variable, and worse when their preferences are balanced. For the filtering system, a similarity function based on the proximity of the solutions has been designed, and on top of that, a threshold is tuned empirically to decide how to filter solutions without losing much of the hypervolume of the space of solutions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Preprint submitted and acepted in Expert Systems with Applications"
    },
    {
        "paper id": "2402.18751",
        "abstract url": "https://arxiv.org/abs/2402.18751",
        "title": "Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean",
        "rating": -1.5,
        "keywords": [
            [
                "biotic"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Soybean production is susceptible to biotic and abiotic stresses, exacerbated by extreme weather events. Water limiting stress, i.e. drought, emerges as a significant risk for soybean production, underscoring the need for advancements in stress monitoring for crop breeding and production. This project combines multi-modal information to identify the most effective and efficient automated methods to investigate drought response. We investigated a set of diverse soybean accessions using multiple sensors in a time series high-throughput phenotyping manner to: (1) develop a pipeline for rapid classification of soybean drought stress symptoms, and (2) investigate methods for early detection of drought stress. We utilized high-throughput time-series phenotyping using UAVs and sensors in conjunction with machine learning (ML) analytics, which offered a swift and efficient means of phenotyping. The red-edge and green bands were most effective to classify canopy wilting stress. The Red-Edge Chlorophyll Vegetation Index (RECI) successfully differentiated susceptible and tolerant soybean accessions prior to visual symptom development. We report pre-visual detection of soybean wilting using a combination of different vegetation indices. These results can contribute to early stress detection methodologies and rapid classification of drought responses in screening nurseries for breeding and production applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "25 pages, 5 figures"
    },
    {
        "paper id": "2402.18826",
        "abstract url": "https://arxiv.org/abs/2402.18826",
        "title": "The Machine Can't Replace the Human Heart",
        "rating": -1.5,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "What is the true heart of mental healthcare -- innovation or humanity? Can virtual therapy ever replicate the profound human bonds where healing arises? As artificial intelligence and immersive technologies promise expanded access, safeguards must ensure technologies remain supplementary tools guided by providers' wisdom. Implementation requires nuance balancing efficiency and empathy. If conscious of ethical risks, perhaps AI could restore humanity by automating tasks, giving providers more time to listen. Yet no algorithm can replicate the seat of dignity within. We must ask ourselves: What future has people at its core? One where AI thoughtfully plays a collaborative role? Or where pursuit of progress leaves vulnerability behind? This commentary argues for a balanced approach thoughtfully integrating technology while retaining care's irreplaceable human essence, at the heart of this profoundly human profession. Ultimately, by nurturing innovation and humanity together, perhaps we reach new heights of empathy previously unimaginable.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00026",
        "abstract url": "https://arxiv.org/abs/2403.00026",
        "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
        "rating": -1.5,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we present the Foundation Model for the Montreal Capacitated Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that approximates high-quality solutions to a variant of the Capacitated Vehicle Routing Problem (CVRP) that characterizes many real-world applications. The so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally described by Bengio et al. (2021), is defined on a fixed and finite graph, which is analogous to a city. Each MCVRP instance is essentially the sub-graph connecting a randomly sampled subset of the nodes in the fixed graph, which represent a set of potential addresses in a real-world delivery problem on a given day. Our work exploits this problem structure to frame the MCVRP as an analogous Natural Language Processing (NLP) task. Specifically, we leverage a Transformer architecture embedded in a Large Language Model (LLM) framework to train our model in a supervised manner on computationally inexpensive, sub-optimal MCVRP solutions obtained algorithmically. Through comprehensive computational experiments, we show that FM-MCVRP produces better MCVRP solutions than the training data and generalizes to larger sized problem instances not seen during training. Even when compared to near-optimal solutions from state-of-the-art heuristics, FM-MCVRP yields competitive results despite being trained on inferior data. For instance, for 400-customer problems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our results further demonstrate that unlike prior works in the literature, FM-MCVRP is a unified model, which performs consistently and reliably on a range of problem instance sizes and parameter values such as the vehicle capacity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00830",
        "abstract url": "https://arxiv.org/abs/2403.00830",
        "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
        "rating": -1.5,
        "keywords": [
            [
                "Medical",
                "healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models (LLMs) are revolutionizing various domains with their remarkable natural language processing (NLP) abilities. However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges. Another challenge lies in delivering medical assistance in remote areas with limited healthcare facilities and infrastructure. To address this, we introduce MedAide, an on-premise healthcare chatbot. It leverages tiny-LLMs integrated with LangChain, providing efficient edge-based preliminary medical diagnostics and support. MedAide employs model optimizations for minimal memory footprint and latency on embedded edge devices without server infrastructure. The training process is optimized using low-rank adaptation (LoRA). Additionally, the model is trained on diverse medical datasets, employing reinforcement learning from human feedback (RLHF) to enhance its domain-specific capabilities. The system is implemented on various consumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\% accuracy in medical consultations and scores 56 in USMLE benchmark, enabling an energy-efficient healthcare assistance platform that alleviates privacy concerns due to edge-based deployment, thereby empowering the community.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "7 pages, 11 figures, ACM conference paper, 33 references"
    },
    {
        "paper id": "2404.01306",
        "abstract url": "https://arxiv.org/abs/2404.01306",
        "title": "NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models",
        "rating": -1.5,
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite our sole objective not being optimizing performance. NeuroPrune is competitive with (or sometimes superior to) baselines on performance and can be up to $10$x faster in terms of training time for a given level of sparsity, simultaneously exhibiting measurable improvements in inference time in many cases.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18062",
        "abstract url": "https://arxiv.org/abs/2402.18062",
        "title": "Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities",
        "rating": -2,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "With recent advances in artificial intelligence (AI) and robotics, unmanned vehicle swarms have received great attention from both academia and industry due to their potential to provide services that are difficult and dangerous to perform by humans. However, learning and coordinating movements and actions for a large number of unmanned vehicles in complex and dynamic environments introduce significant challenges to conventional AI methods. Generative AI (GAI), with its capabilities in complex data feature extraction, transformation, and enhancement, offers great potential in solving these challenges of unmanned vehicle swarms. For that, this paper aims to provide a comprehensive survey on applications, challenges, and opportunities of GAI in unmanned vehicle swarms. Specifically, we first present an overview of unmanned vehicles and unmanned vehicle swarms as well as their use cases and existing issues. Then, an in-depth background of various GAI techniques together with their capabilities in enhancing unmanned vehicle swarms are provided. After that, we present a comprehensive review on the applications and challenges of GAI in unmanned vehicle swarms with various insights and discussions. Finally, we highlight open issues of GAI in unmanned vehicle swarms and discuss potential research directions.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "23 pages"
    },
    {
        "paper id": "2402.18084",
        "abstract url": "https://arxiv.org/abs/2402.18084",
        "title": "Spannotation: Enhancing Semantic Segmentation for Autonomous Navigation with Efficient Image Annotation",
        "rating": -2,
        "keywords": [
            [
                "Navigation"
            ],
            [
                "agricultural"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Spannotation is an open source user-friendly tool developed for image annotation for semantic segmentation specifically in autonomous navigation tasks. This study provides an evaluation of Spannotation, demonstrating its effectiveness in generating accurate segmentation masks for various environments like agricultural crop rows, off-road terrains and urban roads. Unlike other popular annotation tools that requires about 40 seconds to annotate an image for semantic segmentation in a typical navigation task, Spannotation achieves similar result in about 6.03 seconds. The tools utility was validated through the utilization of its generated masks to train a U-Net model which achieved a validation accuracy of 98.27% and mean Intersection Over Union (mIOU) of 96.66%. The accessibility, simple annotation process and no-cost features have all contributed to the adoption of Spannotation evident from its download count of 2098 (as of February 25, 2024) since its launch. Future enhancements of Spannotation aim to broaden its application to complex navigation scenarios and incorporate additional automation functionalities. Given its increasing popularity and promising potential, Spannotation stands as a valuable resource in autonomous navigation and semantic segmentation. For detailed information and access to Spannotation, readers are encouraged to visit the project's GitHub repository at https://github.com/sof-danny/spannotation",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures, 1 table, 1 pseudo code (algorithm), 55 references"
    },
    {
        "paper id": "2402.18099",
        "abstract url": "https://arxiv.org/abs/2402.18099",
        "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
        "rating": -2,
        "keywords": [
            [
                "Model editing"
            ],
            [
                "Medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge. To evaluate the editing impact, we build two benchmark datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18166",
        "abstract url": "https://arxiv.org/abs/2402.18166",
        "title": "Sequence-level Semantic Representation Fusion for Recommender Systems",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "With the rapid development of recommender systems, there is increasing side information that can be employed to improve the recommendation performance. Specially, we focus on the utilization of the associated \\emph{textual data} of items (eg product title) and study how text features can be effectively fused with ID features in sequential recommendation. However, there exists distinct data characteristics for the two kinds of item features, making a direct fusion method (eg adding text and ID embeddings as item representation) become less effective. To address this issue, we propose a novel {\\ul \\emph{Te}}xt-I{\\ul \\emph{D}} semantic fusion approach for sequential {\\ul \\emph{Rec}}ommendation, namely \\textbf{\\our}. The core idea of our approach is to conduct a sequence-level semantic fusion approach by better integrating global contexts. The key strategy lies in that we transform the text embeddings and ID embeddings by Fourier Transform from \\emph{time domain} to \\emph{frequency domain}. In the frequency domain, the global sequential characteristics of the original sequences are inherently aggregated into the transformed representations, so that we can employ simple multiplicative operations to effectively fuse the two kinds of item features. Our fusion approach can be proved to have the same effects of contextual convolution, so as to achieving sequence-level semantic fusion. In order to further improve the fusion performance, we propose to enhance the discriminability of the text embeddings from the text encoder, by adaptively injecting positional information via a mixture-of-experts~(MoE) modulation method. Our implementation is available at this repository: \\textcolor{magenta}{\\url{https://github.com/RUCAIBox/TedRec}}.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2402.18181",
        "abstract url": "https://arxiv.org/abs/2402.18181",
        "title": "CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "dehazing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature distillation (CFD). This strategy combines feature distillation from merged clean-fog features with contrastive learning, ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18205",
        "abstract url": "https://arxiv.org/abs/2402.18205",
        "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
        "rating": -2,
        "keywords": [
            [
                "diagnosis"
            ]
        ],
        "abstract": "Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \\textbf{L}og parsing framework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method for large language models (LLMs). LLMs exhibit exceptional semantic comprehension, deftly distinguishing between parameters and invariant tokens. We have conducted experiments on large-scale public datasets. Extensive evaluation demonstrates that Lemur achieves the state-of-the-art performance and impressive efficiency.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "7 pages"
    },
    {
        "paper id": "2402.18227",
        "abstract url": "https://arxiv.org/abs/2402.18227",
        "title": "Potentials of Green Coding -- Findings and Recommendations for Industry, Education and Science -- Extended Paper",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Progressing digitalization and increasing demand and use of software cause rises in energy- and resource consumption from information and communication technologies (ICT). This raises the issue of sustainability in ICT, which increasingly includes the sustainability of the software products themselves and the art of creating sustainable software. To this end, we conducted an analysis to gather and present existing literature on three research questions relating to the production of ecologically sustainable software (\"Green Coding\") and to provide orientation for stakeholders approaching the subject. We compile the approaches to Green Coding and Green Software Engineering (GSE) that have been published since 2010. Furthermore, we considered ways to integrate the findings into existing industrial processes and higher education curricula to influence future development in an environmentally friendly way.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "This document is the extended version of the literature report published at the German Informatik Conference titled \"Potentials of Green Coding - Findings and Recommendations for Industry, Education and Science\". This document has since been updated and expanded to include relevant sources"
    },
    {
        "paper id": "2402.18240",
        "abstract url": "https://arxiv.org/abs/2402.18240",
        "title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18319",
        "abstract url": "https://arxiv.org/abs/2402.18319",
        "title": "A Multimodal Handover Failure Detection Dataset and Baselines",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at ICRA 2024"
    },
    {
        "paper id": "2402.18354",
        "abstract url": "https://arxiv.org/abs/2402.18354",
        "title": "SuperdropNet: a Stable and Accurate Machine Learning Proxy for Droplet-based Cloud Microphysics",
        "rating": -2,
        "keywords": [
            [
                "forecast"
            ]
        ],
        "abstract": "Cloud microphysics has important consequences for climate and weather phenomena, and inaccurate representations can limit forecast accuracy. While atmospheric models increasingly resolve storms and clouds, the accuracy of the underlying microphysics remains limited by computationally expedient bulk moment schemes based on simplifying assumptions. Droplet-based Lagrangian schemes are more accurate but are underutilized due to their large computational overhead. Machine learning (ML) based schemes can bridge this gap by learning from vast droplet-based simulation datasets, but have so far struggled to match the accuracy and stability of bulk moment schemes. To address this challenge, we developed SuperdropNet, an ML-based emulator of the Lagrangian superdroplet simulations. To improve accuracy and stability, we employ multi-step autoregressive prediction during training, impose physical constraints, and carefully control stochasticity in the training data. Superdropnet predicted hydrometeor states and cloud-to-rain transition times more accurately than previous ML emulators, and matched or outperformed bulk moment schemes in many cases. We further carried out detailed analyses to reveal how multistep autoregressive training improves performance, and how the performance of SuperdropNet and other microphysical schemes hydrometeors' mass, number and size distribution. Together our results suggest that ML models can effectively emulate cloud microphysics, in a manner consistent with droplet-based simulations.",
        "subjects": [
            "physics.ao-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18412",
        "abstract url": "https://arxiv.org/abs/2402.18412",
        "title": "QAOA with random and subgraph phase operators",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "The quantum approximate optimization algorithm (QAOA) is a promising quantum algorithm that can be used to approximately solve combinatorial optimization problems. The usual QAOA ansatz consists of an alternating application of the cost and mixer Hamiltonians. In this work, we study how using Hamiltonians other than the usual cost Hamiltonian, dubbed custom phase operators, can affect the performance of QAOA. We derive an expected value formula for QAOA with custom phase operators at p = 1 and show numerically that some of these custom phase operators can achieve higher approximation ratio than the original algorithm implementation. Out of all the graphs tested, 0.036\\% of the random custom phase operators, 75.9\\% of the subgraph custom phase operators, 95.1\\% of the triangle-removed custom phase operators, and 93.9\\% of the maximal degree edge-removed custom phase operators have a higher approximation ratio than the original QAOA implementation. This finding opens up the question of whether better phase operators can be designed to further improve the performance of QAOA.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18414",
        "abstract url": "https://arxiv.org/abs/2402.18414",
        "title": "Physics-based block preconditioning for mixed-dimensional beam-solid interaction",
        "rating": -2,
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "This paper presents a scalable physics-based block preconditioner for mixed-dimensional models in beam-solid interaction and their application in engineering. In particular, it studies the linear systems arising from a regularized mortar-type approach for embedding geometrically exact beams into solid continua. Due to the lack of block diagonal dominance of the arising 2 x 2 block system, an approximate block factorization preconditioner is used. It exploits the sparsity structure of the beam sub-block to construct a sparse approximate inverse, which is then not only used to explicitly form an approximation of the Schur complement, but also acts as a smoother within the prediction step of the arising SIMPLE-type preconditioner. The correction step utilizes an algebraic multigrid method. Although, for now, the beam sub-block is tackled by a one-level method only, the multi-level nature of the computationally demanding correction step delivers a scalable preconditioner in practice. In numerical test cases, the influence of different algorithmic parameters on the quality of the sparse approximate inverse is studied and the weak scaling behavior of the proposed preconditioner on up to 1000 MPI ranks is demonstrated, before the proposed preconditioner is finally applied for the analysis of steel-reinforced concrete structures in civil engineering.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18420",
        "abstract url": "https://arxiv.org/abs/2402.18420",
        "title": "CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "GNN",
                "graph"
            ]
        ],
        "abstract": "The Cable-Driven Parallel Robots (CDPRs) have gained significant attention due to their high payload capacity and large workspace. When deploying CDPRs in practice, one of the challenges is kinematic modeling. Unlike serial mechanisms, CDPRs have a simple inverse kinematics problem but a complex forward kinematics (FK) issue. Therefore, the development of accurate and efficient FK solvers has been a prominent research focus in CDPR applications. By observing the topology within CDPRs, in this paper, we propose a graph-based representation to model CDPRs and introduce CafkNet, a fast and general FK solver, leveraging Graph Neural Network (GNN). CafkNet is extensively tested on 3D and 2D CDPRs in different configurations, both in simulators and real scenarios. The results demonstrate its ability to learn CDPRs' internal topology and accurately solve the FK problem. Then, the zero-shot generalization from one configuration to another is validated. Also, the sim2real gap can be bridged by CafkNet using both simulation and real-world data. To the best of our knowledge, it is the first study that employs the GNN to solve FK problem for CDPRs.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "To the best of authors' knowledge, it is the first study to employ the GNN for the FK problem of CDPRs. First two authors have equal contribution. Videos and codes are available at https://sites.google.com/view/cafknet/site"
    },
    {
        "paper id": "2402.18445",
        "abstract url": "https://arxiv.org/abs/2402.18445",
        "title": "HyperFedNet: Communication-Efficient Personalized Federated Learning Via Hypernetwork",
        "rating": -2,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "In response to the challenges posed by non-independent and identically distributed (non-IID) data and the escalating threat of privacy attacks in Federated Learning (FL), we introduce HyperFedNet (HFN), a novel architecture that incorporates hypernetworks to revolutionize parameter aggregation and transmission in FL. Traditional FL approaches, characterized by the transmission of extensive parameters, not only incur significant communication overhead but also present vulnerabilities to privacy breaches through gradient analysis. HFN addresses these issues by transmitting a concise set of hypernetwork parameters, thereby reducing communication costs and enhancing privacy protection. Upon deployment, the HFN algorithm enables the dynamic generation of parameters for the basic layer of the FL main network, utilizing local database features quantified by embedding vectors as input. Through extensive experimentation, HFN demonstrates superior performance in reducing communication overhead and improving model accuracy compared to conventional FL methods. By integrating the HFN algorithm into the FL framework, HFN offers a solution to the challenges of non-IID data and privacy threats.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18451",
        "abstract url": "https://arxiv.org/abs/2402.18451",
        "title": "MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation",
        "rating": -2,
        "keywords": [
            [
                "GAN"
            ],
            [
                "Medical",
                "MRI"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The recent Mamba model has shown remarkable adaptability for visual representation learning, including in medical imaging tasks. This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model. The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation. Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods. Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality. The code is publicly available at https://github.com/ayanglab/MambaMIR.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18469",
        "abstract url": "https://arxiv.org/abs/2402.18469",
        "title": "Interval-Constrained Bipartite Matching over Time",
        "rating": -2,
        "keywords": [
            [
                "medical"
            ]
        ],
        "abstract": "Interval-constrained online bipartite matching problem frequently occurs in medical appointment scheduling: unit-time jobs representing patients arrive online and are assigned to a time slot within their given time interval. We consider a variant of this problem where reassignments are allowed and extend it by a notion of current time, which is decoupled from the job arrival events. As jobs appear, the current point in time gradually advances. Jobs that are assigned to the current time unit become processed, which fixes part of the matching and disables these jobs or slots for reassignments in future steps. We refer to these time-dependent restrictions on reassignments as the over-time property. We show that FirstFit with reassignments according to the shortest augmenting path rule is $\\frac{2}{3}$-competitive with respect to the matching cardinality, and that the bound is tight. Interestingly, this bound holds even if the number of reassignments per job is bound by a constant. For the number of reassignments performed by the algorithm, we show that it is in $\u03a9(n \\log n)$ in the worst case, where $n$ is the number of patients or jobs on the online side. This result is in line with lower bounds for the number of reassignments in online bipartite matching with reassignments, and, similarly to this previous work, we also conjecture that this bound should be tight. Known upper bounds like the $O(n \\log^2 n)$ for online bipartite matching with reassignments by Bernstein, Holm, and Rotenberg do not transfer directly: while our interval constraints simplify the problem, the over-time property restricts the set of possible reassignments.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18472",
        "abstract url": "https://arxiv.org/abs/2402.18472",
        "title": "Implementing Online Reinforcement Learning with Clustering Neural Networks",
        "rating": -2,
        "keywords": [
            [
                "biologically"
            ]
        ],
        "abstract": "An agent employing reinforcement learning takes inputs (state variables) from an environment and performs actions that affect the environment in order to achieve some objective. Rewards (positive or negative) guide the agent toward improved future actions. This paper builds on prior clustering neural network research by constructing an agent with biologically plausible neo-Hebbian three-factor synaptic learning rules, with a reward signal as the third factor (in addition to pre- and post-synaptic spikes). The classic cart-pole problem (balancing an inverted pendulum) is used as a running example throughout the exposition. Simulation results demonstrate the efficacy of the approach, and the proposed method may eventually serve as a low-level component of a more general method.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18489",
        "abstract url": "https://arxiv.org/abs/2402.18489",
        "title": "FAST functional connectivity implicates P300 connectivity in working memory deficits in Alzheimer's disease",
        "rating": -2,
        "keywords": [
            [
                "EEG",
                "disease",
                "clinical"
            ]
        ],
        "abstract": "Measuring transient functional connectivity is an important challenge in Electroencephalogram (EEG) research. Here, the rich potential for insightful, discriminative information of brain activity offered by high temporal resolution is confounded by the inherent noise of the medium and the spurious nature of correlations computed over short temporal windows. We propose a novel methodology to overcome these problems called Filter Average Short-Term (FAST) functional connectivity. First, long-term, stable, functional connectivity is averaged across an entire study cohort for a given pair of Visual Short Term Memory (VSTM) tasks. The resulting average connectivity matrix, containing information on the strongest general connections for the tasks, is used as a filter to analyse the transient high temporal resolution functional connectivity of individual subjects. In simulations, we show that this method accurately discriminates differences in noisy Event-Related Potentials (ERPs) between two conditions where standard connectivity and other comparable methods fail. We then apply this to analyse activity related to visual short-term memory binding deficits in two cohorts of familial and sporadic Alzheimer's disease. Reproducible significant differences were found in the binding task with no significant difference in the shape task in the P300 ERP range. This allows new sensitive measurements of transient functional connectivity, which can be implemented to obtain results of clinical significance.",
        "subjects": [
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18615",
        "abstract url": "https://arxiv.org/abs/2402.18615",
        "title": "Unsupervised Airway Tree Clustering with Deep Learning: The Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "biological",
                "health",
                "CT",
                "disease",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "High-resolution full lung CT scans now enable the detailed segmentation of airway trees up to the 6th branching generation. The airway binary masks display very complex tree structures that may encode biological information relevant to disease risk and yet remain challenging to exploit via traditional methods such as meshing or skeletonization. Recent clinical studies suggest that some variations in shape patterns and caliber of the human airway tree are highly associated with adverse health outcomes, including all-cause mortality and incident COPD. However, quantitative characterization of variations observed on CT segmented airway tree remain incomplete, as does our understanding of the clinical and developmental implications of such. In this work, we present an unsupervised deep-learning pipeline for feature extraction and clustering of human airway trees, learned directly from projections of 3D airway segmentations. We identify four reproducible and clinically distinct airway sub-types in the MESA Lung CT cohort.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Accepted to appear in Proceedings of International Symposium on Biomedical Imaging (ISBI), 2024"
    },
    {
        "paper id": "2402.18618",
        "abstract url": "https://arxiv.org/abs/2402.18618",
        "title": "Urban Green Index estimation based on data collected by remote sensing for Romanian cities",
        "rating": -2,
        "keywords": [
            [
                "remote sensing",
                "satellite"
            ]
        ],
        "abstract": "The modernization of offi cial statistics involves the use of new data sources, such as data collected through remote sensing. The document contains a description of how an urban green index, derived from the SDG 11.7 objective, was obtained for Romania's 41 county seat cities based on free data sets collected by remote sensing from the European and North American space agencies. The main result is represented by an estimate of the areas of surfaces covered with vegetation for the 40 county seat towns and the municipality of Bucharest, relative to the total surface. To estimate the area covered with vegetation, we used two data sets obtained by remote sensing, namely data provided by the MODIS mission, the TERRA satellite, and data provided by the Sentinel 2 mission from the Copernicus space program. Based on the results obtained, namely the surface area covered with vegetation, estimated in square kilometers, and the percentage of the total surface area or urban green index, we have created a national top of the county seat cities",
        "subjects": [
            "cs.OH"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18684",
        "abstract url": "https://arxiv.org/abs/2402.18684",
        "title": "Quantum State Compression with Polar Codes",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "In the quantum compression scheme proposed by Schumacher, Alice compresses a message that Bob decompresses. In that approach, there is some probability of failure and, even when successful, some distortion of the state. For sufficiently large blocklengths, both of these imperfections can be made arbitrarily small while achieving a compression rate that asymptotically approaches the source coding bound. However, direct implementation of Schumacher compression suffers from poor circuit complexity. In this paper, we consider a slightly different approach based on classical syndrome source coding. The idea is to use a linear error-correcting code and treat the message to be compressed as an error pattern. If the message is a correctable error (i.e., a coset leader) then Alice can use the error-correcting code to convert her message to a corresponding quantum syndrome. An implementation of this based on polar codes is described and simulated. As in classical source coding based on polar codes, Alice maps the information into the ``frozen\" qubits that constitute the syndrome. To decompress, Bob utilizes a quantum version of successive cancellation coding.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "Extended Version of ISIT 2024 Submission"
    },
    {
        "paper id": "2402.18703",
        "abstract url": "https://arxiv.org/abs/2402.18703",
        "title": "Zero-error communication, scrambling, and ergodicity",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "The long term behaviour of a quantum channel under iterations (i.e. under repeated applications of itself) yields a plethora of interesting properties. These include ergodicity, mixing, eventual scrambling, becoming strictly positive, and the vanishing of its one-shot zero error capacities. We derive relations between these seemingly different properties and find novel bounds on indices which quantify the minimum number of iterations needed for the onset of some of these properties. We obtain a lower bound on the one-shot zero-error classical capacity of $n$ iterations of an ergodic channel (for any positive integer $n$) in terms of the cardinality of its peripheral spectrum. We also find upper bounds on the minimum number of iterations needed for the one-shot capacities of any channel to stabilize. We consider two classes of quantum channels, satisfying certain symmetries, for which upper bounds on the above indices are optimal, since they reduce to the corresponding indices for a stochastic matrix (for which the bounds are known to be optimal). As an auxiliary result, we obtain a trade-off relation between the one-shot zero error classical and quantum capacities of a quantum channel.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "Preliminary version. Comments are welcome"
    },
    {
        "paper id": "2402.18718",
        "abstract url": "https://arxiv.org/abs/2402.18718",
        "title": "Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks",
        "rating": -2,
        "keywords": [
            [
                "Attack"
            ],
            [
                "biometric"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Backdoor attacks allow an attacker to embed a specific vulnerability in a machine learning algorithm, activated when an attacker-chosen pattern is presented, causing a specific misprediction. The need to identify backdoors in biometric scenarios has led us to propose a novel technique with different trade-offs. In this paper we propose to use model pairs on open-set classification tasks for detecting backdoors. Using a simple linear operation to project embeddings from a probe model's embedding space to a reference model's embedding space, we can compare both embeddings and compute a similarity score. We show that this score, can be an indicator for the presence of a backdoor despite models being of different architectures, having been trained independently and on different datasets. Additionally, we show that backdoors can be detected even when both models are backdoored. The source code is made available for reproducibility purposes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2402.18729",
        "abstract url": "https://arxiv.org/abs/2402.18729",
        "title": "A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks",
        "rating": -2,
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data available from direct numerical simulation (DNS) create opportunities to leverage data-driven modeling techniques. Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen. Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and out-of-distribution regimes. In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model. In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames. We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models. We also propose a method for the incorporation of out-of-distribution information in a BNN. The efficacy of the model is demonstrated by a priori evaluation on a dataset consisting of a variety of flame conditions and fuels.",
        "subjects": [
            "physics.flu-dyn"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18753",
        "abstract url": "https://arxiv.org/abs/2402.18753",
        "title": "Like-minded, like-bodied: How users (18-26) trust online eating and health information",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "This paper investigates the relationship between social media and eating practices amongst 42 internet users aged 18-26. We conducted an ethnography in the US and India to observe how they navigated eating and health information online. We found that participants portrayed themselves online through a vocabulary we have labeled \"the good life\": performing holistic health by displaying a socially-ideal body. In doing so, participants unconsciously engaged in behaviors of disordered eating while actively eschewing them. They also valued personal testimonies, and readily tested tips from content creators who shared similar beliefs and bodies to them. In doing so, they discarded probabilistic thinking and opened themselves to harm. Our study found that their social media feeds did not unidirectionally influence participants - they also reflected participants' internalized views of health, in an intertwined, non-linear journey. Reducing the online spread of disordered eating practices requires addressing it within young people's social context.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2402.18755",
        "abstract url": "https://arxiv.org/abs/2402.18755",
        "title": "On Defeating Graph Analysis of Anonymous Transactions",
        "rating": -2,
        "keywords": [
            [
                "Graph"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "In a ring-signature-based anonymous cryptocurrency, signers of a transaction are hidden among a set of potential signers, called a ring, whose size is much smaller than the number of all users. The ring-membership relations specified by the sets of transactions thus induce bipartite transaction graphs, whose distribution is in turn induced by the ring sampler underlying the cryptocurrency. Since efficient graph analysis could be performed on transaction graphs to potentially deanonymise signers, it is crucial to understand the resistance of (the transaction graphs induced by) a ring sampler against graph analysis. Of particular interest is the class of partitioning ring samplers. Although previous works showed that they provide almost optimal local anonymity, their resistance against global, e.g. graph-based, attacks were unclear. In this work, we analyse transaction graphs induced by partitioning ring samplers. Specifically, we show (partly analytically and partly empirically) that, somewhat surprisingly, by setting the ring size to be at least logarithmic in the number of users, a graph-analysing adversary is no better than the one that performs random guessing in deanonymisation up to constant factor of 2.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18774",
        "abstract url": "https://arxiv.org/abs/2402.18774",
        "title": "The Situate AI Guidebook: Co-Designing a Toolkit to Support Multi-Stakeholder Early-stage Deliberations Around Public Sector AI Proposals",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Public sector agencies are rapidly deploying AI systems to augment or automate critical decisions in real-world contexts like child welfare, criminal justice, and public health. A growing body of work documents how these AI systems often fail to improve services in practice. These failures can often be traced to decisions made during the early stages of AI ideation and design, such as problem formulation. However, today, we lack systematic processes to support effective, early-stage decision-making about whether and under what conditions to move forward with a proposed AI project. To understand how to scaffold such processes in real-world settings, we worked with public sector agency leaders, AI developers, frontline workers, and community advocates across four public sector agencies and three community advocacy groups in the United States. Through an iterative co-design process, we created the Situate AI Guidebook: a structured process centered around a set of deliberation questions to scaffold conversations around (1) goals and intended use or a proposed AI system, (2) societal and legal considerations, (3) data and modeling constraints, and (4) organizational governance factors. We discuss how the guidebook's design is informed by participants' challenges, needs, and desires for improved deliberation processes. We further elaborate on implications for designing responsible AI toolkits in collaboration with public sector agency stakeholders and opportunities for future work to expand upon the guidebook. This design approach can be more broadly adopted to support the co-creation of responsible AI toolkits that scaffold key decision-making processes surrounding the use of AI in the public sector and beyond.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18778",
        "abstract url": "https://arxiv.org/abs/2402.18778",
        "title": "X-ResQ: Reverse Annealing for Quantum MIMO Detection with Flexible Parallelism",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum Annealing (QA)-accelerated MIMO detection is an emerging research approach in the context of NextG wireless networks. The opportunity is to enable large MIMO systems and thus improve wireless performance. The approach aims to leverage QA to expedite the computation required for theoretically optimal but computationally-demanding Maximum Likelihood detection to overcome the limitations of the currently deployed linear detectors. This paper presents X-ResQ, a QA-based MIMO detector system featuring fine-grained quantum task parallelism that is uniquely enabled by the Reverse Annealing (RA) protocol. Unlike prior designs, X-ResQ has many desirable system properties for a parallel QA detector and has effectively improved detection performance as more qubits are assigned. In our evaluations on a state-of-the-art quantum annealer, fully parallel X-ResQ achieves near-optimal throughput (over 10 bits/s/Hz) for $4\\times6$ MIMO with 16-QAM using six levels of parallelism with 240 qubits and $220~\u03bc$s QA compute time, achieving 2.5--5$\\times$ gains compared against other tested detectors. For more comprehensive evaluations, we implement and evaluate X-ResQ in the non-quantum digital setting. This non-quantum X-ResQ demonstration showcases the potential to realize ultra-large $1024\\times1024$ MIMO, significantly outperforming other MIMO detectors, including the state-of-the-art RA detector classically implemented in the same way.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2402.18790",
        "abstract url": "https://arxiv.org/abs/2402.18790",
        "title": "The Power of Unentangled Quantum Proofs with Non-negative Amplitudes",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum entanglement is a fundamental property of quantum mechanics and plays a crucial role in quantum computation and information. We study entanglement via the lens of computational complexity by considering quantum generalizations of the class NP with multiple unentangled quantum proofs, the so-called QMA(2) and its variants. The complexity of QMA(2) is a longstanding open problem, and only the trivial bounds QMA $\\subseteq$ QMA(2) $\\subseteq$ NEXP are known. In this work, we study the power of unentangled quantum proofs with non-negative amplitudes, a class which we denote $\\text{QMA}^+(2)$. In this setting, we are able to design proof verification protocols for problems both using logarithmic size quantum proofs and having a constant probability gap in distinguishing yes from no instances. In particular, we design global protocols for small set expansion, unique games, and PCP verification. As a consequence, we obtain NP $\\subseteq \\text{QMA}^+_{\\log}(2)$ with a constant gap. By virtue of the new constant gap, we are able to ``scale up'' this result to $\\text{QMA}^+(2)$, obtaining the full characterization $\\text{QMA}^+(2)$=NEXP by establishing stronger explicitness properties of the PCP for NEXP. One key novelty of these protocols is the manipulation of quantum proofs in a global and coherent way yielding constant gaps. Previous protocols (only available for general amplitudes) are either local having vanishingly small gaps or treat the quantum proofs as classical probability distributions requiring polynomially many proofs thereby not implying non-trivial bounds on QMA(2). Finally, we show that QMA(2) is equal to $\\text{QMA}^+(2)$ provided the gap of the latter is a sufficiently large constant. In particular, if $\\text{QMA}^+(2)$ admits gap amplification, then QMA(2)=NEXP.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "64 pages"
    },
    {
        "paper id": "2402.18796",
        "abstract url": "https://arxiv.org/abs/2402.18796",
        "title": "MOSAIC: A Modular System for Assistive and Interactive Cooking",
        "rating": -2,
        "keywords": [
            [
                "forecasting"
            ]
        ],
        "abstract": "We present MOSAIC, a modular architecture for home robots to perform complex collaborative tasks, such as cooking with everyday users. MOSAIC tightly collaborates with humans, interacts with users using natural language, coordinates multiple robots, and manages an open vocabulary of everyday objects. At its core, MOSAIC employs modularity: it leverages multiple large-scale pre-trained models for general tasks like language and image recognition, while using streamlined modules designed for task-specific control. We extensively evaluate MOSAIC on 60 end-to-end trials where two robots collaborate with a human user to cook a combination of 6 recipes. We also extensively test individual modules with 180 episodes of visuomotor picking, 60 episodes of human motion forecasting, and 46 online user evaluations of the task planner. We show that MOSAIC is able to efficiently collaborate with humans by running the overall system end-to-end with a real human user, completing 68.3% (41/60) collaborative cooking trials of 6 different recipes with a subtask completion rate of 91.6%. Finally, we discuss the limitations of the current system and exciting open challenges in this domain. The project's website is at https://portal-cornell.github.io/MOSAIC/",
        "subjects": [
            "cs.RO"
        ],
        "comment": "22 pages, 13 figures"
    },
    {
        "paper id": "2402.18844",
        "abstract url": "https://arxiv.org/abs/2402.18844",
        "title": "Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D human pose estimation and mesh recovery have attracted widespread research interest in many areas, such as computer vision, autonomous driving, and robotics. Deep learning on 3D human pose estimation and mesh recovery has recently thrived, with numerous methods proposed to address different problems in this area. In this paper, to stimulate future research, we present a comprehensive review of recent progress over the past five years in deep learning methods for this area by delving into over 200 references. To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations. We also present comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions. A regularly updated project page can be found at https://github.com/liuyangme/SOTA-3DHPE-HMR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.05577",
        "abstract url": "https://arxiv.org/abs/2403.05577",
        "title": "Stress Monitoring Using Low-Cost Electroencephalogram Devices: A Systematic Literature Review",
        "rating": -2,
        "keywords": [
            [
                "biomarker",
                "health",
                "EEG"
            ]
        ],
        "abstract": "Introduction. Low-cost health monitoring devices are increasingly being used for mental health related studies including stress. While cortisol response magnitude remains the gold standard indicator for stress assessment, a growing number of studies have started to use low-cost EEG devices as primary recorders of biomarker data. Methods. This study reviews published works contributing and/or using EEG devices for detecting stress and their associated machine learning methods. The reviewed works are selected to answer three general research questions and are then synthesized into four categories of stress assessment using EEG, low-cost EEG devices, available datasets for EEG-based stress measurement, and machine learning techniques for EEG-based stress measurement. Results. A number of studies were identified where low-cost EEG devices were utilized to record brain function during phases of stress and relaxation. These studies generally reported a high predictive accuracy rate, verified using a number of different machine learning validation methods and statistical approaches. Of these studies, 60% can be considered low-powered studies based on the small number of test subjects used during experimentation. Conclusion. Low-cost consumer grade wearable devices including EEG and wrist-based monitors are increasingly being used in stress-related studies. Standardization of EEG signal processing and importance of sensor location still requires further study, and research in this area will continue to provide improvements as more studies become available.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "53 pages, 8 figures"
    },
    {
        "paper id": "2403.05579",
        "abstract url": "https://arxiv.org/abs/2403.05579",
        "title": "Cultural Bias in Explainable AI Research: A Systematic Analysis",
        "rating": -2,
        "keywords": [
            [
                "psychological"
            ]
        ],
        "abstract": "For synergistic interactions between humans and artificial intelligence (AI) systems, AI outputs often need to be explainable to people. Explainable AI (XAI) systems are commonly tested in human user studies. However, whether XAI researchers consider potential cultural differences in human explanatory needs remains unexplored. We highlight psychological research that found significant differences in human explanations between many people from Western, commonly individualist countries and people from non-Western, often collectivist countries. We argue that XAI research currently overlooks these variations and that many popular XAI designs implicitly and problematically assume that Western explanatory needs are shared cross-culturally. Additionally, we systematically reviewed over 200 XAI user studies and found that most studies did not consider relevant cultural variations, sampled only Western populations, but drew conclusions about human-XAI interactions more generally. We also analyzed over 30 literature reviews of XAI studies. Most reviews did not mention cultural differences in explanatory needs or flag overly broad cross-cultural extrapolations of XAI user study results. Combined, our analyses provide evidence of a cultural bias toward Western populations in XAI research, highlighting an important knowledge gap regarding how culturally diverse users may respond to widely used XAI systems that future work can and should address.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.07922",
        "abstract url": "https://arxiv.org/abs/2403.07922",
        "title": "Redefining Aerial Innovation: Autonomous Tethered Drones as a Solution to Battery Life and Data Latency Challenges",
        "rating": -2,
        "keywords": [
            [
                "UAV",
                "drone"
            ]
        ],
        "abstract": "The emergence of tethered drones represents a major advancement in unmanned aerial vehicles (UAVs) offering solutions to key limitations faced by traditional drones. This article explores the potential of tethered drones with a particular focus on their ability to tackle issues related to battery life constraints and data latency commonly experienced by battery operated drones. Through their connection to a ground station via a tether, autonomous tethered drones provide continuous power supply and a secure direct data transmission link facilitating prolonged operational durations and real time data transfer. These attributes significantly enhance the effectiveness and dependability of drone missions in scenarios requiring extended surveillance, continuous monitoring and immediate data processing needs. Examining the advancements, operational benefits and potential future progressions associated with tethered drones, this article shows their increasing significance across various sectors and their pivotal role in pushing the boundaries of current UAV capabilities. The emergence of tethered drone technology not only addresses existing obstacles but also paves the way for new innovations within the UAV industry.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "4 pages, 1 figure, 1 table, 28 references"
    },
    {
        "paper id": "2403.07923",
        "abstract url": "https://arxiv.org/abs/2403.07923",
        "title": "The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments",
        "rating": -2,
        "keywords": [
            [
                "industrial",
                "IoT"
            ]
        ],
        "abstract": "In response to the demand for real-time performance and control quality in industrial Internet of Things (IoT) environments, this paper proposes an optimization control system based on deep reinforcement learning and edge computing. The system leverages cloud-edge collaboration, deploys lightweight policy networks at the edge, predicts system states, and outputs controls at a high frequency, enabling monitoring and optimization of industrial objectives. Additionally, a dynamic resource allocation mechanism is designed to ensure rational scheduling of edge computing resources, achieving global optimization. Results demonstrate that this approach reduces cloud-edge communication latency, accelerates response to abnormal situations, reduces system failure rates, extends average equipment operating time, and saves costs for manual maintenance and replacement. This ensures real-time and stable control.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18434",
        "abstract url": "https://arxiv.org/abs/2402.18434",
        "title": "Graph Regularized Encoder Training for Extreme Classification",
        "rating": -2.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep extreme classification (XC) aims to train an encoder architecture and an accompanying classifier architecture to tag a data point with the most relevant subset of labels from a very large universe of labels. XC applications in ranking, recommendation and tagging routinely encounter tail labels for which the amount of training data is exceedingly small. Graph convolutional networks (GCN) present a convenient but computationally expensive way to leverage task metadata and enhance model accuracies in these settings. This paper formally establishes that in several use cases, the steep computational cost of GCNs is entirely avoidable by replacing GCNs with non-GCN architectures. The paper notices that in these settings, it is much more effective to use graph data to regularize encoder training than to implement a GCN. Based on these insights, an alternative paradigm RAMEN is presented to utilize graph metadata in XC settings that offers significant performance boosts with zero increase in inference computational costs. RAMEN scales to datasets with up to 1M labels and offers prediction accuracy up to 15% higher on benchmark datasets than state of the art methods, including those that use graph metadata to train GCNs. RAMEN also offers 10% higher accuracy over the best baseline on a proprietary recommendation dataset sourced from click logs of a popular search engine. Code for RAMEN will be released publicly.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18477",
        "abstract url": "https://arxiv.org/abs/2402.18477",
        "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes",
        "rating": -2.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \"which variables enter the differential of which other variables\". In this paper, we develop a kernel-based test of conditional independence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirically verify that our developed CI test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18491",
        "abstract url": "https://arxiv.org/abs/2402.18491",
        "title": "Dynamical Regimes of Diffusion Models",
        "rating": -2.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process. The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data. The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for diffusion models. Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "22 pages, 11 figures"
    },
    {
        "paper id": "2402.18573",
        "abstract url": "https://arxiv.org/abs/2402.18573",
        "title": "UniMODE: Unified Monocular 3D Object Detection",
        "rating": -2.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "robot",
                "navigation"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Realizing unified monocular 3D object detection, including both indoor and outdoor scenes, holds great importance in applications like robot navigation. However, involving various scenarios of data to train models poses challenges due to their significantly different characteristics, e.g., diverse geometry properties and heterogeneous domain distributions. To address these challenges, we build a detector based on the bird's-eye-view (BEV) detection paradigm, where the explicit feature projection is beneficial to addressing the geometry learning ambiguity when employing multiple scenarios of data to train detectors. Then, we split the classical BEV detection architecture into two stages and propose an uneven BEV grid design to handle the convergence instability caused by the aforementioned challenges. Moreover, we develop a sparse BEV feature projection strategy to reduce computational cost and a unified domain alignment method to handle heterogeneous domains. Combining these techniques, a unified detector UniMODE is derived, which surpasses the previous state-of-the-art on the challenging Omni3D dataset (a large-scale dataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the first successful generalization of a BEV detector to unified 3D object detection.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This paper has been accepted for publication in CVPR2024"
    },
    {
        "paper id": "2402.18630",
        "abstract url": "https://arxiv.org/abs/2402.18630",
        "title": "GNSS Positioning using Cost Function Regulated Multilateration and Graph Neural Networks",
        "rating": -2.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In urban environments, where line-of-sight signals from GNSS satellites are frequently blocked by high-rise objects, GNSS receivers are subject to large errors in measuring satellite ranges. Heuristic methods are commonly used to estimate these errors and reduce the impact of noisy measurements on localization accuracy. In our work, we replace these error estimation heuristics with a deep learning model based on Graph Neural Networks. Additionally, by analyzing the cost function of the multilateration process, we derive an optimal method to utilize the estimated errors. Our approach guarantees that the multilateration converges to the receiver's location as the error estimation accuracy increases. We evaluate our solution on a real-world dataset containing more than 100k GNSS epochs, collected from multiple cities with diverse characteristics. The empirical results show improvements from 40% to 80% in the horizontal localization error against recent deep learning baselines as well as classical localization approaches.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Published in The Proceedings of the Institute of Navigation GNSS+ 2023"
    },
    {
        "paper id": "2402.18813",
        "abstract url": "https://arxiv.org/abs/2402.18813",
        "title": "Protein Multimer Structure Prediction via Prompt Learning",
        "rating": -2.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "biological"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction~(MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions~(PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a \\textit{poor generalization} to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales~(i.e., chain numbers). Specifically, we propose \\textbf{\\textsc{PromptMSP}}, a pre-training and \\textbf{Prompt} tuning framework for \\textbf{M}ultimer \\textbf{S}tructure \\textbf{P}rediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We provide a meta-learning strategy to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models. The code, data and checkpoints are released at \\url{https://github.com/zqgao22/PromptMSP}.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "International Conference on Learning Representations (ICLR 2024)"
    },
    {
        "paper id": "2403.00833",
        "abstract url": "https://arxiv.org/abs/2403.00833",
        "title": "Position Paper: Agent AI Towards a Holistic Intelligence",
        "rating": -2.5,
        "keywords": [
            [
                "robotics"
            ],
            [
                "healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent advancements in large foundation models have remarkably enhanced our understanding of sensory information in open-world environments. In leveraging the power of foundation models, it is crucial for AI research to pivot away from excessive reductionism and toward an emphasis on systems that function as cohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied system that integrates large foundation models into agent actions. The emerging field of Agent AI spans a wide range of existing embodied and agent-based multimodal interactions, including robotics, gaming, and healthcare systems, etc. In this paper, we propose a novel large action model to achieve embodied intelligent behavior, the Agent Foundation Model. On top of this idea, we discuss how agent AI exhibits remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Furthermore, we discuss the potential of Agent AI from an interdisciplinary perspective, underscoring AI cognition and consciousness within scientific discourse. We believe that those discussions serve as a basis for future research directions and encourage broader societal engagement.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "22 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv:2401.03568"
    },
    {
        "paper id": "2402.18088",
        "abstract url": "https://arxiv.org/abs/2402.18088",
        "title": "Bimanual Manipulation of Steady Hand Eye Robots with Adaptive Sclera Force Control: Cooperative vs. Teleoperation Strategies",
        "rating": -3,
        "keywords": [
            [
                "Robot"
            ],
            [
                "surgical",
                "surgery",
                "retinal"
            ]
        ],
        "abstract": "Performing intricate eye microsurgery, such as retinal vein cannulation (RVC), as a potential treatment for retinal vein occlusion (RVO), without the assistance of a surgical robotic system is very challenging to do safely. The main limitation has to do with the physiological hand tremor of surgeons. Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC. The Steady-Hand Eye Robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively. However, the admittance-based cooperative control mode does not address crucial safety considerations, such as minimizing contact force between the surgical instrument and the sclera surface to prevent tissue damage. An adaptive sclera force control algorithm was proposed to address this limitation using an FBG-based force-sensing tool to measure and minimize the tool-sclera interaction force. Additionally, features like haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework. We implemented a bimanual adaptive teleoperation (BMAT) control mode using SHER 2.0 and SHER 2.1 and compared its performance with a bimanual adaptive cooperative (BMAC) mode. Both BMAT and BMAC modes were tested in sitting and standing postures during a vessel-following experiment under a surgical microscope. It is shown, for the first time to the best of our knowledge in robot-assisted retinal surgery, that integrating the adaptive sclera force control algorithm with the bimanual teleoperation framework enables surgeons to safely perform bimanual telemanipulation of the eye without over-stretching it, even in the absence of registration between the two robots.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18090",
        "abstract url": "https://arxiv.org/abs/2402.18090",
        "title": "Computing Minimal Absent Words and Extended Bispecial Factors with CDAWG Space",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "bioinformatics"
            ]
        ],
        "abstract": "A string $w$ is said to be a minimal absent word (MAW) for a string $S$ if $w$ does not occur in $S$ and any proper substring of $w$ occurs in $S$. We focus on non-trivial MAWs which are of length at least 2. Finding such non-trivial MAWs for a given string is motivated for applications in bioinformatics and data compression. Fujishige et al. [TCS 2023] proposed a data structure of size $\u0398(n)$ that can output the set $\\mathsf{MAW}(S)$ of all MAWs for a given string $S$ of length $n$ in $O(n + |\\mathsf{MAW}(S)|)$ time, based on the directed acyclic word graph (DAWG). In this paper, we present a more space efficient data structure based on the compact DAWG (CDAWG), which can output $\\mathsf{MAW}(S)$ in $O(|\\mathsf{MAW}(S)|)$ time with $O(\\mathsf{e}_\\min)$ space, where $\\mathsf{e}_\\min$ denotes the minimum of the sizes of the CDAWGs for $S$ and for its reversal $S^R$. For any strings of length $n$, it holds that $\\mathsf{e}_\\min < 2n$, and for highly repetitive strings $\\mathsf{e}_\\min$ can be sublinear (up to logarithmic) in $n$. We also show that MAWs and their generalization minimal rare words have close relationships with extended bispecial factors, via the CDAWG.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "Accepted for IWOCA 2024"
    },
    {
        "paper id": "2402.18092",
        "abstract url": "https://arxiv.org/abs/2402.18092",
        "title": "Context-aware Talking Face Video Generation",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we consider a novel and practical case for talking face video generation. Specifically, we focus on the scenarios involving multi-people interactions, where the talking context, such as audience or surroundings, is present. In these situations, the video generation should take the context into consideration in order to generate video content naturally aligned with driving audios and spatially coherent to the context. To achieve this, we provide a two-stage and cross-modal controllable video generation pipeline, taking facial landmarks as an explicit and compact control signal to bridge the driving audio, talking context and generated videos. Inside this pipeline, we devise a 3D video diffusion model, allowing for efficient contort of both spatial conditions (landmarks and context video), as well as audio condition for temporally coherent generation. The experimental results verify the advantage of the proposed method over other baselines in terms of audio-video synchronization, video fidelity and frame consistency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18095",
        "abstract url": "https://arxiv.org/abs/2402.18095",
        "title": "Exergetic Port-Hamiltonian Systems for Multibody Dynamics",
        "rating": -3,
        "keywords": [
            [
                "robotics"
            ],
            [
                "biomechanics"
            ]
        ],
        "abstract": "Multibody dynamics simulation plays an important role in various fields, including mechanical engineering, robotics, and biomechanics. Setting up computational models however becomes increasingly challenging as systems grow in size and complexity. Especially the consistent combination of models across different physical domains usually demands a lot of attention. This motivates us to study formal languages for compositional modeling of multiphysical systems. This article shows how multibody systems, or more precisely assemblies of rigid bodies connected by lower kinematic pairs, fit into the framework of Exergetic Port-Hamiltonian Systems (EPHS). This approach is based on the hierarchical decomposition of systems into their ultimately primitive components, using a simple graphical syntax. Thereby, cognitive load can be reduced and communication is facilitated, even with non-experts. Moreover, the encapsulation and reuse of subsystems promotes efficient model development and management. In contrast to established modeling languages such as Modelica, the primitive components of EPHS are not defined by arbitrary equations. Instead, there are four kinds of components, each defined by a particular geometric structure with a clear physical interpretation. This higher-level approach could make the process of building and maintaining large-scale models simpler and also safer.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18236",
        "abstract url": "https://arxiv.org/abs/2402.18236",
        "title": "Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "graph"
            ],
            [
                "MRI",
                "clinical",
                "cardiac"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Computational fluid dynamics (CFD) can be used for evaluation of hemodynamics. However, its routine use is limited by labor-intensive manual segmentation, CFD mesh creation, and time-consuming simulation. This study aims to train a deep learning model to both generate patient-specific volume-meshes of the pulmonary artery from 3D cardiac MRI data and directly estimate CFD flow fields. This study used 135 3D cardiac MRIs from both a public and private dataset. The pulmonary arteries in the MRIs were manually segmented and converted into volume-meshes. CFD simulations were performed on ground truth meshes and interpolated onto point-point correspondent meshes to create the ground truth dataset. The dataset was split 85/10/15 for training, validation and testing. Image2Flow, a hybrid image and graph convolutional neural network, was trained to transform a pulmonary artery template to patient-specific anatomy and CFD values. Image2Flow was evaluated in terms of segmentation and accuracy of CFD predicted was assessed using node-wise comparisons. Centerline comparisons of Image2Flow and CFD simulations performed using machine learning segmentation were also performed. Image2Flow achieved excellent segmentation accuracy with a median Dice score of 0.9 (IQR: 0.86-0.92). The median node-wise normalized absolute error for pressure and velocity magnitude was 11.98% (IQR: 9.44-17.90%) and 8.06% (IQR: 7.54-10.41), respectively. Centerline analysis showed no significant difference between the Image2Flow and conventional CFD simulated on machine learning-generated volume-meshes. This proof-of-concept study has shown it is possible to simultaneously perform patient specific volume-mesh based segmentation and pressure and flow field estimation. Image2Flow completes segmentation and CFD in ~205ms, which ~7000 times faster than manual methods, making it more feasible in a clinical environment.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "22 pages, 7 figures, 3 tables"
    },
    {
        "paper id": "2402.18241",
        "abstract url": "https://arxiv.org/abs/2402.18241",
        "title": "Affective State Detection using fNIRs and Machine Learning",
        "rating": -3,
        "keywords": [
            [
                "infrared"
            ],
            [
                "health"
            ]
        ],
        "abstract": "Affective states regulate our day to day to function and has a tremendous effect on mental and physical health. Detection of affective states is of utmost importance for mental health monitoring, smart entertainment selection and dynamic workload management. In this paper, we discussed relevant literature on affective state detection using physiology data, the benefits and limitations of different sensors and methods used for collecting physiology data, and our rationale for selecting functional near-infrared spectroscopy. We present the design of an experiment involving nine subjects to evoke the affective states of meditation, amusement and cognitive load and the results of the attempt to classify using machine learning. A mean accuracy of 83.04% was achieved in three class classification with an individual model; 84.39% accuracy was achieved for a group model and 60.57% accuracy was achieved for subject independent model using leave one out cross validation. It was found that prediction accuracy for cognitive load was higher (evoked using a pen and paper task) than the other two classes (evoked using computer bases tasks). To verify that this discrepancy was not due to motor skills involved in the pen and paper task, a second experiment was conducted using four participants and the results of that experiment has also been presented in the paper.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18280",
        "abstract url": "https://arxiv.org/abs/2402.18280",
        "title": "Indirect Job-Shop coding using rank: application to QAOA (IQAOA)",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "The Job-Shop Scheduling Problem (JSSP) stands as one of the most renowned challenges in scheduling. It is characterized as a disjunctive problem, wherein a solution is fully depicted through an oriented disjunctive graph, with earliest starting times computed using a longest path algorithm. The complexity of solving this problem arises in part from the requirement that disjunctive graphs representing solutions must be acyclic. Consequently, enumerating these graphs is feasible for small-scale instances only. A significant advancement in this field, credited to (Bierwith, 1995), is the introduction of the 'vector by repetition' (commonly known as Bierwith's vector). Notably, this vector possesses the property that it can be mapped to an acyclic disjunctive graph, thereby enabling the mapping of a vector to a solution. This property has facilitated the development of highly efficient resolution schemes, as it allows the enumeration of solutions only i.e. acyclic disjunctive graphs. Our objective is to demonstrate how Bierwith's vector can be integrated into a Quantum Approximate Optimization Algorithm (QAOA) to tackle the job-shop problem using a novel quantum approach.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18318",
        "abstract url": "https://arxiv.org/abs/2402.18318",
        "title": "SD-SLAM: A Semantic SLAM Approach for Dynamic Scenes Based on LiDAR Point Clouds",
        "rating": -3,
        "keywords": [
            [
                "Point cloud"
            ],
            [
                "LiDAR",
                "vehicle",
                "SLAM"
            ],
            [
                "navigation"
            ]
        ],
        "abstract": "Point cloud maps generated via LiDAR sensors using extensive remotely sensed data are commonly used by autonomous vehicles and robots for localization and navigation. However, dynamic objects contained in point cloud maps not only downgrade localization accuracy and navigation performance but also jeopardize the map quality. In response to this challenge, we propose in this paper a novel semantic SLAM approach for dynamic scenes based on LiDAR point clouds, referred to as SD-SLAM hereafter. The main contributions of this work are in three aspects: 1) introducing a semantic SLAM framework dedicatedly for dynamic scenes based on LiDAR point clouds, 2) Employing semantics and Kalman filtering to effectively differentiate between dynamic and semi-static landmarks, and 3) Making full use of semi-static and pure static landmarks with semantic information in the SD-SLAM process to improve localization and mapping performance. To evaluate the proposed SD-SLAM, tests were conducted using the widely adopted KITTI odometry dataset. Results demonstrate that the proposed SD-SLAM effectively mitigates the adverse effects of dynamic objects on SLAM, improving vehicle localization and mapping performance in dynamic scenes, and simultaneously constructing a static semantic map with multiple semantic classes for enhanced environment understanding.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18325",
        "abstract url": "https://arxiv.org/abs/2402.18325",
        "title": "Robotising Psychometrics: Validating Wellbeing Assessment Tools in Child-Robot Interactions",
        "rating": -3,
        "keywords": [
            [
                "Robot"
            ],
            [
                "health",
                "psychological"
            ]
        ],
        "abstract": "The interdisciplinary nature of Child-Robot Interaction (CRI) fosters incorporating measures and methodologies from many established domains. However, when employing CRI approaches to sensitive avenues of health and wellbeing, caution is critical in adapting metrics to retain their safety standards and ensure accurate utilisation. In this work, we conducted a secondary analysis to previous empirical work, investigating the reliability and construct validity of established psychological questionnaires such as the Short Moods and Feelings Questionnaire (SMFQ) and three subscales (generalised anxiety, panic and low mood) of the Revised Child Anxiety and Depression Scale (RCADS) within a CRI setting for the assessment of mental wellbeing. Through confirmatory principal component analysis, we have observed that these measures are reliable and valid in the context of CRI. Furthermore, our analysis revealed that scales communicated by a robot demonstrated a better fit than when self-reported, underscoring the efficiency and effectiveness of robot-mediated psychological assessments in these settings. Nevertheless, we have also observed variations in item contributions to the main factor, suggesting potential areas of examination and revision (e.g., relating to physiological changes, inactivity and cognitive demands) when used in CRI. Findings from this work highlight the importance of verifying the reliability and validity of standardised metrics and assessment tools when employed in CRI settings, thus, aiming to avoid any misinterpretations and misrepresentations.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18362",
        "abstract url": "https://arxiv.org/abs/2402.18362",
        "title": "Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model",
        "rating": -3,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "medical",
                "surgical",
                "surgery",
                "cancer"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18396",
        "abstract url": "https://arxiv.org/abs/2402.18396",
        "title": "Deep Confident Steps to New Pockets: Strategies for Docking Generalization",
        "rating": -3,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing benchmarks, however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities. We carefully analyze the scaling laws of ML-based docking and show that, by scaling data and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across benchmarks. Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between diffusion and confidence models and exploits the multi-resolution generation process of diffusion models. We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods.",
        "subjects": [
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18487",
        "abstract url": "https://arxiv.org/abs/2402.18487",
        "title": "Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay",
        "rating": -3,
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "UAV",
                "drone"
            ]
        ],
        "abstract": "The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness. However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground. This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions. We introduce a novel approach based on the reinforcement learning augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations. Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR. Our contributions include (1) a reinforcement learning framework for UAV trajectory planning that dynamically integrates multi-objective considerations, (2) an analysis of human perceptions towards gendered and anthropomorphized drones in SAR contexts, and (3) the application of similarity-based experience replay for enhanced learning efficiency in complex SAR scenarios. The findings offer valuable insights into designing UAV systems that are not only technically proficient but also aligned with human-centric values.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18555",
        "abstract url": "https://arxiv.org/abs/2402.18555",
        "title": "Time-efficient filtering of polarimetric data by checking physical realizability of experimental Mueller matrices",
        "rating": -3,
        "keywords": [
            [
                "Time-efficient"
            ],
            [
                "biomedicine"
            ],
            [
                "remote sensing"
            ]
        ],
        "abstract": "Imaging Mueller polarimetry has already proved its potential for metrology, remote sensing and biomedicine. The real-time applications of this modality require both video rate image acquisition and fast data post-processing algorithms. First, one must check the physical realizability of the experimental Mueller matrices in order to filter out non-physical data, i.e. to test the positive semi-definiteness of the 4x4 Hermitian coherency matrix calculated from the elements of the corresponding Mueller matrix pixel-wise. For this purpose, we compared the execution time for the calculations of i) eigenvalues, ii) Cholesky decomposition, iii) Sylvester's criterion, and iv) coefficients of the characteristic polynomial of the Hermitian coherency matrix using two different approaches, all calculated for the experimental Mueller matrix images (600 pixels x 700 pixels) of mouse uterine cervix. The calculations were performed using C++ and Julia programming languages. Our results showed the superiority of the algorithm iv), in particular, the version based on the simplification via Pauli matrices, in terms of execution time for our dataset, over other algorithms. The sequential implementation of the latter algorithm on a single core already satisfies the requirements of real-time polarimetric imaging in various domains. This can be further amplified by the proposed parallelization (for example, we achieve a 5-fold speed up on 6 cores).",
        "subjects": [
            "physics.optics"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18677",
        "abstract url": "https://arxiv.org/abs/2402.18677",
        "title": "Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks",
        "rating": -3,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "robot"
            ],
            [
                "Attacks"
            ]
        ],
        "abstract": "Safety is a fundamental requirement of many robotic systems. Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems. However, the effectiveness of these approaches highly relies on the choice of CBFs. Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks. In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks. Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF). We derive the necessary and sufficient conditions for FT-NCBFs to guarantee safety, and develop a data-driven method to learn FT-NCBFs by minimizing a loss function constructed using the derived conditions. Using the learned FT-NCBF, we synthesize a control input and formally prove the safety guarantee provided by our approach. We demonstrate our proposed approach using two case studies: obstacle avoidance problem for an autonomous mobile robot and spacecraft rendezvous problem, with code available via https://github.com/HongchaoZhang-HZ/FTNCBF.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18710",
        "abstract url": "https://arxiv.org/abs/2402.18710",
        "title": "Hefty: A Modular Reconfigurable Robot for Advancing Robot Manipulation in Agriculture",
        "rating": -3,
        "keywords": [
            [
                "robotics",
                "Robot"
            ],
            [
                "agricultural"
            ]
        ],
        "abstract": "This paper presents a modular, reconfigurable robot platform for robot manipulation in agriculture. While robot manipulation promises great advancements in automating challenging, complex tasks that are currently best left to humans, it is also an expensive capital investment for researchers and users because it demands significantly varying robot configurations depending on the task. Modular robots provide a way to obtain multiple configurations and reduce costs by enabling incremental acquisition of only the necessary modules. The robot we present, Hefty, is designed to be modular and reconfigurable. It is designed for both researchers and end-users as a means to improve technology transfer from research to real-world application. This paper provides a detailed design and integration process, outlining the critical design decisions that enable modularity in the mobility of the robot as well as its sensor payload, power systems, computing, and fixture mounting. We demonstrate the utility of the robot by presenting five configurations used in multiple real-world agricultural robotics applications.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 11 figures"
    },
    {
        "paper id": "2402.18749",
        "abstract url": "https://arxiv.org/abs/2402.18749",
        "title": "Weighted strategies to guide a multi-objective evolutionary algorithm for multi-UAV mission planning",
        "rating": -3,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Management and mission planning over a swarm of unmanned aerial vehicle (UAV) remains to date as a challenging research trend in what regards to this particular type of aircrafts. These vehicles are controlled by a number of ground control station (GCS), from which they are commanded to cooperatively perform different tasks in specific geographic areas of interest. Mathematically the problem of coordinating and assigning tasks to a swarm of UAV can be modeled as a constraint satisfaction problem, whose complexity and multiple conflicting criteria has hitherto motivated the adoption of multi-objective solvers such as multi-objective evolutionary algorithm (MOEA). The encoding approach consists of different alleles representing the decision variables, whereas the fitness function checks that all constraints are fulfilled, minimizing the optimization criteria of the problem. In problems of high complexity involving several tasks, UAV and GCS, where the space of search is huge compared to the space of valid solutions, the convergence rate of the algorithm increases significantly. To overcome this issue, this work proposes a weighted random generator for the creation and mutation of new individuals. The main objective of this work is to reduce the convergence rate of the MOEA solver for multi-UAV mission planning using weighted random strategies that focus the search on potentially better regions of the solution space. Extensive experimental results over a diverse range of scenarios evince the benefits of the proposed approach, which notably improves this convergence rate with respect to a na\u00efve MOEA approach.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Preprint submitted and accepted in Swarm and Evolutionary Computation"
    },
    {
        "paper id": "2402.18758",
        "abstract url": "https://arxiv.org/abs/2402.18758",
        "title": "Analog Isolated Multilevel Quantizer for Voltage Sensing while Maintaining Galvanic Isolation",
        "rating": -3,
        "keywords": [
            [
                "depth"
            ],
            [
                "health"
            ]
        ],
        "abstract": "A low-power, compact device for performing measurements in electrical systems with isolated voltage domains is proposed. Isolated measurements are required in numerous applications. For instance, a measurement of the bus voltage for a system with a high supply voltage and lower isolated local voltage level may be needed for system health monitoring and control. Such a requirement may necessitate the use of isolation amplifiers to provide voltage telemetry for the local system. Isolation amplifiers require dual galvanically isolated supplies and use magnetic, capacitive, or optical barriers between primary and secondary sides. Producing this supplemental voltage requires an extra voltage converter, which consumes power and generates electromagnetic interference which must, in turn, be filtered. Complex designs incorporating feedback are needed to achieve linear response. The proposed Analog Isolated Multilevel Quantizer (AIMQ) addresses these issues by monitoring the primary-side signal and communicating the results to the secondary side using a novel scheme involving Zener diodes, optocouplers, transistors, one-hot coding, and discrete outputs. The result is a low power isolated transducer that can in principle be extended to an arbitrary bit depth.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "8 pages, 12 Figures"
    },
    {
        "paper id": "2403.00033",
        "abstract url": "https://arxiv.org/abs/2403.00033",
        "title": "Identification of Craving Maps among Marijuana Users via the Analysis of Functional Brain Networks with High-Order Attention Graph Neural Networks",
        "rating": -3,
        "keywords": [
            [
                "Graph"
            ],
            [
                "fMRI",
                "psychological"
            ]
        ],
        "abstract": "The excessive consumption of marijuana can induce substantial psychological and social consequences. In this investigation, we propose an elucidative framework termed high-order graph attention neural networks (HOGANN) for the classification of Marijuana addiction, coupled with an analysis of localized brain network communities exhibiting abnormal activities among chronic marijuana users. HOGANN integrates dynamic intrinsic functional brain networks, estimated from resting-state functional magnetic resonance imaging (rs-fMRI), using long short-term memory (LSTM) to capture temporal network dynamics. We employ a high-order attention module for information fusion and message passing among neighboring nodes, enhancing the network community analysis. Our model is validated across two distinct data cohorts, yielding substantially higher classification accuracy than benchmark algorithms. Furthermore, we discern the most pertinent subnetworks and cognitive regions affected by persistent marijuana consumption, indicating adverse effects on functional brain networks, particularly within the dorsal attention and frontoparietal networks. Intriguingly, our model demonstrates superior performance in cohorts exhibiting prolonged dependence, implying that prolonged marijuana usage induces more pronounced alterations in brain networks. The model proficiently identifies craving brain maps, thereby delineating critical brain regions for analysis.",
        "subjects": [
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00832",
        "abstract url": "https://arxiv.org/abs/2403.00832",
        "title": "Explainable Session-based Recommendation via Path Reasoning",
        "rating": -3,
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "This paper explores providing explainability for session-based recommendation (SR) by path reasoning. Current SR models emphasize accuracy but lack explainability, while traditional path reasoning prioritizes knowledge graph exploration, ignoring sequential patterns present in the session history. Therefore, we propose a generalized hierarchical reinforcement learning framework for SR, which improves the explainability of existing SR models via Path Reasoning, namely PR4SR. Considering the different importance of items to the session, we design the session-level agent to select the items in the session as the starting point for path reasoning and the path-level agent to perform path reasoning. In particular, we design a multi-target reward mechanism to adapt to the skip behaviors of sequential patterns in SR, and introduce path midpoint reward to enhance the exploration efficiency in knowledge graphs. To improve the completeness of the knowledge graph and to diversify the paths of explanation, we incorporate extracted feature information from images into the knowledge graph. We instantiate PR4SR in five state-of-the-art SR models (i.e., GRU4REC, NARM, GCSAN, SR-GNN, SASRec) and compare it with other explainable SR frameworks, to demonstrate the effectiveness of PR4SR for recommendation and explanation tasks through extensive experiments with these approaches on four datasets.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03960",
        "abstract url": "https://arxiv.org/abs/2403.03960",
        "title": "Assessing the Extrapolation Capability of Template-Free Retrosynthesis Models",
        "rating": -3,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "chemical"
            ]
        ],
        "abstract": "Despite the acknowledged capability of template-free models in exploring unseen reaction spaces compared to template-based models for retrosynthesis prediction, their ability to venture beyond established boundaries remains relatively uncharted. In this study, we empirically assess the extrapolation capability of state-of-the-art template-free models by meticulously assembling an extensive set of out-of-distribution (OOD) reactions. Our findings demonstrate that while template-free models exhibit potential in predicting precursors with novel synthesis rules, their top-10 exact-match accuracy in OOD reactions is strikingly modest (< 1%). Furthermore, despite the capability of generating novel reactions, our investigation highlights a recurring issue where more than half of the novel reactions predicted by template-free models are chemically implausible. Consequently, we advocate for the future development of template-free models that integrate considerations of chemical feasibility when navigating unexplored regions of reaction space.",
        "subjects": [
            "physics.chem-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18102",
        "abstract url": "https://arxiv.org/abs/2402.18102",
        "title": "Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging",
        "rating": -4,
        "keywords": [
            [
                "3D",
                "RGB-D"
            ],
            [
                "autonomous driving"
            ],
            [
                "navigation"
            ],
            [
                "medical",
                "surgical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist. Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP) sensors are a potential solution to achieve the same. DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system. However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views. This creates a trade-off between disparity estimation vs. deblurring accuracy. To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor. In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting. Our resulting CADS imaging system demonstrates improvement of >1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings. Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor. Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in simulations and real-world experiments in a passive, snapshot, and compact manner.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18174",
        "abstract url": "https://arxiv.org/abs/2402.18174",
        "title": "Generation of skill-specific maps from graph world models for robotic systems",
        "rating": -4,
        "keywords": [
            [
                "3D"
            ],
            [
                "SLAM"
            ],
            [
                "robot",
                "navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "With the increase in the availability of Building Information Models (BIM) and (semi-) automatic tools to generate BIM from point clouds, we propose a world model architecture and algorithms to allow the use of the semantic and geometric knowledge encoded within these models to generate maps for robot localization and navigation. When heterogeneous robots are deployed within an environment, maps obtained from classical SLAM approaches might not be shared between all agents within a team of robots, e.g. due to a mismatch in sensor type, or a difference in physical robot dimensions. Our approach extracts the 3D geometry and semantic description of building elements (e.g. material, element type, color) from BIM, and represents this knowledge in a graph. Based on queries on the graph and knowledge of the skills of the robot, we can generate skill-specific maps that can be used during the execution of localization or navigation tasks. The approach is validated with data from complex build environments and integrated into existing navigation frameworks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2402.18271",
        "abstract url": "https://arxiv.org/abs/2402.18271",
        "title": "Distributed Intelligent Integrated Sensing and Communications: The 6G-DISAC Approach",
        "rating": -4,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "6G",
                "industrial"
            ]
        ],
        "abstract": "This paper introduces the concept of Distributed Intelligent integrated Sensing and Communications (DISAC), which expands the capabilities of Integrated Sensing and Communications (ISAC) towards distributed architectures. Additionally, the DISAC framework integrates novel waveform design with new semantic and goal-oriented communication paradigms, enabling ISAC technologies to transition from traditional data fusion to the semantic composition of diverse sensed and shared information. This progress facilitates large-scale, energy-efficient support for high-precision spatial-temporal processing, optimizing ISAC resource utilization, and enabling effective multi-modal sensing performance. Addressing key challenges such as efficient data management and connect-compute resource utilization, 6G- DISAC stands to revolutionize applications in diverse sectors including transportation, healthcare, and industrial automation. Our study encapsulates the project vision, methodologies, and potential impact, marking a significant stride towards a more connected and intelligent world.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted for conference publication"
    },
    {
        "paper id": "2402.18456",
        "abstract url": "https://arxiv.org/abs/2402.18456",
        "title": "RF-Flashlight Testbed for Verification of Real-Time Geofencing of EESS Radiometers and Millimeter-Wave Ground-to-Satellite Propagation Models",
        "rating": -4,
        "keywords": [
            [
                "5G",
                "6G"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "A simple 'RF-flashlight' (or ground to satellite) interference testbed is proposed to experimentally verify real-time geofencing (RTG) for protecting passive Earth Exploration Satellite Services (EESS) radiometer measurements from 5G or 6G mm-wave transmissions, and ground to satellite propagation models used in the interference modeling of this spectrum coexistence scenario. RTG is a stronger EESS protection mechanism than the current methodology recommended by the ITU based on a worst-case interference threshold while simultaneously enabling dynamic spectrum sharing and coexistence with 5G or 6G wireless networks. Similarly, verifying more sophisticated RF propagation models that include ground topology, buildings, and non-line-of-sight paths will provide better estimates of interference than the current ITU line-of-sight model and, thus, a more reliable basis for establishing a consensus among the spectrum stakeholders.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18465",
        "abstract url": "https://arxiv.org/abs/2402.18465",
        "title": "Semantic Information in MC: Chemotaxis Beyond Shannon",
        "rating": -4,
        "keywords": [
            [
                "biological",
                "survival",
                "disease"
            ],
            [
                "chemical"
            ]
        ],
        "abstract": "The recently emerged molecular communication (MC) paradigm intends to leverage communication engineering tools for the design of synthetic chemical communication systems. These systems are envisioned to operate at nanoscale and in biological environments, such as the human body, and catalyze the emergence of revolutionary applications in the context of early disease monitoring and drug targeting. Despite the abundance of theoretical (and recently also experimental) MC system designs proposed over the past years, some fundamental questions remain unresolved, hindering the breakthrough of MC in real-world applications. One of these questions is: What can be a useful measure of information in the context of MC applications? While most existing works on MC build upon the concept of syntactic information as introduced by Shannon, in this paper, we explore the framework of semantic information as introduced by Kolchinsky and Wolpert for the information-theoretic analysis of a natural MC system, namely bacterial chemotaxis. Exploiting computational agent-based modeling (ABM), we are able to quantify, for the first time, the amount of information that the considered chemotactic bacterium (CB) utilizes to adapt to and survive in a dynamic environment. In other words, we show how the flow of information between the environment and the CB is related to the effectiveness of communication. Effectiveness here refers to the adaptation of the CB to the dynamic environment in order to ensure survival. Our analysis reveals that it highly depends on the environmental conditions how much information the CB can effectively utilize for improving their survival chances. Encouraged by our results, we envision that the proposed semantic information framework can open new avenues for the development of theoretical and experimental MC system designs for future nanoscale applications.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "7 pages, 5 figures, This work has been submitted in part for possible publication to the IEEE Global Communications Conference (GLOBECOM) 2024"
    },
    {
        "paper id": "2402.18612",
        "abstract url": "https://arxiv.org/abs/2402.18612",
        "title": "Understanding random forests and overfitting: a visualization and simulation study",
        "rating": -4,
        "keywords": [
            [
                "clinical"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "Random forests have become popular for clinical risk prediction modelling. In a case study on predicting ovarian malignancy, we observed training c-statistics close to 1. Although this suggests overfitting, performance was competitive on test data. We aimed to understand the behaviour of random forests by (1) visualizing data space in three real world case studies and (2) a simulation study. For the case studies, risk estimates were visualised using heatmaps in a 2-dimensional subspace. The simulation study included 48 logistic data generating mechanisms (DGM), varying the predictor distribution, the number of predictors, the correlation between predictors, the true c-statistic and the strength of true predictors. For each DGM, 1000 training datasets of size 200 or 4000 were simulated and RF models trained with minimum node size 2 or 20 using ranger package, resulting in 192 scenarios in total. The visualizations suggested that the model learned spikes of probability around events in the training set. A cluster of events created a bigger peak, isolated events local peaks. In the simulation study, median training c-statistics were between 0.97 and 1 unless there were 4 or 16 binary predictors with minimum node size 20. Median test c-statistics were higher with higher events per variable, higher minimum node size, and binary predictors. Median training slopes were always above 1, and were not correlated with median test slopes across scenarios (correlation -0.11). Median test slopes were higher with higher true c-statistic, higher minimum node size, and higher sample size. Random forests learn local probability peaks that often yield near perfect training c-statistics without strongly affecting c-statistics on test data. When the aim is probability estimation, the simulation results go against the common recommendation to use fully grown trees in random forest models.",
        "subjects": [
            "stat.ME"
        ],
        "comment": "20 pages, 8 figures"
    },
    {
        "paper id": "2402.18448",
        "abstract url": "https://arxiv.org/abs/2402.18448",
        "title": "Impact of new 5G network components on out-of-band emissions at 23.8 GHz",
        "rating": -4.5,
        "keywords": [
            [
                "5G",
                "6G"
            ],
            [
                "Satellite"
            ],
            [
                "Workshop"
            ]
        ],
        "abstract": "The impact of 5G networks transmitting between 24.25- 27.5 GHz on Earth Exploration Satellite Services (EESS) microwave sounders used to measure atmospheric water vapor and temperature was widely discussed and modeled in preparation for setting emission recommendations by International Telecommunications Union (ITU) at the 2019 World Radio Congress (WRC-19). Since then, two new classes of network devices - 5G repeaters and high transmission power User Equipment (UE) for fixed wireless services - have been introduced and deployed in 28 GHz networks with expectations that they will also be deployed at 24 GHz. This paper discusses the (potentially significant) increase in interference from these new components along with open questions related to their regulatory status. While this paper discusses increases in interference to 23.8 GHz EESS measurements from 5G transmissions in the \"24 GHz\" band, it is important to recognize that repeaters and high power UEs need to be considered when modeling interference from 5G/6G networks in all bands. This paper also touches on whether the current ITU process and methodology to regulate interference with passive sensors (vendor applied hardware-based filtering based on long-term network forecasts and worst-case Monte Carlo modeling) can keep up with rapidly changing wireless technology and the increased competition for spectrum.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "The RFI 2022 Workshop at ECMWF, 14 - 18 February 2022, 2022 URSI International Union of Radio Science"
    },
    {
        "paper id": "2404.16848",
        "abstract url": "https://arxiv.org/abs/2404.16848",
        "title": "Cyber Security issues and Blockchain-Deep Learning based solutions for UAV and Internet of Drones (FANETs)",
        "rating": -5,
        "keywords": [
            [
                "attack"
            ],
            [
                "industrial"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Safety-critical systems such as automated embedded or industrial systems have a strong dependency on the trustworthiness of data collection. As sensors are the critical component for those systems, it is imperative to address the attack resilience of sensors",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18049",
        "abstract url": "https://arxiv.org/abs/2402.18049",
        "title": "Performance modeling of public permissionless blockchains: A survey",
        "rating": -10,
        "keywords": [],
        "abstract": "Public permissionless blockchains facilitate peer-to-peer digital transactions, yet face performance challenges specifically minimizing transaction confirmation time to decrease energy and time consumption per transaction. Performance evaluation and prediction are crucial in achieving this objective, with performance modeling as a key solution despite the complexities involved in assessing these blockchains. This survey examines prior research concerning the performance modeling blockchain systems, specifically focusing on public permissionless blockchains. Initially, it provides foundational knowledge about these blockchains and the crucial performance parameters for their assessment. Additionally, the study delves into research on the performance modeling of public permissionless blockchains, predominantly considering these systems as bulk service queues. It also examines prior studies on workload and traffic modeling, characterization, and analysis within these blockchain networks. By analyzing existing research, our survey aims to provide insights and recommendations for researchers keen on enhancing the performance of public permissionless blockchains or devising novel mechanisms in this domain.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "28 pages, 8 figures"
    },
    {
        "paper id": "2402.18064",
        "abstract url": "https://arxiv.org/abs/2402.18064",
        "title": "Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning",
        "rating": -10,
        "keywords": [],
        "abstract": "The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate multiple hypotheses correctly. Its effectiveness is also demonstrated on real datasets. The technique is able to identify and leverage hypotheses which show a medium or strong correlation to reduce prediction error by a factor of 1.4--3.4 within the first 7 samples, and poor hypotheses are quickly identified and rejected eventually having no adverse effect.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted for publication and presentation at ICRA 2024"
    },
    {
        "paper id": "2402.18070",
        "abstract url": "https://arxiv.org/abs/2402.18070",
        "title": "A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless Baseband Processing",
        "rating": -10,
        "keywords": [],
        "abstract": "Wireless baseband processing (WBP) is a key element of wireless communications, with a series of signal processing modules to improve data throughput and counter channel fading. Conventional hardware solutions, such as digital signal processors (DSPs) and more recently, graphic processing units (GPUs), provide various degrees of parallelism, yet they both fail to take into account the cyclical and consecutive character of WBP. Furthermore, the large amount of data in WBPs cannot be processed quickly in symmetric multiprocessors (SMPs) due to the unpredictability of memory latency. To address this issue, we propose a hierarchical dataflow-driven architecture to accelerate WBP. A pack-and-ship approach is presented under a non-uniform memory access (NUMA) architecture to allow the subordinate tiles to operate in a bundled access and execute manner. We also propose a multi-level dataflow model and the related scheduling scheme to manage and allocate the heterogeneous hardware resources. Experiment results demonstrate that our prototype achieves $2\\times$ and $2.3\\times$ speedup in terms of normalized throughput and single-tile clock cycles compared with GPU and DSP counterparts in several critical WBP benchmarks. Additionally, a link-level throughput of $288$ Mbps can be achieved with a $45$-core configuration.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "7 pages, 7 figures, conference"
    },
    {
        "paper id": "2402.18074",
        "abstract url": "https://arxiv.org/abs/2402.18074",
        "title": "A One-step Image Retargeing Algorithm Based on Conformal Energy",
        "rating": -10,
        "keywords": [],
        "abstract": "The image retargeting problem is to find a proper mapping to resize an image to one with a prescribed aspect ratio, which is quite popular these days. In this paper, we propose an efficient and orientation-preserving one-step image retargeting algorithm based on minimizing the harmonic energy, which can well preserve the regions of interest (ROIs) and line structures in the image. We also give some mathematical proofs in the paper to ensure the well-posedness and accuracy of our algorithm.",
        "subjects": [
            "math.NA"
        ],
        "comment": "24 pages, 10 figures"
    },
    {
        "paper id": "2402.18089",
        "abstract url": "https://arxiv.org/abs/2402.18089",
        "title": "PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators",
        "rating": -10,
        "keywords": [],
        "abstract": "Processing-in-memory (PIM) has shown extraordinary potential in accelerating neural networks. To evaluate the performance of PIM accelerators, we present an ISA-based simulation framework including a dedicated ISA targeting neural networks running on PIM architectures, a compiler, and a cycleaccurate configurable simulator. Compared with prior works, this work decouples software algorithms and hardware architectures through the proposed ISA, providing a more convenient way to evaluate the effectiveness of software/hardware optimizations. The simulator adopts an event-driven simulation approach and has better support for hardware parallelism. The framework is open-sourced at https://github.com/wangxy-2000/pimsim-nn.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18093",
        "abstract url": "https://arxiv.org/abs/2402.18093",
        "title": "ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection",
        "rating": -10,
        "keywords": [],
        "abstract": "The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in spam filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as spam, risking the possibility of missing important communications or mistakenly trusting phishing emails. This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails. By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems. We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18094",
        "abstract url": "https://arxiv.org/abs/2402.18094",
        "title": "On the Existence of Cyclic Lattice Codes",
        "rating": -10,
        "keywords": [],
        "abstract": "A coding lattice $\u039b_c$ and a shaping lattice $\u039b_s$ forms a nested lattice code $\\mathcal{C}$ if $\u039b_s \\subseteq \u039b_c$. Under some conditions, $\\mathcal{C}$ is a finite cyclic group formed by rectangular encoding. This paper presents the conditions for the existence of such $\\mathcal{C}$ and provides some designs. These designs correspond to solutions to linear Diophantine equations so that a cyclic lattice code $\\mathcal C$ of arbitrary codebook size $M$ can possess group isomorphism, which is an essential property for a nested lattice code to be applied in physical layer network relaying techniques such as compute and forward.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "5 pages, isit 2024 conference"
    },
    {
        "paper id": "2402.18107",
        "abstract url": "https://arxiv.org/abs/2402.18107",
        "title": "Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning for Review Helpfulness Prediction",
        "rating": -10,
        "keywords": [],
        "abstract": "In line with the latest research, the task of identifying helpful reviews from a vast pool of user-generated textual and visual data has become a prominent area of study. Effective modal representations are expected to possess two key attributes: consistency and differentiation. Current methods designed for Multimodal Review Helpfulness Prediction (MRHP) face limitations in capturing distinctive information due to their reliance on uniform multimodal annotation. The process of adding varied multimodal annotations is not only time-consuming but also labor-intensive. To tackle these challenges, we propose an auto-generated scheme based on multi-task learning to generate pseudo labels. This approach allows us to simultaneously train for the global multimodal interaction task and the separate cross-modal interaction subtasks, enabling us to learn and leverage both consistency and differentiation effectively. Subsequently, experimental results validate the effectiveness of pseudo labels, and our approach surpasses previous textual and multimodal baseline models on two widely accessible benchmark datasets, providing a solution to the MRHP problem.",
        "subjects": [
            "cs.MM"
        ],
        "comment": "10 pages,4 figures, 4 tables"
    },
    {
        "paper id": "2402.18110",
        "abstract url": "https://arxiv.org/abs/2402.18110",
        "title": "The Logarithmic Random Bidding for the Parallel Roulette Wheel Selection with Precise Probabilities",
        "rating": -10,
        "keywords": [],
        "abstract": "The roulette wheel selection is a critical process in heuristic algorithms, enabling the probabilistic choice of items based on assigned fitness values. It selects an item with a probability proportional to its fitness value. This technique is commonly employed in ant-colony algorithms to randomly determine the next city to visit when solving the traveling salesman problem. Our study focuses on parallel algorithms designed to select one of multiple processors, each associated with fitness values, using random wheel selection. We propose a novel approach called logarithmic random bidding, which achieves an expected runtime logarithmic to the number of processors with non-zero fitness values, using the CRCW-PRAM model with a shared memory of constant size. Notably, the logarithmic random bidding technique demonstrates efficient performance, particularly in scenarios where only a few processors are assigned non-zero fitness values.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18116",
        "abstract url": "https://arxiv.org/abs/2402.18116",
        "title": "Block and Detail: Scaffolding Sketch-to-Image Generation",
        "rating": -10,
        "keywords": [],
        "abstract": "We introduce a novel sketch-to-image tool that aligns with the iterative refinement process of artists. Our tool lets users sketch blocking strokes to coarsely represent the placement and form of objects and detail strokes to refine their shape and silhouettes. We develop a two-pass algorithm for generating high-fidelity images from such sketches at any point in the iterative process. In the first pass we use a ControlNet to generate an image that strictly follows all the strokes (blocking and detail) and in the second pass we add variation by renoising regions surrounding blocking strokes. We also present a dataset generation scheme that, when used to train a ControlNet architecture, allows regions that do not contain strokes to be interpreted as not-yet-specified regions rather than empty space. We show that this partial-sketch-aware ControlNet can generate coherent elements from partial sketches that only contain a small number of strokes. The high-fidelity images produced by our approach serve as scaffolds that can help the user adjust the shape and proportions of objects or add additional elements to the composition. We demonstrate the effectiveness of our approach with a variety of examples and evaluative comparisons.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "12 pages, 13 figures"
    },
    {
        "paper id": "2402.18119",
        "abstract url": "https://arxiv.org/abs/2402.18119",
        "title": "Modeling and Analysis of Crypto-Backed Over-Collateralized Stable Derivatives in DeFi",
        "rating": -10,
        "keywords": [],
        "abstract": "In decentralized finance (DeFi), stablecoins like DAI are designed to offer a stable value amidst the fluctuating nature of cryptocurrencies. We examine the class of crypto-backed stable derivatives, with a focus on mechanisms for price stabilization, which is exemplified by the well-known stablecoin DAI from MakerDAO. For simplicity, we focus on a single-collateral setting. We introduce a belief parameter to the simulation model of DAI in a previous work (DAISIM), reflecting market sentiments about the value and stability of DAI, and show that it better matches the expected behavior when this parameter is set to a sufficiently high value. We also propose a simple mathematical model of DAI price to explain its stability and dependency on ETH price. Finally, we analyze possible risk factors associated with these stable derivatives to provide valuable insights for stakeholders in the DeFi ecosystem.",
        "subjects": [
            "q-fin.RM"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18182",
        "abstract url": "https://arxiv.org/abs/2402.18182",
        "title": "Handling Open Research Data within the Max Planck Society -- Looking Closer at the Year 2020",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper analyses the practice of publishing research data within the Max Planck Society in the year 2020. The central finding of the study is that up to 40\\% of the empirical text publications had research data available. The aggregation of the available data is predominantly analysed. There are differences between the sections of the Max Planck Society but they are not as great as one might expect. In the case of the journals, it is also apparent that a data policy can increase the availability of data related to textual publications. Finally, we found that the statement on data availability \"upon (reasonable) request\" does not work.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18183",
        "abstract url": "https://arxiv.org/abs/2402.18183",
        "title": "Computational Offloading in Semantic-Aware Cloud-Edge-End Collaborative Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "The trend of massive connectivity pushes forward the explosive growth of end devices. The emergence of various applications has prompted a demand for pervasive connectivity and more efficient computing paradigms. On the other hand, the lack of computational capacity of the end devices restricts the implementation of the intelligent applications, and becomes a bottleneck of the multiple access for supporting massive connectivity. Mobile cloud computing (MCC) and mobile edge computing (MEC) techniques enable end devices to offload local computation-intensive tasks to servers by networks. In this paper, we consider the cloud-edge-end collaborative networks to utilize distributed computing resources. Furthermore, we apply task-oriented semantic communications to tackle the fast-varying channel between the end devices and MEC servers and reduce the communication cost. To minimize long-term energy consumption on constraints queue stability and computational delay, a Lyapunov-guided deep reinforcement learning hybrid (DRLH) framework is proposed to solve the mixed integer non-linear programming (MINLP) problem. The long-term energy consumption minimization problem is transformed into the deterministic problem in each time frame. The DRLH framework integrates a model-free deep reinforcement learning algorithm with a model-based mathematical optimization algorithm to mitigate computational complexity and leverage the scenario information, so that improving the convergence performance. Numerical results demonstrate that the proposed DRLH framework achieves near-optimal performance on energy consumption while stabilizing all queues.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IEEE JSTSP"
    },
    {
        "paper id": "2402.18189",
        "abstract url": "https://arxiv.org/abs/2402.18189",
        "title": "VulMCI : Code Splicing-based Pixel-row Oversampling for More Continuous Vulnerability Image Generation",
        "rating": -10,
        "keywords": [],
        "abstract": "In recent years, the rapid development of deep learning technology has brought new prospects to the field of vulnerability detection. Many vulnerability detection methods involve converting source code into images for detection, yet they often overlook the quality of the generated images. Due to the fact that vulnerability images lack clear and continuous contours, unlike images used in object detection, Convolutional Neural Networks (CNNs) tend to lose semantic information during the convolution and pooling processes. Therefore, this paper proposes a pixel row oversampling method based on code line concatenation to generate more continuous code features, addressing the issue of discontinuity in code image coloration.Building upon these contributions, we propose the vulnerability detection system VulMCI and conduct tests on the SARD and NVD datasets. Experimental results demonstrate that VulMCI outperforms seven state-of-the-art vulnerability detectors (namely Checkmarx, FlawFinder, RATS, VulDeePecker, SySeVR, VulCNN, and Devign). Compared to other image-based methods, VulMCI shows improvements in various metrics, including a 2.877\\% increase in True Positive Rate (TPR), a 5.446\\% increase in True Negative Rate (TNR), and a 5.91\\% increase in Accuracy (ACC). On the NVD real-world dataset, VulMCI achieves an average accuracy of 5.162\\%, confirming its value in practical vulnerability detection applications.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18193",
        "abstract url": "https://arxiv.org/abs/2402.18193",
        "title": "Counting points with Riemann-Roch formulas",
        "rating": -10,
        "keywords": [],
        "abstract": "We provide an algorithm for computing the number of integral points lying in certain triangles that do not have integral vertices. We use techniques from Algebraic Geometry such as the Riemann-Roch formula for weighted projective planes and resolution of singularities. We analyze the complexity of the method and show that the worst case is given by the Fibonacci sequence. At the end of the manuscript a concrete example is developed in detail where the interplay with other invariants of singularity theory is also treated.",
        "subjects": [
            "math.AG"
        ],
        "comment": "35 pages"
    },
    {
        "paper id": "2402.18194",
        "abstract url": "https://arxiv.org/abs/2402.18194",
        "title": "Formalized Identification Of Key Factors In Safety-Relevant Failure Scenarios",
        "rating": -10,
        "keywords": [],
        "abstract": "This research article presents a methodical data-based approach to systematically identify key factors in safety-related failure scenarios, with a focus on complex product-environmental systems in the era of Industry 4.0. The study addresses the uncertainty arising from the growing complexity of modern products. The method uses scenario analysis and focuses on failure analysis within technical product development. The approach involves a derivation of influencing factors based on information from failure databases. The failures described here are documented individually in failure sequence diagrams and then related to each other in a relationship matrix. This creates a network of possible failure scenarios from individual failure cases that can be used in product development. To illustrate the application of the methodology, a case study of 41 Rapex safety alerts for a hair dryer is presented. The failure sequence diagrams and influencing factor relationship matrices show 46 influencing factors that lead to safety-related failures. The predominant harm is burns and electric shocks, which are highlighted by the active and passive sum diagrams. The research demonstrates a robust method for identifying key factors in safety-related failure scenarios using information from failure databases. The methodology provides valuable insights into product development and emphasizes the frequency of influencing factors and their interconnectedness.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18224",
        "abstract url": "https://arxiv.org/abs/2402.18224",
        "title": "Utilization of Reconfigurable Intelligent Surfaces with Context Information: Use Cases",
        "rating": -10,
        "keywords": [],
        "abstract": "In terms of complex radio environments especially in dense urban areas, a very interesting topic is considered - the utilization of reconfigurable intelligent surfaces. Basically, based on simple controls of the angle of reflection of the signal from the surface, it is possible to achieve different effects in a radio communication system. Maximizing or minimizing the received power at specific locations near the reflecting surface is the most important effect. Thanks to this, it is possible to: receive a signal in a place where it was not possible, detect spectrum occupancy in a place where the sensor could not make a correct detection, or minimize interference in a specific receiver. In this paper, all three concepts are presented, and, using a simple ray tracing simulation, the potential profit in each scenario is shown. In addition, a scenario was analyzed in which several of the aforementioned situations are combined.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "4 pages, 7 figures, English language"
    },
    {
        "paper id": "2402.18231",
        "abstract url": "https://arxiv.org/abs/2402.18231",
        "title": "Joint Beamforming Design and Stream Allocation for Non-Coherent Joint Transmission in Cell-Free MIMO Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider joint beamforming and stream allocation to maximize the weighted sum rate (WSR) for non-coherent joint transmission (NCJT) in user-centric cell-free MIMO networks, where distributed access points (APs) are organized in clusters to transmit different signals to serve each user equipment (UE). We for the first time consider the common limits of maximum number of receive streams at UEs in practical networks, and formulate a joint beamforming and transmit stream allocation problem for WSR maximization under per-AP transmit power constraints. Since the integer number of transmit streams determines the dimension of the beamformer, the joint optimization problem is mixed-integer and nonconvex with coupled decision variables that is inherently NP-hard. In this paper, we first propose a distributed low-interaction reduced weighted minimum mean square error (RWMMSE) beamforming algorithm for WSR maximization with fixed streams. Our proposed RWMMSE algorithm requires significantly less interaction across the network and has the current lowest computational complexity that scales linearly with the number of transmit antennas, without any compromise on WSR. We draw insights on the joint beamforming and stream allocation problem to decouple the decision variables and relax the mixed-integer constraints. We then propose a joint beamforming and linear stream allocation algorithm, termed as RWMMSE-LSA, which yields closed-form updates with linear stream allocation complexity and is guaranteed to converge to the stationary points of the original joint optimization problem. Simulation results demonstrate substantial performance gain of our proposed algorithms over the current best alternatives in both WSR performance and convergence time.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18255",
        "abstract url": "https://arxiv.org/abs/2402.18255",
        "title": "How open are hybrid journals included in transformative agreements?",
        "rating": -10,
        "keywords": [],
        "abstract": "The ongoing controversy surrounding transformative agreements, which aim to transition journal publishing to full open access, highlight the need for large-scale studies assessing the impact of these agreements on hybrid open access. By combining publicly available data from various sources, including cOAlition S Journal Checker, Crossref, and OpenAlex, this study presents a novel approach that analyses over 700 agreements. Results suggest a strong growth in open access between 2018 and 2022 from 4.3% to 15%. During this period, 11,189 hybrid journals provided open access to 742,369 out of 8,146,958 articles, representing a five-year open access proportion of 9.1%. Authors who could make use of transformative agreements at the time of publication contributed 328,957 open access articles. In 2022, 143,615 out of 249,511 open access articles in hybrid journals or 58% were enabled by transformative agreements. This trend was largely driven by the three commercial publishers Elsevier, Springer Nature, and Wiley, but the open access uptake varied substantially across journals, publishers, disciplines, and country affiliations. In particular, the OECD and BRICS areas revealed different publication trends. In conclusion, this study suggests that current levels of implementation of transformative agreements is insufficient to bring about a large-scale transition to full open access.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "37 pages, 7 figures"
    },
    {
        "paper id": "2402.18256",
        "abstract url": "https://arxiv.org/abs/2402.18256",
        "title": "The Cost of Permissionless Liquidity Provision in Automated Market Makers",
        "rating": -10,
        "keywords": [],
        "abstract": "Automated market makers (AMMs) allocate fee revenue proportional to the amount of liquidity investors deposit. In this paper, we study the economic consequences of the competition between passive liquidity providers (LPs) caused by this allocation rule. We employ a game-theoretic model in which $N$ strategic agents optimally provide liquidity. In this setting, we find that competition drives LPs to provide excess liquidity. In the limit, the excess liquidity converges to a constant that linearly increases with the amount of base demand, demand that is insensitive to trading costs. Providing excess liquidity is costly as more capital is exposed to adverse selection costs, leading to a loss in welfare. Our main result is that the price of anarchy, defined over the liquidity provider performance, is $O(N)$, implying that the welfare loss scales linearly with the number of liquidity providers. We show that this result is still observable when using richer aggregate demand models.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18263",
        "abstract url": "https://arxiv.org/abs/2402.18263",
        "title": "Max-Cut with $\u03b5$-Accurate Predictions",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the approximability of the MaxCut problem in the presence of predictions. Specifically, we consider two models: in the noisy predictions model, for each vertex we are given its correct label in $\\{-1,+1\\}$ with some unknown probability $1/2 + \u03b5$, and the other (incorrect) label otherwise. In the more-informative partial predictions model, for each vertex we are given its correct label with probability $\u03b5$ and no label otherwise. We assume only pairwise independence between vertices in both models. We show how these predictions can be used to improve on the worst-case approximation ratios for this problem. Specifically, we give an algorithm that achieves an $\u03b1+ \\widetilde\u03a9(\u03b5^4)$-approximation for the noisy predictions model, where $\u03b1\\approx 0.878$ is the MaxCut threshold. While this result also holds for the partial predictions model, we can also give a $\u03b2+ \u03a9(\u03b5)$-approximation, where $\u03b2\\approx 0.858$ is the approximation ratio for MaxBisection given by Raghavendra and Tan. This answers a question posed by Ola Svensson in his plenary session talk at SODA'23.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2402.18276",
        "abstract url": "https://arxiv.org/abs/2402.18276",
        "title": "Fractional Linear Matroid Matching is in quasi-NC",
        "rating": -10,
        "keywords": [],
        "abstract": "The matching and linear matroid intersection problems are solvable in quasi-NC, meaning that there exist deterministic algorithms that run in polylogarithmic time and use quasi-polynomially many parallel processors. However, such a parallel algorithm is unknown for linear matroid matching, which generalizes both of these problems. In this work, we propose a quasi-NC algorithm for fractional linear matroid matching, which is a relaxation of linear matroid matching and commonly generalizes fractional matching and linear matroid intersection. Our algorithm builds upon the connection of fractional matroid matching to non-commutative Edmonds' problem recently revealed by Oki and Soma~(2023). As a corollary, we also solve black-box non-commutative Edmonds' problem with rank-two skew-symmetric coefficients.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18288",
        "abstract url": "https://arxiv.org/abs/2402.18288",
        "title": "Development of Context-Sensitive Formulas to Obtain Constant Luminance Perception for a Foreground Object in Front of Backgrounds of Varying Luminance",
        "rating": -10,
        "keywords": [],
        "abstract": "In this article, we present a framework for developing context-sensitive luminance correction formulas that can produce constant luminance perception for foreground objects. Our formulas make the foreground object slightly translucent to mix with the blurred version of the background. This mix can quickly produce any desired illusion of luminance in foreground objects based on the luminance of the background. The translucency formula has only one parameter; the relative size of the foreground object, which is a number between zero and one. We have identified the general structure of the translucency formulas as a power function of the relative size of the foreground object. We have implemented a web-based interactive program in Shadertoy. Using this program, we determined the coefficients of the polynomial exponents of the power function. To intuitively control the coefficients of the polynomial functions, we have used a B\u00e9zier form. Our final translucency formula uses a quadratic polynomial and requires only three coefficients. We also identified a simpler affine formula, which requires only two coefficients. We made our program publicly available in Shadertoy so that anyone can access and improve it. In this article, we also explain how to intuitively change the polynomial part of the formula. Using our explanation, users change the polynomial part of the formula to obtain their own perceptively constant luminance. This can be used as a crowd-sourcing experiment for further improvement of the formula.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "20 pages"
    },
    {
        "paper id": "2402.18301",
        "abstract url": "https://arxiv.org/abs/2402.18301",
        "title": "Quantification and Modeling of Broken Links Prevalence in Hyper Traffic Websites Homepages",
        "rating": -10,
        "keywords": [],
        "abstract": "Broken links in websites external resources pose a serious threat to cybersecurity and the credibility of websites. They can be hijacked to eavesdrop user traffic or to inject malicious software. In this paper, we present the first result of an ongoing research. We focus on the prevalence of broken links in external resources on home pages of the most visited websites in the world. The analysis was conducted on the top 88 000 homepages extracted from the Majestic Million rankings. 35,2% of them have at least one broken link. We also identify the common causes of these broken links and highlight improper implementation of testing phases to prevent such errors. We provide a formal model for the distribution of external links. At the next research step, we are exploring the potential impact on privacy of broken links by analyzing inherited traffic of purchasable expired domains.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "4 pages, 3 tables, 1 figure"
    },
    {
        "paper id": "2402.18355",
        "abstract url": "https://arxiv.org/abs/2402.18355",
        "title": "COPR -- Efficient, large-scale log storage and retrieval",
        "rating": -10,
        "keywords": [],
        "abstract": "Modern, large scale monitoring systems have to process and store vast amounts of log data in near real-time. At query time the systems have to find relevant logs based on the content of the log message using support structures that can scale to these amounts of data while still being efficient to use. We present our novel Compressed Probabilistic Retrieval algorithm (COPR), capable of answering Multi-Set Multi-Membership-Queries, that can be used as an alternative to existing indexing structures for streamed log data. In our experiments, COPR required up to 93% less storage space than the tested state-of-the-art inverted index and had up to four orders of magnitude less false-positives than the tested state-of-the-art membership sketch. Additionally, COPR achieved up to 250 times higher query throughput than the tested inverted index and up to 240 times higher query throughput than the tested membership sketch.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "14 pages, 8 figures"
    },
    {
        "paper id": "2402.18360",
        "abstract url": "https://arxiv.org/abs/2402.18360",
        "title": "Similarity-based analogical proportions",
        "rating": -10,
        "keywords": [],
        "abstract": "The author has recently introduced abstract algebraic frameworks of analogical proportions and similarity within the general setting of universal algebra. The purpose of this paper is to build a bridge from similarity to analogical proportions by formulating the latter in terms of the former. The benefit of this similarity-based approach is that the connection between proportions and similarity is built into the framework and therefore evident which is appealing since proportions and similarity are both at the center of analogy; moreover, future results on similarity can directly be applied to analogical proportions.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18375",
        "abstract url": "https://arxiv.org/abs/2402.18375",
        "title": "Low-Modeling of Software Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "There is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems. New types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle. In the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage. In this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18386",
        "abstract url": "https://arxiv.org/abs/2402.18386",
        "title": "TrustRate: A Decentralized Platform for Hijack-Resistant Anonymous Reviews",
        "rating": -10,
        "keywords": [],
        "abstract": "Reviews and ratings by users form a central component in several widely used products today (e.g., product reviews, ratings of online content, etc.), but today's platforms for managing such reviews are ad-hoc and vulnerable to various forms of tampering and hijack by fake reviews either by bots or motivated paid workers. We define a new metric called 'hijack-resistance' for such review platforms, and then present TrustRate, an end-to-end decentralized, hijack-resistant platform for authentic, anonymous, tamper-proof reviews. With a prototype implementation and evaluation at the scale of thousands of nodes, we demonstrate the efficacy and performance of our platform, towards a new paradigm for building products based on trusted reviews by end users without having to trust a single organization that manages the reviews.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "23 pages"
    },
    {
        "paper id": "2402.18387",
        "abstract url": "https://arxiv.org/abs/2402.18387",
        "title": "Precoding for Multi-Cell ISAC: from Coordinated Beamforming to Coordinated Multipoint and Bi-Static Sensing",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes a framework for designing robust precoders for a multi-input single-output (MISO) system that performs integrated sensing and communication (ISAC) across multiple cells and users. We use Cramer-Rao-Bound (CRB) to measure the sensing performance and derive its expressions for two multi-cell scenarios, namely coordinated beamforming (CBF) and coordinated multi-point (CoMP). In the CBF scheme, a BS shares channel state information (CSI) and estimates target parameters using monostatic sensing. In contrast, a BS in the CoMP scheme shares the CSI and data, allowing bistatic sensing through inter-cell reflection. We consider both block-level (BL) and symbol-level (SL) precoding schemes for both the multi-cell scenarios that are robust to channel state estimation errors. The formulated optimization problems to minimize the CRB in estimating the parameters of a target and maximize the minimum communication signal-to-interference-plus-noise-ratio (SINR) while satisfying a given total transmit power budget are non-convex. We tackle the non-convexity using a combination of semidefinite relaxation (SDR) and alternating optimization (AO) techniques. Simulations suggest that neglecting the inter-cell reflection and communication links degrades the performance of an ISAC system. The CoMP scenario employing SL precoding performs the best, whereas the BL precoding applied in the CBF scenario produces relatively high estimation error for a given minimum SINR value.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "13 pages, 9 figures, subnitted to IEEE Trans. on Wireless Communications"
    },
    {
        "paper id": "2402.18391",
        "abstract url": "https://arxiv.org/abs/2402.18391",
        "title": "Sound Concurrent Traces for Online Monitoring Technical Report",
        "rating": -10,
        "keywords": [],
        "abstract": "Monitoring concurrent programs typically rely on collecting traces to abstract program executions. However, existing approaches targeting general behavioral properties are either not tailored for online monitoring, are no longer maintained, or implement naive instrumentation that often leads to unsound verdicts. We first define the notion of when a trace is representative of a concurrent execution. We then present a non-blocking vector clock algorithm to collect sound concurrent traces on the fly reflecting the partial order between events. Moreover, concurrent events in the representative trace pose a soundness problem for monitors synthesized from total order formalisms. For this, we extract a causal dependence relation from the monitor to check if the trace has the needed orderings and define the conditions to decide at runtime when a collected trace is monitorable. We implement our contributions in a tool, FACTS, which instruments programs compiling to Java bytecode, constructs sound representative traces, and warns the monitor about non-monitorable traces. We evaluate our work and compare it with existing approaches.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18405",
        "abstract url": "https://arxiv.org/abs/2402.18405",
        "title": "Multi-cell Coordinated Joint Sensing and Communications",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes block-level precoder (BLP) designs for a multi-input single-output (MISO) system that performs joint sensing and communication across multiple cells and users. The Cramer-Rao-Bound for estimating a target's azimuth angle is determined for coordinated beamforming (CBF) and coordinated multi-point (CoMP) scenarios while considering inter-cell communication and sensing links. The formulated optimization problems to minimize the CRB and maximize the minimum-signal-to-interference-plus-noise-ratio (SINR) are non-convex and are represented in the semidefinite relaxed (SDR) form to solve using an alternate optimization algorithm. The proposed solutions show improved performance compared to the baseline scenario that neglects the signal component from neighboring cells.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "5 pages, 4 figures"
    },
    {
        "paper id": "2402.18415",
        "abstract url": "https://arxiv.org/abs/2402.18415",
        "title": "ECCBO: An Inherently Safe Bayesian Optimization with Embedded Constraint Control for Real-Time Optimization",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper introduces a model-free real-time optimization (RTO) framework based on unconstrained Bayesian optimization with embedded constraint control. The main contribution lies in demonstrating how this approach simplifies the black-box optimization problem while ensuring \"always-feasible\" setpoints, addressing a critical challenge in real-time optimization with unknown cost and constraints. Noting that controlling the constraint does not require detailed process models, the key idea of this paper is to control the constraints to \"some\" setpoint using simple feedback controllers. Bayesian optimization then computes the optimum setpoint for the constraint controllers. By searching over the setpoints for the constraint controllers, as opposed to searching directly over the RTO degrees of freedom, this paper achieves an inherently safe and practical model-free RTO scheme. In particular, this paper shows that the proposed approach can achieve zero cumulative constraint violation without relying on assumptions about the Gaussian process model used in Bayesian optimization. The effectiveness of the proposed approach is demonstrated on a benchmark Williams-Otto reactor example.",
        "subjects": [
            "math.OC"
        ],
        "comment": "IFAC ADCHEM 2024"
    },
    {
        "paper id": "2402.18480",
        "abstract url": "https://arxiv.org/abs/2402.18480",
        "title": "Libfork: portable continuation-stealing with stackless coroutines",
        "rating": -10,
        "keywords": [],
        "abstract": "Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling. The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages -- where it is often impossible without modifying the compiler or resorting to non-portable techniques. We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks. We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks. Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory. Similarly, compared to Intel's TBB, libfork is on average 2.7x faster and consumes 6.2x less memory. Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18484",
        "abstract url": "https://arxiv.org/abs/2402.18484",
        "title": "A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics",
        "rating": -10,
        "keywords": [],
        "abstract": "Due to the rapidly changing climate, the frequency and severity of extreme weather is expected to increase over the coming decades. As fully-resolved climate simulations remain computationally intractable, policy makers must rely on coarse-models to quantify risk for extremes. However, coarse models suffer from inherent bias due to the ignored \"sub-grid\" scales. We propose a framework to non-intrusively debias coarse-resolution climate predictions using neural-network (NN) correction operators. Previous efforts have attempted to train such operators using loss functions that match statistics. However, this approach falls short with events that have longer return period than that of the training data, since the reference statistics have not converged. Here, the scope is to formulate a learning method that allows for correction of dynamics and quantification of extreme events with longer return period than the training data. The key obstacle is the chaotic nature of the underlying dynamics. To overcome this challenge, we introduce a dynamical systems approach where the correction operator is trained using reference data and a coarse model simulation nudged towards that reference. The method is demonstrated on debiasing an under-resolved quasi-geostrophic model and the Energy Exascale Earth System Model (E3SM). For the former, our method enables the quantification of events that have return period two orders longer than the training data. For the latter, when trained on 8 years of ERA5 data, our approach is able to correct the coarse E3SM output to closely reflect the 36-year ERA5 statistics for all prognostic variables and significantly reduce their spatial biases.",
        "subjects": [
            "physics.ao-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18485",
        "abstract url": "https://arxiv.org/abs/2402.18485",
        "title": "A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist",
        "rating": -10,
        "keywords": [],
        "abstract": "Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.",
        "subjects": [
            "q-fin.TR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18498",
        "abstract url": "https://arxiv.org/abs/2402.18498",
        "title": "Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration",
        "rating": -10,
        "keywords": [],
        "abstract": "Although recent developments in generative AI have greatly enhanced the capabilities of conversational agents such as Google's Gemini (formerly Bard) or OpenAI's ChatGPT, it's unclear whether the usage of these agents aids users across various contexts. To better understand how access to conversational AI affects productivity and trust, we conducted a mixed-methods, task-based user study, observing 76 software engineers (N=76) as they completed a programming exam with and without access to Bard. Effects on performance, efficiency, satisfaction, and trust vary depending on user expertise, question type (open-ended \"solve\" vs. definitive \"search\" questions), and measurement type (demonstrated vs. self-reported). Our findings include evidence of automation complacency, increased reliance on the AI over the course of the task, and increased performance for novices on \"solve\"-type questions when using the AI. We discuss common behaviors, design recommendations, and impact considerations to improve collaborations with conversational AI.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "15 pages. Published in the 29th International Conference on Intelligent User Interfaces (IUI '24)"
    },
    {
        "paper id": "2402.18511",
        "abstract url": "https://arxiv.org/abs/2402.18511",
        "title": "Leveraging Compliant Tactile Perception for Haptic Blind Surface Reconstruction",
        "rating": -10,
        "keywords": [],
        "abstract": "Non-flat surfaces pose difficulties for robots operating in unstructured environments. Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions. This study achieves blind surface reconstruction by harnessing the robotic manipulator's kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors. The module's flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects. While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points. These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface. Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy. Moreover, this compliant haptic method works effectively even when the manipulator's approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 9 figures, 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)"
    },
    {
        "paper id": "2402.18526",
        "abstract url": "https://arxiv.org/abs/2402.18526",
        "title": "Mental Models of Meeting Goals: Supporting Intentionality in Meeting Technologies",
        "rating": -10,
        "keywords": [],
        "abstract": "Ineffective meetings due to unclear goals are major obstacles to productivity, yet support for intentionality is surprisingly scant in our meeting and allied workflow technologies. To design for intentionality, we need to understand workers' attitudes and practices around goals. We interviewed 21 employees of a global technology company and identified contrasting mental models of meeting goals: meetings as a means to an end, and meetings as an end in themselves. We explore how these mental models impact how meeting goals arise, goal prioritization, obstacles to considering goals, and how lack of alignment around goals may create tension between organizers and attendees. We highlight the challenges in balancing preparation, constraining scope, and clear outcomes, with the need for intentional adaptability and discovery in meetings. Our findings have implications for designing systems which increase effectiveness in meetings by catalyzing intentionality and reducing tension in the organisation of meetings.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18537",
        "abstract url": "https://arxiv.org/abs/2402.18537",
        "title": "On the enumeration of signatures of XOR-CNF's",
        "rating": -10,
        "keywords": [],
        "abstract": "Given a CNF formula $\\varphi$ with clauses $C_1, \\dots, C_m$ over a set of variables $V$, a truth assignment $\\mathbf{a} : V \\to \\{0, 1\\}$ generates a binary sequence $\u03c3_\\varphi(\\mathbf{a})=(C_1(\\mathbf{a}), \\ldots, C_m(\\mathbf{a}))$, called a signature of $\\varphi$, where $C_i(\\mathbf{a})=1$ if clause $C_i$ evaluates to 1 under assignment $\\mathbf{a}$, and $C_i(\\mathbf{a})=0$ otherwise. Signatures and their associated generation problems have given rise to new yet promising research questions in algorithmic enumeration. In a recent paper, B\u00e9rczi et al. interestingly proved that generating signatures of a CNF is tractable despite the fact that verifying a solution is hard. They also showed the hardness of finding maximal signatures of an arbitrary CNF due to the intractability of satisfiability in general. Their contribution leaves open the problem of efficiently generating maximal signatures for tractable classes of CNFs, i.e., those for which satisfiability can be solved in polynomial time. Stepping into that direction, we completely characterize the complexity of generating all, minimal, and maximal signatures for XOR-CNFs.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "21 pages, 5 figures"
    },
    {
        "paper id": "2402.18554",
        "abstract url": "https://arxiv.org/abs/2402.18554",
        "title": "Extended Kalman filter -- Koopman operator for tractable stochastic optimal control",
        "rating": -10,
        "keywords": [],
        "abstract": "The theory of dual control was introduced more than seven decades ago. Although it has provided rich insights to the fields of control, estimation, and system identification, dual control is generally computationally prohibitive. In recent years, however, the use of Koopman operator theory for control applications has been emerging. This paper presents a new reformulation of the stochastic optimal control problem that, employing the Koopman operator, yields a standard LQR problem with the dual control as its solution. We provide a numerical example that demonstrates the effectiveness of the proposed approach compared with certainty equivalence control, when applied to systems with varying observability.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2402.18616",
        "abstract url": "https://arxiv.org/abs/2402.18616",
        "title": "JCLEC-MO: a Java suite for solving many-objective optimization engineering problems",
        "rating": -10,
        "keywords": [],
        "abstract": "Although metaheuristics have been widely recognized as efficient techniques to solve real-world optimization problems, implementing them from scratch remains difficult for domain-specific experts without programming skills. In this scenario, metaheuristic optimization frameworks are a practical alternative as they provide a variety of algorithms composed of customized elements, as well as experimental support. Recently, many engineering problems require to optimize multiple or even many objectives, increasing the interest in appropriate metaheuristic algorithms and frameworks that might integrate new specific requirements while maintaining the generality and reusability principles they were conceived for. Based on this idea, this paper introduces JCLEC-MO, a Java framework for both multi- and many-objective optimization that enables engineers to apply, or adapt, a great number of multi-objective algorithms with little coding effort. A case study is developed and explained to show how JCLEC-MO can be used to address many-objective engineering problems, often requiring the inclusion of domain-specific elements, and to analyze experimental outcomes by means of conveniently connected R utilities.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "41 pages, 5 figures, journal paper"
    },
    {
        "paper id": "2402.18652",
        "abstract url": "https://arxiv.org/abs/2402.18652",
        "title": "Spatial Variation-Aware Read Disturbance Defenses: Experimental Analysis of Real DRAM Chips and Implications on Future Solutions",
        "rating": -10,
        "keywords": [],
        "abstract": "Read disturbance in modern DRAM chips is a widespread phenomenon and is reliably used for breaking memory isolation, a fundamental building block for building robust systems. RowHammer and RowPress are two examples of read disturbance in DRAM where repeatedly accessing (hammering) or keeping active (pressing) a memory location induces bitflips in other memory locations. Unfortunately, shrinking technology node size exacerbates read disturbance in DRAM chips over generations. As a result, existing defense mechanisms suffer from significant performance and energy overheads, limited effectiveness, or prohibitively high hardware complexity. In this paper, we tackle these shortcomings by leveraging the spatial variation in read disturbance across different memory locations in real DRAM chips. To do so, we 1) present the first rigorous real DRAM chip characterization study of spatial variation of read disturbance and 2) propose Sv\u00e4rd, a new mechanism that dynamically adapts the aggressiveness of existing solutions based on the row-level read disturbance profile. Our experimental characterization on 144 real DDR4 DRAM chips representing 10 chip designs demonstrates a large variation in read disturbance vulnerability across different memory locations: in the part of memory with the worst read disturbance vulnerability, 1) up to 2x the number of bitflips can occur and 2) bitflips can occur at an order of magnitude fewer accesses, compared to the memory locations with the least vulnerability to read disturbance. Sv\u00e4rd leverages this variation to reduce the overheads of five state-of-the-art read disturbance solutions, and thus significantly increases system performance.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "A shorter version of this work is to appear at the 30th IEEE International Symposium on High-Performance Computer Architecture (HPCA-30), 2024"
    },
    {
        "paper id": "2402.18683",
        "abstract url": "https://arxiv.org/abs/2402.18683",
        "title": "Integrated Sensing and Communication Meets Smart Propagation Engineering: Opportunities and Challenges",
        "rating": -10,
        "keywords": [],
        "abstract": "Both smart propagation engineering as well as integrated sensing and communication (ISAC) constitute promising candidates for next-generation (NG) mobile networks. We provide a synergistic view of these technologies, and explore their mutual benefits. First, moving beyond just intelligent surfaces, we provide a holistic view of the engineering aspects of smart propagation environments. By delving into the fundamental characteristics of intelligent surfaces, fluid antennas, and unmanned aerial vehicles, we reveal that more efficient control of the pathloss and fading can be achieved, thus facilitating intrinsic integration and mutual assistance between sensing and communication functionalities. In turn, with the exploitation of the sensing capabilities of ISAC to orchestrate the efficient configuration of radio environments, both the computational effort and signaling overheads can be reduced. We present indicative simulation results, which verify that cooperative smart propagation environment design significantly enhances the ISAC performance. Finally, some promising directions are outlined for combining ISAC with smart propagation engineering.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "7 pages, 5 figures, submitted to IEEE journal for possible publication"
    },
    {
        "paper id": "2402.18688",
        "abstract url": "https://arxiv.org/abs/2402.18688",
        "title": "Exploring AI Problem Formulation with Children via Teachable Machines",
        "rating": -10,
        "keywords": [],
        "abstract": "Emphasizing problem formulation in AI literacy activities with children is vital, yet we lack empirical studies on their structure and affordances. We propose that participatory design involving teachable machines facilitates problem formulation activities. To test this, we integrated problem reduction heuristics into storyboarding and invited a university-based intergenerational design team of 10 children (ages 8-13) and 9 adults to co-design a teachable machine. We find that children draw from personal experiences when formulating AI problems; they assume voice and video capabilities, explore diverse machine learning approaches, and plan for error handling. Their ideas promote human involvement in AI, though some are drawn to more autonomous systems. Their designs prioritize values like capability, logic, helpfulness, responsibility, and obedience, and a preference for a comfortable life, family security, inner harmony, and excitement as end-states. We conclude by discussing how these results can inform the design of future participatory AI activities.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18697",
        "abstract url": "https://arxiv.org/abs/2402.18697",
        "title": "Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting",
        "rating": -10,
        "keywords": [],
        "abstract": "A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results. However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF. Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates. When IPF fails to converge on sparse network data, we introduce a principled algorithm that guarantees IPF converges under minimal changes to the network structure. Finally, we conduct experiments with synthetic and real-world data, which demonstrate the practical value of our theoretical and algorithmic contributions.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18702",
        "abstract url": "https://arxiv.org/abs/2402.18702",
        "title": "Characterizing Multimedia Information Environment through Multi-modal Clustering of YouTube Videos",
        "rating": -10,
        "keywords": [],
        "abstract": "This study aims to investigate the comprehensive characterization of information content in multimedia (videos), particularly on YouTube. The research presents a multi-method framework for characterizing multimedia content by clustering signals from various modalities, such as audio, video, and text. With a focus on South China Sea videos as a case study, this approach aims to enhance our understanding of online content, especially on YouTube. The dataset includes 160 videos, and our findings offer insights into content themes and patterns within different modalities of a video based on clusters. Text modality analysis revealed topical themes related to geopolitical countries, strategies, and global security, while video and audio modality analysis identified distinct patterns of signals related to diverse sets of videos, including news analysis/reporting, educational content, and interviews. Furthermore, our findings uncover instances of content repurposing within video clusters, which were identified using the barcode technique and audio similarity assessments. These findings indicate potential content amplification techniques. In conclusion, this study uniquely enhances our current understanding of multimedia content information based on modality clustering techniques.",
        "subjects": [
            "cs.MM"
        ],
        "comment": "14 pages, In the 4th International Conference on SMART MULTIMEDIA, 2024"
    },
    {
        "paper id": "2402.18705",
        "abstract url": "https://arxiv.org/abs/2402.18705",
        "title": "How Platform Exchange and Safeguards Matter: The Case of Sexual Risk in Airbnb and Couchsurfing",
        "rating": -10,
        "keywords": [],
        "abstract": "Recent work in CHI and CSCW has devoted increasing attention to how the design of network hospitality platforms shapes user experiences and relational outcomes. In this article, I interrogate how different risk factors emerge based on the type of exchanges these platforms facilitate. To do so, I juxtapose two prominent network hospitality platforms: one facilitating negotiated exchange (i.e., Airbnb) with another facilitating reciprocal exchange (i.e., Couchsurfing). Homing in on sexual risk, an underexplored form of platform danger, and drawing on interviews with 40 female dual-platform users, I argue that Airbnb's provision of binding negotiated exchange and institutional safeguards reduces risk through three mechanisms: casting initial guest-host relation into a buyer-seller arrangement, stabilizing interactional scripts, and formalizing sexual violence recourse. Conversely, Couchsurfing's focus on reciprocal exchange and lack of safeguards increase sexual precarity for users both on- and off-platform. This study demonstrates how platforms with strong prosocial motivations can jeopardize sociality and concludes with design implications for protecting vulnerable user populations.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18707",
        "abstract url": "https://arxiv.org/abs/2402.18707",
        "title": "Embodied Supervision: Haptic Display of Automation Command to Improve Supervisory Performance",
        "rating": -10,
        "keywords": [],
        "abstract": "A human operator using a manual control interface has ready access to their own command signal, both by efference copy and proprioception. In contrast, a human supervisor typically relies on visual information alone. We propose supplying a supervisor with a copy of the operators command signal, hypothesizing improved performance, especially when that copy is provided through haptic display. We experimentally compared haptic with visual access to the command signal, quantifying the performance of N equals 10 participants attempting to determine which of three reference signals was being tracked by an operator. Results indicate an improved accuracy in identifying the tracked target when haptic display was available relative to visual display alone. We conjecture the benefit follows from the relationship of haptics to the supervisor's own experience, perhaps muscle memory, as an operator.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "IEEE Haptics Symposium 2024"
    },
    {
        "paper id": "2402.18708",
        "abstract url": "https://arxiv.org/abs/2402.18708",
        "title": "Bluebell: An Alliance of Relational Lifting and Independence For Probabilistic Reasoning",
        "rating": -10,
        "keywords": [],
        "abstract": "We present Bluebell, a program logic for reasoning about probabilistic programs where unary and relational styles of reasoning come together to create new reasoning tools. Unary-style reasoning is very expressive and is powered by foundational mechanisms to reason about probabilistic behaviour like independence and conditioning. The relational style of reasoning, on the other hand, naturally shines when the properties of interest compare the behaviour of similar programs (e.g. when proving differential privacy) managing to avoid having to characterize the output distributions of the individual programs. So far, the two styles of reasoning have largely remained separate in the many program logics designed for the deductive verification of probabilistic programs. In Bluebell, we unify these styles of reasoning through the introduction of a new modality called \"joint conditioning\" that can encode and illuminate the rich interaction between conditional independence and relational liftings; the two powerhouses from the two styles of reasoning.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "23 pages + 53 pages of appendix"
    },
    {
        "paper id": "2402.18709",
        "abstract url": "https://arxiv.org/abs/2402.18709",
        "title": "Nonlinear identification algorithm for online and offline study of pulmonary mechanical ventilation",
        "rating": -10,
        "keywords": [],
        "abstract": "This work presents an algorithm for determining the parameters of a nonlinear dynamic model of the respiratory system in patients undergoing assisted ventilation. Using the pressure and flow signals measured at the mouth, the model's quadratic pressure-volume characteristic is fit to this data in each respiratory cycle by appropriate estimates of the model parameters. Parameter changes during ventilation can thus also be detected. The algorithm is first refined and assessed using data derived from simulated patients represented through a sigmoidal pressure-volume characteristic with hysteresis. As satisfactory results are achieved with the simulated data, the algorithm is evaluated with real data obtained from actual patients undergoing assisted ventilation. The proposed nonlinear dynamic model and associated parameter estimation algorithm yield closer fits than the static linear models computed by respiratory machines, with only a minor increase in computation. They also provide more information to the physician, such as the pressure-volume (P-V) curvature and the condition of the lung (whether normal, under-inflated, or over-inflated). This information can be used to provide safer ventilation for patients, for instance by ventilating them in the linear region of the respiratory system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18719",
        "abstract url": "https://arxiv.org/abs/2402.18719",
        "title": "MaxCUCL: Max-Consensus with Deterministic Convergence in Networks with Unreliable Communication",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we present a novel distributed algorithm (herein called MaxCUCL) designed to guarantee that max-consensus is reached in networks characterized by unreliable communication links (i.e., links suffering from packet drops). Our proposed algorithm is the first algorithm that achieves max-consensus in a deterministic manner (i.e., nodes always calculate the maximum of their states regardless of the nature of the probability distribution of the packet drops). Furthermore, it allows nodes to determine whether convergence has been achieved (enabling them to transition to subsequent tasks). The operation of MaxCUCL relies on the deployment of narrowband error-free feedback channels used for acknowledging whether a packet transmission between nodes was successful. We analyze the operation of our algorithm and show that it converges after a finite number of time steps. Finally, we demonstrate our algorithm's effectiveness and practical applicability by applying it to a sensor network deployed for environmental monitoring.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18728",
        "abstract url": "https://arxiv.org/abs/2402.18728",
        "title": "Not All the Same: Understanding and Informing Similarity Estimation in Tile-Based Video Games",
        "rating": -10,
        "keywords": [],
        "abstract": "Similarity estimation is essential for many game AI applications, from the procedural generation of distinct assets to automated exploration with game-playing agents. While similarity metrics often substitute human evaluation, their alignment with our judgement is unclear. Consequently, the result of their application can fail human expectations, leading to e.g. unappreciated content or unbelievable agent behaviour. We alleviate this gap through a multi-factorial study of two tile-based games in two representations, where participants (N=456) judged the similarity of level triplets. Based on this data, we construct domain-specific perceptual spaces, encoding similarity-relevant attributes. We compare 12 metrics to these spaces and evaluate their approximation quality through several quantitative lenses. Moreover, we conduct a qualitative labelling study to identify the features underlying the human similarity judgement in this popular genre. Our findings inform the selection of existing metrics and highlight requirements for the design of new similarity metrics benefiting game development and research.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), 11-16 May 2024, Honolulu, HI, USA"
    },
    {
        "paper id": "2402.18736",
        "abstract url": "https://arxiv.org/abs/2402.18736",
        "title": "Functionally-Complete Boolean Logic in Real DRAM Chips: Experimental Characterization and Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "Processing-using-DRAM (PuD) is an emerging paradigm that leverages the analog operational properties of DRAM circuitry to enable massively parallel in-DRAM computation. PuD has the potential to reduce or eliminate costly data movement between processing elements and main memory. Prior works experimentally demonstrate three-input MAJ (MAJ3) and two-input AND and OR operations in commercial off-the-shelf (COTS) DRAM chips. Yet, demonstrations on COTS DRAM chips do not provide a functionally complete set of operations. We experimentally demonstrate that COTS DRAM chips are capable of performing 1) functionally-complete Boolean operations: NOT, NAND, and NOR and 2) many-input (i.e., more than two-input) AND and OR operations. We present an extensive characterization of new bulk bitwise operations in 256 off-the-shelf modern DDR4 DRAM chips. We evaluate the reliability of these operations using a metric called success rate: the fraction of correctly performed bitwise operations. Among our 19 new observations, we highlight four major results. First, we can perform the NOT operation on COTS DRAM chips with a 98.37% success rate on average. Second, we can perform up to 16-input NAND, NOR, AND, and OR operations on COTS DRAM chips with high reliability (e.g., 16-input NAND, NOR, AND, and OR with an average success rate of 94.94%, 95.87%, 94.94%, and 95.85%, respectively). Third, data pattern only slightly affects bitwise operations. Our results show that executing NAND, NOR, AND, and OR operations with random data patterns decreases the success rate compared to all logic-1/logic-0 patterns by 1.39%, 1.97%, 1.43%, and 1.98%, respectively. Fourth, bitwise operations are highly resilient to temperature changes, with small success rate fluctuations of at most 1.66% when the temperature is increased from 50C to 95C. We open-source our infrastructure at https://github.com/CMU-SAFARI/FCDRAM",
        "subjects": [
            "cs.AR"
        ],
        "comment": "A shorter version of this work is to appear at the 30th IEEE International Symposium on High-Performance Computer Architecture (HPCA-30), 2024"
    },
    {
        "paper id": "2402.18744",
        "abstract url": "https://arxiv.org/abs/2402.18744",
        "title": "Timer-Based Coverage Control for Mobile Sensors",
        "rating": -10,
        "keywords": [],
        "abstract": "This work studies the coverage control problem over a static, bounded, and convex workspace and develops a hybrid extension of the continuous-time Lloyd algorithm. Each agent in a multi-agent system (MAS) is equipped with a timer that generates intermittent sampling events, which may occur asynchronously between agents. At each sampling event, the corresponding agents update their controllers, which are otherwise held constant. These controllers are shown to drive the MAS into a neighborhood of the configurations corresponding to a centroidal Voronoi tessellation, that is, a local minimizer of the standard locational cost. The result is a distributed control strategy that leverages intermittent and asynchronous position measurements to disperse the agents within the workspace. The combination of continuous-time dynamics with intermittently updated control inputs is modeled as a hybrid system. The coverage control objective is posed as a set stabilization problem for hybrid systems, where an invariance based convergence analysis yields sufficient conditions that ensure all maximal solutions of the hybrid system asymptotically converge to a desired set. A brief simulation example is included to showcase the result.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18746",
        "abstract url": "https://arxiv.org/abs/2402.18746",
        "title": "Accelerating Computer Architecture Simulation through Machine Learning",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper presents our approach to accelerate computer architecture simulation by leveraging machine learning techniques. Traditional computer architecture simulations are time-consuming, making it challenging to explore different design choices efficiently. Our proposed model utilizes a combination of application features and micro-architectural features to predict the performance of an application. These features are derived from simulations of a small portion of the application. We demonstrate the effectiveness of our approach by building and evaluating a machine learning model that offers significant speedup in architectural exploration. This model demonstrates the ability to predict IPC values for the testing data with a root mean square error of less than 0.1.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18769",
        "abstract url": "https://arxiv.org/abs/2402.18769",
        "title": "CoMeT: Count-Min-Sketch-based Row Tracking to Mitigate RowHammer at Low Cost",
        "rating": -10,
        "keywords": [],
        "abstract": "We propose a new RowHammer mitigation mechanism, CoMeT, that prevents RowHammer bitflips with low area, performance, and energy costs in DRAM-based systems at very low RowHammer thresholds. The key idea of CoMeT is to use low-cost and scalable hash-based counters to track DRAM row activations. CoMeT uses the Count-Min Sketch technique that maps each DRAM row to a group of counters, as uniquely as possible, using multiple hash functions. When a DRAM row is activated, CoMeT increments the counters mapped to that DRAM row. Because the mapping from DRAM rows to counters is not completely unique, activating one row can increment one or more counters mapped to another row. Thus, CoMeT may overestimate, but never underestimates, a DRAM row's activation count. This property of CoMeT allows it to securely prevent RowHammer bitflips while properly configuring its hash functions reduces overestimations. As a result, CoMeT 1) implements substantially fewer counters than the number of DRAM rows in a DRAM bank and 2) does not significantly overestimate a DRAM row's activation count. Our comprehensive evaluations show that CoMeT prevents RowHammer bitflips with an average performance overhead of only 4.01% across 61 benign single-core workloads for a very low RowHammer threshold of 125, normalized to a system with no RowHammer mitigation. CoMeT achieves a good trade-off between performance, energy, and area overheads. Compared to the best-performing state-of-the-art mitigation, CoMeT requires 74.2x less area overhead at the RowHammer threshold 125 and incurs a small performance overhead on average for all RowHammer thresholds. Compared to the best-performing low-area-cost mechanism, at a very low RowHammer threshold of 125, CoMeT improves performance by up to 39.1% while incurring a similar area overhead. CoMeT is openly and freely available at https://github.com/CMU-SAFARI/CoMeT.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "To appear at HPCA 2024"
    },
    {
        "paper id": "2402.18775",
        "abstract url": "https://arxiv.org/abs/2402.18775",
        "title": "How to Evaluate Human-likeness of Interaction-aware Driver Models",
        "rating": -10,
        "keywords": [],
        "abstract": "This study proposes a method for qualitatively evaluating and designing human-like driver models for autonomous vehicles. While most existing research on human-likeness has been focused on quantitative evaluation, it is crucial to consider qualitative measures to accurately capture human perception. To this end, we conducted surveys utilizing both video study and human experience-based study. The findings of this research can significantly contribute to the development of naturalistic and human-like driver models for autonomous vehicles, enabling them to safely and efficiently coexist with human-driven vehicles in diverse driving scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This paper could benefit from further refinement to enhance the significance of its results"
    },
    {
        "paper id": "2402.18797",
        "abstract url": "https://arxiv.org/abs/2402.18797",
        "title": "ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality",
        "rating": -10,
        "keywords": [],
        "abstract": "Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Conditionally accepted by CHI '24"
    },
    {
        "paper id": "2402.18814",
        "abstract url": "https://arxiv.org/abs/2402.18814",
        "title": "New topological subsystem codes from semi-regular tessellations",
        "rating": -10,
        "keywords": [],
        "abstract": "In this work, we present new constructions for topological subsystem codes using semi-regular Euclidean and hyperbolic tessellations. They give us new families of codes, and we also provide a new family of codes obtained through an already existing construction, due to Sarvepalli and Brown. We also prove new results that allow us to obtain the parameters of these new codes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18818",
        "abstract url": "https://arxiv.org/abs/2402.18818",
        "title": "CEBin: A Cost-Effective Framework for Large-Scale Binary Code Similarity Detection",
        "rating": -10,
        "keywords": [],
        "abstract": "Binary code similarity detection (BCSD) is a fundamental technique for various application. Many BCSD solutions have been proposed recently, which mostly are embedding-based, but have shown limited accuracy and efficiency especially when the volume of target binaries to search is large. To address this issue, we propose a cost-effective BCSD framework, CEBin, which fuses embedding-based and comparison-based approaches to significantly improve accuracy while minimizing overheads. Specifically, CEBin utilizes a refined embedding-based approach to extract features of target code, which efficiently narrows down the scope of candidate similar code and boosts performance. Then, it utilizes a comparison-based approach that performs a pairwise comparison on the candidates to capture more nuanced and complex relationships, which greatly improves the accuracy of similarity detection. By bridging the gap between embedding-based and comparison-based approaches, CEBin is able to provide an effective and efficient solution for detecting similar code (including vulnerable ones) in large-scale software ecosystems. Experimental results on three well-known datasets demonstrate the superiority of CEBin over existing state-of-the-art (SOTA) baselines. To further evaluate the usefulness of BCSD in real world, we construct a large-scale benchmark of vulnerability, offering the first precise evaluation scheme to assess BCSD methods for the 1-day vulnerability detection task. CEBin could identify the similar function from millions of candidate functions in just a few seconds and achieves an impressive recall rate of $85.46\\%$ on this more practical but challenging task, which are several order of magnitudes faster and $4.07\\times$ better than the best SOTA baseline. Our code is available at https://github.com/Hustcw/CEBin.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18830",
        "abstract url": "https://arxiv.org/abs/2402.18830",
        "title": "Training-set-free two-stage deep learning for spectroscopic data de-noising",
        "rating": -10,
        "keywords": [],
        "abstract": "De-noising is a prominent step in the spectra post-processing procedure. Previous machine learning-based methods are fast but mostly based on supervised learning and require a training set that may be typically expensive in real experimental measurements. Unsupervised learning-based algorithms are slow and require many iterations to achieve convergence. Here, we bridge this gap by proposing a training-set-free two-stage deep learning method. We show that the fuzzy fixed input in previous methods can be improved by introducing an adaptive prior. Combined with more advanced optimization techniques, our approach can achieve five times acceleration compared to previous work. Theoretically, we study the landscape of a corresponding non-convex linear problem, and our results indicates that this problem has benign geometry for first-order algorithms to converge.",
        "subjects": [
            "cond-mat.mtrl-sci"
        ],
        "comment": null
    },
    {
        "paper id": "2402.18847",
        "abstract url": "https://arxiv.org/abs/2402.18847",
        "title": "Flexible Precoding for Multi-User Movable Antenna Communications",
        "rating": -10,
        "keywords": [],
        "abstract": "This letter rethinks traditional precoding in multi-user wireless communications with movable antennas (MAs). Utilizing MAs for optimal antenna positioning, we introduce a sparse optimization (SO)-based approach focusing on regularized zero-forcing (RZF). This framework targets the optimization of antenna positions and the precoding matrix to minimize inter-user interference and transmit power. We propose an off-grid regularized least squares-based orthogonal matching pursuit (RLS-OMP) method for this purpose. Moreover, we provide deeper insights into antenna position optimization using RLS-OMP, viewed from a subspace projection angle. Overall, our proposed flexible precoding scheme demonstrates a sum rate that exceeds more than twice that of fixed antenna positions.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.00028",
        "abstract url": "https://arxiv.org/abs/2403.00028",
        "title": "Lower Bounds for Differential Privacy Under Continual Observation and Online Threshold Queries",
        "rating": -10,
        "keywords": [],
        "abstract": "One of the most basic problems for studying the \"price of privacy over time\" is the so called private counter problem, introduced by Dwork et al. (2010) and Chan et al. (2010). In this problem, we aim to track the number of events that occur over time, while hiding the existence of every single event. More specifically, in every time step $t\\in[T]$ we learn (in an online fashion) that $\u0394_t\\geq 0$ new events have occurred, and must respond with an estimate $n_t\\approx\\sum_{j=1}^t \u0394_j$. The privacy requirement is that all of the outputs together, across all time steps, satisfy event level differential privacy. The main question here is how our error needs to depend on the total number of time steps $T$ and the total number of events $n$. Dwork et al. (2015) showed an upper bound of $O\\left(\\log(T)+\\log^2(n)\\right)$, and Henzinger et al. (2023) showed a lower bound of $\u03a9\\left(\\min\\{\\log n, \\log T\\}\\right)$. We show a new lower bound of $\u03a9\\left(\\min\\{n,\\log T\\}\\right)$, which is tight w.r.t. the dependence on $T$, and is tight in the sparse case where $\\log^2 n=O(\\log T)$. Our lower bound has the following implications: $\\bullet$ We show that our lower bound extends to the \"online thresholds problem\", where the goal is to privately answer many \"quantile queries\" when these queries are presented one-by-one. This resolves an open question of Bun et al. (2017). $\\bullet$ Our lower bound implies, for the first time, a separation between the number of mistakes obtainable by a private online learner and a non-private online learner. This partially resolves a COLT'22 open question published by Sanyal and Ramponi. $\\bullet$ Our lower bound also yields the first separation between the standard model of private online learning and a recently proposed relaxed variant of it, called private online prediction.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.06992",
        "abstract url": "https://arxiv.org/abs/2403.06992",
        "title": "Phase autoencoder for limit-cycle oscillators",
        "rating": -10,
        "keywords": [],
        "abstract": "We present a phase autoencoder that encodes the asymptotic phase of a limit-cycle oscillator, a fundamental quantity characterizing its synchronization dynamics. This autoencoder is trained in such a way that its latent variables directly represent the asymptotic phase of the oscillator. The trained autoencoder can perform two functions without relying on the mathematical model of the oscillator: first, it can evaluate the asymptotic phase and phase sensitivity function of the oscillator; second, it can reconstruct the oscillator state on the limit cycle in the original space from the phase value as an input. Using several examples of limit-cycle oscillators, we demonstrate that the asymptotic phase and phase sensitivity function can be estimated only from time-series data by the trained autoencoder. We also present a simple method for globally synchronizing two oscillators as an application of the trained autoencoder.",
        "subjects": [
            "nlin.AO"
        ],
        "comment": "12 pages, 16 figures"
    },
    {
        "paper id": "2405.00028",
        "abstract url": "https://arxiv.org/abs/2405.00028",
        "title": "MaRDIFlow: A CSE workflow framework for abstracting meta-data from FAIR computational experiments",
        "rating": -10,
        "keywords": [],
        "abstract": "Numerical algorithms and computational tools are instrumental in navigating and addressing complex simulation and data processing tasks. The exponential growth of metadata and parameter-driven simulations has led to an increasing demand for automated workflows that can replicate computational experiments across platforms. In general, a computational workflow is defined as a sequential description for accomplishing a scientific objective, often described by tasks and their associated data dependencies. If characterized through input-output relation, workflow components can be structured to allow interchangeable utilization of individual tasks and their accompanying metadata. In the present work, we develop a novel computational framework, namely, MaRDIFlow, that focuses on the automation of abstracting meta-data embedded in an ontology of mathematical objects. This framework also effectively addresses the inherent execution and environmental dependencies by incorporating them into multi-layered descriptions. Additionally, we demonstrate a working prototype with example use cases and methodically integrate them into our workflow tool and data provenance framework. Furthermore, we show how to best apply the FAIR principles to computational workflows, such that abstracted components are Findable, Accessible, Interoperable, and Reusable in nature.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "13 pages, 7 figures"
    }
]