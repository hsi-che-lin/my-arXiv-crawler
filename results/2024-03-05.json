[
    {
        "paper id": "2403.02714",
        "abstract url": "https://arxiv.org/abs/2403.02714",
        "title": "DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization",
        "rating": 2.5,
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "cs.CV"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Currently in review for ICML 2024"
    },
    {
        "paper id": "2403.02781",
        "abstract url": "https://arxiv.org/abs/2403.02781",
        "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
        "rating": 2.5,
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts. The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024. Project Page: https://zhengli97.github.io/PromptKD. Code: https://github.com/zhengli97/PromptKD"
    },
    {
        "paper id": "2403.02969",
        "abstract url": "https://arxiv.org/abs/2403.02969",
        "title": "Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception",
        "rating": 2.5,
        "keywords": [
            [
                "visual-language"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Multimodal Large Language Model (MLLMs) leverages Large Language Models as a cognitive framework for diverse visual-language tasks. Recent efforts have been made to equip MLLMs with visual perceiving and grounding capabilities. However, there still remains a gap in providing fine-grained pixel-level perceptions and extending interactions beyond text-specific inputs. In this work, we propose {\\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object perceptions and natural language descriptions from multi-modality references, such as texts, boxes, images, or audio. This innovation empowers users with greater flexibility to engage with the model beyond textual and regional prompts, without modality-specific designs. Through our proposed refocusing mechanism, the generated grounding output is guided to better focus on the referenced object, implicitly incorporating additional pixel-level supervision. This simple modification utilizes attention scores generated during the inference of LLM, eliminating the need for extra computations while exhibiting performance enhancements in both grounding masks and referring expressions. With only publicly available training data, our model achieves state-of-the-art results across multiple benchmarks, including diverse modality referring segmentation and region-level referring expression generation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.02991",
        "abstract url": "https://arxiv.org/abs/2403.02991",
        "title": "MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer",
        "rating": 2.5,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens. Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch. Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples. To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs. Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities. We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances. Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance. Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "19 pages, 9 figures, Published in CVPR2024"
    },
    {
        "paper id": "2403.03095",
        "abstract url": "https://arxiv.org/abs/2403.03095",
        "title": "Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization",
        "rating": 2.5,
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues. In our work, we focus on semi-supervised AVSL with pseudo-labeling. To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn from each other with the cross-refine mechanism to avoid bias accumulation. We equip XPL with two effective components. Firstly, the soft pseudo-labels with sharpening and pseudo-label exponential moving average mechanisms enable models to achieve gradual self-improvement and ensure stable training. Secondly, the curriculum data selection module adaptively selects pseudo-labels with high quality during training to mitigate potential bias. Experimental results demonstrate that XPL significantly outperforms existing methods, achieving state-of-the-art performance while effectively mitigating confirmation bias and ensuring training stability.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted To ICASSP2024"
    },
    {
        "paper id": "2403.03145",
        "abstract url": "https://arxiv.org/abs/2403.03145",
        "title": "Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization",
        "rating": 2.5,
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel semi-supervised learning framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The sufficient utilization of both labeled and unlabeled data and the proposed unbiased framework enable DMT to outperform current state-of-the-art methods by a large margin, with CIoU of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%, 9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods respectively, given only 3% positional-annotations. We also extend our framework to some existing AVSL methods and consistently boost their performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to NeurIPS2023"
    },
    {
        "paper id": "2403.03346",
        "abstract url": "https://arxiv.org/abs/2403.03346",
        "title": "Enhancing Vision-Language Pre-training with Rich Supervisions",
        "rating": 2.5,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We propose Strongly Supervised pre-training with ScreenShots (S4) - a novel pre-training paradigm for Vision-Language Models using data from large-scale web screenshot rendering. Using web screenshots unlocks a treasure trove of visual and textual cues that are not present in using image-text pairs. In S4, we leverage the inherent tree-structured hierarchy of HTML elements and the spatial localization to carefully design 10 pre-training tasks with large scale annotated data. These tasks resemble downstream tasks across different domains and the annotations are cheap to obtain. We demonstrate that, compared to current screenshot pre-training objectives, our innovative pre-training method significantly enhances performance of image-to-text model in nine varied and popular downstream tasks - up to 76.1% improvements on Table Detection, and at least 1% on Widget Captioning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024"
    },
    {
        "paper id": "2403.02938",
        "abstract url": "https://arxiv.org/abs/2403.02938",
        "title": "AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models",
        "rating": 2,
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear. This method can be used to produce fast but intelligible speech. In the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03003",
        "abstract url": "https://arxiv.org/abs/2403.03003",
        "title": "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models",
        "rating": 2,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite remarkable progress, existing multimodal large language models (MLLMs) are still inferior in granular visual recognition. Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming. Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also greatly reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR. We conduct extensive experiments on 11 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$\\times$ inference speed than LLaVA-1.5. Source codes are released at: https://github.com/luogen1996/LLaVA-HR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03432",
        "abstract url": "https://arxiv.org/abs/2403.03432",
        "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
        "rating": 2,
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Furthermore, each LoRA model can be iteratively adapted to a new domain, allowing for quick domain-specific adaptation. Experiments on diverse tasks demonstrate superior and robust performance, which can further promote the wide application of domain-specific LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages, COLING24 Accepted"
    },
    {
        "paper id": "2403.02746",
        "abstract url": "https://arxiv.org/abs/2403.02746",
        "title": "Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer) to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "11 pages, 9 figures, accepted by CVPR 2024"
    },
    {
        "paper id": "2403.02753",
        "abstract url": "https://arxiv.org/abs/2403.02753",
        "title": "Learning Group Activity Features Through Person Attribute Prediction",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "This paper proposes Group Activity Feature (GAF) learning in which features of multi-person activity are learned as a compact latent vector. Unlike prior work in which the manual annotation of group activities is required for supervised learning, our method learns the GAF through person attribute prediction without group activity annotations. By learning the whole network in an end-to-end manner so that the GAF is required for predicting the person attributes of people in a group, the GAF is trained as the features of multi-person activity. As a person attribute, we propose to use a person's action class and appearance features because the former is easy to annotate due to its simpleness, and the latter requires no manual annotation. In addition, we introduce a location-guided attribute prediction to disentangle the complex GAF for extracting the features of each target person properly. Various experimental results validate that our method outperforms SOTA methods quantitatively and qualitatively on two public datasets. Visualization of our GAF also demonstrates that our method learns the GAF representing fined-grained group activity classes. Code: https://github.com/chihina/GAFL-CVPR2024.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR2024"
    },
    {
        "paper id": "2403.02767",
        "abstract url": "https://arxiv.org/abs/2403.02767",
        "title": "DeconfuseTrack:Dealing with Confusion for Multi-Object Tracking",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT). However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association. To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA). DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues. Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections. Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT. Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers. Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR2024"
    },
    {
        "paper id": "2403.02899",
        "abstract url": "https://arxiv.org/abs/2403.02899",
        "title": "Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation",
        "rating": 1.5,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are imposed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02946",
        "abstract url": "https://arxiv.org/abs/2403.02946",
        "title": "SAFFIRA: a Framework for Assessing the Reliability of Systolic-Array-Based DNN Accelerators",
        "rating": 1.5,
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications. However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators. While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process. This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03122",
        "abstract url": "https://arxiv.org/abs/2403.03122",
        "title": "NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge. To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space. To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm. We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times. NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model. We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance. Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024. Project page: https://virtualhumans.mpi-inf.mpg.de/nrdf"
    },
    {
        "paper id": "2403.03203",
        "abstract url": "https://arxiv.org/abs/2403.03203",
        "title": "CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
        "rating": 1.5,
        "keywords": [
            [
                "vision language"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to use existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments, we observe that the low performance of pre-trained vision language models like CLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC ascertains the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "17 pages, 10 images, Accepted at LREC-COLING 2024 - The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation"
    },
    {
        "paper id": "2403.03421",
        "abstract url": "https://arxiv.org/abs/2403.03421",
        "title": "LEAD: Learning Decomposition for Source-free Universal Domain Adaptation",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments across various UniDA scenarios have demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD is also appealing in that it is complementary to most existing methods. The code is available at https://github.com/ispc-lab/LEAD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "To appear in CVPR 2024"
    },
    {
        "paper id": "2403.03461",
        "abstract url": "https://arxiv.org/abs/2403.03461",
        "title": "A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Dense object counting or crowd counting has come a long way thanks to the recent development in the vision community. However, indiscernible object counting, which aims to count the number of targets that are blended with respect to their surroundings, has been a challenge. Image-based object counting datasets have been the mainstream of the current publicly available datasets. Therefore, we propose a large-scale dataset called YoutubeFish-35, which contains a total of 35 sequences of high-definition videos with high frame-per-second and more than 150,000 annotated center points across a selected variety of scenes. For benchmarking purposes, we select three mainstream methods for dense object counting and carefully evaluate them on the newly collected dataset. We propose TransVidCount, a new strong baseline that combines density and regression branches along the temporal domain in a unified framework and can effectively tackle indiscernible object counting with state-of-the-art performance on YoutubeFish-35 dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ICASSP 2024 (IEEE International Conference on Acoustics, Speech, and Signal Processing)"
    },
    {
        "paper id": "2403.04799",
        "abstract url": "https://arxiv.org/abs/2403.04799",
        "title": "AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "Workshop",
                "ICLR"
            ]
        ],
        "abstract": "To effectively navigate the AI revolution, AI literacy is crucial. However, content predominantly exists in dominant languages, creating a gap for low-resource languages like Yoruba (41 million native speakers). This case study explores bridging this gap by creating and distributing AI videos in Yoruba.The project developed 26 videos covering foundational, intermediate, and advanced AI concepts, leveraging storytelling and accessible explanations. These videos were created using a cost-effective methodology and distributed across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of 22 countries. Analysis of YouTube reveals insights into viewing patterns, with the 25-44 age group contributing the most views. Notably, over half of the traffic originated from external sources, highlighting the potential of cross-platform promotion.This study demonstrates the feasibility and impact of creating AI literacy content in low-resource languages. It emphasizes that accurate interpretation requires both technical expertise in AI and fluency in the target language. This work contributes a replicable methodology, a 22-word Yoruba AI vocabulary, and data-driven insights into audience demographics and acquisition channel",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at the Global AI Cultures Workshop, ICLR 2024"
    },
    {
        "paper id": "2403.04804",
        "abstract url": "https://arxiv.org/abs/2403.04804",
        "title": "AttentionStitch: How Attention Solves the Speech Editing Problem",
        "rating": 1.5,
        "keywords": [
            [
                "eess.AS"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The generation of natural and high-quality speech from text is a challenging problem in the field of natural language processing. In addition to speech generation, speech editing is also a crucial task, which requires the seamless and unnoticeable integration of edited speech into synthesized speech. We propose a novel approach to speech editing by leveraging a pre-trained text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double attention block network on top of it to automatically merge the synthesized mel-spectrogram with the mel-spectrogram of the edited text. We refer to this model as AttentionStitch, as it harnesses attention to stitch audio samples together. We evaluate the proposed AttentionStitch model against state-of-the-art baselines on both single and multi-speaker datasets, namely LJSpeech and VCTK. We demonstrate its superior performance through an objective and a subjective evaluation test involving 15 human participants. AttentionStitch is capable of producing high-quality speech, even for words not seen during training, while operating automatically without the need for human intervention. Moreover, AttentionStitch is fast during both training and inference and is able to generate human-sounding edited speech.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted in Machine Learning for Audio workship in NeurIPS 2023"
    },
    {
        "paper id": "2403.02677",
        "abstract url": "https://arxiv.org/abs/2403.02677",
        "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the MLM filter.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Website: https://mlm-filter.github.io"
    },
    {
        "paper id": "2403.02689",
        "abstract url": "https://arxiv.org/abs/2403.02689",
        "title": "Deep Common Feature Mining for Efficient Video Semantic Segmentation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in video semantic segmentation have made substantial progress by exploiting temporal correlations. Nevertheless, persistent challenges, including redundant computation and the reliability of the feature propagation process, underscore the need for further innovation. In response, we present Deep Common Feature Mining (DCFM), a novel approach strategically designed to address these challenges by leveraging the concept of feature sharing. DCFM explicitly decomposes features into two complementary components. The common representation extracted from a key-frame furnishes essential high-level information to neighboring non-key frames, allowing for direct re-utilization without feature propagation. Simultaneously, the independent feature, derived from each video frame, captures rapidly changing information, providing frame-specific clues crucial for segmentation. To achieve such decomposition, we employ a symmetric training strategy tailored for sparsely annotated data, empowering the backbone to learn a robust high-level representation enriched with common information. Additionally, we incorporate a self-supervised loss function to reinforce intra-class feature similarity and enhance temporal consistency. Experimental evaluations on the VSPW and Cityscapes datasets demonstrate the effectiveness of our method, showing a superior balance between accuracy and efficiency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02690",
        "abstract url": "https://arxiv.org/abs/2403.02690",
        "title": "Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework. With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix. Empirically, RENT consistently outperforms existing transition matrix utilization methods, which includes reweighting, on various benchmark datasets. Our code is available at \\url{https://github.com/BaeHeeSun/RENT}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "35 pages, 20 figures, Accepted to the twelfth International Conference on Learninig Representations (ICLR 24)"
    },
    {
        "paper id": "2403.02701",
        "abstract url": "https://arxiv.org/abs/2403.02701",
        "title": "Fighting Game Adaptive Background Music for Improved Gameplay",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features. The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition. The BGM adapts by changing the volume of each category of instruments. Each category is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI agent that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "This is an updated version of our IEEE CoG 2023 paper (https://ieeexplore.ieee.org/document/10333245). This version has revised the description of the association between the distance between the two players (PD) and the instrument's volume on page 2. arXiv admin note: substantial text overlap with arXiv:2303.15734"
    },
    {
        "paper id": "2403.02712",
        "abstract url": "https://arxiv.org/abs/2403.02712",
        "title": "Breeze-7B Technical Report",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Breeze-7B is an open-source language model based on Mistral-7B, designed to address the need for improved language comprehension and chatbot-oriented capabilities in Traditional Chinese. This technical report provides an overview of the additional pretraining, finetuning, and evaluation stages for the Breeze-7B model. The Breeze-7B family of base and chat models exhibits good performance on language comprehension and chatbot-oriented tasks, reaching the top in several benchmarks among models comparable in its complexity class.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02713",
        "abstract url": "https://arxiv.org/abs/2403.02713",
        "title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 200M model on our AitZ dataset achieves on par performance with CogAgent-Chat-18B.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Dataset could be found in https://github.com/IMNearth/CoAT"
    },
    {
        "paper id": "2403.02715",
        "abstract url": "https://arxiv.org/abs/2403.02715",
        "title": "Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights underscore the significance of meticulous fine-tuning with high-quality datasets in enhancing LLM performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "33 pages"
    },
    {
        "paper id": "2403.02718",
        "abstract url": "https://arxiv.org/abs/2403.02718",
        "title": "DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Continuous Relation Extraction (CRE) aims to incrementally learn relation knowledge from a non-stationary stream of data. Since the introduction of new relational tasks can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain. Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set. To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition. This framework examines alterations in the embedding space as new relation classes emerge, distinctly managing the preservation and acquisition of knowledge. Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across two datasets.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted By LREC-Coling-2024, 10 pages with 2 pages of appendix"
    },
    {
        "paper id": "2403.02727",
        "abstract url": "https://arxiv.org/abs/2403.02727",
        "title": "HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR). Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and think step-by-step strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform all the baselines on both datasets. Our findings indicate that by effective prompting, LLMs can interpret raw IMU data based on their knowledge base, possessing a promising potential to analyze raw sensor data of the physical world effectively.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02738",
        "abstract url": "https://arxiv.org/abs/2403.02738",
        "title": "Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to obtain the representation of the samples precisely and estimate the causal effect more accurately, contrastive learning is used to fine-tune the encoder of the samples by aligning the space of the encoder with the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance on 3 natural language processing datasets on both open-source and closed-source LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02756",
        "abstract url": "https://arxiv.org/abs/2403.02756",
        "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains. Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02757",
        "abstract url": "https://arxiv.org/abs/2403.02757",
        "title": "In-Memory Learning: A Declarative Learning Framework for Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02799",
        "abstract url": "https://arxiv.org/abs/2403.02799",
        "title": "DPPA: Pruning Method for Large Language Model to Model Merging",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Model merging is to combine fine-tuned models derived from multiple domains, with the intent of enhancing the model's proficiency across various domains. The principal concern is the resolution of parameter conflicts. A substantial amount of existing research remedy this issue during the merging stage, with the latest study focusing on resolving this issue throughout the pruning stage. The DARE approach has exhibited promising outcomes when applied to a simplistic fine-tuned model. However, the efficacy of this method tends to wane when employed on complex fine-tuned models that show a significant parameter bias relative to the baseline model. In this paper, we introduce a dual-stage method termed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the challenge of merging complex fine-tuned models. Initially, we introduce Dynamically Pruning (DP), an improved approach based on magnitude pruning, which aim is to enhance performance at higher pruning rates. Subsequently, we propose Dynamically Partition Amplification (DPA), a rescaling strategy, is designed to dynamically amplify parameter partitions in relation to their significance levels. The experimental results show that our method maintains a mere 20% of domain-specific parameters and yet delivers a performance comparable to other methodologies that preserve up to 90% of parameters. Furthermore, our method displays outstanding performance post-pruning, leading to a significant improvement of nearly 20% performance in model merging. We make our code on Github.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02839",
        "abstract url": "https://arxiv.org/abs/2403.02839",
        "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02875",
        "abstract url": "https://arxiv.org/abs/2403.02875",
        "title": "Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples",
        "rating": 1,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show significant improvements in fine-grained concept understanding across a wide range of vision-language datasets, including our InpaintCOCO dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2403.02886",
        "abstract url": "https://arxiv.org/abs/2403.02886",
        "title": "Revisiting Confidence Estimation: Towards Reliable Failure Prediction",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failure prediction performance under various settings including balanced, long-tailed, and covariate-shift classification scenarios. Our study not only provides a strong baseline for reliable confidence estimation but also acts as a bridge between understanding calibration, OOD detection, and failure prediction. The code is available at \\url{https://github.com/Impression2805/FMFP}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IEEE TPAMI. arXiv admin note: text overlap with arXiv:2303.02970; text overlap with arXiv:2007.01458 by other authors"
    },
    {
        "paper id": "2403.02889",
        "abstract url": "https://arxiv.org/abs/2403.02889",
        "title": "In Search of Truth: An Interrogation Approach to Hallucination Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying on external knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02892",
        "abstract url": "https://arxiv.org/abs/2403.02892",
        "title": "Enhancing Long-Term Person Re-Identification Using Global, Local Body Part, and Head Streams",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This work addresses the task of long-term person re-identification. Typically, person re-identification assumes that people do not change their clothes, which limits its applications to short-term scenarios. To overcome this limitation, we investigate long-term person re-identification, which considers both clothes-changing and clothes-consistent scenarios. In this paper, we propose a novel framework that effectively learns and utilizes both global and local information. The proposed framework consists of three streams: global, local body part, and head streams. The global and head streams encode identity-relevant information from an entire image and a cropped image of the head region, respectively. Both streams encode the most distinct, less distinct, and average features using the combinations of adversarial erasing, max pooling, and average pooling. The local body part stream extracts identity-related information for each body part, allowing it to be compared with the same body part from another image. Since body part annotations are not available in re-identification datasets, pseudo-labels are generated using clustering. These labels are then utilized to train a body part segmentation head in the local body part stream. The proposed framework is trained by backpropagating the weighted summation of the identity classification loss, the pair-based loss, and the pseudo body part segmentation loss. To demonstrate the effectiveness of the proposed method, we conducted experiments on three publicly available datasets (Celeb-reID, PRCC, and VC-Clothes). The experimental results demonstrate that the proposed method outperforms the previous state-of-the-art method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2403.02902",
        "abstract url": "https://arxiv.org/abs/2403.02902",
        "title": "Demonstrating Mutual Reinforcement Effect through Information Flow",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ information flow analysis to observe and substantiate the MRE theory. Our experiments on six MRE hybrid datasets revealed the presence of MRE in the model and its impact. Additionally, we conducted fine-tuning experiments, whose results were consistent with those of the information flow experiments. The convergence of findings from both experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in five out of six datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, 15 figure"
    },
    {
        "paper id": "2403.02909",
        "abstract url": "https://arxiv.org/abs/2403.02909",
        "title": "Gaze-Vector Estimation in the Dark with Temporally Encoded Event-driven Neural Networks",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we address the intricate challenge of gaze vector prediction, a pivotal task with applications ranging from human-computer interaction to driver monitoring systems. Our innovative approach is designed for the demanding setting of extremely low-light conditions, leveraging a novel temporal event encoding scheme, and a dedicated neural network architecture. The temporal encoding method seamlessly integrates Dynamic Vision Sensor (DVS) events with grayscale guide frames, generating consecutively encoded images for input into our neural network. This unique solution not only captures diverse gaze responses from participants within the active age group but also introduces a curated dataset tailored for low-light conditions. The encoded temporal frames paired with our network showcase impressive spatial localization and reliable gaze direction in their predictions. Achieving a remarkable 100-pixel accuracy of 100%, our research underscores the potency of our neural network to work with temporally consecutive encoded images for precise gaze vector predictions in challenging low-light videos, contributing to the advancement of gaze prediction technologies.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02910",
        "abstract url": "https://arxiv.org/abs/2403.02910",
        "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
        "rating": 1,
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "attack"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02930",
        "abstract url": "https://arxiv.org/abs/2403.02930",
        "title": "A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in Advances in Information Retrieval, 46th European Conference on Information Retrieval, ECIR 2024. 16 pages, 4 figures"
    },
    {
        "paper id": "2403.02932",
        "abstract url": "https://arxiv.org/abs/2403.02932",
        "title": "RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task. Within this framework, the inaccurate pseudo labels assigned to texts and the imprecise logical rules associated with categories mutually enhance each other in an alternative manner. That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point. Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods. What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by WWW 2024"
    },
    {
        "paper id": "2403.02944",
        "abstract url": "https://arxiv.org/abs/2403.02944",
        "title": "Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms of LPIPS, with some room for even more improvements when we use more carefully generated captions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The first two authors contributed equally"
    },
    {
        "paper id": "2403.02951",
        "abstract url": "https://arxiv.org/abs/2403.02951",
        "title": "Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "26pages, 6figures, 14tables"
    },
    {
        "paper id": "2403.02959",
        "abstract url": "https://arxiv.org/abs/2403.02959",
        "title": "SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt. Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision-making of judge. (3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02966",
        "abstract url": "https://arxiv.org/abs/2403.02966",
        "title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02990",
        "abstract url": "https://arxiv.org/abs/2403.02990",
        "title": "Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a foundational guide for researchers and practitioners in this field.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03014",
        "abstract url": "https://arxiv.org/abs/2403.03014",
        "title": "The Case for Evaluating Multimodal Translation Models on Text Datasets",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "A good evaluation framework should evaluate multimodal machine translation (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only machine translation. However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties. Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only machine translation models are evaluated against. Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance against complex sentences, and 3) the Multi30k test sets, for measuring MMT model performance against a real MMT dataset. Finally, we evaluate recent MMT models trained solely against the Multi30k dataset against our proposed evaluation framework and demonstrate the dramatic drop performance against text-only testing sets compared to recent text-only MT models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03031",
        "abstract url": "https://arxiv.org/abs/2403.03031",
        "title": "Learning to Use Tools via Cooperative and Interactive Agents",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "20 pages"
    },
    {
        "paper id": "2403.03045",
        "abstract url": "https://arxiv.org/abs/2403.03045",
        "title": "Adding Multimodal Capabilities to a Text-only Translation Model",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model. We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03075",
        "abstract url": "https://arxiv.org/abs/2403.03075",
        "title": "Detecting Concrete Visual Tokens for Multimodal Machine Translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The challenge of visual grounding and masking in multimodal machine translation (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking. We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with object detection, and a joint detection-verification technique. We also introduce new methods for selection of detected tokens, including shortest $n$ tokens, longest $n$ tokens, and all detected concrete tokens. We utilize the GRAM MMT architecture to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03082",
        "abstract url": "https://arxiv.org/abs/2403.03082",
        "title": "Recall-Oriented Continual Learning with Generative Adversarial Meta-Model",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios. Our code is available at: https://github.com/bigdata-inha/recall-oriented-cl-framework.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted in AAAI-2024 (Oral presentation)"
    },
    {
        "paper id": "2403.03102",
        "abstract url": "https://arxiv.org/abs/2403.03102",
        "title": "\"In Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03121",
        "abstract url": "https://arxiv.org/abs/2403.03121",
        "title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03129",
        "abstract url": "https://arxiv.org/abs/2403.03129",
        "title": "CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional open-source datasets, indicate that: 1) Large-scale models perform well when provided with user context but struggle in the absence of such context. 2) While specialized smaller models fine-tuned on the synthetic dataset show promise, they still lag behind their larger counterparts. 3) Our CoGenesis framework, utilizing mixed-scale models, showcases competitive performance, providing a feasible solution to privacy issues.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03134",
        "abstract url": "https://arxiv.org/abs/2403.03134",
        "title": "Simplicity in Complexity : Explaining Visual Complexity using Deep Segmentation Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite complex. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images. This suggests that the complexity of images can be surprisingly simple.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03141",
        "abstract url": "https://arxiv.org/abs/2403.03141",
        "title": "Language Guided Exploration for RL Agents in Text Environments",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03163",
        "abstract url": "https://arxiv.org/abs/2403.03163",
        "title": "Design2Code: How Far Are We From Automating Front-End Engineering?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that successfully matches the performance of Gemini Pro Vision. Both human evaluation and automatic metrics show that GPT-4V performs the best on this task compared to other models. Moreover, annotators think GPT-4V generated webpages can replace the original reference webpages in 49% of cases in terms of visual appearance and content; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages are considered better than the original reference webpages. Our fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper finetuning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Technical Report; The first two authors contributed equally"
    },
    {
        "paper id": "2403.03167",
        "abstract url": "https://arxiv.org/abs/2403.03167",
        "title": "PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with https://github.com/GGLAB-KU/paradise.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03173",
        "abstract url": "https://arxiv.org/abs/2403.03173",
        "title": "Solving the bongard-logo problem by modeling a probabilistic model",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Abstract reasoning problems challenge the perceptual and cognitive abilities of AI algorithms, demanding deeper pattern discernment and inductive reasoning beyond explicit image features. This study introduces PMoC, a tailored probability model for the Bongard-Logo problem, achieving high reasoning accuracy by constructing independent probability models. Additionally, we present Pose-Transformer, an enhanced Transformer-Encoder designed for complex abstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM. Pose-Transformer incorporates positional information learning, inspired by capsule networks' pose matrices, enhancing its focus on local positional relationships in image data processing. When integrated with PMoC, it further improves reasoning accuracy. Our approach effectively addresses reasoning difficulties associated with abstract entities' positional changes, outperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM databases. This research contributes to advancing AI's capabilities in abstract reasoning and cognitive pattern recognition.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 11 figures, 3 tables"
    },
    {
        "paper id": "2403.03187",
        "abstract url": "https://arxiv.org/abs/2403.03187",
        "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03190",
        "abstract url": "https://arxiv.org/abs/2403.03190",
        "title": "Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Abstract reasoning poses significant challenges to artificial intelligence algorithms, demanding a higher level of cognitive ability than that required for perceptual tasks. In this study, we introduce the Triple-CFN method to tackle the Bongard Logo problem, achieving remarkable reasoning accuracy by implicitly reorganizing the conflicting concept spaces of instances. Furthermore, with necessary modifications, the Triple-CFN paradigm has also proven effective on the RPM (Raven's Progressive Matrices) problem, yielding competitive results. To further enhance Triple-CFN's performance on the RPM problem, we have upgraded it to the Meta Triple-CFN network, which explicitly constructs the concept space of RPM problems, ensuring high reasoning accuracy while achieving conceptual interpretability. The success of Meta Triple-CFN can be attributed to its paradigm of modeling the concept space, which is tantamount to normalizing reasoning information. Based on this idea, we have introduced the Re-space layer, boosting the performance of both Meta Triple-CFN and Triple-CFN. This paper aims to contribute to the advancement of machine intelligence and pave the way for further breakthroughs in this field by exploring innovative network designs for solving abstract reasoning problems.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 14 figures, 5 tables"
    },
    {
        "paper id": "2403.03296",
        "abstract url": "https://arxiv.org/abs/2403.03296",
        "title": "CenterDisks: Real-time instance segmentation with disk covering",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Increasing the accuracy of instance segmentation methods is often done at the expense of speed. Using coarser representations, we can reduce the number of parameters and thus obtain real-time masks. In this paper, we take inspiration from the set cover problem to predict mask approximations. Given ground-truth binary masks of objects of interest as training input, our method learns to predict the approximate coverage of these objects by disks without supervision on their location or radius. Each object is represented by a fixed number of disks with different radii. In the learning phase, we consider the radius as proportional to a standard deviation in order to compute the error to propagate on a set of two-dimensional Gaussian functions rather than disks. We trained and tested our instance segmentation method on challenging datasets showing dense urban settings with various road users. Our method achieve state-of-the art results on the IDD and KITTI dataset with an inference time of 0.040 s on a single RTX 3090 GPU.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03305",
        "abstract url": "https://arxiv.org/abs/2403.03305",
        "title": "Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching. Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data. Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher. In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data. Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. Human interventions to the rules for the TACRED relation \\texttt{org:parents} boost the performance on that relation by as much as 26\\% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03309",
        "abstract url": "https://arxiv.org/abs/2403.03309",
        "title": "Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visual understanding and segmentation of materials and their states is fundamental to understanding the physical world. The myriad textures, shapes, and often blurry boundaries formed by materials make this task particularly hard to generalize. Whether it's identifying wet regions of a surface, minerals in rocks, infected regions in plants, or pollution in water, each material state has its own unique form. For neural nets to learn general class-agnostic material segmentation, it is necessary to first collect and annotate data that captures this complexity. Collecting and manually annotating real-world images is limited by the cost and precision of manual labor. In contrast, synthetic CGI data is highly accurate and almost cost-free, but fails to replicate the vast diversity of the material world. This work offers a method to bridge this crucial gap by implanting patterns extracted from real-world images in synthetic data. Hence, patterns automatically collected from natural images are used to map materials into synthetic scenes. This unsupervised approach allows the generated data to capture the vast complexity of the real world while maintaining the precision and scale of synthetic data. We also present the first general benchmark for zero-shot material state segmentation. The benchmark contains a wide range of real-world images of material states, like food, rocks, construction, plants, liquids, and many others, each in various states (wet/dry/stained/cooked/burned/worn/rusted/sediment/foam, etc.). The annotation includes both partial similarity between regions with similar but not identical materials, and hard segmentation of only points in the exact same material state. We show that net trains on MatSeg significantly outperform existing state-of-the-art methods on this task. The dataset, code, and trained model are available",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03334",
        "abstract url": "https://arxiv.org/abs/2403.03334",
        "title": "DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated with a final stance label. On average, the videos have 200 comments each, and the stance of the comments skews slightly towards the \"against\" characterization for both the U.S. Army and the videos posted on the channel.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Paper under review for dataset track of ICWSM 2024. 11 pages, 5 figures"
    },
    {
        "paper id": "2403.03348",
        "abstract url": "https://arxiv.org/abs/2403.03348",
        "title": "Learning to Maximize Mutual Information for Chain-of-Thought Distillation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve this optimization problem using a learning-based method. Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS. Our findings offer insightful guidance for future research on language model distillation as well as applications involving CoT. Code and models will be released soon.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03382",
        "abstract url": "https://arxiv.org/abs/2403.03382",
        "title": "Adaptive Discovering and Merging for Incremental Novel Class Discovery",
        "rating": 1.0,
        "keywords": [
            [
                "cs.AI"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner. The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge. To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge. To discover novel classes adaptively, we decouple representation learning and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment. To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of the novel branch on the old classes to preserve the previous knowledge, and merges the novel branch to the base model without performance loss and parameter growth. Extensive experiments on several datasets show that ADM significantly outperforms existing class-incremental Novel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits the class-incremental Learning (class-IL) task by alleviating the catastrophic forgetting problem.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "AAAI 2024. arXiv admin note: text overlap with arXiv:2207.08605 by other authors"
    },
    {
        "paper id": "2403.03395",
        "abstract url": "https://arxiv.org/abs/2403.03395",
        "title": "Interactive Melody Generation System for Enhancing the Creativity of Musicians",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "This study proposes a system designed to enumerate the process of collaborative composition among humans, using automatic music composition technology. By integrating multiple Recurrent Neural Network (RNN) models, the system provides an experience akin to collaborating with several composers, thereby fostering diverse creativity. Through dynamic adaptation to the user's creative intentions, based on feedback, the system enhances its capability to generate melodies that align with user preferences and creative needs. The system's effectiveness was evaluated through experiments with composers of varying backgrounds, revealing its potential to facilitate musical creativity and suggesting avenues for further refinement. The study underscores the importance of interaction between the composer and AI, aiming to make music composition more accessible and personalized. This system represents a step towards integrating AI into the creative process, offering a new tool for composition support and collaborative artistic exploration.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03396",
        "abstract url": "https://arxiv.org/abs/2403.03396",
        "title": "Japanese-English Sentence Translation Exercises Dataset for Automatic Grading",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning. We formalize the task as grading student responses for each rubric criterion pre-specified by the educators. We then create a dataset for STE between Japanese and English including 21 questions, along with a total of 3, 498 student responses (167 on average). The answer responses were collected from students and crowd workers. Using this dataset, we demonstrate the performance of baselines including finetuned BERT and GPT models with few-shot in-context learning. Experimental results show that the baseline model with finetuned BERT was able to classify correct responses with approximately 90% in F1, but only less than 80% for incorrect responses. Furthermore, the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task presents a challenging issue, even for the stateof-the-art large language models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2403.03411",
        "abstract url": "https://arxiv.org/abs/2403.03411",
        "title": "CrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional Encoding for Single- and Multi-Channel Speaker Separation",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "We introduce CrossNet, a complex spectral mapping approach to speaker separation and enhancement in reverberant and noisy conditions. The proposed architecture comprises an encoder layer, a global multi-head self-attention module, a cross-band module, a narrow-band module, and an output layer. CrossNet captures global, cross-band, and narrow-band correlations in the time-frequency domain. To address performance degradation in long utterances, we introduce a random chunk positional encoding. Experimental results on multiple datasets demonstrate the effectiveness and robustness of CrossNet, achieving state-of-the-art performance in tasks including reverberant and noisy-reverberant speaker separation. Furthermore, CrossNet exhibits faster and more stable training in comparison to recent baselines. Additionally, CrossNet's high performance extends to multi-microphone conditions, demonstrating its versatility in various acoustic scenarios.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2403.03419",
        "abstract url": "https://arxiv.org/abs/2403.03419",
        "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrate that D$^2$O is equivalent to learning a distributional instead of instance-level preference model reflecting human dispreference against the distribution of negative responses. Besides, D$^2$O integrates an implicit Jeffrey Divergence regularization to balance the exploitation and exploration of reference policies and converges to a non-negative one during training. Extensive experiments demonstrate that our method achieves comparable generation quality and surpasses the latest baselines in producing less harmful and more informative responses with better training stability and faster convergence.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03435",
        "abstract url": "https://arxiv.org/abs/2403.03435",
        "title": "VLSP 2023 -- LTER: A Summary of the Challenge on Legal Textual Entailment Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this new era of rapid AI development, especially in language processing, the demand for AI in the legal domain is increasingly critical. In the context where research in other languages such as English, Japanese, and Chinese has been well-established, we introduce the first fundamental research for the Vietnamese language in the legal domain: legal textual entailment recognition through the Vietnamese Language and Speech Processing workshop. In analyzing participants' results, we discuss certain linguistic aspects critical in the legal domain that pose challenges that need to be addressed.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03452",
        "abstract url": "https://arxiv.org/abs/2403.03452",
        "title": "D4C Glove-train: Solving the RPM and Bongard-logo Problem by Circumscribing and Building Distribution for Concepts",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper achieves noteworthy progress in the realm of abstract reasoning, particularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo challenges. Initially, we introduce Lico-Net, a novel baseline model that resolves RPM problems with remarkable accuracy. Leveraging this foundation, we advance with the D3C approach, which advocates representing the underlying concepts in abstract reasoning problems through distributions. This perspective enhances the performance of both Lico-Net and a baseline model excelling in Bongard-Logo tasks. To bolster the computational efficiency of D3C, we present the D3C-cos variant, offering a streamlined yet precise solution. Furthermore, we propose the D2C method, redefining conceptual boundaries within these domains and bridging the divide between high-level abstractions and their lower-dimensional counterparts. Finally, we extend our methodology to D4C, employing adversarial techniques to refine conceptual boundaries further and demonstrate substantial improvements in both RPM and Bongard-Logo challenges. Overall, our contributions present a fresh outlook and practical advancements in the field of abstract reasoning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "18 pages, 19 figures, 6 tables"
    },
    {
        "paper id": "2403.03458",
        "abstract url": "https://arxiv.org/abs/2403.03458",
        "title": "Slot Abstractors: Toward Scalable Abstract Visual Reasoning",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects. Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to multi-object visual inputs. Here we combine the strengths of the above approaches and propose Slot Abstractors, an approach to abstract visual reasoning that can be scaled to problems involving a large number of objects and multiple relations among them. The approach displays state-of-the-art performance across four abstract visual reasoning tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "15 pages, 5 figures"
    },
    {
        "paper id": "2403.04797",
        "abstract url": "https://arxiv.org/abs/2403.04797",
        "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper aims to overcome the \"lost-in-the-middle\" challenge of large language models (LLMs). While recent advancements have successfully enabled LLMs to perform stable language modeling with up to 4 million tokens, the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled. To address this problem, this paper introduces Multi-scale Positional Encoding (Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the capacity of LLMs to handle the relevant information located in the middle of the context, without fine-tuning or introducing any additional overhead. Ms-PoE leverages the position indice rescaling to relieve the long-term decay effect introduced by RoPE, while meticulously assigning distinct scaling ratios to different attention heads to preserve essential knowledge learned during the pre-training step, forming a multi-scale context fusion from short to long distance. Extensive experiments with a wide range of LLMs demonstrate the efficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of up to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are available at https://github.com/VITA-Group/Ms-PoE.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.04798",
        "abstract url": "https://arxiv.org/abs/2403.04798",
        "title": "JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents our system development for SemEval-2024 Task 3: \"The Competition of Multimodal Emotion Cause Analysis in Conversations\". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experimental codes are available on Github.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Paper Accepted at SemEval 2024"
    },
    {
        "paper id": "2403.04800",
        "abstract url": "https://arxiv.org/abs/2403.04800",
        "title": "(Un)paired signal-to-signal translation with 1D conditional GANs",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "I show that a one-dimensional (1D) conditional generative adversarial network (cGAN) with an adversarial training architecture is capable of unpaired signal-to-signal (\"sig2sig\") translation. Using a simplified CycleGAN model with 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe two-dimensional (2D) image generation as 1D audio generation, I show that recasting the 2D image-to-image translation task to a 1D signal-to-signal translation task with deep convolutional GANs is possible without substantial modification to the conventional U-Net model and adversarial architecture developed as CycleGAN. With this I show for a small tunable dataset that noisy test signals unseen by the 1D CycleGAN model and without paired training transform from the source domain to signals similar to paired test signals in the translated domain, especially in terms of frequency, and I quantify these differences in terms of correlation and error.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.13826",
        "abstract url": "https://arxiv.org/abs/2403.13826",
        "title": "Measuring Diversity in Co-creative Image Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Quality and diversity have been proposed as reasonable heuristics for assessing content generated by co-creative systems, but to date there has been little agreement around what constitutes the latter or how to measure it. Proposed approaches for assessing generative models in terms of diversity have limitations in that they compare the model's outputs to a ground truth that in the era of large pre-trained generative models might not be available, or entail an impractical number of computations. We propose an alternative based on entropy of neural network encodings for comparing diversity between sets of images that does not require ground-truth knowledge and is easy to compute. We also compare two pre-trained networks and show how the choice relates to the notion of diversity that we want to evaluate. We conclude with a discussion of the potential applications of these measures for ideation in interactive systems, model evaluation, and more broadly within computational creativity.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02681",
        "abstract url": "https://arxiv.org/abs/2403.02681",
        "title": "SGD with Partial Hessian for Deep Neural Networks Optimization",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The proposed method, namely SGD with Partial Hessian (SGD-PH), inherits the advantages of both first-order and second-order optimizers. Compared with first-order optimizers, it adopts a certain amount of information from the Hessian matrix to assist optimization, while compared with the existing second-order optimizers, it keeps the good generalization performance of first-order optimizers. Experiments on image classification tasks demonstrate the effectiveness of our proposed optimizer SGD-PH. The code is publicly available at \\url{https://github.com/myingysun/SGDPH}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02695",
        "abstract url": "https://arxiv.org/abs/2403.02695",
        "title": "Controllable Prompt Tuning For Balancing Group Distributional Robustness",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring only 0.4% tunable parameters.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "29 pages, 11 figures, 13 tables"
    },
    {
        "paper id": "2403.02698",
        "abstract url": "https://arxiv.org/abs/2403.02698",
        "title": "Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment",
        "rating": 0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Conventional multi-hop fact verification models are prone to rely on spurious correlations from the annotation artifacts, leading to an obvious performance decline on unbiased datasets. Among the various debiasing works, the causal inference-based methods become popular by performing theoretically guaranteed debiasing such as casual intervention or counterfactual reasoning. However, existing causal inference-based debiasing methods, which mainly formulate fact verification as a single-hop reasoning task to tackle shallow bias patterns, cannot deal with the complicated bias patterns hidden in multiple hops of evidence. To address the challenge, we propose Causal Walk, a novel method for debiasing multi-hop fact verification from a causal perspective with front-door adjustment. Specifically, in the structural causal model, the reasoning path between the treatment (the input claim-evidence graph) and the outcome (the veracity label) is introduced as the mediator to block the confounder. With the front-door adjustment, the causal effect between the treatment and the outcome is decomposed into the causal effect between the treatment and the mediator, which is estimated by applying the idea of random walk, and the causal effect between the mediator and the outcome, which is estimated with normalized weighted geometric mean approximation. To investigate the effectiveness of the proposed method, an adversarial multi-hop fact verification dataset and a symmetric multi-hop fact verification dataset are proposed with the help of the large language model. Experimental results show that Causal Walk outperforms some previous debiasing methods on both existing datasets and the newly constructed datasets. Code and data will be released at https://github.com/zcccccz/CausalWalk.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by AAAI 2024"
    },
    {
        "paper id": "2403.02730",
        "abstract url": "https://arxiv.org/abs/2403.02730",
        "title": "A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimentally demonstrate that our method produces models that satisfy the constraints and also improves their predictive performance. Thus, ensuring compliance with critical system properties and also contributing to reducing data quantity requirements. Furthermore, we show that the proposed method improves the convergence to an optimal solution and improves the explainability of Neural ODE models. Our proposed two-stage training method can be used with any NN architectures.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02737",
        "abstract url": "https://arxiv.org/abs/2403.02737",
        "title": "Neural Fractional Differential Equations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours. This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions. Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data. This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural ODE in modelling systems with memory or dependencies on past states, and it can effectively be applied to learn more intricate dynamical systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02745",
        "abstract url": "https://arxiv.org/abs/2403.02745",
        "title": "CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an \u03b5-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02768",
        "abstract url": "https://arxiv.org/abs/2403.02768",
        "title": "An Empirical Analysis on the Use and Reporting of National Security Letters",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "National Security Letters (NSLs) are similar to administrative subpoenas and can be issued directly by elements of the executive branch without requiring prior approval from a court or grand jury. Importantly, NSLs authorize the imposition of nondisclosure orders (aka \"gag orders\") on the receiving party. Controversy about potential abuses of this authority has driven a range of legal and policy discussions. To address these concerns, both the public sector and the private sector have sought to document the usage of NSLs in aggregated form. However, each data source is limited in scope, time, and kind. In this paper, we consolidate the available data around NSLs and answer two questions: (1) what can the public effectively learn from the reported data and does this information suffice to assess the NSL usage? (2) how accessible is this data collection? We show that longitudinal trends in the usage of NSLs can be observed. For instance, we find a significant increase in NSL requests for non-US persons and that the policy reforms to decrease the mandated nondisclosure period appear to be effective. The observed trends suggest that the current transparency mechanisms are viable safeguards against the excessive use of NSLs. However, aggregating and normalizing the data requires manual reviewing, parsing, and validating. We even find inconsistencies within and across official data sources. Overall, the laborious data collection process hinders external and internal auditing efforts and demonstrates the need for a unified and more usable dataset for NSLs.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Accepted as short presentation at CSLAW 2024 (not in proceeding): https://computersciencelaw.org/2024/accepted-papers/"
    },
    {
        "paper id": "2403.02775",
        "abstract url": "https://arxiv.org/abs/2403.02775",
        "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02782",
        "abstract url": "https://arxiv.org/abs/2403.02782",
        "title": "Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos",
        "rating": 0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In this paper, we explore the capability of an agent to construct a logical sequence of action steps, thereby assembling a strategic procedural plan. This plan is crucial for navigating from an initial visual observation to a target visual outcome, as depicted in real-life instructional videos. Existing works have attained partial success by extensively leveraging various sources of information available in the datasets, such as heavy intermediate visual observations, procedural names, or natural language step-by-step instructions, for features or supervision signals. However, the task remains formidable due to the implicit causal constraints in the sequencing of steps and the variability inherent in multiple feasible plans. To tackle these intricacies that previous efforts have overlooked, we propose to enhance the capabilities of the agent by infusing it with procedural knowledge. This knowledge, sourced from training procedure plans and structured as a directed weighted graph, equips the agent to better navigate the complexities of step sequencing and its potential variations. We coin our approach KEPP, a novel Knowledge-Enhanced Procedure Planning system, which harnesses a probabilistic procedural knowledge graph extracted from training data, effectively acting as a comprehensive textbook for the training domain. Experimental evaluations across three widely-used datasets under settings of varying complexity reveal that KEPP attains superior, state-of-the-art results while requiring only minimal supervision.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures, (supplementary material: 8 pages, 5 figures), accepted to CVPR 2024"
    },
    {
        "paper id": "2403.02783",
        "abstract url": "https://arxiv.org/abs/2403.02783",
        "title": "Where the Really Hard Quadratic Assignment Problems Are: the QAP-SAT instances",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The Quadratic Assignment Problem (QAP) is one of the major domains in the field of evolutionary computation, and more widely in combinatorial optimization. This paper studies the phase transition of the QAP, which can be described as a dramatic change in the problem's computational complexity and satisfiability, within a narrow range of the problem parameters. To approach this phenomenon, we introduce a new QAP-SAT design of the initial problem based on submodularity to capture its difficulty with new features. This decomposition is studied experimentally using branch-and-bound and tabu search solvers. A phase transition parameter is then proposed. The critical parameter of phase transition satisfaction and that of the solving effort are shown to be highly correlated for tabu search, thus allowing the prediction of difficult instances.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02795",
        "abstract url": "https://arxiv.org/abs/2403.02795",
        "title": "Evaluating and Optimizing Educational Content with Large Language Model Judgments",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instructional materials using the judgments of another LM as a reward function. We apply this approach to create math word problem worksheets aimed at maximizing student learning gains. Human teachers' evaluations of these LM-generated worksheets show a significant alignment between the LM judgments and human teacher preferences. We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2403.02821",
        "abstract url": "https://arxiv.org/abs/2403.02821",
        "title": "An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the electricity production.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02833",
        "abstract url": "https://arxiv.org/abs/2403.02833",
        "title": "SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent, where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (SGD) with momentum. The extensive experiments on training deep learning models using several benchmark image classification datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and several state-of-the-art Newton optimization methods in term of the convergence speed for achieving the pre-specified objectives of training and test losses as well as test accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02850",
        "abstract url": "https://arxiv.org/abs/2403.02850",
        "title": "Scalable Syndrome-based Neural Decoders for Bit-Interleaved Coded Modulations",
        "rating": 0.5,
        "keywords": [
            [
                "ICML"
            ]
        ],
        "abstract": "In this work, we introduce a framework that enables the use of Syndrome-Based Neural Decoders (SBND) for high-order Bit-Interleaved Coded Modulations (BICM). To this end, we extend the previous results on SBND, for which the validity is limited to Binary Phase-Shift Keying (BPSK), by means of a theoretical channel modeling of the bit Log-Likelihood Ratio (bit-LLR) induced outputs. We implement the proposed SBND system for two polar codes $(64,32)$ and $(128,64)$, using a Recurrent Neural Network (RNN) and a Transformer-based architecture. Both implementations are compared in Bit Error Rate (BER) performance and computational complexity.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "6 pages, 7 figures. To be published in Proc. IEEE International Conference on Machine Learning for Communication and Networking (ICMLCN 2024), Stockholm, Sweden, May 5-8, 2024. \u00a92024 IEEE"
    },
    {
        "paper id": "2403.02873",
        "abstract url": "https://arxiv.org/abs/2403.02873",
        "title": "A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2403.02908",
        "abstract url": "https://arxiv.org/abs/2403.02908",
        "title": "Preserving Tangible and Intangible Cultural Heritage: the Cases of Volterra and Atari",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "At first glance, the ruins of the Roman Theatre in the Italian town of Volterra have little in common with cassette tapes containing Atari games. One is certainly considered an important historical landmark, while the consensus on the importance of the other is partial at best. Still, both are remnants of times vastly different from the present and are at risk of oblivion. Unearthed architectural structures are exposed to the elements just as the deteriorating signals stored on magnetic tapes. However, the rate of deterioration is much faster with the magnetic media, as their life expectancy is counted in decades, whereas the Roman Theater, which is already in ruin, measures its lifespan in centuries. Hence, both would benefit from some form of digital preservation and reconstruction. In this panel, we discuss how to sustainably preserve tangible and intangible cultural artifacts for future generations.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "8 pages, including 1 page of bibliography, 9 figures. Panel summary to be published in proceedings from 11th Machine Intelligence and Digital Interaction MIDI Conference"
    },
    {
        "paper id": "2403.02920",
        "abstract url": "https://arxiv.org/abs/2403.02920",
        "title": "TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers. Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification benchmark across five tasks involving long sequences reveals no degradation in accuracy when employing Transformers equipped with TaylorShift. For reproducibility, we provide access to our code under https://github.com/tobna/TaylorShift.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02931",
        "abstract url": "https://arxiv.org/abs/2403.02931",
        "title": "Improving the quality of individual-level online information tracking: challenges of existing approaches and introduction of a new content- and long-tail sensitive academic solution",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "This article evaluates the quality of data collection in individual-level desktop information tracking used in the social sciences and shows that the existing approaches face sampling issues, validity issues due to the lack of content-level data and their disregard of the variety of devices and long-tail consumption patterns as well as transparency and privacy issues. To overcome some of these problems, the article introduces a new academic tracking solution, WebTrack, an open source tracking tool maintained by a major European research institution. The design logic, the interfaces and the backend requirements for WebTrack, followed by a detailed examination of strengths and weaknesses of the tool, are discussed. Finally, using data from 1185 participants, the article empirically illustrates how an improvement in the data collection through WebTrack leads to new innovative shifts in the processing of tracking data. As WebTrack allows collecting the content people are exposed to on more than classical news platforms, we can strongly improve the detection of politics-related information consumption in tracking data with the application of automated content analysis compared to traditional approaches that rely on the list-based identification of news.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "73 pages"
    },
    {
        "paper id": "2403.02933",
        "abstract url": "https://arxiv.org/abs/2403.02933",
        "title": "Fuzzy Datalog$^\\exists$ over Arbitrary t-Norms",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "One of the main challenges in the area of Neuro-Symbolic AI is to perform logical reasoning in the presence of both neural and symbolic data. This requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more. To allow for such reasoning, we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies. The resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard Datalog setting. In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, reasoning has the same complexity as in the classical setting.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02936",
        "abstract url": "https://arxiv.org/abs/2403.02936",
        "title": "AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN Accelerators",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02962",
        "abstract url": "https://arxiv.org/abs/2403.02962",
        "title": "WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Tabular data, as a crucial form of data representation, exists in diverse formats on the Web. When confronted with complex and irregular tables, manual modification becomes a laborious task. This paper investigates the performance of Large Language Models (LLMs) in the context of table editing tasks. Existing research mainly focuses on regular-shaped tables, wherein instructions are used to generate code in SQL, Python, or Excel Office-script for manipulating the tables. Nevertheless, editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code. To address this, we introduce the WikiTableEdit dataset. Leveraging 26,531 tables from the WikiSQL dataset, we automatically generate natural language instructions for six distinct basic operations and the corresponding outcomes, resulting in over 200,000 instances. Subsequently, we evaluate several representative large language models on the WikiTableEdit dataset to demonstrate the challenge of this task. The dataset will be released to the community to promote related researches.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02993",
        "abstract url": "https://arxiv.org/abs/2403.02993",
        "title": "Localized Zeroth-Order Prompt Optimization",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I). The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order prompt optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization. Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03017",
        "abstract url": "https://arxiv.org/abs/2403.03017",
        "title": "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03020",
        "abstract url": "https://arxiv.org/abs/2403.03020",
        "title": "SplAgger: Split Aggregation for Meta-Reinforcement Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. Black box methods do so by training off-the-shelf sequence models end-to-end. By contrast, task inference methods explicitly infer a posterior distribution over the unknown task, typically using distinct objectives and sequence models designed to enable task inference. Recent work has shown that task inference methods are not necessary for strong performance. However, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models with permutation invariant aggregation, which exploit the fact that, due to the Markov property, the task posterior does not depend on the order of data. We empirically confirm the advantage of permutation invariant sequence models without the use of task inference objectives. However, we also find, surprisingly, that there are multiple conditions under which permutation variance remains useful. Therefore, we propose SplAgger, which uses both permutation variant and invariant components to achieve the best of both worlds, outperforming all baselines on continuous control and memory environments.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03028",
        "abstract url": "https://arxiv.org/abs/2403.03028",
        "title": "Word Importance Explains How Prompts Affect Language Model Outputs",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The emergence of large language models (LLMs) has revolutionized numerous applications across industries. However, their \"black box\" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use. This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs. This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs. Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc. This procedure also enables measuring impact when attention weights are not available. To test the fidelity of this approach, we explore the effect of adding different suffixes to multiple different system prompts and comparing subsequent generations with different large language models. Results show that word importance scores are closely related to the expected suffix importances for multiple scoring functions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03061",
        "abstract url": "https://arxiv.org/abs/2403.03061",
        "title": "When Industry meets Trustworthy AI: A Systematic Review of AI for Industry 5.0",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Industry is at the forefront of adopting new technologies, and the process followed by the adoption has a significant impact on the economy and society. In this work, we focus on analysing the current paradigm in which industry evolves, making it more sustainable and Trustworthy. In Industry 5.0, Artificial Intelligence (AI), among other technology enablers, is used to build services from a sustainable, human-centric and resilient perspective. It is crucial to understand those aspects that can bring AI to industry, respecting Trustworthy principles by collecting information to define how it is incorporated in the early stages, its impact, and the trends observed in the field. In addition, to understand the challenges and gaps in the transition from Industry 4.0 to Industry 5.0, a general perspective on the industry's readiness for new technologies is described. This provides practitioners with novel opportunities to be explored in pursuit of the adoption of Trustworthy AI in the sector.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Review Manuscript"
    },
    {
        "paper id": "2403.03069",
        "abstract url": "https://arxiv.org/abs/2403.03069",
        "title": "Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the task of estimating variational autoencoders (VAEs) when the training data is incomplete. We show that missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case. The increased complexity may adversely affect the fit of the model due to a mismatch between the variational and model posterior distributions. We introduce two strategies based on (i) finite variational-mixture and (ii) imputation-based variational-mixture distributions to address the increased posterior complexity. Through a comprehensive evaluation of the proposed approaches, we show that variational mixtures are effective at improving the accuracy of VAE estimation from incomplete data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03103",
        "abstract url": "https://arxiv.org/abs/2403.03103",
        "title": "Emergent Equivariance in Deep Ensembles",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages + 15 pages appendices"
    },
    {
        "paper id": "2403.03150",
        "abstract url": "https://arxiv.org/abs/2403.03150",
        "title": "Deep-Learned Compression for Radio-Frequency Signal Classification",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples. This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data. The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making. Moving large amounts of data to AI agents may result in significant bandwidth and latency costs. We propose a deep learned compression (DLC) model, HQARF, based on learned vector quantization (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes. We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal. Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient use of the bandwidth and storage for non-real-time analytics, and for a decreased delay in real-time applications. While exploring the effectiveness of the HQARF signal reconstructions in modulation classification tasks, we highlight the DLC optimization space and some open problems related to the training of the VQ embedded in HQARF.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03170",
        "abstract url": "https://arxiv.org/abs/2403.03170",
        "title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection",
        "rating": 0.5,
        "keywords": [
            [
                "CVPR"
            ]
        ],
        "abstract": "Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model's discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.",
        "subjects": [
            "cs.MM"
        ],
        "comment": "To appear in CVPR 2024"
    },
    {
        "paper id": "2403.03172",
        "abstract url": "https://arxiv.org/abs/2403.03172",
        "title": "Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the \"curse of dimensinality\" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient consensus mechanism can guide all agents cooperatively reaching valuable future states. Results on Multi-agent Particle-Environments and Google Research Football environment demonstrate the superiority of MAGI in both sample efficiency and performance.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03176",
        "abstract url": "https://arxiv.org/abs/2403.03176",
        "title": "Unifying and Certifying Top-Quality Planning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The growing utilization of planning tools in practical scenarios has sparked an interest in generating multiple high-quality plans. Consequently, a range of computational problems under the general umbrella of top-quality planning were introduced over a short time period, each with its own definition. In this work, we show that the existing definitions can be unified into one, based on a dominance relation. The different computational problems, therefore, simply correspond to different dominance relations. Given the unified definition, we can now certify the top-quality of the solutions, leveraging existing certification of unsolvability and optimality. We show that task transformations found in the existing literature can be employed for the efficient certification of various top-quality planning problems and propose a novel transformation to efficiently certify loopless top-quality planning.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "To appear at ICAPS 2024"
    },
    {
        "paper id": "2403.03183",
        "abstract url": "https://arxiv.org/abs/2403.03183",
        "title": "How Well Can Transformers Emulate In-context Newton's Method?",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\u03b5$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03185",
        "abstract url": "https://arxiv.org/abs/2403.03185",
        "title": "Preventing Reward Hacking with Occupancy Measure Regularization",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reward hacking occurs when an agent performs very well with respect to a \"proxy\" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a \"safe\" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM). Thus, we propose regularizing based on the OM divergence between policies instead of AD divergence to prevent reward hacking. We theoretically establish that OM regularization can more effectively avoid large drops in true reward. Then, we empirically demonstrate in a variety of realistic environments that OM divergence is superior to AD divergence for preventing reward hacking by regularizing towards a safe policy. Furthermore, we show that occupancy measure divergence can also regularize learned policies away from reward hacking behavior. Our code and data are available at https://github.com/cassidylaidlaw/orpo",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03186",
        "abstract url": "https://arxiv.org/abs/2403.03186",
        "title": "Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Despite the success in specific tasks and scenarios, existing foundation agents, empowered by large models (LMs) and advanced tools, still cannot generalize to different scenarios, mainly due to dramatic differences in the observations and actions across scenarios. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. The main challenges of achieving GCC are: 1) the multimodal observations for decision-making, 2) the requirements of accurate control of keyboard and mouse, 3) the need for long-term memory and reasoning, and 4) the abilities of efficient exploration and self-improvement. To target GCC, we introduce Cradle, an agent framework with six main modules, including: 1) information gathering to extract multi-modality information, 2) self-reflection to rethink past experiences, 3) task inference to choose the best next task, 4) skill curation for generating and updating relevant skills for given tasks, 5) action planning to generate specific operations for keyboard and mouse control, and 6) memory for storage and retrieval of past experiences and known skills. To demonstrate the capabilities of generalization and self-improvement of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards GCC with a challenging target. To our best knowledge, our work is the first to enable LMM-based agents to follow the main storyline and finish real missions in complex AAA games, with minimal reliance on prior knowledge or resources. The project website is at https://baai-agents.github.io/Cradle/.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03219",
        "abstract url": "https://arxiv.org/abs/2403.03219",
        "title": "LC-Tsallis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\u03b2\\in (0, \\infty]$. We then show that the algorithm's regret satisfies $O\\left(\\left\\{\\log(T)\\right\\}^{\\frac{1+\u03b2}{2+\u03b2}}T^{\\frac{1}{2+\u03b2}}\\right)$. Here, $\u03b2= \\infty$ corresponds to the case in the existing studies where a lower bound exists in the suboptimality gap, and our regret satisfies $O(\\log(T))$ in that case. Our proposed algorithm is based on the Follow-The-Regularized-Leader with the Tsallis entropy and referred to as the $\u03b1$-Linear-Contextual (LC)-Tsallis-INF.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03221",
        "abstract url": "https://arxiv.org/abs/2403.03221",
        "title": "FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation",
        "rating": 0.5,
        "keywords": [
            [
                "6DoF"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Estimating relative camera poses between images has been a central problem in computer vision. Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision. We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales. At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024. Project Page: https://crockwell.github.io/far/"
    },
    {
        "paper id": "2403.03281",
        "abstract url": "https://arxiv.org/abs/2403.03281",
        "title": "Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03288",
        "abstract url": "https://arxiv.org/abs/2403.03288",
        "title": "Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In the rapidly evolving field of Large Language Models (LLMs), there is a critical need to thoroughly analyze their capabilities and risks. Central to our investigation are two novel elements. Firstly, it is the innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of \"ready-to-hand\" and \"present-at-hand,\" which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world. This comparison lays the groundwork for positioning LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning. Secondly, a structural analysis of human reasoning, viewed through Heidegger's notion of truth as \"unconcealment\" is conducted This foundational principle enables us to map out the inputs and outputs of the reasoning system and divide reasoning into four distinct categories. Respective cognitive faculties are delineated, allowing us to place LLMs within the broader schema of human reasoning, thus clarifying their strengths and inherent limitations. Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement. The potential and risks of LLMs when they are augmented with other AI technologies are also evaluated. The results indicate that although LLMs have achieved proficiency in some reasoning abilities, the aspiration to match or exceed human intellectual capabilities is yet unattained. This research not only enriches our comprehension of LLMs but also propels forward the discourse on AI's potential and its bounds, paving the way for future explorations into AI's evolving landscape.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "39 pages"
    },
    {
        "paper id": "2403.03292",
        "abstract url": "https://arxiv.org/abs/2403.03292",
        "title": "Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "State-of-the-art decentralized learning algorithms typically require the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the data distribution across the agents can have significant heterogeneity. In this work, we propose averaging rate scheduling as a simple yet effective way to reduce the impact of heterogeneity in decentralized learning. Our experiments illustrate the superiority of the proposed method (~3% improvement in test accuracy) compared to the conventional approach of employing a constant averaging rate.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages, 3 figures, 4 tables. arXiv admin note: text overlap with arXiv:2305.04792"
    },
    {
        "paper id": "2403.03328",
        "abstract url": "https://arxiv.org/abs/2403.03328",
        "title": "An Ensemble Framework for Explainable Geospatial Machine Learning Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Analyzing spatial varying effect is pivotal in geographic analysis. Yet, accurately capturing and interpreting this variability is challenging due to the complexity and non-linearity of geospatial data. Herein, we introduce an integrated framework that merges local spatial weighting scheme, Explainable Artificial Intelligence (XAI), and cutting-edge machine learning technologies to bridge the gap between traditional geographic analysis models and general machine learning approaches. Through tests on synthetic datasets, this framework is verified to enhance the interpretability and accuracy of predictions in both geographic regression and classification by elucidating spatial variability. It significantly boosts prediction precision, offering a novel approach to understanding spatial phenomena.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03329",
        "abstract url": "https://arxiv.org/abs/2403.03329",
        "title": "Guardrail Baselines for Unlearning in LLMs",
        "rating": 0.5,
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.CL"
            ],
            [
                "workshop",
                "ICLR"
            ]
        ],
        "abstract": "Recent work has demonstrated that fine-tuning is a promising approach to `unlearn' concepts from large language models. However, fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to fine-tuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive fine-tuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. fine-tuning, and highlights scenarios where guardrails themselves may be advantageous for unlearning, such as in generating examples for fine-tuning or unlearning when only API access is available.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Preliminary work, accepted to ICLR workshop SeT-LLM 2024"
    },
    {
        "paper id": "2403.03362",
        "abstract url": "https://arxiv.org/abs/2403.03362",
        "title": "Level Set Teleportation: An Optimization Perspective",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a teleportation oracle uniformly out-perform their standard versions on a variety of learning problems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Thirty-five pages including appendices"
    },
    {
        "paper id": "2403.03370",
        "abstract url": "https://arxiv.org/abs/2403.03370",
        "title": "F$^3$Loc: Fusion and Filtering for Floorplan Localization",
        "rating": 0.5,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In this paper we propose an efficient data-driven solution to self-localization within a floorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal filtering module. Operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 11 figure, accepted to CVPR 2024"
    },
    {
        "paper id": "2403.03375",
        "abstract url": "https://arxiv.org/abs/2403.03375",
        "title": "Complexity Matters: Dynamics of Feature Learning in the Presence of Spurious Correlations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Existing research often posits spurious features as \"easier\" to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover they mainly focus on the end performance intead of the learning dynamics of feature learning. In this paper, we propose a theoretical framework and associated synthetic dataset grounded in boolean function analysis which allows for fine-grained control on the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlation. Our setup uncovers several interesting phenomenon: (1) stronger spurious correlations or simpler spurious features slow down the rate of learning for the core features, (2) learning phases of spurious features and core features are not always separable, (3) spurious features are not forgotten even after core features are fully learned. We show that our findings justify the success of retraining the last layer to remove spurious correlation and also identifies limitations of popular debiasing algorithms that exploit early learning of spurious features. We support our empirical findings with theoretical analyses for the case of learning XOR features with a one-hidden-layer ReLU network.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Code for the project is available at https://github.com/NayutaQiu/Boolean_Spurious"
    },
    {
        "paper id": "2403.03401",
        "abstract url": "https://arxiv.org/abs/2403.03401",
        "title": "BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective reveals a novel end-to-end system that improves on prior work. We also provide a qualitative analysis, illustrating that improved performance is associated with more semantically-aware embeddings. By streamlining the implementation and comparison of Machine Learning algorithms in the ITP context, we anticipate BAIT will be a springboard for future research.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03406",
        "abstract url": "https://arxiv.org/abs/2403.03406",
        "title": "An EnKF-LSTM Assimilation Algorithm for Crop Growth Model",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed on a farm.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03412",
        "abstract url": "https://arxiv.org/abs/2403.03412",
        "title": "Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the dynamic realms of machine learning and deep learning, the robustness and reliability of models are paramount, especially in critical real-world applications. A fundamental challenge in this sphere is managing Out-of-Distribution (OOD) samples, significantly increasing the risks of model misclassification and uncertainty. Our work addresses this challenge by enhancing the detection and management of OOD samples in neural networks. We introduce OOD-R (Out-of-Distribution-Rectified), a meticulously curated collection of open-source datasets with enhanced noise reduction properties. In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate evaluation of detection algorithms. Recognizing this, OOD-R incorporates noise filtering technologies to refine the datasets, ensuring a more accurate and reliable evaluation of OOD detection algorithms. This approach not only improves the overall quality of data but also aids in better distinguishing between OOD and ID samples, resulting in up to a 2.5\\% improvement in model accuracy and a minimum 3.2\\% reduction in false positives. Furthermore, we present ActFun, an innovative method that fine-tunes the model's response to diverse inputs, thereby improving the stability of feature extraction and minimizing specificity issues. ActFun addresses the common problem of model overconfidence in OOD detection by strategically reducing the influence of hidden units, which enhances the model's capability to estimate OOD uncertainty more accurately. Implementing ActFun in the OOD-R dataset has led to significant performance enhancements, including an 18.42\\% increase in AUROC of the GradNorm method and a 16.93\\% decrease in FPR95 of the Energy method. Overall, our research not only advances the methodologies in OOD detection but also emphasizes the importance of dataset integrity for accurate algorithm evaluation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03448",
        "abstract url": "https://arxiv.org/abs/2403.03448",
        "title": "Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. By emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear information and significantly improving clustering precision, supported by theoretical rationale. We assess the performance of our algorithm on 13 challenging benchmark datasets, demonstrating its superiority over contemporary state-of-the-art MKKM techniques.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "36 pages. This paper was accepted by Pattern Recognition on January 31, 2024"
    },
    {
        "paper id": "2403.03454",
        "abstract url": "https://arxiv.org/abs/2403.03454",
        "title": "Learning Constrained Optimization with Deep Augmented Lagrangian Methods",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poor convergence properties of classical Dual Ascent are reflected in poor convergence of the proposed training scheme. Then, by incorporating techniques from practical Augmented Lagrangian methods, we show how the training scheme can be improved to learn highly accurate constrained optimization solvers, for both convex and nonconvex problems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.12993",
        "abstract url": "https://arxiv.org/abs/2403.12993",
        "title": "Simple Full-Spectrum Correlated k-Distribution Model based on Multilayer Perceptron",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "While neural networks have been successfully applied to the full-spectrum k-distribution (FSCK) method at a large range of thermodynamics with k-values predicted by a trained multilayer perceptron (MLP) model, the required a-values still need to be calculated on-the-fly, which theoretically degrades the FSCK method and may lead to errors. On the other hand, too complicated structure of the current MLP model inevitably slows down the calculation efficiency. Therefore, to compensate among accuracy, efficiency and storage, the simple MLP designed based on the nature of FSCK method are developed, i.e., the simple FSCK MLP (SFM) model, from which those correlated k-values and corresponding ka-values can be efficiently obtained. Several test cases have been carried out to compare the developed SFM model and other FSCK tools including look-up tables and traditional FSCK MLP (TFM) model. Results show that the SFM model can achieve excellent accuracy that is even better than look-up tables at a tiny computational cost that is far less than that of TFM model. Considering accuracy, efficiency and portability, the SFM model is not only an excellent tool for the prediction of spectral properties, but also provides a method to reduce the errors due to nonlinear effects.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.14671",
        "abstract url": "https://arxiv.org/abs/2403.14671",
        "title": "Understanding the Transit Gap: A Comparative Study of On-Demand Bus Services and Urban Climate Resilience in South End, Charlotte, NC and Avondale, Chattanooga, TN",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Urban design significantly impacts sustainability, particularly in the context of public transit efficiency and carbon emissions reduction. This study explores two neighborhoods with distinct urban designs: South End, Charlotte, NC, featuring a dynamic mixed-use urban design pattern, and Avondale, Chattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM tool, we assess the impact of increased bus utilization in these different urban settings on traffic and CO2 emissions. Our results highlight the critical role of urban design and planning in transit system efficiency. In South End, the mixed-use design led to more substantial emission reductions, indicating that urban layout can significantly influence public transit outcomes. Tailored strategies that consider the unique urban design elements are essential for climate resilience. Notably, doubling bus utilization decreased daily emissions by 10.18% in South End and 8.13% in Avondale, with a corresponding reduction in overall traffic. A target of 50% bus utilization saw emissions drop by 21.45% in South End and 14.50% in Avondale. At an idealistic goal of 70% bus utilization, South End and Avondale witnessed emission reductions of 37.22% and 27.80%, respectively. These insights are crucial for urban designers and policymakers in developing sustainable urban landscapes.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "6 pages, 4 figures, Recently accepted by the PLEA 2024 Conference for Sustainable Architecture and Urban Design (Re-Thinking Resilience), Wroclaw, Poland"
    },
    {
        "paper id": "2403.15412",
        "abstract url": "https://arxiv.org/abs/2403.15412",
        "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define \"culture,\" which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of \"culture.\" We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of \"culture,\" such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations, we provide several recommendations for a holistic and practically useful research agenda for furthering cultural inclusion in LLMs and LLM-based applications.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02691",
        "abstract url": "https://arxiv.org/abs/2403.02691",
        "title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
        "rating": 0,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative. In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "28 pages, 5 figures, 9 tables"
    },
    {
        "paper id": "2403.02707",
        "abstract url": "https://arxiv.org/abs/2403.02707",
        "title": "Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation",
        "rating": 0,
        "keywords": [
            [
                "visual language"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Leveraging pre-trained visual language models has become a widely adopted approach for improving performance in downstream visual question answering (VQA) applications. However, in the specialized field of medical VQA, the scarcity of available data poses a significant barrier to achieving reliable model generalization. Numerous methods have been proposed to enhance model generalization, addressing the issue from data-centric and model-centric perspectives. Data augmentation techniques are commonly employed to enrich the dataset, while various regularization approaches aim to prevent model overfitting, especially when training on limited data samples. In this paper, we introduce a method that incorporates gradient-guided parameter perturbations to the visual encoder of the multimodality model during both pre-training and fine-tuning phases, to improve model generalization for downstream medical VQA tasks. The small perturbation is adaptively generated by aligning with the direction of the moving average gradient in the optimization landscape, which is opposite to the directions of the optimizer's historical updates. It is subsequently injected into the model's visual encoder. The results show that, even with a significantly smaller pre-training image caption dataset, our approach achieves competitive outcomes on both VQA-RAD and SLAKE datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02818",
        "abstract url": "https://arxiv.org/abs/2403.02818",
        "title": "Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Current state-of-the-art (SOTA) 3D object detection methods often require a large amount of 3D bounding box annotations for training. However, collecting such large-scale densely-supervised datasets is notoriously costly. To reduce the cumbersome data annotation process, we propose a novel sparsely-annotated framework, in which we just annotate one 3D object per scene. Such a sparse annotation strategy could significantly reduce the heavy annotation burden, while inexact and incomplete sparse supervision may severely deteriorate the detection performance. To address this issue, we develop the SS3D++ method that alternatively improves 3D detector training and confident fully-annotated scene generation in a unified learning scheme. Using sparse annotations as seeds, we progressively generate confident fully-annotated scenes based on designing a missing-annotated instance mining module and reliable background mining module. Our proposed method produces competitive results when compared with SOTA weakly-supervised methods using the same or even more annotation costs. Besides, compared with SOTA fully-supervised methods, we achieve on-par or even better performance on the KITTI dataset with about 5x less annotation cost, and 90% of their performance on the Waymo dataset with about 15x less annotation cost. The additional unlabeled training scenes could further boost the performance. The code will be available at https://github.com/gaocq/SS3D2.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02827",
        "abstract url": "https://arxiv.org/abs/2403.02827",
        "title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation",
        "rating": 0,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image-to-video (I2V) generation tasks always suffer from keeping high fidelity in the open domains. Traditional image animation techniques primarily focus on specific domains such as faces or human poses, making them difficult to generalize to open domains. Several recent I2V frameworks based on diffusion models can generate dynamic content for open domain images but fail to maintain fidelity. We found that two main factors of low fidelity are the loss of image details and the noise prediction biases during the denoising process. To this end, we propose an effective method that can be applied to mainstream video diffusion models. This method achieves high fidelity based on supplementing more precise image information and noise rectification. Specifically, given a specified image, our method first adds noise to the input image latent to keep more details, then denoises the noisy latent with proper rectification to alleviate the noise prediction biases. Our method is tuning-free and plug-and-play. The experimental results demonstrate the effectiveness of our approach in improving the fidelity of generated videos. For more image-to-video generated results, please refer to the project website: https://noise-rectification.github.io.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02884",
        "abstract url": "https://arxiv.org/abs/2403.02884",
        "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
        "rating": 0,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2403.02887",
        "abstract url": "https://arxiv.org/abs/2403.02887",
        "title": "Enhancing the Rate-Distortion-Perception Flexibility of Learned Image Codecs with Conditional Diffusion Decoders",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Learned image compression codecs have recently achieved impressive compression performances surpassing the most efficient image coding architectures. However, most approaches are trained to minimize rate and distortion which often leads to unsatisfactory visual results at low bitrates since perceptual metrics are not taken into account. In this paper, we show that conditional diffusion models can lead to promising results in the generative compression task when used as a decoder, and that, given a compressed representation, they allow creating new tradeoff points between distortion and perception at the decoder side based on the sampling method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02893",
        "abstract url": "https://arxiv.org/abs/2403.02893",
        "title": "Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Event Causality Identification (ECI) refers to the detection of causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource languages, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over a document. Then, to improve cross-lingual transferability of causal knowledge learned from the source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms the previous state-of-the-art model by 9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios respectively. Notably, in the multilingual scenario, our zero-shot framework even exceeds GPT-3.5 with few-shot learning by 24.3% in overall performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.02919",
        "abstract url": "https://arxiv.org/abs/2403.02919",
        "title": "Cross-Domain Image Conversion by CycleDM",
        "rating": 0,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The purpose of this paper is to enable the conversion between machine-printed character images (i.e., font images) and handwritten character images through machine learning. For this purpose, we propose a novel unpaired image-to-image domain conversion method, CycleDM, which incorporates the concept of CycleGAN into the diffusion model. Specifically, CycleDM has two internal conversion models that bridge the denoising processes of two image domains. These conversion models are efficiently trained without explicit correspondence between the domains. By applying machine-printed and handwritten character images to the two modalities, CycleDM realizes the conversion between them. Our experiments for evaluating the converted images quantitatively and qualitatively found that ours performs better than other comparable approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03174",
        "abstract url": "https://arxiv.org/abs/2403.03174",
        "title": "MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting",
        "rating": 0,
        "keywords": [
            [
                "vision language",
                "VLMs"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03194",
        "abstract url": "https://arxiv.org/abs/2403.03194",
        "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
        "rating": 0,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative \\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03206",
        "abstract url": "https://arxiv.org/abs/2403.03206",
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Synthesis",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03304",
        "abstract url": "https://arxiv.org/abs/2403.03304",
        "title": "Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data",
        "rating": 0,
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments. To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect to roles observed in the source domain. Our experiments show that MLA augmentation can boost RDF1 performance by an average of 5.85 points compared to non-augmented datasets.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03307",
        "abstract url": "https://arxiv.org/abs/2403.03307",
        "title": "Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots",
        "rating": 0,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Educational chatbots are a promising tool for assisting student learning. However, the development of effective chatbots in education has been challenging, as high-quality data is seldom available in this domain. In this paper, we propose a framework for generating synthetic teacher-student interactions grounded in a set of textbooks. Our approaches capture one aspect of learning interactions where curious students with partial knowledge interactively ask a teacher questions about the material in the textbook. We highlight various quality criteria that such dialogues should fulfill and compare several approaches relying on either prompting or fine-tuning large language models. We use synthetic dialogues to train educational chatbots and show benefits of further fine-tuning in different educational domains. However, human evaluation shows that our best data synthesis method still suffers from hallucinations and tends to reiterate information from previous conversations. Our findings offer insights for future efforts in synthesizing conversational data that strikes a balance between size and quality. We will open-source our data and code.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "24 pages, 19 tables, 2 figures"
    },
    {
        "paper id": "2403.03405",
        "abstract url": "https://arxiv.org/abs/2403.03405",
        "title": "Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation",
        "rating": 0,
        "keywords": [
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision-and-Language Navigation (VLN) has gained significant research interest in recent years due to its potential applications in real-world scenarios. However, existing VLN methods struggle with the issue of spurious associations, resulting in poor generalization with a significant performance gap between seen and unseen environments. In this paper, we tackle this challenge by proposing a unified framework CausalVLN based on the causal learning paradigm to train a robust navigator capable of learning unbiased feature representations. Specifically, we establish reasonable assumptions about confounders for vision and language in VLN using the structured causal model (SCM). Building upon this, we propose an iterative backdoor-based representation learning (IBRL) method that allows for the adaptive and effective intervention on confounders. Furthermore, we introduce the visual and linguistic backdoor causal encoders to enable unbiased feature expression for multi-modalities during training and validation, enhancing the agent's capability to generalize across different environments. Experiments on three VLN datasets (R2R, RxR, and REVERIE) showcase the superiority of our proposed method over previous state-of-the-art approaches. Moreover, detailed visualization analysis demonstrates the effectiveness of CausalVLN in significantly narrowing down the performance gap between seen and unseen environments, underscoring its strong generalization capability.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2403.03408",
        "abstract url": "https://arxiv.org/abs/2403.03408",
        "title": "Scene Depth Estimation from Traditional Oriental Landscape Paintings",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Scene depth estimation from paintings can streamline the process of 3D sculpture creation so that visually impaired people appreciate the paintings with tactile sense. However, measuring depth of oriental landscape painting images is extremely challenging due to its unique method of depicting depth and poor preservation. To address the problem of scene depth estimation from oriental landscape painting images, we propose a novel framework that consists of two-step Image-to-Image translation method with CLIP-based image matching at the front end to predict the real scene image that best matches with the given oriental landscape painting image. Then, we employ a pre-trained SOTA depth estimation model for the generated real scene image. In the first step, CycleGAN converts an oriental landscape painting image into a pseudo-real scene image. We utilize CLIP to semantically match landscape photo images with an oriental landscape painting image for training CycleGAN in an unsupervised manner. Then, the pseudo-real scene image and oriental landscape painting image are fed into DiffuseIT to predict a final real scene image in the second step. Finally, we measure depth of the generated real scene image using a pre-trained depth estimation model such as MiDaS. Experimental results show that our approach performs well enough to predict real scene images corresponding to oriental landscape painting images. To the best of our knowledge, this is the first study to measure the depth of oriental landscape painting images. Our research potentially assists visually impaired people in experiencing paintings in diverse ways. We will release our code and resulting dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03431",
        "abstract url": "https://arxiv.org/abs/2403.03431",
        "title": "Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Synthesis",
                "Image Editing",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have recently gained significant popularity for creative Text-to-image generation. Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process. However, little is known about what semantic meanings these attention layers have learned and which parts of the attention maps contribute to the success of image editing. In this paper, we conduct an in-depth probing analysis and demonstrate that cross-attention maps in Stable Diffusion often contain object attribution information that can result in editing failures. In contrast, self-attention maps play a crucial role in preserving the geometric and shape details of the source image during the transformation to the target image. Our analysis offers valuable insights into understanding cross and self-attention maps in diffusion models. Moreover, based on our findings, we simplify popular image editing methods and propose a more straightforward yet more stable and efficient tuning-free procedure that only modifies self-attention maps of the specified attention layers during the denoising process. Experimental results show that our simplified method consistently surpasses the performance of popular approaches on multiple datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03456",
        "abstract url": "https://arxiv.org/abs/2403.03456",
        "title": "DLP-GAN: learning to draw modern Chinese landscape photos with generative adversarial network",
        "rating": 0,
        "keywords": [
            [
                "GAN"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (Draw Modern Chinese Landscape Photos with Generative Adversarial Network), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based on our collection of modern landscape and sketch datasets, we compare the images generated by our model with other benchmarks. Extensive experiments including user studies show that our model outperforms state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Corrected typos"
    },
    {
        "paper id": "2403.04801",
        "abstract url": "https://arxiv.org/abs/2403.04801",
        "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs",
        "rating": 0,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other LLMs can open a new avenue of automated attacks that we should further study and explore. The code can be found at https://github.com/Alymostafa/Instruction_based_attack .",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.09700",
        "abstract url": "https://arxiv.org/abs/2403.09700",
        "title": "Shapley Values-Powered Framework for Fair Reward Split in Content Produced by GenAI",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "It is evident that, currently, generative models are surpassed in quality by human professionals. However, with the advancements in Artificial Intelligence, this gap will narrow, leading to scenarios where individuals who have dedicated years of their lives to mastering a skill become obsolete due to their high costs, which are inherently linked to the time they require to complete a task -- a task that AI could accomplish in minutes or seconds. To avoid future social upheavals, we must, even now, contemplate how to fairly assess the contributions of such individuals in training generative models and how to compensate them for the reduction or complete loss of their incomes. In this work, we propose a method to structure collaboration between model developers and data providers. To achieve this, we employ Shapley Values to quantify the contribution of artist(s) in an image generated by the Stable Diffusion-v1.5 model and to equitably allocate the reward among them.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "36 pages, 32 figures"
    },
    {
        "paper id": "2403.02694",
        "abstract url": "https://arxiv.org/abs/2403.02694",
        "title": "Privacy-Aware Semantic Cache for Large Language Models",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) like ChatGPT and Llama2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters where inference demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries which constitute about 31% of the total queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a user-centric semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. Existing caching solutions for LLMs raise privacy and scalability concerns and perform wasteful query requests. MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model across LLM users without violating privacy. By placing a local cache in each user's device and using FL, MeanCache reduces the latency and costs and enhances model performance, resulting in lower false hit rates. MeanCache compresses the embedding dimensions to minimize cache storage and also finds the optimal cosine similarity threshold. Our experiments benchmarked against the state-of-the-art caching method, reveal that MeanCache attains an approximately 17% higher F-score and a 20% increase in precision during semantic cache hit-and-miss decisions. It also reduces the storage requirement by 83% and accelerates semantic cache hit-and-miss decisions by 11%.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This study presents the first privacy aware semantic cache for LLMs based on Federated Learning. Total pages 13"
    },
    {
        "paper id": "2403.02719",
        "abstract url": "https://arxiv.org/abs/2403.02719",
        "title": "Multi-Scale Subgraph Contrastive Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Graph-level contrastive learning, aiming to learn the representations for each graph by contrasting two augmented graphs, has attracted considerable attention. Previous studies usually simply assume that a graph and its augmented graph as a positive pair, otherwise as a negative pair. However, it is well known that graph structure is always complex and multi-scale, which gives rise to a fundamental question: after graph augmentation, will the previous assumption still hold in reality? By an experimental analysis, we discover the semantic information of an augmented graph structure may be not consistent as original graph structure, and whether two augmented graphs are positive or negative pairs is highly related with the multi-scale structures. Based on this finding, we propose a multi-scale subgraph contrastive learning architecture which is able to characterize the fine-grained semantic information. Specifically, we generate global and local views at different scales based on subgraph sampling, and construct multiple contrastive relationships according to their semantic associations to provide richer self-supervised signals. Extensive experiments and parametric analyzes on eight graph classification real-world datasets well demonstrate the effectiveness of the proposed method.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "The 32nd International Joint Conference on Artificial Intelligence (IJCAI-2023)"
    },
    {
        "paper id": "2403.02769",
        "abstract url": "https://arxiv.org/abs/2403.02769",
        "title": "HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes",
        "rating": -0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Human-centric 3D scene understanding has recently drawn increasing attention, driven by its critical impact on robotics. However, human-centric real-life scenarios are extremely diverse and complicated, and humans have intricate motions and interactions. With limited labeled data, supervised methods are difficult to generalize to general scenarios, hindering real-life applications. Mimicking human intelligence, we propose an unsupervised 3D detection method for human-centric scenarios by transferring the knowledge from synthetic human instances to real scenes. To bridge the gap between the distinct data representations and feature distributions of synthetic models and real point clouds, we introduce novel modules for effective instance-to-scene representation transfer and synthetic-to-real feature alignment. Remarkably, our method exhibits superior performance compared to current state-of-the-art techniques, achieving 87.8% improvement in mAP and closely approaching the performance of fully supervised methods (62.15 mAP vs. 69.02 mAP) on HuCenLife Dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2403.02867",
        "abstract url": "https://arxiv.org/abs/2403.02867",
        "title": "Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02870",
        "abstract url": "https://arxiv.org/abs/2403.02870",
        "title": "Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices",
        "rating": -0.5,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model. Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no model information about the victim model.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Accepted by 27th European Symposium on Research in Computer Security (ESORICS 2022)"
    },
    {
        "paper id": "2403.02957",
        "abstract url": "https://arxiv.org/abs/2403.02957",
        "title": "On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02974",
        "abstract url": "https://arxiv.org/abs/2403.02974",
        "title": "Online Learning of Human Constraints from Feedback in Shared Autonomy",
        "rating": -0.5,
        "keywords": [
            [
                "robot"
            ],
            [
                "Workshop",
                "AAAI"
            ]
        ],
        "abstract": "Real-time collaboration with humans poses challenges due to the different behavior patterns of humans resulting from diverse physical constraints. Existing works typically focus on learning safety constraints for collaboration, or how to divide and distribute the subtasks between the participating agents to carry out the main task. In contrast, we propose to learn a human constraints model that, in addition, considers the diverse behaviors of different human operators. We consider a type of collaboration in a shared-autonomy fashion, where both a human operator and an assistive robot act simultaneously in the same task space that affects each other's actions. The task of the assistive agent is to augment the skill of humans to perform a shared task by supporting humans as much as possible, both in terms of reducing the workload and minimizing the discomfort for the human operator. Therefore, we propose an augmentative assistant agent capable of learning and adapting to human physical constraints, aligning its actions with the ergonomic preferences and limitations of the human operator.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted to AAAI-24 Bridge Program on Collaborative AI and Modeling of Humans & AAAI-24 Workshop on Ad Hoc Teamwork"
    },
    {
        "paper id": "2403.03008",
        "abstract url": "https://arxiv.org/abs/2403.03008",
        "title": "Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content. Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations. However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM. Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner. We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners. Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03333",
        "abstract url": "https://arxiv.org/abs/2403.03333",
        "title": "Solution Simplex Clustering for Heterogeneous Federated Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and personalized FL with minimal computational overhead.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03339",
        "abstract url": "https://arxiv.org/abs/2403.03339",
        "title": "An Online Approach to Solving Public Transit Stationing and Dispatch Problem",
        "rating": -0.5,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Public bus transit systems provide critical transportation services for large sections of modern communities. On-time performance and maintaining the reliable quality of service is therefore very important. Unfortunately, disruptions caused by overcrowding, vehicular failures, and road accidents often lead to service performance degradation. Though transit agencies keep a limited number of vehicles in reserve and dispatch them to relieve the affected routes during disruptions, the procedure is often ad-hoc and has to rely on human experience and intuition to allocate resources (vehicles) to affected trips under uncertainty. In this paper, we describe a principled approach using non-myopic sequential decision procedures to solve the problem and decide (a) if it is advantageous to anticipate problems and proactively station transit buses near areas with high-likelihood of disruptions and (b) decide if and which vehicle to dispatch to a particular problem. Our approach was developed in partnership with the Metropolitan Transportation Authority for a mid-sized city in the USA and models the system as a semi-Markov decision problem (solved as a Monte-Carlo tree search procedure) and shows that it is possible to obtain an answer to these two coupled decision problems in a way that maximizes the overall reward (number of people served). We sample many possible futures from generative models, each is assigned to a tree and processed using root parallelization. We validate our approach using 3 years of data from our partner agency. Our experiments show that the proposed framework serves 2% more passengers while reducing deadhead miles by 40%.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03359",
        "abstract url": "https://arxiv.org/abs/2403.03359",
        "title": "RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging",
        "rating": -0.5,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social Value Orientation to weight the vehicle's level of social cooperation and is divided into ego vehicle and surrounding vehicle utility which are weighted according to the model's designated Social Value Orientation. A two-lane highway with an on-ramp divided into a taper-style and parallel-style section is considered. Simulation results indicated the importance of considering surrounding vehicles in reward function design and show that the proposed model matches or surpasses those in literature in terms of collisions while also introducing socially courteous behaviour avoiding near misses and anti-social behaviour through direct consideration of the effect of merging on surrounding vehicles.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Updated explanation of TTC, page 7"
    },
    {
        "paper id": "2403.03372",
        "abstract url": "https://arxiv.org/abs/2403.03372",
        "title": "TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations",
        "rating": -0.5,
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce TartanAviation, an open-source multi-modal dataset focused on terminal-area airspace operations. TartanAviation provides a holistic view of the airport environment by concurrently collecting image, speech, and ADS-B trajectory data using setups installed inside airport boundaries. The datasets were collected at both towered and non-towered airfields across multiple months to capture diversity in aircraft operations, seasons, aircraft types, and weather conditions. In total, TartanAviation provides 3.1M images, 3374 hours of Air Traffic Control speech data, and 661 days of ADS-B trajectory data. The data was filtered, processed, and validated to create a curated dataset. In addition to the dataset, we also open-source the code-base used to collect and pre-process the dataset, further enhancing accessibility and usability. We believe this dataset has many potential use cases and would be particularly vital in allowing AI and machine learning technologies to be integrated into air traffic control systems and advance the adoption of autonomous aircraft in the airspace.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 6 figures, 2 tables"
    },
    {
        "paper id": "2403.03380",
        "abstract url": "https://arxiv.org/abs/2403.03380",
        "title": "On the Monotonicity of Information Aging",
        "rating": -0.5,
        "keywords": [
            [
                "Workshop"
            ]
        ],
        "abstract": "In this paper, we analyze the monotonicity of information aging in a remote estimation system, where historical observations of a Gaussian autoregressive AR(p) process are used to predict its future values. We consider two widely used loss functions in estimation: (i) logarithmic loss function for maximum likelihood estimation and (ii) quadratic loss function for MMSE estimation. The estimation error of the AR(p) process is written as a generalized conditional entropy which has closed-form expressions. By using a new information-theoretic tool called $\u03b5$-Markov chain, we can evaluate the divergence of the AR(p) process from being a Markov chain. When the divergence $\u03b5$ is large, the estimation error of the AR(p) process can be far from a non-decreasing function of the Age of Information (AoI). Conversely, for small divergence $\u03b5$, the inference error is close to a non-decreasing AoI function. Each observation is a short sequence taken from the AR(p) process. As the observation sequence length increases, the parameter $\u03b5$ progressively reduces to zero, and hence the estimation error becomes a non-decreasing AoI function. These results underscore a connection between the monotonicity of information aging and the divergence of from being a Markov chain.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Part of this work has been accepted by IEEE INFOCOM ASoI Workshop, 2024"
    },
    {
        "paper id": "2403.03407",
        "abstract url": "https://arxiv.org/abs/2403.03407",
        "title": "Human vs. Machine: Language Models and Wargames",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03409",
        "abstract url": "https://arxiv.org/abs/2403.03409",
        "title": "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction. We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Published as a conference paper at ICLR 2024"
    },
    {
        "paper id": "2403.03433",
        "abstract url": "https://arxiv.org/abs/2403.03433",
        "title": "I3DE: An IDE for Inspecting Inconsistencies in PL/SQL Code",
        "rating": -0.5,
        "keywords": [
            [
                "Workshop"
            ]
        ],
        "abstract": "In this paper, we introduce I3DE (Inconsistency Inspecting IDE) - an IDE plugin to inspect inconsistencies in PL/SQL code. We first observed the potential issues, e.g., misuses or bugs, that are introduced by the inconsistent understanding of PL/SQL semantics by PL/SQL programmers and DBMS developers, and propose a metamorphic testing-based approach for inspecting such inconsistencies in PL/SQL code. We design and implement our approach in I3DE, a widely usable plugin for the IntelliJ Platform. We conducted a comparative user study involving 16 participants, and the findings indicate that I3DE is consistently effective and efficient in helping programmers identify and avoid inconsistencies across different programming difficulties",
        "subjects": [
            "cs.SE"
        ],
        "comment": "In 2024 First IDE Workshop (IDE' 24), April 20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 6 pages"
    },
    {
        "paper id": "2403.03447",
        "abstract url": "https://arxiv.org/abs/2403.03447",
        "title": "HDRFlow: Real-Time HDR Video Reconstruction with Large Motions",
        "rating": -0.5,
        "keywords": [
            [
                "HDR"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Reconstructing High Dynamic Range (HDR) video from image sequences captured with alternating exposures is challenging, especially in the presence of large camera or object motion. Existing methods typically align low dynamic range sequences using optical flow or attention mechanism for deghosting. However, they often struggle to handle large complex motions and are computationally expensive. To address these challenges, we propose a robust and efficient flow estimator tailored for real-time HDR video reconstruction, named HDRFlow. HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an efficient flow network with a multi-size large kernel (MLK), and a new HDR flow training scheme. The HALoss supervises our flow network to learn an HDR-oriented flow for accurate alignment in saturated and dark regions. The MLK can effectively model large motions at a negligible cost. In addition, we incorporate synthetic data, Sintel, into our training dataset, utilizing both its provided forward flow and backward flow generated by us to supervise our flow network, enhancing our performance in large motion regions. Extensive experiments demonstrate that our HDRFlow outperforms previous methods on standard benchmarks. To the best of our knowledge, HDRFlow is the first real-time HDR video reconstruction method for video sequences captured with alternating exposures, capable of processing 720p resolution inputs at 25ms.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024; Project website: https://openimaginglab.github.io/HDRFlow/"
    },
    {
        "paper id": "2403.14670",
        "abstract url": "https://arxiv.org/abs/2403.14670",
        "title": "Electric Vehicle Enquiry (EVE) Pilot",
        "rating": -0.5,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "This data paper presents the dataset from a study on the use of electric vehicles (EVs). This dataset covers the first dataset collected in this study: the usage data from a Renault Zoe over 3 years. The process of collection of the dataset, its treatment, and descriptions of all the included variables are detailed. The collection of this dataset represents an iteration of participative research in the personal mobility domain as the dataset was collected with low-cost commercially available equipment and open-source software. Some of the challenges of providing the dataset are also discussed: the most pertinent being the intermittent nature of data collection as an android phone and OBDII adapter were used to collect the dataset.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02665",
        "abstract url": "https://arxiv.org/abs/2403.02665",
        "title": "DGAP: Efficient Dynamic Graph Analysis on Persistent Memory",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Dynamic graphs, featuring continuously updated vertices and edges, have grown in importance for numerous real-world applications. To accommodate this, graph frameworks, particularly their internal data structures, must support both persistent graph updates and rapid graph analysis simultaneously, leading to complex designs to orchestrate `fast but volatile' and `persistent but slow' storage devices. Emerging persistent memory technologies, such as Optane DCPMM, offer a promising alternative to simplify the designs by providing data persistence, low latency, and high IOPS together. In light of this, we propose DGAP, a framework for efficient dynamic graph analysis on persistent memory. Unlike traditional dynamic graph frameworks, which combine multiple graph data structures (e.g., edge list or adjacency list) to achieve the required performance, DGAP utilizes a single mutable Compressed Sparse Row (CSR) graph structure with new designs for persistent memory to construct the framework. Specifically, DGAP introduces a \\textit{per-section edge log} to reduce write amplification on persistent memory; a \\textit{per-thread undo log} to enable high-performance, crash-consistent rebalancing operations; and a data placement schema to minimize in-place updates on persistent memory. Our extensive evaluation results demonstrate that DGAP can achieve up to $3.2\\times$ better graph update performance and up to $3.77\\times$ better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, such as XPGraph, LLAMA, and GraphOne.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02667",
        "abstract url": "https://arxiv.org/abs/2403.02667",
        "title": "G-EvoNAS: Evolutionary Neural Architecture Search Based on Network Growth",
        "rating": -1,
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ]
        ],
        "abstract": "The evolutionary paradigm has been successfully applied to neural network search(NAS) in recent years. Due to the vast search complexity of the global space, current research mainly seeks to repeatedly stack partial architectures to build the entire model or to seek the entire model based on manually designed benchmark modules. The above two methods are attempts to reduce the search difficulty by narrowing the search space. To efficiently search network architecture in the global space, this paper proposes another solution, namely a computationally efficient neural architecture evolutionary search framework based on network growth (G-EvoNAS). The complete network is obtained by gradually deepening different Blocks. The process begins from a shallow network, grows and evolves, and gradually deepens into a complete network, reducing the search complexity in the global space. Then, to improve the ranking accuracy of the network, we reduce the weight coupling of each network in the SuperNet by pruning the SuperNet according to elite groups at different growth stages. The G-EvoNAS is tested on three commonly used image classification datasets, CIFAR10, CIFAR100, and ImageNet, and compared with various state-of-the-art algorithms, including hand-designed networks and NAS networks. Experimental results demonstrate that G-EvoNAS can find a neural network architecture comparable to state-of-the-art designs in 0.2 GPU days.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02674",
        "abstract url": "https://arxiv.org/abs/2403.02674",
        "title": "Revisiting Meta-evaluation for Grammatical Error Correction",
        "rating": -1,
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to TACL. This arXiv version is a pre-MIT Press publication version"
    },
    {
        "paper id": "2403.02679",
        "abstract url": "https://arxiv.org/abs/2403.02679",
        "title": "An Optimal Baseband Delay-Based Beam Squint Removal Scheme across a Range of Steering Angles for Digital Wideband Beamformers in Radars",
        "rating": -1,
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "This paper is an attempt to mitigate the beam squint happening due to frequency-dependent phase shifts in the wideband beamforming scenario, specifically in radar applications. The estimation of the direction of arrival is significant for precise target detection in radars. The undesirable beam squint effect due to the phase shift-only mechanism in conventional phased array systems, which becomes exacerbated when dealing with wide bandwidth signals, is analyzed for a large set of steering angles in this paper. An optimum baseband delay combined with the phase shift technique is proposed for wideband radar beamforming to mitigate beam squint effectively. This technique has been demonstrated to function properly with 1-GHz carrier frequency for signals with wide bandwidths of up to +/-250MHz and for steering angles ranging from 0 to 90 degrees.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "6 pages, 5 figures, submitted and accepted for presentation at IEEE Radar Conference 2024"
    },
    {
        "paper id": "2403.02687",
        "abstract url": "https://arxiv.org/abs/2403.02687",
        "title": "Enhanced DareFightingICE Competitions: Sound Design and AI Competitions",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "This paper presents a new and improved DareFightingICE platform, a fighting game platform with a focus on visually impaired players (VIPs), in the Unity game engine. It also introduces the separation of the DareFightingICE Competition into two standalone competitions called DareFightingICE Sound Design Competition and DareFightingICE AI Competition--at the 2024 IEEE Conference on Games (CoG)--in which a new platform will be used. This new platform is an enhanced version of the old DareFightingICE platform, having a better audio system to convey 3D sound and a better way to send audio data to AI agents. With this enhancement and by utilizing Unity, the new DareFightingICE platform is more accessible in terms of adding new features for VIPs and future audio research. This paper also improves the evaluation method for evaluating sound designs in the Sound Design Competition which will ensure a better sound design for VIPs as this competition continues to run at future CoG. To the best of our knowledge, both of our competitions are first of their kind, and the connection between the competitions to mutually improve the entries' quality with time makes these competitions an important part of representing an often overlooked segment within the broader gaming community, VIPs.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "This paper describes a new competition platform using Unity for our competitions at the 2024 IEEE Conference on Games (CoG 2024). It was accepted for presentation at CoG 2024. However, we recently discovered a much more effective way to do this task without using Unity, leading to our decision to withdraw the paper from CoG 2024 and ArXiv"
    },
    {
        "paper id": "2403.02692",
        "abstract url": "https://arxiv.org/abs/2403.02692",
        "title": "Uplift Modeling for Target User Attacks on Recommender Systems",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by WWW'24 as an oral paper"
    },
    {
        "paper id": "2403.02697",
        "abstract url": "https://arxiv.org/abs/2403.02697",
        "title": "Noise misleads rotation invariant algorithms on sparse targets",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "It is well known that the class of rotation invariant algorithms are suboptimal even for learning sparse linear problems when the number of examples is below the \"dimension\" of the problem. This class includes any gradient descent trained neural net with a fully-connected input layer (initialized with a rotationally symmetric distribution). The simplest sparse problem is learning a single feature out of $d$ features. In that case the classification error or regression loss grows with $1-k/n$ where $k$ is the number of examples seen. These lower bounds become vacuous when the number of examples $k$ reaches the dimension $d$. We show that when noise is added to this sparse linear problem, rotation invariant algorithms are still suboptimal after seeing $d$ or more examples. We prove this via a lower bound for the Bayes optimal algorithm on a rotationally symmetrized problem. We then prove much lower upper bounds on the same problem for simple non-rotation invariant algorithms. Finally we analyze the gradient flow trajectories of many standard optimization algorithms in some simple cases and show how they veer toward or away from the sparse targets. We believe that our trajectory categorization will be useful in designing algorithms that can exploit sparse targets and our method for proving lower bounds will be crucial for analyzing other families of algorithms that admit different classes of invariances.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02736",
        "abstract url": "https://arxiv.org/abs/2403.02736",
        "title": "Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery",
        "rating": -1,
        "keywords": [
            [
                "Satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Rare object detection is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with. This paper addresses the problem of bootstrapping such a rare object detection task assuming there is no labeled data and no spatial prior over the area of interest. We propose novel offline and online cluster-based approaches for sampling patches that are significantly more efficient, in terms of exposing positive samples to a human annotator, than random sampling. We apply our methods for identifying bomas, or small enclosures for herd animals, in the Serengeti Mara region of Kenya and Tanzania. We demonstrate a significant enhancement in detection efficiency, achieving a positive sampling rate increase from 2% (random) to 30%. This advancement enables effective machine learning mapping even with minimal labeling budgets, exemplified by an F1 score on the boma detection task of 0.51 with a budget of 300 total patches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02742",
        "abstract url": "https://arxiv.org/abs/2403.02742",
        "title": "Towards Training A Chinese Large Language Model for Anesthesiology",
        "rating": -1,
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Medical large language models (LLMs) have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of LLMs in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one LLM to assess the quality of the generated data from another LLM and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by fine-tuning LLMs using the general medicine data and subsequently improving the fine-tuned LLMs using data specifically from Anesthesiology. The general medical data supplement the medical expertise in Anesthesiology and enhance the effectiveness of Hypnos' generation. 3) We introduce a standardized benchmark for evaluating medical LLM in Anesthesiology. Our benchmark includes both publicly available instances from the Internet and privately obtained cases from the Hospital. Hypnos outperforms other medical LLMs in anesthesiology in metrics, GPT-4, and human evaluation on the benchmark dataset.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02773",
        "abstract url": "https://arxiv.org/abs/2403.02773",
        "title": "LodeStar: Maritime Radar Descriptor for Semi-Direct Radar Odometry",
        "rating": -1,
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "Maritime radars are prevalently adopted to capture the vessel's omnidirectional data as imagery. Nevertheless, inherent challenges persist with marine radars, including limited frequency, suboptimal resolution, and indeterminate detections. Additionally, the scarcity of discernible landmarks in the vast marine expanses remains a challenge, resulting in consecutive scenes that often lack matching feature points. In this context, we introduce a resilient maritime radar scan representation LodeStar, and an enhanced feature extraction technique tailored for marine radar applications. Moreover, we embark on estimating marine radar odometry utilizing a semi-direct approach. LodeStar-based approach markedly attenuates the errors in odometry estimation, and our assertion is corroborated through meticulous experimental validation.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "IEEE Robotics and Automation Letter"
    },
    {
        "paper id": "2403.02774",
        "abstract url": "https://arxiv.org/abs/2403.02774",
        "title": "Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on the downscaling task. Further, our method generalizes to climate states unseen during training without explicitly formulated physical constraints.",
        "subjects": [
            "physics.ao-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02784",
        "abstract url": "https://arxiv.org/abs/2403.02784",
        "title": "DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation",
        "rating": -1,
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semantic segmentation of remote sensing images is a challenging and hot issue due to the large amount of unlabeled data. Unsupervised domain adaptation (UDA) has proven to be advantageous in incorporating unclassified information from the target domain. However, independently fine-tuning UDA models on the source and target domains has a limited effect on the outcome. This paper proposes a hybrid training strategy as well as a novel dual-domain image fusion strategy that effectively utilizes the original image, transformation image, and intermediate domain information. Moreover, to enhance the precision of pseudo-labels, we present a pseudo-label region-specific weight strategy. The efficacy of our approach is substantiated by extensive benchmark experiments and ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02803",
        "abstract url": "https://arxiv.org/abs/2403.02803",
        "title": "Towards Robust Federated Learning via Logits Calibration on Non-IID Data",
        "rating": -1,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Federated learning (FL) is a privacy-preserving distributed management framework based on collaborative model training of distributed devices in edge networks. However, recent studies have shown that FL is vulnerable to adversarial examples (AEs), leading to a significant drop in its performance. Meanwhile, the non-independent and identically distributed (non-IID) challenge of data distribution between edge devices can further degrade the performance of models. Consequently, both AEs and non-IID pose challenges to deploying robust learning models at the edge. In this work, we adopt the adversarial training (AT) framework to improve the robustness of FL models against adversarial example (AE) attacks, which can be termed as federated adversarial training (FAT). Moreover, we address the non-IID challenge by implementing a simple yet effective logits calibration strategy under the FAT framework, which can enhance the robustness of models when subjected to adversarial attacks. Specifically, we employ a direct strategy to adjust the logits output by assigning higher weights to classes with small samples during training. This approach effectively tackles the class imbalance in the training data, with the goal of mitigating biases between local and global models. Experimental results on three dataset benchmarks, MNIST, Fashion-MNIST, and CIFAR-10 show that our strategy achieves competitive results in natural and robust accuracy compared to several baselines.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IEEE NOMS 2024"
    },
    {
        "paper id": "2403.02808",
        "abstract url": "https://arxiv.org/abs/2403.02808",
        "title": "Face-hitting Dominating Sets in Planar Graphs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A dominating set of a graph $G$ is a subset $S$ of its vertices such that each vertex of $G$ not in $S$ has a neighbor in $S$. A face-hitting set of a plane graph $G$ is a set $T$ of vertices in $G$ such that every face of $G$ contains at least one vertex of $T$. We show that the vertex-set of every plane (multi-)graph without isolated vertices, self-loops or $2$-faces can be partitioned into two disjoint sets so that both the sets are dominating and face-hitting. We also show that all the three assumptions above are necessary for the conclusion. As a corollary, we show that every $n$-vertex simple plane triangulation has a dominating set of size at most $(1 - \u03b1)n/2$, where $\u03b1n$ is the maximum size of an independent set in the triangulation. Matheson and Tarjan [European J. Combin., 1996] conjectured that every plane triangulation with a sufficiently large number of vertices $n$ has a dominating set of size at most $n / 4$. Currently, the best known general bound for this is by Christiansen, Rotenberg and Rutschmann [SODA, 2024] who showed that every plane triangulation on $n > 10$ vertices has a dominating set of size at most $2n/7$. Our corollary improves their bound for $n$-vertex plane triangulations which contain a maximal independent set of size either less than $2n/7$ or more than $3n/7$.",
        "subjects": [
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02817",
        "abstract url": "https://arxiv.org/abs/2403.02817",
        "title": "Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "In the past year, numerous companies have incorporated Generative AI (GenAI) capabilities into new and existing applications, forming interconnected Generative AI (GenAI) ecosystems consisting of semi/fully autonomous agents powered by GenAI services. While ongoing research highlighted risks associated with the GenAI layer of agents (e.g., dialog poisoning, membership inference, prompt leaking, jailbreaking), a critical question emerges: Can attackers develop malware to exploit the GenAI component of an agent and launch cyber-attacks on the entire GenAI ecosystem? This paper introduces Morris II, the first worm designed to target GenAI ecosystems through the use of adversarial self-replicating prompts. The study demonstrates that attackers can insert such prompts into inputs that, when processed by GenAI models, prompt the model to replicate the input as output (replication), engaging in malicious activities (payload). Additionally, these inputs compel the agent to deliver them (propagate) to new agents by exploiting the connectivity within the GenAI ecosystem. We demonstrate the application of Morris II against GenAIpowered email assistants in two use cases (spamming and exfiltrating personal data), under two settings (black-box and white-box accesses), using two types of input data (text and images). The worm is tested against three different GenAI models (Gemini Pro, ChatGPT 4.0, and LLaVA), and various factors (e.g., propagation rate, replication, malicious activity) influencing the performance of the worm are evaluated.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Website: https://sites.google.com/view/compromptmized"
    },
    {
        "paper id": "2403.02877",
        "abstract url": "https://arxiv.org/abs/2403.02877",
        "title": "ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm. One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate. The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution. In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical. In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD. Specifically, we design a planning-oriented active learning method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes. Empirically, we show that our planning-oriented approach could outperform general active learning methods by a large margin. Notably, our method achieves comparable performance with state-of-the-art end-to-end AD methods - by using only 30% nuScenes data. We hope our work could inspire future works to explore end-to-end AD from a data-centric perspective in addition to methodology efforts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02882",
        "abstract url": "https://arxiv.org/abs/2403.02882",
        "title": "Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization",
        "rating": -1,
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "Most of the current studies on autonomous vehicle decision-making and control tasks based on reinforcement learning are conducted in simulated environments. The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance. It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes. In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO. We trained policies with deep reinforcement learning algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and high-fidelity microscopic traffic flow. Results indicate that the policy trained under domain randomization traffic flow has significantly better success rate and calculative reward compared to the models trained under other microscopic traffic flows.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02898",
        "abstract url": "https://arxiv.org/abs/2403.02898",
        "title": "Federated Learning Using Coupled Tensor Train Decomposition",
        "rating": -1,
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "Coupled tensor decomposition (CTD) can extract joint features from multimodal data in various applications. It can be employed for federated learning networks with data confidentiality. Federated CTD achieves data privacy protection by sharing common features and keeping individual features. However, traditional CTD schemes based on canonical polyadic decomposition (CPD) may suffer from low computational efficiency and heavy communication costs. Inspired by the efficient tensor train decomposition, we propose a coupled tensor train (CTT) decomposition for federated learning. The distributed coupled multi-way data are decomposed into a series of tensor trains with shared factors. In this way, we can extract common features of coupled modes while maintaining the different features of uncoupled modes. Thus the privacy preservation of information across different network nodes can be ensured. The proposed CTT approach is instantiated for two fundamental network structures, namely master-slave and decentralized networks. Experimental results on synthetic and real datasets demonstrate the superiority of the proposed schemes over existing methods in terms of both computational efficiency and communication rounds. In a classification task, experimental results show that the CTT-based federated learning achieves almost the same accuracy performance as that of the centralized counterpart.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02905",
        "abstract url": "https://arxiv.org/abs/2403.02905",
        "title": "MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model",
        "rating": -1,
        "keywords": [
            [
                "Diffusion",
                "synthesizing"
            ]
        ],
        "abstract": "The body movements accompanying speech aid speakers in expressing their ideas. Co-speech motion generation is one of the important approaches for synthesizing realistic avatars. Due to the intricate correspondence between speech and motion, generating realistic and diverse motion is a challenging task. In this paper, we propose MMoFusion, a Multi-modal co-speech Motion generation framework based on the diffusion model to ensure both the authenticity and diversity of generated motion. We propose a progressive fusion strategy to enhance the interaction of inter-modal and intra-modal, efficiently integrating multi-modal information. Specifically, we employ a masked style matrix based on emotion and identity information to control the generation of different motion styles. Temporal modeling of speech and motion is partitioned into style-guided specific feature encoding and shared feature encoding, aiming to learn both inter-modal and intra-modal features. Besides, we propose a geometric loss to enforce the joints' velocity and acceleration coherence among frames. Our framework generates vivid, diverse, and style-controllable motion of arbitrary length through inputting speech and editing identity and emotion. Extensive experiments demonstrate that our method outperforms current co-speech motion generation methods including upper body and challenging full body.",
        "subjects": [
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02917",
        "abstract url": "https://arxiv.org/abs/2403.02917",
        "title": "A Miniaturized Device for Ultrafast On-demand Drug Release based on a Gigahertz Ultrasonic Resonator",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "On-demand controlled drug delivery is essential for the treatment of a wide range of chronic diseases. As the drug is released at the time when required, its efficacy is boosted and the side effects are minimized. However, so far, drug delivery devices often rely on the passive diffusion process for a sustained release, which is slow and uncontrollable. Here, we present a miniaturized microfluidic device for wirelessly controlled ultrafast active drug delivery, driven by an oscillating solid-liquid interface. The oscillation generates acoustic streaming in the drug reservoir, which opens an elastic valve to deliver the drug. High-speed microscopy reveals the fast response of the valve on the order of 1 ms, which is more than three orders of magnitude faster than the start-of-the-art. The amount of the released drug exhibits a linear relationship with the working time and the electric power applied to the ultrasonic resonator. The trigger of the release is wirelessly controlled via a magnetic field, and the system shows stable output in a continuous experiment for two weeks. The integrated system shows great promise as a long-term controlled drug delivery implant for chronic diseases.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "19 pages, 6 figures, 1 table"
    },
    {
        "paper id": "2403.02918",
        "abstract url": "https://arxiv.org/abs/2403.02918",
        "title": "Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "In this paper, we study how well human speech can automatically be filtered when this overlaps with the voice and fan noise of a social robot, Pepper. We ultimately aim for an HRI scenario where the microphone can remain open when the robot is speaking, enabling a more natural turn-taking scheme where the human can interrupt the robot. To respond appropriately, the robot would need to understand what the interlocutor said in the overlapping part of the speech, which can be accomplished by target speech extraction (TSE). To investigate how well TSE can be accomplished in the context of the popular social robot Pepper, we set out to manufacture a datase composed of a mixture of recorded speech of Pepper itself, its fan noise (which is close to the microphones), and human speech as recorded by the Pepper microphone, in a room with low reverberation and high reverberation. Comparing a signal processing approach, with and without post-filtering, and a convolutional recurrent neural network (CRNN) approach to a state-of-the-art speaker identification-based TSE model, we found that the signal processing approach without post-filtering yielded the best performance in terms of Word Error Rate on the overlapping speech signals with low reverberation, while the CRNN approach is more robust for reverberation. These results show that estimating the human voice in overlapping speech with a robot is possible in real-life application, provided that the room reverberation is low and the human speech has a high volume or high pitch.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted by ACM Technological Advances in Human-Robot Interaction. 9 pages"
    },
    {
        "paper id": "2403.02928",
        "abstract url": "https://arxiv.org/abs/2403.02928",
        "title": "User-Driven Adaptation: Tailoring Autonomous Driving Systems with Dynamic Preferences",
        "rating": -1,
        "keywords": [
            [
                "Autonomous Driving"
            ]
        ],
        "abstract": "In the realm of autonomous vehicles, dynamic user preferences are critical yet challenging to accommodate. Existing methods often misrepresent these preferences, either by overlooking their dynamism or overburdening users as humans often find it challenging to express their objectives mathematically. The previously introduced framework, which interprets dynamic preferences as inherent uncertainty and includes a ``human-on-the-loop'' mechanism enabling users to give feedback when dissatisfied with system behaviors, addresses this gap. In this study, we further enhance the approach with a user study of 20 participants, focusing on aligning system behavior with user expectations through feedback-driven adaptation. The findings affirm the approach's ability to effectively merge algorithm-driven adjustments with user complaints, leading to improved participants' subjective satisfaction in autonomous systems.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "accepted by CHI LBW 2024"
    },
    {
        "paper id": "2403.02955",
        "abstract url": "https://arxiv.org/abs/2403.02955",
        "title": "XAI-Based Detection of Adversarial Attacks on Deepfake Detectors",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "We introduce a novel methodology for identifying adversarial attacks on deepfake detectors using eXplainable Artificial Intelligence (XAI). In an era characterized by digital advancement, deepfakes have emerged as a potent tool, creating a demand for efficient detection systems. However, these systems are frequently targeted by adversarial attacks that inhibit their performance. We address this gap, developing a defensible deepfake detector by leveraging the power of XAI. The proposed methodology uses XAI to generate interpretability maps for a given method, providing explicit visualizations of decision-making factors within the AI models. We subsequently employ a pretrained feature extractor that processes both the input image and its corresponding XAI image. The feature embeddings extracted from this process are then used for training a simple yet effective classifier. Our approach contributes not only to the detection of deepfakes but also enhances the understanding of possible adversarial attacks, pinpointing potential vulnerabilities. Furthermore, this approach does not change the performance of the deepfake detector. The paper demonstrates promising results suggesting a potential pathway for future deepfake detection mechanisms. We believe this study will serve as a valuable contribution to the community, sparking much-needed discourse on safeguarding deepfake detectors.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02965",
        "abstract url": "https://arxiv.org/abs/2403.02965",
        "title": "ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities",
        "rating": -1,
        "keywords": [
            [
                "biometrics",
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02997",
        "abstract url": "https://arxiv.org/abs/2403.02997",
        "title": "Cover Edge-Based Novel Triangle Counting",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Listing and counting triangles in graphs is a key algorithmic kernel for network analyses, including community detection, clustering coefficients, k-trusses, and triangle centrality. In this paper, we propose the novel concept of a cover-edge set that can be used to find triangles more efficiently. Leveraging the breadth-first search (BFS) method, we can quickly generate a compact cover-edge set. Novel sequential and parallel triangle counting algorithms that employ cover-edge sets are presented. The novel sequential algorithm performs competitively with the fastest previous approaches on both real and synthetic graphs, such as those from the Graph500 Benchmark and the MIT/Amazon/IEEE Graph Challenge. We implement 22 sequential algorithms for performance evaluation and comparison. At the same time, we employ OpenMP to parallelize 11 sequential algorithms, presenting an in-depth analysis of their parallel performance. Furthermore, we develop a distributed parallel algorithm that can asymptotically reduce communication on massive graphs. In our estimate from massive-scale Graph500 graphs, our distributed parallel algorithm can reduce the communication on a scale~36 graph by 1156x and on a scale~42 graph by 2368x. Comprehensive experiments are conducted on the recently launched Intel Xeon 8480+ processor and shed light on how graph attributes, such as topology, diameter, and degree distribution, can affect the performance of these algorithms.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03013",
        "abstract url": "https://arxiv.org/abs/2403.03013",
        "title": "The clique chromatic number of sparse random graphs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The clique chromatic number of a graph is the smallest number of colors in a vertex coloring so that no maximal clique is monochromatic. In this paper, we determine the order of magnitude of the clique chromatic number of the random graph G_{n,p} for most edge-probabilities p in the range n^{-2/5} \\ll p \\ll 1. This resolves open problems and questions of Lichev, Mitsche and Warnke as well as Alon and Krievelevich. One major proof difficulty stems from high-degree vertices, which prevent maximal cliques in their neighborhoods: we deal with these vertices by an intricate union bound argument, that combines the probabilistic method with new degree counting arguments in order to enable Janson's inequality. This way we determine the asymptotics of the clique chromatic number of G_{n,p} in some ranges, and discover a surprising new phenomenon that contradicts earlier predictions for edge-probabilities p close to n^{-2/5}.",
        "subjects": [
            "math.CO"
        ],
        "comment": "24 pages"
    },
    {
        "paper id": "2403.03029",
        "abstract url": "https://arxiv.org/abs/2403.03029",
        "title": "Socratic Reasoning Improves Positive Text Rewriting",
        "rating": -1,
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \\textsc{SocraticReframe}. \\textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both automatic and human evaluations guided by criteria from psychotherapy research.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03055",
        "abstract url": "https://arxiv.org/abs/2403.03055",
        "title": "Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The simulation results verify our theoretical findings.",
        "subjects": [
            "cs.MA"
        ],
        "comment": "14 pages, 6 figures"
    },
    {
        "paper id": "2403.03101",
        "abstract url": "https://arxiv.org/abs/2403.03101",
        "title": "KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents",
        "rating": -1,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in https://github.com/zjunlp/KnowAgent.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress. Project page: https://zjunlp.github.io/project/KnowAgent/ Code: https://github.com/zjunlp/KnowAgent"
    },
    {
        "paper id": "2403.03111",
        "abstract url": "https://arxiv.org/abs/2403.03111",
        "title": "Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and Novel Outliers Detection",
        "rating": -1,
        "keywords": [
            [
                "LiDAR",
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Perception is a key element for enabling intelligent autonomous navigation. Understanding the semantics of the surrounding environment and accurate vehicle pose estimation are essential capabilities for autonomous vehicles, including self-driving cars and mobile robots that perform complex tasks. Fast moving platforms like self-driving cars impose a hard challenge for localization and mapping algorithms. In this work, we propose a novel framework for real-time LiDAR odometry and mapping based on LOAM architecture for fast moving platforms. Our framework utilizes semantic information produced by a deep learning model to improve point-to-line and point-to-plane matching between LiDAR scans and build a semantic map of the environment, leading to more accurate motion estimation using LiDAR data. We observe that including semantic information in the matching process introduces a new type of outlier matches to the process, where matching occur between different objects of the same semantic class. To this end, we propose a novel algorithm that explicitly identifies and discards potential outliers in the matching process. In our experiments, we study the effect of improving the matching process on the robustness of LiDAR odometry against high speed motion. Our experimental evaluations on KITTI dataset demonstrate that utilizing semantic information and rejecting outliers significantly enhance the robustness of LiDAR odometry and mapping when there are large gaps between scan acquisition poses, which is typical for fast moving platforms.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03114",
        "abstract url": "https://arxiv.org/abs/2403.03114",
        "title": "Equilibria in Two-Stage Facility Location with Atomic Clients",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We consider competitive facility location as a two-stage multi-agent system with two types of clients. For a given host graph with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities. Then, the clients strategically select which of the opened facilities in their neighborhood to patronize. Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility. All recently studied versions of this model assume that clients can split their weight strategically. We consider clients with unsplittable weights, but allow mixed strategies. So clients may randomize over which facility to patronize. Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible. As our main result, we show that pure subgame perfect equilibria always exist if all client weights are identical. For this, we use a novel potential function argument, employing a hierarchical classification of the clients and sophisticated rounding in each step. In contrast, for non-identical clients, we show that deciding the existence of even approximately stable states is computationally intractable. On the positive side, we give a tight bound of 2 on the price of anarchy which implies high social welfare of equilibria, if they exist.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03120",
        "abstract url": "https://arxiv.org/abs/2403.03120",
        "title": "Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "endoscopic"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Real-time computational speed and a high degree of precision are requirements for computer-assisted interventions. Applying a segmentation network to a medical video processing task can introduce significant inter-frame prediction noise. Existing approaches can reduce inconsistencies by including temporal information but often impose requirements on the architecture or dataset. This paper proposes a method to include temporal information in any segmentation model and, thus, a technique to improve video segmentation performance without alterations during training or additional labeling. With Motion-Corrected Moving Average, we refine the exponential moving average between the current and previous predictions. Using optical flow to estimate the movement between consecutive frames, we can shift the prior term in the moving-average calculation to align with the geometry of the current frame. The optical flow calculation does not require the output of the model and can therefore be performed in parallel, leading to no significant runtime penalty for our approach. We evaluate our approach on two publicly available segmentation datasets and two proprietary endoscopic datasets and show improvements over a baseline approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03157",
        "abstract url": "https://arxiv.org/abs/2403.03157",
        "title": "Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks",
        "rating": -1,
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "This study explores the benefits of integrating the novel clustered federated learning (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels. A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented. Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties. Specifically, users' data distributions are parameterized as concentration parameters and grouped using spectral clustering, with Dirichlet distribution serving as the prior. The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the derived closed-form solution. The extensive simulation results show that the proposed cluster-based FL framework can outperform FL baselines in terms of both test accuracy and convergence rate. Moreover, jointly optimizing sub-channel and power allocation in NOMA-enhanced networks can lead to a significant improvement.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03161",
        "abstract url": "https://arxiv.org/abs/2403.03161",
        "title": "PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning",
        "rating": -1,
        "keywords": [
            [
                "UAV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Palms play an outsized role in tropical forests and are important resources for humans and wildlife. A central question in tropical ecosystems is understanding palm distribution and abundance. However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest. This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests. Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patches in two distinct sizes. These patches are then used to train models with an identical architecture, consisting of an unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with specifically trained parameters. Subsequently, PalmProbNet employs a sliding window technique on the landscape orthomosaic, using both small and large window sizes to generate a probability heatmap. This heatmap effectively visualizes the distribution of palms, showcasing the scalability and adaptability of our approach in various forest densities. Despite the challenging terrain, our method demonstrated remarkable performance, achieving an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "11 pages, 6 figures, and 1 table, to appear in ACMSE 2024"
    },
    {
        "paper id": "2403.03205",
        "abstract url": "https://arxiv.org/abs/2403.03205",
        "title": "Finding Super-spreaders in Network Cascades",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Suppose that a cascade (e.g., an epidemic) spreads on an unknown graph, and only the infection times of vertices are observed. What can be learned about the graph from the infection times caused by multiple distinct cascades? Most of the literature on this topic focuses on the task of recovering the entire graph, which requires $\u03a9( \\log n)$ cascades for an $n$-vertex bounded degree graph. Here we ask a different question: can the important parts of the graph be estimated from just a few (i.e., constant number) of cascades, even as $n$ grows large? In this work, we focus on identifying super-spreaders (i.e., high-degree vertices) from infection times caused by a Susceptible-Infected process on a graph. Our first main result shows that vertices of degree greater than $n^{3/4}$ can indeed be estimated from a constant number of cascades. Our algorithm for doing so leverages a novel connection between vertex degrees and the second derivative of the cumulative infection curve. Conversely, we show that estimating vertices of degree smaller than $n^{1/2}$ requires at least $\\log(n) / \\log \\log (n)$ cascades. Surprisingly, this matches (up to $\\log \\log n$ factors) the number of cascades needed to learn the \\emph{entire} graph if it is a tree.",
        "subjects": [
            "math.ST"
        ],
        "comment": "32 pages, 3 figures. Main updates are (1) a relaxation of graph assumptions and (2) a slight sharpening of previous techniques that allows us to estimate the infection time of high-degree vertices from a single cascade"
    },
    {
        "paper id": "2403.03241",
        "abstract url": "https://arxiv.org/abs/2403.03241",
        "title": "A Deep Learning Framework for Wireless Radiation Field Reconstruction and Channel Prediction",
        "rating": -1,
        "keywords": [
            [
                "NeRF",
                "Radiance Fields"
            ]
        ],
        "abstract": "We present NeWRF, a deep learning framework for predicting wireless channels. Wireless channel prediction is a long-standing problem in the wireless community and is a key technology for improving the coverage of wireless network deployments. Today, a wireless deployment is evaluated by a site survey which is a cumbersome process requiring an experienced engineer to perform extensive channel measurements. To reduce the cost of site surveys, we develop NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF). NeWRF trains a neural network model with a sparse set of channel measurements, and predicts the wireless channel accurately at any location in the site. We introduce a series of techniques that integrate wireless propagation properties into the NeRF framework to account for the fundamental differences between the behavior of light and wireless signals. We conduct extensive evaluations of our framework and show that our approach can accurately predict channels at unvisited locations with significantly lower measurement density than prior state-of-the-art",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03273",
        "abstract url": "https://arxiv.org/abs/2403.03273",
        "title": "DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation",
        "rating": -1,
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning models have emerged as the cornerstone of medical image segmentation, but their efficacy hinges on the availability of extensive manually labeled datasets and their adaptability to unforeseen categories remains a challenge. Few-shot segmentation (FSS) offers a promising solution by endowing models with the capacity to learn novel classes from limited labeled examples. A leading method for FSS is ALPNet, which compares features between the query image and the few available support segmented images. A key question about using ALPNet is how to design its features. In this work, we delve into the potential of using features from DINOv2, which is a foundational self-supervised learning model in computer vision. Leveraging the strengths of ALPNet and harnessing the feature extraction capabilities of DINOv2, we present a novel approach to few-shot segmentation that not only enhances performance but also paves the way for more robust and adaptable medical image analysis.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03326",
        "abstract url": "https://arxiv.org/abs/2403.03326",
        "title": "AnatoMix: Anatomy-aware Data Augmentation for Multi-organ Segmentation",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "CT",
                "organ"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Multi-organ segmentation in medical images is a widely researched task and can save much manual efforts of clinicians in daily routines. Automating the organ segmentation process using deep learning (DL) is a promising solution and state-of-the-art segmentation models are achieving promising accuracy. In this work, We proposed a novel data augmentation strategy for increasing the generalizibility of multi-organ segmentation datasets, namely AnatoMix. By object-level matching and manipulation, our method is able to generate new images with correct anatomy, i.e. organ segmentation mask, exponentially increasing the size of the segmentation dataset. Initial experiments have been done to investigate the segmentation performance influenced by our method on a public CT dataset. Our augmentation method can lead to mean dice of 76.1, compared with 74.8 of the baseline method.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03336",
        "abstract url": "https://arxiv.org/abs/2403.03336",
        "title": "Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse",
        "rating": -1,
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we develop an LLM-powered framework for the curation and evaluation of emerging opinion mining in online health communities. We formulate emerging opinion mining as a pairwise stance detection problem between (title, comment) pairs sourced from Reddit, where post titles contain emerging health-related claims on a topic that is not predefined. The claims are either explicitly or implicitly expressed by the user. We detail (i) a method of claim identification -- the task of identifying if a post title contains a claim and (ii) an opinion mining-driven evaluation framework for stance detection using LLMs. We facilitate our exploration by releasing a novel test dataset, Long COVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of claim identification and stance detection in online health communities. Long Covid is an emerging post-COVID disorder with uncertain and complex treatment guidelines, thus making it a suitable use case for our task. LC-Stance contains long COVID treatment related discourse sourced from a Reddit community. Our evaluation shows that GPT-4 significantly outperforms prior works on zero-shot stance detection. We then perform thorough LLM model diagnostics, identifying the role of claim type (i.e. implicit vs explicit claims) and comment length as sources of model error.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03353",
        "abstract url": "https://arxiv.org/abs/2403.03353",
        "title": "Hypothesis Spaces for Deep Learning",
        "rating": -1,
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03385",
        "abstract url": "https://arxiv.org/abs/2403.03385",
        "title": "Multi-modal Deep Learning",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "healthcare",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "This article investigates deep learning methodologies for single-modality clinical data analysis, as a crucial precursor to multi-modal medical research. Building on Guo JingYuan's work, the study refines clinical data processing through Compact Convolutional Transformer (CCT), Patch Up, and the innovative CamCenterLoss technique, establishing a foundation for future multimodal investigations. The proposed methodology demonstrates improved prediction accuracy and at tentiveness to critically ill patients compared to Guo JingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained vision transformer backbone to perform transfer learning time-series clinical data.The study highlights the potential of CCT, Patch Up, and novel CamCenterLoss in processing single modality clinical data within deep learning frameworks, paving the way for future multimodal medical research and promoting precision and personalized healthcare",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Master's thesis"
    },
    {
        "paper id": "2403.03390",
        "abstract url": "https://arxiv.org/abs/2403.03390",
        "title": "Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection",
        "rating": -1,
        "keywords": [
            [
                "agricultural"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Effective weed control plays a crucial role in optimizing crop yield and enhancing agricultural product quality. However, the reliance on herbicide application not only poses a critical threat to the environment but also promotes the emergence of resistant weeds. Fortunately, recent advances in precision weed management enabled by ML and DL provide a sustainable alternative. Despite great progress, existing algorithms are mainly developed based on supervised learning approaches, which typically demand large-scale datasets with manual-labeled annotations, which is time-consuming and labor-intensive. As such, label-efficient learning methods, especially semi-supervised learning, have gained increased attention in the broader domain of computer vision and have demonstrated promising performance. These methods aim to utilize a small number of labeled data samples along with a great number of unlabeled samples to develop high-performing models comparable to the supervised learning counterpart trained on a large amount of labeled data samples. In this study, we assess the effectiveness of a semi-supervised learning framework for multi-class weed detection, employing two well-known object detection frameworks, namely FCOS and Faster-RCNN. Specifically, we evaluate a generalized student-teacher framework with an improved pseudo-label generation module to produce reliable pseudo-labels for the unlabeled data. To enhance generalization, an ensemble student network is employed to facilitate the training process. Experimental results show that the proposed approach is able to achieve approximately 76\\% and 96\\% detection accuracy as the supervised methods with only 10\\% of labeled data in CottenWeedDet3 and CottonWeedDet12, respectively. We offer access to the source code, contributing a valuable resource for ongoing semi-supervised learning research in weed detection and beyond.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "11 pages, 7 figures"
    },
    {
        "paper id": "2403.03391",
        "abstract url": "https://arxiv.org/abs/2403.03391",
        "title": "CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-reduced Monte Carlo gradient estimator, CoRFM solves the Ising problems in a self-train fashion without data/evidence, and the inference tasks can be executed by directly sampling from RNN. Theoretically, we establish a provably tighter error bound than naive mean-field by using the matrix cut decomposition machineries. Numerically, we demonstrate the utility of this framework on a series of Ising datasets.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03400",
        "abstract url": "https://arxiv.org/abs/2403.03400",
        "title": "Contrastive Learning of Person-independent Representations for Facial Action Unit Detection",
        "rating": -1,
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Facial action unit (AU) detection, aiming to classify AU present in the facial image, has long suffered from insufficient AU annotations. In this paper, we aim to mitigate this data scarcity issue by learning AU representations from a large number of unlabelled facial videos in a contrastive learning paradigm. We formulate the self-supervised AU representation learning signals in two-fold: (1) AU representation should be frame-wisely discriminative within a short video clip; (2) Facial frames sampled from different identities but show analogous facial AUs should have consistent AU representations. As to achieve these goals, we propose to contrastively learn the AU representation within a video clip and devise a cross-identity reconstruction mechanism to learn the person-independent representations. Specially, we adopt a margin-based temporal contrastive learning paradigm to perceive the temporal AU coherence and evolution characteristics within a clip that consists of consecutive input facial frames. Moreover, the cross-identity reconstruction mechanism facilitates pushing the faces from different identities but show analogous AUs close in the latent embedding space. Experimental results on three public AU datasets demonstrate that the learned AU representation is discriminative for AU detection. Our method outperforms other contrastive learning methods and significantly closes the performance gap between the self-supervised and supervised AU detection approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03404",
        "abstract url": "https://arxiv.org/abs/2403.03404",
        "title": "Secure Total Domination Number in Maximal Outerplanar Graphs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A subset $S$ of vertices in a graph $G$ is a secure total dominating set of $G$ if $S$ is a total dominating set of $G$ and, for each vertex $u \\not\\in S$, there is a vertex $v \\in S$ such that $uv$ is an edge and $(S \\setminus \\{v\\}) \\cup \\{u\\}$ is also a total dominating set of $G$. We show that if $G$ is a maximal outerplanar graph of order $n$, then $G$ has a total secure dominating set of size at most $\\lfloor 2n/3 \\rfloor$. Moreover, if an outerplanar graph $G$ of order $n$, then each secure total dominating set has at least $\\lceil (n+2)/3 \\rceil$ vertices. We show that these bounds are best possible.",
        "subjects": [
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03429",
        "abstract url": "https://arxiv.org/abs/2403.03429",
        "title": "Generative Explanations for Program Synthesizers",
        "rating": -1,
        "keywords": [
            [
                "synthesis"
            ]
        ],
        "abstract": "Despite great advances in program synthesis techniques, they remain algorithmic black boxes. Although they guarantee that when synthesis is successful, the implementation satisfies the specification, they provide no additional information regarding how the implementation works or the manner in which the specification is realized. One possibility to answer these questions is to use large language models (LLMs) to construct human-readable explanations. Unfortunately, experiments reveal that LLMs frequently produce nonsensical or misleading explanations when applied to the unidiomatic code produced by program synthesizers. In this paper, we develop an approach to reliably augment the implementation with explanatory names. We recover fine-grained input-output data from the synthesis algorithm to enhance the prompt supplied to the LLM, and use a combination of a program verifier and a second language model to validate the proposed explanations before presenting them to the user. Together, these techniques massively improve the accuracy of the proposed names, from 24% to 79% respectively. Through a pair of small user studies, we find that users significantly prefer the explanations produced by our technique (76% of responses indicating the appropriateness of the presenting names) to the baseline (with only 2% of responses approving of the suggestions), and that the proposed names measurably help users in understanding the synthesized implementation.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03462",
        "abstract url": "https://arxiv.org/abs/2403.03462",
        "title": "Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori. However, each user's environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction. The architecture builds on core cognitive principles of learning and memory for efficient and real-time learning of new knowledge from humans. We integrate our architecture with a physical mobile manipulator robot and perform extensive system evaluations in a laboratory environment over two months. Our results demonstrate the effectiveness of our architecture to allow a physical robot to continually adapt to the changes in the environment from limited data provided by the users (experimenters), and use the learned knowledge to perform object fetching tasks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at the IEEE International Conference on Robotics and Automation (ICRA), 2024"
    },
    {
        "paper id": "2403.05370",
        "abstract url": "https://arxiv.org/abs/2403.05370",
        "title": "On the Certification of the Kinematics of 3-DOF Spherical Parallel Manipulators",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This paper aims to study a specific kind of parallel robot: Spherical Parallel Manipulators (SPM) that are capable of unlimited rolling. A focus is made on the kinematics of such mechanisms, especially taking into account uncertainties (e.g. on conception & fabrication parameters, measures) and their propagations. Such considerations are crucial if we want to control our robot correctly without any undesirable behavior in its workspace (e.g. effects of singularities). In this paper, we will consider two different approaches to study the kinematics and the singularities of the robot of interest: symbolic and semi-numerical. By doing so, we can compute a singularity-free zone in the work- and joint spaces, considering given uncertainties on the parameters. In this zone, we can use any control law to inertially stabilize the upper platform of the robot.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.12991",
        "abstract url": "https://arxiv.org/abs/2403.12991",
        "title": "Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free Traffic via a Spatio-Temporal Framework",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "GNN",
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vehicle flow, a crucial indicator for transportation, is often limited by detector coverage. With the advent of extensive mobile network coverage, we can leverage mobile user activities, or cellular traffic, on roadways as a proxy for vehicle flow. However, as counts of cellular traffic may not directly align with vehicle flow due to data from various user types, we present a new task: predicting vehicle flow in camera-free areas using cellular traffic. To uncover correlations within multi-source data, we deployed cameras on selected roadways to establish the Tel2Veh dataset, consisting of extensive cellular traffic and sparse vehicle flows. Addressing this challenge, we propose a framework that independently extracts features and integrates them with a graph neural network (GNN)-based fusion to discern disparities, thereby enabling the prediction of unseen vehicle flows using cellular traffic. This work advances the use of telecom data in transportation and pioneers the fusion of telecom and vision-based data, offering solutions for traffic management.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "4 pages, 5 figures, 4 tables. Accepted by WWW'24, to appear"
    },
    {
        "paper id": "2403.02683",
        "abstract url": "https://arxiv.org/abs/2403.02683",
        "title": "Learning to Defer to a Population: A Meta-Learning Approach",
        "rating": -1.5,
        "keywords": [
            [
                "diagnosis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis benchmarks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024)"
    },
    {
        "paper id": "2403.02708",
        "abstract url": "https://arxiv.org/abs/2403.02708",
        "title": "Backfire Effect Reveals Early Controversy in Online Media",
        "rating": -1.5,
        "keywords": [
            [
                "psychological"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "The rapid development of online media has significantly facilitated the public's information consumption, knowledge acquisition, and opinion exchange. However, it has also led to more violent conflicts in online discussions. Therefore, controversy detection becomes important for computational and social sciences. Previous research on detection methods has primarily focused on larger datasets and more complex computational models but has rarely examined the underlying mechanisms of conflict, particularly the psychological motivations behind them. In this paper, we present evidence that conflicting posts tend to have a high proportion of \"ascending gradient of likes\", i.e., replies get more likes than comments. Additionally, there is a gradient in the number of replies between the neighboring tiers as well. We develop two new gradient features and demonstrate the common enhancement effect of our features in terms of controversy detection models. Further, multiple evaluation algorithms are used to compare structural, interactive, and textual features with the new features across multiple Chinese and English media. The results show that it is a general case that gradient features are significantly different in terms of controversy and are more important than other features. More thoroughly, we discuss the mechanism by which the ascending gradient emerges, suggesting that the case is related to the \"backfire effect\" in ideological conflicts that have received recent attention. The features formed by the psychological mechanism also show excellent detection performance in application scenarios where only a few hot information or early information are considered. Our findings can provide a new perspective for online conflict behavior analysis and early detection.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "17 pages, 6 figures"
    },
    {
        "paper id": "2403.02723",
        "abstract url": "https://arxiv.org/abs/2403.02723",
        "title": "Minimum Topology Attacks for Graph Neural Networks",
        "rating": -1.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "Attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention. Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node. However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility. To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node. To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology. Extensive results on three GNNs and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges. Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Published on WWW 2023. Proceedings of the ACM Web Conference 2023"
    },
    {
        "paper id": "2403.02760",
        "abstract url": "https://arxiv.org/abs/2403.02760",
        "title": "Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations",
        "rating": -1.5,
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences. Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information. It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions. At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and reasoning capabilities. As a result, recent research has sought to harness the power of LLM to improve recommendation systems. Given the rapid development of this research direction in the field of recommendation systems, there is an urgent need for a systematic review of existing LLM-driven recommendation systems for researchers and practitioners in related fields to gain insight into. More specifically, we first introduced a representative approach to learning user and item representations using LLM as a feature encoder. We then reviewed the latest advances in LLMs techniques for collaborative filtering enhanced recommendation systems from the three paradigms of pre-training, fine-tuning, and prompting. Finally, we had a comprehensive discussion on the future direction of this emerging field.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02765",
        "abstract url": "https://arxiv.org/abs/2403.02765",
        "title": "G4-Attention: Deep Learning Model with Attention for predicting DNA G-Quadruplexes",
        "rating": -1.5,
        "keywords": [
            [
                "biological",
                "DNA"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary structures, formed by the stacking arrangement of the guanine tetramers. They are involved in a wide range of biological roles because of their exceptionally unique and distinct structural characteristics. After the completion of the human genome sequencing project, a lot of bioinformatic algorithms were introduced to predict the active G4s regions \\textit{in vitro} based on the canonical G4 sequence elements, G-\\textit{richness}, and G-\\textit{skewness}, as well as the non-canonical sequence features. Recently, sequencing techniques like G4-seq and G4-ChIP-seq were developed to map the G4s \\textit{in vitro}, and \\textit{in vivo} respectively at a few hundred base resolution. Subsequently, several machine learning approaches were developed for predicting the G4 regions using the existing databases. However, their prediction models were simplistic, and the prediction accuracy was notably poor. In response, here, we propose a novel convolutional neural network with Bi-LSTM and attention layers, named G4-attention, to predict the G4 forming sequences with improved accuracy. G4-attention achieves high accuracy and attains state-of-the-art results in the G4 prediction task. Our model also predicts the G4 regions accurately in the highly class-imbalanced datasets. In addition, the developed model trained on the human genome dataset can be applied to any non-human genome DNA sequences to predict the G4 formation propensities.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02780",
        "abstract url": "https://arxiv.org/abs/2403.02780",
        "title": "Data Collaboration Analysis Over Matrix Manifolds",
        "rating": -1.5,
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of this data in developing robust ML models. Within this realm, the Non-Readily Identifiable Data Collaboration (NRI-DC) framework emerges as an innovative approach, potentially resolving the 'data island' issue among institutions through non-iterative communication and robust privacy protections. However, in its current state, the NRI-DC framework faces model performance instability due to theoretical unsteadiness in creating collaboration functions. This study establishes a rigorous theoretical foundation for these collaboration functions and introduces new formulations through optimization problems on matrix manifolds and efficient solutions. Empirical analyses demonstrate that the proposed approach, particularly the formulation over orthogonal matrix manifolds, significantly enhances performance, maintaining consistency and efficiency without compromising communication efficiency or privacy protections.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "29 pages"
    },
    {
        "paper id": "2403.02814",
        "abstract url": "https://arxiv.org/abs/2403.02814",
        "title": "InjectTST: A Transformer Method of Injecting Global Information into Independent Channels for Long Time Series Forecasting",
        "rating": -1.5,
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformer has become one of the most popular architectures for multivariate time series (MTS) forecasting. Recent Transformer-based MTS models generally prefer channel-independent structures with the observation that channel independence can alleviate noise and distribution drift issues, leading to more robustness. Nevertheless, it is essential to note that channel dependency remains an inherent characteristic of MTS, carrying valuable information. Designing a model that incorporates merits of both channel-independent and channel-mixing structures is a key to further improvement of MTS forecasting, which poses a challenging conundrum. To address the problem, an injection method for global information into channel-independent Transformer, InjectTST, is proposed in this paper. Instead of designing a channel-mixing model directly, we retain the channel-independent backbone and gradually inject global information into individual channels in a selective way. A channel identifier, a global mixing module and a self-contextual attention module are devised in InjectTST. The channel identifier can help Transformer distinguish channels for better representation. The global mixing module produces cross-channel global information. Through the self-contextual attention module, the independent channels can selectively concentrate on useful global information without robustness degradation, and channel mixing is achieved implicitly. Experiments indicate that InjectTST can achieve stable improvement compared with state-of-the-art models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02820",
        "abstract url": "https://arxiv.org/abs/2403.02820",
        "title": "Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry",
        "rating": -1.5,
        "keywords": [
            [
                "biological",
                "X-ray"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In the wood industry, logs are commonly quality screened by discrete X-ray scans on a moving conveyor belt from a few source positions. Typically, two-dimensional (2D) slice-wise measurements are obtained by a sequential scanning geometry. Each 2D slice alone does not carry sufficient information for a three-dimensional tomographic reconstruction in which biological features of interest in the log are well preserved. In the present work, we propose a learned iterative reconstruction method based on the Learned Primal-Dual neural network, suited for sequential scanning geometries. Our method accumulates information between neighbouring slices, instead of only accounting for single slices during reconstruction. Our quantitative and qualitative evaluations with as few as five source positions show that our method yields reconstructions of logs that are sufficiently accurate to identify biological features like knots (branches), heartwood and sapwood.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02846",
        "abstract url": "https://arxiv.org/abs/2403.02846",
        "title": "FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive Models",
        "rating": -1.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) thrives in training a global model with numerous clients by only sharing the parameters of their local models trained with their private training datasets. Therefore, without revealing the private dataset, the clients can obtain a deep learning (DL) model with high performance. However, recent research proposed poisoning attacks that cause a catastrophic loss in the accuracy of the global model when adversaries, posed as benign clients, are present in a group of clients. Therefore, recent studies suggested byzantine-robust FL methods that allow the server to train an accurate global model even with the adversaries present in the system. However, many existing methods require the knowledge of the number of malicious clients or the auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when the private dataset was non-independently and identically distributed (non-IID). In this work, we propose FLGuard, a novel byzantine-robust FL method that detects malicious clients and discards malicious local updates by utilizing the contrastive learning technique, which showed a tremendous improvement as a self-supervised learning method. With contrastive models, we design FLGuard as an ensemble scheme to maximize the defensive capability. We evaluate FLGuard extensively under various poisoning attacks and compare the accuracy of the global model with existing byzantine-robust FL methods. FLGuard outperforms the state-of-the-art defense methods in most cases and shows drastic improvement, especially in non-IID settings. https://github.com/201younghanlee/FLGuard",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by 28th European Symposium on Research in Computer Security (ESORICS 2023)"
    },
    {
        "paper id": "2403.02901",
        "abstract url": "https://arxiv.org/abs/2403.02901",
        "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods",
        "rating": -1.5,
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02945",
        "abstract url": "https://arxiv.org/abs/2403.02945",
        "title": "Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?",
        "rating": -1.5,
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The use of unsupervised learning to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs). By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group. However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner. In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset. We extracted 16 features representing medical resource need and used consensus clustering to derive patient subgroups, replicating the previous study. We found limited similarities between our results and those of the previous study, providing evidence against the hypothesis. Our findings imply that there is significant variation between ICUs; thus, a standardised restructuring approach is unlikely to be appropriate. Instead, potential efficiency gains might be greater when the number and nature of the subunits are tailored to each ICU individually.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02950",
        "abstract url": "https://arxiv.org/abs/2403.02950",
        "title": "A general approach to enhance the survivability of backdoor attacks by decision path coupling",
        "rating": -1.5,
        "keywords": [
            [
                "unlearning"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences. One of the mainstream backdoor defenses is model reconstruction-based. Such defenses adopt model unlearning or pruning to eliminate backdoors. However, little attention has been paid to survive from such defenses. To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses. We formalize Venom as a binary-task optimization problem. The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability. To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples, which makes backdoors difficult to eliminate. Our extensive evaluation on two DNNs and three datasets has demonstrated that Venom significantly improves the survivability of eight state-of-the-art attacks against eight state-of-the-art defenses without impacting the capability of the original attacks.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02981",
        "abstract url": "https://arxiv.org/abs/2403.02981",
        "title": "Doubly Abductive Counterfactual Inference for Text-based Image Editing",
        "rating": -1.5,
        "keywords": [
            [
                "Image Editing"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We study text-based image editing (TBIE) of a single image by counterfactual inference because it is an elegant formulation to precisely address the requirement: the edited image should retain the fidelity of the original one. Through the lens of the formulation, we find that the crux of TBIE is that existing techniques hardly achieve a good trade-off between editability and fidelity, mainly due to the overfitting of the single-image fine-tuning. To this end, we propose a Doubly Abductive Counterfactual inference framework (DAC). We first parameterize an exogenous variable as a UNet LoRA, whose abduction can encode all the image details. Second, we abduct another exogenous variable parameterized by a text encoder LoRA, which recovers the lost editability caused by the overfitted first abduction. Thanks to the second abduction, which exclusively encodes the visual transition from post-edit to pre-edit, its inversion -- subtracting the LoRA -- effectively reverts pre-edit back to post-edit, thereby accomplishing the edit. Through extensive experiments, our DAC achieves a good trade-off between editability and fidelity. Thus, we can support a wide spectrum of user editing intents, including addition, removal, manipulation, replacement, style transfer, and facial change, which are extensively validated in both qualitative and quantitative evaluations. Codes are in https://github.com/xuesong39/DAC.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2403.03018",
        "abstract url": "https://arxiv.org/abs/2403.03018",
        "title": "CRISPR: Ensemble Model",
        "rating": -1.5,
        "keywords": [
            [
                "biology",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene editing technology that has revolutionized the fields of biology and medicine. However, one of the challenges of using CRISPR is predicting the on-target efficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This is because most existing methods are trained on separate datasets with different genes and cells, which limits their generalizability. In this paper, we propose a novel ensemble learning method for sgRNA design that is accurate and generalizable. Our method combines the predictions of multiple machine learning models to produce a single, more robust prediction. This approach allows us to learn from a wider range of data, which improves the generalizability of our model. We evaluated our method on a benchmark dataset of sgRNA designs and found that it outperformed existing methods in terms of both accuracy and generalizability. Our results suggest that our method can be used to design sgRNAs with high sensitivity and specificity, even for new genes or cells. This could have important implications for the clinical use of CRISPR, as it would allow researchers to design more effective and safer treatments for a variety of diseases.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03188",
        "abstract url": "https://arxiv.org/abs/2403.03188",
        "title": "Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement",
        "rating": -1.5,
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses. However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making. Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies. And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public's understanding of flood risks. To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the GPT-4 Large Language Model. This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge. The new framework utilizes GPT-4's advanced natural language understanding and function calling capabilities to provide immediate flood alerts and respond to various flood-related inquiries. Our developed prototype integrates real-time flood warnings with flood maps and social vulnerability data. It also effectively translates complex flood zone information into actionable risk management advice. To assess its performance, we evaluated the prototype using six criteria within three main categories: relevance, error resilience, and understanding of context. Our research marks a significant step towards a more accessible and user-friendly approach in flood risk management. This study highlights the potential of advanced AI tools like GPT-4 in democratizing information and enhancing public engagement in critical social and environmental issues.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "48 pages, 3 figures and an appendix with 2 supplementary tables detailing experimental results and observations. Supported by Rutgers's Research Incubator in Climate and Health, Seed Funding Initiative and Research Council Award - \"Engaged Climate Action\". Source code and data available at https://github.com/RafaelaMartelo/FloodGPT-4_Prototype"
    },
    {
        "paper id": "2403.03293",
        "abstract url": "https://arxiv.org/abs/2403.03293",
        "title": "AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis",
        "rating": -1.5,
        "keywords": [
            [
                "Cancer"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper discusses the effectiveness of leveraging Chatbot: Generative Pre-trained Transformer (ChatGPT) versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys. The study selected the \\textit{Application of Artificial Intelligence in Breast Cancer Treatment} as the research topic. Research papers related to this topic were collected from three major publication databases Google Scholar, Pubmed, and Scopus. ChatGPT models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to Breast Cancer Treatment (BCT), organization of papers according to scope, and identification of key information for survey paper writing. Evaluations performed using ground truth data annotated using subject experts reveal, that GPT-4 achieves 77.3\\% accuracy in identifying the research paper categories and 50\\% of the papers were correctly identified by GPT-4 for their scopes. Further, the results demonstrate that GPT-4 can generate reasons for its decisions with an average of 27\\% new words, and 67\\% of the reasons given by the model were completely agreeable to the subject experts.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03357",
        "abstract url": "https://arxiv.org/abs/2403.03357",
        "title": "The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa",
        "rating": -1.5,
        "keywords": [
            [
                "medical",
                "Health",
                "healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit. Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.This paper seeks to explore fairness for global health, with Africa as a case study. We conduct a scoping review to propose axes of disparities for fairness consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities. Our analysis focuses on colonialism as the attribute of interest and examines the interplay between artificial intelligence (AI), health, and colonialism. Among the pre-identified attributes, we found that colonial history, country of origin, and national income level were specific axes of disparities that participants believed would cause an AI system to be biased.However, there was also divergence of opinion between experts and general population participants. Whereas experts generally expressed a shared view about the relevance of colonial history for the development and implementation of AI technologies in Africa, the majority of the general population participants surveyed did not think there was a direct link between AI and colonialism. Based on these findings, we provide practical recommendations for developing fairness-aware ML solutions for health in Africa.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "11 pages, 4 figures. arXiv admin note: text overlap with arXiv:2304.02190"
    },
    {
        "paper id": "2403.03444",
        "abstract url": "https://arxiv.org/abs/2403.03444",
        "title": "Uncertainty quantification for deeponets with ensemble kalman inversion",
        "rating": -1.5,
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our innovative application of EKI enables us to efficiently train ensembles of DeepONets while obtaining informative uncertainty estimates for the output of interest. We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage. Furthermore, we introduce a heuristic method to estimate the artificial dynamics covariance, thereby improving our uncertainty estimates. Finally, we demonstrate the effectiveness and versatility of our proposed methodology across various benchmark problems, showcasing its potential to address the pressing challenges of uncertainty quantification in DeepONets, especially for practical applications with limited and noisy data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2403.04805",
        "abstract url": "https://arxiv.org/abs/2403.04805",
        "title": "Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge",
        "rating": -1.5,
        "keywords": [
            [
                "biology"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural structure learning is of paramount importance for scientific discovery and interpretability. Yet, contemporary pruning algorithms that focus on computational resource efficiency face algorithmic barriers to select a meaningful model that aligns with domain expertise. To mitigate this challenge, we propose DASH, which guides pruning by available domain-specific structural information. In the context of learning dynamic gene regulatory network models, we show that DASH combined with existing general knowledge on interaction partners provides data-specific insights aligned with biology. For this task, we show on synthetic data with ground truth information and two real world applications the effectiveness of DASH, which outperforms competing methods by a large margin and provides more meaningful biological insights. Our work shows that domain specific structural information bears the potential to improve model-derived scientific insights.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02688",
        "abstract url": "https://arxiv.org/abs/2403.02688",
        "title": "DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators",
        "rating": -2,
        "keywords": [
            [
                "Thermal"
            ]
        ],
        "abstract": "Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally drifting noises. The DOCTOR framework intelligently monitors the chip status using adaptive probing and performs fast in-situ training-free calibration to restore accuracy when necessary. Recognizing nonuniform spatial variation distributions across devices and tensor cores, we also propose a variation-aware architectural remapping strategy to avoid executing critical tasks on noisy devices. Extensive experiments show that our proposed framework can guarantee sustained performance under drifting variations with 34% higher accuracy and 2-3 orders-of-magnitude lower overhead compared to state-of-the-art on-chip training methods.",
        "subjects": [
            "cs.ET"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2403.02735",
        "abstract url": "https://arxiv.org/abs/2403.02735",
        "title": "Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators",
        "rating": -2,
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Monte Carlo (MC) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science. Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC simulations. This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading. Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code's capabilities to Intel GPUs. We present a comprehensive benchmarking and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids). The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code's superiority over its CPU counterpart as computational load increases.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Further improvements are needed to gain a significant value for this paper. Unfortunately, we would not be able to submit a replacement in the following months"
    },
    {
        "paper id": "2403.02786",
        "abstract url": "https://arxiv.org/abs/2403.02786",
        "title": "Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease",
        "rating": -2.0,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "health",
                "healthcare",
                "Disease",
                "clinical"
            ],
            [
                "cs.LG"
            ],
            [
                "workshop",
                "AAAI"
            ]
        ],
        "abstract": "Addressing the challenge of limited labeled data in clinical settings, particularly in the prediction of fatty liver disease, this study explores the potential of graph representation learning within a semi-supervised learning framework. Leveraging graph neural networks (GNNs), our approach constructs a subject similarity graph to identify risk patterns from health checkup data. The effectiveness of various GNN approaches in this context is demonstrated, even with minimal labeled samples. Central to our methodology is the inclusion of human-centric explanations through explainable GNNs, providing personalized feature importance scores for enhanced interpretability and clinical relevance, thereby underscoring the potential of our approach in advancing healthcare practices with a keen focus on graph representation learning and human-centric explanation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Paper accepted in Human-Centric Representation Learning workshop at AAAI 2024 (https://hcrl-workshop.github.io/2024/)"
    },
    {
        "paper id": "2403.02794",
        "abstract url": "https://arxiv.org/abs/2403.02794",
        "title": "A Distance Metric Learning Model Based On Variational Information Bottleneck",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "In recent years, personalized recommendation technology has flourished and become one of the hot research directions. The matrix factorization model and the metric learning model which proposed successively have been widely studied and applied. The latter uses the Euclidean distance instead of the dot product used by the former to measure the latent space vector. While avoiding the shortcomings of the dot product, the assumption of Euclidean distance is neglected, resulting in limited recommendation quality of the model. In order to solve this problem, this paper combines the Variationl Information Bottleneck with metric learning model for the first time, and proposes a new metric learning model VIB-DML (Variational Information Bottleneck Distance Metric Learning) for rating prediction, which limits the mutual information of the latent space feature vector to improve the robustness of the model and satisfiy the assumption of Euclidean distance by decoupling the latent space feature vector. In this paper, the experimental results are compared with the root mean square error (RMSE) on the three public datasets. The results show that the generalization ability of VIB-DML is excellent. Compared with the general metric learning model MetricF, the prediction error is reduced by 7.29%. Finally, the paper proves the strong robustness of VIBDML through experiments.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02822",
        "abstract url": "https://arxiv.org/abs/2403.02822",
        "title": "Highly Reproducible and CMOS-compatible VO2-based Oscillators for Brain-inspired Computing",
        "rating": -2,
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "With remarkable electrical and optical switching properties induced at low power and near room temperature (68C), vanadium dioxide (VO2) has sparked rising interest in unconventional computing among the phase-change materials research community. The scalability and the potential to compute beyond the von Neumann model make VO2 especially appealing for implementation in oscillating neural networks for artificial intelligence (AI) applications, to solve constraint satisfaction problems, and for pattern recognition. Its integration into large networks of oscillators on a Silicon platform still poses challenges associated with the stabilization in the correct oxidation state and the ability to fabricate a structure with predictable electrical behavior showing very low variability. In this work, the role played by the different annealing parameters applied by three methods (slow thermal annealing, flash annealing, and rapid thermal annealing), following the vanadium oxide atomic layer deposition (ALD), on the formation of VO2 grains is studied and an optimal substrate stack configuration that minimizes variability between devices is proposed. Material and electrical characterizations are performed on the different films and a step-by-step recipe to build reproducible VO2-based oscillators is presented, which is argued to be made possible thanks to the introduction of a hafnium oxide (HfO2) layer between the silicon substrate and the vanadium oxide layer. Up to seven nearly identical VO2-based devices are contacted simultaneously to create a network of oscillators, paving the way for large-scale implementation of VO2 oscillating neural networks.",
        "subjects": [
            "physics.app-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02825",
        "abstract url": "https://arxiv.org/abs/2403.02825",
        "title": "Contrastive Pre-training for Deep Session Data Understanding",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Session data has been widely used for understanding user's behavior in e-commerce. Researchers are trying to leverage session data for different tasks, such as purchase intention prediction, remaining length prediction, recommendation, etc., as it provides context clues about the user's dynamic interests. However, online shopping session data is semi-structured and complex in nature, which contains both unstructured textual data about the products, search queries, and structured user action sequences. Most existing works focus on leveraging the coarse-grained item sequences for specific tasks, while largely ignore the fine-grained information from text and user action details. In this work, we delve into deep session data understanding via scrutinizing the various clues inside the rich information in user sessions. Specifically, we propose to pre-train a general-purpose User Behavior Model (UBM) over large-scale session data with rich details, such as product title, attributes and various kinds of user actions. A two-stage pre-training scheme is introduced to encourage the model to self-learn from various augmentations with contrastive learning objectives, which spans different granularity levels of session data. Then the well-trained session understanding model can be easily fine-tuned for various downstream tasks. Extensive experiments show that UBM better captures the complex intra-item semantic relations, inter-item connections and inter-interaction dependencies, leading to large performance gains as compared to the baselines on several downstream tasks. And it also demonstrates strong robustness when data is sparse.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02831",
        "abstract url": "https://arxiv.org/abs/2403.02831",
        "title": "SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies",
        "rating": -2,
        "keywords": [
            [
                "flight"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons. The robot weighs 5.2kg and has a body size of 245mm while using space-qualifiable components. Furthermore, SpaceHopper's design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing. We control the leg motion for reorientation using Deep Reinforcement Learning policies. In a simulation of Ceres' gravity (0.029g), the robot can reliably jump to commanded positions up to 6m away. Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 degree inside a rotational gimbal and jump in a counterweight setup in Earth's gravity. Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "To be published in the 2024 IEEE International Conference on Robotics and Automation"
    },
    {
        "paper id": "2403.02845",
        "abstract url": "https://arxiv.org/abs/2403.02845",
        "title": "OORD: The Oxford Offroad Radar Dataset",
        "rating": -2,
        "keywords": [
            [
                "Radar",
                "vehicle"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "There is a growing academic interest as well as commercial exploitation of millimetre-wave scanning radar for autonomous vehicle localisation and scene understanding. Although several datasets to support this research area have been released, they are primarily focused on urban or semi-urban environments. Nevertheless, rugged offroad deployments are important application areas which also present unique challenges and opportunities for this sensor technology. Therefore, the Oxford Offroad Radar Dataset (OORD) presents data collected in the rugged Scottish highlands in extreme weather. The radar data we offer to the community are accompanied by GPS/INS reference - to further stimulate research in radar place recognition. In total we release over 90GiB of radar scans as well as GPS and IMU readings by driving a diverse set of four routes over 11 forays, totalling approximately 154km of rugged driving. This is an area increasingly explored in literature, and we therefore present and release examples of recent open-sourced radar place recognition systems and their performance on our dataset. This includes a learned neural network, the weights of which we also release. The data and tools are made freely available to the community at https://oxford-robotics-institute.github.io/oord-dataset.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02856",
        "abstract url": "https://arxiv.org/abs/2403.02856",
        "title": "Quantum Data Management: From Theory to Opportunities",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum computing has emerged as a transformative tool for future data management. Classical problems in database domains, including query optimization, data integration, and transaction management, have recently been addressed using quantum computing techniques. This tutorial aims to establish the theoretical foundation essential for enhancing methodologies and practical implementations in this line of research. Moreover, this tutorial takes a forward-looking approach by delving into recent strides in quantum internet technologies and the nonlocality theory. We aim to shed light on the uncharted territory of future data systems tailored for the quantum internet.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "Accepted at the 40th IEEE International Conference on Data Engineering (ICDE 2024)"
    },
    {
        "paper id": "2403.02871",
        "abstract url": "https://arxiv.org/abs/2403.02871",
        "title": "Quantum Mixed-State Self-Attention Network",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks. Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. Additionally, we propose an innovative quantum positional encoding scheme, implemented through fixed quantum gates within the quantum circuit, to enhance the model's accuracy. Experimental validation on various datasets demonstrates that QMSAN model outperforms existing quantum and classical models in text classification, achieving significant performance improvements. QMSAN model not only significantly reduces the number of parameters but also exceeds classical self-attention networks in performance, showcasing its strong capability in data representation and information extraction. Furthermore, our study investigates the model's robustness in different quantum noise environments, showing that QMSAN possesses commendable robustness to low noise.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02879",
        "abstract url": "https://arxiv.org/abs/2403.02879",
        "title": "Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement",
        "rating": -2,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Image Enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02894",
        "abstract url": "https://arxiv.org/abs/2403.02894",
        "title": "DIFNet: SAR RFI suppression based on domain invariant features",
        "rating": -2,
        "keywords": [
            [
                "inpainting"
            ],
            [
                "radar"
            ]
        ],
        "abstract": "Synthetic aperture radar is a high-resolution two-dimensional imaging radar, however, during the imaging process, SAR is susceptible to intentional and unintentional interference, with radio frequency interference (RFI) being the most common type, leading to a severe degradation in image quality. Although inpainting networks have achieved excellent results, their generalization is unclear, and whether they still work effectively in cross-sensor experiments needs further verification. Through time-frequency analysis of interference signals, we find that interference holds domain invariant features between different sensors. Therefore, this paper reconstructs the loss function and extracts the domain invariant features to improve the generalization. Ultimately, this paper proposes a SAR RFI suppression method based on domain invariant features, and embeds the RFI suppression into SAR imaging process. Compared to traditional notch filtering methods, the proposed approach not only removes interference but also effectively preserves strong scattering targets. Compared to PISNet, our method can extract domain invariant features and holds better generalization ability, and even in the cross-sensor experiment, our method can still achieve excellent results.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "five pages"
    },
    {
        "paper id": "2403.02942",
        "abstract url": "https://arxiv.org/abs/2403.02942",
        "title": "Tensor Decomposition-based Time Varying Channel Estimation for mmWave MIMO-OFDM Systems",
        "rating": -2,
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "In this paper, we consider the time-varying channel estimation in millimeter wave (mmWave) multiple-input multiple-output MIMO systems with hybrid beamforming architectures. Different from the existing contributions that considered single-carrier mmWave systems with high mobility, the wideband orthogonal frequency division multiplexing (OFDM) system is considered in this work. To solve the channel estimation problem under channel double selectivity, we propose a pilot transmission scheme based on 5G OFDM, and the received signals are formed as a fourth-order tensor, which fits the low-rank CANDECOMP/PARAFAC (CP) model. By further exploring the Vandermonde structure of factor matrix, a tensor-subspace decomposition based channel estimation method is proposed to solve the CP decomposition, where the uniqueness condition is analyzed. Based on the decomposed factor matrices, the channel parameters, including angles of arrival/departure, delays, channel gains and Doppler shifts are estimated, and the Cram\u00e9r-Rao bound (CRB) results are derived as performance metrics. Simulation results demonstrate the superior performance of the proposed method over other benchmarks. Furthermore, the channel estimation methods are tested based on the channel parameters generated by Wireless InSites, and simulation results show the effectiveness of the proposed method in practical scenarios.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02968",
        "abstract url": "https://arxiv.org/abs/2403.02968",
        "title": "Hamiltonian Property Testing",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\\tilde\u03a9(2^n)$ many time evolution queries and an expected total evolution time of $\\tilde\u03a9(2^n / \\varepsilon)$, and even coherent testers need $\u03a9(2^{n/2})$ many queries and $\u03a9(2^{n/2}/\\varepsilon)$ total evolution time. In contrast, when distances are measured according to the normalized Frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent Hamiltonian locality testing algorithm based on randomized measurements. In fact, our procedure can be used to simultaneously test a wide class of Hamiltonian properties beyond locality. Finally, we prove that learning a general Hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between Hamiltonian testing and learning. Our work initiates the study of property testing for quantum Hamiltonians, demonstrating that a broad class of Hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning Hamiltonian testing as an independent area of research alongside Hamiltonian learning.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "39+21 pages, 3 figures; improved upper bounds, added tolerant tester, corrected some typos"
    },
    {
        "paper id": "2403.02983",
        "abstract url": "https://arxiv.org/abs/2403.02983",
        "title": "Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks",
        "rating": -2,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attack"
            ]
        ],
        "abstract": "Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data. During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks. In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect. We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach. In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data. For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm. The datasets used in this experiment were CIC and UNSW related to computer networks. We generated adversarial samples using the two attacks mentioned above, which were applied to a small percentage of datasets. Subsequently, we trained and tested the accuracy of the model on adversarial datasets. We recorded the results for both benign and manipulated datasets and observed significant differences between the accuracy of the models on different datasets. From the experimental results, it is evident that the LF attack failed, whereas the FP attack showed effective results, which proved its significance in fooling a server. With a 1% LF attack on the CIC, the accuracy was approximately 0.0428 and the ASR was 0.9564; hence, the attack is easily detectable, while with a 1% FP attack, the accuracy and ASR were both approximately 0.9600, hence, FP attacks are difficult to detect. We repeated the experiment with different poisoning percentages.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03005",
        "abstract url": "https://arxiv.org/abs/2403.03005",
        "title": "Implicit-Explicit simulation of Mass-Spring-Charge Systems",
        "rating": -2,
        "keywords": [
            [
                "biology"
            ]
        ],
        "abstract": "Point masses connected by springs, or mass-spring systems, are widely used in computer animation to approximate the behavior of deformable objects. One of the restrictions imposed by these models is that points that are not topologically constrained (linked by a spring) are unable to interact with each other explicitly. Such interactions would introduce a new dimension for artistic control and animation within the computer graphics community. Beyond graphics, such a model could be an effective proxy to use for model-based learning of complex physical systems such as molecular biology. We propose to imbue masses in a mass-spring system with electrostatic charge leading a system with internal forces between all pairs of charged points -- regardless of whether they are linked by a spring. We provide a practical and stable algorithm to simulate charged mass-spring systems over long time horizons. We demonstrate how these systems may be controlled via parameters such as guidance electric fields or external charges, thus presenting fresh opportunities for artistic authoring. Our method is especially appropriate for computer graphics applications due to its robustness at larger simulation time steps.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03035",
        "abstract url": "https://arxiv.org/abs/2403.03035",
        "title": "Mars 2.0: A Toolchain for Modeling, Analysis, Verification and Code Generation of Cyber-Physical Systems",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "We introduce Mars 2.0 for modeling, analysis, verification and code generation of Cyber-Physical Systems. Mars 2.0 integrates Mars 1.0 with several important extensions and improvements, allowing the design of cyber-physical systems using the combination of AADL and Simulink/Stateflow, which provide a unified graphical framework for modeling the functionality, physicality and architecture of the system to be developed. For a safety-critical system, formal analysis and verification of its combined AADL and Simulink/Stateflow model can be conducted via the following steps. First, the toolchain automatically translates AADL and Simulink/Stateflow models into Hybrid CSP (HCSP), an extension of CSP for formally modeling hybrid systems. Second, the HCSP processes can be simulated using the HCSP simulator, and to complement incomplete simulation, they can be verified using the Hybrid Hoare Logic prover in Isabelle/HOL, as well as the more automated HHLPy prover. Finally, implementations in SystemC or C can be automatically generated from the verified HCSP processes. The transformation from AADL and Simulink/Stateflow to HCSP, and the one from HCSP to SystemC or C, are both guaranteed to be correct with formal proofs. This approach allows model-driven design of safety-critical cyber-physical systems based on graphical and formal models and proven-correct translation procedures. We demonstrate the use of the toolchain on several benchmarks of varying complexity, including several industrial-sized examples.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03037",
        "abstract url": "https://arxiv.org/abs/2403.03037",
        "title": "A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives",
        "rating": -2,
        "keywords": [
            [
                "robot"
            ],
            [
                "forecast"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4D benchmarks, outperforming current state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024. Project webpage at https://sapeirone.github.io/EgoPack"
    },
    {
        "paper id": "2403.03053",
        "abstract url": "https://arxiv.org/abs/2403.03053",
        "title": "Neural Codebook Design for Network Beam Management",
        "rating": -2,
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Obtaining accurate and timely channel state information (CSI) is a fundamental challenge for large antenna systems. Mobile systems like 5G use a beam management framework that joins the initial access, beamforming, CSI acquisition, and data transmission. The design of codebooks for these stages, however, is challenging due to their interrelationships, varying array sizes, and site-specific channel and user distributions. Furthermore, beam management is often focused on single-sector operations while ignoring the overarching network- and system-level optimization. In this paper, we proposed an end-to-end learned codebook design algorithm, network beamspace learning (NBL), that captures and optimizes codebooks to mitigate interference while maximizing the achievable performance with extremely large hybrid arrays. The proposed algorithm requires limited shared information yet designs codebooks that outperform traditional codebooks by over 10dB in beam alignment and achieve more than 25% improvements in network spectral efficiency.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "To be submitted to IEEE Transactions on Wireless Communications"
    },
    {
        "paper id": "2403.03067",
        "abstract url": "https://arxiv.org/abs/2403.03067",
        "title": "Enumeration for MSO-Queries on Compressed Trees",
        "rating": -2,
        "keywords": [
            [
                "grammar"
            ]
        ],
        "abstract": "We present a linear preprocessing and output-linear delay enumeration algorithm for MSO-queries over trees that are compressed in the well-established grammar-based framework. Time bounds are measured with respect to the size of the compressed representation of the tree. Our result extends previous work on the enumeration of MSO-queries over uncompressed trees and on the enumeration of document spanners over compressed text documents.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03077",
        "abstract url": "https://arxiv.org/abs/2403.03077",
        "title": "MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "diagnosis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03089",
        "abstract url": "https://arxiv.org/abs/2403.03089",
        "title": "VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism",
        "rating": -2,
        "keywords": [
            [
                "cancer",
                "clinical"
            ]
        ],
        "abstract": "The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in revolutionizing the field through its advanced predictive capabilities, thereby contributing to the optimization of cancer treatment strategies.",
        "subjects": [
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03100",
        "abstract url": "https://arxiv.org/abs/2403.03100",
        "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
        "rating": -2,
        "keywords": [
            [
                "Diffusion",
                "Synthesis"
            ],
            [
                "neural codec"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Achieving human-level quality and naturalness on multi-speaker datasets (e.g., LibriSpeech) in a zero-shot way"
    },
    {
        "paper id": "2403.03105",
        "abstract url": "https://arxiv.org/abs/2403.03105",
        "title": "Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand",
        "rating": -2,
        "keywords": [
            [
                "Biomechanical"
            ]
        ],
        "abstract": "Current studies on human locomotion focus mainly on solid ground walking conditions. In this paper, we present a biomechanic comparison of human walking locomotion on solid ground and sand. A novel dataset containing 3-dimensional motion and biomechanical data from 20 able-bodied adults for locomotion on solid ground and sand is collected. We present the data collection methods and report the sensor data along with the kinematic and kinetic profiles of joint biomechanics. A comprehensive analysis of human gait and joint stiffness profiles is presented. The kinematic and kinetic analysis reveals that human walking locomotion on sand shows different ground reaction forces and joint torque profiles, compared with those patterns from walking on solid ground. These gait differences reflect that humans adopt motion control strategies for yielding terrain conditions such as sand. The dataset also provides a source of locomotion data for researchers to study human activity recognition and assistive devices for walking on different terrains.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "20 pages, 10 figures, submitted to Journal of Biomechanics"
    },
    {
        "paper id": "2403.03143",
        "abstract url": "https://arxiv.org/abs/2403.03143",
        "title": "Using Smartphones to Study Vaccination Decisions in the Wild",
        "rating": -2,
        "keywords": [
            [
                "health",
                "disease",
                "clinical"
            ]
        ],
        "abstract": "One of the most important tools available to limit the spread and impact of infectious diseases is vaccination. It is therefore important to understand what factors determine people's vaccination decisions. To this end, previous behavioural research made use of, (i) controlled but often abstract or hypothetical studies (e.g., vignettes) or, (ii) realistic but typically less flexible studies that make it difficult to understand individual decision processes (e.g., clinical trials). Combining the best of these approaches, we propose integrating real-world Bluetooth contacts via smartphones in several rounds of a game scenario, as a novel methodology to study vaccination decisions and disease spread. In our 12-week proof-of-concept study conducted with $N$ = 494 students, we found that participants strongly responded to some of the information provided to them during or after each decision round, particularly those related to their individual health outcomes. In contrast, information related to others' decisions and outcomes (e.g., the number of vaccinated or infected individuals) appeared to be less important. We discuss the potential of this novel method and point to fruitful areas for future research.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03149",
        "abstract url": "https://arxiv.org/abs/2403.03149",
        "title": "Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks",
        "rating": -2,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attacks"
            ]
        ],
        "abstract": "Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack. In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark datasets and perform a comparison with ten baseline methods. The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks. Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "To appear in The Web Conference 2024 (WWW '24)"
    },
    {
        "paper id": "2403.03154",
        "abstract url": "https://arxiv.org/abs/2403.03154",
        "title": "Quantum Many-Body Physics Calculations with Large Language Models",
        "rating": -2,
        "keywords": [
            [
                "Quantum",
                "Physics"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning. We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases. Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps. Overall, the requisite skill for doing these calculations is at the graduate level in quantum condensed matter theory. We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases. The strong performance is the first step for developing algorithms that automatically explore theoretical hypotheses at an unprecedented scale.",
        "subjects": [
            "physics.comp-ph"
        ],
        "comment": "9 pages, 4 figures. Supplemental material in the source file"
    },
    {
        "paper id": "2403.03196",
        "abstract url": "https://arxiv.org/abs/2403.03196",
        "title": "SmartSantander: IoT Experimentation over a Smart City Testbed",
        "rating": -2,
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "This paper describes the deployment and experimentation architecture of the Internet of Things experimentation facility being deployed at Santander city. The facility is implemented within the SmartSantander project, one of the projects of the Future Internet Research and Experimentation initiative of the European Commission and represents a unique in the world city-scale experimental research facility. Additionally, this facility supports typical applications and services of a smart city. Tangible results are expected to influence the definition and specification of Future Internet architecture design from viewpoints of Internet of Things and Internet of Services. The facility comprises a large number of Internet of Things devices deployed in several urban scenarios which will be federated into a single testbed. In this paper the deployment being carried out at the main location, namely Santander city, is described. Besides presenting the current deployment, in this article the main insights in terms of the architectural design of a large-scale IoT testbed are presented as well. Furthermore, solutions adopted for implementation of the different components addressing the required testbed functionalities are also sketched out. The IoT experimentation facility described in this paper is conceived to provide a suitable platform for large scale experimentation and evaluation of IoT concepts under real-life conditions.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "This work is published in Elsevier Computer Networks. This work has been funded by research project SmartSantander, under FP7-ICT-2009-5 of the 7th Framework Programme of the European Community"
    },
    {
        "paper id": "2403.03215",
        "abstract url": "https://arxiv.org/abs/2403.03215",
        "title": "A Safety-Critical Framework for UGVs in Complex Environments: A Data-Driven Discrepancy-Aware Approach",
        "rating": -2,
        "keywords": [
            [
                "trajectory",
                "LiDAR",
                "vehicle"
            ],
            [
                "navigation"
            ]
        ],
        "abstract": "This work presents a novel data-driven multi-layered planning and control framework for the safe navigation of a class of unmanned ground vehicles (UGVs) in the presence of unknown stationary obstacles and additive modeling uncertainties. The foundation of this framework is a novel robust model predictive planner, designed to generate optimal collision-free trajectories given an occupancy grid map, and a paired ancillary controller, augmented to provide robustness against model uncertainties extracted from learning data. To tackle modeling discrepancies, we identify both matched (input discrepancies) and unmatched model residuals between the true and the nominal reduced-order models using closed-loop tracking errors as training data. Utilizing conformal prediction, we extract probabilistic upper bounds for the unknown model residuals, which serve to construct a robustifying ancillary controller. Further, we also determine maximum tracking discrepancies, also known as the robust control invariance tube, under the augmented policy, formulating them as collision buffers. Employing a LiDAR-based occupancy map to characterize the environment, we construct a discrepancy-aware cost map that incorporates these collision buffers. This map is then integrated into a sampling-based model predictive path planner that generates optimal and safe trajectories that can be robustly tracked by the augmented ancillary controller in the presence of model mismatches. The effectiveness of the framework is experimentally validated for autonomous high-speed trajectory tracking in a cluttered environment with four different vehicle-terrain configurations. We also showcase the framework's versatility by reformulating it as a driver-assist program, providing collision avoidance corrections based on user joystick commands.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03217",
        "abstract url": "https://arxiv.org/abs/2403.03217",
        "title": "Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "medical",
                "MRI",
                "CT",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D patient body modeling is critical to the success of automated patient positioning for smart medical scanning and operating rooms. Existing CNN-based end-to-end patient modeling solutions typically require a) customized network designs demanding large amount of relevant training data, covering extensive realistic clinical scenarios (e.g., patient covered by sheets), which leads to suboptimal generalizability in practical deployment, b) expensive 3D human model annotations, i.e., requiring huge amount of manual effort, resulting in systems that scale poorly. To address these issues, we propose a generic modularized 3D patient modeling method consists of (a) a multi-modal keypoint detection module with attentive fusion for 2D patient joint localization, to learn complementary cross-modality patient body information, leading to improved keypoint localization robustness and generalizability in a wide variety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy occlusions); and (b) a self-supervised 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment. We demonstrate the efficacy of the proposed method by extensive patient positioning experiments on both public and clinical data. Our evaluation results achieve superior patient positioning performance across various imaging modalities in real clinical scenarios.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "MICCAI 2022"
    },
    {
        "paper id": "2403.03237",
        "abstract url": "https://arxiv.org/abs/2403.03237",
        "title": "Efficiency of k-Local Quantum Search and its Adiabatic Variant on Random k-SAT",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "The computational complexity of random $k$-SAT problem is contingent on the clause number $m$. In classical computing, a satisfiability threshold is identified at $m=r_k n$, marking the transition of random $k$-SAT from solubility to insolubility. However, beyond this established threshold, comprehending the complexity remains challenging. On quantum computers, direct application of Grover's unstructured quantum search still yields exponential time requirements due to oversight of structural information. This paper introduces a family of structured quantum search algorithms, termed $k$-local quantum search, designed to address the $k$-SAT problem. Because search algorithm necessitates the presence of a target, our focus is specifically on the satisfiable side of $k$-SAT, i.e., max-$k$-SAT on satisfiable instances, denoted as max-$k$-SSAT, with a small $k \\ge 3$. For random instances with $m=\u03a9(n^{2+\u03b5})$, general exponential acceleration is proven for any small $\u03b5>0$ and sufficiently large $n$. Furthermore, adiabatic $k$-local quantum search improves the bound of general efficiency to $m=\u03a9(n^{1+\u03b5})$, within an evolution time of $\\mathcal{O}(n^2)$. Specifically, for $m=\u0398(n^{1+\u03b4+\u03b5})$, the efficiency is guaranteed in a probability of $1-\\mathcal{O}(\\mathrm{erfc}(n^{\u03b4/2}))$. By modifying this algorithm capable of solving all instances, we prove that the max-$k$-SSAT is polynomial on average if $m=\u03a9(n^{2+\u03b5})$ based on the average-case complexity theory.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "28 pages, 6 figures"
    },
    {
        "paper id": "2403.03239",
        "abstract url": "https://arxiv.org/abs/2403.03239",
        "title": "Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster's Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms. By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time. Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing fake news, thereby influencing user interactions and engagement levels. Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments. Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effectiveness of digital health platforms. This research ventures into an uncharted territory where optical phenomena such as Brewster's Angle and Snell's Law, along with the concept of spin solitons, are metaphorically applied to address the challenge of fake news dissemination. By exploring the analogy between light refraction, reflection, and the propagation of information in digital platforms, we unveil a novel perspective on how the 'angle' at which information is presented can significantly affect its acceptance and spread. Additionally, we propose the use of tellurium nanoparticles to manage 'information waves' through mechanisms akin to plasmonic resonance and soliton dynamics. This theoretical exploration aims to bridge the gap between physical sciences and digital communication, offering insights into the development of strategies for mitigating misinformation.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": "Tellurium Nanoparticles, Snell's Law, Soliton Solution, Anamorphic Surfaces, Nonlinear Dynamics, Fake News Adsorption, User Behavior Modeling, Health Improvement Strategies, Plasmonic Sensors This paper is partially an attempt to utilize \"Generative AI\" and was written with educational intent. There are currently no plans for it to become a peer-reviewed paper"
    },
    {
        "paper id": "2403.03245",
        "abstract url": "https://arxiv.org/abs/2403.03245",
        "title": "Neural Network Learning and Quantum Gravity",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "The landscape of low-energy effective field theories stemming from string theory is too vast for a systematic exploration. However, the meadows of the string landscape may be fertile ground for the application of machine learning techniques. Employing neural network learning may allow for inferring novel, undiscovered properties that consistent theories in the landscape should possess, or checking conjectural statements about alleged characteristics thereof. The aim of this work is to describe to what extent the string landscape can be explored with neural network-based learning. Our analysis is motivated by recent studies that show that the string landscape is characterized by finiteness properties, emerging from its underlying tame, o-minimal structures. Indeed, employing these results, we illustrate that any low-energy effective theory of string theory is endowed with certain statistical learnability properties. Consequently, several learning problems therein formulated, including interpolations and multi-class classification problems, can be concretely addressed with machine learning, delivering results with sufficiently high accuracy.",
        "subjects": [
            "hep-th"
        ],
        "comment": "32 pages, 8 figures"
    },
    {
        "paper id": "2403.03269",
        "abstract url": "https://arxiv.org/abs/2403.03269",
        "title": "Active Information Gathering for Long-Horizon Navigation Under Uncertainty by Learning the Value of Information",
        "rating": -2,
        "keywords": [
            [
                "Navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "We address the task of long-horizon navigation in partially mapped environments for which active gathering of information about faraway unseen space is essential for good behavior. We present a novel planning strategy that, at training time, affords tractable computation of the value of information associated with revealing potentially informative regions of unseen space, data used to train a graph neural network to predict the goodness of temporally-extended exploratory actions. Our learning-augmented model-based planning approach predicts the expected value of information of revealing unseen space and is capable of using these predictions to actively seek information and so improve long-horizon navigation. Across two simulated office-like environments, our planner outperforms competitive learned and non-learned baseline navigation strategies, achieving improvements of up to 63.76% and 36.68%, demonstrating its capacity to actively seek performance-critical information.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted at IROS'24. arXiv admin note: text overlap with arXiv:2307.14501"
    },
    {
        "paper id": "2403.03274",
        "abstract url": "https://arxiv.org/abs/2403.03274",
        "title": "From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE",
        "rating": -2,
        "keywords": [
            [
                "Health",
                "disease"
            ]
        ],
        "abstract": "Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfactual simulation.",
        "subjects": [
            "q-bio.QM"
        ],
        "comment": "6 figures"
    },
    {
        "paper id": "2403.03276",
        "abstract url": "https://arxiv.org/abs/2403.03276",
        "title": "ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures",
        "rating": -2,
        "keywords": [
            [
                "EEG"
            ]
        ],
        "abstract": "We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneous datasets, including the CHB-MIT and UPenn and Mayos Clinic, CHB-MIT datasets. The empirical findings suggest that the ARNN model outperforms baseline methods such as LSTM, Vision Transformer (ViT), Compact Convolution Transformer (CCT), and R-Transformer (RT), showcasing superior performance and faster processing capabilities across a wide range of tasks. The code has been made publicly accessible at \\url{https://github.com/Salim-Lysiun/ARNN}.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "9 pages, 7 figures, Journal Paper"
    },
    {
        "paper id": "2403.03295",
        "abstract url": "https://arxiv.org/abs/2403.03295",
        "title": "Proper vs Improper Quantum PAC learning",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "A basic question in the PAC model of learning is whether proper learning is harder than improper learning. In the classical case, there are examples of concept classes with VC dimension $d$ that have sample complexity $\u03a9\\left(\\frac d\u03b5\\log\\frac1\u03b5\\right)$ for proper learning with error $\u03b5$, while the complexity for improper learning is O$\\!\\left(\\frac d\u03b5\\right)$. One such example arises from the Coupon Collector problem. Motivated by the efficiency of proper versus improper learning with quantum samples, Arunachalam, Belovs, Childs, Kothari, Rosmanis, and de Wolf (TQC 2020) studied an analogue, the Quantum Coupon Collector problem. Curiously, they discovered that for learning size $k$ subsets of $[n]$ the problem has sample complexity $\u0398(k\\log\\min\\{k,n-k+1\\})$, in contrast with the complexity of $\u0398(k\\log k)$ for Coupon Collector. This effectively negates the possibility of a separation between the two modes of learning via the quantum problem, and Arunachalam et al.\\ posed the possibility of such a separation as an open question. In this work, we first present an algorithm for the Quantum Coupon Collector problem with sample complexity that matches the sharper lower bound of $(1-o_k(1))k\\ln\\min\\{k,n-k+1\\}$ shown recently by Bab Hadiashar, Nayak, and Sinha (IEEE TIT 2024), for the entire range of the parameter $k$. Next, we devise a variant of the problem, the Quantum Padded Coupon Collector. We prove that its sample complexity matches that of the classical Coupon Collector problem for both modes of learning, thereby exhibiting the same asymptotic separation between proper and improper quantum learning as mentioned above. The techniques we develop in the process can be directly applied to any form of padded quantum data. We hope that padding can more generally lift other forms of classical learning behaviour to the quantum setting.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "23 Pages"
    },
    {
        "paper id": "2403.03349",
        "abstract url": "https://arxiv.org/abs/2403.03349",
        "title": "A consensus-constrained parsimonious Gaussian mixture model for clustering hyperspectral images",
        "rating": -2,
        "keywords": [
            [
                "hyperspectral images"
            ]
        ],
        "abstract": "The use of hyperspectral imaging to investigate food samples has grown due to the improved performance and lower cost of spectroscopy instrumentation. Food engineers use hyperspectral images to classify the type and quality of a food sample, typically using classification methods. In order to train these methods, every pixel in each training image needs to be labelled. Typically, computationally cheap threshold-based approaches are used to label the pixels, and classification methods are trained based on those labels. However, threshold-based approaches are subjective and cannot be generalized across hyperspectral images taken in different conditions and of different foods. Here a consensus-constrained parsimonious Gaussian mixture model (ccPGMM) is proposed to label pixels in hyperspectral images using a model-based clustering approach. The ccPGMM utilizes available information on the labels of a small number of pixels and the relationship between those pixels and neighbouring pixels as constraints when clustering the rest of the pixels in the image. A latent variable model is used to represent the high-dimensional data in terms of a small number of underlying latent factors. To ensure computational feasibility, a consensus clustering approach is employed, where the data are divided into multiple randomly selected subsets of variables and constrained clustering is applied to each data subset; the clustering results are then consolidated across all data subsets to provide a consensus clustering solution. The ccPGMM approach is applied to simulated datasets and real hyperspectral images of three types of puffed cereal, corn, rice, and wheat. Improved clustering performance and computational efficiency are demonstrated when compared to other current state-of-the-art approaches.",
        "subjects": [
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03376",
        "abstract url": "https://arxiv.org/abs/2403.03376",
        "title": "Scalable Network Tomography for Dynamic Spectrum Access",
        "rating": -2,
        "keywords": [
            [
                "5G",
                "6G"
            ]
        ],
        "abstract": "Mobile networks have increased spectral efficiency through advanced multiplexing strategies that are coordinated by base stations (BS) in licensed spectrum. However, external interference on clients leads to significant performance degradation during dynamic (unlicensed) spectrum access (DSA). We introduce the notion of network tomography for DSA, whereby clients are transformed into spectrum sensors, whose joint access statistics are measured and used to account for interfering sources. Albeit promising, performing such tomography naively incurs an impractical overhead that scales exponentially with the multiplexing order of the strategies deployed -- which will only continue to grow with 5G/6G technologies. To this end, we propose a novel, scalable network tomography framework called NeTo-X that estimates joint client access statistics with just linear overhead, and forms a blue-print of the interference, thus enabling efficient DSA for future networks. NeTo-X's design incorporates intelligent algorithms that leverage multi-channel diversity and the spatial locality of interference impact on clients to accurately estimate the desired interference statistics from just pair-wise measurements of its clients. The merits of its framework are showcased in the context of resource management and jammer localization applications, where its performance significantly outperforms baseline approaches and closely approximates optimal performance at a scalable overhead.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Accepted to IEEE INFOCOM 2024. 11 pages, 11 figures"
    },
    {
        "paper id": "2403.03424",
        "abstract url": "https://arxiv.org/abs/2403.03424",
        "title": "Generative News Recommendation",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "Most existing news recommendation methods tackle this task by conducting semantic matching between candidate news and user representation produced by historical clicked news. However, they overlook the high-level connections among different news articles and also ignore the profound relationship between these news articles and users. And the definition of these methods dictates that they can only deliver news articles as-is. On the contrary, integrating several relevant news articles into a coherent narrative would assist users in gaining a quicker and more comprehensive understanding of events. In this paper, we propose a novel generative news recommendation paradigm that includes two steps: (1) Leveraging the internal knowledge and reasoning capabilities of the Large Language Model (LLM) to perform high-level matching between candidate news and user representation; (2) Generating a coherent and logically structured narrative based on the associations between related news and user interests, thus engaging users in further reading of the news. Specifically, we propose GNR to implement the generative news recommendation paradigm. First, we compose the dual-level representation of news and users by leveraging LLM to generate theme-level representations and combine them with semantic-level representations. Next, in order to generate a coherent narrative, we explore the news relation and filter the related news according to the user preference. Finally, we propose a novel training method named UIFT to train the LLM to fuse multiple news articles in a coherent narrative. Extensive experiments show that GNR can improve recommendation accuracy and eventually generate more personalized and factually consistent narratives.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by WWW 2024"
    },
    {
        "paper id": "2403.03459",
        "abstract url": "https://arxiv.org/abs/2403.03459",
        "title": "TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs",
        "rating": -2,
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework. Building on the recent development of the GPT-PINN that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities. Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime. We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations.",
        "subjects": [
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03460",
        "abstract url": "https://arxiv.org/abs/2403.03460",
        "title": "Foot Shape-Dependent Resistive Force Model for Bipedal Walkers on Granular Terrains",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments. However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains. We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction. The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense. The model is validated by extensive foot intrusion experiments with a bipedal robot. The results confirm the model accuracy on the given type of granular terrains. The model can be further integrated with the motion control of bipedal robotic walkers.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "ICRA 2024"
    },
    {
        "paper id": "2403.05591",
        "abstract url": "https://arxiv.org/abs/2403.05591",
        "title": "Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes",
        "rating": -2,
        "keywords": [
            [
                "Biometric"
            ]
        ],
        "abstract": "Hand-intensive manufacturing processes, such as composite layup and textile draping, require significant human dexterity to accommodate task complexity. These strenuous hand motions often lead to musculoskeletal disorders and rehabilitation surgeries. We develop a data-driven ergonomic risk assessment system with a special focus on hand and finger activity to better identify and address ergonomic issues related to hand-intensive manufacturing processes. The system comprises a multi-modal sensor testbed to collect and synchronize operator upper body pose, hand pose and applied forces; a Biometric Assessment of Complete Hand (BACH) formulation to measure high-fidelity hand and finger risks; and industry-standard risk scores associated with upper body posture, RULA, and hand activity, HAL. Our findings demonstrate that BACH captures injurious activity with a higher granularity in comparison to the existing metrics. Machine learning models are also used to automate RULA and HAL scoring, and generalize well to unseen participants. Our assessment system, therefore, provides ergonomic interpretability of the manufacturing processes studied, and could be used to mitigate risks through minor workplace optimization and posture corrections.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "26 pages, 7 figures"
    },
    {
        "paper id": "2403.02777",
        "abstract url": "https://arxiv.org/abs/2403.02777",
        "title": "A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire Navigation",
        "rating": -2.5,
        "keywords": [
            [
                "Navigation"
            ],
            [
                "X-ray"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Purpose: The treatment of cardiovascular diseases requires complex and challenging navigation of a guidewire and catheter. This often leads to lengthy interventions during which the patient and clinician are exposed to X-ray radiation. Deep Reinforcement Learning approaches have shown promise in learning this task and may be the key to automating catheter navigation during robotized interventions. Yet, existing training methods show limited capabilities at generalizing to unseen vascular anatomies, requiring to be retrained each time the geometry changes. Methods: In this paper, we propose a zero-shot learning strategy for three-dimensional autonomous endovascular navigation. Using a very small training set of branching patterns, our reinforcement learning algorithm is able to learn a control that can then be applied to unseen vascular anatomies without retraining. Results: We demonstrate our method on 4 different vascular systems, with an average success rate of 95% at reaching random targets on these anatomies. Our strategy is also computationally efficient, allowing the training of our controller to be performed in only 2 hours. Conclusion: Our training method proved its ability to navigate unseen geometries with different characteristics, thanks to a nearly shape-invariant observation space.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "International Journal of Computer Assisted Radiology and Surgery, In press"
    },
    {
        "paper id": "2403.02810",
        "abstract url": "https://arxiv.org/abs/2403.02810",
        "title": "Dynamic Gaussian Graph Operator: Learning parametric partial differential equations in arbitrary discrete mechanics problems",
        "rating": -2.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "forecast"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning methods have access to be employed for solving physical systems governed by parametric partial differential equations (PDEs) due to massive scientific data. It has been refined to operator learning that focuses on learning non-linear mapping between infinite-dimensional function spaces, offering interface from observations to solutions. However, state-of-the-art neural operators are limited to constant and uniform discretization, thereby leading to deficiency in generalization on arbitrary discretization schemes for computational domain. In this work, we propose a novel operator learning algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands neural operators to learning parametric PDEs in arbitrary discrete mechanics problems. The Dynamic Gaussian Graph (DGG) kernel learns to map the observation vectors defined in general Euclidean space to metric vectors defined in high-dimensional uniform metric space. The DGG integral kernel is parameterized by Gaussian kernel weighted Riemann sum approximating and using dynamic message passing graph to depict the interrelation within the integral term. Fourier Neural Operator is selected to localize the metric vectors on spatial and frequency domains. Metric vectors are regarded as located on latent uniform domain, wherein spatial and spectral transformation offer highly regular constraints on solution space. The efficiency and robustness of DGGO are validated by applying it to solve numerical arbitrary discrete mechanics problems in comparison with mainstream neural operators. Ablation experiments are implemented to demonstrate the effectiveness of spatial transformation in the DGG kernel. The proposed method is utilized to forecast stress field of hyper-elastic material with geometrically variable void as engineering application.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "The number of figures is 13. The number of tables is 7. The number of words is 9854"
    },
    {
        "paper id": "2403.02985",
        "abstract url": "https://arxiv.org/abs/2403.02985",
        "title": "Evolution Transformer: In-Context Evolutionary Optimization",
        "rating": -2.5,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "biological"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Evolutionary optimization algorithms are often derived from loose biological analogies and struggle to leverage information obtained during the sequential course of optimization. An alternative promising approach is to leverage data and directly discover powerful optimization principles via meta-optimization. In this work, we follow such a paradigm and introduce Evolution Transformer, a causal Transformer architecture, which can flexibly characterize a family of Evolution Strategies. Given a trajectory of evaluations and search distribution statistics, Evolution Transformer outputs a performance-improving update to the search distribution. The architecture imposes a set of suitable inductive biases, i.e. the invariance of the distribution update to the order of population members within a generation and equivariance to the order of the search dimensions. We train the model weights using Evolutionary Algorithm Distillation, a technique for supervised optimization of sequence models using teacher algorithm trajectories. The resulting model exhibits strong in-context optimization performance and shows strong generalization capabilities to otherwise challenging neuroevolution tasks. We analyze the resulting properties of the Evolution Transformer and propose a technique to fully self-referentially train the Evolution Transformer, starting from a random initialization and bootstrapping its own learning progress. We provide an open source implementation under https://github.com/RobertTLange/evosax.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03107",
        "abstract url": "https://arxiv.org/abs/2403.03107",
        "title": "On-demand Mobility Services for Urban Resilience: A Review Towards Human-Machine Collaborative Future",
        "rating": -2.5,
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "medical"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Mobility-on-demand (MOD) services have the potential to significantly improve the adaptiveness and recovery of urban logistics and transportation infrastructure, in the wake of disruptive events. This paper presents a survey on the usage of MOD services for resilience improvement (MOD-R) and finds a noticeable increase within recent years on this topic across four main areas: resilient MOD services, novel usage of MOD-R services for improving supply chain resilience, empirical impact evaluation, and supporting technologies. MOD-R services have been utilized for anomaly detection, essential supply delivery, evacuation and rescue, on-site medical care, power grid stabilization, transit service substitution during downtime, and infrastructure and equipment repair. The review reveals integrating electrification, automation, and advanced communication technologies offers significant synergistic benefits. The review also suggests the importance of harnessing the collective capabilities of humans and intelligent machines to effectively implement versatile, multi-functional MOD-R services during crises.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2403.03165",
        "abstract url": "https://arxiv.org/abs/2403.03165",
        "title": "Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks",
        "rating": -2.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated. A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as Federated Learning (FL), which enables data owners to train models without having to transfer raw data to third-party servers. However, FL networks are expected to involve thousands of heterogeneous distributed devices. As a result, communication efficiency remains a key bottleneck. To reduce node failures and device exits, a Hierarchical Federated Learning (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation. Therefore, based on the improvement of edge server resource utilization, this paper can effectively make up for the limitation of cache capacity. In order to mitigate the impact of soft clicks on the quality of user experience (QoE), the authors model the user QoE as a comprehensive system cost. To solve the formulaic problem, the authors propose a decentralized caching algorithm with federated deep reinforcement learning (DRL) and federated learning (FL), where multiple agents learn and make decisions independently",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03181",
        "abstract url": "https://arxiv.org/abs/2403.03181",
        "title": "Behavior Generation with Latent Actions",
        "rating": -2.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies. Videos and code can be found https://sjlee.cc/vq-bet",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Github repo: https://github.com/jayLEE0301/vq_bet_official"
    },
    {
        "paper id": "2403.03368",
        "abstract url": "https://arxiv.org/abs/2403.03368",
        "title": "Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures",
        "rating": -2.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Biobank",
                "medical",
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The effectiveness of clopidogrel, a widely used antiplatelet medication, varies significantly among individuals, necessitating the development of precise predictive models to optimize patient care. In this study, we leverage federated learning strategies to address clopidogrel treatment failure detection. Our research harnesses the collaborative power of multiple healthcare institutions, allowing them to jointly train machine learning models while safeguarding sensitive patient data. Utilizing the UK Biobank dataset, which encompasses a vast and diverse population, we partitioned the data based on geographic centers and evaluated the performance of federated learning. Our results show that while centralized training achieves higher Area Under the Curve (AUC) values and faster convergence, federated learning approaches can substantially narrow this performance gap. Our findings underscore the potential of federated learning in addressing clopidogrel treatment failure detection, offering a promising avenue for enhancing patient care through personalized treatment strategies while respecting data privacy. This study contributes to the growing body of research on federated learning in healthcare and lays the groundwork for secure and privacy-preserving predictive models for various medical conditions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03410",
        "abstract url": "https://arxiv.org/abs/2403.03410",
        "title": "Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression",
        "rating": -2.5,
        "keywords": [
            [
                "SVM",
                "Support Vector Machine"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapid development of information technology, especially the Internet, has facilitated users with a quick and easy way to seek information. With these convenience offered by internet services, many individuals who initially invested in gold and precious metals are now shifting into digital investments in form of cryptocurrencies. However, investments in crypto coins are filled with uncertainties and fluctuation in daily basis. This risk posed as significant challenges for coin investors that could result in substantial investment losses. The uncertainty of the value of these crypto coins is a critical issue in the field of coin investment. Forecasting, is one of the methods used to predict the future value of these crypto coins. By utilizing the models of Long Short Term Memory, Support Vector Machine, and Polynomial Regression algorithm for forecasting, a performance comparison is conducted to determine which algorithm model is most suitable for predicting crypto currency prices. The mean square error is employed as a benchmark for the comparison. By applying those three constructed algorithm models, the Support Vector Machine uses a linear kernel to produce the smallest mean square error compared to the Long Short Term Memory and Polynomial Regression algorithm models, with a mean square error value of 0.02. Keywords: Cryptocurrency, Forecasting, Long Short Term Memory, Mean Square Error, Polynomial Regression, Support Vector Machine",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Asian Journal of Engineering, Social and Health Volume 3, No. 2 February 2024 (308-319)"
    },
    {
        "paper id": "2403.02710",
        "abstract url": "https://arxiv.org/abs/2403.02710",
        "title": "FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird's-Eye View and Perspective View",
        "rating": -3,
        "keywords": [
            [
                "3D",
                "voxel"
            ],
            [
                "autonomous driving",
                "vehicle"
            ],
            [
                "Bird's-Eye View",
                "BEV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In autonomous driving, 3D occupancy prediction outputs voxel-wise status and semantic labels for more comprehensive understandings of 3D scenes compared with traditional perception tasks, such as 3D object detection and bird's-eye view (BEV) semantic segmentation. Recent researchers have extensively explored various aspects of this task, including view transformation techniques, ground-truth label generation, and elaborate network design, aiming to achieve superior performance. However, the inference speed, crucial for running on an autonomous vehicle, is neglected. To this end, a new method, dubbed FastOcc, is proposed. By carefully analyzing the network effect and latency from four parts, including the input image resolution, image backbone, view transformation, and occupancy prediction head, it is found that the occupancy prediction head holds considerable potential for accelerating the model while keeping its accuracy. Targeted at improving this component, the time-consuming 3D convolution network is replaced with a novel residual-like architecture, where features are mainly digested by a lightweight 2D BEV convolution network and compensated by integrating the 3D voxel features interpolated from the original image features. Experiments on the Occ3D-nuScenes benchmark demonstrate that our FastOcc achieves state-of-the-art results with a fast inference speed.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ICRA 2024"
    },
    {
        "paper id": "2403.02726",
        "abstract url": "https://arxiv.org/abs/2403.02726",
        "title": "Bias in Generative AI",
        "rating": -3,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "facial"
            ]
        ],
        "abstract": "This study analyzed images generated by three popular generative artificial intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 - representing various occupations to investigate potential bias in AI generators. Our analysis revealed two overarching areas of concern in these AI generators, including (1) systematic gender and racial biases, and (2) subtle biases in facial expressions and appearances. Firstly, we found that all three AI generators exhibited bias against women and African Americans. Moreover, we found that the evident gender and racial biases uncovered in our analysis were even more pronounced than the status quo when compared to labor force statistics or Google images, intensifying the harmful biases we are actively striving to rectify in our society. Secondly, our study uncovered more nuanced prejudices in the portrayal of emotions and appearances. For example, women were depicted as younger with more smiles and happiness, while men were depicted as older with more neutral expressions and anger, posing a risk that generative AI models may unintentionally depict women as more submissive and less competent than men. Such nuanced biases, by their less overt nature, might be more problematic as they can permeate perceptions unconsciously and may be more difficult to rectify. Although the extent of bias varied depending on the model, the direction of bias remained consistent in both commercial and open-source AI generators. As these tools become commonplace, our study highlights the urgency to identify and mitigate various biases in generative AI, reinforcing the commitment to ensuring that AI technologies benefit all of humanity for a more inclusive future.",
        "subjects": [
            "econ.GN"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02750",
        "abstract url": "https://arxiv.org/abs/2403.02750",
        "title": "Speckle Noise Reduction in Ultrasound Images using Denoising Auto-encoder with Skip Connection",
        "rating": -3,
        "keywords": [
            [
                "medical",
                "diagnosis",
                "cancer"
            ],
            [
                "image enhancement"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Ultrasound is a widely used medical tool for non-invasive diagnosis, but its images often contain speckle noise which can lower their resolution and contrast-to-noise ratio. This can make it more difficult to extract, recognize, and analyze features in the images, as well as impair the accuracy of computer-assisted diagnostic techniques and the ability of doctors to interpret the images. Reducing speckle noise, therefore, is a crucial step in the preprocessing of ultrasound images. Researchers have proposed several speckle reduction methods, but no single method takes all relevant factors into account. In this paper, we compare seven such methods: Median, Gaussian, Bilateral, Average, Weiner, Anisotropic and Denoising auto-encoder without and with skip connections in terms of their ability to preserve features and edges while effectively reducing noise. In an experimental study, a convolutional noise-removing auto-encoder with skip connection, a deep learning method, was used to improve ultrasound images of breast cancer. This method involved adding speckle noise at various levels. The results of the deep learning method were compared to those of traditional image enhancement methods, and it was found that the proposed method was more effective. To assess the performance of these algorithms, we use three established evaluation metrics and present both filtered images and statistical data.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Selected for presentation at 2024 IEEE South Asian Ultrasonics Symposium"
    },
    {
        "paper id": "2403.02751",
        "abstract url": "https://arxiv.org/abs/2403.02751",
        "title": "Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps",
        "rating": -3,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "trajectory"
            ],
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "We present Splat-Nav, a real-time navigation pipeline designed to work with environment representations generated by Gaussian Splatting (GSplat), a popular emerging 3D scene representation from computer vision. Splat-Nav consists of two components: 1) Splat-Plan, a safe planning module, and 2) Splat-Loc, a robust pose estimation module. Splat-Plan builds a safe-by-construction polytope corridor through the map based on mathematically rigorous collision constraints and then constructs a B\u00e9zier curve trajectory through this corridor. Splat-Loc provides a robust state estimation module, leveraging the point-cloud representation inherent in GSplat scenes for global pose initialization, in the absence of prior knowledge, and recursive real-time pose localization, given only RGB images. The most compute-intensive procedures in our navigation pipeline, such as the computation of the B\u00e9zier trajectories and the pose optimization problem run primarily on the CPU, freeing up GPU resources for GPU-intensive tasks, such as online training of Gaussian Splats. We demonstrate the safety and robustness of our pipeline in both simulation and hardware experiments, where we show online re-planning at 5 Hz and pose estimation at about 25 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02937",
        "abstract url": "https://arxiv.org/abs/2403.02937",
        "title": "Quantum Algorithms in a Superposition of Spacetimes",
        "rating": -3,
        "keywords": [
            [
                "Graph"
            ],
            [
                "Quantum",
                "physics"
            ]
        ],
        "abstract": "Quantum computers are expected to revolutionize our ability to process information. The advancement from classical to quantum computing is a product of our advancement from classical to quantum physics -- the more our understanding of the universe grows, so does our ability to use it for computation. A natural question that arises is, what will physics allow in the future? Can more advanced theories of physics increase our computational power, beyond quantum computing? An active field of research in physics studies theoretical phenomena outside the scope of explainable quantum mechanics, that form when attempting to combine Quantum Mechanics (QM) with General Relativity (GR) into a unified theory of Quantum Gravity (QG). QG is known to present the possibility of a quantum superposition of causal structure and event orderings. In the literature of quantum information theory, this translates to a superposition of unitary evolution orders. In this work we show a first example of a natural computational model based on QG, that provides an exponential speedup over standard quantum computation (under standard hardness assumptions). We define a model and complexity measure for a quantum computer that has the ability to generate a superposition of unitary evolution orders, and show that such computer is able to solve in polynomial time two of the fundamental problems in computer science: The Graph Isomorphism Problem ($\\mathsf{GI}$) and the Gap Closest Vector Problem ($\\mathsf{GapCVP}$), with gap $O\\left( n^{2} \\right)$. These problems are believed by experts to be hard to solve for a regular quantum computer. Interestingly, our model does not seem overpowered, and we found no obvious way to solve entire complexity classes that are considered hard in computer science, like the classes $\\mathbf{NP}$ and $\\mathbf{SZK}$.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02975",
        "abstract url": "https://arxiv.org/abs/2403.02975",
        "title": "A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching",
        "rating": -3,
        "keywords": [
            [
                "Medical"
            ],
            [
                "recommendation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \\textit{keywords} and \\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the model from the reliance on NER models. To this end, we devise a \\underline{M}ulti-\\underline{C}oncept \\underline{P}arsed \\underline{S}emantic \\underline{M}atching framework based on the pre-trained language models, abbreviated as \\textbf{MCP-SM}, to extract various concepts and infuse them into the classification tokens. We conduct comprehensive experiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM. Besides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding performance further prove MCP-SM's applicability in low-resource languages.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "arXiv admin comment: This version has been removed by arXiv administrators as the submitter did not have the rights to agree to the license at the time of submission"
    },
    {
        "paper id": "2403.02995",
        "abstract url": "https://arxiv.org/abs/2403.02995",
        "title": "Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees",
        "rating": -3,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "healthcare"
            ]
        ],
        "abstract": "Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations. Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks. These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa. This manipulation results in misclassification and leads to incorrect model behavior. Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks. The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizing the critical importance of effective defense strategies, this paper contributes to the ongoing efforts to fortify ML models against adversarial threats within the ML domain in network security. We propose an innovative alarm system that detects the presence of poisoned labels and a defense mechanism designed to uncover the original class labels with the aim of mitigating backdoor attacks on ensemble tree classifiers. We conducted a case study using the Alexa and Phishing Site URL datasets and showed that LF attacks can be addressed using our proposed defense mechanism. Our experimental results prove that the LF attack achieved an Attack Success Rate (ASR) between 50-65% within 2-5%, and the innovative defense method successfully detected poisoned labels with an accuracy of up to 100%.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03063",
        "abstract url": "https://arxiv.org/abs/2403.03063",
        "title": "CrackNex: a Few-shot Low-light Crack Segmentation Model Based on Retinex Theory for UAV Inspections",
        "rating": -3,
        "keywords": [
            [
                "health"
            ],
            [
                "UAV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Routine visual inspections of concrete structures are imperative for upholding the safety and integrity of critical infrastructure. Such visual inspections sometimes happen under low-light conditions, e.g., checking for bridge health. Crack segmentation under such conditions is challenging due to the poor contrast between cracks and their surroundings. However, most deep learning methods are designed for well-illuminated crack images and hence their performance drops dramatically in low-light scenes. In addition, conventional approaches require many annotated low-light crack images which is time-consuming. In this paper, we address these challenges by proposing CrackNex, a framework that utilizes reflectance information based on Retinex Theory to help the model learn a unified illumination-invariant representation. Furthermore, we utilize few-shot segmentation to solve the inefficient training data problem. In CrackNex, both a support prototype and a reflectance prototype are extracted from the support set. Then, a prototype fusion module is designed to integrate the features from both prototypes. CrackNex outperforms the SOTA methods on multiple datasets. Additionally, we present the first benchmark dataset, LCSD, for low-light crack segmentation. LCSD consists of 102 well-illuminated crack images and 41 low-light crack images. The dataset and code are available at https://github.com/zy1296/CrackNex.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 4 figures, IEEE International Conference on Robotics and Automation (ICRA) 2024"
    },
    {
        "paper id": "2403.03310",
        "abstract url": "https://arxiv.org/abs/2403.03310",
        "title": "Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm",
        "rating": -3,
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "In recent years, quantum computing has emerged as a transformative force in the field of combinatorial optimization, offering novel approaches to tackling complex problems that have long challenged classical computational methods. Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out for its potential to efficiently solve the Max-Cut problem, a quintessential example of combinatorial optimization. However, practical application faces challenges due to current limitations on quantum computational resource. Our work optimizes QAOA initialization, using Graph Neural Networks (GNN) as a warm-start technique. This sacrifices affordable computational resource on classical computer to reduce quantum computational resource overhead, enhancing QAOA's effectiveness. Experiments with various GNN architectures demonstrate the adaptability and stability of our framework, highlighting the synergy between quantum algorithms and machine learning. Our findings show GNN's potential in improving QAOA performance, opening new avenues for hybrid quantum-classical approaches in quantum computing and contributing to practical applications.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03434",
        "abstract url": "https://arxiv.org/abs/2403.03434",
        "title": "An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand",
        "rating": -3,
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "health",
                "disease"
            ]
        ],
        "abstract": "Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation. In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) network. The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases. This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks. This, in turn, helps us make more informed decision when developing effective strategies that strike a balance between managing outbreaks and minimizing disruptions to everyday life.",
        "subjects": [
            "cs.MA"
        ],
        "comment": "11 pages, 9 figures"
    },
    {
        "paper id": "2403.04803",
        "abstract url": "https://arxiv.org/abs/2403.04803",
        "title": "Enhancing Security in Federated Learning through Adaptive Consensus-Based Model Update Validation",
        "rating": -3,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "anomaly detection"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "This paper introduces an advanced approach for fortifying Federated Learning (FL) systems against label-flipping attacks. We propose a simplified consensus-based verification process integrated with an adaptive thresholding mechanism. This dynamic thresholding is designed to adjust based on the evolving landscape of model updates, offering a refined layer of anomaly detection that aligns with the real-time needs of distributed learning environments. Our method necessitates a majority consensus among participating clients to validate updates, ensuring that only vetted and consensual modifications are applied to the global model. The efficacy of our approach is validated through experiments on two benchmark datasets in deep learning, CIFAR-10 and MNIST. Our results indicate a significant mitigation of label-flipping attacks, bolstering the FL system's resilience. This method transcends conventional techniques that depend on anomaly detection or statistical validation by incorporating a verification layer reminiscent of blockchain's participatory validation without the associated cryptographic overhead. The innovation of our approach rests in striking an optimal balance between heightened security measures and the inherent limitations of FL systems, such as computational efficiency and data privacy. Implementing a consensus mechanism specifically tailored for FL environments paves the way for more secure, robust, and trustworthy distributed machine learning applications, where safeguarding data integrity and model robustness is critical.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.13825",
        "abstract url": "https://arxiv.org/abs/2403.13825",
        "title": "Deep Generative Models for Ultra-High Granularity Particle Physics Detector Simulation: A Voyage From Emulation to Extrapolation",
        "rating": -3,
        "keywords": [
            [
                "GAN"
            ],
            [
                "Physics"
            ]
        ],
        "abstract": "Simulating ultra-high-granularity detector responses in Particle Physics represents a critical yet computationally demanding task. This thesis aims to overcome this challenge for the Pixel Vertex Detector (PXD) at the Belle II experiment, which features over 7.5M pixel channels-the highest spatial resolution detector simulation dataset ever analysed with generative models. This thesis starts off by a comprehensive and taxonomic review on generative models for simulating detector signatures. Then, it presents the Intra-Event Aware Generative Adversarial Network (IEA-GAN), a new geometry-aware generative model that introduces a relational attentive reasoning and Self-Supervised Learning to approximate an \"event\" in the detector. This study underscores the importance of intra-event correlation for downstream physics analyses. Building upon this, the work drifts towards a more generic approach and presents YonedaVAE, a Category Theory-inspired generative model that tackles the open problem of Out-of-Distribution (OOD) simulation. YonedaVAE introduces a learnable Yoneda embedding to capture the entirety of an event based on its sensor relationships, formulating a Category theoretical language for intra-event relational reasoning. This is complemented by introducing a Self-Supervised learnable prior for VAEs and an Adaptive Top-q sampling mechanism, enabling the model to sample point clouds with variable intra-category cardinality in a zero-shot manner. Variable Intra-event cardinality has not been approached before and is vital for simulating irregular detector geometries. Trained on an early experiment data, YonedaVAE can reach a reasonable OOD simulation precision of a later experiment with almost double luminosity. This study introduces, for the first time, the results of using deep generative models for ultra-high granularity detector simulation in Particle Physics.",
        "subjects": [
            "physics.ins-det"
        ],
        "comment": "PhD thesis, 234 pages"
    },
    {
        "paper id": "2404.16849",
        "abstract url": "https://arxiv.org/abs/2404.16849",
        "title": "Smart Grids Secured By Dynamic Watermarking: How Secure?",
        "rating": -3,
        "keywords": [
            [
                "attack"
            ],
            [
                "Watermarking"
            ]
        ],
        "abstract": "Unconditional security for smart grids is defined. Cryptanalyses of the watermarked security of smart grids indicate that watermarking cannot guarantee unconditional security unless the communication within the grid system is unconditionally secure. The successful attack against the dynamically watermarked smart grid remains valid even with the presence of internal noise from the grid. An open question arises: if unconditionally authenticated secure communications within the grid, together with tamper resistance of the critical elements, are satisfactory conditions to provide unconditional security for the grid operation.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted for publication in Fluct. Noise Lett"
    },
    {
        "paper id": "2403.02682",
        "abstract url": "https://arxiv.org/abs/2403.02682",
        "title": "Time Weaver: A Conditional Time Series Generation Model",
        "rating": -3.5,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "vehicle"
            ],
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.). Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that Time Weaver outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 27% in downstream classification tasks on real-world energy, medical, air quality, and traffic data sets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02914",
        "abstract url": "https://arxiv.org/abs/2403.02914",
        "title": "DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting",
        "rating": -3.5,
        "keywords": [
            [
                "industrial"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Worse still, the complex technical design may ultimately lead to a model with weak generalizability. In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level. However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps. To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02922",
        "abstract url": "https://arxiv.org/abs/2403.02922",
        "title": "From Spectra to Biophysical Insights: End-to-End Learning with a Biased Radiative Transfer Model",
        "rating": -3.5,
        "keywords": [
            [
                "Biophysical"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Advances in machine learning have boosted the use of Earth observation data for climate change research. Yet, the interpretability of machine-learned representations remains a challenge, particularly in understanding forests' biophysical reactions to climate change. Traditional methods in remote sensing that invert radiative transfer models (RTMs) to retrieve biophysical variables from spectral data often fail to account for biases inherent in the RTM, especially for complex forests. We propose to integrate RTMs into an auto-encoder architecture, creating an end-to-end learning approach. Our method not only corrects biases in RTMs but also outperforms traditional techniques for variable retrieval like neural network regression. Furthermore, our framework has potential generally for inverting biased physical models. The code is available on https://github.com/yihshe/ai-refined-rtm.git.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03414",
        "abstract url": "https://arxiv.org/abs/2403.03414",
        "title": "Leveraging The Finite States of Emotion Processing to Study Late-Life Mental Health",
        "rating": -3.5,
        "keywords": [
            [
                "Health",
                "fMRI"
            ],
            [
                "grammar"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Traditional approaches in mental health research apply General Linear Models (GLM) to describe the longitudinal dynamics of observed psycho-behavioral measurements (questionnaire summary scores). Similarly, GLMs are also applied to characterize relationships between neurobiological measurements (regional fMRI signals) and perceptual stimuli or other regional signals. While these methods are useful for exploring linear correlations among the isolated signals of those constructs (i.e., summary scores or fMRI signals), these classical frameworks fall short in providing insights into the comprehensive system-level dynamics underlying observable changes. Hidden Markov Models (HMM) are a statistical model that enable us to describe the sequential relations among multiple observable constructs, and when applied through the lens of Finite State Automata (FSA), can provide a more integrated and intuitive framework for modeling and understanding the underlying controller (the prescription for how to respond to inputs) that fundamentally defines any system, as opposed to linearly correlating output signals produced by the controller. We present a simple and intuitive HMM processing pipeline vcHMM (See Preliminary Data) that highlights FSA theory and is applicable for both behavioral analysis of questionnaire data and fMRI data. HMMs offer theoretic promise as they are computationally equivalent to the FSA, the control processor of a Turing Machine (TM) The dynamic programming Viterbi algorithm is used to leverage the HMM model. It efficiently identifies the most likely sequence of hidden states. The vcHMM pipeline leverages this grammar to understand how behavior and neural activity relate to depression.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03425",
        "abstract url": "https://arxiv.org/abs/2403.03425",
        "title": "Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for Text-Oriented Molecular Optimization",
        "rating": -3.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The integration of deep learning, particularly AI-Generated Content, with high-quality data derived from ab initio calculations has emerged as a promising avenue for transforming the landscape of scientific research. However, the challenge of designing molecular drugs or materials that incorporate multi-modality prior knowledge remains a critical and complex undertaking. Specifically, achieving a practical molecular design necessitates not only meeting the diversity requirements but also addressing structural and textural constraints with various symmetries outlined by domain experts. In this article, we present an innovative approach to tackle this inverse design problem by formulating it as a multi-modality guidance generation/optimization task. Our proposed solution involves a textural-structure alignment symmetric diffusion framework for the implementation of molecular generation/optimization tasks, namely 3DToMolo. 3DToMolo aims to harmonize diverse modalities, aligning them seamlessly to produce molecular structures adhere to specified symmetric structural and textural constraints by experts in the field. Experimental trials across three guidance generation settings have shown a superior hit generation performance compared to state-of-the-art methodologies. Moreover, 3DToMolo demonstrates the capability to generate novel molecules, incorporating specified target substructures, without the need for prior knowledge. This work not only holds general significance for the advancement of deep learning methodologies but also paves the way for a transformative shift in molecular design strategies. 3DToMolo creates opportunities for a more nuanced and effective exploration of the vast chemical space, opening new frontiers in the development of molecular entities with tailored properties and functionalities.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02744",
        "abstract url": "https://arxiv.org/abs/2403.02744",
        "title": "Self-adaptive Traffic Anomaly Detection System for IoT Smart Home Environments",
        "rating": -4,
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "attacks"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "With the growth of internet of things (IoT) devices, cyberattacks, such as distributed denial of service, that exploit vulnerable devices infected with malware have increased. Therefore, vendors and users must keep their device firmware updated to eliminate vulnerabilities and quickly handle unknown cyberattacks. However, it is difficult for both vendors and users to continually keep the devices safe because vendors must provide updates quickly and the users must continuously manage the conditions of all deployed devices. Therefore, to ensure security, it is necessary for a system to adapt autonomously to changes in cyberattacks. In addition, it is important to consider network-side security that detects and filters anomalous traffic at the gateway to comprehensively protect those devices. This paper proposes a self-adaptive anomaly detection system for IoT traffic, including unknown attacks. The proposed system comprises a honeypot server and a gateway. The honeypot server continuously captures traffic and adaptively generates an anomaly detection model using real-time captured traffic. Thereafter, the gateway uses the generated model to detect anomalous traffic. Thus, the proposed system can adapt to unknown attacks to reflect pattern changes in anomalous traffic based on real-time captured traffic. Three experiments were conducted to evaluate the proposed system: a virtual experiment using pre-captured traffic from various regions across the world, a demonstration experiment using real-time captured traffic, and a virtual experiment using a public dataset containing the traffic generated by malware. The experimental results indicate that a system adaptable in real time to evolving cyberattacks is a novel approach for ensuring the comprehensive security of IoT devices against both known and unknown attacks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "12 pages, 43 figures"
    },
    {
        "paper id": "2403.02921",
        "abstract url": "https://arxiv.org/abs/2403.02921",
        "title": "Quantum for Good and the Societal Impact of Quantum Computing",
        "rating": -4,
        "keywords": [
            [
                "disease"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum computing promises to help humanity solve problems that would otherwise be intractable on classical computers. Unlike today's machines, quantum computers use a novel computing process that leverages the foundational quantum mechanical laws of nature. This unlocks unparalleled compute power for certain applications and promises to help solve some of our generation's gravest challenges, including the climate crisis, food insecurity, and widespread disease. No one entity will be able to realize this end state alone. Developing a fault-tolerant quantum supercomputer and a vibrant ecosystem around it will require deep partnerships between industry, governments, and academia. It will also require collective action to enable and promote positive applications of quantum computing and ensure that the safe and responsible use of the technology is at the center of its development and deployment. Achieving these objectives will require focusing on three priorities: 1. Impact. Ensure quantum computing benefits all of humankind by developing quantum solutions to solve critical, global problems. 2. Use. Protect against malicious use by accelerating the deployment of quantum-safe cryptography and developing governance processes and controls for the responsible use of quantum machines. 3. Access. Democratize the potential for economic growth across all of society through skilling, workforce and ecosystem development, and digital infrastructure. This paper discusses each in turn.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": "to be published in Pontificiae Academiae Scientiarvm Scripta Varia"
    },
    {
        "paper id": "2403.03355",
        "abstract url": "https://arxiv.org/abs/2403.03355",
        "title": "The vehicle routing problem with synchronization constraints and support vehicle-dependent service times",
        "rating": -4,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "graph"
            ],
            [
                "agricultural"
            ]
        ],
        "abstract": "Many production processes require the cooperation of various resources. Especially when using expensive machines, their utilization plays a decisive role in efficient production. In agricultural production or civil construction processes, e.g., harvesting or road building, the machines are typically mobile, and synchronization of different machine types is required to perform operations. In addition, the productivity of one type often depends on the availability of another type. In this paper, we consider two types of vehicles, called primary and support vehicles. Primary vehicles perform operations and are assisted by at least one support vehicle, with more support vehicles resulting in faster service times for primary vehicles. We call this practical problem the vehicle routing and scheduling problem with support vehicle-dependent service times and introduce two mixed-integer linear programming models. The first represents each support vehicle individually with binary decision variables, while the second considers the cumulative flow of support vehicles with integer decision variables. Furthermore, the models are defined on a graph that allows easy transformation into multiple variants. These variants are based on allowing or prohibiting switching support vehicles between primary vehicles and splitting services among primary vehicles. We show in our extensive computational experiments that: i) the integer representation of support vehicles is superior to the binary representation, ii) the benefit of additional vehicles is subject to saturation effects and depends on the ratio of support and primary vehicles, and iii) switching and splitting lead to problems that are more difficult to solve, but also result in better solutions with higher primary vehicle utilization.",
        "subjects": [
            "math.OC"
        ],
        "comment": "21 pages, 6 figures"
    },
    {
        "paper id": "2403.03394",
        "abstract url": "https://arxiv.org/abs/2403.03394",
        "title": "Multiple Update Particle Filter: Position Estimation by Combining GNSS Pseudorange and Carrier Phase Observations",
        "rating": -4,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "satellite"
            ]
        ],
        "abstract": "This paper presents an efficient method for updating particles in a particle filter (PF) to address the position estimation problem when dealing with sharp-peaked likelihood functions derived from multiple observations. Sharp-peaked likelihood functions commonly arise from millimeter-accurate distance observations of carrier phases in the global navigation satellite system (GNSS). However, when such likelihood functions are used for particle weight updates, the absence of particles within the peaks leads to all particle weights becoming zero. To overcome this problem, in this study, a straightforward and effective approach is introduced for updating particles when dealing with sharp-peaked likelihood functions obtained from multiple observations. The proposed method, termed as the multiple update PF, leverages prior knowledge regarding the spread of distribution for each likelihood function and conducts weight updates and resampling iteratively in the particle update process, prioritizing the likelihood function spreads. Experimental results demonstrate the efficacy of our proposed method, particularly when applied to position estimation utilizing GNSS pseudorange and carrier phase observations. The multiple update PF exhibits faster convergence with fewer particles when compared to the conventional PF. Moreover, vehicle position estimation experiments conducted in urban environments reveal that the proposed method outperforms conventional GNSS positioning techniques, yielding more accurate position estimates.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)"
    },
    {
        "paper id": "2403.02772",
        "abstract url": "https://arxiv.org/abs/2403.02772",
        "title": "Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives",
        "rating": -4.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "clinical"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entire dataset to train a single model applicable to all exercise types. This model, with a Spatial-Temporal Graph Convolutional Network (ST-GCN) architecture, demonstrated enhanced generalizability across exercises and a decrease in overall complexity. Through extensive experiments on three publicly available rehabilitation exercise assessment datasets, the University of Idaho-Physical Rehabilitation Movement Data (UI-PRMD), IntelliRehabDS (IRDS), and KInematic assessment of MOvement and clinical scores for remote monitoring of physical REhabilitation (KIMORE), our method has shown to surpass existing methods, setting a new benchmark in rehabilitation exercise assessment accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03218",
        "abstract url": "https://arxiv.org/abs/2403.03218",
        "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "rating": -4.5,
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "biological"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai",
        "subjects": [
            "cs.LG"
        ],
        "comment": "See the project page at https://wmdp.ai"
    },
    {
        "paper id": "2403.12996",
        "abstract url": "https://arxiv.org/abs/2403.12996",
        "title": "UAV-Based Solution for Extending the Lifetime of IoT Devices: Efficiency, Design and Sustainability",
        "rating": -5,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "IoT"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Internet of Things (IoT) technology is named as a key ingredient in the evolution towards digitization of many applications and services. A deployment based on battery-powered remote Internet of Things (IoT) devices enables easy installation and operation, yet the autonomy of these devices poses a crucial challenge. A too short lifespan is undesirable from a functional, economical, and ecological point of view. This paper presents a unmanned aerial vehicle (UAV)-based approach to recharge remote Internet of Things (IoT) nodes. An in-depth study of the charging efficiency and optimization of key parameters, and measurements-based verification, is reported on. An actual corresponding design and implementation of the full UAV-based charging system and its proof-of-concept validation are presented. Finally, the sustainability of the proposed solution is discussed. The results presented in this paper hence confirm that the proposed UAV-based approach and design are functionally successful and efficient charging can be achieved, provided the constraints and challenges coming with the approach are adequately dealt with. Moreover, it comes with an overall reduction in ecological footprint for IoT applications relying on battery-powered nodes in need of medium energy and/or considerable lifetime expectation (5 years or more).",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02863",
        "abstract url": "https://arxiv.org/abs/2403.02863",
        "title": "Spintronic Implementation of UNet for Image Segmentation",
        "rating": -6,
        "keywords": [
            [
                "medical",
                "diagnosis"
            ],
            [
                "satellite"
            ],
            [
                "physics"
            ]
        ],
        "abstract": "Image segmentation plays a crucial role in computer vision applications like self-driving cars, satellite imagery analysis, and medical diagnosis. Implementing these complex deep neural networks on conventional hardware is highly inefficient. In this work, we propose hardware implementation of UNet for segmentation tasks, using spintronic devices. Our approach involves designing hardware for convolution, deconvolution, ReLU, and max pooling layers of the UNet architecture. We demonstrate the synaptic behavior of the domain wall MTJ, and design convolution and deconvolution layers using the domain wall-based crossbar array. We utilize the orthogonal current injected MTJ with its continuous resistance change and showcase the ReLU and max pooling functions. We employ a hybrid simulation setup by coupling micromagnetic simulation, non-equilibrium Green's function, Landau-Lifshitz-Gilbert-Slonczewski equations, and circuit simulation with Python programming to incorporate the diverse physics of spin-transport, magnetization dynamics, and CMOS elements in our proposed designs. We evaluate our UNet design on the CamVid dataset and achieve segmentation accuracies that are comparable to software implementation. During training, our design consumes 43.59pJ of energy for synaptic weight updates.",
        "subjects": [
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03175",
        "abstract url": "https://arxiv.org/abs/2403.03175",
        "title": "Remote sensing of soil moisture using Rydberg atoms and satellite signals of opportunity",
        "rating": -6,
        "keywords": [
            [
                "radar"
            ],
            [
                "navigation"
            ],
            [
                "Remote sensing",
                "satellite"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "Spaceborne radar remote sensing of the earth system is essential to study natural and man-made changes in the ecosystem, water and energy cycles, weather and air quality, sea level, and surface dynamics. A major challenge with current approaches is the lack of broad spectrum tunability due to narrow band microwave electronics, that limit systems to specific science variable retrievals. This results in a significant limitation in studying dynamic coupled earth system processes such as surface and subsurface hydrology, where broad spectrum radar remote sensing is needed to sense multiple variables simultaneously. Rydberg atomic sensors are highly sensitive broad-spectrum quantum detectors that can be dynamically tuned to cover micro-to-millimeter waves with no requirement for band-specific electronics. Rydberg atomic sensors can use existing transmitted signals such as navigation and communication satellites to enable remote sensing. We demonstrate remote sensing of soil moisture, an important earth system variable, via ground-based radar reflectometry with Rydberg atomic systems. To do this, we sensitize the atoms to XM satellite radio signals and use signal correlations to demonstrate use of these satellite signals for remote sensing of soil moisture. Our approach provides a step towards satellite-based broad-spectrum Rydberg atomic remote sensing.",
        "subjects": [
            "physics.app-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02661",
        "abstract url": "https://arxiv.org/abs/2403.02661",
        "title": "How to Save My Gas Fees: Understanding and Detecting Real-world Gas Issues in Solidity Programs",
        "rating": -10,
        "keywords": [],
        "abstract": "The execution of smart contracts on Ethereum, a public blockchain system, incurs a fee called gas fee for its computation and data-store consumption. When programmers develop smart contracts (e.g., in the Solidity programming language), they could unknowingly write code snippets that unnecessarily cause more gas fees. These issues, or what we call gas wastes, could lead to significant monetary waste for users. Yet, there have been no systematic examination of them or effective tools for detecting them. This paper takes the initiative in helping Ethereum users reduce their gas fees in two important steps: we conduct the first empirical study on gas wastes in popular smart contracts written in Solidity by understanding their root causes and fixing strategies; we then develop a static tool, PeCatch, to effectively detect gas wastes with simple fixes in Solidity programs based on our study findings. Overall, we make seven insights and four suggestions from our gas-waste study, which could foster future tool development, language improvement, and programmer awareness, and develop eight gas-waste checkers, which pinpoint 383 previously unknown gas wastes from famous Solidity libraries.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02680",
        "abstract url": "https://arxiv.org/abs/2403.02680",
        "title": "A Dual-Level Cancelable Framework for Palmprint Verification and Hack-Proof Data Storage",
        "rating": -10,
        "keywords": [],
        "abstract": "In recent years, palmprints have been widely used for individual verification. The rich privacy information in palmprint data necessitates its protection to ensure security and privacy without sacrificing system performance. Existing systems often use cancelable technologies to protect templates, but these technologies ignore the potential risk of data leakage. Upon breaching the system and gaining access to the stored database, a hacker could easily manipulate the stored templates, compromising the security of the verification system. To address this issue, we propose a dual-level cancelable palmprint verification framework in this paper. Specifically, the raw template is initially encrypted using a competition hashing network with a first-level token, facilitating the end-to-end generation of cancelable templates. Different from previous works, the protected template undergoes further encryption to differentiate the second-level protected template from the first-level one. The system specifically creates a negative database (NDB) with the second-level token for dual-level protection during the enrollment stage. Reversing the NDB is NP-hard and a fine-grained algorithm for NDB generation is introduced to manage the noise and specified bits. During the verification stage, we propose an NDB matching algorithm based on matrix operation to accelerate the matching process of previous NDB methods caused by dictionary-based matching rules. This approach circumvents the need to store templates identical to those utilized for verification, reducing the risk of potential data leakage. Extensive experiments conducted on public palmprint datasets have confirmed the effectiveness and generality of the proposed framework. Upon acceptance of the paper, the code will be accessible at https://github.com/Deep-Imaging-Group/NPR.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02693",
        "abstract url": "https://arxiv.org/abs/2403.02693",
        "title": "Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video Streaming",
        "rating": -10,
        "keywords": [],
        "abstract": "Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames. Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks. Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability. In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to introduce minimal transmission and computation overhead for mobile terminals. We also propose a model-agnostic meta-learning (MAML) based saliency prediction network trainer, which provides a few-sample fast training solution to obtain the prediction model by utilizing the information from the past models. We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem. Extensive experiment results show that our prediction approach can work in real-time for live video streaming and can achieve higher accuracies compared to other existing prediction methods on mobile end, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects. We observe the accuracy of MFVP is 8.1$\\%$ to 28.7$\\%$ higher than other algorithms and achieves 3.73$\\%$ to 14.96$\\%$ higher average quality level and 49.6$\\%$ to 74.97$\\%$ less quality level change than other algorithms.",
        "subjects": [
            "cs.MM"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2403.02709",
        "abstract url": "https://arxiv.org/abs/2403.02709",
        "title": "RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches",
        "rating": -10,
        "keywords": [],
        "abstract": "Natural language and images are commonly used as goal representations in goal-conditioned imitation learning (IL). However, natural language can be ambiguous and images can be over-specified. In this work, we propose hand-drawn sketches as a modality for goal specification in visual imitation learning. Sketches are easy for users to provide on the fly like language, but similar to images they can also help a downstream policy to be spatially-aware and even go beyond images to disambiguate task-relevant from task-irrelevant objects. We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions. We train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches. We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop. Experimentally we find that RT-Sketch is able to perform on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present. Additionally, we show that RT-Sketch has the capacity to interpret and act upon sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings. For supplementary material and videos, please refer to our website: http://rt-sketch.github.io.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02711",
        "abstract url": "https://arxiv.org/abs/2403.02711",
        "title": "Reducing computational effort in topology optimization considering the deformation in additive manufacturing",
        "rating": -10,
        "keywords": [],
        "abstract": "Integrating topology optimization and additive manufacturing (AM) technology can facilitate innovative product development. However, laser powder bed fusion, which is the predominant method in metal AM, can lead to issues such as residual stress and deformation. Recently, topology optimization methods considering these stresses and deformations have been proposed; however, they suffer from challenges caused by an increased computational cost. In this study, we propose a method for reducing computational cost in topology optimization considering the deformation in AM. An inherent strain method-based analytical model is presented for simulating the residual stress and deformation in the AM process. Subsequently, a constraint condition to suppress the deformation is formulated, and a method to reduce the computational cost of the adjoint analysis in deriving sensitivity is proposed. The minimum mean compliance problem considering AM deformation and self-support constraints can then be incorporated into the level set-based topology optimization framework. Finally, numerical examples are presented for validating the effectiveness of the proposed topology optimization method.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "26 pages, 15 figures"
    },
    {
        "paper id": "2403.02716",
        "abstract url": "https://arxiv.org/abs/2403.02716",
        "title": "Pre-trained Model-based Actionable Warning Identification: A Feasibility Study",
        "rating": -10,
        "keywords": [],
        "abstract": "Actionable Warning Identification (AWI) plays a pivotal role in improving the usability of static code analyzers. Currently, Machine Learning (ML)-based AWI approaches, which mainly learn an AWI classifier from labeled warnings, are notably common. However, these approaches still face the problem of restricted performance due to the direct reliance on a limited number of labeled warnings to develop a classifier. Very recently, Pre-Trained Models (PTMs), which have been trained through billions of text/code tokens and demonstrated substantial success applications on various code-related tasks, could potentially circumvent the above problem. Nevertheless, the performance of PTMs on AWI has not been systematically investigated, leaving a gap in understanding their pros and cons. In this paper, we are the first to explore the feasibility of applying various PTMs for AWI. By conducting the extensive evaluation on 10K+ SpotBugs warnings from 10 large-scale and open-source projects, we observe that all studied PTMs are consistently 9.85%~21.12% better than the state-of-the-art ML-based AWI approaches. Besides, we investigate the impact of three primary aspects (i.e., data preprocessing, model training, and model prediction) in the typical PTM-based AWI workflow. Further, we identify the reasons for current PTMs' underperformance on AWI. Based on our findings, we provide several practical guidelines to enhance PTM-based AWI in future work.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02741",
        "abstract url": "https://arxiv.org/abs/2403.02741",
        "title": "State-Constrained Zero-Sum Differential Games with One-Sided Information",
        "rating": -10,
        "keywords": [],
        "abstract": "We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to either violate the state constraints, or otherwise, to maximize the payoff. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to differential games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing the behavioral strategies. Compared with existing works on imperfect-information dynamic games that focus on scalability and generalization, our focus is instead on revealing the mechanism of belief manipulation behaviors resulted from information asymmetry and state constraints. We use a simplified football game to demonstrate the utility of this work, where we reveal player positions and belief states in which the attacker should (or should not) play specific random fake moves to take advantage of information asymmetry, and compute how the defender should respond.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02752",
        "abstract url": "https://arxiv.org/abs/2403.02752",
        "title": "HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents",
        "rating": -10,
        "keywords": [],
        "abstract": "Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates Large Language Models (LLMs) as both a general NLP task solver and an intelligent agent. By leveraging the extraction capability of LLMs in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus. The constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity. The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate sensemaking. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02754",
        "abstract url": "https://arxiv.org/abs/2403.02754",
        "title": "Learning to Ask Critical Questions for Assisting Product Search",
        "rating": -10,
        "keywords": [],
        "abstract": "Product search plays an essential role in eCommerce. It was treated as a special type of information retrieval problem. Most existing works make use of historical data to improve the search performance, which do not take the opportunity to ask for user's current interest directly. Some session-aware methods take the user's clicks within the session as implicit feedback, but it is still just a guess on user's preference. To address this problem, recent conversational or question-based search models interact with users directly for understanding the user's interest explicitly. However, most users do not have a clear picture on what to buy at the initial stage. Asking critical attributes that the user is looking for after they explored for a while should be a more efficient way to help them searching for the target items. In this paper, we propose a dual-learning model that hybrids the best from both implicit session feedback and proactively clarifying with users on the most critical questions. We first establish a novel utility score to measure whether a clicked item provides useful information for finding the target. Then we develop the dual Selection Net and Ranking Net for choosing the critical questions and ranking the items. It innovatively links traditional click-stream data and text-based questions together. To verify our proposal, we did extensive experiments on a public dataset, and our model largely outperformed other state-of-the-art methods.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "SIGIR eCom'22"
    },
    {
        "paper id": "2403.02778",
        "abstract url": "https://arxiv.org/abs/2403.02778",
        "title": "Abstracting Denotational Interpreters",
        "rating": -10,
        "keywords": [],
        "abstract": "We explore denotational interpreters: denotational semantics that produce coinductive traces of a corresponding small-step operational semantics. By parameterising our denotational interpreter over the semantic domain and then varying it, we recover dynamic semantics with different evaluation strategies as well as summary-based static analyses such as type analysis, all from the same generic interpreter. Among our contributions is the first provably adequate denotational semantics for call-by-need. The generated traces lend themselves well to describe operational properties such as evaluation cardinality, and hence to static analyses abstracting these operational properties. Since static analysis and dynamic semantics share the same generic interpreter definition, soundness proofs via abstract interpretation decompose into showing small abstraction laws about the abstract domain, thus obviating complicated ad-hoc preservation-style proof frameworks.",
        "subjects": [
            "cs.PL"
        ],
        "comment": "Preprint; submitted to ICFP'24"
    },
    {
        "paper id": "2403.02811",
        "abstract url": "https://arxiv.org/abs/2403.02811",
        "title": "Linear quadratic control of nonlinear systems with Koopman operator learning and the Nystr\u00f6m method",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems. While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\u00f6m approximation) can be used to achieve huge computational savings while preserving accuracy. Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\u00f6m approximation. More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size. Theoretical findings are complemented by numerical experiments corroborating our results.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02854",
        "abstract url": "https://arxiv.org/abs/2403.02854",
        "title": "STAR-RIS Assisted Wireless-Powered and Backscattering Mobile Edge Computing Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "Wireless powered and backscattering mobile edge computing (WPB-MEC) network is a novel network paradigm to supply energy supplies and computing resource to wireless sensors (WSs). However, its performance is seriously affected by severe attenuations and inappropriate assumptions of infinite computing capability at the hybrid access point (HAP). To address the above issues, in this paper, we propose a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) aided scheme for boosting the performance of WPB-MEC network under the constraint of finite computing capability. Specifically, energy-constrained WSs are able to offload tasks actively or passively from them to the HAP. In this process, the STAR-RIS is utilized to improve the quantity of harvested energy and strengthen the offloading efficiency by adapting its operating protocols. We then maximize the sum computational bits (SCBs) under the finite computing capability constraint. To handle the solving challenges, we first present interesting results in closed-form and then design a block coordinate descent (BCD) based algorithm, ensuring a near-optimal solution. Finally, simulation results are provided to confirm that our proposed scheme can improve the SCBs by 9.9 times compared to the local computing only scheme.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted by China Communications. 13 pages, 8 figures"
    },
    {
        "paper id": "2403.02866",
        "abstract url": "https://arxiv.org/abs/2403.02866",
        "title": "Unlocking Electro-optic Resonant Phase Shifting for Multi-dimensional, Ultra-dynamic Photonic Switches",
        "rating": -10,
        "keywords": [],
        "abstract": "Optical circuit switching is connection-oriented, being deterministic through the reservation of a complete wavelength channel or spatial path for a certain period. However, this comes at a trade-off against link dynamics, and overall capacity can thus be constrained by the time slot reservations, especially for switches with microsecond- to millisecond-scale reconfiguration times. For data-intensive applications, the communication patterns associated with random data sets typically yield short-lived flows. This situation calls for a new multi-dimensional switching paradigm that fully exploits not only the space and wavelength domains but also with nanosecond-scale reconfigurable capability in the time domain to enable ultra-dynamic links. In this work, we focus on the exploitation of micro-ring resonant phase shifters (RPSs) that are wavelength selective for optical switching in a single plane. By proposing an innovative analytical method with transmission circle chart, we fully unlock the power of RPS with nanosecond-scale reconfigurability and the capability to arbitrarily manipulate its phase and amplitude. Such a compact model offers fresh insights into designs with under and critically coupled RPSs beyond the commonly explored over-coupling condition. This creates not only versatile switch elements but also perfect absorbers for robust multi-wavelength operations. The proposed device can bring about a breakthrough in the optical switching capacity that potentially addresses the challenges faced by modern data center networks, as well as other photonic circuits for high-throughput signal processing.",
        "subjects": [
            "physics.optics"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2403.02906",
        "abstract url": "https://arxiv.org/abs/2403.02906",
        "title": "Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects",
        "rating": -10,
        "keywords": [],
        "abstract": "Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "10 pages, 11 figures, MIDI 2023 conference"
    },
    {
        "paper id": "2403.02912",
        "abstract url": "https://arxiv.org/abs/2403.02912",
        "title": "Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\\varepsilon, \u03b4)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\\log(d)/\\sqrt{n} + \\log(d)/[n\\varepsilon]^{1/2}$ with constant success probability. This result provides evidence of the near-optimality of the approach. Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral setting that is not based on Frank-Wolfe methods. For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $\\sqrt{\\log(d)/n} + \\log(d)^{7/10}/[n\\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $\\sqrt{\\log(d)/n} + \\log(d)/\\sqrt{n\\varepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma, which may be of independent interest.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02929",
        "abstract url": "https://arxiv.org/abs/2403.02929",
        "title": "Loss Design for Single-carrier Joint Communication and Neural Network-based Sensing",
        "rating": -10,
        "keywords": [],
        "abstract": "We evaluate the influence of multi-snapshot sensing and varying signal-to-noise ratio (SNR) on the overall performance of neural network (NN)-based joint communication and sensing (JCAS) systems. To enhance the training behavior, we decouple the loss functions from the respective SNR values and the number of sensing snapshots, using bounds of the sensing performance. Pre-processing is done through conventional sensing signal processing steps on the inputs to the sensing NN. The proposed method outperforms classical algorithms, such as a Neyman-Pearson-based power detector for object detection and ESPRIT for angle of arrival (AoA) estimation for quadrature amplitude modulation (QAM) at low SNRs.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "accepted to WSA2024"
    },
    {
        "paper id": "2403.02934",
        "abstract url": "https://arxiv.org/abs/2403.02934",
        "title": "iSummary: Workload-based, Personalized Summaries for Knowledge Graphs",
        "rating": -10,
        "keywords": [],
        "abstract": "The explosion in the size and the complexity of the available Knowledge Graphs on the web has led to the need for efficient and effective methods for their understanding and exploration. Semantic summaries have recently emerged as methods to quickly explore and understand the contents of various sources. However in most cases they are static not incorporating user needs and preferences and cannot scale. In this paper we present iSummary a novel scalable approach for constructing personalized summaries. As the size and the complexity of the Knowledge Graphs for constructing personalized summaries prohibit efficient summary construction, in our approach we exploit query logs. The main idea behind our approach is to exploit knowledge captured in existing user queries for identifying the most interesting resources and linking them constructing as such highquality personalized summaries. We present an algorithm with theoretical guarantees on the summarys quality linear in the number of queries available in the query log. We evaluate our approach using three realworld datasets and several baselines showing that our approach dominates other methods in terms of both quality and efficiency.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02939",
        "abstract url": "https://arxiv.org/abs/2403.02939",
        "title": "PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers",
        "rating": -10,
        "keywords": [],
        "abstract": "With the rapid growth of scholarly archives, researchers subscribe to \"paper alert\" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "Accepted to CHI 2024"
    },
    {
        "paper id": "2403.02953",
        "abstract url": "https://arxiv.org/abs/2403.02953",
        "title": "Single-level Robust Bidding of Renewable-only Virtual Power Plant in Energy and Ancillary Service Markets for Worst-case Profit",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes a novel single-level robust mathematical approach to model the RES-only Virtual Power Plant (RVPP) bidding problem in the simultaneous Day Ahead Market (DAM) and Secondary Reserve Market (SRM). The worst-case profit of RVPP due to uncertainties related to electricity prices, Non-dispatchable Renewable Energy Sources (ND-RES) production, and flexible demand is captured. In order to find the worst-case profit in a single-level model, the relationship between price and energy uncertainties leads to some non-linear constraints, which are appropriately linearized. The simulation results show the superiority of the proposed robust model compared to those in the literature, as well as its computational efficiency.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02963",
        "abstract url": "https://arxiv.org/abs/2403.02963",
        "title": "Opportunistic User Scheduling for Secure RIS-aided Wireless Communications",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we provide expressions for the secrecy outage probability (SOP) for suboptimal and optimal opportunistic scheduling schemes in a reconfigurable intelligent surface (RIS) aided system with multiple eavesdroppers in approximate closed form. A suboptimal scheduling (SS) scheme is analyzed, which is used when the channel state information (CSI) of the eavesdropping links is unavailable, and the optimal scheduling (OS) scheme is also analyzed, which is used when the global CSI is available. For each scheme, we provide a simplified expression for the SOP in the high signal-to-noise ratio (SNR) regime to demonstrate its behavior as a function of the key system parameters. At high SNR, the SOP saturates to a constant level which decreases exponentially with the number of RIS elements in the SS scheme and with the product of the number of RIS elements and the number of users in the OS scheme. We compare the performance of the opportunistic user scheduling schemes with that of a non-orthogonal multiple access (NOMA) based scheduling scheme which chooses a pair of users in each time slot for scheduling and we show that the opportunistic schemes outperform the NOMA-based scheme. We also derive a closed-form expression for the SOP of a decode-and-forward (DF) relay-aided scheduling scheme in order to compare it with that of the RIS-aided system. It is found that the RIS-aided system outperforms the relay-aided systems when the number of RIS elements is sufficiently large. An increased number of RIS elements is required to outperform the relay-aided system at higher operating frequencies.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02967",
        "abstract url": "https://arxiv.org/abs/2403.02967",
        "title": "Non-Convex Stochastic Composite Optimization with Polyak Momentum",
        "rating": -10,
        "keywords": [],
        "abstract": "The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02971",
        "abstract url": "https://arxiv.org/abs/2403.02971",
        "title": "Space Complexity of Euclidean Clustering",
        "rating": -10,
        "keywords": [],
        "abstract": "The $(k, z)$-Clustering problem in Euclidean space $\\mathbb{R}^d$ has been extensively studied. Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature. However, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\\varepsilon$, remains unclear in existing literature. This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds. Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme. Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\u0398( n d )$ for terminal embedding, where $n$ represents the dataset size. Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "Accepted by SoCG2024"
    },
    {
        "paper id": "2403.02972",
        "abstract url": "https://arxiv.org/abs/2403.02972",
        "title": "Bodioid: philosophical reflections on the hybrid of bodies and artefacts towards post-human",
        "rating": -10,
        "keywords": [],
        "abstract": "The advent of the post-human era has blurred the boundary between the body and artefacts. Further, external materials and information are more deeply integrated into the body, making emerging technology a key driving force for shaping post-human existence and promoting bodily evolution. Based on this, this study analyses the transformation process of three technological forms, namely tools, machines, and cyborgs, and reveals the construction of bodies and artefacts. From the phenomenological perspective, the essences of body and artefact existences are reflected upon, and the 'existence is construction' viewpoint is proposed. Furthermore, a technological design concept, 'bodioid', is proposed to meticulously depict the characteristics of integrating similarities and differences towards unity between the body and artefacts, based on the theoretical foundation of technology mediation and the materialization of morality. Finally, through analogizing the organizational form of language, the two key forms and specific mechanisms of bodioid construction, namely extension and mirroring, are indicated. With this in mind, the post-human existence landscape is discussed with the objective of providing theoretical insights into the study of the underlying philosophical principles of technological design.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02973",
        "abstract url": "https://arxiv.org/abs/2403.02973",
        "title": "Model Predictive Control for setpoint tracking",
        "rating": -10,
        "keywords": [],
        "abstract": "The main objective of tracking control is to steer the tracking error, that is the difference between the reference and the output, to zero while the plant's operation limits are satisfied. This requires that some assumptions on the evolution of the future values of the reference must be taken into account. Typically a simple evolution of the reference is considered, such as step, ramp, or parabolic reference signals. It is important to notice that the tracking problem considers possible variations in the reference to be tracked, such as steps or slope variations of the ramps. Then the tracking control problem is inherently uncertain, since the reference may differ from what is expected. If the value of the reference is changed, then there is no guarantee that the feasibility and stability properties of the resulting control law hold. This report presents the MPC for tracking (MPCT) approach, which ensures recursive feasibility and asymptotic stability of the setpoint when the value of the reference is changed.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02977",
        "abstract url": "https://arxiv.org/abs/2403.02977",
        "title": "Fast Iterative Region Inflation for Computing Large 2-D/3-D Convex Regions of Obstacle-Free Space",
        "rating": -10,
        "keywords": [],
        "abstract": "1) Restrictive Inflation is designed to ensure the managibility of the generated convex polytope. Based on its characteristic of few variables but rich constraints, an efficient and numerically stable solver is designed. 2) A novel method that formulates the MVIE problem into SOCP formulation is proposed, which avoids directly confronting the positive definite constraints and improves the computational efficiency. 3) Especially for 2-D MVIE, a linear-time exact algorithm is introduced for the first time, filling a gap that existed for several decades and further enabling ultra-fast computational performance. 4) Building upon the above methods, a reliable convex polytope generation algorithm FIRI is proposed. Extensive experiments verify its superior comprehensive performance in terms of quality, efficiency, and managibility. High-performance implementation of FIRI will be open-sourced for the reference of the community.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02992",
        "abstract url": "https://arxiv.org/abs/2403.02992",
        "title": "Adaptive Integrate-and-Fire Time Encoding Machine",
        "rating": -10,
        "keywords": [],
        "abstract": "An integrate-and-fire time-encoding machine (IF-TEM) is an effective asynchronous sampler that translates amplitude information into non-uniform time sequences. In this work, we propose a novel Adaptive IF-TEM (AIF-TEM) approach. This design dynamically adjusts the TEM's sensitivity to changes in the input signal's amplitude and frequency in real-time. We provide a comprehensive analysis of AIF-TEM's oversampling and distortion properties. By the adaptive adjustments, AIF-TEM as we show can achieve significant performance improvements in practical finite regime, in terms of sampling rate-distortion. We demonstrate empirically that in the scenarios tested AIF-TEM outperforms classical IF-TEM and traditional Nyquist (i.e., periodic) sampling methods for band-limited signals. In terms of Mean Square Error (MSE), the reduction reaches at least 12dB (fixing the oversampling rate).",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.02996",
        "abstract url": "https://arxiv.org/abs/2403.02996",
        "title": "A Convex Optimization Framework for Computing Robustness Margins of Kalman Filters",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes a novel convex optimization framework for designing robust Kalman filters that guarantee a user-specified steady-state error while maximizing process and sensor noise. The proposed framework simultaneously determines the Kalman gain and the robustness margin in terms of the process and sensor noise. This is the first paper to present such a joint formulation for Kalman filtering. The proposed methodology is validated through two distinct examples: the Clohessy-Wiltshire-Hill equations for a chaser spacecraft in an elliptical orbit and the longitudinal motion model of an F-16 aircraft.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03002",
        "abstract url": "https://arxiv.org/abs/2403.03002",
        "title": "Mem-elements based Neuromorphic Hardware for Neural Network Application",
        "rating": -10,
        "keywords": [],
        "abstract": "The thesis investigates the utilization of memristive and memcapacitive crossbar arrays in low-power machine learning accelerators, offering a comprehensive co-design framework for deep neural networks (DNN). The model, implemented through a hybrid Python and PyTorch approach, accounts for various non-idealities, achieving exceptional training accuracies of 90.02% and 91.03% for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on an 8-layer VGG network. Additionally, the thesis introduces a novel approach to emulate meminductor devices using Operational Transconductance Amplifiers (OTA) and capacitors, showcasing adjustable behavior. Transistor-level simulations in 180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed meminductor emulator's viability with a power consumption of 0.337 mW. The design is further validated in neuromorphic circuits and CNN accelerators, achieving training and testing accuracies of 91.04% and 88.82%, respectively. Notably, the exclusive use of MOS transistors ensures the feasibility of monolithic IC fabrication. This research significantly contributes to the exploration of advanced hardware solutions for efficient and high-performance machine-learning applications.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Master's Thesis"
    },
    {
        "paper id": "2403.03015",
        "abstract url": "https://arxiv.org/abs/2403.03015",
        "title": "Low Complexity Channel Estimation for RIS-Assisted THz Systems with Beam Split",
        "rating": -10,
        "keywords": [],
        "abstract": "To support extremely high data rates, reconfigurable intelligent surface (RIS)-assisted terahertz (THz) communication is considered to be a promising technology for future sixth-generation networks. However, due to the typical employment of hybrid beamforming architecture in THz systems, as well as the passive nature of RIS which lacks the capability to process pilot signals, obtaining channel state information (CSI) is facing significant challenges. To accurately estimate the cascaded channel, we propose a novel low-complexity channel estimation scheme, which includes three steps. Specifically, we first estimate full CSI within a small subset of subcarriers (SCs). Then, we acquire angular information at base station and RIS based on the full CSI. Finally, we derive spatial directions and recover full-CSI for the remaining SCs. Theoretical analysis and simulation results demonstrate that the proposed scheme can achieve superior performance in terms of normalized mean-square-error and exhibit a lower computational complexity compared with the existing algorithms.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03024",
        "abstract url": "https://arxiv.org/abs/2403.03024",
        "title": "Toward Improved Deep Learning-based Vulnerability Detection",
        "rating": -10,
        "keywords": [],
        "abstract": "Deep learning (DL) has been a common thread across several recent techniques for vulnerability detection. The rise of large, publicly available datasets of vulnerabilities has fueled the learning process underpinning these techniques. While these datasets help the DL-based vulnerability detectors, they also constrain these detectors' predictive abilities. Vulnerabilities in these datasets have to be represented in a certain way, e.g., code lines, functions, or program slices within which the vulnerabilities exist. We refer to this representation as a base unit. The detectors learn how base units can be vulnerable and then predict whether other base units are vulnerable. We have hypothesized that this focus on individual base units harms the ability of the detectors to properly detect those vulnerabilities that span multiple base units (or MBU vulnerabilities). For vulnerabilities such as these, a correct detection occurs when all comprising base units are detected as vulnerable. Verifying how existing techniques perform in detecting all parts of a vulnerability is important to establish their effectiveness for other downstream tasks. To evaluate our hypothesis, we conducted a study focusing on three prominent DL-based detectors: ReVeal, DeepWukong, and LineVul. Our study shows that all three detectors contain MBU vulnerabilities in their respective datasets. Further, we observed significant accuracy drops when detecting these types of vulnerabilities. We present our study and a framework that can be used to help DL-based detectors toward the proper inclusion of MBU vulnerabilities.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03030",
        "abstract url": "https://arxiv.org/abs/2403.03030",
        "title": "Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint. By extending Lin-Sontag's universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed. The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios. Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula. The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function. Finally, the essential properties of the unified controllers are analyzed, including smoothness, continuity at the origin, stability margin, and inverse optimality. Simulations validate the approach, showcasing its effectiveness in addressing a challenging stabilizing control problem of a nonlinear system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03032",
        "abstract url": "https://arxiv.org/abs/2403.03032",
        "title": "Logic Programming with Multiplicative Structures",
        "rating": -10,
        "keywords": [],
        "abstract": "In the logic programming paradigm, a program is defined by a set of methods, each of which can be executed when specific conditions are met during the current state of an execution. The semantics of these programs can be elegantly represented using sequent calculi, in which each method is linked to an inference rule. In this context, proof search mirrors the program's execution. Previous works introduced a framework in which the process of constructing proof nets is employed to model executions, as opposed to the traditional approach of proof search in sequent calculus. This paper further extends this investigation by focussing on the pure multiplicative fragment of this framework. We demonstrate, providing practical examples, the capability to define logic programming methods with context-sensitive behaviors solely through specific resource-preserving and context-free operations, corresponding to certain generalized multiplicative connectives explored in existing literature. We show how some of these methods, although still multiplicative, escape the purely multiplicative fragment of Linear Logic (MLL).",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03046",
        "abstract url": "https://arxiv.org/abs/2403.03046",
        "title": "The Exchange Problem",
        "rating": -10,
        "keywords": [],
        "abstract": "Auctions are widely used in exchanges to match buy and sell requests. Once the buyers and sellers place their requests, the exchange determines how these requests are to be matched. The two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing. In this work, we study the algorithmic complexity of the problems arising from these matching tasks. We present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $O(n\\log n)$ time to match $n$ requests. For dynamic price matching, we establish a lower bound of $\u03a9(n \\log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2403.03048",
        "abstract url": "https://arxiv.org/abs/2403.03048",
        "title": "Design of Stochastic Quantizers for Privacy Preservation",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we examine the role of stochastic quantizers for privacy preservation. We first employ a static stochastic quantizer and investigate its corresponding privacy-preserving properties. Specifically, we demonstrate that a sufficiently large quantization step guarantees $(0, \u03b4)$ differential privacy. Additionally, the degradation of control performance caused by quantization is evaluated as the tracking error of output regulation. These two analyses characterize the trade-off between privacy and control performance, determined by the quantization step. This insight enables us to use quantization intentionally as a means to achieve the seemingly conflicting two goals of maintaining control performance and preserving privacy at the same time; towards this end, we further investigate a dynamic stochastic quantizer. Under a stability assumption, the dynamic stochastic quantizer can enhance privacy, more than the static one, while achieving the same control performance. We further handle the unstable case by additionally applying input Gaussian noise.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "11 pages, 4 figures"
    },
    {
        "paper id": "2403.03057",
        "abstract url": "https://arxiv.org/abs/2403.03057",
        "title": "Efficient Interaction-Based Offline Runtime Verification of Distributed Systems with Lifeline Removal",
        "rating": -10,
        "keywords": [],
        "abstract": "Runtime Verification (RV) refers to a family of techniques in which system executions are observed and confronted to formal specifications, with the aim of identifying faults. In Offline RV, observation is done in a first step and verification in a second, on a static artifact collected during observation. In this paper, we define an approach to offline RV of Distributed Systems (DS) against interactions. Interactions are formal models describing communications within a DS. DS are composed of subsystems deployed on different machines and interacting via message passing. Therefore, observing executions of a DS entails logging a collection of local execution traces, one for each subsystem, that we call a multi-trace. A major challenge in analyzing multi-traces is that there are no practical means to synchronize the ends of observations of all local traces. We address this via an operation, called lifeline removal, which we apply on-the-fly on the specification during verification once a local trace has been entirely analyzed. This operation removes from the interaction the specification of actions occurring on the subsystem that is no-longer observed. This may allow further execution of the specification via removing deadlocks due to the partial orders of actions. We prove the correctness of the resulting RV algorithm and introduce two optimization techniques which we also prove correct. We implement a Partial Order Reduction (POR) technique via the selection of a one-unambiguous action (as a unique first step to a linearization) which existence is determined via another use of the lifeline removal operator. Additionally, Local Analyses (LOC) i.e., the verification of local traces, can be leveraged during the global multi-trace analysis to prove failure more quickly. Experiments illustrate the application of our RV approach and the benefits of our optimizations.",
        "subjects": [
            "cs.FL"
        ],
        "comment": "38 pages (31 in the article proper, 1 page of references, 6 pages in appendices), 18 figures, submitted to Science of Computer Programming journal"
    },
    {
        "paper id": "2403.03066",
        "abstract url": "https://arxiv.org/abs/2403.03066",
        "title": "Tracking-in-range Formulations for Numerical Optimal Control",
        "rating": -10,
        "keywords": [],
        "abstract": "In contrast to set-point tracking which aims to reduce the tracking error between the tracker and the reference, tracking-in-range problems only focus on whether the tracker is within a given range around the reference, making it more suitable for the mission specifications of many practical applications. In this work, we present novel optimal control formulations to solve tracking-in-range problems, for both problems requiring the tracker to be always in range, and problems allowing the tracker to go out of range to yield overall better outcomes. As the problem naturally involves discontinuous functions, we present alternative formulations and regularisation strategies to improve the performance of numerical solvers. The extension to in-range tracking with multiple trackers and in-range tracking in high dimensional space are also discussed and illustrated with numerical examples, demonstrating substantial increases in mission duration in comparison to traditional set-point tracking.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03071",
        "abstract url": "https://arxiv.org/abs/2403.03071",
        "title": "On a Neural Implementation of Brenier's Polar Factorization",
        "rating": -10,
        "keywords": [],
        "abstract": "In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of \\citeauthor{Brenier1991PolarFA}'s polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03083",
        "abstract url": "https://arxiv.org/abs/2403.03083",
        "title": "Tooling Offline Runtime Verification against Interaction Models : recognizing sliced behaviors using parameterized simulation",
        "rating": -10,
        "keywords": [],
        "abstract": "Offline runtime verification involves the static analysis of executions of a system against a specification. For distributed systems, it is generally not possible to characterize executions in the form of global traces, given the absence of a global clock. To account for this, we model executions as collections of local traces called multi-traces, with one local trace per group of co-localized actors that share a common clock. Due to the difficulty of synchronizing the start and end of the recordings of local traces, events may be missing at their beginning or end. Considering such partially observed multi-traces is challenging for runtime verification. To that end, we propose an algorithm that verifies the conformity of such traces against formal specifications called Interactions (akin to Message Sequence Charts). It relies on parameterized simulation to reconstitute unobserved behaviors.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "28 double-column pages (25 in the paper proper, 3 additional pages for references and appendices), 19 figures (1 in the appendices), submitted to the Journal of Object Technology"
    },
    {
        "paper id": "2403.03085",
        "abstract url": "https://arxiv.org/abs/2403.03085",
        "title": "A 2-categorical analysis of context comprehension",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider the equivalence between the two main categorical models for the type-theoretical operation of context comprehension, namely P. Dybjer's categories with families and B. Jacobs' comprehension categories, and generalise it to the non-discrete case. The classical equivalence can be summarised in the slogan: \"terms as sections\". By recognising \"terms as coalgebras\", we show how to use the structure-semantics adjunction to prove that a 2-category of comprehension categories is biequivalent to a 2-category of (non-discrete) categories with families. The biequivalence restricts to the classical one proved by Hofmann in the discrete case. It also provides a framework where to compare different morphisms of these structures that have appeared in the literature, varying on the degree of preservation of the relevant structure. We consider in particular morphisms defined by Claraimbault-Dybjer, Jacobs, Larrea, and Uemura.",
        "subjects": [
            "math.CT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03097",
        "abstract url": "https://arxiv.org/abs/2403.03097",
        "title": "Tappy: Predicting Tap Accuracy of User-Interface Elements by Reverse-Engineering Webpage Structures",
        "rating": -10,
        "keywords": [],
        "abstract": "Selecting a UI element is a fundamental operation on webpages, and the ease of tapping a target object has a significant impact on usability. It is thus important to analyze existing UIs in order to design better ones. However, tools proposed in previous studies cannot identify whether an element is tappable on modern webpages. In this study, we developed Tappy that can identify tappable UI elements on webpages and estimate the tap-success rate based on the element size. Our interviews of professional designers and engineers showed that Tappy helped discussions of UI design on the basis of its quantitative metric. Furthermore, we have launched this tool to be freely available to external users, so readers can access Tappy by visiting the website (https://tappy.yahoo.co.jp).",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03104",
        "abstract url": "https://arxiv.org/abs/2403.03104",
        "title": "Low-rank approximated Kalman-Bucy filters using Oja's principal component flow for linear time-invariant systems",
        "rating": -10,
        "keywords": [],
        "abstract": "The Kalman-Bucy filter is widely used in various applications. However, the filter becomes computationally complex under large-scale systems. To address this problem, a low-rank approximated Kalman-Bucy filter consisting of Oja's principal component flow and a low-dimensional Riccati differential equation was proposed. However, the estimation error was established only for linear time-invariant systems with a symmetric system matrix. This study removes restrictions on the symmetricity of the system matrix and reveals the equilibrium points of the Oja flow and their stability for general real square matrices. In addition, the attraction domain for a set of stable equilibrium points is estimated. Based on these results, we demonstrate that the low-rank approximated Kalman-Bucy filter has a bounded estimation error covariance matrix when the system is controllable and observable.",
        "subjects": [
            "math.OC"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2403.03117",
        "abstract url": "https://arxiv.org/abs/2403.03117",
        "title": "Necessary and Sufficient Conditions to the Problem of Input-Output Extension of Internally Controlled Underactuated Nonlinear Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "In this letter, we address the problem of re-targeting a commercial under-actuated robotic system to a higher dimensional output task. Commercial platforms are equipped with an on-board low-level internal controller that provides the user some virtual references inputs and cannot be replaced. Such controller may control in practice only a subset of the total degrees-of-freedom of the system. Our primary objective is to augment these systems by introducing supplementary inputs, attaining increased output capability. Integrating such additional inputs into the control framework introduces a layer of complexity, raising questions about the compatibility and cohesiveness of the overall system. We derive necessary and sufficient conditions under which it is possible to extend the controlled outputs by adding extra inputs when one is forced to keep the internal controller unmodified. We underscore the efficacy and universality of our proposed methodology through the presentation of two relevant examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03126",
        "abstract url": "https://arxiv.org/abs/2403.03126",
        "title": "A Federated Deep Learning Approach for Privacy-Preserving Real-Time Transient Stability Predictions in Power Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Maintaining the privacy of power system data is essential for protecting sensitive information and ensuring the operation security of critical infrastructure. Therefore, the adoption of centralized deep learning (DL) transient stability assessment (TSA) frameworks can introduce risks to electric utilities. This is because these frameworks make utility data susceptible to cyber threats and communication issues when transmitting data to a central server for training a single TSA model. Additionally, the centralized approach demands significant computational resources, which may not always be readily available. In light of these challenges, this paper introduces a federated DL-based TSA framework designed to identify the operating states of the power system. Instead of local utilities transmitting their data to a central server for centralized model training, they independently train their own TSA models using their respective datasets. Subsequently, the parameters of each local TSA model are sent to a central server for model aggregation, and the resulting model is shared back with the local clients. This approach not only preserves the integrity of local utility data, making it resilient against cyber threats but also reduces the computational demands for local TSA model training. The proposed approach is tested on four local clients each having the IEEE 39-bus test system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "2024 International Conference on Smart Grid Synchronized Measurements & Analytics (SGSMA)"
    },
    {
        "paper id": "2403.03168",
        "abstract url": "https://arxiv.org/abs/2403.03168",
        "title": "Learning Explicitly Conditioned Sparsifying Transforms",
        "rating": -10,
        "keywords": [],
        "abstract": "Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.",
        "subjects": [
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03180",
        "abstract url": "https://arxiv.org/abs/2403.03180",
        "title": "Shuffling Momentum Gradient Algorithm for Convex Optimization",
        "rating": -10,
        "keywords": [],
        "abstract": "The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs. Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature.",
        "subjects": [
            "math.OC"
        ],
        "comment": "Vietnam Journal of Mathematics (VJOM), Special issue dedicated to Dr. Tam\u00e1s Terlaky on the occasion of his 70th birthday, 2024"
    },
    {
        "paper id": "2403.03182",
        "abstract url": "https://arxiv.org/abs/2403.03182",
        "title": "On the computation of stable coupled state-space models for dynamic substructuring applications",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper aims at introducing a methodology to compute stable coupled state-space models for dynamic substructuring applications by introducing two novel approaches targeted to accomplish this task: a) a procedure to impose Newtons's second law without relying on the use of undamped RCMs (residual compensation modes) and b) a novel approach to impose stability on unstable coupled state-space models. The enforcement of stability is performed by dividing the unstable model into two different models, one composed by the stable poles (stable model) and the other composed by the unstable ones (unstable model). Then, the poles of the unstable state-space model are forced to be stable, leading to the computation of a stabilized state-space model. Afterwards, to make sure that the Frequency Response Functions (FRFs) of the stabilized model well match the FRFs of the unstable model, the Least-Squares Frequency Domain (LSFD) method is exploited to update the modal parameters of the stabilized model composed by the pairs of complex conjugate poles. The validity of the proposed methodologies is presented and discussed by exploiting experimental data. Indeed, by exploiting the FRFs of a real system, accurate state-space models respecting Newton's second law are computed. Then, decoupling and coupling operations are performed with the identified state-space models, no matter the models resultant from the decoupling/coupling operations are unstable. Stability is then imposed on the computed unstable coupled model by following the approach proposed in this paper. The methodology proved to work well on these data. Moreover, the paper also shows that the coupled state-space models obtained using this methodology are suitable to be exploited in time-domain analyses and simulations.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03193",
        "abstract url": "https://arxiv.org/abs/2403.03193",
        "title": "VeriEQL: Bounded Equivalence Verification for Complex SQL Queries with Integrity Constraints",
        "rating": -10,
        "keywords": [],
        "abstract": "The task of SQL query equivalence checking is important in various real-world applications (including query rewriting and automated grading) that involve complex queries with integrity constraints; yet, state-of-the-art techniques are very limited in their capability of reasoning about complex features (e.g., those that involve sorting, case statement, rich integrity constraints, etc.) in real-life queries. To the best of our knowledge, we propose the first SMT-based approach and its implementation, VeriEQL, capable of proving and disproving bounded equivalence of complex SQL queries. VeriEQL is based on a new logical encoding that models query semantics over symbolic tuples using the theory of integers with uninterpreted functions. It is simple yet highly practical -- our comprehensive evaluation on over 20,000 benchmarks shows that VeriEQL outperforms all state-of-the-art techniques by more than one order of magnitude in terms of the number of benchmarks that can be proved or disproved. VeriEQL can also generate counterexamples that facilitate many downstream tasks (such as finding serious bugs in systems like MySQL and Apache Calcite).",
        "subjects": [
            "cs.PL"
        ],
        "comment": "OOPSLA 2024"
    },
    {
        "paper id": "2403.03208",
        "abstract url": "https://arxiv.org/abs/2403.03208",
        "title": "Active Statistical Inference",
        "rating": -10,
        "keywords": [],
        "abstract": "Inspired by the concept of active learning, we propose active inference$\\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03235",
        "abstract url": "https://arxiv.org/abs/2403.03235",
        "title": "Faithful Dynamic Timing Analysis of Digital Circuits Using Continuous Thresholded Mode-Switched ODEs",
        "rating": -10,
        "keywords": [],
        "abstract": "Thresholded hybrid systems are restricted dynamical systems, where the current mode, and hence the ODE system describing its behavior, is solely determined by externally supplied digital input signals and where the only output signals are digital ones generated by comparing an internal state variable to a threshold value. An attractive feature of such systems is easy composition, which is facilitated by their purely digital interface. A particularly promising application domain of thresholded hybrid systems is digital integrated circuits: Modern digital circuit design considers them as a composition of Millions and even Billions of elementary logic gates, like inverters, GOR and Gand. Since every such logic gate is eventually implemented as an electronic circuit, however, which exhibits a behavior that is governed by some ODE system, thresholded hybrid systems are ideally suited for making the transition from the analog to the digital world rigorous. In this paper, we prove that the mapping from digital input signals to digital output signals is continuous for a large class of thresholded hybrid systems. Moreover, we show that, under some mild conditions regarding causality, this continuity also continues to hold for arbitrary compositions, which in turn guarantees that the composition faithfully captures the analog reality. By applying our generic results to some recently developed thresholded hybrid gate models, both for single-input single-output gates like inverters and for a two-input CMOS NOR gate, we show that they are continuous. Moreover, we provide a novel thresholded hybrid model for the two-input NOR gate, which is not only continuous but also, unlike the existing one, faithfully models all multi-input switching effects.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "There was a missing author from the last submission. arXiv admin note: substantial text overlap with arXiv:2303.14048"
    },
    {
        "paper id": "2403.03240",
        "abstract url": "https://arxiv.org/abs/2403.03240",
        "title": "Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects",
        "rating": -10,
        "keywords": [],
        "abstract": "This study investigates the estimation and the statistical inference about Conditional Average Treatment Effects (CATEs), which have garnered attention as a metric representing individualized causal effects. In our data-generating process, we assume linear models for the outcomes associated with binary treatments and define the CATE as a difference between the expected outcomes of these linear models. This study allows the linear models to be high-dimensional, and our interest lies in consistent estimation and statistical inference for the CATE. In high-dimensional linear regression, one typical approach is to assume sparsity. However, in our study, we do not assume sparsity directly. Instead, we consider sparsity only in the difference of the linear models. We first use a doubly robust estimator to approximate this difference and then regress the difference on covariates with Lasso regularization. Although this regression estimator is consistent for the CATE, we further reduce the bias using the techniques in double/debiased machine learning (DML) and debiased Lasso, leading to $\\sqrt{n}$-consistency and confidence intervals. We refer to the debiased estimator as the triple/debiased Lasso (TDL), applying both DML and debiased Lasso techniques. We confirm the soundness of our proposed method through simulation studies.",
        "subjects": [
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03265",
        "abstract url": "https://arxiv.org/abs/2403.03265",
        "title": "Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline",
        "rating": -10,
        "keywords": [],
        "abstract": "Cyber threats continue to evolve in complexity, thereby traditional Cyber Threat Intelligence (CTI) methods struggle to keep pace. AI offers a potential solution, automating and enhancing various tasks, from data ingestion to resilience verification. This paper explores the potential of integrating Artificial Intelligence (AI) into CTI. We provide a blueprint of an AI-enhanced CTI processing pipeline, and detail its components and functionalities. The pipeline highlights the collaboration of AI and human expertise, which is necessary to produce timely and high-fidelity cyber threat intelligence. We also explore the automated generation of mitigation recommendations, harnessing AI's capabilities to provide real-time, contextual, and predictive insights. However, the integration of AI into CTI is not without challenges. Thereby, we discuss ethical dilemmas, potential biases, and the imperative for transparency in AI-driven decisions. We address the need for data privacy, consent mechanisms, and the potential misuse of technology. Moreover, we highlights the importance of addressing biases both during CTI analysis and AI models warranting their transparency and interpretability. Lastly, our work points out future research directions such as the exploration of advanced AI models to augment cyber defences, and the human-AI collaboration optimization. Ultimately, the fusion of AI with CTI appears to hold significant potential in cybersecurity domain.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03267",
        "abstract url": "https://arxiv.org/abs/2403.03267",
        "title": "TTPXHunter: Actionable Threat Intelligence Extraction as TTPs from Finished Cyber Threat Reports",
        "rating": -10,
        "keywords": [],
        "abstract": "Understanding the modus operandi of adversaries aids organizations in employing efficient defensive strategies and sharing intelligence in the community. This knowledge is often present in unstructured natural language text within threat analysis reports. A translation tool is needed to interpret the modus operandi explained in the sentences of the threat report and translate it into a structured format. This research introduces a methodology named TTPXHunter for the automated extraction of threat intelligence in terms of Tactics, Techniques, and Procedures (TTPs) from finished cyber threat reports. It leverages cyber domain-specific state-of-the-art natural language processing (NLP) to augment sentences for minority class TTPs and refine pinpointing the TTPs in threat analysis reports significantly. The knowledge of threat intelligence in terms of TTPs is essential for comprehensively understanding cyber threats and enhancing detection and mitigation strategies. We create two datasets: an augmented sentence-TTP dataset of 39,296 samples and a 149 real-world cyber threat intelligence report-to-TTP dataset. Further, we evaluate TTPXHunter on the augmented sentence dataset and the cyber threat reports. The TTPXHunter achieves the highest performance of 92.42% f1-score on the augmented dataset, and it also outperforms existing state-of-the-art solutions in TTP extraction by achieving an f1-score of 97.09% when evaluated over the report dataset. TTPXHunter significantly improves cybersecurity threat intelligence by offering quick, actionable insights into attacker behaviors. This advancement automates threat intelligence analysis, providing a crucial tool for cybersecurity professionals fighting cyber threats.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2403.03270",
        "abstract url": "https://arxiv.org/abs/2403.03270",
        "title": "Bi-KVIL: Keypoints-based Visual Imitation Learning of Bimanual Manipulation Tasks",
        "rating": -10,
        "keywords": [],
        "abstract": "Visual imitation learning has achieved impressive progress in learning unimanual manipulation tasks from a small set of visual observations, thanks to the latest advances in computer vision. However, learning bimanual coordination strategies and complex object relations from bimanual visual demonstrations, as well as generalizing them to categorical objects in novel cluttered scenes remain unsolved challenges. In this paper, we extend our previous work on keypoints-based visual imitation learning (\\mbox{K-VIL})~\\cite{gao_kvil_2023} to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called \\emph{Hybrid Master-Slave Relationships} (HMSR) among objects and hands, bimanual coordination strategies, and sub-symbolic task representations. Our bimanual task representation is object-centric, embodiment-independent, and viewpoint-invariant, thus generalizing well to categorical objects in novel scenes. We evaluate our approach in various real-world applications, showcasing its ability to learn fine-grained bimanual manipulation tasks from a small number of human demonstration videos. Videos and source code are available at https://sites.google.com/view/bi-kvil.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03271",
        "abstract url": "https://arxiv.org/abs/2403.03271",
        "title": "Low-Complexity Linear Decoupling of Users for Uplink Massive MU-MIMO Detection",
        "rating": -10,
        "keywords": [],
        "abstract": "Massive MIMO (mMIMO) enables users with different requirements to get connected to the same base station (BS) on the same set of resources. In the uplink of Multiuser massive MIMO (MU-mMIMO), while such heterogeneous users are served, decoupling facilitates the use of user-specific detection schemes. In this paper, we propose a low-complexity linear decoupling scheme called Sequential Decoupler (SD), which aids in the parallel detection of each user's data stream. The proposed algorithm shows significant complexity reduction. Simulations reveal that the complexity of the proposed scheme is only 0.15% of the conventional Singular Value Decomposition (SVD) based decoupling and is about 47% of the pseudo-inverse based decoupling schemes when 80 users with two antennas each are served by the BS. Also, the proposed scheme is scalable when new users are added to the system and requires fewer operations than computing the decoupler all over again. Further numerical analyses indicate that the proposed scheme achieves significant complexity reduction without any degradation in performance and is a promising low-complex alternative to the existing decoupling schemes.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03278",
        "abstract url": "https://arxiv.org/abs/2403.03278",
        "title": "Linear Codes for Hyperdimensional Computing",
        "rating": -10,
        "keywords": [],
        "abstract": "Hyperdimensional Computing (HDC) is an emerging computational paradigm for representing compositional information as high-dimensional vectors, and has a promising potential in applications ranging from machine learning to neuromorphic computing. One of the long-standing challenges in HDC is factoring a compositional representation to its constituent factors, also known as the recovery problem. In this paper we take a novel approach to solve the recovery problem, and propose the use of random linear codes. These codes are subspaces over the Boolean field, and are a well-studied topic in information theory with various applications in digital communication. We begin by showing that hyperdimensional encoding using random linear codes retains favorable properties of the prevalent (ordinary) random codes, and hence HD representations using the two methods have comparable information storage capabilities. We proceed to show that random linear codes offer a rich subcode structure that can be used to form key-value stores, which encapsulate most use cases of HDC. Most importantly, we show that under the framework we develop, random linear codes admit simple recovery algorithms to factor (either bundled or bound) compositional representations. The former relies on constructing certain linear equation systems over the Boolean field, the solution to which reduces the search space dramatically and strictly outperforms exhaustive search in many cases. The latter employs the subspace structure of these codes to achieve provably correct factorization. Both methods are strictly faster than the state-of-the-art resonator networks, often by an order of magnitude. We implemented our techniques in Python using a benchmark software library, and demonstrated promising experimental results.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Author's final version. The article has been accepted for publication in Neural Computation (MIT press)"
    },
    {
        "paper id": "2403.03290",
        "abstract url": "https://arxiv.org/abs/2403.03290",
        "title": "Maintaining Light Spanners via Minimal Updates",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the problem of maintaining a lightweight bounded-degree $(1+\\varepsilon)$-spanner of a dynamic point set in a $d$-dimensional Euclidean space, where $\\varepsilon>0$ and $d$ are arbitrary constants. In our fully-dynamic setting, points are allowed to be inserted as well as deleted, and our objective is to maintain a $(1+\\varepsilon)$-spanner that has constant bounds on its maximum degree and its lightness (the ratio of its weight to that of the minimum spanning tree), while minimizing the recourse, which is the number of edges added or removed by each point insertion or deletion. We present a fully-dynamic algorithm that handles point insertion with amortized constant recourse and point deletion with amortized $O(\\log\u0394)$ recourse, where $\u0394$ is the aspect ratio of the point set.",
        "subjects": [
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03294",
        "abstract url": "https://arxiv.org/abs/2403.03294",
        "title": "Small-Noise Sensitivity Analysis of Locating Pulses in the Presence of Adversarial Perturbation",
        "rating": -10,
        "keywords": [],
        "abstract": "A fundamental small-noise sensitivity analysis of spike localization in the presence of adversarial perturbations and arbitrary point spread function (PSF) is presented. The analysis leverages the local Lipschitz property of the inverse map from measurement noise to parameter estimate. In the small noise regime, the local Lipschitz constant converges to the spectral norm of the noiseless Jacobian of the inverse map. An interpretable upper bound in terms of the minimum separation of spikes, norms, and flatness of the PSF and its derivative, and the distribution of spike amplitudes is provided. Numerical experiments highlighting the relevance of the theoretical bound as a proxy to the local Lipschitz constant and its dependence on the key attributes of the problem are presented.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03297",
        "abstract url": "https://arxiv.org/abs/2403.03297",
        "title": "\"It's the only thing I can trust\": Envisioning Large Language Model Use by Autistic Workers for Communication Assistance",
        "rating": -10,
        "keywords": [],
        "abstract": "Autistic adults often experience stigma and discrimination at work, leading them to seek social communication support from coworkers, friends, and family despite emotional risks. Large language models (LLMs) are increasingly considered an alternative. In this work, we investigate the phenomenon of LLM use by autistic adults at work and explore opportunities and risks of LLMs as a source of social communication advice. We asked 11 autistic participants to present questions about their own workplace-related social difficulties to (1) a GPT-4-based chatbot and (2) a disguised human confederate. Our evaluation shows that participants strongly preferred LLM over confederate interactions. However, a coach specializing in supporting autistic job-seekers raised concerns that the LLM was dispensing questionable advice. We highlight how this divergence in participant and practitioner attitudes reflects existing schisms in HCI on the relative privileging of end-user wants versus normative good and propose design considerations for LLMs to center autistic experiences.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "19 pages, 6 figure, CHI '24 Conference"
    },
    {
        "paper id": "2403.03306",
        "abstract url": "https://arxiv.org/abs/2403.03306",
        "title": "Wrist-bound Guanxi, Jiazu, and Kuolie: Unpacking Chinese Adolescent Smartwatch-Mediated Socialization",
        "rating": -10,
        "keywords": [],
        "abstract": "Adolescent peer relationships, essential for their development, are increasingly mediated by digital technologies. As this trend continues, wearable devices, especially smartwatches tailored for adolescents, are reshaping their socialization. In China, smartwatches like XTC have gained wide popularity, introducing unique features such as \"Bump-to-Connect\" and exclusive social platforms. Nonetheless, how these devices influence adolescents' peer experience remains unknown. Addressing this, we interviewed 18 Chinese adolescents (age: 11 -- 16), discovering a smartwatch-mediated social ecosystem. Our findings highlight the ice-breaking role of smartwatches in friendship initiation and their use for secret messaging with local peers. Within the online smartwatch community, peer status is determined by likes and visibility, leading to diverse pursuit activities (i.e., chu guanxi, jiazu, kuolie) and negative social dynamics. We discuss the core affordances of smartwatches and Chinese cultural factors that influence adolescent social behavior and offer implications for designing future wearables that responsibly and safely support adolescent socialization.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Conditionally Accepted at CHI 2024"
    },
    {
        "paper id": "2403.03312",
        "abstract url": "https://arxiv.org/abs/2403.03312",
        "title": "Beyond the Dashboard: Investigating Distracted Driver Communication Preferences for ADAS",
        "rating": -10,
        "keywords": [],
        "abstract": "Distracted driving is a major cause of road fatalities. With improvements in driver (in)attention detection, these distracted situations can be caught early to alert drivers and improve road safety and comfort. However, drivers may have differing preferences for the modes of such communication based on the driving scenario and their current distraction state. To this end, we present a user study (N=147) where videos of simulated driving scenarios were utilized to learn drivers preferences for modes of communication and their evolution with the drivers changing attention. The survey queried participants preferred modes of communication for scenarios such as collisions or stagnation at a green light. We validate our hypotheses and provide key results that inform the future of communication between drivers and their vehicles. We showcase the different driver preferences based on the nature of the driving scenario and also show that they evolve as the drivers distraction state changes.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "10 pages, 6 figures. All materials associated with the study can be found at https://sites.google.com/illinois.edu/driver-preference-for-modes"
    },
    {
        "paper id": "2403.03314",
        "abstract url": "https://arxiv.org/abs/2403.03314",
        "title": "Collision Avoidance Verification of Multiagent Systems with Learned Policies",
        "rating": -10,
        "keywords": [],
        "abstract": "For many multiagent control problems, neural networks (NNs) have enabled promising new capabilities. However, many of these systems lack formal guarantees (e.g., collision avoidance, robustness), which prevents leveraging these advances in safety-critical settings. While there is recent work on formal verification of NN-controlled systems, most existing techniques cannot handle scenarios with more than one agent. To address this research gap, this paper presents a backward reachability-based approach for verifying the collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs). Given the dynamics models and trained control policies of each agent, the proposed algorithm computes relative backprojection sets by (simultaneously) solving a series of Mixed Integer Linear Programs (MILPs) offline for each pair of agents. We account for state measurement uncertainties, making it well aligned with real-world scenarios. Using those results, the agents can quickly check for collision avoidance online by solving low-dimensional Linear Programs (LPs). We demonstrate the proposed algorithm can verify collision-free properties of a MA-NFL with agents trained to imitate a collision avoidance algorithm (Reciprocal Velocity Obstacles). We further demonstrate the computational scalability of the approach on systems with up to 10 agents.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 6 figures"
    },
    {
        "paper id": "2403.03322",
        "abstract url": "https://arxiv.org/abs/2403.03322",
        "title": "Deep Configuration Performance Learning: A Systematic Survey and Taxonomy",
        "rating": -10,
        "keywords": [],
        "abstract": "Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03323",
        "abstract url": "https://arxiv.org/abs/2403.03323",
        "title": "Automated Software Verification of Hyperliveness",
        "rating": -10,
        "keywords": [],
        "abstract": "Hyperproperties relate multiple executions of a program and are commonly used to specify security and information-flow policies. Most existing work has focused on the verification of $k$-safety properties, i.e., properties that state that all $k$-tuples of execution traces satisfy a given property. In this paper, we study the automated verification of richer properties that combine universal and existential quantification over executions. Concretely, we consider $\\forall^k\\exists^l$ properties, which state that for all $k$ executions, there exist $l$ executions that, together, satisfy a property. This captures important non-$k$-safety requirements, including hyperliveness properties such as generalized non-interference, opacity, refinement, and robustness. We design an automated constraint-based algorithm for the verification of $\\forall^k\\exists^l$ properties. Our algorithm leverages a sound-and-complete program logic and a (parameterized) strongest postcondition computation. We implement our algorithm in a tool called ForEx and report on encouraging experimental results.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "TACAS 2024"
    },
    {
        "paper id": "2403.03330",
        "abstract url": "https://arxiv.org/abs/2403.03330",
        "title": "Assortment Optimization For Conference Goodies With Indifferent Attendees",
        "rating": -10,
        "keywords": [],
        "abstract": "Conferences such as FUN with Algorithms routinely buy goodies (e.g., t-shirts, coffee mugs, etc) for their attendees. Often, said goodies come in different types, varying by color or design, and organizers need to decide how many goodies of each type to buy. We study the problem of buying optimal amounts of each type under a simple model of preferences by the attendees: they are indifferent to the types but want to be able to choose between more than one type of goodies at the time of their arrival. The indifference of attendees suggests that the optimal policy is to buy roughly equal amounts for every goodie type. Despite how intuitive this conjecture sounds, we show that this simple model of assortment optimization is quite rich, and even though we make progress towards proving the conjecture (e.g., we succeed when the number of goodie types is 2 or 3), the general case with K types remains open. We also present asymptotic results and computer simulations, and finally, to motivate further progress, we offer a reward of $100usd for a full proof.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03337",
        "abstract url": "https://arxiv.org/abs/2403.03337",
        "title": "Fine-Grained Privacy Guarantees for Coverage Problems",
        "rating": -10,
        "keywords": [],
        "abstract": "We introduce a new notion of neighboring databases for coverage problems such as Max Cover and Set Cover under differential privacy. In contrast to the standard privacy notion for these problems, which is analogous to node-privacy in graphs, our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy. We illustrate several scenarios of Set Cover and Max Cover where our privacy notion is desired one for the application. Our main result is an $\u03b5$-edge differentially private algorithm for Max Cover which obtains an $(1-1/e-\u03b7,\\tilde{O}(k/\u03b5))$-approximation with high probability. Furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\u03a9(k/\u03b5)$ is necessary under edge-differential privacy. Via group privacy properties, this implies a new algorithm for $\u03b5$-node differentially private Max Cover which obtains an $(1-1/e-\u03b7,\\tilde{O}(fk/\u03b5))$-approximation, where $f$ is the maximum degree of an element in the set system. When $f\\ll k$, this improves over the best known algorithm for Max Cover under pure (node) differential privacy, which obtains an $(1-1/e,\\tilde{O}(k^2/\u03b5))$-approximation.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "14 pages; abstract shortened to fit requirements"
    },
    {
        "paper id": "2403.03344",
        "abstract url": "https://arxiv.org/abs/2403.03344",
        "title": "Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation",
        "rating": -10,
        "keywords": [],
        "abstract": "The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's \"green capacity\", based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03360",
        "abstract url": "https://arxiv.org/abs/2403.03360",
        "title": "Bridge the Future: High-Performance Networks in Confidential VMs without Trusted I/O devices",
        "rating": -10,
        "keywords": [],
        "abstract": "Trusted I/O (TIO) is an appealing solution to improve I/O performance for confidential VMs (CVMs), with the potential to eliminate broad sources of I/O overhead. However, this paper emphasizes that not all types of I/O can derive substantial benefits from TIO, particularly network I/O. Given the obligatory use of encryption protocols for network traffic in CVM's threat model, TIO's approach of I/O encryption over the PCIe bus becomes redundant. Furthermore, TIO solutions need to expand the Trusted Computing Base (TCB) to include TIO devices and are commercially unavailable. Motivated by these insights, the goal of this paper is to propose a software solution that helps CVMs immediately benefit from high-performance networks, while confining trust only to the on-chip CVM. We present FOLIO, a software solution crafted from a secure and efficient Data Plane Development Kit (DPDK) extension compatible with the latest version of AMD Secure Encrypted Virtualization (SEV), a.k.a., Secure Nested Paging (SNP). Our design is informed by a thorough analysis of all possible factors that impact SNP VM's network performance. By extensively removing overhead sources, we arrive at a design that approaches the efficiency of an optimal TIO-based configuration. Evaluation shows that FOLIO has a performance dip less than 6% relative to the optimal TIO configuration, while only relying on off-the-shelf CPUs.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03361",
        "abstract url": "https://arxiv.org/abs/2403.03361",
        "title": "Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper studies the Bayesian regret of a variant of the Thompson-Sampling algorithm for bandit problems. It builds upon the information-theoretic framework of [Russo and Van Roy, 2015] and, more specifically, on the rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a bound with regret rate of $O(d\\sqrt{T \\log(T)})$ for the $d$-dimensional linear bandit setting. We focus on bandit problems with a metric action space and, using a chaining argument, we establish new bounds that depend on the metric entropy of the action space for a variant of Thompson-Sampling. Under suitable continuity assumption of the rewards, our bound offers a tight rate of $O(d\\sqrt{T})$ for $d$-dimensional linear bandit problems.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "15 pages: 8 of main text and 7 of appendices"
    },
    {
        "paper id": "2403.03367",
        "abstract url": "https://arxiv.org/abs/2403.03367",
        "title": "am-AMM: An Auction-Managed Automated Market Maker",
        "rating": -10,
        "keywords": [],
        "abstract": "Automated market makers (AMMs) have emerged as the dominant market mechanism for trading on decentralized exchanges implemented on blockchains. This paper presents a single mechanism that targets two important unsolved problems for AMMs: reducing losses to informed orderflow, and maximizing revenue from uninformed orderflow. The \"auction-managed AMM\" works by running a censorship-resistant onchain auction for the right to temporarily act as \"pool manager\" for a constant-product AMM. The pool manager sets the swap fee rate on the pool, and also receives the accrued fees from swaps. The pool manager can exclusively capture some arbitrage by trading against the pool in response to small price movements, and also can set swap fees incorporating price sensitivity of retail orderflow and adapting to changing market conditions, with the benefits from both ultimately accruing to liquidity providers. Liquidity providers can enter and exit the pool freely in response to changing rent, though they must pay a small fee on withdrawal. We prove that under certain assumptions, this AMM should have higher liquidity in equilibrium than any standard, fixed-fee AMM.",
        "subjects": [
            "q-fin.TR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03377",
        "abstract url": "https://arxiv.org/abs/2403.03377",
        "title": "Junctiond: Extending FaaS Runtimes with Kernel-Bypass",
        "rating": -10,
        "keywords": [],
        "abstract": "This report explores the use of kernel-bypass networking in FaaS runtimes and demonstrates how using Junction, a novel kernel-bypass system, as the backend for executing components in faasd can enhance performance and isolation. Junction achieves this by reducing network and compute overheads and minimizing interactions with the host operating system. Junctiond, the integration of Junction with faasd, reduces median and P99 latency by 37.33% and 63.42%, respectively, and can handle 10 times more throughput while decreasing latency by 2x at the median and 3.5 times at the tail.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03384",
        "abstract url": "https://arxiv.org/abs/2403.03384",
        "title": "A Probabilistic Focalization Approach for Single Receiver Underwater Localization",
        "rating": -10,
        "keywords": [],
        "abstract": "We introduce a Bayesian estimation approach for the passive localization of an acoustic source in shallow water using a single mobile receiver. The proposed probabilistic focalization method estimates the time-varying source location in the presence of measurement-origin uncertainty. In particular, probabilistic data association is performed to match time-differences-of-arrival (TDOA) observations extracted from the acoustic signal to TDOAs predictions provided by the statistical model. The performance of our approach is evaluated using real acoustic data recorded by a single mobile receiver.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03397",
        "abstract url": "https://arxiv.org/abs/2403.03397",
        "title": "Explaining Genetic Programming Trees using Large Language Models",
        "rating": -10,
        "keywords": [],
        "abstract": "Genetic programming (GP) has the potential to generate explainable results, especially when used for dimensionality reduction. In this research, we investigate the potential of leveraging eXplainable AI (XAI) and large language models (LLMs) like ChatGPT to improve the interpretability of GP-based non-linear dimensionality reduction. Our study introduces a novel XAI dashboard named GP4NLDR, the first approach to combine state-of-the-art GP with an LLM-powered chatbot to provide comprehensive, user-centred explanations. We showcase the system's ability to provide intuitive and insightful narratives on high-dimensional data reduction processes through case studies. Our study highlights the importance of prompt engineering in eliciting accurate and pertinent responses from LLMs. We also address important considerations around data privacy, hallucinatory outputs, and the rapid advancements in generative AI. Our findings demonstrate its potential in advancing the explainability of GP algorithms. This opens the door for future research into explaining GP models with LLMs.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03416",
        "abstract url": "https://arxiv.org/abs/2403.03416",
        "title": "On discrete-time polynomial dynamical systems on hypergraphs",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper studies the stability of discrete-time polynomial dynamical systems on hypergraphs by utilizing the Perron-Frobenius theorem for nonnegative tensors with respect to the tensors' Z-eigenvalues and Z-eigenvectors. First of all, for a multilinear polynomial system on a uniform hypergraph, we study the stability of the origin of the corresponding systems. Afterward, we extend our results to non-homogeneous polynomial systems on non-uniform hypergraphs. We confirm that the local stability of any discrete-time polynomial system is in general dominated by pairwise terms. In particular, given the origin is locally stable, we construct a conservative (but explicit) region of attraction from the system parameters. Finally, we validate our results via some numerical examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2401.03652"
    },
    {
        "paper id": "2403.03426",
        "abstract url": "https://arxiv.org/abs/2403.03426",
        "title": "Combined optimization ghost imaging based on random speckle field",
        "rating": -10,
        "keywords": [],
        "abstract": "Ghost imaging is a non local imaging technology, which can obtain target information by measuring the second-order intensity correlation between the reference light field and the target detection light field. However, the current imaging environment requires a large number of measurement data, and the imaging results also have the problems of low image resolution and long reconstruction time. Therefore, using orthogonal methods such as QR decomposition, a variety of optimization methods for speckle patterns are designed combined with Kronecker product,which can help to shorten the imaging time, improve the imaging quality and image noise resistance.",
        "subjects": [
            "physics.optics"
        ],
        "comment": "6 pages, 5 figures"
    },
    {
        "paper id": "2403.03427",
        "abstract url": "https://arxiv.org/abs/2403.03427",
        "title": "Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics",
        "rating": -10,
        "keywords": [],
        "abstract": "Exoplanet discovery at long orbital periods requires reliably detecting individual transits without additional information about the system. Techniques like phase-folding of light curves and periodogram analysis of radial velocity data are more sensitive to planets with shorter orbital periods, leaving a dearth of planet discoveries at long periods. We present a novel technique using an ensemble of Convolutional Neural Networks incorporating the onboard spacecraft diagnostics of \\emph{Kepler} to classify transits within a light curve. We create a pipeline to recover the location of individual transits, and the period of the orbiting planet, which maintains $>80\\%$ transit recovery sensitivity out to an 800-day orbital period. Our neural network pipeline has the potential to discover additional planets in the \\emph{Kepler} dataset, and crucially, within the $\u03b7$-Earth regime. We report our first candidate from this pipeline, KOI 1271.02. KOI 1271.01 is known to exhibit strong Transit Timing Variations (TTVs), and so we jointly model the TTVs and transits of both transiting planets to constrain the orbital configuration and planetary parameters and conclude with a series of potential parameters for KOI 1271.02, as there is not enough data currently to uniquely constrain the system. We conclude that KOI 1271.02 has a radius of 5.32 $\\pm$ 0.20 $R_{\\oplus}$ and a mass of $28.94^{0.23}_{-0.47}$ $M_{\\oplus}$. Future constraints on the nature of KOI 1271.02 require measuring additional TTVs of KOI 1271.01 or observing a second transit of KOI 1271.02.",
        "subjects": [
            "astro-ph.EP"
        ],
        "comment": "23 pages, 23 figures, submitted to AJ"
    },
    {
        "paper id": "2403.03442",
        "abstract url": "https://arxiv.org/abs/2403.03442",
        "title": "CAMASim: A Comprehensive Simulation Framework for Content-Addressable Memory based Accelerators",
        "rating": -10,
        "keywords": [],
        "abstract": "Content addressable memory (CAM) stands out as an efficient hardware solution for memory-intensive search operations by supporting parallel computation in memory. However, developing a CAM-based accelerator architecture that achieves acceptable accuracy, while minimizing hardware cost and catering to both exact and approximate search, still presents a significant challenge especially when considering a broader spectrum of applications. This complexity stems from CAM's rapid evolution across multiple levels--algorithms, architectures, circuits, and underlying devices. This paper introduces CAMASim, a first comprehensive CAM accelerator simulation framework, emphasizing modularity, flexibility, and generality. CAMASim establishes the detailed design space for CAM-based accelerators, incorporates automated functional simulation for accuracy, and enables hardware performance prediction, by leveraging a circuit-level CAM modeling tool. This work streamlines the design space exploration for CAM-based accelerator, aiding researchers in developing effective CAM-based accelerators for various search-intensive applications.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.03449",
        "abstract url": "https://arxiv.org/abs/2403.03449",
        "title": "SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization",
        "rating": -10,
        "keywords": [],
        "abstract": "The voluminous nature of geospatial temporal data from physical monitors and simulation models poses challenges to efficient data access, often resulting in cumbersome temporal selection experiences in web-based data portals. Thus, selecting a subset of time steps for prioritized visualization and pre-loading is highly desirable. Addressing this issue, this paper establishes a multifaceted definition of salient time steps via extensive need-finding studies with domain experts to understand their workflows. Building on this, we propose a novel approach that leverages autoencoders and dynamic programming to facilitate user-driven temporal selections. Structural features, statistical variations, and distance penalties are incorporated to make more flexible selections. User-specified priorities, spatial regions, and aggregations are used to combine different perspectives. We design and implement a web-based interface to enable efficient and context-aware selection of time steps and evaluate its efficacy and usability through case studies, quantitative evaluations, and expert interviews.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI'24), May 11-16, 2024, Honolulu, HI, USA"
    },
    {
        "paper id": "2403.03455",
        "abstract url": "https://arxiv.org/abs/2403.03455",
        "title": "Robust Control Lyapunov-Value Functions for Nonlinear Disturbed Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Control Lyapunov Functions (CLFs) have been extensively used in the control community. A well-known drawback is the absence of a systematic way to construct CLFs for general nonlinear systems, and the problem can become more complex with input or state constraints. Our preliminary work on constructing Control Lyapunov Value Functions (CLVFs) using Hamilton-Jacobi (HJ) reachability analysis provides a method for finding a non-smooth CLF. In this paper, we extend our work on CLVFs to systems with bounded disturbance and define the Robust CLVF (R-CLVF). The R-CLVF naturally inherits all properties of the CLVF; i.e., it first identifies the \"smallest robust control invariant set (SRCIS)\" and stabilizes the system to it with a user-specified exponential rate. The region from which the exponential rate can be met is called the \"region of exponential stabilizability (ROES).\" We provide clearer definitions of the SRCIS and more rigorous proofs of several important theorems. Since the computation of the R-CLVF suffers from the \"curse of dimensionality,\" we also provide two techniques (warmstart and system decomposition) that solve it, along with necessary proofs. Three numerical examples are provided, validating our definition of SRCIS, illustrating the trade-off between a faster decay rate and a smaller ROES, and demonstrating the efficiency of computation using warmstart and decomposition.",
        "subjects": [
            "math.OC"
        ],
        "comment": "13 pages, 5 figures"
    },
    {
        "paper id": "2403.10540",
        "abstract url": "https://arxiv.org/abs/2403.10540",
        "title": "A Hybrid Delay Model for Interconnected Multi-Input Gates",
        "rating": -10,
        "keywords": [],
        "abstract": "Dynamic digital timing analysis aims at substituting highly accurate but slow analog simulations of digital circuits with less accurate but fast digital approaches to facilitate tracing timing relations between individual transitions in a signal trace. This primarily requires gate delay models, where the input-to-output delay of a transition also depends on the signal history. We focus on a recently proposed hybrid delay model for CMOS multi-input gates, exemplified by a 2-input \\NOR\\ gate, which is the only delay model known to us that faithfully captures both single-input switching (SIS) and multi-input switching (MIS) effects, also known as ``Charlie effects''. Despite its simplicity as a first-order model, simulations have revealed that suitably parametrized versions of the model predict the actual delays of NOR gates accurately. However, the approach considers isolated gates without their interconnect. In this work, we augment the existing model and its theoretical analysis by a first-order interconnect, and conduct a systematic evaluation of the resulting modeling accuracy: Using SPICE simulations, we study both SIS and MIS effects on the overall delay of \\NOR\\ gates under variation of input driving strength, wire length, load capacitance and CMOS technology, and compare it to the predictions of appropriately parametrized versions of our model. Overall, our results reveal a surprisingly good accuracy of our fast delay model.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.12994",
        "abstract url": "https://arxiv.org/abs/2403.12994",
        "title": "Fast Iterative Configuration of Reconfigurable Intelligent Surfaces in mmWave Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Reconfigurable intelligent surfaces (RISs) are a promising solution to improve the coverage of cellular networks, thanks to their ability to steer impinging signals in desired directions. However, they introduce an overhead in the communication process since the optimal configuration of a RIS depends on the channels to and from the RIS, which must be estimated. In this paper, we propose a novel fast iterative configuration (FIC) protocol to determine the optimal RIS configuration that exploits the small number of paths of millimetre-wave (mmWave) channels and an adaptive choice of the explored RIS configurations. In particular, we split the elements of the RIS into a number of subsets equal to the number of channel taps. For each subset, then an iterative procedure finds at each iteration the optimal RIS configuration in a codebook exploring a two-dimensional grid of possible angles of arrival and departure of the path at the RIS. Over the iterations, the grid is made finer around the point identified in previous iterations. Numerical results obtained using an urban channel model confirm that the proposed solution is fast and provides a configuration close to the optimal in a shorter time than other existing approaches.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.12995",
        "abstract url": "https://arxiv.org/abs/2403.12995",
        "title": "Multi-Scale Protein Language Model for Unified Molecular Modeling",
        "rating": -10,
        "keywords": [],
        "abstract": "Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ms-ESM (multi-scale ESM), a novel approach that enables multi-scale unified molecular modeling. ms-ESM achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ms-ESM not only gains molecular knowledge but also retains its understanding of proteins.",
        "subjects": [
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2404.04262",
        "abstract url": "https://arxiv.org/abs/2404.04262",
        "title": "The Future of MEV",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper analyzes the Execution Tickets proposal on Ethereum Research, unveiling its potential to revolutionize the Ethereum blockchain's economic model. At the core of this proposal lies a novel ticketing mechanism poised to redefine how the Ethereum protocol distributes the value associated with proposing execution payloads. This innovative approach enables the Ethereum protocol to directly broker Maximal Extractable Value (MEV), traditionally an external revenue stream for validators. The implementation of Execution Tickets goes beyond optimizing validator compensation; it also introduces a new Ethereum native asset with a market capitalization expected to correlate closely with the present value of all value associated with future block production. The analysis demonstrates that the Execution Ticket system can facilitate a more equitable distribution of value within the Ethereum ecosystem, and pave the way for a more secure and economically robust blockchain network.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "15 pages"
    }
]