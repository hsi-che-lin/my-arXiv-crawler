[
    {
        "paper id": "2403.17589",
        "abstract url": "https://arxiv.org/abs/2403.17589",
        "title": "Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models",
        "rating": 2.5,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set. This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data. The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers. Our approach is tested across 11 datasets under the three task settings. Remarkably, in the zero-shot scenario, it outperforms existing methods by over 3\\% and even shows superior results against methods utilizing external training data. Additionally, our method exhibits robust performance against natural distribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"
    },
    {
        "paper id": "2403.17536",
        "abstract url": "https://arxiv.org/abs/2403.17536",
        "title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler",
        "rating": 2,
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks. This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1--32.2 percentage points. Additionally, our in-depth ablation study demonstrates that parameter-efficient fine-tuning requires less than 6% of training data to yield comparable performance with traditional full-weight fine-tuning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.17553",
        "abstract url": "https://arxiv.org/abs/2403.17553",
        "title": "RuBia: A Russian Language Bias Detection Dataset",
        "rating": 2,
        "keywords": [
            [
                "social biases"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Warning: this work contains upsetting or disturbing content. Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data. To test if an LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourcing workers. Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To illustrate the dataset's purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2403.17556",
        "abstract url": "https://arxiv.org/abs/2403.17556",
        "title": "m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt",
        "rating": 2,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results show that m3P outperforms previous text-only baselines and multilingual multimodal methods by a large margin. Furthermore, the probing experiments validate the effectiveness of our method in enhancing translation under the low-resource and massively multilingual scenario.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "COLING 2024"
    },
    {
        "paper id": "2403.17839",
        "abstract url": "https://arxiv.org/abs/2403.17839",
        "title": "ReMamber: Referring Image Segmentation with Mamba Twister",
        "rating": 2,
        "keywords": [
            [
                "visual-language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough analyses of ReMamber and discuss other fusion designs using Mamba. These provide valuable perspectives for future research.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17887",
        "abstract url": "https://arxiv.org/abs/2403.17887",
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
        "rating": 2,
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient finetuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 + 10 pages, 5 + 4 figures"
    },
    {
        "paper id": "2403.17926",
        "abstract url": "https://arxiv.org/abs/2403.17926",
        "title": "FastCAR: Fast Classification And Regression Multi-Task Learning via Task Consolidation for Modelling a Continuous Property Variable of Object Classes",
        "rating": 2,
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL) for a classification and a regression task, despite task heterogeneity with only subtle correlation. It addresses object classification and continuous property variable regression, a crucial use case in science and engineering. FastCAR involves a labeling transformation approach that can be used with a single-task regression network architecture. FastCAR outperforms traditional MTL model families, parametrized in the landscape of architecture and loss weighting schemes, when learning of both tasks are collectively considered (classification accuracy of 99.54%, regression mean absolute percentage error of 2.3%). The experiments performed used an Advanced Steel Property dataset contributed by us. The dataset comprises 4536 images of 224x224 pixels, annotated with object classes and hardness properties that take continuous values. With the labeling transformation and single-task regression network architecture, FastCAR achieves reduced latency and time efficiency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17937",
        "abstract url": "https://arxiv.org/abs/2403.17937",
        "title": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory",
        "rating": 2,
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, transformer-based approaches have shown promising results for semi-supervised video object segmentation. However, these approaches typically struggle on long videos due to increased GPU memory demands, as they frequently expand the memory bank every few frames. We propose a transformer-based approach, named MAVOS, that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion. The proposed MCA effectively encodes both local and global features at various levels of granularity while efficiently maintaining consistent speed regardless of the video length. Extensive experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017, demonstrate the effectiveness of our proposed contributions leading to real-time inference and markedly reduced memory demands without any degradation in segmentation accuracy on long videos. Compared to the best existing transformer-based approach, our MAVOS increases the speed by 7.6x, while significantly reducing the GPU memory by 87% with comparable segmentation performance on short and long video datasets. Notably on the LVOS dataset, our MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second (FPS) on a single V100 GPU. Our code and models will be publicly available at: https://github.com/Amshaker/MAVOS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18193",
        "abstract url": "https://arxiv.org/abs/2403.18193",
        "title": "Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking",
        "rating": 2,
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "RGB-T tracking, a vital downstream task of object tracking, has made remarkable progress in recent years. Yet, it remains hindered by two major challenges: 1) the trade-off between performance and efficiency; 2) the scarcity of training data. To address the latter challenge, some recent methods employ prompts to fine-tune pre-trained RGB tracking models and leverage upstream knowledge in a parameter-efficient manner. However, these methods inadequately explore modality-independent patterns and disregard the dynamic reliability of different modalities in open scenarios. We propose M3PT, a novel RGB-T prompt tracking method that leverages middle fusion and multi-modal and multi-stage visual prompts to overcome these challenges. We pioneer the use of the middle fusion framework for RGB-T tracking, which achieves a balance between performance and efficiency. Furthermore, we incorporate the pre-trained RGB tracking model into the framework and utilize multiple flexible prompt strategies to adapt the pre-trained model to the comprehensive exploration of uni-modal patterns and the improved modeling of fusion-modal features, harnessing the potential of prompt learning in RGB-T tracking. Our method outperforms the state-of-the-art methods on four challenging benchmarks, while attaining 46.1 fps inference speed.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17409",
        "abstract url": "https://arxiv.org/abs/2403.17409",
        "title": "Neural Clustering based Visual Representation Learning",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting clustering, one of the most classic approaches in machine learning and data analysis. Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions. Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution. In this work, we propose feature extraction with clustering (FEC), a conceptually elegant yet surprisingly ad-hoc interpretable neural clustering framework, which views feature extraction as a process of selecting representatives from data and thus automatically captures the underlying data distribution. Given an image, FEC alternates between grouping pixels into individual clusters to abstract representatives and updating the deep features of pixels with current representatives. Such an iterative working mechanism is implemented in the form of several neural layers and the final representatives can be used for downstream tasks. The cluster assignments across layers, which can be viewed and inspected by humans, make the forward process of FEC fully transparent and empower it with promising ad-hoc interpretability. Extensive experiments on various visual recognition models and tasks verify the effectiveness, generality, and interpretability of FEC. We expect this work will provoke a rethink of the current de facto grid-style paradigm.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024. Code: https://github.com/guikunchen/FEC/"
    },
    {
        "paper id": "2403.17420",
        "abstract url": "https://arxiv.org/abs/2403.17420",
        "title": "Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The goal of the multi-sound source localization task is to localize sound sources from the mixture individually. While recent multi-sound source localization methods have shown improved performance, they face challenges due to their reliance on prior information about the number of objects to be separated. In this paper, to overcome this limitation, we present a novel multi-sound source localization method that can perform localization without prior knowledge of the number of sound sources. To achieve this goal, we propose an iterative object identification (IOI) module, which can recognize sound-making objects in an iterative manner. After finding the regions of sound-making objects, we devise object similarity-aware clustering (OSC) loss to guide the IOI module to effectively combine regions of the same object but also distinguish between different objects and backgrounds. It enables our method to perform accurate localization of sound-making objects without any prior knowledge. Extensive experimental results on the MUSIC and VGGSound benchmarks show the significant performance improvements of the proposed method over the existing methods for both single and multi-source. Our code is available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2403.17683",
        "abstract url": "https://arxiv.org/abs/2403.17683",
        "title": "Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI",
        "rating": 1.5,
        "keywords": [
            [
                "VLM"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person's emotion through an artistic work with a comment. The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures. The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem. In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of multimodal models and a well-designed prompt to reduce cultural differences problem. To clarify, our approach contains two main blocks: (1)XLM-R\\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific prompt. Our approach ranked first in the final test with a score of 0.627.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17709",
        "abstract url": "https://arxiv.org/abs/2403.17709",
        "title": "Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Visual Relationship Detection (VRD) has seen significant advancements with Transformer-based architectures recently. However, we identify two key limitations in a conventional label assignment for training Transformer-based VRD models, which is a process of mapping a ground-truth (GT) to a prediction. Under the conventional assignment, an unspecialized query is trained since a query is expected to detect every relation, which makes it difficult for a query to specialize in specific relations. Furthermore, a query is also insufficiently trained since a GT is assigned only to a single prediction, therefore near-correct or even correct predictions are suppressed by being assigned no relation as a GT. To address these issues, we propose Groupwise Query Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise Query Specialization trains a specialized query by dividing queries and relations into disjoint groups and directing a query in a specific query group solely toward relations in the corresponding relation group. Quality-Aware Multi-Assignment further facilitates the training by assigning a GT to multiple predictions that are significantly close to a GT in terms of a subject, an object, and the relation in between. Experimental results and analyses show that SpeaQ effectively trains specialized queries, which better utilize the capacity of a model, resulting in consistent performance gains with zero additional inference cost across multiple VRD models and benchmarks. Code is available at https://github.com/mlvlab/SpeaQ.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.17749",
        "abstract url": "https://arxiv.org/abs/2403.17749",
        "title": "Multi-Task Dense Prediction via Mixture of Low-Rank Experts",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Previous multi-task dense prediction methods based on the Mixture of Experts (MoE) have received great performance but they neglect the importance of explicitly modeling the global relations among all tasks. In this paper, we present a novel decoder-focused method for multi-task dense prediction, called Mixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships, MLoRE adds a generic convolution path to the original MoE structure, where each task feature can go through this path for explicit parameter sharing. Furthermore, to control the parameters and computational cost brought by the increase in the number of experts, we take inspiration from LoRA and propose to leverage the low-rank format of a vanilla convolution in the expert network. Since the low-rank experts have fewer parameters and can be dynamically parameterized into the generic convolution, the parameters and computational cost do not change much with the increase of experts. Benefiting from this design, we increase the number of experts and its reception field to enlarge the representation capacity, facilitating multiple dense tasks learning in a unified network. Extensive experiments on the PASCAL-Context and NYUD-v2 benchmarks show that our MLoRE achieves superior performance compared to previous state-of-the-art methods on all metrics. Our code is available at https://github.com/YuqiYang213/MLoRE.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2403.17919",
        "abstract url": "https://arxiv.org/abs/2403.17919",
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "rating": 1.5,
        "keywords": [
            [
                "Parameter Efficient",
                "Efficient Fine-Tuning",
                "GPU memory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17935",
        "abstract url": "https://arxiv.org/abs/2403.17935",
        "title": "OmniVid: A Generative Framework for Universal Video Understanding",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive training corpora. Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. In this way, a variety of video tasks could be formulated as video-grounded token generation. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video benchmarks, providing a novel perspective for more universal video understanding. Code is available at https://github.com/wangjk666/OmniVid.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2403.17998",
        "abstract url": "https://arxiv.org/abs/2403.17998",
        "title": "Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The increasing prevalence of video clips has sparked growing interest in text-video retrieval. Recent advances focus on establishing a joint embedding space for text and video, relying on consistent embedding representations to compute similarity. However, the text content in existing datasets is generally short and concise, making it hard to fully describe the redundant semantics of a video. Correspondingly, a single text embedding may be less expressive to capture the video embedding and empower the retrieval. In this study, we propose a new stochastic text modeling method T-MASS, i.e., text is modeled as a stochastic embedding, to enrich text embedding with a flexible and resilient semantic range, yielding a text mass. To be specific, we introduce a similarity-aware radius module to adapt the scale of the text mass upon the given text-video pairs. Plus, we design and develop a support text regularization to further control the text mass during the training. The inference pipeline is also tailored to fully exploit the text mass for accurate retrieval. Empirical evidence suggests that T-MASS not only effectively attracts relevant text-video pairs while distancing irrelevant ones, but also enables the determination of precise text embeddings for relevant pairs. Our experimental results show a substantial improvement of T-MASS over baseline (3% to 6.3% by R@1). Also, T-MASS achieves state-of-the-art performance on five benchmark datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024, code and model are available at https://github.com/Jiamian-Wang/T-MASS-text-video-retrieval"
    },
    {
        "paper id": "2403.18092",
        "abstract url": "https://arxiv.org/abs/2403.18092",
        "title": "OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The scarcity of ground-truth labels poses one major challenge in developing optical flow estimation models that are both generalizable and robust. While current methods rely on data augmentation, they have yet to fully exploit the rich information available in labeled video sequences. We propose OCAI, a method that supports robust frame interpolation by generating intermediate video frames alongside optical flows in between. Utilizing a forward warping approach, OCAI employs occlusion awareness to resolve ambiguities in pixel values and fills in missing values by leveraging the forward-backward consistency of optical flows. Additionally, we introduce a teacher-student style semi-supervised learning method on top of the interpolated frames. Using a pair of unlabeled frames and the teacher model's predicted optical flow, we generate interpolated frames and flows to train a student model. The teacher's weights are maintained using Exponential Moving Averaging of the student. Our evaluations demonstrate perceptually superior interpolation quality and enhanced optical flow accuracy on established benchmarks such as Sintel and KITTI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.18125",
        "abstract url": "https://arxiv.org/abs/2403.18125",
        "title": "For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "workshop",
                "AAAI"
            ]
        ],
        "abstract": "While the rise of large language models (LLMs) has created rich new opportunities to learn about digital technology, many on the margins of this technology struggle to gain and maintain competency due to lexical or conceptual barriers that prevent them from asking appropriate questions. Although there have been many efforts to understand factuality of LLM-created content and ability of LLMs to answer questions, it is not well understood how unclear or nonstandard language queries affect the model outputs. We propose the creation of a dataset that captures questions of digital newcomers and outsiders, utilizing data we have compiled from a decade's worth of one-on-one tutoring. In this paper we lay out our planned efforts and some potential uses of this dataset.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Presented at the AI4ED workshop at AAAI 2024"
    },
    {
        "paper id": "2403.17385",
        "abstract url": "https://arxiv.org/abs/2403.17385",
        "title": "ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as ''One Sense Per Discourse'', using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also achieves over 75% of the performance of a strong, fully supervised model trained on gold data. Our code is available at: https://github.com/hriaz17/ELLEN.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2403.17402",
        "abstract url": "https://arxiv.org/abs/2403.17402",
        "title": "Infrastructure-less Localization from Indoor Environmental Sounds Based on Spectral Decomposition and Spatial Likelihood Model",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Human and/or asset tracking using an attached sensor units helps understand their activities. Most common indoor localization methods for human tracking technologies require expensive infrastructures, deployment and maintenance. To overcome this problem, environmental sounds have been used for infrastructure-free localization. While they achieve room-level classification, they suffer from two problems: low signal-to-noise-ratio (SNR) condition and non-uniqueness of sound over the coverage area. A microphone localization method was proposed using supervised spectral decomposition and spatial likelihood to solve these problems. The proposed method was evaluated with actual recordings in an experimental room with a size of 12 x 30 m. The results showed that the proposed method with supervised NMF was robust under low-SNR condition compared to a simple feature (mel frequency cepstrum coefficient: MFCC). Additionally, the proposed method could be easily integrated with prior distribution, which is available from other Bayesian localizations. The proposed method can be used to evaluate the spatial likelihood from environmental sounds.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "6 pages, 6 figures, accepted to IEEE/SICE SII 2023"
    },
    {
        "paper id": "2403.17407",
        "abstract url": "https://arxiv.org/abs/2403.17407",
        "title": "Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or \"district\" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each district. The DGT technique is applied to fine-tune several transformer-based models, on this new dataset. Experimental results demonstrate the effectiveness of DGT, with the ByT5 model achieving superior performance over word-based models like mT5, BanglaT5, and umT5. This is attributed to ByT5's ability to handle a high percentage of out-of-vocabulary words in the test set. The proposed approach highlights the importance of incorporating regional dialect information into ubiquitous natural language processing systems for languages with diverse phonological variations. The following work was a result of the \"Bhashamul\" challenge, which is dedicated to solving the problem of Bengali text with regional dialects to IPA transcription https://www.kaggle.com/competitions/regipa/. The training and inference notebooks are available through the competition link.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Updated missing references to the dataset and corrected some sentences in sections 1 and 2. This work became the champion of the Bhashamul challenge"
    },
    {
        "paper id": "2403.17411",
        "abstract url": "https://arxiv.org/abs/2403.17411",
        "title": "PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Prompt compression is an innovative method for efficiently condensing input prompts while preserving essential information. To facilitate quick-start services, user-friendly interfaces, and compatibility with common datasets and metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is a unified plug-and-play solution for compressing prompts in Large Language Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and metrics for comprehensive performance evaluation. PCToolkit boasts a modular design, allowing for easy integration of new datasets and metrics through portable and user-friendly interfaces. In this paper, we outline the key components and functionalities of PCToolkit. We conducted evaluations of the compressors within PCToolkit across various natural language tasks, including reconstruction, summarization, mathematical problem-solving, question answering, few-shot learning, synthetic tasks, code completion, boolean expressions, multiple choice questions, and lies recognition.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "For open-source repository, see https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression"
    },
    {
        "paper id": "2403.17486",
        "abstract url": "https://arxiv.org/abs/2403.17486",
        "title": "KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results. However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods' overall performance. In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective. Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative. Experimental results on widely used Semantic Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024"
    },
    {
        "paper id": "2403.17497",
        "abstract url": "https://arxiv.org/abs/2403.17497",
        "title": "Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players' assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement -- which invites further research in the direction of cost-sharing in collaborative interactions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.17503",
        "abstract url": "https://arxiv.org/abs/2403.17503",
        "title": "DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Class-incremental learning (CIL) under an exemplar-free constraint has presented a significant challenge. Existing methods adhering to this constraint are prone to catastrophic forgetting, far more so than replay-based techniques that retain access to past samples. In this paper, to solve the exemplar-free CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The DS-AL contains a main stream offering an analytical (i.e., closed-form) linear solution, and a compensation stream improving the inherent under-fitting limitation due to adopting linear mapping. The main stream redefines the CIL problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an equivalence between the CIL and its joint-learning counterpart. The compensation stream is governed by a Dual-Activation Compensation (DAC) module. This module re-activates the embedding with a different activation function from the main stream one, and seeks fitting compensation by projecting the embedding to the null space of the main stream's linear mapping. Empirical results demonstrate that the DS-AL, despite being an exemplar-free technique, delivers performance comparable with or better than that of replay-based methods across various datasets, including CIFAR-100, ImageNet-100 and ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to execute CIL in a phase-invariant manner. This is evidenced by a never-before-seen 500-phase CIL ImageNet task, which performs on a level identical to a 5-phase one. Our codes are available at https://github.com/ZHUANGHP/Analytic-continual-learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted in AAAI 2024"
    },
    {
        "paper id": "2403.17508",
        "abstract url": "https://arxiv.org/abs/2403.17508",
        "title": "Correlation of Fr\u00e9chet Audio Distance With Human Perception of Environmental Audio Is Embedding Dependant",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "This paper explores whether considering alternative domain-specific embeddings to calculate the Fr\u00e9chet Audio Distance (FAD) metric can help the FAD to correlate better with perceptual ratings of environmental sounds. We used embeddings from VGGish, PANNs, MS-CLAP, L-CLAP, and MERT, which are tailored for either music or environmental sound evaluation. The FAD scores were calculated for sounds from the DCASE 2023 Task 7 dataset. Using perceptual data from the same task, we find that PANNs-WGM-LogMel produces the best correlation between FAD scores and perceptual ratings of both audio quality and perceived fit with a Spearman correlation higher than 0.5. We also find that music-specific embeddings resulted in significantly lower results. Interestingly, VGGish, the embedding used for the original Fr\u00e9chet calculation, yielded a correlation below 0.1. These results underscore the critical importance of the choice of embedding for the FAD metric design.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17514",
        "abstract url": "https://arxiv.org/abs/2403.17514",
        "title": "Speaker Distance Estimation in Enclosures from Single-Channel Audio",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Distance estimation from audio plays a crucial role in various applications, such as acoustic scene analysis, sound source localization, and room modeling. Most studies predominantly center on employing a classification approach, where distances are discretized into distinct categories, enabling smoother model training and achieving higher accuracy but imposing restrictions on the precision of the obtained sound source position. Towards this direction, in this paper we propose a novel approach for continuous distance estimation from audio signals using a convolutional recurrent neural network with an attention module. The attention mechanism enables the model to focus on relevant temporal and spectral features, enhancing its ability to capture fine-grained distance-related information. To evaluate the effectiveness of our proposed method, we conduct extensive experiments using audio recordings in controlled environments with three levels of realism (synthetic room impulse response, measured response with convolved speech, and real recordings) on four datasets (our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental results show that the model achieves an absolute error of 0.11 meters in a noiseless synthetic scenario. Moreover, the results showed an absolute error of about 1.30 meters in the hybrid scenario. The algorithm's performance in the real scenario, where unpredictable environmental factors and noise are prevalent, yields an absolute error of approximately 0.50 meters. For reproducible research purposes we make model, code, and synthetic datasets available at https://github.com/michaelneri/audio-distance-estimation.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
        "paper id": "2403.17516",
        "abstract url": "https://arxiv.org/abs/2403.17516",
        "title": "MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and highlight a critical correlation: the more precisely we map brain activities to text embeddings, the better the text reconstruction results. Such insight can simplify the task of reconstructing language from brain activities for future work, emphasizing the importance of improving brain-to-text-embedding mapping techniques.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024 main conference"
    },
    {
        "paper id": "2403.17520",
        "abstract url": "https://arxiv.org/abs/2403.17520",
        "title": "Boosting Adversarial Training via Fisher-Rao Norm-based Regularization",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks. Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training. Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training process. Building upon this observation, we propose a novel regularization framework, called Logit-Oriented Adversarial Training (LOAT), which can mitigate the trade-off between robustness and accuracy while imposing only a negligible increase in computational overhead. Our extensive experiments demonstrate that the proposed regularization strategy can boost the performance of the prevalent adversarial training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT, across various network architectures. Our code will be available at https://github.com/TrustAI/LOAT.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This paper has been accepted to CVPR2024"
    },
    {
        "paper id": "2403.17528",
        "abstract url": "https://arxiv.org/abs/2403.17528",
        "title": "Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters. We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly noteworthy that languages with fewer resources or those with less linguistic similarity to English benefited more from the parameter increase. Our model is available at https://huggingface.co/pkshatech/m-ST5.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted in LREC-COLING 2024"
    },
    {
        "paper id": "2403.17545",
        "abstract url": "https://arxiv.org/abs/2403.17545",
        "title": "A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Situated conversations, which refer to visual information as visual question answering (VQA), often contain ambiguities caused by reliance on directive information. This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms. Such ambiguities in questions are often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information. In this study, we propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information by focusing on a clarification process complemented by gaze information. We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks. Our experimental results showed that the proposed method improved the performance in some cases of a VQA system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024"
    },
    {
        "paper id": "2403.17552",
        "abstract url": "https://arxiv.org/abs/2403.17552",
        "title": "Naive Bayes-based Context Extension for Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM's maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes' theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to main NAACL 2024"
    },
    {
        "paper id": "2403.17562",
        "abstract url": "https://arxiv.org/abs/2403.17562",
        "title": "Deep functional multiple index models with an application to SER",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "Speech Emotion Recognition (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities. We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model. Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework. Simulations for this new model show good performances. This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the effectiveness of our approach on the benchmark IEMOCAP database, achieving good performance compared to existing methods.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "5 pages, 1 figure"
    },
    {
        "paper id": "2403.17564",
        "abstract url": "https://arxiv.org/abs/2403.17564",
        "title": "Task-Oriented Paraphrase Analytics",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Since paraphrasing is an ill-defined task, the term \"paraphrasing\" covers text transformation tasks with different characteristics. Consequently, existing paraphrasing studies have applied quite different (explicit and implicit) criteria as to when a pair of texts is to be considered a paraphrase, all of which amount to postulating a certain level of semantic or lexical similarity. In this paper, we conduct a literature review and propose a taxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using classifiers trained to identify the tasks that a given paraphrasing instance fits, we find that the distributions of task-specific instances in the known paraphrase corpora vary substantially. This means that the use of these corpora, without the respective paraphrase conditions being clearly defined (which is the normal case), must lead to incomparable and misleading results.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.17608",
        "abstract url": "https://arxiv.org/abs/2403.17608",
        "title": "Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and Swin-T detectors on the GenImage dataset, achieving state-of-the-art results. We provide the dataset and source codes of this paper on the anonymous website: https://www.unbiased-genimage.org",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17611",
        "abstract url": "https://arxiv.org/abs/2403.17611",
        "title": "Denoising Table-Text Retrieval for Open-Domain Question Answering",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions. Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table. To encode this ranking information, we fine-tune a rank-aware column encoder to identify minimum and maximum values within a column. Experimental results demonstrate that DoTTeR significantly outperforms strong baselines on both retrieval recall and downstream QA tasks. Our code is available at https://github.com/deokhk/DoTTeR.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2403.17612",
        "abstract url": "https://arxiv.org/abs/2403.17612",
        "title": "\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Labeling corpora constitutes a bottleneck to create models for new tasks or domains. Large language models mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments. Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best-worst scaling. This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks. To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best-worst scaling. We find that the latter shows the highest reliability. A transformer regressor fine-tuned on these data performs nearly on par with a model trained on the original manual annotations.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "accepted for publication in NAACL 2024"
    },
    {
        "paper id": "2403.17636",
        "abstract url": "https://arxiv.org/abs/2403.17636",
        "title": "Mix-Initiative Response Generation with Dynamic Prefix Tuning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in guiding generation dynamically. The prefix parameters can be tuned towards accurate initiative prediction as well as mix-initiative response generation. Extensive experiments on two public dialogue datasets show that the proposed IDPT outperforms previous baselines on both automatic metrics and human evaluations. It also manages to generate appropriate responses with manipulated initiatives.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to the main conference of NAACL 2024"
    },
    {
        "paper id": "2403.17639",
        "abstract url": "https://arxiv.org/abs/2403.17639",
        "title": "High-Resolution Image Translation Model Based on Grayscale Redefinition",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Image-to-image translation is a technique that focuses on transferring images from one domain to another while maintaining the essential content representations. In recent years, image-to-image translation has gained significant attention and achieved remarkable advancements due to its diverse applications in computer vision and image processing tasks. In this work, we propose an innovative method for image translation between different domains. For high-resolution image translation tasks, we use a grayscale adjustment method to achieve pixel-level translation. For other tasks, we utilize the Pix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and improved loss to enhance the image translation performance. On the other hand, to tackle the issue of sparse training data, we adopt model weight initialization from other task to optimize the performance of the current task.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17640",
        "abstract url": "https://arxiv.org/abs/2403.17640",
        "title": "REFeREE: A REference-FREE Model-Based Metric for Text Simplification",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Text simplification lacks a universal standard of quality, and annotated reference simplifications are scarce and costly. We propose to alleviate such limitations by introducing REFeREE, a reference-free model-based metric with a 3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage and can be applied to any quality standard as long as a small number of human annotations are available. Our experiments show that our metric outperforms existing reference-based metrics in predicting overall ratings and reaches competitive and consistent performance in predicting specific ratings while requiring no reference simplifications at inference time.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.17645",
        "abstract url": "https://arxiv.org/abs/2403.17645",
        "title": "DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to adapt swiftly to domain-specific entities for the NEC task. A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach. DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities. More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by LREC-COLING 2024"
    },
    {
        "paper id": "2403.17651",
        "abstract url": "https://arxiv.org/abs/2403.17651",
        "title": "Exploring Dynamic Transformer for Efficient Object Tracking",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The speed-precision trade-off is a critical problem for visual object tracking which usually requires low latency and deployment on constrained resources. Existing solutions for efficient tracking mainly focus on adopting light-weight backbones or modules, which nevertheless come at the cost of a sacrifice in precision. In this paper, inspired by dynamic network routing, we propose DyTrack, a dynamic transformer framework for efficient tracking. Real-world tracking scenarios exhibit diverse levels of complexity. We argue that a simple network is sufficient for easy frames in video sequences, while more computation could be assigned to difficult ones. DyTrack automatically learns to configure proper reasoning routes for various inputs, gaining better utilization of the available computational budget. Thus, it can achieve higher performance with the same running speed. We formulate instance-specific tracking as a sequential decision problem and attach terminating branches to intermediate layers of the entire model. Especially, to fully utilize the computations, we introduce the feature recycling mechanism to reuse the outputs of predecessors. Furthermore, a target-aware self-distillation strategy is designed to enhance the discriminating capabilities of early predictions by effectively mimicking the representation pattern of the deep model. Extensive experiments on multiple benchmarks demonstrate that DyTrack achieves promising speed-precision trade-offs with only a single model. For instance, DyTrack obtains 64.9% AUC on LaSOT with a speed of 256 fps.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17661",
        "abstract url": "https://arxiv.org/abs/2403.17661",
        "title": "Language Models for Text Classification: Is In-Context Learning Enough?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.17695",
        "abstract url": "https://arxiv.org/abs/2403.17695",
        "title": "PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the need for special tokens. We evaluate PlainMamba on a variety of visual recognition tasks including image classification, semantic segmentation, object detection, and instance segmentation. Our method achieves performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives. For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance. Code and models are available at https://github.com/ChenhongyiYang/PlainMamba",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17706",
        "abstract url": "https://arxiv.org/abs/2403.17706",
        "title": "Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed \"Topic Refinement\". This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined. By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by various models. Our comprehensive evaluation across three unique datasets has shown that our topic refinement approach significantly enhances the semantic coherence of topics.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "6 pages, 4 figures"
    },
    {
        "paper id": "2403.17725",
        "abstract url": "https://arxiv.org/abs/2403.17725",
        "title": "Deep Learning for Segmentation of Cracks in High-Resolution Images of Steel Bridges",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Automating the current bridge visual inspection practices using drones and image processing techniques is a prominent way to make these inspections more effective, robust, and less expensive. In this paper, we investigate the development of a novel deep-learning method for the detection of fatigue cracks in high-resolution images of steel bridges. First, we present a novel and challenging dataset comprising of images of cracks in steel bridges. Secondly, we integrate the ConvNext neural network with a previous state-of-the-art encoder-decoder network for crack segmentation. We study and report, the effects of the use of background patches on the network performance when applied to high-resolution images of cracks in steel bridges. Finally, we introduce a loss function that allows the use of more background patches for the training process, which yields a significant reduction in false positive rates.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17727",
        "abstract url": "https://arxiv.org/abs/2403.17727",
        "title": "FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Quickly understanding lengthy lecture videos is essential for learners with limited time and interest in various topics to improve their learning efficiency. To this end, video summarization has been actively researched to enable users to view only important scenes from a video. However, these studies focus on either the visual or audio information of a video and extract important segments in the video. Therefore, there is a risk of missing important information when both the teacher's speech and visual information on the blackboard or slides are important, such as in a lecture video. To tackle this issue, we propose FastPerson, a video summarization approach that considers both the visual and auditory information in lecture videos. FastPerson creates summary videos by utilizing audio transcriptions along with on-screen images and text, minimizing the risk of overlooking crucial information for learners. Further, it provides a feature that allows learners to switch between the summary and original videos for each chapter of the video, enabling them to adjust the pace of learning based on their interests and level of understanding. We conducted an evaluation with 40 participants to assess the effectiveness of our method and confirmed that it reduced viewing time by 53\\% at the same level of comprehension as that when using traditional video playback methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17733",
        "abstract url": "https://arxiv.org/abs/2403.17733",
        "title": "Continual Few-shot Event Detection via Hierarchical Augmentation Networks",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Traditional continual event detection relies on abundant labeled data for training, which is often impractical to obtain in real-world applications. In this paper, we introduce continual few-shot event detection (CFED), a more commonly encountered scenario when a substantial number of labeled samples are not accessible. The CFED task is challenging as it involves memorizing previous event types and learning new event types with few-shot samples. To mitigate these challenges, we propose a memory-based framework: Hierarchical Augmentation Networks (HANet). To memorize previous event types with limited memory, we incorporate prototypical augmentation into the memory set. For the issue of learning new event types in few-shot scenarios, we propose a contrastive augmentation module for token representations. Despite comparing with previous state-of-the-art methods, we also conduct comparisons with ChatGPT. Experiment results demonstrate that our method significantly outperforms all of these methods in multiple continual few-shot event detection tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2403.17752",
        "abstract url": "https://arxiv.org/abs/2403.17752",
        "title": "Can multiple-choice questions really be useful in detecting the abilities of LLMs?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs' output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17760",
        "abstract url": "https://arxiv.org/abs/2403.17760",
        "title": "Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024"
    },
    {
        "paper id": "2403.17768",
        "abstract url": "https://arxiv.org/abs/2403.17768",
        "title": "SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related to this work are available at https://dongqi.me/projects/SciNews.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024 Main Conference Paper"
    },
    {
        "paper id": "2403.17811",
        "abstract url": "https://arxiv.org/abs/2403.17811",
        "title": "Are Compressed Language Models Less Subgroup Robust?",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)"
    },
    {
        "paper id": "2403.17823",
        "abstract url": "https://arxiv.org/abs/2403.17823",
        "title": "Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Self-supervised pre-training of image encoders is omnipresent in the literature, particularly following the introduction of Masked autoencoders (MAE). Current efforts attempt to learn object-centric representations from motion in videos. In particular, SiamMAE recently introduced a Siamese network, training a shared-weight encoder from two frames of a video with a high asymmetric masking ratio (95%). In this work, we propose CropMAE, an alternative approach to the Siamese pre-training introduced by SiamMAE. Our method specifically differs by exclusively considering pairs of cropped images sourced from the same image but cropped differently, deviating from the conventional pairs of frames extracted from a video. CropMAE therefore alleviates the need for video datasets, while maintaining competitive performances and drastically reducing pre-training time. Furthermore, we demonstrate that CropMAE learns similar object-centric representations without explicit motion, showing that current self-supervised learning methods do not learn objects from motion, but rather thanks to the Siamese architecture. Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling the reconstruction of images using only two visible patches. Our code is available at https://github.com/alexandre-eymael/CropMAE.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "19 pages, 6 figures, 3 tables, 1 page of supplementary material"
    },
    {
        "paper id": "2403.17830",
        "abstract url": "https://arxiv.org/abs/2403.17830",
        "title": "Assessment of Multimodal Large Language Models in Alignment with Human Values",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations. To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations. Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle. We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives. Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2311.02692"
    },
    {
        "paper id": "2403.17848",
        "abstract url": "https://arxiv.org/abs/2403.17848",
        "title": "ArabicaQA: A Comprehensive Dataset for Arabic Question Answering",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we address the significant gap in Arabic natural language processing (NLP) resources by introducing ArabicaQA, the first large-scale dataset for machine reading comprehension and open-domain question answering in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable questions created by crowdworkers to look similar to answerable ones, along with additional labels of open-domain questions marks a crucial advancement in Arabic NLP resources. We also present AraDPR, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking of LLMs in Arabic question answering offer significant advancements in the field of Arabic NLP. The dataset and code are publicly accessible for further research https://github.com/DataScienceUIBK/ArabicaQA.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at SIGIR 2024"
    },
    {
        "paper id": "2403.17853",
        "abstract url": "https://arxiv.org/abs/2403.17853",
        "title": "Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17856",
        "abstract url": "https://arxiv.org/abs/2403.17856",
        "title": "Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural language inference task and the non-prototypical syntactic category task, as the massive GPT-4.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024"
    },
    {
        "paper id": "2403.17859",
        "abstract url": "https://arxiv.org/abs/2403.17859",
        "title": "ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models. At the same time, many benchmark datasets have become available for QA and MRC tasks. However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web. Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models. To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a large-scale dataset with 485K question-answer pairs created based on the historical newspaper collection Chronicling America. Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years. One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of OCR text. Therefore, to enable realistic testing of QA models, our dataset can be used in three different ways: answering questions from raw and noisy content, answering questions from cleaner, corrected version of the content, as well as answering questions from scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA spans the longest time period among available QA datasets make it quite a unique and useful resource.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at SIGIR 2024"
    },
    {
        "paper id": "2403.17860",
        "abstract url": "https://arxiv.org/abs/2403.17860",
        "title": "Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17905",
        "abstract url": "https://arxiv.org/abs/2403.17905",
        "title": "Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2",
        "rating": 1,
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)\" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation, considering radial k-space sampling acquisition sequences. Our preliminary results suggest that R2D2 achieves: (i) suboptimal performance compared to its unrolled incarnation R2D2-Net, which is however non-scalable due to the necessary embedding of NUFFT-based data-consistency layers; (ii) superior reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based approximation for data consistency; (iii) superior reconstruction quality to PnP, while only requiring few iterations.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "submitted to IEEE EUSIPCO 2024"
    },
    {
        "paper id": "2403.17929",
        "abstract url": "https://arxiv.org/abs/2403.17929",
        "title": "Towards Explaining Hypercomplex Neural Networks",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Hypercomplex neural networks are gaining increasing interest in the deep learning community. The attention directed towards hypercomplex models originates from several aspects, spanning from purely theoretical and mathematical characteristics to the practical advantage of lightweight models over conventional networks, and their unique properties to capture both global and local relations. In particular, a branch of these architectures, parameterized hypercomplex neural networks (PHNNs), has also gained popularity due to their versatility across a multitude of application domains. Nonetheless, only few attempts have been made to explain or interpret their intricacies. In this paper, we propose inherently interpretable PHNNs and quaternion-like networks, thus without the need for any post-hoc method. To achieve this, we define a type of cosine-similarity transform within the parameterized hypercomplex domain. This PHB-cos transform induces weight alignment with relevant input features and allows to reduce the model into a single linear transform, rendering it directly interpretable. In this work, we start to draw insights into how this unique branch of neural models operates. We observe that hypercomplex networks exhibit a tendency to concentrate on the shape around the main object of interest, in addition to the shape of the object itself. We provide a thorough analysis, studying single neurons of different layers and comparing them against how real-valued networks learn. The code of the paper is available at https://github.com/ispamm/HxAI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The paper has been accepted at IEEE WCCI 2024"
    },
    {
        "paper id": "2403.18018",
        "abstract url": "https://arxiv.org/abs/2403.18018",
        "title": "DORE: A Dataset For Portuguese Definition Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings of this paper will facilitate research and study of Portuguese in wider contexts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LREC-COLING 2024 (The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation)"
    },
    {
        "paper id": "2403.18024",
        "abstract url": "https://arxiv.org/abs/2403.18024",
        "title": "Enriching Word Usage Graphs with Cluster Definitions",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present a dataset of word usage graphs (WUGs), where the existing WUGs for multiple languages are enriched with cluster labels functioning as sense definitions. They are generated from scratch by fine-tuned encoder-decoder language models. The conducted human evaluation has shown that these definitions match the existing clusters in WUGs better than the definitions chosen from WordNet by two baseline systems. At the same time, the method is straightforward to use and easy to extend to new languages. The resulting enriched datasets can be extremely helpful for moving on to explainable semantic change modeling.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024"
    },
    {
        "paper id": "2403.18031",
        "abstract url": "https://arxiv.org/abs/2403.18031",
        "title": "The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for unsupervised neural machine translation. Theoretically, however, the method should not work in general. We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties. We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation. We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of unsupervised methods based on back-translation. Overall, the success of unsupervised machine translation was far from being analytically guaranteed. Instead, it is another proof that languages of the world share deep similarities, and we hope to show how to identify which of these similarities can serve the development of unsupervised, cross-linguistic tools.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18051",
        "abstract url": "https://arxiv.org/abs/2403.18051",
        "title": "Supervisory Prompt Training",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \\textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT advances LLMs by refining prompts to enhance performance and reduce hallucinations, offering an efficient and scalable alternative to traditional model fine-tuning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18063",
        "abstract url": "https://arxiv.org/abs/2403.18063",
        "title": "Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance over the state-of-the-art methods while reducing the number of parameters. Through extensive experiments, we show that SCT-C-small gives state-of-the-art performance on the ImageNet dataset and reaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge reaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project page is available on this webpage.\\url{https://github.com/badripatro/sct}",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18067",
        "abstract url": "https://arxiv.org/abs/2403.18067",
        "title": "State of the art applications of deep learning within tracking and detecting marine debris: A survey",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years. We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris. From cross referencing the research paper results, the YOLO family significantly outperforms all other methods of object detection but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning. Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database. We conclude this survey with over 40 future research recommendations and open challenges.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Review paper, 60 pages including references, 1 figure, 3 tables, 1 supplementary data"
    },
    {
        "paper id": "2403.18074",
        "abstract url": "https://arxiv.org/abs/2403.18074",
        "title": "Every Shot Counts: Using Exemplars for Repetition Counting in Videos",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video repetition counting infers the number of repetitions of recurring actions or motion within a video. We propose an exemplar-based approach that discovers visual correspondence of video exemplars across repetitions within target videos. Our proposed Every Shot Counts (ESCounts) model is an attention-based encoder-decoder that encodes videos of varying lengths alongside exemplars from the same and different videos. In training, ESCounts regresses locations of high correspondence to the exemplars within the video. In tandem, our method learns a latent that encodes representations of general repetitive motions, which we use for exemplar-free, zero-shot inference. Extensive experiments over commonly used datasets (RepCount, Countix, and UCFRep) showcase ESCounts obtaining state-of-the-art performance across all three datasets. On RepCount, ESCounts increases the off-by-one from 0.39 to 0.56 and decreases the mean absolute error from 0.38 to 0.21. Detailed ablations further demonstrate the effectiveness of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project website: https://sinhasaptarshi.github.io/escounts"
    },
    {
        "paper id": "2403.18093",
        "abstract url": "https://arxiv.org/abs/2403.18093",
        "title": "Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "JURISIN 2024"
    },
    {
        "paper id": "2403.18094",
        "abstract url": "https://arxiv.org/abs/2403.18094",
        "title": "A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Hand function is critical for our interactions and quality of life. Spinal cord injuries (SCI) can impair hand function, reducing independence. A comprehensive evaluation of function in home and community settings requires a hand grasp taxonomy for individuals with impaired hand function. Developing such a taxonomy is challenging due to unrepresented grasp types in standard taxonomies, uneven data distribution across injury levels, and limited data. This study aims to automatically identify the dominant distinct hand grasps in egocentric video using semantic clustering. Egocentric video recordings collected in the homes of 19 individual with cervical SCI were used to cluster grasping actions with semantic significance. A deep learning model integrating posture and appearance data was employed to create a personalized hand taxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with with 18.0% +- 21.8% redundancy. Qualitative assessment revealed meaningful clusters in video content. This methodology provides a flexible and effective strategy to analyze hand function in the wild. It offers researchers and clinicians an efficient tool for evaluating hand function, aiding sensitive assessments and tailored intervention plans.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18098",
        "abstract url": "https://arxiv.org/abs/2403.18098",
        "title": "GPTs and Language Barrier: A Cross-Lingual Legal QA Examination",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NLP 2024, Kobe, Japan"
    },
    {
        "paper id": "2403.18105",
        "abstract url": "https://arxiv.org/abs/2403.18105",
        "title": "Large Language Models for Education: A Survey and Outlook",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18116",
        "abstract url": "https://arxiv.org/abs/2403.18116",
        "title": "QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media images in crisis management has proven effective in various situations. However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area. Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively. The employment of satellites to monitor changes around the globe grants the possibility of exploiting instrumentation that is not limited by the visible spectrum, the presence of land infrastructures, and people in the affected areas. In this work, we propose a new dataset composed of images taken from Sentinel-1 and a new series of tasks to help monitor earthquakes from a new detailed view. Coupled with the data, we provide a series of traditional machine learning and deep learning models as baselines to assess the effectiveness of ML-based models in earthquake analysis.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at ISCRAM 2024"
    },
    {
        "paper id": "2403.18117",
        "abstract url": "https://arxiv.org/abs/2403.18117",
        "title": "TDIP: Tunable Deep Image Processing, a Real Time Melt Pool Monitoring Solution",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the era of Industry 4.0, Additive Manufacturing (AM), particularly metal AM, has emerged as a significant contributor due to its innovative and cost-effective approach to fabricate highly intricate geometries. Despite its potential, this industry still lacks real-time capable process monitoring algorithms. Recent advancements in this field suggest that Melt Pool (MP) signatures during the fabrication process contain crucial information about process dynamics and quality. To obtain this information, various sensory approaches, such as high-speed cameras-based vision modules are employed for online fabrication monitoring. However, many conventional in-depth analyses still cannot process all the recorded data simultaneously. Although conventional Image Processing (ImP) solutions provide a targeted tunable approach, they pose a trade-off between convergence certainty and convergence speed. As a result, conventional methods are not suitable for a dynamically changing application like MP monitoring. Therefore, this article proposes the implementation of a Tunable Deep Image Processing (TDIP) method to address the data-rich monitoring needs in real-time. The proposed model is first trained to replicate an ImP algorithm with tunable features and methodology. The TDIP model is then further improved to account for MP geometries and fabrication quality based on the vision input and process parameters. The TDIP model achieved over 94% estimation accuracy with more than 96% R2 score for quality, geometry, and MP signature estimation and isolation. The TDIP model can process 500 images per second, while conventional methods taking a few minutes per image. This significant processing time reduction enables the integration of vision-based monitoring in real-time for processes and quality estimation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18120",
        "abstract url": "https://arxiv.org/abs/2403.18120",
        "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
        "rating": 1.0,
        "keywords": [
            [
                "cs.AI"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "ICLR 2024"
    },
    {
        "paper id": "2403.18121",
        "abstract url": "https://arxiv.org/abs/2403.18121",
        "title": "ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by LREC-COLING 2024"
    },
    {
        "paper id": "2403.18140",
        "abstract url": "https://arxiv.org/abs/2403.18140",
        "title": "Juru: Legal Brazilian Large Language Model from Reputable Sources",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\u00e1-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18148",
        "abstract url": "https://arxiv.org/abs/2403.18148",
        "title": "Large Language Models Produce Responses Perceived to be Empathic",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles\", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is important.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18152",
        "abstract url": "https://arxiv.org/abs/2403.18152",
        "title": "Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify outputs that may require expert attention. Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2403.18158",
        "abstract url": "https://arxiv.org/abs/2403.18158",
        "title": "The Effects of Short Video-Sharing Services on Video Copy Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The short video-sharing services that allow users to post 10-30 second videos (e.g., YouTube Shorts and TikTok) have attracted a lot of attention in recent years. However, conventional video copy detection (VCD) methods mainly focus on general video-sharing services (e.g., YouTube and Bilibili), and the effects of short video-sharing services on video copy detection are still unclear. Considering that illegally copied videos in short video-sharing services have service-distinctive characteristics, especially in those time lengths, the pros and cons of VCD in those services are required to be analyzed. In this paper, we examine the effects of short video-sharing services on VCD by constructing a dataset that has short video-sharing service characteristics. Our novel dataset is automatically constructed from the publicly available dataset to have reference videos and fixed short-time-length query videos, and such automation procedures assure the reproducibility and data privacy preservation of this paper. From the experimental results focusing on segment-level and video-level situations, we can see that three effects: \"Segment-level VCD in short video-sharing services is more difficult than those in general video-sharing services\", \"Video-level VCD in short video-sharing services is easier than those in general video-sharing services\", \"The video alignment component mainly suppress the detection performance in short video-sharing services\".",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18167",
        "abstract url": "https://arxiv.org/abs/2403.18167",
        "title": "Mechanisms of non-factual hallucinations in language models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics for the two mechanistic causes of hallucinations. We also highlight how attribution features from our causal analysis can effectively construct hallucination detectors. Our work proposes a mechanistic understanding of LM factual errors.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18182",
        "abstract url": "https://arxiv.org/abs/2403.18182",
        "title": "ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech corpus. The corpus comprises twelve hours of Zoom meetings involving multiple speakers role-playing a work situation where Students brainstorm ideas for a certain topic and then discuss it with an Interlocutor. The meetings cover different topics and are divided into phases with different language setups. The corpus presents a challenging set for automatic speech recognition (ASR), including two languages (Arabic and English) with Arabic spoken in multiple variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English used with various accents. Adding to the complexity of the corpus, there is also code-switching between these languages and dialects. As part of our work, we take inspiration from established sets of transcription guidelines to present a set of guidelines handling issues of conversational speech, code-switching and orthography of both languages. We further enrich the corpus with two layers of annotations; (1) dialectness level annotation for the portion of the corpus where mixing occurs between different variants of Arabic, and (2) automatic morphological annotations, including tokenization, lemmatization, and part-of-speech tagging.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2403.18251",
        "abstract url": "https://arxiv.org/abs/2403.18251",
        "title": "Since the Scientific Literature Is Multilingual, Our Models Should Be Too",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "English has long been assumed the $\\textit{lingua franca}$ of scientific research, and this notion is reflected in the natural language processing (NLP) research involving scientific document representation. In this position piece, we quantitatively show that the literature is largely multilingual and argue that current models and benchmarks should reflect this linguistic diversity. We provide evidence that text-based models fail to create meaningful representations for non-English papers and highlight the negative user-facing impacts of using English-only models non-discriminately across a multilingual domain. We end with suggestions for the NLP community on how to improve performance on non-English documents.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18252",
        "abstract url": "https://arxiv.org/abs/2403.18252",
        "title": "Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our model can consistently outperform the state-of-the-art (SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone visual representations, our model can closely match or even beat the SOTA MLLMs that are built on CLIP visual embeddings. Our code is available at https://github.com/LaVi-Lab/Visual-Table.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://github.com/LaVi-Lab/Visual-Table"
    },
    {
        "paper id": "2403.18253",
        "abstract url": "https://arxiv.org/abs/2403.18253",
        "title": "Enhancing Metaphor Detection through Soft Labels and Target Word Prediction",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Metaphors play a significant role in our everyday communication, yet detecting them presents a challenge. Traditional methods often struggle with improper application of language rules and a tendency to overlook data sparsity. To address these issues, we integrate knowledge distillation and prompt learning into metaphor detection. Our approach revolves around a tailored prompt learning framework specifically designed for metaphor detection. By strategically masking target words and providing relevant prompt data, we guide the model to accurately predict the contextual meanings of these words. This approach not only mitigates confusion stemming from the literal meanings of the words but also ensures effective application of language rules for metaphor detection. Furthermore, we've introduced a teacher model to generate valuable soft labels. These soft labels provide a similar effect to label smoothing and help prevent the model from becoming over confident and effectively addresses the challenge of data sparsity. Experimental results demonstrate that our model has achieved state-of-the-art performance, as evidenced by its remarkable results across various datasets.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19713",
        "abstract url": "https://arxiv.org/abs/2403.19713",
        "title": "NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.14415",
        "abstract url": "https://arxiv.org/abs/2404.14415",
        "title": "Domain Adaptation in Intent Classification Systems: A Review",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Dialogue agents, which perform specific tasks, are part of the long-term goal of NLP researchers to build intelligent agents that communicate with humans in natural language. Such systems should adapt easily from one domain to another to assist users in completing tasks. Researchers have developed a broad range of techniques, objectives, and datasets for intent classification to achieve such systems. Despite the progress in developing intent classification systems (ICS), a systematic review of the progress from a technical perspective is yet to be conducted. In effect, important implementation details of intent classification remain restricted and unclear, making it hard for natural language processing (NLP) researchers to develop new methods. To fill this gap, we review contemporary works in intent classification. Specifically, we conduct a thorough technical review of the datasets, domains, tasks, and methods needed to train the intent classification part of dialogue systems. Our structured analysis describes why intent classification is difficult and studies the limitations to domain adaptation while presenting opportunities for future work.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17381",
        "abstract url": "https://arxiv.org/abs/2403.17381",
        "title": "Application-Driven Innovation in Machine Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 3 figures"
    },
    {
        "paper id": "2403.17395",
        "abstract url": "https://arxiv.org/abs/2403.17395",
        "title": "An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We propose an open-source end-to-end logic optimization framework for large-scale boolean network with reinforcement learning.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "5 pages, 4 figures, 1 table"
    },
    {
        "paper id": "2403.17404",
        "abstract url": "https://arxiv.org/abs/2403.17404",
        "title": "Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, the total number of experts, the sparsity in expert selection, the complexity of the routing mechanism, and the complexity of individual experts. Our analysis sheds light on \\textit{how \\textbf{sparsity} contributes to the MoE's generalization}, offering insights from the perspective of classical learning theory.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17405",
        "abstract url": "https://arxiv.org/abs/2403.17405",
        "title": "The recessionary pressures of generative AI: A threat to wellbeing",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Generative Artificial Intelligence (AI) stands as a transformative force that presents a paradox; it offers unprecedented opportunities for productivity growth while potentially posing significant threats to economic stability and societal wellbeing. Many consider generative AI as akin to previous technological advancements, using historical precedent to argue that fears of widespread job displacement are unfounded, while others contend that generative AI`s unique capacity to undertake non-routine cognitive tasks sets it apart from other forms of automation capital and presents a threat to the quality and availability of work that underpin stable societies. This paper explores the conditions under which both may be true. We posit the existence of an AI-capital-to-labour ratio threshold beyond which a self-reinforcing cycle of recessionary pressures could be triggered, exacerbating social disparities, reducing social cohesion, heightening tensions, and requiring sustained government intervention to maintain stability. To prevent this, the paper underscores the urgent need for proactive policy responses, making recommendations to reduce these risks through robust regulatory frameworks and a new social contract characterised by progressive social and economic policies. This approach aims to ensure a sustainable, inclusive, and resilient economic future where human contribution to the economy is retained and integrated with generative AI to enhance the Mental Wealth of nations.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "7 pages, 3 figures"
    },
    {
        "paper id": "2403.17410",
        "abstract url": "https://arxiv.org/abs/2403.17410",
        "title": "On permutation-invariant neural networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of the diverse problem settings and ongoing research efforts pertaining to neural networks that approximate set functions. By delving into the intricacies of these approaches and elucidating the associated challenges, the survey aims to equip readers with a comprehensive understanding of the field. Through this comprehensive perspective, we hope that researchers can gain valuable insights into the potential applications, inherent limitations, and future directions of set-based neural networks. Indeed, from this survey we gain two insights: i) Deep Sets and its variants can be generalized by differences in the aggregation function, and ii) the behavior of Deep Sets is sensitive to the choice of the aggregation function. From these observations, we show that Deep Sets, one of the well-known permutation-invariant neural networks, can be generalized in the sense of a quasi-arithmetic mean.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17419",
        "abstract url": "https://arxiv.org/abs/2403.17419",
        "title": "AI Safety: Necessary, but insufficient and possibly problematic",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This article critically examines the recent hype around AI safety. We first start with noting the nature of the AI safety hype as being dominated by governments and corporations, and contrast it with other avenues within AI research on advancing social good. We consider what 'AI safety' actually means, and outline the dominant concepts that the digital footprint of AI safety aligns with. We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense. We note that the AI safety debate has already influenced some regulatory efforts in AI, perhaps in not so desirable directions. We also share our concerns on how AI safety may normalize AI that advances structural harm through providing exploitative and harmful AI with a veneer of safety.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "AI & Soc (2024)"
    },
    {
        "paper id": "2403.17422",
        "abstract url": "https://arxiv.org/abs/2403.17422",
        "title": "InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024, project page: https://jyunlee.github.io/projects/interhandgen/"
    },
    {
        "paper id": "2403.17445",
        "abstract url": "https://arxiv.org/abs/2403.17445",
        "title": "Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 5 tables, 3 figures"
    },
    {
        "paper id": "2403.17447",
        "abstract url": "https://arxiv.org/abs/2403.17447",
        "title": "Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Convolutional neural networks (CNNs) have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance. To release this burden, model compression has become an important research focus. Many approaches like quantization, pruning, early exit, and knowledge distillation have demonstrated the effect of reducing redundancy in neural networks. Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined. To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network. Validated on the image-based regression and classification networks across different data sets, our proposed Chain of Compression can significantly compress the computation cost by 100-1000 times with ignorable accuracy loss compared with the baseline model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 15 figures"
    },
    {
        "paper id": "2403.17465",
        "abstract url": "https://arxiv.org/abs/2403.17465",
        "title": "LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.17466",
        "abstract url": "https://arxiv.org/abs/2403.17466",
        "title": "Green HPC: An analysis of the domain based on Top500",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The demand in computing power has never stopped growing over the years. Today, the performance of the most powerful systems exceeds the exascale and the number of petascale systems continues to grow. Unfortunately, this growth also goes hand in hand with ever-increasing energy costs, which in turn means a significant carbon footprint. In view of the environmental crisis, this paper intents to look at the often hidden issue of energy consumption of HPC systems. As it is not easy to access the data of the constructors, we then consider the Top500 as the tip of the iceberg to identify the trends of the whole domain.The objective of this work is to analyze Top500 and Green500 data from several perspectives in order to identify the dynamic of the domain regarding its environmental impact. The contributions are to take stock of the empirical laws governing the evolution of HPC computing systems both from the performance and energy perspectives, to analyze the most relevant data for developing the performance and energy efficiency of large-scale computing systems, to put these analyses into perspective with effects and impacts (lifespan of the HPC systems) and finally to derive a predictive model for the weight of HPC sector within the horizon 2030.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17496",
        "abstract url": "https://arxiv.org/abs/2403.17496",
        "title": "Dr.Hair: Reconstructing Scalp-Connected Hair Strands without Pre-training via Differentiable Rendering of Line Segments",
        "rating": 0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In the film and gaming industries, achieving a realistic hair appearance typically involves the use of strands originating from the scalp. However, reconstructing these strands from observed surface images of hair presents significant challenges. The difficulty in acquiring Ground Truth (GT) data has led state-of-the-art learning-based methods to rely on pre-training with manually prepared synthetic CG data. This process is not only labor-intensive and costly but also introduces complications due to the domain gap when compared to real-world data. In this study, we propose an optimization-based approach that eliminates the need for pre-training. Our method represents hair strands as line segments growing from the scalp and optimizes them using a novel differentiable rendering algorithm. To robustly optimize a substantial number of slender explicit geometries, we introduce 3D orientation estimation utilizing global optimization, strand initialization based on Laplace's equation, and reparameterization that leverages geometric connectivity and spatial proximity. Unlike existing optimization-based methods, our method is capable of reconstructing internal hair flow in an absolute direction. Our method exhibits robust and accurate inverse rendering, surpassing the quality of existing methods and significantly improving processing speed.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.17542",
        "abstract url": "https://arxiv.org/abs/2403.17542",
        "title": "VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Despite the considerable attention given to the questions of \\textit{how much} and \\textit{how to} explore in deep reinforcement learning, the investigation into \\textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\u03b5$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \\textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a novel approach for efficient exploration timing. Experimental results on the Atari suite demonstrate the superiority of our strategy over traditional methods such as $\u03b5$-greedy and Boltzmann, as well as more sophisticated techniques like Noisy Nets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17561",
        "abstract url": "https://arxiv.org/abs/2403.17561",
        "title": "A Survey on Deep Learning and State-of-the-art Applications",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing. We highlight the key features of the models and their effectiveness in solving the problems within each domain. Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures. Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Submitted to Elsevier Neural Networks"
    },
    {
        "paper id": "2403.17592",
        "abstract url": "https://arxiv.org/abs/2403.17592",
        "title": "On the Benefits of Over-parameterization for Out-of-Distribution Generalization",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess in-distribution (ID) loss. We demonstrate that in this scenario, further increasing the model's parameterization can significantly reduce the OOD loss. Intuitively, the variance term of ID loss remains low due to orthogonality of long-tail features, meaning overfitting noise during training generally doesn't raise testing loss. However, in OOD cases, distributional shift increases the variance term. Thankfully, the inherent shift is unrelated to individual x, maintaining the orthogonality of long-tail features. Expanding the hidden dimension can additionally improve this orthogonality by mapping the features into higher-dimensional spaces, thereby reducing the variance term. We further show that model ensembles also improve OOD loss, akin to increasing model capacity. These insights explain the empirical phenomenon of enhanced OOD generalization through model ensembles, supported by consistent simulations with theoretical results.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17610",
        "abstract url": "https://arxiv.org/abs/2403.17610",
        "title": "MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors",
        "rating": 0.5,
        "keywords": [
            [
                "RGBD"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Foot contact is an important cue for human motion capture, understanding, and generation. Existing datasets tend to annotate dense foot contact using visual matching with thresholding or incorporating pressure signals. However, these approaches either suffer from low accuracy or are only designed for small-range and slow motion. There is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024"
    },
    {
        "paper id": "2403.17632",
        "abstract url": "https://arxiv.org/abs/2403.17632",
        "title": "Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for energy modelling research related to E-Scooters and E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption modelling based on the dataset using a set of representative machine learning algorithms and compare their performance against the contemporary mathematical models as a baseline. Our results demonstrate a notable advantage for data-driven models in comparison to the corresponding mathematical models for estimating energy consumption. Specifically, data-driven models outperform physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for E-Scooters based on an in-depth analysis of the dataset under certain assumptions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "7 pages, 5 figures, 4 tables. This manuscript has been accepted by the IEEE ITEC 2024"
    },
    {
        "paper id": "2403.17637",
        "abstract url": "https://arxiv.org/abs/2403.17637",
        "title": "PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \\textit{PettingZoo}-based interface for RL agent deployment in both solo and multi-agent setups. Furthermore, we demonstrate the utility of the environment through experiments with Deep Reinforcement Learning agents, showcasing the potential of RL-based approaches to significantly enhance offloading strategies in distributed computing settings. PeersimGym thus bridges the gap between theoretical RL models and their practical applications, paving the way for advancements in efficient task offloading methodologies.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17643",
        "abstract url": "https://arxiv.org/abs/2403.17643",
        "title": "S+t-SNE -- Bringing dimensionality reduction to data streams",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. We will soon add a link to the final version of this contribution that underwent peer-review and post-acceptance improvements and was presented at IDA2024 (https://ida2024.org/)"
    },
    {
        "paper id": "2403.17646",
        "abstract url": "https://arxiv.org/abs/2403.17646",
        "title": "Uncertainty-aware Distributional Offline Reinforcement Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17653",
        "abstract url": "https://arxiv.org/abs/2403.17653",
        "title": "An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We present an extension-based approach for computing and verifying preferences in an abstract argumentation system. Although numerous argumentation semantics have been developed previously for identifying acceptable sets of arguments from an argumentation framework, there is a lack of justification behind their acceptability based on implicit argument preferences. Preference-based argumentation frameworks allow one to determine what arguments are justified given a set of preferences. Our research considers the inverse of the standard reasoning problem, i.e., given an abstract argumentation framework and a set of justified arguments, we compute what the possible preferences over arguments are. Furthermore, there is a need to verify (i.e., assess) that the computed preferences would lead to the acceptable sets of arguments. This paper presents a novel approach and algorithm for exhaustively computing and enumerating all possible sets of preferences (restricted to three identified cases) for a conflict-free set of arguments in an abstract argumentation framework. We prove the soundness, completeness and termination of the algorithm. The research establishes that preferences are determined using an extension-based approach after the evaluation phase (acceptability of arguments) rather than stated beforehand. In this work, we focus our research study on grounded, preferred and stable semantics. We show that the complexity of computing sets of preferences is exponential in the number of arguments, and thus, describe an approximate approach and algorithm to compute the preferences. Furthermore, we present novel algorithms for verifying (i.e., assessing) the computed preferences. We provide details of the implementation of the algorithms (source code has been made available), various experiments performed to evaluate the algorithms and the analysis of the results.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17673",
        "abstract url": "https://arxiv.org/abs/2403.17673",
        "title": "How Private is DP-SGD?",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17698",
        "abstract url": "https://arxiv.org/abs/2403.17698",
        "title": "MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation",
        "rating": 0.5,
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Kernel Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kernel function adheres to a consistent mean weight coefficient, harnessing the synergistic advantages of different kernels to formulate an innovative bias function. Subsequently, specific slopes are tailored for each kernel function, applying penalties at varying rates, to enhance the model's extrapolation capabilities. Finally, this bias is seamlessly incorporated as a penalty to the post-softmax scores. We present two distinct versions of our method: a parameter-free variant that requires no new learnable parameters, which enhances length extrapolation capabilities without compromising training efficiency, and a parameterized variant capable of integrating state-of-the-art techniques. Empirical evaluations across diverse datasets have demonstrated that both variants of our method achieve state-of-the-art performance, outperforming traditional parameter-free and parameterized approaches.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17726",
        "abstract url": "https://arxiv.org/abs/2403.17726",
        "title": "Tiny Models are the Computational Saver for Large Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding will help the research community in exploring new compression methods to address the escalating computational demands posed by rapidly evolving AI models. Our evaluation of this approach in ImageNet-1k classification demonstrates its potential to reduce the number of compute operations by up to 90%, with only negligible losses in performance, across various modern vision models. The code of this work will be available.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17742",
        "abstract url": "https://arxiv.org/abs/2403.17742",
        "title": "Using Stratified Sampling to Improve LIME Image Explanations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator. Experiments show the efficacy of the proposed approach.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17776",
        "abstract url": "https://arxiv.org/abs/2403.17776",
        "title": "Exploring the Boundaries of Ambient Awareness in Twitter",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Ambient awareness refers to the ability of social media users to obtain knowledge about who knows what (i.e., users' expertise) in their network, by simply being exposed to other users' content (e.g, tweets on Twitter). Previous work, based on user surveys, reveals that individuals self-report ambient awareness only for parts of their networks. However, it is unclear whether it is their limited cognitive capacity or the limited exposure to diagnostic tweets (i.e., online content) that prevents people from developing ambient awareness for their complete network. In this work, we focus on in-wall ambient awareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that allows us to explore to which extent IWAA is likely, or even possible. First, we rely on reactions (e.g., likes), as strong evidence of users being aware of experts in Twitter. Unfortunately, such strong evidence can be only measured for active users, which represent the minority in the network. Thus to study the boundaries of IWAA to a larger extent, in the second part of our analysis, we instead focus on the passive exposure to content generated by other users -- which we refer to as in-wall visibility. This analysis shows that (in line with \\citet{levordashka2016ambient}) only for a subset of users IWAA is plausible, while for the majority it is unlikely, if even possible, to develop IWAA. We hope that our methodology paves the way for the emergence of data-driven approaches for the study of ambient awareness.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17787",
        "abstract url": "https://arxiv.org/abs/2403.17787",
        "title": "Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications. These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges. We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations. Our results highlight a significant divergence in performance, with Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models. The ViT models, on the other hand, demonstrate exceptional accuracy, achieving near-perfect performance on both tasks. This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications but also emphasizes the unmatched efficacy of fine-tuned ViT models for precise and dependable tasks.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17806",
        "abstract url": "https://arxiv.org/abs/2403.17806",
        "title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using causal interventions. We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17826",
        "abstract url": "https://arxiv.org/abs/2403.17826",
        "title": "On the Computational Complexity of Stackelberg Planning and Meta-Operator Verification: Technical Report",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Stackelberg planning is a recently introduced single-turn two-player adversarial planning model, where two players are acting in a joint classical planning task, the objective of the first player being hampering the second player from achieving its goal. This places the Stackelberg planning problem somewhere between classical planning and general combinatorial two-player games. But, where exactly? All investigations of Stackelberg planning so far focused on practical aspects. We close this gap by conducting the first theoretical complexity analysis of Stackelberg planning. We show that in general Stackelberg planning is actually no harder than classical planning. Under a polynomial plan-length restriction, however, Stackelberg planning is a level higher up in the polynomial complexity hierarchy, suggesting that compilations into classical planning come with a worst-case exponential plan-length increase. In attempts to identify tractable fragments, we further study its complexity under various planning task restrictions, showing that Stackelberg planning remains intractable where classical planning is not. We finally inspect the complexity of meta-operator verification, a problem that has been recently connected to Stackelberg planning.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Presented at ICAPS24"
    },
    {
        "paper id": "2403.17831",
        "abstract url": "https://arxiv.org/abs/2403.17831",
        "title": "Learning the Optimal Power Flow: Environment Design Matters",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17844",
        "abstract url": "https://arxiv.org/abs/2403.17844",
        "title": "Mechanistic Design and Scaling of Hybrid Architectures",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17845",
        "abstract url": "https://arxiv.org/abs/2403.17845",
        "title": "TractOracle: towards an anatomically-informed reward function for RL-based tractography",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL)-based tractography is a competitive alternative to machine learning and classical tractography algorithms due to its high anatomical accuracy obtained without the need for any annotated data. However, the reward functions so far used to train RL agents do not encapsulate anatomical knowledge which causes agents to generate spurious false positives tracts. In this paper, we propose a new RL tractography system, TractOracle, which relies on a reward network trained for streamline classification. This network is used both as a reward function during training as well as a mean for stopping the tracking process early and thus reduce the number of false positive streamlines. This makes our system a unique method that evaluates and reconstructs WM streamlines at the same time. We report an improvement of true positive ratios by almost 20\\% and a reduction of 3x of false positive ratios on one dataset and an increase between 2x and 7x in the number true positive streamlines on another dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17852",
        "abstract url": "https://arxiv.org/abs/2403.17852",
        "title": "Counterfactual Fairness through Transforming Data Orthogonal to Bias",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine learning models and tasks, and includes a sparse variant to enhance numerical stability through regularization. Through empirical evaluation on simulated and real-world datasets - including the adult income and the COMPAS recidivism datasets - our methodology demonstrates its capacity to enable fairer outcomes without compromising accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17870",
        "abstract url": "https://arxiv.org/abs/2403.17870",
        "title": "Boosting Diffusion Models with Moving Average Sampling in Frequency Domain",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Diffusion models have recently brought a powerful revolution in image generation. Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability. In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples. Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid distribution shift across timesteps. In view that diffusion models evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component. We name the complete approach \"Moving Average Sampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into mainstream pre-trained diffusion models and sampling schedules. Extensive experiments on both unconditional and conditional diffusion models demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.17886",
        "abstract url": "https://arxiv.org/abs/2403.17886",
        "title": "Neural Embedding Compression For Efficient Multi-Task Earth Observation Modelling",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Published at IGARSS 2024"
    },
    {
        "paper id": "2403.17891",
        "abstract url": "https://arxiv.org/abs/2403.17891",
        "title": "Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types. This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers. Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance. To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection. Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted in IISE Transaction"
    },
    {
        "paper id": "2403.17914",
        "abstract url": "https://arxiv.org/abs/2403.17914",
        "title": "Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we need to understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT. To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels. The effectiveness of our framework is evaluated with the data collected by National Transportation Safety Board (NTSB). It has been shown that fine-level prediction accuracy is highly improved, and the regularization term can be beneficial to the rare event identification problem.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Accepted in INFORMS Journal of Data Science"
    },
    {
        "paper id": "2403.17918",
        "abstract url": "https://arxiv.org/abs/2403.17918",
        "title": "AgentStudio: A Toolkit for Building General Virtual Agents",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development. This includes environment setups, data collection, agent evaluation, and visualization. The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces. This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings. To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both created with our graphical interfaces. Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc. We have open-sourced the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17936",
        "abstract url": "https://arxiv.org/abs/2403.17936",
        "title": "ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion",
                "Synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Gestures play a key role in human communication. Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance. Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words. Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis. Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing. Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures. To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. We urge the reader to watch our supplementary video at our website.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024. Project Page: https://vcai.mpi-inf.mpg.de/projects/ConvoFusion/"
    },
    {
        "paper id": "2403.18057",
        "abstract url": "https://arxiv.org/abs/2403.18057",
        "title": "Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost. In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages. Nonetheless, they also present challenges for multiagent reinforcement learning, including addressing the non-stationary problem and managing an imbalanced number of agents with different types. We propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method to address large-scale heterogeneous cooperation problems. PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization. Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agents. Next, we use Unreal Engine to design a large-scale heterogeneous cooperation benchmark named Large-Scale Multiagent Operation (LSMO), which is a complex two-team competition scenario that requires collaboration from both ground and airborne agents. We use experiments to show that PHLRL outperforms state-of-the-art methods, including QTRAN and QPLEX in LSMO.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18101",
        "abstract url": "https://arxiv.org/abs/2403.18101",
        "title": "Towards Explainable Clustering: A Constrained Declarative based Approach",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The domain of explainable AI is of interest in all Machine Learning fields, and it is all the more important in clustering, an unsupervised task whose result must be validated by a domain expert. We aim at finding a clustering that has high quality in terms of classic clustering criteria and that is explainable, and we argue that these two dimensions must be considered when building the clustering. We consider that a good global explanation of a clustering should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination). Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected clustering or on its explanations. In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained clustering method called ECS for declarative clustering with Explainabilty-driven Cluster Selection that integrates structural or domain expert knowledge expressed by means of constraints. It is based on the notion of coverage and discrimination that are formalized at different levels (cluster / clustering), each allowing for exceptions through parameterized thresholds. Our method relies on four steps: generation of a set of partitions, computation of frequent patterns for each cluster, pruning clusters that violates some constraints, and selection of clusters and associated patterns to build an interpretable clustering. This last step is combinatorial and we have developed a Constraint-Programming (CP) model to solve it. The method can integrate prior knowledge in the form of user constraints, both before or in the CP model.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18107",
        "abstract url": "https://arxiv.org/abs/2403.18107",
        "title": "The Need for Climate Data Stewardship: 10 Tensions and Reflections regarding Climate Data Governance",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Datafication -- the increase in data generation and advancements in data analysis -- offers new possibilities for governing and tackling worldwide challenges such as climate change. However, employing new data sources in policymaking carries various risks, such as exacerbating inequalities, introducing biases, and creating gaps in access. This paper articulates ten core tensions related to climate data and its implications for climate data governance, ranging from the diversity of data sources and stakeholders to issues of quality, access, and the balancing act between local needs and global imperatives. Through examining these tensions, the article advocates for a paradigm shift towards multi-stakeholder governance, data stewardship, and equitable data practices to harness the potential of climate data for public good. It underscores the critical role of data stewards in navigating these challenges, fostering a responsible data ecology, and ultimately contributing to a more sustainable and just approach to climate action and broader social issues.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "13 pages, 2 figures"
    },
    {
        "paper id": "2403.18127",
        "abstract url": "https://arxiv.org/abs/2403.18127",
        "title": "A Correction of Pseudo Log-Likelihood Method",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method used in various fields including contextual bandits, influence maximization of social networks, and causal bandits. However, in previous literature \\citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function may not be bounded, which may result in the algorithm they proposed not well-defined. In this paper, we give a counterexample that the maximum pseudo log-likelihood estimation fails and then provide a solution to correct the algorithms in \\citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "7 pages"
    },
    {
        "paper id": "2403.18133",
        "abstract url": "https://arxiv.org/abs/2403.18133",
        "title": "AE SemRL: Learning Semantic Association Rules with Autoencoders",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster execution time than state-of-the-art ARM approaches in many scenarios. We believe that this study advances a new way of extracting associations from representations and has the potential to inspire more research in this field.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18147",
        "abstract url": "https://arxiv.org/abs/2403.18147",
        "title": "Divide, Conquer, Combine Bayesian Decision Tree Sampling",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Decision trees are commonly used predictive models due to their flexibility and interpretability. This paper is directed at quantifying the uncertainty of decision tree predictions by employing a Bayesian inference approach. This is challenging because these approaches need to explore both the tree structure space and the space of decision parameters associated with each tree structure. This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a Markov Chain is constructed to provide samples from the desired Bayesian estimate. Importantly, the structure and the decision parameters are tightly coupled; small changes in the tree structure can demand vastly different decision parameters to provide accurate predictions. A challenge for existing MCMC approaches is proposing joint changes in both the tree structure and the decision parameters that result in efficient sampling. This paper takes a different approach, where each distinct tree structure is associated with a unique set of decision parameters. The proposed approach, entitled DCC-Tree, is inspired by the work in Zhou et al. [23] for probabilistic programs and Cochrane et al. [4] for Hamiltonian Monte Carlo (HMC) based sampling for decision trees. Results show that DCC-Tree performs comparably to other HMC-based methods and better than existing Bayesian tree methods while improving on consistency and reducing the per-proposal complexity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "38 pages, 5 figures"
    },
    {
        "paper id": "2403.18163",
        "abstract url": "https://arxiv.org/abs/2403.18163",
        "title": "A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "In this work we consider the impact of information spread in time-varying social networks, where agents request to follow other agents with aligned opinions while dropping ties to neighbors whose posts are too dissimilar to their own views. Opinion control and rhetorical influence has a very long history, employing various methods including education, persuasion, propaganda, marketing, and manipulation through mis-, dis-, and mal-information. The automation of opinion controllers, however, has only recently become easily deployable at a wide scale, with the advent of large language models (LLMs) and generative AI that can translate the quantified commands from opinion controllers into actual content with the appropriate nuance. Automated agents in social networks can be deployed for various purposes, such as breaking up echo chambers, bridging valuable new connections between agents, or shaping the opinions of a target population -- and all of these raise important ethical concerns that deserve serious attention and thoughtful discussion and debate. This paper attempts to contribute to this discussion by considering three archetypal influencing styles observed by human drivers in these settings, comparing and contrasting the impact of these different control methods on the opinions of agents in the network. We will demonstrate the efficacy of current generative AI for generating nuanced content consistent with the command signal from automatic opinion controllers like these, and we will report on frameworks for approaching the relevant ethical considerations.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "Submission to IEEE 2024 Conference on Decision and Control. 8 pages, 7 figures, 1 table"
    },
    {
        "paper id": "2403.18176",
        "abstract url": "https://arxiv.org/abs/2403.18176",
        "title": "Mistake, Manipulation and Margin Guarantees in Online Strategic Classification",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider an online strategic classification problem where each arriving agent can manipulate their true feature vector to obtain a positive predicted label, while incurring a cost that depends on the amount of manipulation. The learner seeks to predict the agent's true label given access to only the manipulated features. After the learner releases their prediction, the agent's true label is revealed. Previous algorithms such as the strategic perceptron guarantee finitely many mistakes under a margin assumption on agents' true feature vectors. However, these are not guaranteed to encourage agents to be truthful. Promoting truthfulness is intimately linked to obtaining adequate margin on the predictions, thus we provide two new algorithms aimed at recovering the maximum margin classifier in the presence of strategic agent behavior. We prove convergence, finite mistake and finite manipulation guarantees for a variety of agent cost structures. We also provide generalized versions of the strategic perceptron with mistake guarantees for different costs. Our numerical study on real and synthetic data demonstrates that the new algorithms outperform previous ones in terms of margin, number of manipulation and number of mistakes.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18181",
        "abstract url": "https://arxiv.org/abs/2403.18181",
        "title": "Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning methods allow the prediction of nonlinear dynamical systems from data alone. The Koopman operator is one of them, which enables us to employ linear analysis for nonlinear dynamical systems. The linear characteristics of the Koopman operator are hopeful to understand the nonlinear dynamics and perform rapid predictions. The extended dynamic mode decomposition (EDMD) is one of the methods to approximate the Koopman operator as a finite-dimensional matrix. In this work, we propose a method to compress the Koopman matrix using hierarchical clustering. Numerical demonstrations for the cart-pole model and comparisons with the conventional singular value decomposition (SVD) are shown; the results indicate that the hierarchical clustering performs better than the naive SVD compressions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages, 10 figures"
    },
    {
        "paper id": "2403.18188",
        "abstract url": "https://arxiv.org/abs/2403.18188",
        "title": "Integrating urban digital twins with cloud-based geospatial dashboards for coastal resilience planning: A case study in Florida",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Coastal communities are confronted with a growing incidence of climate-induced flooding, necessitating adaptation measures for resilience. In this paper, we introduce a framework that integrates an urban digital twin with a geospatial dashboard to allow visualization of the vulnerabilities within critical infrastructure across a range of spatial and temporal scales. The synergy between these two technologies fosters heightened community awareness about increased flood risks to establish a unified understanding, the foundation for collective decision-making in adaptation plans. The paper also elucidates ethical considerations while developing the platform, including ensuring accessibility, promoting transparency and equity, and safeguarding individual privacy.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18192",
        "abstract url": "https://arxiv.org/abs/2403.18192",
        "title": "Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains. Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches. However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch. Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses. Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data. However, batch selection methods have not yet been applied and validated in multi-label data. In this study, we introduce a simple yet effective adaptive batch selection algorithm tailored to multi-label deep learning models. It adaptively selects each batch by prioritizing hard samples related to minority labels. A variant of our method also takes informative label correlations into consideration. Comprehensive experiments combining five multi-label deep learning models on thirteen benchmark datasets show that our method converges faster and performs better than random batch selection.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18205",
        "abstract url": "https://arxiv.org/abs/2403.18205",
        "title": "Exploring the Privacy Protection Capabilities of Chinese Large Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models (LLMs), renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence. Yet, these advancements have raised growing concerns about privacy and security implications. To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems. This framework consists of progressively complex and in-depth privacy test tasks at each tier. Our primary objective is to comprehensively evaluate the sensitivity of large language models to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios. This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches. Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2403.18218",
        "abstract url": "https://arxiv.org/abs/2403.18218",
        "title": "Leveraging Large Language Models for Fuzzy String Matching in Political Science",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Fuzzy string matching remains a key issue when political scientists combine data from different sources. Existing matching methods invariably rely on string distances, such as Levenshtein distance and cosine similarity. As such, they are inherently incapable of matching strings that refer to the same entity with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and ''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In this letter, we propose to use large language models to entirely sidestep this problem in an easy and intuitive manner. Extensive experiments show that our proposed methods can improve the state of the art by as much as 39% in terms of average precision while being substantially easier and more intuitive to use by political scientists. Moreover, our results are robust against various temperatures. We further note that enhanced prompting can lead to additional performance improvements.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "7 pages, 2 figures, 1 table;"
    },
    {
        "paper id": "2403.18230",
        "abstract url": "https://arxiv.org/abs/2403.18230",
        "title": "Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models (LLMs), in conjunction with various reasoning reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge. In this paper, we delve into the reasoning abilities of LLMs within complex human systems. We propose a novel reasoning framework, termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting generative-agents-based simulation technique. In the MEOW framework, simulated data are utilized to train an expert model concentrating ``experience'' about a specific task in each independent time of simulation. It is the accumulated ``experience'' through the simulation that makes for an expert on a task in a complex human system. We conduct the experiments within a communication game that mirrors real-world security scenarios. The results indicate that our proposed methodology can cooperate with existing methodologies to enhance the reasoning abilities of LLMs in complex human systems.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18243",
        "abstract url": "https://arxiv.org/abs/2403.18243",
        "title": "Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge. Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied. In this paper, we propose a conversation-level RAG approach, which incorporates fine-grained retrieval augmentation and self-check for conversational question answering (CQA). In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings. Extensive experiments demonstrate the great advantages of our approach over the state-of-the-art baselines. Moreover, we also release a Chinese CQA dataset with new features including reformulated question, extracted keyword, retrieved paragraphs and their helpfulness, which facilitates further researches in RAG enhanced CQA.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17390",
        "abstract url": "https://arxiv.org/abs/2403.17390",
        "title": "SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework, which designed specifically for point cloud data. Leveraging the characteristics of non-coincidence and weak correlation of target objects in point cloud, we adopt a strategy of retaining only the truth-determining pseudo labels and trimming the other fuzzy labels with points, instead of pursuing a balance between the quantity and quality of pseudo labels. Besides, we notice that changing the filter will make the model meet different distributed targets, which is beneficial to break the training bottleneck. Two mechanism are introduced to achieve above ideas: strict threshold and filter switching. The experiments are conducted to analyze the effectiveness of above approaches and their impact on the overall performance of the system. Evaluating on the KITTI dataset, SSF3D exhibits superior performance compared to the current state-of-the-art methods. The code will be released here.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17431",
        "abstract url": "https://arxiv.org/abs/2403.17431",
        "title": "Robust and Scalable Model Editing for Large Language Models",
        "rating": 0,
        "keywords": [
            [
                "Model Editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. To better evaluate the robustness of model editors, we collect a new dataset, that contains irrelevant questions that are more challenging than the ones in existing datasets. Empirical results show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa). The source code can be found at https://github.com/thunlp/EREN.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024 paper, 16 pages, 4 figures"
    },
    {
        "paper id": "2403.17477",
        "abstract url": "https://arxiv.org/abs/2403.17477",
        "title": "DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360\u00b0 Images",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360\u00b0 images based on a conditional score-based denoising diffusion model. Generating human gaze on 360\u00b0 images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360\u00b0 images as condition and uses two transformers to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360\u00b0 image benchmarks for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks. We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17491",
        "abstract url": "https://arxiv.org/abs/2403.17491",
        "title": "DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The method of training language models based on domain datasets has obtained significant achievements in the task of generating scientific paper abstracts. However, such models face problems of generalization and expensive training costs. The use of large language models (LLMs) to solve the task of generating paper abstracts saves the cost of model training. However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs. In this paper, we propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages of the existing GoT prompt approach, but also dynamically adjust the graph structure according to data characteristics while reducing model reasoning cost. Experimental results show that our method's cost-effectiveness in abstract generation tasks is only 43.7% to 56.4% of other multi-round query prompt approaches. Our code is available at https://github.com/JayceNing/DGoT.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by LREC-COLING 2024"
    },
    {
        "paper id": "2403.17529",
        "abstract url": "https://arxiv.org/abs/2403.17529",
        "title": "Detection of Deepfake Environmental Audio",
        "rating": 0,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.SD"
            ]
        ],
        "abstract": "With the ever-rising quality of deep generative models, it is increasingly important to be able to discern whether the audio data at hand have been recorded or synthesized. Although the detection of fake speech signals has been studied extensively, this is not the case for the detection of fake environmental audio. We propose a simple and efficient pipeline for detecting fake environmental sounds based on the CLAP audio embedding. We evaluate this detector using audio data from the 2023 DCASE challenge task on Foley sound synthesis. Our experiments show that fake sounds generated by 44 state-of-the-art synthesizers can be detected on average with 98% accuracy. We show that using an audio embedding learned on environmental audio is beneficial over a standard VGGish one as it provides a 10% increase in detection performance. Informal listening to Incorrect Negative examples demonstrates audible features of fake sounds missed by the detector such as distortion and implausible background noise.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17647",
        "abstract url": "https://arxiv.org/abs/2403.17647",
        "title": "Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the evaluations of human assessors, acting as automatic metrics for the generated explanatory subgraphs. Our implementation is available at https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.17691",
        "abstract url": "https://arxiv.org/abs/2403.17691",
        "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Sc\u00e8nes \u00e0 faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing \"data-driven bias\" to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Presented at ACM CSLAW 2024"
    },
    {
        "paper id": "2403.17692",
        "abstract url": "https://arxiv.org/abs/2403.17692",
        "title": "Manifold-Guided Lyapunov Control with Diffusion Models",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using diffusion models. The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding. To achieve this, we employ a diffusion model trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions. Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast zero-shot control and generalizability.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2403.17702",
        "abstract url": "https://arxiv.org/abs/2403.17702",
        "title": "The Solution for the CVPR 2023 1st foundation model challenge-Track2",
        "rating": 0,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we propose a solution for cross-modal transportation retrieval. Due to the cross-domain problem of traffic images, we divide the problem into two sub-tasks of pedestrian retrieval and vehicle retrieval through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the base model and specifically design an Attribute Classification to mine the knowledge implied by attribute labels. More importantly, We use the strategy of Inclusion Relation Matching to make the image-text pairs with inclusion relation have similar representation in the feature space. For the vehicle retrieval task, we use BLIP as the base model. Since aligning the color attributes of vehicles is challenging, we introduce attribute-based object detection techniques to add color patch blocks to vehicle images for color data augmentation. This serves as strong prior information, helping the model perform the image-text alignment. At the same time, we incorporate labeled attributes into the image-text alignment loss to learn fine-grained alignment and prevent similar images and texts from being incorrectly separated. Our approach ranked first in the final B-board test with a score of 70.9.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17801",
        "abstract url": "https://arxiv.org/abs/2403.17801",
        "title": "Towards 3D Vision with Low-Cost Single-Photon Cameras",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a method for reconstructing 3D shape of arbitrary Lambertian objects based on measurements by miniature, energy-efficient, low-cost single-photon cameras. These cameras, operating as time resolved image sensors, illuminate the scene with a very fast pulse of diffuse light and record the shape of that pulse as it returns back from the scene at a high temporal resolution. We propose to model this image formation process, account for its non-idealities, and adapt neural rendering to reconstruct 3D geometry from a set of spatially distributed sensors with known poses. We show that our approach can successfully recover complex 3D shapes from simulated data. We further demonstrate 3D object reconstruction from real-world captures, utilizing measurements from a commodity proximity sensor. Our work draws a connection between image-based modeling and active range scanning and is a step towards 3D vision with single-photon cameras.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17804",
        "abstract url": "https://arxiv.org/abs/2403.17804",
        "title": "Improving Text-to-Image Consistency via Automatic Prompt Optimization",
        "rating": 0,
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17816",
        "abstract url": "https://arxiv.org/abs/2403.17816",
        "title": "Graph Language Model (GLM): A new graph-based approach to detect social instabilities",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This scientific report presents a novel methodology for the early prediction of important political events using News datasets. The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data. Initially, we designed a preliminary version of the method and tested it on a few events. This analysis revealed limitations in the initial research phase. We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data. After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests. Results demonstrate the superiority of our approach compared to baseline methods. Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17864",
        "abstract url": "https://arxiv.org/abs/2403.17864",
        "title": "Synthesizing Soundscapes: Leveraging Text-to-Audio Models for Environmental Sound Classification",
        "rating": 0,
        "keywords": [
            [
                "Synthesizing"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "In the past few years, text-to-audio models have emerged as a significant advancement in automatic audio generation. Although they represent impressive technological progress, the effectiveness of their use in the development of audio applications remains uncertain. This paper aims to investigate these aspects, specifically focusing on the task of classification of environmental sounds. This study analyzes the performance of two different environmental classification systems when data generated from text-to-audio models is used for training. Two cases are considered: a) when the training dataset is augmented by data coming from two different text-to-audio models; and b) when the training dataset consists solely of synthetic audio generated. In both cases, the performance of the classification task is tested on real data. Results indicate that text-to-audio models are effective for dataset augmentation, whereas the performance of the models drops when relying on only generated audio.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Submitted to EUSIPCO 2024"
    },
    {
        "paper id": "2403.17869",
        "abstract url": "https://arxiv.org/abs/2403.17869",
        "title": "To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for 3D transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principled understanding of both when and why 3D transfer learning methods are applicable. Remarkably, even the applicability of standard supervised pre-training is poorly understood. In this work, we conduct the first in-depth quantitative and qualitative investigation of supervised and contrastive pre-training strategies and their utility in downstream 3D tasks. We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks. Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of supervised pre-training. Our work thus sheds light onto both the specific challenges of 3D transfer learning, as well as strategies to overcome them.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17893",
        "abstract url": "https://arxiv.org/abs/2403.17893",
        "title": "A Survey on 3D Egocentric Human Pose Estimation",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Egocentric human pose estimation aims to estimate human body poses and develop body representations from a first-person camera perspective. It has gained vast popularity in recent years because of its wide range of applications in sectors like XR-technologies, human-computer interaction, and fitness tracking. However, to the best of our knowledge, there is no systematic literature review based on the proposed solutions regarding egocentric 3D human pose estimation. To that end, the aim of this survey paper is to provide an extensive overview of the current state of egocentric pose estimation research. In this paper, we categorize and discuss the popular datasets and the different pose estimation models, highlighting the strengths and weaknesses of different methods by comparative analysis. This survey can be a valuable resource for both researchers and practitioners in the field, offering insights into key concepts and cutting-edge solutions in egocentric pose estimation, its wide-ranging applications, as well as the open problems with future scope.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17898",
        "abstract url": "https://arxiv.org/abs/2403.17898",
        "title": "Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "Gaussian splatting",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations. While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum. This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details. Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation. Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results. Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://city-super.github.io/octree-gs/"
    },
    {
        "paper id": "2403.17921",
        "abstract url": "https://arxiv.org/abs/2403.17921",
        "title": "The Need for Speed: Pruning Transformers with One Recipe",
        "rating": 0.0,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. To address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitive accuracy performance and improved throughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP baselines and a $0.5$% improvement from state-of-the-art methods on image classification at competitive FLOPs reductions. We further demonstrate the generalization of tasks and architecture with comparative performance using Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents one of the first one-shot efficient frameworks for compressing transformer architectures that generalizes well across different class domains, in particular: natural language and image-related tasks, without $\\textit{re-training}$.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted in the International Conference on Learning Representations (ICLR) 2024"
    },
    {
        "paper id": "2403.17924",
        "abstract url": "https://arxiv.org/abs/2403.17924",
        "title": "AID: Attention Interpolation of Text-to-Image Diffusion",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at https://github.com/QY-H00/attention-interpolation-diffusion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17931",
        "abstract url": "https://arxiv.org/abs/2403.17931",
        "title": "Track Everything Everywhere Fast and Robustly",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video. The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications. OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence. To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions. While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models. Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process. Our experiments demonstrate a substantial improvement in training speed (more than \\textbf{10 times} faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "project page: https://timsong412.github.io/FastOmniTrack/"
    },
    {
        "paper id": "2403.17934",
        "abstract url": "https://arxiv.org/abs/2403.17934",
        "title": "AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh recovery) involves the human body, hand, and expression estimation. Most existing methods have tackled this task in a two-stage manner, first detecting the human body part with an off-the-shelf detection model and inferring the different human body parts individually. Despite the impressive results achieved, these methods suffer from 1) loss of valuable contextual information via cropping, 2) introducing distractions, and 3) lacking inter-association among different persons and body parts, inevitably causing performance degradation, especially for crowded scenes. To address these issues, we introduce a novel all-in-one-stage framework, AiOS, for multiple expressive human pose and shape recovery without an additional human detection step. Specifically, our method is built upon DETR, which treats multi-person whole-body mesh recovery task as a progressive set prediction problem with various sequential detection. We devise the decoder tokens and extend them to our task. Specifically, we first employ a human token to probe a human location in the image and encode global features for each instance, which provides a coarse location for the later transformer block. Then, we introduce a joint-related token to probe the human joint in the image and encoder a fine-grained local feature, which collaborates with the global feature to regress the whole-body mesh. This straightforward but effective model outperforms previous state-of-the-art methods by a 9% reduction in NMVE on AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a 3% reduction in PVE on EgoBody.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Homepage: https://ttxskk.github.io/AiOS/"
    },
    {
        "paper id": "2403.17994",
        "abstract url": "https://arxiv.org/abs/2403.17994",
        "title": "Solution for Point Tracking Task of ICCV 1st Perception Test Challenge 2023",
        "rating": 0,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This report proposes an improved method for the Tracking Any Point (TAP) task, which tracks any physical surface through a video. Several existing approaches have explored the TAP by considering the temporal relationships to obtain smooth point motion trajectories, however, they still suffer from the cumulative error caused by temporal prediction. To address this issue, we propose a simple yet effective approach called TAP with confident static points (TAPIR+), which focuses on rectifying the tracking of the static point in the videos shot by a static camera. To clarify, our approach contains two key components: (1) Multi-granularity Camera Motion Detection, which could identify the video sequence by the static camera shot. (2) CMR-based point trajectory prediction with one moving object segmentation approach to isolate the static point from the moving object. Our approach ranked first in the final test with a score of 0.46.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17995",
        "abstract url": "https://arxiv.org/abs/2403.17995",
        "title": "Semi-Supervised Image Captioning Considering Wasserstein Graph Matching",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features. Existing approaches are mostly supervised ones, i.e., each image has a corresponding sentence in the training set. However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., image-text pairs) and a large number of undescribed images in real-world applications. Thereby, a dilemma is the \"Semi-Supervised Image Captioning\". To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences. Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately comparable information among heterogeneous modalities. In this paper, SSIC-WGM adopts the successful scene graphs as intermediate information, and constrains the generated sentences from two aspects: 1) inter-modal consistency. SSIC-WGM constructs the scene graphs of the raw image and generated sentence respectively, then employs the wasserstein distance to better measure the similarity between region embeddings of different graphs. 2) intra-modal consistency. SSIC-WGM takes the data augmentation techniques for the raw images, then constrains the consistency among augmented images and generated sentences. Consequently, SSIC-WGM combines the cross-modal pseudo supervision and structure invariant measure for efficiently using the undescribed images, and learns more reasonable mapping function.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18040",
        "abstract url": "https://arxiv.org/abs/2403.18040",
        "title": "Global Point Cloud Registration Network for Large Transformations",
        "rating": 0,
        "keywords": [
            [
                "Point Cloud",
                "avatar"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Three-dimensional data registration is an established yet challenging problem that is key in many different applications, such as mapping the environment for autonomous vehicles, and modeling objects and people for avatar creation, among many others. Registration refers to the process of mapping multiple data into the same coordinate system by means of matching correspondences and transformation estimation. Novel proposals exploit the benefits of deep learning architectures for this purpose, as they learn the best features for the data, providing better matches and hence results. However, the state of the art is usually focused on cases of relatively small transformations, although in certain applications and in a real and practical environment, large transformations are very common. In this paper, we present ReLaTo (Registration for Large Transformations), an architecture that faces the cases where large transformations happen while maintaining good performance for local transformations. This proposal uses a novel Softmax pooling layer to find correspondences in a bilateral consensus manner between two point sets, sampling the most confident matches. These matches are used to estimate a coarse and global registration using weighted Singular Value Decomposition (SVD). A target-guided denoising step is then applied to both the obtained matches and latent features, estimating the final fine registration considering the local geometry. All these steps are carried out following an end-to-end approach, which has been shown to improve 10 state-of-the-art registration methods in two datasets commonly used for this task (ModelNet40 and KITTI), especially in the case of large transformations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18058",
        "abstract url": "https://arxiv.org/abs/2403.18058",
        "title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&A communities, Wikis, examinations, and existing NLP datasets. This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset. Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses. The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets. We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks. Data are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18080",
        "abstract url": "https://arxiv.org/abs/2403.18080",
        "title": "EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present EgoPoseFormer, a simple yet effective transformer-based model for stereo egocentric human pose estimation. The main challenge in egocentric pose estimation is overcoming joint invisibility, which is caused by self-occlusion or a limited field of view (FOV) of head-mounted cameras. Our approach overcomes this challenge by incorporating a two-stage pose estimation paradigm: in the first stage, our model leverages the global information to estimate each joint's coarse location, then in the second stage, it employs a DETR style transformer to refine the coarse locations by exploiting fine-grained stereo visual features. In addition, we present a deformable stereo operation to enable our transformer to effectively process multi-view features, which enables it to accurately localize each joint in the 3D world. We evaluate our method on the stereo UnrealEgo dataset and show it significantly outperforms previous approaches while being computationally efficient: it improves MPJPE by 27.4mm (45% improvement) with only 7.9% model parameters and 13.1% FLOPs compared to the state-of-the-art. Surprisingly, with proper training techniques, we find that even our first-stage pose proposal network can achieve superior performance compared to previous arts. We also show that our method can be seamlessly extended to monocular settings, which achieves state-of-the-art performance on the SceneEgo dataset, improving MPJPE by 25.5mm (21% improvement) compared to the best existing method with only 60.7% model parameters and 36.4% FLOPs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Tech Report"
    },
    {
        "paper id": "2403.18118",
        "abstract url": "https://arxiv.org/abs/2403.18118",
        "title": "EgoLifter: Open-world 3D Segmentation for Egocentric Perception",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Preprint. Project page: https://egolifter.github.io/"
    },
    {
        "paper id": "2403.18159",
        "abstract url": "https://arxiv.org/abs/2403.18159",
        "title": "Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models",
        "rating": 0.0,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ],
            [
                "Workshop",
                "ICLR"
            ]
        ],
        "abstract": "Large generative models such as large language models (LLMs) and diffusion models have revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that ov-freeze results in near floating point precision performance, i.e., less than 0.7% loss of accuracy on Commonsense Reasoning benchmarks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted at Practical ML for Low Resource Settings Workshop at ICLR 2024"
    },
    {
        "paper id": "2403.18186",
        "abstract url": "https://arxiv.org/abs/2403.18186",
        "title": "Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting",
        "rating": 0,
        "keywords": [
            [
                "synthesis",
                "Inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "cvpr 2024"
    },
    {
        "paper id": "2403.18187",
        "abstract url": "https://arxiv.org/abs/2403.18187",
        "title": "LayoutFlow: Flow Matching for Layout Generation",
        "rating": 0,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Finding a suitable layout represents a crucial task for diverse applications in graphic design. Motivated by simpler and smoother sampling trajectories, we explore the use of Flow Matching as an alternative to current diffusion-based layout generation models. Specifically, we propose LayoutFlow, an efficient flow-based model capable of generating high-quality layouts. Instead of progressively denoising the elements of a noisy layout, our method learns to gradually move, or flow, the elements of an initial sample until it reaches its final prediction. In addition, we employ a conditioning scheme that allows us to handle various generation tasks with varying degrees of conditioning with a single model. Empirically, LayoutFlow performs on par with state-of-the-art models while being significantly faster.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18208",
        "abstract url": "https://arxiv.org/abs/2403.18208",
        "title": "An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition",
        "rating": 0,
        "keywords": [
            [
                "Architecture Search"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Hand gesture recognition (HGR) based on multimodal data has attracted considerable attention owing to its great potential in applications. Various manually designed multimodal deep networks have performed well in multimodal HGR (MHGR), but most of existing algorithms require a lot of expert experience and time-consuming manual trials. To address these issues, we propose an evolutionary network architecture search framework with the adaptive multimodel fusion (AMF-ENAS). Specifically, we design an encoding space that simultaneously considers fusion positions and ratios of the multimodal data, allowing for the automatic construction of multimodal networks with different architectures through decoding. Additionally, we consider three input streams corresponding to intra-modal surface electromyography (sEMG), intra-modal accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to various datasets, the ENAS framework is designed to automatically search a MHGR network with appropriate fusion positions and ratios. To the best of our knowledge, this is the first time that ENAS has been utilized in MHGR to tackle issues related to the fusion position and ratio of multimodal data. Experimental results demonstrate that AMF-ENAS achieves state-of-the-art performance on the Ninapro DB2, DB3, and DB7 datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18228",
        "abstract url": "https://arxiv.org/abs/2403.18228",
        "title": "Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification",
        "rating": 0,
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "biologically"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial Transformer, whereby the Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower computational cost. However, it seems that self-attention is not always necessary, especially in sparse spike-form calculation manners. In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets. The FWformer can achieve comparable or even higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$ for training and $19\\%$-$70\\%$ for inference), reduced theoretical energy consumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$), compared to the standard spikformer. Our result indicates the continuous refinement of new Transformers, that are inspired either by biological discovery (spike-form), or information theory (Fourier or Wavelet Transform), is promising.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "18 pages, 2 figures. arXiv admin note: substantial text overlap with arXiv:2308.02557"
    },
    {
        "paper id": "2403.17436",
        "abstract url": "https://arxiv.org/abs/2403.17436",
        "title": "Particle identification with machine learning from incomplete data in the ALICE experiment",
        "rating": -0.5,
        "keywords": [
            [
                "workshop"
            ]
        ],
        "abstract": "The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.",
        "subjects": [
            "hep-ex"
        ],
        "comment": "Proceedings of 3rd Artificial Intelligence for the Electron Ion Collider workshop -- AI4EIC2023, 28.11-1.12.2023. Prepared for submission to JINST"
    },
    {
        "paper id": "2403.17456",
        "abstract url": "https://arxiv.org/abs/2403.17456",
        "title": "Imitating Cost-Constrained Behaviors in Reinforcement Learning",
        "rating": -0.5,
        "keywords": [
            [
                "trajectory",
                "vehicle"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the time available. In such problems, imitation learning is challenging as decisions are not only dictated by the reward model but are also dependent on a cost-constrained model. In this paper, we provide multiple methods that match expert distributions in the presence of trajectory cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to find a good trade-off between expected return and minimizing constraint violation; and (c) Cost-violation-based alternating gradient. We empirically show that leading imitation learning approaches imitate cost-constrained behaviors poorly and our meta-gradient-based approach achieves the best performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to the 34th International Conference on Automated Planning and Scheduling (ICAPS-24)"
    },
    {
        "paper id": "2403.17467",
        "abstract url": "https://arxiv.org/abs/2403.17467",
        "title": "A Unified Kernel for Neural Network Learning",
        "rating": -0.5,
        "keywords": [
            [
                "kernel learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoretically characterize the uniform tightness and learning convergence of the UNK kernel, providing comprehensive insights into this unified kernel. Experimental results underscore the effectiveness of our proposed method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17532",
        "abstract url": "https://arxiv.org/abs/2403.17532",
        "title": "KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The goal of knowledge graph completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative large language models (LLMs) have shown outstanding performance on several tasks such as information extraction and dialog systems. Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained knowledge and powerful generative capabilities. However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission. To this end, we introduce KC-GenRe, a knowledge-constrained generative re-ranking method based on LLMs for KGC. To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative LLMs. To tackle the misordering issue, we develop a knowledge-guided interactive training method that enhances the identification and ranking of candidates. To address the omission issue, we design a knowledge-augmented constrained inference method that enables contextual prompting and controlled generation, so as to obtain valid rankings. Experimental results show that KG-GenRe achieves state-of-the-art performance on four datasets, with gains of up to 6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and 9.0% and 11.1% compared to that without re-ranking. Extensive analysis demonstrates the effectiveness of components in KG-GenRe.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "This paper has been accepted for publication in the proceedings of LREC-COLING 2024"
    },
    {
        "paper id": "2403.17533",
        "abstract url": "https://arxiv.org/abs/2403.17533",
        "title": "BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat",
        "rating": -0.5,
        "keywords": [
            [
                "flight"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots' time. Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome. For this reason, we created a reinforcement learning environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym. This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat. Some existing environments provide high-fidelity simulations but are either not open source or are not adapted to the BVR air combat domain. Other environments are open source but use less accurate simulation models. Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat domain. This article describes the building blocks of the environment and some use cases.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 8 figures"
    },
    {
        "paper id": "2403.17537",
        "abstract url": "https://arxiv.org/abs/2403.17537",
        "title": "NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using Heuristics-Guided Segmentation",
        "rating": -0.5,
        "keywords": [
            [
                "3D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely \"Heuristics-Guided Segmentation\" (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "To appear in CVPR2024"
    },
    {
        "paper id": "2403.17572",
        "abstract url": "https://arxiv.org/abs/2403.17572",
        "title": "Enhancing Privacy in Federated Learning through Local Training",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper we propose the federated private local training algorithm (Fed-PLT) for federated learning, to overcome the challenges of (i) expensive communications and (ii) privacy preservation. We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents. The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy. Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent. Further, we investigate how employing local training can enhance privacy, addressing point (ii). In particular, we derive differential privacy bounds and highlight their dependence on the number of local training epochs. We assess the effectiveness of the proposed algorithm by comparing it to alternative techniques, considering both theoretical analysis and numerical results from a classification task.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17601",
        "abstract url": "https://arxiv.org/abs/2403.17601",
        "title": "LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation",
        "rating": -0.5,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware of learner state distribution. Our method, applied to urban traffic simulation, demonstrates significant improvements over existing state-of-the-art baselines in both short-term microscopic and long-term macroscopic realism when evaluated on the real-world dataset pNEUMA.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "My company has this rule: if the data for external use is not published on official website, it needs to be disclosed through external data disclosure. I have not done external data disclosure before submitting it"
    },
    {
        "paper id": "2403.17638",
        "abstract url": "https://arxiv.org/abs/2403.17638",
        "title": "Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency",
        "rating": -0.5,
        "keywords": [
            [
                "Voxel",
                "depth",
                "Radiance Fields"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We propose a voxel-based optimization framework, ReVoRF, for few-shot radiance fields that strategically address the unreliability in pseudo novel view synthesis. Our method pivots on the insight that relative depth relationships within neighboring regions are more reliable than the absolute color values in disoccluded areas. Consequently, we devise a bilateral geometric consistency loss that carefully navigates the trade-off between color fidelity and geometric accuracy in the context of depth consistency for uncertain regions. Moreover, we present a reliability-guided learning strategy to discern and utilize the variable quality across synthesized views, complemented by a reliability-aware voxel smoothing algorithm that smoothens the transition between reliable and unreliable data patches. Our approach allows for a more nuanced use of all available data, promoting enhanced learning from regions previously considered unsuitable for high-quality reconstruction. Extensive experiments across diverse datasets reveal that our approach attains significant gains in efficiency and accuracy, delivering rendering speeds of 3 FPS, 7 mins to train a $360^\\circ$ scene, and a 5\\% improvement in PSNR over existing few-shot methods. Code is available at https://github.com/HKCLynn/ReVoRF.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024 final version"
    },
    {
        "paper id": "2403.17654",
        "abstract url": "https://arxiv.org/abs/2403.17654",
        "title": "Improving the Spatial Correlation Characteristics of Antenna Arrays using Linear Operators and Wide-band Modelling",
        "rating": -0.5,
        "keywords": [
            [
                "Workshop"
            ]
        ],
        "abstract": "The analysis of wireless communication channels at the mmWave, sub-THz and THz bands gives rise to difficulties in the construction of antenna arrays due to the small maximum inter-element spacing constraints at these frequencies. Arrays with uniform spacing greater than half the wavelength for a certain carrier frequency exhibit aliasing side-lobes in the angular domain, prohibiting non-ambiguous estimates of a propagating wave-front's angle of arrival. In this paper, we present how wide-band modelling of the array response is useful in mitigating this spatial aliasing effect. This approach aims to reduce the grating lobes by exploiting the angle- and frequency-dependent phase-shifts observed in the response of the array to a planar wave-front travelling across it. Furthermore, we propose a method by which the spatial correlation characteristics of an array operating at 33 GHz carrier frequency with an instantaneous bandwidth of 1 GHz can be improved such that the angular-domain side-lobes are reduced by 5-10 dB. This method, applicable to arbitrary antenna array manifolds, makes use of a linear operator that is applied to the base-band samples of the channel transfer function measured in space and frequency domains. By means of synthetically simulated arrays, we show that when operating with a bandwidth of 1 GHz, the use of a derived linear operator applied to the array output results in the spatial correlation characteristics approaching those of the array operating at a bandwidth of 12 GHz. Hence, non-ambiguous angle estimates can be obtained in the field without the use of expensive high-bandwidth RF front-end components.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "7 pages, 4 figures, 27th Workshop on Smart Antennas, 2024"
    },
    {
        "paper id": "2403.17660",
        "abstract url": "https://arxiv.org/abs/2403.17660",
        "title": "CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Optimal Power Flow (OPF) refers to a wide range of related optimization problems with the goal of operating power systems efficiently and securely. In the simplest setting, OPF determines how much power to generate in order to minimize costs while meeting demand for power and satisfying physical and operational constraints. In even the simplest case, power grid operators use approximations of the AC-OPF problem because solving the exact problem is prohibitively slow with state-of-the-art solvers. These approximations sacrifice accuracy and operational feasibility in favor of speed. This trade-off leads to costly \"uplift payments\" and increased carbon emissions, especially for large power grids. In the present work, we train a deep learning system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF cost) without compromising speed (running in as little as 33--65 ms). Importantly, CANOS scales to realistic grid sizes with promising empirical results on grids containing as many as 10,000 buses. Finally, because CANOS is a Graph Neural Network, it is robust to changes in topology. We show that CANOS is accurate across N-1 topological perturbations of a base grid typically used in security-constrained analysis. This paves the way for more efficient optimization of more complex OPF problems which alter grid connectivity such as unit commitment, topology optimization and security-constrained OPF.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17735",
        "abstract url": "https://arxiv.org/abs/2403.17735",
        "title": "Out-of-distribution Rumor Detection via Test-Time Adaptation",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data distribution for model training. However, significant distribution shifts between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity. This leads to a substantial decline in the performance of these existing methods in Out-Of-Distribution (OOD) situations. To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under distribution shifts (TARD). This method models the propagation of news in the form of a propagation graph, and builds propagation graph test-time adaptation framework, enhancing the model's adaptability and robustness when facing OOD problems. Extensive experiments conducted on two group datasets collected from real-world social platforms demonstrate that our framework outperforms the state-of-the-art methods in performance.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17757",
        "abstract url": "https://arxiv.org/abs/2403.17757",
        "title": "Noise2Noise Denoising of CRISM Hyperspectral Data",
        "rating": -0.5,
        "keywords": [
            [
                "Hyperspectral Data"
            ],
            [
                "cs.CV"
            ],
            [
                "Workshop",
                "ICLR"
            ]
        ],
        "abstract": "Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024 ML4RS Workshop"
    },
    {
        "paper id": "2403.17778",
        "abstract url": "https://arxiv.org/abs/2403.17778",
        "title": "Towards a FAIR Documentation of Workflows and Models in Applied Mathematics",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics. The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows. MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template. Central to these workflows are mathematical models. MaRDI addresses them with the MathModDB ontology, offering a structured formal model description. Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities. This demonstration underscores the versatility of both services beyond their original numerical domain.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17833",
        "abstract url": "https://arxiv.org/abs/2403.17833",
        "title": "GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency. Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment. To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions. We also employ an Exploit-Explore mechanism to enhance performance. Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2403.17878",
        "abstract url": "https://arxiv.org/abs/2403.17878",
        "title": "Empowering Data Mesh with Federated Learning",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17902",
        "abstract url": "https://arxiv.org/abs/2403.17902",
        "title": "Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models",
        "rating": -0.5,
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "Image Restoration"
            ],
            [
                "eess.IV"
            ],
            [
                "workshop"
            ]
        ],
        "abstract": "The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms. However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while maintaining a compact model size.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "7 pages, 5 figures, preliminary workshop submission of a comprehensive work to be released soon"
    },
    {
        "paper id": "2403.17993",
        "abstract url": "https://arxiv.org/abs/2403.17993",
        "title": "Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "35 pages, 9 figures"
    },
    {
        "paper id": "2403.18036",
        "abstract url": "https://arxiv.org/abs/2403.18036",
        "title": "Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance",
        "rating": -0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024; 16 pages"
    },
    {
        "paper id": "2403.18056",
        "abstract url": "https://arxiv.org/abs/2403.18056",
        "title": "Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators. HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural networks. ECG is a three-layer graph consisting of an agent node layer, a cluster node layer, and a target node layer. To manipulate the ECG topology in response to changing environmental conditions, four graph operators are trained to adjust the edge connections of ECG dynamically. The hierarchical feature of ECG provides a unique approach to merge primitive actions (actions executed by the agents) and cooperative actions (actions executed by the clusters) into a unified action space, allowing us to integrate fundamental cooperative knowledge into an extensible interface. In our experiments, the HCGL model has shown outstanding performance in multi-agent benchmarks with sparse rewards. We also verify that HCGL can easily be transferred to large-scale scenarios with high zero-shot transfer success rates.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18103",
        "abstract url": "https://arxiv.org/abs/2403.18103",
        "title": "Tutorial on Diffusion Models for Imaging and Vision",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18142",
        "abstract url": "https://arxiv.org/abs/2403.18142",
        "title": "HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks",
        "rating": -0.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification method applicable to normalized and regularized graph Laplacians that ensures tighter bounds for our algorithm than existing spectral sparsifiers do. Experiments on real-world datasets verify the superiority of HERTA as well as its adaptability to various loss functions and optimizers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18145",
        "abstract url": "https://arxiv.org/abs/2403.18145",
        "title": "A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution",
        "rating": -0.5,
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "One area of research in multi-agent path finding is to determine how replanning can be efficiently achieved in the case of agents being delayed during execution. One option is to reschedule the passing order of agents, i.e., the sequence in which agents visit the same location. In response, we propose Switchable-Edge Search (SES), an A*-style algorithm designed to find optimal passing orders. We prove the optimality of SES and evaluate its efficiency via simulations. The best variant of SES takes less than 1 second for small- and medium-sized problems and runs up to 4 times faster than baselines for large-sized problems.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "ICAPS 2024"
    },
    {
        "paper id": "2403.18173",
        "abstract url": "https://arxiv.org/abs/2403.18173",
        "title": "LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices",
        "rating": -0.5,
        "keywords": [
            [
                "Workshop"
            ]
        ],
        "abstract": "Efficient and accurate information extraction from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new information retrieval system using state-of-the-art Large Language Models (LLMs) in combination with structured text analysis techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using LLMs in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified information of 300 CHI 2020-2022 papers, to evaluate the performance of the two large language models, GPT-3.5 (text-davinci-003) and Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model gains an accuracy of 58\\% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56\\% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by LLMs, our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for LLM use in HCI data work.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and Evaluations in HCI Data Work"
    },
    {
        "paper id": "2403.18191",
        "abstract url": "https://arxiv.org/abs/2403.18191",
        "title": "The process of polarisation as a loss of dimensionality: measuring changes in polarisation using Singular Value Decomposition of network graphs",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "The increasing polarisation in our societies is a major international concern. Current approaches to defining and detecting polarisation largely rely on finding evidence of bimodality in social networks or voter opinion surveys. It is difficult to detect temporal trends in polarisation, as the results usually fall into a binary of polarised or non-polarised, which cannot robustly show that subsequent increases in bimodality are statistically significant. Our work is aligned with Baldassari and Gelman's theory that polarisation should be defined as increasing correlation between positions in the ideological field. We also draw from post-structuralist work which argues that polarisation is the process of both the ideological and material layers of society being segregated into two poles, as in cases of apartheid. Thus, in order to measure the polarisation in a society, it would be beneficial to be able to assess social networks directly. In this paper we use Random Dot Product Graphs to embed social networks in metric spaces. In the case of a social network, the embedded dimensionality corresponds to the number of reasons any two people may form a social connection. A decrease in the optimal dimensionality for the embedding of the network graph, as measured using truncated Singular Value Decomposition of the graph adjacency matrix, indicates increasing polarisation in the network. We apply this method to two different Twitter networks based on discussions of climate change, and show that our methods agree with other researchers' detection of polarisation in this space. We also use networks generated by stochastic block models to explore how an increase of the isolation between distinct communities in a network, or the increase in the predominance of one community over the other, are identifiable as polarisation processes.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "24 pages, 7 figures, abstract presented at ICCS 2023"
    },
    {
        "paper id": "2403.18869",
        "abstract url": "https://arxiv.org/abs/2403.18869",
        "title": "Efficient Unsupervised Community Search with Pre-trained Graph Transformer",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Community search has aroused widespread interest in the past decades. Among existing solutions, the learning-based models exhibit outstanding performance in terms of accuracy by leveraging labels to 1) train the model for community score learning, and 2) select the optimal threshold for community identification. However, labeled data are not always available in real-world scenarios. To address this notable limitation of learning-based models, we propose a pre-trained graph Transformer based community search framework that uses Zero label (i.e., unsupervised), termed TransZero. TransZero has two key phases, i.e., the offline pre-training phase and the online search phase. Specifically, in the offline pretraining phase, we design an efficient and effective community search graph transformer (CSGphormer) to learn node representation. To pre-train CSGphormer without the usage of labels, we introduce two self-supervised losses, i.e., personalization loss and link loss, motivated by the inherent uniqueness of node and graph topology, respectively. In the online search phase, with the representation learned by the pre-trained CSGphormer, we compute the community score without using labels by measuring the similarity of representations between the query nodes and the nodes in the graph. To free the framework from the usage of a label-based threshold, we define a new function named expected score gain to guide the community identification process. Furthermore, we propose two efficient and effective algorithms for the community identification process that run without the usage of labels. Extensive experiments over 10 public datasets illustrate the superior performance of TransZero regarding both accuracy and efficiency.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18872",
        "abstract url": "https://arxiv.org/abs/2403.18872",
        "title": "Targeted Visualization of the Backbone of Encoder LLMs",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Attention based Large Language Models (LLMs) are the state-of-the-art in natural language processing (NLP). The two most common architectures are encoders such as BERT, and decoders like the GPT models. Despite the success of encoder models, on which we focus in this work, they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues. While there does exist various local explainability methods focusing on the prediction of single inputs, global methods based on dimensionality reduction for classification inspection, which have emerged in other domains and that go further than just using t-SNE in the embedding space, are not widely spread in NLP. To reduce this gap, we investigate the application of DeepView, a method for visualizing a part of the decision function together with a data set in two dimensions, to the NLP domain. While in previous work, DeepView has been used to inspect deep image classification models, we demonstrate how to apply it to BERT-based NLP classifiers and investigate its usability in this domain, including settings with adversarially perturbed input samples and pre-trained, fine-tuned, and multi-task models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18874",
        "abstract url": "https://arxiv.org/abs/2403.18874",
        "title": "Neural Attributed Community Search at Billion Scale",
        "rating": -0.5,
        "keywords": [
            [
                "GNNs",
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Community search has been extensively studied in the past decades. In recent years, there is a growing interest in attributed community search that aims to identify a community based on both the query nodes and query attributes. A set of techniques have been investigated. Though the recent methods based on advanced learning models such as graph neural networks (GNNs) can achieve state-of-the-art performance in terms of accuracy, we notice that 1) they suffer from severe efficiency issues; 2) they directly model community search as a node classification problem and thus cannot make good use of interdependence among different entities in the graph. Motivated by these, in this paper, we propose a new neurAL attrIbuted Community sEarch model for large-scale graphs, termed ALICE. ALICE first extracts a candidate subgraph to reduce the search scope and subsequently predicts the community by the Consistency-aware Net , termed ConNet. Specifically, in the extraction phase, we introduce the density sketch modularity that uses a unified form to combine the strengths of two existing powerful modularities, i.e., classical modularity and density modularity. Based on the new modularity metric, we first adaptively obtain the candidate subgraph, formed by the k-hop neighbors of the query nodes, with the maximum modularity. Then, we construct a node-attribute bipartite graph to take attributes into consideration. After that, ConNet adopts a cross-attention encoder to encode the interaction between the query and the graph. The training of the model is guided by the structure-attribute consistency and the local consistency to achieve better performance. Extensive experiments over 11 real-world datasets including one billion-scale graph demonstrate the superiority of ALICE in terms of accuracy, efficiency, and scalability.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.01559",
        "abstract url": "https://arxiv.org/abs/2405.01559",
        "title": "Untangling Knots: Leveraging LLM for Error Resolution in Computational Notebooks",
        "rating": -0.5,
        "keywords": [
            [
                "Workshop"
            ]
        ],
        "abstract": "Computational notebooks became indispensable tools for research-related development, offering unprecedented interactivity and flexibility in the development process. However, these benefits come at the cost of reproducibility and an increased potential for bugs. There are many tools for bug fixing; however, they are generally targeted at the classical linear code. With the rise of code-fluent Large Language Models, a new stream of smart bug-fixing tools has emerged. However, the applicability of those tools is still problematic for non-linear computational notebooks. In this paper, we propose a potential solution for resolving errors in computational notebooks via an iterative LLM-based agent. We discuss the questions raised by this approach and share a novel dataset of computational notebooks containing bugs to facilitate the research of the proposed approach.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "accepted at 1st ACM CHI Workshop on Human-Notebook Interactions"
    },
    {
        "paper id": "2403.17382",
        "abstract url": "https://arxiv.org/abs/2403.17382",
        "title": "Characterizing Dependency Update Practice of NPM, PyPI and Cargo Packages",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Keeping dependencies up-to-date prevents software supply chain attacks through outdated and vulnerable dependencies. Developers may use packages' dependency update practice as one of the selection criteria for choosing a package as a dependency. However, the lack of metrics characterizing packages' dependency update practice makes this assessment difficult. To measure the up-to-date characteristics of packages, we focus on the dependency management aspect and propose two update metrics: Time-Out-Of-Date (TOOD) and Post-Fix-Exposure-Time (PFET), to measure the updatedness of dependencies and updatedness of vulnerable dependencies, respectively. We design an algorithm to stabilize the dependency relationships in different time intervals and compute the proposed metrics for each package. Using our proposed metrics, we conduct a large-scale empirical study of update metrics with 2.9M packages, 66.8M package versions, and 26.8M unique package-dependency relations in NPM, PyPI, and Cargo, ranging from the year 2004 to 2023. We analyze the characteristics of the proposed metrics for capturing packages' dependency update practice in the three ecosystems. Given that the TOOD metric generates a greater volume of data than the PFET metric, we further explore the numerical relationship between these metrics to assess their potential as substitutes for vulnerability counts metrics. We find that PyPI packages update dependencies faster than NPM and Cargo. Conversely, Cargo packages update their vulnerable dependencies faster than NPM and PyPI. We also find that the general purpose update metric, TOOD, can be a proxy for the security-focused update metric, PFET.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "currently under review"
    },
    {
        "paper id": "2403.17413",
        "abstract url": "https://arxiv.org/abs/2403.17413",
        "title": "LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction",
        "rating": -1,
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Over-correction is a critical problem in Chinese grammatical error correction (CGEC) task. Recent work using model ensemble methods based on voting can effectively mitigate over-correction and improve the precision of the GEC system. However, these methods still require the output of several GEC systems and inevitably lead to reduced error recall. In this light, we propose the LM-Combiner, a rewriting model that can directly modify the over-correction of GEC system outputs without a model ensemble. Specifically, we train the model on an over-correction dataset constructed through the proposed K-fold cross inference method, which allows it to directly generate filtered sentences by combining the original and the over-corrected text. In the inference stage, we directly take the original sentences and the output results of other systems as input and then obtain the filtered sentences through LM-Combiner. Experiments on the FCGEC dataset show that our proposed method effectively alleviates the over-correction of the original system (+18.2 Precision) while ensuring the error recall remains unchanged. Besides, we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of black-box GEC systems (e.g., ChatGPT).",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to COLING 2024"
    },
    {
        "paper id": "2403.17421",
        "abstract url": "https://arxiv.org/abs/2403.17421",
        "title": "MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification",
        "rating": -1,
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\u03b1$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\u03b1$-NDCG, while achieving high training efficiency. We conducted preliminary experiments on public TREC datasets to demonstrate the effectiveness and potential of MA4DIV. Considering the limited number of queries in public TREC datasets, we construct a large-scale dataset from industry sources and show that MA4DIV achieves substantial improvements in both effectiveness and efficiency than existing baselines on a industrial scale dataset.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17423",
        "abstract url": "https://arxiv.org/abs/2403.17423",
        "title": "Test-time Adaptation Meets Image Enhancement: Improving Accuracy via Uncertainty-aware Logit Switching",
        "rating": -1,
        "keywords": [
            [
                "Image Enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep neural networks have achieved remarkable success in a variety of computer vision applications. However, there is a problem of degrading accuracy when the data distribution shifts between training and testing. As a solution of this problem, Test-time Adaptation~(TTA) has been well studied because of its practicality. Although TTA methods increase accuracy under distribution shift by updating the model at test time, using high-uncertainty predictions is known to degrade accuracy. Since the input image is the root of the distribution shift, we incorporate a new perspective on enhancing the input image into TTA methods to reduce the prediction's uncertainty. We hypothesize that enhancing the input image reduces prediction's uncertainty and increase the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the classification model is combined with the image enhancement model that transforms input images into recognition-friendly ones, and these models are updated by existing TTA methods. Furthermore, we found that the prediction from the enhanced image does not always have lower uncertainty than the prediction from the original image. Thus, we propose logit switching, which compares the uncertainty measure of these predictions and outputs the lower one. In our experiments, we evaluate TECA with various TTA methods and show that TECA reduces prediction's uncertainty and increases accuracy of TTA methods despite having no hyperparameters and little parameter overhead.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to IJCNN2024"
    },
    {
        "paper id": "2403.17432",
        "abstract url": "https://arxiv.org/abs/2403.17432",
        "title": "Integrating Mamba Sequence Model and Hierarchical Upsampling Network for Accurate Semantic Segmentation of Multiple Sclerosis Legion",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Integrating components from convolutional neural networks and state space models in medical image segmentation presents a compelling approach to enhance accuracy and efficiency. We introduce Mamba HUNet, a novel architecture tailored for robust and efficient segmentation tasks. Leveraging strengths from Mamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet), Mamba HUNet combines convolutional neural networks local feature extraction power with state space models long range dependency modeling capabilities. We first converted HUNet into a lighter version, maintaining performance parity and then integrated this lighter HUNet into Mamba HUNet, further enhancing its efficiency. The architecture partitions input grayscale images into patches, transforming them into 1D sequences for processing efficiency akin to Vision Transformers and Mamba models. Through Visual State Space blocks and patch merging layers, hierarchical features are extracted while preserving spatial information. Experimental results on publicly available Magnetic Resonance Imaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate Mamba HUNet's effectiveness across diverse segmentation tasks. The model's robustness and flexibility underscore its potential in handling complex anatomical structures. These findings establish Mamba HUNet as a promising solution in advancing medical image segmentation, with implications for improving clinical decision making processes.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2403.17448",
        "abstract url": "https://arxiv.org/abs/2403.17448",
        "title": "Adaptive Line-Of-Sight guidance law based on vector fields path following for underactuated unmanned surface vehicle",
        "rating": -1,
        "keywords": [
            [
                "trajectory",
                "vehicle"
            ]
        ],
        "abstract": "The focus of this paper is to develop a methodology that enables an unmanned surface vehicle (USV) to efficiently track a planned path. The introduction of a vector field-based adaptive line of-sight guidance law (VFALOS) for accurate trajectory tracking and minimizing the overshoot response time during USV tracking of curved paths improves the overall line-of-sight (LOS) guidance method. These improvements contribute to faster convergence to the desired path, reduce oscillations, and can mitigate the effects of persistent external disturbances. It is shown that the proposed guidance law exhibits k-exponential stability when converging to the desired path consisting of straight and curved lines. The results in the paper show that the proposed method effectively improves the accuracy of the USV tracking the desired path while ensuring the safety of the USV work.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17452",
        "abstract url": "https://arxiv.org/abs/2403.17452",
        "title": "Five-fingered Hand with Wide Range of Thumb Using Combination of Machined Springs and Variable Stiffness Joints",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Human hands can not only grasp objects of various shape and size and manipulate them in hands but also exert such a large gripping force that they can support the body in the situations such as dangling a bar and climbing a ladder. On the other hand, it is difficult for most robot hands to manage both. Therefore in this paper we developed the hand which can grasp various objects and exert large gripping force. To develop such hand, we focused on the thumb CM joint with wide range of motion and the MP joints of four fingers with the DOF of abduction and adduction. Based on the hand with large gripping force and flexibility using machined spring, we applied above mentioned joint mechanism to the hand. The thumb CM joint has wide range of motion because of the combination of three machined springs and MP joints of four fingers have variable rigidity mechanism instead of driving each joint independently in order to move joint in limited space and by limited actuators. Using the developed hand, we achieved the grasping of various objects, supporting a large load and several motions with an arm.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "accepted at IROS2018"
    },
    {
        "paper id": "2403.17459",
        "abstract url": "https://arxiv.org/abs/2403.17459",
        "title": "High-Power, Flexible, Robust Hand: Development of Musculoskeletal Hand Using Machined Springs and Realization of Self-Weight Supporting Motion with Humanoid",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Human can not only support their body during standing or walking, but also support them by hand, so that they can dangle a bar and others. But most humanoid robots support their body only in the foot and they use their hand just to manipulate objects because their hands are too weak to support their body. Strong hands are supposed to enable humanoid robots to act in much broader scene. Therefore, we developed new life-size five-fingered hand that can support the body of life-size humanoid robot. It is tendon-driven and underactuated hand and actuators in forearms produce large gripping force. This hand has flexible joints using machined springs, which can be designed integrally with the attachment. Thus, it has both structural strength and impact resistance in spite of small size. As other characteristics, this hand has force sensors to measure external force and the fingers can be flexed along objects though the number of actuators to flex fingers is less than that of fingers. We installed the developed hand on musculoskeletal humanoid \"Kengoro\" and achieved two self-weight supporting motions: push-up motion and dangling motion.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "accepted at IROS2017"
    },
    {
        "paper id": "2403.17512",
        "abstract url": "https://arxiv.org/abs/2403.17512",
        "title": "Random-coupled Neural Network",
        "rating": -1,
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Improving the efficiency of current neural networks and modeling them in biological neural systems have become popular research directions in recent years. Pulse-coupled neural network (PCNN) is a well applicated model for imitating the computation characteristics of the human brain in computer vision and neural network fields. However, differences between the PCNN and biological neural systems remain: limited neural connection, high computational cost, and lack of stochastic property. In this study, random-coupled neural network (RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic computing via a random inactivation process. This process randomly closes some neural connections in the RCNN model, realized by the random inactivation weight matrix of link input. This releases the computational burden of PCNN, making it affordable to achieve vast neural connections. Furthermore, the image and video processing mechanisms of RCNN are researched. It encodes constant stimuli as periodic spike trains and periodic stimuli as chaotic spike trains, the same as biological neural information encoding characteristics. Finally, the RCNN is applicated to image segmentation, fusion, and pulse shape discrimination subtasks. It is demonstrated to be robust, efficient, and highly anti-noised, with outstanding performance in all applications mentioned above.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17525",
        "abstract url": "https://arxiv.org/abs/2403.17525",
        "title": "Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation",
        "rating": -1,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The drawing order of a sketch records how it is created stroke-by-stroke by a human being. For graphic sketch representation learning, recent studies have injected sketch drawing orders into graph edge construction by linking each patch to another in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since a sketch could have variants of drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for learning graphic sketch representation. Instead of injecting sketch drawings into graph edges, we embed these sequential information into graph nodes only. More specifically, each patch embedding is equipped with a sinusoidal absolute PE to highlight the sequential position in the drawing order. And its neighboring patches, ranked by the values of self-attention scores between patch embeddings, are equipped with learnable relative PEs to restore the contextual positions within a neighborhood. During message aggregation via graph convolutional networks, a node receives both semantic contents from patch embeddings and contextual patterns from PEs by its neighbors, arriving at drawing-order-enhanced sketch representations. Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17530",
        "abstract url": "https://arxiv.org/abs/2403.17530",
        "title": "Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification",
        "rating": -1,
        "keywords": [
            [
                "Medical",
                "MRI",
                "cancer",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level. This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing. We demonstrate the effectiveness of the proposed approach through a series of experiments exploring several backbones, as well as diverse pre-training and fine-tuning schemes, on two distinct medical tasks, i.e., classification of prostate cancer aggressiveness from MRI data and classification of breast cancer malignity from microscopic images. Results: Our results indicate that the proposed approach consistently yields superior performance w.r.t. ablation experiments, maintaining competitiveness even when a distribution shift between training and evaluation data occurs. Conclusion: Extensive experiments demonstrate the effectiveness and wide applicability of the proposed approach. We hope that this work will add another solution to the arsenal of addressing learning issues in data-scarce imaging domains.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024"
    },
    {
        "paper id": "2403.17534",
        "abstract url": "https://arxiv.org/abs/2403.17534",
        "title": "Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks",
        "rating": -1,
        "keywords": [
            [
                "Grammar"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Descriptive grammars are highly valuable, but writing them is time-consuming and difficult. Furthermore, while linguists typically use corpora to create them, grammar descriptions often lack quantitative data. As for formal grammars, they can be challenging to interpret. In this paper, we propose a new method to extract and explore significant fine-grained grammar patterns and potential syntactic grammar rules from treebanks, in order to create an easy-to-understand corpus-based grammar. More specifically, we extract descriptions and rules across different languages for two linguistic phenomena, agreement and word order, using a large search space and paying special attention to the ranking order of the extracted rules. For that, we use a linear classifier to extract the most salient features that predict the linguistic phenomena under study. We associate statistical information to each rule, and we compare the ranking of the model's results to those of other quantitative and statistical measures. Our method captures both well-known and less well-known significant grammar rules in Spanish, French, and Wolof.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Published in LREC-Coling 2024 proceedings"
    },
    {
        "paper id": "2403.17540",
        "abstract url": "https://arxiv.org/abs/2403.17540",
        "title": "Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction",
        "rating": -1,
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC). In this study, we investigate the performance of LLMs in GEC evaluation by employing prompts designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that GPT-4 achieved Kendall's rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent GEC evaluations, we have underscored the significance of the LLMs scale and particularly emphasized the importance of fluency among evaluation criteria.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17541",
        "abstract url": "https://arxiv.org/abs/2403.17541",
        "title": "WordRobe: Text-Guided Generation of Textured 3D Garments",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "synthesize"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures. We propose \"WordRobe\", a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text prompts. We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation. Subsequently, we align the garment latent space to the CLIP embedding space in a weakly supervised manner, enabling text-driven 3D garment generation and editing. For appearance modeling, we leverage the zero-shot generation capability of ControlNet to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods. We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study. The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth simulation & animation pipelines without any post-processing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17546",
        "abstract url": "https://arxiv.org/abs/2403.17546",
        "title": "Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining",
        "rating": -1,
        "keywords": [
            [
                "psychological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals. We use innovative methods and tools of text mining and social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics. The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs, text mining techniques, and an innovative measure, namely, the Semantic Brand Score. We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics. In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the seniority level of the open position in order to understand their nuances. Finally, topic modeling is used to examine key components and their relative significance in job descriptions. By employing a novel methodology and considering contextual factors, we provide an innovative understanding of the attitudinal traits that differentiate professionals. This research contributes to talent management, recruitment practices, and professional development initiatives, since it provides new figures and perspectives to improve the effectiveness and success of Operations Management and Supply Chain Management professionals.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17550",
        "abstract url": "https://arxiv.org/abs/2403.17550",
        "title": "DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks. The code of our approach will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures"
    },
    {
        "paper id": "2403.17565",
        "abstract url": "https://arxiv.org/abs/2403.17565",
        "title": "Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control via Spectral Method Model",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "In this work, we present a model-based optimal boundary control design for an aerial robotic system composed of a quadrotor carrying a flexible cable. The whole system is modeled by partial differential equations (PDEs) combined with boundary conditions described by ordinary differential equations (ODEs). The proper orthogonal decomposition (POD) method is adopted to project the original infinite-dimensional system on a subspace spanned by orthogonal basis functions. Based on the reduced order model, nonlinear model predictive control (NMPC) is implemented online to realize shape trajectory tracking of the flexible cable in an optimal predictive fashion. The proposed reduced modeling and optimal control paradigms are numerically verified against an accurate high-dimensional FDM-based model in different scenarios and the controller's superior performance is shown compared to an optimally tuned PID controller.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17567",
        "abstract url": "https://arxiv.org/abs/2403.17567",
        "title": "Piecewise Linear Expectation Analysis via $k$-Induction for Probabilistic Programs",
        "rating": -1,
        "keywords": [
            [
                "synthesizing"
            ]
        ],
        "abstract": "Quantitative analysis of probabilistic programs aims at deriving tight numerical bounds for probabilistic properties such as expectation and assertion probability, and plays a crucial role in the verification of probabilistic programs. Along this line of research, most existing works consider numerical bounds over the whole state space monolithically and do not consider piecewise bounds. Clearly, monolithic bounds are either conservative, or not expressive and succinct enough in general. To derive more succinct, expressive and precise numerical bounds for probabilistic properties, we propose a novel approach for synthesizing piecewise linear bounds in this work. To this end, we first show how to extract a piecewise feature w.r.t. a given quantitative property from a probabilistic program using latticed $k$-induction that captures a wide and representative class of piecewise bound functions. Second, we develop an algorithmic approach to synthesize piecewise linear upper and lower bounds from the piecewise feature, for which we show that the synthesis of piecewise linear bounds can be reduced to bilinear programming. Third, we implement our approach with the bilinear programming solver Gurobi. The experimental results indicate that our approach is capable of generating tight or even accurate piecewise linear bounds for an extensive set of benchmarks compared with the state of the art.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17568",
        "abstract url": "https://arxiv.org/abs/2403.17568",
        "title": "A Caro-Wei bound for induced linear forests in graphs",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A well-known result due to Caro (1979) and Wei (1981) states that every graph $G$ has an independent set of size at least $\\sum_{v\\in V(G)} \\frac{1}{d(v) + 1}$, where $d(v)$ denotes the degree of vertex $v$. Alon, Kahn, and Seymour (1987) showed the following generalization: For every $k\\geq 0$, every graph $G$ has a $k$-degenerate induced subgraph with at least $\\sum_{v \\in V(G)}\\min\\{1, \\frac {k+1}{d(v)+1}\\}$ vertices. In particular, for $k=1$, every graph $G$ with no isolated vertices has an induced forest with at least $\\sum_{v\\in V(G)} \\frac{2}{d(v) + 1}$ vertices. Akbari, Amanihamedani, Mousavi, Nikpey, and Sheybani (2019) conjectured that, if $G$ has minimum degree at least $2$, then one can even find an induced linear forest of that order in $G$, that is, a forest where each component is a path. In this paper, we prove this conjecture and show a number of related results. In particular, if there is no restriction on the minimum degree of $G$, we show that there are infinitely many ``best possible'' functions $f$ such that $\\sum_{v\\in V(G)} f(d(v))$ is a lower bound on the maximum order of a linear forest in $G$, and we give a full characterization of all such functions $f$.",
        "subjects": [
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17582",
        "abstract url": "https://arxiv.org/abs/2403.17582",
        "title": "Towards a Zero-Data, Controllable, Adaptive Dialog System",
        "rating": -1,
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Conversational Tree Search (V\u00e4th et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms. Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17606",
        "abstract url": "https://arxiv.org/abs/2403.17606",
        "title": "Interactive Identification of Granular Materials using Force Measurements",
        "rating": -1,
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "The ability to identify granular materials facilitates the emergence of various new applications in robotics, ranging from cooking at home to truck loading at mining sites. However, granular material identification remains a challenging and underexplored area. In this work, we present a novel interactive material identification framework that enables robots to identify a wide range of granular materials using only a force-torque sensor for perception. Our framework, comprising interactive exploration, feature extraction, and classification stages, prioritizes simplicity and transparency for seamless integration into various manipulation pipelines. We evaluate the proposed approach through extensive experiments with a real-world dataset comprising 11 granular materials, which we also make publicly available. Additionally, we conducted a comprehensive qualitative analysis of the dataset to offer deeper insights into its nature, aiding future development. Our results show that the proposed method is capable of accurately identifying a wide range of granular materials solely relying on force measurements obtained from direct interaction with the materials. Code and dataset are available at: https://irobotics.aalto.fi/indentify_granular/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)"
    },
    {
        "paper id": "2403.17627",
        "abstract url": "https://arxiv.org/abs/2403.17627",
        "title": "Waveform Design for Joint Communication and SAR Imaging Under Random Signaling",
        "rating": -1,
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "Conventional synthetic aperture radar (SAR) imaging systems typically employ deterministic signal designs, which lack the capability to convey communication information and are thus not suitable for integrated sensing and communication (ISAC) scenarios. In this letter, we propose a joint communication and SAR imaging (JCASAR) system based on orthogonal frequency-division multiplexing (OFDM) signal with cyclic prefix (CP), which is capable of reconstructing the target profile while serving a communication user. In contrast to traditional matched filters, we propose a least squares (LS) estimator for range profiling. Then the SAR image is obtained followed by range cell migration correction (RCMC) and azimuth processing. By minimizing the mean squared error (MSE) of the proposed LS estimator, we investigate the optimal waveform design for SAR imaging, and JCASAR under random signaling, where power allocation strategies are conceived for Gaussian-distributed ISAC signals, in an effort to strike a flexible performance tradeoff between the communication and SAR imaging tasks. Numerical results are provided to validate the effectiveness of the proposed ISAC waveform design for JCASAR systems.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2403.17658",
        "abstract url": "https://arxiv.org/abs/2403.17658",
        "title": "Generalising the maximum independent set algorithm via Boolean networks",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A simple greedy algorithm to find a maximal independent set (MIS) in a graph starts with the empty set and visits every vertex, adding it to the set if and only if none of its neighbours are already in the set. In this paper, we consider the generalisation of this MIS algorithm by letting it start with any set of vertices and we prove the hardness of many decision problems related to this generalisation. Our results are based on two main strategies. Firstly, we view the MIS algorithm as a sequential update of a Boolean network, which we refer to as the MIS network, according to a permutation of the vertex set. The set of fixed points of the MIS network corresponds to the set of MIS of the graph. Our generalisation then consists in starting from any configuration and following a sequential update given by a word of vertices. Secondly, we introduce the concept of a colony of a graph, that is a set of vertices that is dominated by an independent set. Deciding whether a set of vertices is a colony is NP-complete; decision problems related to the MIS algorithm will be reduced from the Colony problem. We first show that deciding whether a configuration can reach all maximal independent sets is coNP-complete. Second, we consider so-called fixing words, that allow to reach a MIS for any initial configuration, and fixing permutations, which we call permises; deciding whether a permutation is fixing is coNP-complete. Third, we show that deciding whether a graph has a permis is coNP-hard. Finally, we generalise the MIS algorithm to digraphs. The algorithm then uses the so-called kernel network, whose fixed points are the kernels of the digraph. Deciding whether the kernel network of a given digraph is fixable is coNP-hard, even for digraphs that have a kernel. Alternatively, we introduce two fixable Boolean networks whose sets of fixed points contain all kernels.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "Significantly extends arxiv:2307.05216"
    },
    {
        "paper id": "2403.17667",
        "abstract url": "https://arxiv.org/abs/2403.17667",
        "title": "Learning Goal-Directed Object Pushing in Cluttered Scenes with Location-Based Attention",
        "rating": -1,
        "keywords": [
            [
                "robot",
                "navigation"
            ]
        ],
        "abstract": "Non-prehensile planar pushing is a challenging task due to its underactuated nature with hybrid-dynamics, where a robot needs to reason about an object's long-term behaviour and contact-switching, while being robust to contact uncertainty. The presence of clutter in the environment further complicates this task, introducing the need to include more sophisticated spatial analysis to avoid collisions. Building upon prior work on reinforcement learning (RL) with multimodal categorical exploration for planar pushing, in this paper we incorporate location-based attention to enable robust navigation through clutter. Unlike previous RL literature addressing this obstacle avoidance pushing task, our framework requires no predefined global paths and considers the target orientation of the manipulated object. Our results demonstrate that the learned policies successfully navigate through a wide range of complex obstacle configurations, including dynamic obstacles, with smooth motions, achieving the desired target object pose. We also validate the transferability of the learned policies to robotic hardware using the KUKA iiwa robot arm.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
        "paper id": "2403.17677",
        "abstract url": "https://arxiv.org/abs/2403.17677",
        "title": "Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention",
        "rating": -1,
        "keywords": [
            [
                "hyperspectral images"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning methods have traditionally been difficult to apply to compression of hyperspectral images onboard of spacecrafts, due to the large computational complexity needed to achieve adequate representational power, as well as the lack of suitable datasets for training and testing. In this paper, we depart from the traditional autoencoder approach and we design a predictive neural network, called LineRWKV, that works recursively line-by-line to limit memory consumption. In order to achieve that, we adopt a novel hybrid attentive-recursive operation that combines the representational advantages of Transformers with the linear complexity and recursive implementation of recurrent neural networks. The compression algorithm performs prediction of each pixel using LineRWKV, followed by entropy coding of the residual. Experiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV is the first deep-learning method to outperform CCSDS-123.0-B-2 at lossless and near-lossless compression. Promising throughput results are also evaluated on a 7W embedded system.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17693",
        "abstract url": "https://arxiv.org/abs/2403.17693",
        "title": "ExpressEdit: Video Editing with Natural Language and Sketching",
        "rating": -1,
        "keywords": [
            [
                "Video Editing"
            ]
        ],
        "abstract": "Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality$-$natural language (NL) and sketching, which are natural modalities humans use for expression$-$can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame. Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching. The system implements the interpreted edits, which then the user can iterate on. An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas. The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user's multimodal edit commands and supporting iterations on the editing commands. This work offers insights into the design of future multimodal interfaces and AI-based pipelines for video editing.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "22 pages, 5 figures, to be published in ACM IUI 2024"
    },
    {
        "paper id": "2403.17701",
        "abstract url": "https://arxiv.org/abs/2403.17701",
        "title": "Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation",
        "rating": -1,
        "keywords": [
            [
                "Medical",
                "diagnosis"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets, demonstrating the superior segmentation performance of our proposed TM-UNet. Additionally, compared to the previous VM-UNet, our model achieves a one-third reduction in parameters.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Experimental method encountered errors, undergoing experiment again"
    },
    {
        "paper id": "2403.17708",
        "abstract url": "https://arxiv.org/abs/2403.17708",
        "title": "Panonut360: A Head and Eye Tracking Dataset for Panoramic Video",
        "rating": -1,
        "keywords": [
            [
                "quality assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions. Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos. The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data. The dataset is available on website: https://dianvrlab.github.io/Panonut360/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "7 pages,ACM MMSys'24 accepted"
    },
    {
        "paper id": "2403.17710",
        "abstract url": "https://arxiv.org/abs/2403.17710",
        "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
        "rating": -1,
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17748",
        "abstract url": "https://arxiv.org/abs/2403.17748",
        "title": "UCxn: Typologically Informed Annotation of Constructions Atop Universal Dependencies",
        "rating": -1,
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The Universal Dependencies (UD) project has created an invaluable collection of treebanks with contributions in over 140 languages. However, the UD annotations do not tell the full story. Grammatical constructions that convey meaning through a particular combination of several morphosyntactic elements -- for example, interrogative sentences with special markers and/or word orders -- are not labeled holistically. We argue for (i) augmenting UD annotations with a 'UCxn' annotation layer for such meaning-bearing grammatical constructions, and (ii) approaching this in a typologically informed way so that morphosyntactic strategies can be compared across languages. As a case study, we consider five construction families in ten languages, identifying instances of each construction in UD treebanks through the use of morphosyntactic patterns. In addition to findings regarding these particular constructions, our study yields important insights on methodology for describing and identifying constructions in language-general and language-particular ways, and lays the foundation for future constructional enrichment of UD treebanks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024"
    },
    {
        "paper id": "2403.17765",
        "abstract url": "https://arxiv.org/abs/2403.17765",
        "title": "MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash Representations",
        "rating": -1,
        "keywords": [
            [
                "RGB-D"
            ],
            [
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing multiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM effectively tracks camera positions and incrementally builds a scalable multi-map representation for both small and large indoor environments. It dynamically allocates sub-maps for newly observed local regions, enabling constraint-free mapping without prior scene information. Unlike traditional grid-based methods, we use three orthogonal axis-aligned planes for hash-encoding scene properties, significantly reducing hash collisions and the number of trainable parameters. This hybrid approach not only speeds up convergence but also enhances the fidelity of surface reconstruction. Furthermore, our optimization strategy concurrently optimizes all sub-maps intersecting with the current camera frustum, ensuring global consistency. Extensive testing on both real-world and synthetic datasets has shown that MUTE-SLAM delivers state-of-the-art surface reconstruction quality and competitive tracking performance across diverse indoor settings. The code will be made public upon acceptance of the paper.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17766",
        "abstract url": "https://arxiv.org/abs/2403.17766",
        "title": "Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We study the computational limits of the following general hypothesis testing problem. Let H=H_n be an \\emph{arbitrary} undirected graph on n vertices. We study the detection task between a ``null'' Erd\u0151s-R\u00e9nyi random graph G(n,p) and a ``planted'' random graph which is the union of G(n,p) together with a random copy of H=H_n. Our notion of planted model is a generalization of a plethora of recently studied models initiated with the study of the planted clique model (Jerrum 1992), which corresponds to the special case where H is a k-clique and p=1/2. Over the last decade, several papers have studied the power of low-degree polynomials for limited choices of H's in the above task. In this work, we adopt a unifying perspective and characterize the power of \\emph{constant degree} polynomials for the detection task, when \\emph{H=H_n is any arbitrary graph} and for \\emph{any p=\u03a9(1).} Perhaps surprisingly, we prove that the optimal constant degree polynomial is always given by simply \\emph{counting stars} in the input random graph. As a direct corollary, we conclude that the class of constant-degree polynomials is only able to ``sense'' the degree distribution of the planted graph H, and no other graph theoretic property of it.",
        "subjects": [
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17779",
        "abstract url": "https://arxiv.org/abs/2403.17779",
        "title": "Optical Flow Based Detection and Tracking of Moving Objects for Autonomous Vehicles",
        "rating": -1,
        "keywords": [
            [
                "point cloud"
            ]
        ],
        "abstract": "Accurate velocity estimation of surrounding moving objects and their trajectories are critical elements of perception systems in Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These are non-trivial problems due to the diverse types and sizes of such objects and their dynamic and random behaviour. Recent point cloud based solutions often use Iterative Closest Point (ICP) techniques, which are known to have certain limitations. For example, their computational costs are high due to their iterative nature, and their estimation error often deteriorates as the relative velocities of the target objects increase (>2 m/sec). Motivated by such shortcomings, this paper first proposes a novel Detection and Tracking of Moving Objects (DATMO) for AVs based on an optical flow technique, which is proven to be computationally efficient and highly accurate for such problems. \\textcolor{black}{This is achieved by representing the driving scenario as a vector field and applying vector calculus theories to ensure spatiotemporal continuity.} We also report the results of a comprehensive performance evaluation of the proposed DATMO technique, carried out in this study using synthetic and real-world data. The results of this study demonstrate the superiority of the proposed technique, compared to the DATMO techniques in the literature, in terms of estimation accuracy and processing time in a wide range of relative velocities of moving objects. Finally, we evaluate and discuss the sensitivity of the estimation error of the proposed DATMO technique to various system and environmental parameters, as well as the relative velocities of the moving objects.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This manuscript has been accepted as a regular paper in Transactions on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)"
    },
    {
        "paper id": "2403.17780",
        "abstract url": "https://arxiv.org/abs/2403.17780",
        "title": "CaseLink: Inductive Graph Learning for Legal Case Retrieval",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "In case law, the precedents are the relevant cases that are used to support the decisions made by the judges and the opinions of lawyers towards a given case. This relevance is referred to as the case-to-case reference relation. To efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners. Existing legal case retrieval models mainly work by comparing the text representations of individual cases. Although they obtain a decent retrieval accuracy, the intrinsic case connectivity relationships among cases have not been well exploited for case encoding, therefore limiting the further improvement of retrieval performance. In a case pool, there are three types of case connectivity relationships: the case reference relationship, the case semantic relationship, and the case legal charge relationship. Due to the inductive manner in the task of legal case retrieval, using case reference as input is not applicable for testing. Thus, in this paper, a CaseLink model based on inductive graph learning is proposed to utilise the intrinsic case connectivity for legal case retrieval, a novel Global Case Graph is incorporated to represent both the case semantic relationship and the case legal charge relationship. A novel contrastive objective with a regularisation on the degree of case nodes is proposed to leverage the information carried by the case reference relationship to optimise the model. Extensive experiments have been conducted on two benchmark datasets, which demonstrate the state-of-the-art performance of CaseLink. The code has been released on https://github.com/yanran-tang/CaseLink.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17782",
        "abstract url": "https://arxiv.org/abs/2403.17782",
        "title": "GenesisTex: Adapting Image Denoising Diffusion to Texture Space",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion",
                "synthesizing",
                "inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions. GenesisTex adapts the pretrained image diffusion model to texture space by texture space sampling. Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint. The sampled latent texture maps are then decoded into a final texture map. During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures. Finally, we apply reference-based inpainting and img2img on denser views for texture refinement. Our approach overcomes the limitations of slow optimization in distillation-based methods and instability in inpainting-based methods. Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 10 figures"
    },
    {
        "paper id": "2403.17793",
        "abstract url": "https://arxiv.org/abs/2403.17793",
        "title": "Neural Exponential Stabilization of Control-affine Nonlinear Systems",
        "rating": -1,
        "keywords": [
            [
                "synthesize"
            ]
        ],
        "abstract": "This paper proposes a novel learning-based approach for achieving exponential stabilization of nonlinear control-affine systems. We leverage the Control Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction Metrics (NCMs) and Neural Network (NN) controllers. First, we transform the infinite-dimensional semi-definite program (SDP) for CCM computation into a tractable inequality feasibility problem using element-wise bounds of matrix-valued functions. The terms in the inequality can be efficiently computed by our novel algorithms. Second, we propose a free parametrization of NCMs guaranteeing positive definiteness and the satisfaction of a partial differential equation, regardless of trainable parameters. Third, this parametrization and the inequality condition enable the design of contractivity-enforcing regularizers, which can be incorporated while designing the NN controller for exponential stabilization of the underlying nonlinear systems. Furthermore, when the training loss goes to zero, we provide formal guarantees on verification of the NCM and the exponentional stabilization under the NN controller. Finally, we validate our method through benchmark experiments on set-point stabilization and increasing the region of attraction of a locally pre-stabilized closed-loop system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This paper is submitted in CDC2024 for a possible publication"
    },
    {
        "paper id": "2403.17800",
        "abstract url": "https://arxiv.org/abs/2403.17800",
        "title": "Steering Feedback in Dynamic Driving Simulators: The Influence of Steering Wheel Vibration and Vehicle Motion Frequency",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "The validity of the subjective evaluation of steering feedback in driving simulators is crucial for modern vehicle development. Although there are established objective steering characteristics for the assessment of both stationary and dynamic feedback behaviour, factors such as steering wheel vibrations and vehicle body motion, particularly in high-frequency ranges, present challenges in simulator fidelity. This work investigates the influence of steering wheel vibration and vehicle body motion frequency content on the subjective evaluation of steering feedback during closed-loop driving in a dynamic driving simulator. A controlled subject study with 30 participants consisting of a back-to-back comparison of a reference vehicle with an electrical power steering system and three variants of its virtual representation on a dynamic driving simulator was performed. Subjective evaluation focused on the representation of road feedback in comparison to the reference vehicle. The statistical analysis of subjective results show that there is a significant influence of the frequency content of both steering wheel torque and vehicle motion on the subjective evaluation of steering feedback in a dynamic driving simulator. The results suggest an influence of frequency content on the subjective evaluation quality of steering feedback characteristics that are not associated with the dynamic feedback behaviour in the context of established performance indicators.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "12 pages, 7 figures, 9 tables, submitted to the IEEE Transactions on Intelligent Vehicles"
    },
    {
        "paper id": "2403.17805",
        "abstract url": "https://arxiv.org/abs/2403.17805",
        "title": "Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving",
        "rating": -1,
        "keywords": [
            [
                "Autonomous Driving"
            ]
        ],
        "abstract": "The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks. Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies. However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex simulation environments. In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator. MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents. This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated with techniques from unsupervised environment design to automate the generation of adaptive auto-curricula. The code is available at https://github.com/AutonomousDrivingExaminer/mats-gym.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 Pages, Under Review"
    },
    {
        "paper id": "2403.17822",
        "abstract url": "https://arxiv.org/abs/2403.17822",
        "title": "DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "Depth"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in https://github.com/maturk/dn-splatter.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17827",
        "abstract url": "https://arxiv.org/abs/2403.17827",
        "title": "DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion",
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be prompted in the interaction phase. For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions. Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://diffh2o.github.io/"
    },
    {
        "paper id": "2403.17849",
        "abstract url": "https://arxiv.org/abs/2403.17849",
        "title": "Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial Vehicles",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple agents from respective start to goal locations such that no paths conflict. We address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles which are subject to location-dependent noise restrictions. We solve this problem by searching a constraint tree for which the subproblem at each node is a set of shortest path problems subject to the noise and fuel constraints and conflict zone avoidance. A labeling algorithm is presented to solve this subproblem, including the conflict zones which are treated as dynamic obstacles. We present the experimental results of the algorithms for various graph sizes and number of agents.",
        "subjects": [
            "math.OC"
        ],
        "comment": "6 pages, 7 figures"
    },
    {
        "paper id": "2403.17851",
        "abstract url": "https://arxiv.org/abs/2403.17851",
        "title": "A survey on road safety and traffic efficiency vehicular applications based on C-V2X technologies",
        "rating": -1,
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "In recent years, the use of cellular network technologies to provide communication-based applications to vehicles has received considerable attention. 3GPP, the standardization body responsible for cellular networks specifications, is developing technologies to meet the requirements of vehicular communication applications, and the research community is testing and validating the ability of those technologies to implement different applications. This survey presents the body of work dealing with the use of cellular technologies to implement communication-based applications for the connected vehicle. We focus on basic and advanced road safety and traffic efficiency applications, which are critically important for the future of vehicular networks. We start by describing the different cellular-related technologies that have a role to play in providing services to the connected vehicle, propose a classification of types of communication used in vehicular applications, and then apply this classification to organize and present recent research work on the topic. Finally, we identify the main challenges in the use of cellular technologies to develop applications for the connected vehicle.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17861",
        "abstract url": "https://arxiv.org/abs/2403.17861",
        "title": "Stealthy Deactivation of Safety Filters",
        "rating": -1,
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Safety filters ensure that only safe control actions are executed. We propose a simple and stealthy false-data injection attack for deactivating such safety filters; in particular, we focus on deactivating safety filters that are based on control-barrier functions. The attack injects false sensor measurements to bias state estimates to the interior of a safety region, which makes the safety filter accept unsafe control actions. To detect such attacks, we also propose a detector that detects biases manufactured by the proposed attack policy, which complements conventional detectors when safety filters are used. The proposed attack policy and detector are illustrated on a double integrator example.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "ECC24"
    },
    {
        "paper id": "2403.17883",
        "abstract url": "https://arxiv.org/abs/2403.17883",
        "title": "Superior and Pragmatic Talking Face Generation with Teacher-Student Framework",
        "rating": -1,
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Talking face generation technology creates talking videos from arbitrary appearance and motion signal, with the \"arbitrary\" offering ease of use but also introducing challenges in practical applications. Existing methods work well with standard inputs but suffer serious performance degradation with intricate real-world ones. Moreover, efficiency is also an important concern in deployment. To comprehensively address these issues, we introduce SuperFace, a teacher-student framework that balances quality, robustness, cost and editability. We first propose a simple but effective teacher model capable of handling inputs of varying qualities to generate high-quality results. Building on this, we devise an efficient distillation strategy to acquire an identity-specific student model that maintains quality with significantly reduced computational load. Our experiments validate that SuperFace offers a more comprehensive solution than existing methods for the four mentioned objectives, especially in reducing FLOPs by 99\\% with the student model. SuperFace can be driven by both video and audio and allows for localized facial attributes editing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17884",
        "abstract url": "https://arxiv.org/abs/2403.17884",
        "title": "Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data",
        "rating": -1,
        "keywords": [
            [
                "remote sensing",
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications. To advance the development of machine learning algorithms in this domain, our study introduces the \\textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection. This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each patch has a size of 512$\\times$512 pixels with 13 bands. Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI). The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance. Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection. The code and dataset are available online (https://zenodo.org/records/10881058).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17888",
        "abstract url": "https://arxiv.org/abs/2403.17888",
        "title": "2D Gaussian Splatting for Geometrically Accurate Radiance Fields",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "depth",
                "Radiance Fields"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering. Our code will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 12 figures"
    },
    {
        "paper id": "2403.17909",
        "abstract url": "https://arxiv.org/abs/2403.17909",
        "title": "ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection",
        "rating": -1,
        "keywords": [
            [
                "Remote Sensing",
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning has shown remarkable success in remote sensing change detection (CD), aiming to identify semantic change regions between co-registered satellite image pairs acquired at distinct time stamps. However, existing convolutional neural network and transformer-based frameworks often struggle to accurately segment semantic change regions. Moreover, transformers-based methods with standard self-attention suffer from quadratic computational complexity with respect to the image resolution, making them less practical for CD tasks with limited training data. To address these issues, we propose an efficient change detection framework, ELGC-Net, which leverages rich contextual information to precisely estimate change regions while reducing the model size. Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The focus of our design is the introduction of an Efficient Local-Global Context Aggregator module within the encoder, capturing enhanced global context and local spatial information through a novel pooled-transpose (PT) attention and depthwise convolution, respectively. The PT attention employs pooling operations for robust feature extraction and minimizes computational cost with transposed attention. Extensive experiments on three challenging CD datasets demonstrate that ELGC-Net outperforms existing methods. Compared to the recent transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in intersection over union metric on the LEVIR-CD dataset, while significantly reducing trainable parameters. Our proposed ELGC-Net sets a new state-of-the-art performance in remote sensing change detection benchmarks. Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly reduced computational complexity, suitable for resource-constrained settings, while achieving comparable performance. Project url https://github.com/techmn/elgcnet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "accepted at IEEE TGRS"
    },
    {
        "paper id": "2403.17916",
        "abstract url": "https://arxiv.org/abs/2403.17916",
        "title": "CMP: Cooperative Motion Prediction with Multi-Agent Communication",
        "rating": -1,
        "keywords": [
            [
                "LiDAR",
                "Vehicle"
            ]
        ],
        "abstract": "The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction. Through extensive experiments and ablation studies, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction tasks. In particular, CMP reduces the average prediction error by 17.2\\% with fewer missing detections compared with the no cooperation setting. Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18015",
        "abstract url": "https://arxiv.org/abs/2403.18015",
        "title": "A Constructive Method for Designing Safe Multirate Controllers for Differentially-Flat Systems",
        "rating": -1,
        "keywords": [
            [
                "synthesize"
            ]
        ],
        "abstract": "We present a multi-rate control architecture that leverages fundamental properties of differential flatness to synthesize controllers for safety-critical nonlinear dynamical systems. We propose a two-layer architecture, where the high-level generates reference trajectories using a linear Model Predictive Controller, and the low-level tracks this reference using a feedback controller. The novelty lies in how we couple these layers, to achieve formal guarantees on recursive feasibility of the MPC problem, and safety of the nonlinear system. Furthermore, using differential flatness, we provide a constructive means to synthesize the multi-rate controller, thereby removing the need to search for suitable Lyapunov or barrier functions, or to approximately linearize/discretize nonlinear dynamics. We show the synthesized controller is a convex optimization problem, making it amenable to real-time implementations. The method is demonstrated experimentally on a ground rover and a quadruped robotic system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 3 figures, accepted at IEEE Control Systems Letters 2021"
    },
    {
        "paper id": "2403.18025",
        "abstract url": "https://arxiv.org/abs/2403.18025",
        "title": "Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER",
        "rating": -1,
        "keywords": [
            [
                "Biomedical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Paper alrerady accepted for publishing by the NAACL 2024 conference (main conference paper)"
    },
    {
        "paper id": "2403.18066",
        "abstract url": "https://arxiv.org/abs/2403.18066",
        "title": "Path Integral Control with Rollout Clustering and Dynamic Obstacles",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Model Predictive Path Integral (MPPI) control has proven to be a powerful tool for the control of uncertain systems (such as systems subject to disturbances and systems with unmodeled dynamics). One important limitation of the baseline MPPI algorithm is that it does not utilize simulated trajectories to their fullest extent. For one, it assumes that the average of all trajectories weighted by their performance index will be a safe trajectory. In this paper, multiple examples are shown where the previous assumption does not hold, and a trajectory clustering technique is presented that reduces the chances of the weighted average crossing in an unsafe region. Secondly, MPPI does not account for dynamic obstacles, so the authors put forward a novel cost function that accounts for dynamic obstacles without adding significant computation time to the overall algorithm. The novel contributions proposed in this paper were evaluated with extensive simulations to demonstrate improvements upon the state-of-the-art MPPI techniques.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 5 figures, extended version of ACC 2024 submission"
    },
    {
        "paper id": "2403.18071",
        "abstract url": "https://arxiv.org/abs/2403.18071",
        "title": "From Sontag s to Cardano-Lyapunov Formula for Systems Not Affine in the Control: Convection-Enabled PDE Stabilization",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "We propose the first generalization of Sontag s universal controller to systems not affine in the control, particularly, to PDEs with boundary actuation. We assume that the system admits a control Lyapunov function (CLF) whose derivative, rather than being affine in the control, has either a depressed cubic, quadratic, or depressed quartic dependence on the control. For each case, a continuous universal controller that vanishes at the origin and achieves global exponential stability is derived. We prove our result in the context of convectionreaction-diffusion PDEs with Dirichlet actuation. We show that if the convection has a certain structure, then the L2 norm of the state is a CLF. In addition to generalizing Sontag s formula to some non-affine systems, we present the first general Lyapunov approach for boundary control of nonlinear PDEs. We illustrate our results via a numerical example.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "To be presented at the 2024 American Control Conference"
    },
    {
        "paper id": "2403.18072",
        "abstract url": "https://arxiv.org/abs/2403.18072",
        "title": "Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field.",
        "subjects": [
            "stat.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18086",
        "abstract url": "https://arxiv.org/abs/2403.18086",
        "title": "Generalizing Better Response Paths and Weakly Acyclic Games",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Weakly acyclic games generalize potential games and are fundamental to the study of game theoretic control. In this paper, we present a generalization of weakly acyclic games, and we observe its importance in multi-agent learning when agents employ experimental strategy updates in periods where they fail to best respond. While weak acyclicity is defined in terms of path connectivity properties of a game's better response graph, our generalization is defined using a generalized better response graph. We provide sufficient conditions for this notion of generalized weak acyclicity in both two-player games and $n$-player games. To demonstrate that our generalization is not trivial, we provide examples of games admitting a pure Nash equilibrium that are not generalized weakly acyclic. The generalization presented in this work is closely related to the recent theory of satisficing paths, and the counterexamples presented here constitute the first negative results in that theory.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18096",
        "abstract url": "https://arxiv.org/abs/2403.18096",
        "title": "Efficient Multi-Band Temporal Video Filter for Reducing Human-Robot Interaction",
        "rating": -1,
        "keywords": [
            [
                "Robot",
                "navigation"
            ]
        ],
        "abstract": "Although mobile robots have on-board sensors to perform navigation, their efficiency in completing paths can be enhanced by planning to avoid human interaction. Infrastructure cameras can capture human activity continuously for the purpose of compiling activity analytics to choose efficient times and routes. We describe a cascade temporal filtering method to efficiently extract short- and long-term activity in two time dimensions, isochronal and chronological, for use in global path planning and local navigation respectively. The temporal filter has application either independently, or, if object recognition is also required, it can be used as a pre-filter to perform activity-gating of the more computationally expensive neural network processing. For a testbed 32-camera network, we show how this hybrid approach can achieve over 8 times improvement in frames per second throughput and 6.5 times reduction of system power use. We also show how the cost map of static objects in the ROS robot software development framework is augmented with dynamic regions determined from the temporal filter.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "15 pages, 5 figures, 4 tables"
    },
    {
        "paper id": "2403.18111",
        "abstract url": "https://arxiv.org/abs/2403.18111",
        "title": "Scrolly2Reel: Turning News Graphics into TikToks by Adjusting Narrative Beats and Pacing",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "As media evolves, storytelling evolves. In 2012, newspapers introduced scrollytelling sequences, or \"scrollies,\" to make news more immersive and interactive on the web. As users scroll through an article, graphics like animation, charts, and 3D visualizations appear to provide visual dynamics to the story. Today, news consumption is shifting to short-video platforms like TikTok, particularly among younger audiences. We propose repurposing the assets from scrollies and computationally transform them into videos. By shortening the original written text and precisely synchronizing the timing of audio narrative with features in the visual scrolling assets, we can create reels with dynamic pacing and narrative beats. We argue that text shortening is essential to producing fast paced videos that are compelling and visually interesting, and show that when beats are preserved in the output reel, topical alignment between them and the visual assets is crucial to the viewing experience. Understanding narrative pacing and beats in creative forms is key to user experience of media. They are an important primitive to effective editing, repurposing, and retargeting content while maintaining a cohesive narrative.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "9 pages, 3 figures"
    },
    {
        "paper id": "2403.18130",
        "abstract url": "https://arxiv.org/abs/2403.18130",
        "title": "Generalized Maximum Entropy Differential Dynamic Programming",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "We present a sampling-based trajectory optimization method derived from the maximum entropy formulation of Differential Dynamic Programming with Tsallis entropy. This method can be seen as a generalization of the legacy work with Shannon entropy, which leads to a Gaussian optimal control policy for exploration during optimization. With the Tsallis entropy, the optimal control policy takes the form of $q$-Gaussian, which further encourages exploration with its heavy-tailed shape. Moreover, in our formulation, the exploration variance, which was scaled by a fixed constant inverse temperature in the original formulation with Shannon entropy, is automatically scaled based on the value function of the trajectory. Due to this property, our algorithms can promote exploration when necessary, that is, the cost of the trajectory is high, rather than using the same scaling factor. The simulation results demonstrate the properties of the proposed algorithm described above.",
        "subjects": [
            "math.OC"
        ],
        "comment": "7 pages, 5 figures, This paper is for CDC 2024"
    },
    {
        "paper id": "2403.18162",
        "abstract url": "https://arxiv.org/abs/2403.18162",
        "title": "Optimizing Cyber Response Time on Temporal Active Directory Networks Using Decoys",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Microsoft Active Directory (AD) is the default security management system for Window domain network. We study the problem of placing decoys in AD network to detect potential attacks. We model the problem as a Stackelberg game between an attacker and a defender on AD attack graphs where the defender employs a set of decoys to detect the attacker on their way to Domain Admin (DA). Contrary to previous works, we consider time-varying (temporal) attack graphs. We proposed a novel metric called response time, to measure the effectiveness of our decoy placement in temporal attack graphs. Response time is defined as the duration from the moment attackers trigger the first decoy to when they compromise the DA. Our goal is to maximize the defender's response time to the worst-case attack paths. We establish the NP-hard nature of the defender's optimization problem, leading us to develop Evolutionary Diversity Optimization (EDO) algorithms. EDO algorithms identify diverse sets of high-quality solutions for the optimization problem. Despite the polynomial nature of the fitness function, it proves experimentally slow for larger graphs. To enhance scalability, we proposed an algorithm that exploits the static nature of AD infrastructure in the temporal setting. Then, we introduce tailored repair operations, ensuring the convergence to better results while maintaining scalability for larger graphs.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "To be appear in ACM GECCO 2024"
    },
    {
        "paper id": "2403.18166",
        "abstract url": "https://arxiv.org/abs/2403.18166",
        "title": "Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An Auction-Based Approach",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The rise of advanced air mobility (AAM) is expected to become a multibillion-dollar industry in the near future. Market-based mechanisms are touted to be an integral part of AAM operations, which comprise heterogeneous operators with private valuations. In this work, we study the problem of designing a mechanism to coordinate the movement of electric vertical take-off and landing (eVTOL) aircraft, operated by multiple operators each having heterogeneous valuations associated with their fleet, between vertiports, while enforcing the arrival, departure, and parking constraints at vertiports. Particularly, we propose an incentive-compatible and individually rational vertiport reservation mechanism that maximizes a social welfare metric, which encapsulates the objective of maximizing the overall valuations of all operators while minimizing the congestion at vertiports. Additionally, we improve the computational tractability of designing the reservation mechanism by proposing a mixed binary linear programming approach that is based on constructing network flow graph corresponding to the underlying problem.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "26 pages, 2 figures, 1 table"
    },
    {
        "paper id": "2403.18180",
        "abstract url": "https://arxiv.org/abs/2403.18180",
        "title": "Multi-Layer Dense Attention Decoder for Polyp Segmentation",
        "rating": -1,
        "keywords": [
            [
                "diagnosis",
                "cancer"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting and segmenting polyps is crucial for expediting the diagnosis of colon cancer. This is a challenging task due to the large variations of polyps in color, texture, and lighting conditions, along with subtle differences between the polyp and its surrounding area. Recently, vision Transformers have shown robust abilities in modeling global context for polyp segmentation. However, they face two major limitations: the inability to learn local relations among multi-level layers and inadequate feature aggregation in the decoder. To address these issues, we propose a novel decoder architecture aimed at hierarchically aggregating locally enhanced multi-level dense features. Specifically, we introduce a novel module named Dense Attention Gate (DAG), which adaptively fuses all previous layers' features to establish local feature relations among all layers. Furthermore, we propose a novel nested decoder architecture that hierarchically aggregates decoder features, thereby enhancing semantic features. We incorporate our novel dense decoder with the PVT backbone network and conduct evaluations on five polyp segmentation datasets: Kvasir, CVC-300, CVC-ColonDB, CVC-ClinicDB, and ETIS. Our experiments and comparisons with nine competing segmentation models demonstrate that the proposed architecture achieves state-of-the-art performance and outperforms the previous models on four datasets. The source code is available at: https://github.com/krushi1992/Dense-Decoder.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18196",
        "abstract url": "https://arxiv.org/abs/2403.18196",
        "title": "Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities",
        "rating": -1.0,
        "keywords": [
            [
                "Health",
                "diagnosis",
                "X-ray",
                "disease"
            ],
            [
                "cs.LG"
            ],
            [
                "ICCV"
            ]
        ],
        "abstract": "There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that our framework achieves an optimal tradeoff between accuracy and fairness compared to baseline methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICCV CVAMD 2023"
    },
    {
        "paper id": "2403.18198",
        "abstract url": "https://arxiv.org/abs/2403.18198",
        "title": "Generative Medical Segmentation",
        "rating": -1,
        "keywords": [
            [
                "Medical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Rapid advancements in medical image segmentation performance have been significantly driven by the development of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, these models introduce high computational demands and often have limited ability to generalize across diverse medical imaging datasets. In this manuscript, we introduce Generative Medical Segmentation (GMS), a novel approach leveraging a generative model for image segmentation. Concretely, GMS employs a robust pre-trained Variational Autoencoder (VAE) to derive latent representations of both images and masks, followed by a mapping model that learns the transition from image to mask in the latent space. This process culminates in generating a precise segmentation mask within the image space using the pre-trained VAE decoder. The design of GMS leads to fewer learnable parameters in the model, resulting in a reduced computational burden and enhanced generalization capability. Our extensive experimental analysis across five public datasets in different medical imaging domains demonstrates GMS outperforms existing discriminative segmentation models and has remarkable domain generalization. Our experiments suggest GMS could set a new benchmark for medical image segmentation, offering a scalable and effective solution. GMS implementation and model weights are available at https://github.com/King-HAW/GMS.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18207",
        "abstract url": "https://arxiv.org/abs/2403.18207",
        "title": "Road Obstacle Detection based on Unknown Objectness Scores",
        "rating": -1,
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "anomaly detection"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ICRA 2024"
    },
    {
        "paper id": "2403.18212",
        "abstract url": "https://arxiv.org/abs/2403.18212",
        "title": "Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies",
        "rating": -1,
        "keywords": [
            [
                "synthesize"
            ]
        ],
        "abstract": "Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to transform a partially ordered preference over temporal goals into a computational model, called preference automaton, which is a semi-automaton with a partial order over acceptance conditions. In the second step, we prove that finding a most preferred policy is equivalent to computing a Pareto-optimal policy in a multi-objective MDP that is constructed from the original MDP, the preference automaton, and the chosen stochastic ordering relation. Throughout the paper, we employ running examples to illustrate the proposed preference specification and solution approaches. We demonstrate the efficacy of our algorithm using these examples, providing detailed analysis, and then discuss several potential future directions.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2209.12267"
    },
    {
        "paper id": "2403.18233",
        "abstract url": "https://arxiv.org/abs/2403.18233",
        "title": "Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data",
        "rating": -1,
        "keywords": [
            [
                "Cancer"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in ultrasound images typically employ convolutional networks (CNNs) to detect cancer in small regions of interest (ROI) along a needle trace region. However, this approach suffers from weak labelling, since the ground-truth histopathology labels do not describe the properties of individual ROIs. Recently, multi-scale approaches have sought to mitigate this issue by combining the context awareness of transformers with a CNN feature extractor to detect cancer from multiple ROIs using multiple-instance learning (MIL). In this work, we present a detailed study of several image transformer architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs and transformers for ultrasound-based prostate cancer classification. We also design a novel multi-objective learning strategy that combines both ROI and core predictions to further mitigate label noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer classification, then use the strongest model to tune a multi-scale classifier with MIL. We train our MIL models using our novel multi-objective learning strategy and compare our results to existing baselines. RESULTS: We find that for both ROI-scale and multi-scale PCa detection, image transformer backbones lag behind their CNN counterparts. This deficit in performance is even more noticeable for larger models. When using multi-objective learning, we can improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for modelling sparse datasets of prostate ultrasounds, producing more robust features than transformers in PCa detection. Multi-scale methods remain the best architecture for this task, with multi-objective learning presenting an effective way to improve performance.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "early draft, 7 pages; Accepted to SPIE Medical Imaging 2024"
    },
    {
        "paper id": "2403.18236",
        "abstract url": "https://arxiv.org/abs/2403.18236",
        "title": "Multi-AGV Path Planning Method via Reinforcement Learning and Particle Filters",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "The Reinforcement Learning (RL) algorithm, renowned for its robust learning capability and search stability, has garnered significant attention and found extensive application in Automated Guided Vehicle (AGV) path planning. However, RL planning algorithms encounter challenges stemming from the substantial variance of neural networks caused by environmental instability and significant fluctuations in system structure. These challenges manifest in slow convergence speed and low learning efficiency. To tackle this issue, this paper presents the Particle Filter-Double Deep Q-Network (PF-DDQN) approach, which incorporates the Particle Filter (PF) into multi-AGV reinforcement learning path planning. The PF-DDQN method leverages the imprecise weight values of the network as state values to formulate the state space equation. Through the iterative fusion process of neural networks and particle filters, the DDQN model is optimized to acquire the optimal true weight values, thus enhancing the algorithm's efficiency. The proposed method's effectiveness and superiority are validated through numerical simulations. Overall, the simulation results demonstrate that the proposed algorithm surpasses the traditional DDQN algorithm in terms of path planning superiority and training time indicators by 92.62% and 76.88%, respectively. In conclusion, the PF-DDQN method addresses the challenges encountered by RL planning algorithms in AGV path planning. By integrating the Particle Filter and optimizing the DDQN model, the proposed method achieves enhanced efficiency and outperforms the traditional DDQN algorithm in terms of path planning superiority and training time indicators.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18238",
        "abstract url": "https://arxiv.org/abs/2403.18238",
        "title": "TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes",
        "rating": -1,
        "keywords": [
            [
                "remote sensing",
                "drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "As drone technology advances, using unmanned aerial vehicles for aerial surveys has become the dominant trend in modern low-altitude remote sensing. The surge in aerial video data necessitates accurate prediction for future scenarios and motion states of the interested target, particularly in applications like traffic management and disaster response. Existing video prediction methods focus solely on predicting future scenes (video frames), suffering from the neglect of explicitly modeling target's motion states, which is crucial for aerial video interpretation. To address this issue, we introduce a novel task called Target-Aware Aerial Video Prediction, aiming to simultaneously predict future scenes and motion states of the target. Further, we design a model specifically for this task, named TAFormer, which provides a unified modeling approach for both video and target motion states. Specifically, we introduce Spatiotemporal Attention (STA), which decouples the learning of video dynamics into spatial static attention and temporal dynamic attention, effectively modeling the scene appearance and motion. Additionally, we design an Information Sharing Mechanism (ISM), which elegantly unifies the modeling of video and target motion by facilitating information interaction through two sets of messenger tokens. Moreover, to alleviate the difficulty of distinguishing targets in blurry predictions, we introduce Target-Sensitive Gaussian Loss (TSGL), enhancing the model's sensitivity to both target's position and content. Extensive experiments on UAV123VP and VisDroneVP (derived from single-object tracking datasets) demonstrate the exceptional performance of TAFormer in target-aware video prediction, showcasing its adaptability to the additional requirements of aerial video interpretation for target awareness.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "17 pages, 9 figures"
    },
    {
        "paper id": "2403.18241",
        "abstract url": "https://arxiv.org/abs/2403.18241",
        "title": "NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a transformer-based autoencoder structure, promoting the preservation of spatial relationships in the generated 3D shapes. This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, multi-modal shape completion, single-view reconstruction, and text-to-shape synthesis.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18250",
        "abstract url": "https://arxiv.org/abs/2403.18250",
        "title": "Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with Multi-Band Reconfigurability and Antenna-VSWR Resilience",
        "rating": -1,
        "keywords": [
            [
                "GaN"
            ]
        ],
        "abstract": "This paper presents the first-ever highly linear and load-insensitive three-way load-modulation power amplifier (PA) based on reconfigurable hybrid asymmetrical load modulated balanced amplifier (H-ALMBA). Through proper amplitude and phase controls, the carrier, control amplifier (CA), and two peaking balanced amplifiers (BA1 and BA2) can form a linear high-order load modulation over wide bandwidth. Moreover, it is theoretically unveiled that the load modulation behavior of H-ALMBA can be insensitive to load mismatch by leveraging bias reconfiguration and the intrinsic load-insensitivity of balanced topology. Specifically, the PA's linearity and efficiency profiles can be maintained against arbitrary load mismatch through $Z_\\mathrm{L}$-dependent reconfiguration of CA supply voltage ($V_\\mathrm{DD,CA}$) and turning-on sequence of BA1 and BA2. Based on the proposed theory, an RF-input linear H-ALMBA is developed with GaN transistors and wideband quadrature hybrids. Over the design bandwidth from $1.7$-$2.9$ GHz, an efficiency of $56.8\\%$$-$$72.9\\%$ at peak power and $49.8\\%$$-$$61.2\\%$ at $10$-dB PBO are measured together with linear AMAM and AMPM responses. In modulated evaluation with 4G LTE signal, an EVM of $3.1\\%$, ACPR of $-39$ dB, and average efficiency of up to $52\\%$ are measured. Moreover, the reconfigurable H-ALMBA experimentally maintains an excellent average efficiency and linearity against arbitrary load mismatch at $2:1$ VSWR, and this mismatch-resilient operation can be achieved at any in-band frequencies. The overall measured performance favorably outperforms the state-of-the-art.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "paper id": "2403.18870",
        "abstract url": "https://arxiv.org/abs/2403.18870",
        "title": "SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification",
        "rating": -1,
        "keywords": [
            [
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf disease classification was greatly increased by this addition. Following this, several comparative studies between the average ensemble and individual models were carried out, indicating that the ensemble technique performed better. The average ensemble of all modified pre-trained models produced outstanding outcomes: 100%, 99%, 99%, and 99.45% for f1 score, precision, recall, and accuracy, respectively. Performance was further enhanced by the implementation of an optimized weighted average ensemble technique incorporated with grid search. This optimized sugarcaneNet2024 model performed the best for detecting sugarcane diseases, having achieved accuracy, precision, recall, and F1 score of 99.67%, 100%, 100%, and 100% , respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "32 pages, 11 Figures, 13 Tables"
    },
    {
        "paper id": "2403.18871",
        "abstract url": "https://arxiv.org/abs/2403.18871",
        "title": "Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification",
        "rating": -1,
        "keywords": [
            [
                "disease",
                "Clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Background: Pneumothorax is an acute thoracic disease caused by abnormal air collection between the lungs and chest wall. To address the opaqueness often associated with deep learning (DL) models, explainable artificial intelligence (XAI) methods have been introduced to outline regions related to pneumothorax diagnoses made by DL models. However, these explanations sometimes diverge from actual lesion areas, highlighting the need for further improvement. Method: We propose a template-guided approach to incorporate the clinical knowledge of pneumothorax into model explanations generated by XAI methods, thereby enhancing the quality of these explanations. Utilizing one lesion delineation created by radiologists, our approach first generates a template that represents potential areas of pneumothorax occurrence. This template is then superimposed on model explanations to filter out extraneous explanations that fall outside the template's boundaries. To validate its efficacy, we carried out a comparative analysis of three XAI methods with and without our template guidance when explaining two DL models in two real-world datasets. Results: The proposed approach consistently improved baseline XAI methods across twelve benchmark scenarios built on three XAI methods, two DL models, and two datasets. The average incremental percentages, calculated by the performance improvements over the baseline performance, were 97.8% in Intersection over Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model explanations and ground-truth lesion areas. Conclusions: In the context of pneumothorax diagnoses, we proposed a template-guided approach for improving AI explanations. We anticipate that our template guidance will forge a fresh approach to elucidating AI models by integrating clinical domain expertise.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17387",
        "abstract url": "https://arxiv.org/abs/2403.17387",
        "title": "Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection",
        "rating": -1.5,
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We delve into pseudo-labeling for semi-supervised monocular 3D object detection (SSM3OD) and discover two primary issues: a misalignment between the prediction quality of 3D and 2D attributes and the tendency of depth supervision derived from pseudo-labels to be noisy, leading to significant optimization conflicts with other reliable forms of supervision. We introduce a novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach features a Decoupled Pseudo-label Generation (DPG) module, designed to efficiently generate pseudo-labels by separately processing 2D and 3D attributes. This module incorporates a unique homography-based method for identifying dependable pseudo-labels in BEV space, specifically for 3D attributes. Additionally, we present a DepthGradient Projection (DGP) module to mitigate optimization conflicts caused by noisy depth supervision of pseudo-labels, effectively decoupling the depth gradient and removing conflicting gradients. This dual decoupling strategy-at both the pseudo-label generation and gradient levels-significantly improves the utilization of pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark demonstrate the superiority of our method over existing approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "To appear in CVPR2024"
    },
    {
        "paper id": "2403.17428",
        "abstract url": "https://arxiv.org/abs/2403.17428",
        "title": "Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization",
        "rating": -1.5,
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the symptom delineation task and the summarization task. This research contributes to the nascent field of applying LLMs to psychiatric interview and demonstrates their potential effectiveness in aiding mental health practitioners.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17460",
        "abstract url": "https://arxiv.org/abs/2403.17460",
        "title": "Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model",
        "rating": -1.5,
        "keywords": [
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "remote sensing"
            ],
            [
                "eess.IV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Reference-based super-resolution (RefSR) has the potential to build bridges across spatial and temporal resolutions of remote sensing images. However, existing RefSR methods are limited by the faithfulness of content reconstruction and the effectiveness of texture transfer in large scaling factors. Conditional diffusion models have opened up new opportunities for generating realistic high-resolution images, but effectively utilizing reference images within these models remains an area for further exploration. Furthermore, content fidelity is difficult to guarantee in areas without relevant reference information. To solve these issues, we propose a change-aware diffusion model named Ref-Diff for RefSR, using the land cover change priors to guide the denoising process explicitly. Specifically, we inject the priors into the denoising model to improve the utilization of reference information in unchanged areas and regulate the reconstruction of semantically relevant content in changed areas. With this powerful guidance, we decouple the semantics-guided denoising and reference texture-guided denoising processes to improve the model performance. Extensive experiments demonstrate the superior effectiveness and robustness of the proposed method compared with state-of-the-art RefSR methods in both quantitative and qualitative evaluations. The code and data are available at https://github.com/dongrunmin/RefDiff.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Accepted by CVPR2024"
    },
    {
        "paper id": "2403.17500",
        "abstract url": "https://arxiv.org/abs/2403.17500",
        "title": "Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification",
        "rating": -1.5,
        "keywords": [
            [
                "SLAM"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph representation learning is a fundamental research issue in various domains of applications, of which the inductive learning problem is particularly challenging as it requires models to generalize to unseen graph structures during inference. In recent years, graph neural networks (GNNs) have emerged as powerful graph models for inductive learning tasks such as node classification, whereas they typically heavily rely on the annotated nodes under a fully supervised training setting. Compared with the GNN-based methods, variational graph auto-encoders (VGAEs) are known to be more generalizable to capture the internal structural information of graphs independent of node labels and have achieved prominent performance on multiple unsupervised learning tasks. However, so far there is still a lack of work focusing on leveraging the VGAE framework for inductive learning, due to the difficulties in training the model in a supervised manner and avoiding over-fitting the proximity information of graphs. To solve these problems and improve the model performance of VGAEs for inductive graph representation learning, in this work, we propose the Self-Label Augmented VGAE model. To leverage the label information for training, our model takes node labels as one-hot encoded inputs and then performs label reconstruction in model training. To overcome the scarcity problem of node labels for semi-supervised settings, we further propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels generated by our model with a node-wise masking approach to enhance the label information. Experiments on benchmark inductive learning graph datasets verify that our proposed model archives promising results on node classification with particular superiority under semi-supervised learning settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17588",
        "abstract url": "https://arxiv.org/abs/2403.17588",
        "title": "Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models",
        "rating": -1.5,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Random Forest (RF) is well-known as an efficient ensemble learning method in terms of predictive performance. It is also considered a Black Box because of its hundreds of deep decision trees. This lack of interpretability can be a real drawback for acceptance of RF models in several real-world applications, especially those affecting one's lives, such as in healthcare, security, and law. In this work, we present Forest-ORE, a method that makes RF interpretable via an optimized rule ensemble (ORE) for local and global interpretation. Unlike other rule-based approaches aiming at interpreting the RF model, this method simultaneously considers several parameters that influence the choice of an interpretable rule ensemble. Existing methods often prioritize predictive performance over interpretability coverage and do not provide information about existing overlaps or interactions between rules. Forest-ORE uses a mixed-integer optimization program to build an ORE that considers the trade-off between predictive performance, interpretability coverage, and model size (size of the rule ensemble, rule lengths, and rule overlaps). In addition to providing an ORE competitive in predictive performance with RF, this method enriches the ORE through other rules that afford complementary information. It also enables monitoring of the rule selection process and delivers various metrics that can be used to generate a graphical representation of the final model. This framework is illustrated through an example, and its robustness is assessed through 36 benchmark datasets. A comparative analysis of well-known methods shows that Forest-ORE provides an excellent trade-off between predictive performance, interpretability coverage, and model size.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "48 pages, 11 figures"
    },
    {
        "paper id": "2403.17728",
        "abstract url": "https://arxiv.org/abs/2403.17728",
        "title": "Masked Autoencoders are PDE Learners",
        "rating": -1.5,
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 3 figures"
    },
    {
        "paper id": "2403.17753",
        "abstract url": "https://arxiv.org/abs/2403.17753",
        "title": "CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model",
        "rating": -1.5,
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Accurate, and effective traffic forecasting is vital for smart traffic systems, crucial in urban traffic planning and management. Current Spatio-Temporal Transformer models, despite their prediction capabilities, struggle with balancing computational efficiency and accuracy, favoring global over local information, and handling spatial and temporal data separately, limiting insight into complex interactions. We introduce the Criss-Crossed Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA), Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified Temporal Self-attention (ReTSA). These modules aim to lower computational needs via sparse attention, focus on local information for better traffic dynamics understanding, and merge spatial and temporal insights through a unique learning method. Extensive tests on six real-world datasets highlight CCDSReFormer's superior performance. An ablation study also confirms the significant impact of each component on the model's predictive accuracy, showcasing our model's ability to forecast traffic flow effectively.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2403.17761",
        "abstract url": "https://arxiv.org/abs/2403.17761",
        "title": "Makeup Prior Models for 3D Facial Makeup Estimation and Applications",
        "rating": -1.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "Facial"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In this work, we introduce two types of makeup prior models to extend existing 3D face prior models: PCA-based and StyleGAN2-based priors. The PCA-based prior model is a linear model that is easy to construct and is computationally efficient. However, it retains only low-frequency information. Conversely, the StyleGAN2-based model can represent high-frequency information with relatively higher computational cost than the PCA-based model. Although there is a trade-off between the two models, both are applicable to 3D facial makeup estimation and related applications. By leveraging makeup prior models and designing a makeup consistency module, we effectively address the challenges that previous methods faced in robustly estimating makeup, particularly in the context of handling self-occluded faces. In experiments, we demonstrate that our approach reduces computational costs by several orders of magnitude, achieving speeds up to 180 times faster. In addition, by improving the accuracy of the estimated makeup, we confirm that our methods are highly advantageous for various 3D facial makeup applications such as 3D makeup face reconstruction, user-friendly makeup editing, makeup transfer, and interpolation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page"
    },
    {
        "paper id": "2403.17775",
        "abstract url": "https://arxiv.org/abs/2403.17775",
        "title": "Secure Aggregation is Not Private Against Membership Inference Attacks",
        "rating": -1.5,
        "keywords": [
            [
                "federated learning"
            ],
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offers weak privacy against membership inference attacks even in a single training round. Indeed, it is difficult to hide a local update by adding other independent local updates when the updates are of high dimension. Our findings underscore the imperative for additional privacy-enhancing mechanisms, such as noise injection, in federated learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17814",
        "abstract url": "https://arxiv.org/abs/2403.17814",
        "title": "D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting",
        "rating": -1.5,
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In time series forecasting, effectively disentangling intricate temporal patterns is crucial. While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal. Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability. In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting. Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the \"shallow\" aspect. A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the \"deep\" aspect. After that, an interaction and fusion (IF) module is used to further analyze the components. Extensive experiments on seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art performance, outperforming the best baseline by an average of 9.48% and 7.15% in MSE and MAE, respectively.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17879",
        "abstract url": "https://arxiv.org/abs/2403.17879",
        "title": "Low-Latency Neural Stereo Streaming",
        "rating": -1.5,
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "neural codec"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR2024"
    },
    {
        "paper id": "2403.18035",
        "abstract url": "https://arxiv.org/abs/2403.18035",
        "title": "Bidirectional Consistency Models",
        "rating": -1.5,
        "keywords": [
            [
                "Diffusion",
                "inpainting",
                "image editing"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, largely reducing the number of iterations. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation and inversion tasks within one framework. Notably, our proposed method enables one-step generation and inversion while also allowing the use of additional steps to enhance generation quality or reduce reconstruction error. Furthermore, by leveraging our model's bidirectional consistency, we introduce a sampling strategy that can enhance FID while preserving the generated image content. We further showcase our model's capabilities in several downstream tasks, such as interpolation and inpainting, and present demonstrations of potential applications, including blind restoration of compressed images and defending black-box adversarial attacks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "40 pages, 25 figures"
    },
    {
        "paper id": "2403.18132",
        "abstract url": "https://arxiv.org/abs/2403.18132",
        "title": "Recommendation of data-free class-incremental learning algorithms by simulating future data",
        "rating": -1.5,
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close to that of an oracle choosing the best algorithm in each setting. This work contributes to facilitate the practical deployment of incremental learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18136",
        "abstract url": "https://arxiv.org/abs/2403.18136",
        "title": "Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs",
        "rating": -1.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high detection performance, marking a significant advancement in safeguarding GNNs against backdoor attacks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18144",
        "abstract url": "https://arxiv.org/abs/2403.18144",
        "title": "Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from Federated Learning",
        "rating": -1.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attacks"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Federated learning is a decentralized learning paradigm introduced to preserve privacy of client data. Despite this, prior work has shown that an attacker at the server can still reconstruct the private training data using only the client updates. These attacks are known as data reconstruction attacks and fall into two major categories: gradient inversion (GI) and linear layer leakage attacks (LLL). However, despite demonstrating the effectiveness of these attacks in breaching privacy, prior work has not investigated the usefulness of the reconstructed data for downstream tasks. In this work, we explore data reconstruction attacks through the lens of training and improving models with leaked data. We demonstrate the effectiveness of both GI and LLL attacks in maliciously training models using the leaked data more accurately than a benign federated learning strategy. Counter-intuitively, this bump in training quality can occur despite limited reconstruction quality or a small total number of leaked images. Finally, we show the limitations of these attacks for downstream training, individually for GI attacks and for LLL attacks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted to CVPR 2024"
    },
    {
        "paper id": "2403.18183",
        "abstract url": "https://arxiv.org/abs/2403.18183",
        "title": "Can AI Models Appreciate Document Aesthetics? An Exploration of Legibility and Layout Quality in Relation to Prediction Confidence",
        "rating": -1.5,
        "keywords": [
            [
                "psychological"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "A well-designed document communicates not only through its words but also through its visual eloquence. Authors utilize aesthetic elements such as colors, fonts, graphics, and layouts to shape the perception of information. Thoughtful document design, informed by psychological insights, enhances both the visual appeal and the comprehension of the content. While state-of-the-art document AI models demonstrate the benefits of incorporating layout and image data, it remains unclear whether the nuances of document aesthetics are effectively captured. To bridge the gap between human cognition and AI interpretation of aesthetic elements, we formulated hypotheses concerning AI behavior in document understanding tasks, specifically anchored in document design principles. With a focus on legibility and layout quality, we tested four aspects of aesthetic effects: noise, font-size contrast, alignment, and complexity, on model confidence using correlational analysis. The results and observations highlight the value of model analysis rooted in document design theories. Our work serves as a trailhead for further studies and we advocate for continued research in this topic to deepen our understanding of how AI interprets document aesthetics.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18203",
        "abstract url": "https://arxiv.org/abs/2403.18203",
        "title": "EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications",
        "rating": -1.5,
        "keywords": [
            [
                "biological",
                "medical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Artificial intelligence (AI) techniques are widely applied in the life sciences. However, applying innovative AI techniques to understand and deconvolute biological complexity is hindered by the learning curve for life science scientists to understand and use computing languages. An open-source, user-friendly interface for AI models, that does not require programming skills to analyze complex biological data will be extremely valuable to the bioinformatics community. With easy access to different sequencing technologies and increased interest in different 'omics' studies, the number of biological datasets being generated has increased and analyzing these high-throughput datasets is computationally demanding. The majority of AI libraries today require advanced programming skills as well as machine learning, data preprocessing, and visualization skills. In this research, we propose a web-based end-to-end pipeline that is capable of preprocessing, training, evaluating, and visualizing machine learning (ML) models without manual intervention or coding expertise. By integrating traditional machine learning and deep neural network models with visualizations, our library assists in recognizing, classifying, clustering, and predicting a wide range of multi-modal, multi-sensor datasets, including images, languages, and one-dimensional numerical data, for drug discovery, pathogen classification, and medical diagnostics.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "2024 7th International Conference on Information and Computer Technologies (ICICT)"
    },
    {
        "paper id": "2403.18219",
        "abstract url": "https://arxiv.org/abs/2403.18219",
        "title": "From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries",
        "rating": -1.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "Navigation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms. This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions. A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics. The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension. The research aims to address the question: How do reinforcement learning agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings? Through empirical analysis, the study evaluates agents' learning trajectories and adaptation processes, revealing insights into the efficacy of RL algorithms in navigating complex, multi-dimensional spaces. Reflections on the findings prompt considerations for future research, particularly in understanding the dynamics of learning in higher-dimensional environments.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17400",
        "abstract url": "https://arxiv.org/abs/2403.17400",
        "title": "A Survey on Resource Management in Joint Communication and Computing-Embedded SAGIN",
        "rating": -2,
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "The advent of the 6G era aims for ubiquitous connectivity, with the integration of non-terrestrial networks (NTN) offering extensive coverage and enhanced capacity. As manufacturing advances and user demands evolve, space-air-ground integrated networks (SAGIN) with computational capabilities emerge as a viable solution for services requiring low latency and high computational power. Resource management within joint communication and computing-embedded SAGIN (JCC-SAGIN) presents greater complexity than traditional terrestrial networks. This complexity arises from the spatiotemporal dynamics of network topology and service demand, the interdependency of large-scale resource variables, and intricate tradeoffs among various performance metrics. Thus, a thorough examination of resource management strategies in JCC-SAGIN is crucial, emphasizing the role of non-terrestrial platforms with processing capabilities in 6G. This paper begins by reviewing the architecture, enabling technologies, and applications in JCC-SAGIN. Then, we offer a detailed overview of resource management modeling and optimization methods, encompassing both traditional optimization approaches and learning-based intelligent decision-making frameworks. Finally, we outline the prospective research directions in JCC-SAGIN.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "43 pages, 17 figures"
    },
    {
        "paper id": "2403.17426",
        "abstract url": "https://arxiv.org/abs/2403.17426",
        "title": "Knowledge-Powered Recommendation for an Improved Diet Water Footprint",
        "rating": -2.0,
        "keywords": [
            [
                "graph"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "According to WWF, 1.1 billion people lack access to water, and 2.7 billion experience water scarcity at least one month a year. By 2025, two-thirds of the world's population may be facing water shortages. This highlights the urgency of managing water usage efficiently, especially in water-intensive sectors like food. This paper proposes a recommendation engine, powered by knowledge graphs, aiming to facilitate sustainable and healthy food consumption. The engine recommends ingredient substitutes in user recipes that improve nutritional value and reduce environmental impact, particularly water footprint. The system architecture includes source identification, information extraction, schema alignment, knowledge graph construction, and user interface development. The research offers a promising tool for promoting healthier eating habits and contributing to water conservation efforts.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "3 pages, 1 figure, AAAI'24"
    },
    {
        "paper id": "2403.17442",
        "abstract url": "https://arxiv.org/abs/2403.17442",
        "title": "Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation",
        "rating": -2,
        "keywords": [
            [
                "industrial",
                "Recommendation"
            ]
        ],
        "abstract": "As user behaviors become complicated on business platforms, online recommendations focus more on how to touch the core conversions, which are highly related to the interests of platforms. These core conversions are usually continuous targets, such as \\textit{watch time}, \\textit{revenue}, and so on, whose predictions can be enhanced by previous discrete conversion actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets. However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion. Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks. In this paper, we study the MTL problem with hybrid targets for the first time and propose the model named Hybrid Targets Learning Network (HTLNet) to explore task dependence and enhance optimization. Specifically, we introduce label embedding for each task to explicitly transfer the label information among these tasks, which can effectively explore logical task dependence. We also further design the gradient adjustment regime between the final regression task and other classification tasks to enhance the optimization. Extensive experiments on two offline public datasets and one real-world industrial dataset are conducted to validate the effectiveness of HTLNet. Moreover, online A/B tests on the financial recommender system also show our model has superior improvement.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17479",
        "abstract url": "https://arxiv.org/abs/2403.17479",
        "title": "Natural Language Requirements Testability Measurement Based on Requirement Smells",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Requirements form the basis for defining software systems' obligations and tasks. Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests. However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability. This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain. Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary is required to detect such words. We offer a neural word-embedding technique to generate such a dictionary automatically. Using the dictionary, we could automatically detect Polysemy smell (domain-specific ambiguity) for the first time in 10 application domains. Our empirical study on nearly 1000 software requirements from six well-known industrial and academic projects demonstrates that the proposed smell detection approach outperforms Smella, a state-of-the-art tool, in detecting requirements smells. The precision and recall of smell detection are improved with an average of 0.03 and 0.33, respectively, compared to the state-of-the-art. The proposed requirement testability model measures the testability of 985 requirements with a mean absolute error of 0.12 and a mean squared error of 0.03, demonstrating the model's potential for practical use.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "45 pages, 16 figures, and 13 tables; submitted as a journal paper"
    },
    {
        "paper id": "2403.17494",
        "abstract url": "https://arxiv.org/abs/2403.17494",
        "title": "FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart Electrical Grids",
        "rating": -2,
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "Predicting and classifying faults in electricity networks is crucial for uninterrupted provision and keeping maintenance costs at a minimum. Thanks to the advancements in the field provided by the smart grid, several data-driven approaches have been proposed in the literature to tackle fault prediction tasks. Implementing these systems brought several improvements, such as optimal energy consumption and quick restoration. Thus, they have become an essential component of the smart grid. However, the robustness and security of these systems against adversarial attacks have not yet been extensively investigated. These attacks can impair the whole grid and cause additional damage to the infrastructure, deceiving fault detection systems and disrupting restoration. In this paper, we present FaultGuard, the first framework for fault type and zone classification resilient to adversarial attacks. To ensure the security of our system, we employ an Anomaly Detection System (ADS) leveraging a novel Generative Adversarial Network training layer to identify attacks. Furthermore, we propose a low-complexity fault prediction model and an online adversarial training technique to enhance robustness. We comprehensively evaluate the framework's performance against various adversarial attacks using the IEEE13-AdvAttack dataset, which constitutes the state-of-the-art for resilient fault prediction benchmarking. Our model outclasses the state-of-the-art even without considering adversaries, with an accuracy of up to 0.958. Furthermore, our ADS shows attack detection capabilities with an accuracy of up to 1.000. Finally, we demonstrate how our novel training layers drastically increase performances across the whole framework, with a mean increase of 154% in ADS accuracy and 118% in model accuracy.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17502",
        "abstract url": "https://arxiv.org/abs/2403.17502",
        "title": "SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder",
        "rating": -2,
        "keywords": [
            [
                "super-resolution"
            ],
            [
                "image restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The data bottleneck has emerged as a fundamental challenge in learning based image restoration methods. Researchers have attempted to generate synthesized training data using paired or unpaired samples to address this challenge. This study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages both paired and unpaired datasets to generate realistic degraded data. Our approach is based on modeling the conditional distribution of degraded and clean images with a specially designed graphical model. Under the variational inference framework, we develop an objective function for handling both paired and unpaired data. We employ our method to generate paired training samples for real-world image denoising and super-resolution tasks. Our approach excels in the quality of synthetic degraded images compared to other unpaired and paired noise modeling methods. Furthermore, our approach demonstrates remarkable performance in downstream image restoration tasks, even with limited paired data. With more paired data, our method achieves the best performance on the SIDD dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17571",
        "abstract url": "https://arxiv.org/abs/2403.17571",
        "title": "Ransomware: Analysis and Evaluation of Live Forensic Techniques and the Impact on Linux based IoT Systems",
        "rating": -2,
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Ransomware has been predominantly a threat to Windows systems. But, Linux systems became interesting for cybercriminals and this trend is expected to continue. This endangers IoT ecosystems, whereas many IoT systems are based on Linux (e.g. cloud infrastructure and gateways). This paper researches how currently employed forensic techniques can be applied to Linux ransomware and evaluates the maturity as well as the impact on the system. While Windows-based ransomware predominantly uses RSA and AES for key management, a variety of approaches was identified for Linux. Cybercriminals appear to be deliberately moving away from RSA and AES to make Live forensic investigations more difficult. Linux ransomware is developed for a predefined goal and does not exploit the full potential of damage. It appears in an early stage and is expected to reach a similar potential to Windows-based malware. The results generated provided an excellent basic understanding to discuss and assess implications on the IoT industry at an early stage of development.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17603",
        "abstract url": "https://arxiv.org/abs/2403.17603",
        "title": "END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17615",
        "abstract url": "https://arxiv.org/abs/2403.17615",
        "title": "Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles from 3D Cell Painting Images",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "biologically"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Despite their black-box nature, deep learning models are extensively used in image-based drug discovery to extract feature vectors from single cells in microscopy images. To better understand how these networks perform representation learning, we employ visual explainability techniques (e.g., Grad-CAM). Our analyses reveal several mechanisms by which supervised models cheat, exploiting biologically irrelevant pixels when extracting morphological features from images, such as noise in the background. This raises doubts regarding the fidelity of learned single-cell representations and their relevance when investigating downstream biological questions. To address this misalignment between researcher expectations and machine behavior, we introduce Grad-CAMO, a novel single-cell interpretability score for supervised feature extractors. Grad-CAMO measures the proportion of a model's attention that is concentrated on the cell of interest versus the background. This metric can be assessed per-cell or averaged across a validation set, offering a tool to audit individual features vectors or guide the improved design of deep learning architectures. Importantly, Grad-CAMO seamlessly integrates into existing workflows, requiring no dataset or model modifications, and is compatible with both 2D and 3D Cell Painting data. Additional results are available at https://github.com/eigenvivek/Grad-CAMO.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17633",
        "abstract url": "https://arxiv.org/abs/2403.17633",
        "title": "UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17648",
        "abstract url": "https://arxiv.org/abs/2403.17648",
        "title": "Healthcare Data Governance, Privacy, and Security -- A Conceptual Framework",
        "rating": -2,
        "keywords": [
            [
                "Healthcare"
            ]
        ],
        "abstract": "The abundance of data has transformed the world in every aspect. It has become the core element in decision making, problem solving, and innovation in almost all areas of life, including business, science, healthcare, education, and many others. Despite all these advances, privacy and security remain critical concerns of the healthcare industry. It is important to note that healthcare data can also be a liability if it is not managed correctly. This data mismanagement can have severe consequences for patients and healthcare organisations, including patient safety, legal liability, damage to reputation, financial loss, and operational inefficiency. Healthcare organisations must comply with a range of regulations to protect patient data. We perform a classification of data governance elements or components in a manner that thoroughly assesses the healthcare data chain from a privacy and security standpoint. After deeply analysing the existing literature, we propose a conceptual privacy and security driven healthcare data governance framework.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17649",
        "abstract url": "https://arxiv.org/abs/2403.17649",
        "title": "Towards a Dutch hybrid quantum/HPC infrastructure",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Quantum Inspire has taken important steps to enable quantum applications by developing a setting that allows the execution of hybrid algorithms. Currently, the setting uses a classical server (HPC node) co-located with the quantum computer for the high frequency coupling needed by hybrid algorithms. A fast task manager (dispatcher) has been developed to orchestrate the interaction between the server and the quantum computer. Although successful, the setting imposes a specific hybrid job-structure. This is most likely always going to be the case and we are currently discussing how to make sure this does not hamper the uptake of the setting. Furthermore, first steps have been taken towards the integration with the Dutch National High-Performance Computing (HPC) Center, hosted by SURF. As a first approach we have setup a setting consisting of two SLURM clusters, one in the HPC (C1) and the second (C2) co-located with Quantum Inspire API. Jobs are submitted from C1 to C2. Quantum Inspire can then schedule with C2 the jobs to the quantum computer. With this setting, we enable control from both SURF and Quantum Inspire on the jobs being executed. By using C1 for the jobs submission we remove the accounting burden from Quantum Inspire. By having C2 co-located with Quantum Inspire API, we make the setting more resilient towards network failures. This setting can be extended for other HPC centers to submit jobs to Quantum Inspire backends.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "6 pages, 3 figures"
    },
    {
        "paper id": "2403.17652",
        "abstract url": "https://arxiv.org/abs/2403.17652",
        "title": "Leveraging A Variety of Anchors in Cellular Network for Ubiquitous Sensing",
        "rating": -2,
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Integrated sensing and communication (ISAC) has recently attracted tremendous attention from both academia and industry, being envisioned as a key part of the standards for the sixth-generation (6G) cellular network. A key challenge of 6G-oriented ISAC lies in how to perform ubiquitous sensing based on the communication signals and devices. Previous works have made great progresses on studying the signal waveform design that leads to optimal communication-sensing performance tradeoff. In this article, we aim to focus on issues arising from the exploitation of the communication devices for sensing in 6G network. Particularly, we will discuss about how to leverage various nodes available in the cellular network as anchors to perform ubiquitous sensing. On one hand, the base stations (BSs) will be the most important anchors in the future 6G ISAC network, since they can generate/process radio signals with high range/angle resolutions, and their positions are precisely known. Correspondingly, we will first study the BS-based sensing technique. On the other hand, the BSs alone may not enable ubiquitous sensing, since they cannot cover all the places with strong line-of-sight (LOS) links. This motivates us to investigate the possibility of using other nodes that are with higher density in the network to act as the anchors. Along this line, we are interested in two types of new anchors - user equipments (UEs) and reconfigurable intelligent surfaces (RISs). This paper will shed light on the opportunities and challenges brought by UE-assisted sensing and RIS-assisted sensing. Our goal is to devise a novel 6G-oriented sensing architecture where BSs, UEs, and RISs can work together to provide ubiquitous sensing services.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "to appear in IEEE Communications Magazine"
    },
    {
        "paper id": "2403.17678",
        "abstract url": "https://arxiv.org/abs/2403.17678",
        "title": "Hierarchical Light Transformer Ensembles for Multimodal Trajectory Forecasting",
        "rating": -2,
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate trajectory forecasting is crucial for the performance of various systems, such as advanced driver-assistance systems and self-driving vehicles. These forecasts allow to anticipate events leading to collisions and, therefore, to mitigate them. Deep Neural Networks have excelled in motion forecasting, but issues like overconfidence and uncertainty quantification persist. Deep Ensembles address these concerns, yet applying them to multimodal distributions remains challenging. In this paper, we propose a novel approach named Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently training an ensemble of Transformer architectures using a novel hierarchical loss function. HLT-Ens leverages grouped fully connected layers, inspired by grouped convolution techniques, to capture multimodal distributions, effectively. Through extensive experimentation, we demonstrate that HLT-Ens achieves state-of-the-art performance levels, offering a promising avenue for improving trajectory forecasting techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17679",
        "abstract url": "https://arxiv.org/abs/2403.17679",
        "title": "Shape Optimization of Geometrically Nonlinear Modal Coupling Coefficients: An Application to MEMS Gyroscopes",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Micro- and nanoelectromechanical system (MEMS and NEMS) resonators can exhibit rich nonlinear dynamics as they are often operated at large amplitudes with high quality factors and possess a high mode density with a variety of nonlinear modal couplings. Their impact is strongly influenced by internal resonance conditions and by the strength of the modal coupling coefficients. On one hand, strong nonlinear couplings are of academic interest and promise novel device concepts. On the other hand, however, they have the potential to disturb the linear system behavior on which industrial devices such as gyroscopes and micro mirrors are based on. In either case, being able to optimize the coupling coefficients by design is certainly beneficial. A main source of nonlinear modal couplings are geometric nonlinearities. In this work, we apply node-based shape optimization to tune the geometrically nonlinear 3-wave coupling coefficients of a MEMS gyroscope. We demonstrate that individual coupling coefficients can be tuned over several orders of magnitude by shape optimization, while satisfying typical constraints on manufacturability and operability of the devices. The optimized designs contain unintuitive geometrical features far away from any solution an experienced human MEMS or NEMS designer could have thought of. Thus, this work demonstrates the power of shape optimization for tailoring the complex nonlinear dynamic properties of MEMS and NEMS resonators.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17688",
        "abstract url": "https://arxiv.org/abs/2403.17688",
        "title": "Large Language Models Enhanced Collaborative Filtering",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have attracted considerable interest among researchers to leverage these models to enhance Recommender Systems (RSs). Existing work predominantly utilizes LLMs to generate knowledge-rich texts or utilizes LLM-derived embeddings as features to improve RSs. Although the extensive world knowledge embedded in LLMs generally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collaborative filtering information. Considering its crucial role in RSs, one key challenge in enhancing RSs with LLMs lies in providing better collaborative filtering information through LLMs. In this paper, drawing inspiration from the in-context learning and chain of thought reasoning in LLMs, we propose the Large Language Models enhanced Collaborative Filtering (LLM-CF) framework, which distils the world knowledge and reasoning capabilities of LLMs into collaborative filtering. We also explored a concise and efficient instruction-tuning method, which improves the recommendation capabilities of LLMs while preserving their general functionalities (e.g., not decreasing on the LLM benchmark). Comprehensive experiments on three real-world datasets demonstrate that LLM-CF significantly enhances several backbone recommendation models and consistently outperforms competitive baselines, showcasing its effectiveness in distilling the world knowledge and reasoning capabilities of LLM into collaborative filtering.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2403.17711",
        "abstract url": "https://arxiv.org/abs/2403.17711",
        "title": "Using quantum computers in control: interval matrix properties",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Quantum computing provides a powerful framework for tackling computational problems that are classically intractable. The goal of this paper is to explore the use of quantum computers for solving relevant problems in systems and control theory. In the recent literature, different quantum algorithms have been developed to tackle binary optimization, which plays an important role in various control-theoretic problems. As a prototypical example, we consider the verification of interval matrix properties such as non-singularity and stability on a quantum computer. We present a quantum algorithm solving these problems and we study its performance in simulation. Our results demonstrate that quantum computers provide a promising tool for control whose applicability to further computationally complex problems remains to be explored.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Final version, accepted for publication in Proc. European Control Conference (ECC), 2024"
    },
    {
        "paper id": "2403.17712",
        "abstract url": "https://arxiv.org/abs/2403.17712",
        "title": "Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark",
        "rating": -2,
        "keywords": [
            [
                "infrared"
            ],
            [
                "industrial",
                "Thermal",
                "chemical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity. Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas. However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets. In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images. Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes. Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively. The code and data will be made available soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17734",
        "abstract url": "https://arxiv.org/abs/2403.17734",
        "title": "Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models",
        "rating": -2,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "biomedical",
                "medical",
                "CT"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The rapid advancement of Artificial Intelligence (AI) in biomedical imaging and radiotherapy is hindered by the limited availability of large imaging data repositories. With recent research and improvements in denoising diffusion probabilistic models (DDPM), high quality synthetic medical scans are now possible. Despite this, there is currently no way of generating multiple related images, such as a corresponding ground truth which can be used to train models, so synthetic scans are often manually annotated before use. This research introduces a novel architecture that is able to generate multiple, related PET-CT-tumour mask pairs using paired networks and conditional encoders. Our approach includes innovative, time step-controlled mechanisms and a `noise-seeding' strategy to improve DDPM sampling consistency. While our model requires a modified perceptual loss function to ensure accurate feature alignment we show generation of clearly aligned synthetic images and improvement in segmentation accuracy with generated images.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "to be published in IEEE International Symposium on Biomedical Imaging 2024"
    },
    {
        "paper id": "2403.17740",
        "abstract url": "https://arxiv.org/abs/2403.17740",
        "title": "All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important interactions via the observed data. In the experiments, we evaluate our model under three cold-start settings on three real-world datasets. The experimental results show that HIRE outperforms other baselines by a large margin. Furthermore, we visualize the inferred interactions of HIRE to confirm the contribution of our model.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "14 pages, 9 figures"
    },
    {
        "paper id": "2403.17751",
        "abstract url": "https://arxiv.org/abs/2403.17751",
        "title": "Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS Systems",
        "rating": -2,
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Reconfigurable intelligent surface (RIS)-assisted index modulation system schemes are considered a promising technology for sixth-generation (6G) wireless communication systems, which can enhance various system capabilities such as coverage and reliability. However, obtaining perfect channel state information (CSI) is challenging due to the lack of a radio frequency chain in RIS. In this paper, we investigate the RIS-assisted full-duplex (FD) two-way space shift keying (SSK) system under imperfect CSI, where the signal emissions are augmented by deploying RISs in the vicinity of two FD users. The maximum likelihood detector is utilized to recover the transmit antenna index. With this in mind, we derive closed-form average bit error probability (ABEP) expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide the upper bound and asymptotic ABEP expressions in the presence of channel estimation errors. To gain more insights, we also derive the outage probability and provide the throughput of the proposed scheme with imperfect CSI. The correctness of the analytical derivation results is confirmed via Monte Carlo simulations. It is demonstrated that increasing the number of elements of RIS can significantly improve the ABEP performance of the FD system over the half-duplex (HD) system. Furthermore, in the high SNR region, the ABEP performance of the FD system is better than that of the HD system.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17770",
        "abstract url": "https://arxiv.org/abs/2403.17770",
        "title": "CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation",
        "rating": -2,
        "keywords": [
            [
                "Diffusion",
                "Synthesis"
            ],
            [
                "medical",
                "diagnosis",
                "CT"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Despite the significant success achieved by deep learning methods in medical image segmentation, researchers still struggle in the computer-aided diagnosis of abdominal lymph nodes due to the complex abdominal environment, small and indistinguishable lesions, and limited annotated data. To address these problems, we present a pipeline that integrates the conditional diffusion model for lymph node generation and the nnU-Net model for lymph node segmentation to improve the segmentation performance of abdominal lymph nodes through synthesizing a diversity of realistic abdominal lymph node data. We propose LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical structure masks as model conditions. These conditions work in two conditioning mechanisms: global structure conditioning and local detail conditioning, to distinguish between lymph nodes and their surroundings and better capture lymph node characteristics. The obtained paired abdominal lymph node images and masks are used for the downstream segmentation task. Experimental results on the abdominal lymph node datasets demonstrate that LN-DDPM outperforms other generative methods in the abdominal lymph node image synthesis and better assists the downstream abdominal lymph node segmentation task.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17790",
        "abstract url": "https://arxiv.org/abs/2403.17790",
        "title": "A PAC-Bayesian Framework for Optimal Control with Stability Guarantees",
        "rating": -2,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost function that averages out the random uncertainties affecting the dynamics of nonlinear systems. For tractability reasons, this problem is typically addressed by minimizing an empirical cost, which represents the average cost across a finite dataset of sampled disturbances. However, this approach raises the challenge of quantifying the control performance against out-of-sample uncertainties. Particularly, in scenarios where the training dataset is small, SNOC policies are prone to overfitting, resulting in significant discrepancies between the empirical cost and the true cost, i.e., the average SNOC cost incurred during control deployment. Therefore, establishing generalization bounds on the true cost is crucial for ensuring reliability in real-world applications. In this paper, we introduce a novel approach that leverages PAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on these bounds, we propose a new method for designing optimal controllers, offering a principled way to incorporate prior knowledge into the synthesis process, which aids in improving the control policy and mitigating overfitting. Furthermore, by leveraging recent parametrizations of stabilizing controllers for nonlinear systems, our framework inherently ensures closed-loop stability. The effectiveness of our proposed method in incorporating prior knowledge and combating overfitting is shown by designing neural network controllers for tasks in cooperative robotics.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17792",
        "abstract url": "https://arxiv.org/abs/2403.17792",
        "title": "A Novel Temperature-based Model for SWIPT",
        "rating": -2,
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "In this letter, a novel communication paradigm for simultaneous wireless information and power transfer (SWIPT) is proposed, which leverages the thermal characteristics of electromagnetic signals. In particular, the proposed scheme exploits the inherent thermal dynamics of electromagnetic signals, enabling the seamless integration of information decoding and energy harvesting (EH). As a consequence, in contrast to conventional SWIPT techniques, the proposed model eliminates the need to divide the received signal into orthogonal components. By exploiting the thermal correlation between consecutive time slots, the communication channel is converted to a virtual multiple-input multiple-output (MIMO) channel with memory. We evaluate the achievable rate of the proposed temperature-modulated channel for uniform and exponential input distributions and assess its performance in terms of harvested energy through a non-linear harvesting model. Our numerical results reveal that the exponential distribution outperforms the uniform distribution in rate and harvested energy at low input power levels, while the uniform distribution achieves a better EH performance at high input power levels.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "This work has been accepted for publication in IEEE Wireless Communications Letters"
    },
    {
        "paper id": "2403.17807",
        "abstract url": "https://arxiv.org/abs/2403.17807",
        "title": "Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing",
        "rating": -2,
        "keywords": [
            [
                "Sign Language",
                "facial"
            ]
        ],
        "abstract": "Previous research underscored the potential of danmaku--a text-based commenting feature on videos--in engaging hearing audiences. Yet, for many Deaf and hard-of-hearing (DHH) individuals, American Sign Language (ASL) takes precedence over English. To improve inclusivity, we introduce \"Signmaku,\" a new commenting mechanism that uses ASL, serving as a sign language counterpart to danmaku. Through a need-finding study (N=12) and a within-subject experiment (N=20), we evaluated three design styles: real human faces, cartoon-like figures, and robotic representations. The results showed that cartoon-like signmaku not only entertained but also encouraged participants to create and share ASL comments, with fewer privacy concerns compared to the other designs. Conversely, the robotic representations faced challenges in accurately depicting hand movements and facial expressions, resulting in higher cognitive demands on users. Signmaku featuring real human faces elicited the lowest cognitive load and was the most comprehensible among all three types. Our findings offered novel design implications for leveraging generative AI to create signmaku comments, enriching co-learning experiences for DHH individuals.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "14 pages, CHI 2024"
    },
    {
        "paper id": "2403.17808",
        "abstract url": "https://arxiv.org/abs/2403.17808",
        "title": "Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields",
        "rating": -2,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Biomedical",
                "cancer"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology. These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts. Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed. These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated data in the biomedical imaging domain. To address this limitation, we propose Biomedical Video Diffusion Model (BVDM), capable of generating realistic-looking synthetic microscopy videos. Trained only on a single real video, BVDM can generate videos of arbitrary length with pixel-level annotations that can be used for training data-hungry models. It is composed of a denoising diffusion probabilistic model (DDPM) generating high-fidelity synthetic cell microscopy images and a flow prediction model (FPM) predicting the non-rigid transformation between consecutive video frames. During inference, initially, the DDPM imposes realistic cell textures on synthetic cell masks which are generated based on real data statistics. The flow prediction model predicts the flow field between consecutive masks and applies that to the DDPM output from the previous time frame to create the next one while keeping temporal consistency. BVDM outperforms state-of-the-art synthetic live cell microscopy video generation models. Furthermore, we demonstrate that a sufficiently large synthetic dataset enhances the performance of cell segmentation and tracking models compared to using a limited amount of available real data.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17810",
        "abstract url": "https://arxiv.org/abs/2403.17810",
        "title": "Environment Reconstruction based on Multi-User Selection and Multi-Modal Fusion in ISAC",
        "rating": -2,
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Integrated sensing and communications (ISAC) has been deemed as a key technology for the sixth generation (6G) wireless communications systems. In this paper, we explore the inherent clustered nature of wireless users and design a multi-user based environment reconstruction scheme. Specifically, we first select users based on the estimation precision of channel's multipath, including the line-of-sight (LOS) and the non-line-of-sight (NLOS) paths, to enhance the accuracy of environment reconstruction. Then, we develop a fusion strategy that merges communications signalling with camera image to increase the accuracy and robustness of environment reconstruction. The simulation results demonstrate that the proposed algorithm can achieve a remarkable sensing accuracy of centimeter level, which is about 17 times better than the scheme without user selection. Meanwhile, the fusion of communications data and vision data leads to a threefold accuracy improvement over the image only method, especially under challenging weather conditions like raining and snowing.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17834",
        "abstract url": "https://arxiv.org/abs/2403.17834",
        "title": "A foundation model utilizing chest CT volumes and radiology reports for supervised-level zero-shot detection of abnormalities",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "medical",
                "healthcare",
                "CT",
                "radiology"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "A major challenge in computational research in 3D medical imaging is the lack of comprehensive datasets. Addressing this issue, our study introduces CT-RATE, the first 3D medical imaging dataset that pairs images with textual reports. CT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188 through various reconstructions, from 21,304 unique patients, along with corresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP, a CT-focused contrastive language-image pre-training framework. As a versatile, self-supervised model, CT-CLIP is designed for broad application and does not require task-specific training. Remarkably, CT-CLIP outperforms state-of-the-art, fully supervised methods in multi-abnormality detection across all key metrics, thus eliminating the need for manual annotation. We also demonstrate its utility in case retrieval, whether using imagery or textual queries, thereby advancing knowledge dissemination. The open-source release of CT-RATE and CT-CLIP marks a significant advancement in medical AI, enhancing 3D imaging analysis and fostering innovation in healthcare.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17837",
        "abstract url": "https://arxiv.org/abs/2403.17837",
        "title": "GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "HDR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications. However, capturing HDR content from real-world scenes is expensive and time-consuming. Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community. A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance). To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game. We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the state-of-the-art HDR image reconstruction methods. Furthermore, we demonstrate the effectiveness of the proposed dataset and its impact on additional computer vision tasks including 3D human pose estimation, human body part segmentation, and holistic scene segmentation. The dataset, data collection pipeline, and evaluation code are available at: https://github.com/HrishavBakulBarua/GTA-HDR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Submitted to IEEE"
    },
    {
        "paper id": "2403.17843",
        "abstract url": "https://arxiv.org/abs/2403.17843",
        "title": "A Sociotechnical Framework For Addressing Stigma and Designing Personalized Digital Health Products",
        "rating": -2,
        "keywords": [
            [
                "Health",
                "healthcare",
                "disease",
                "psychological"
            ]
        ],
        "abstract": "Stigma, a recognized global barrier to effective disease management, impacts social interactions, resource access, and psychological well-being. In this study, we developed a patient-centered framework for deriving design requirements and interventions for health conditions subject to social stigma. This study introduces a patient-centered framework, grounded in sociotechnical systems theory, to create tailored interventions and design requirements for health conditions influenced by social stigma. We tested this framework through a mixed-method study on chronic pelvic pain patients. Our approach led to the identification of ten design requirements that encompass behavioral and psychological support and strategies for day-to-day living. The findings reveal a preference among CPP patients for priming and social support interventions. This study underscores the value of a systems-based perspective in healthcare, advocating for a nuanced, patient-centered approach that addresses the complex nature of health conditions affected by social stigma. It contributes to the ongoing discourse on integrating STS theory into healthcare frameworks, highlighting the need for targeted strategies to combat the complexities of stigma in patient care.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "19 pages, 5 Tables, 1 Figure"
    },
    {
        "paper id": "2403.17846",
        "abstract url": "https://arxiv.org/abs/2403.17846",
        "title": "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation",
        "rating": -2,
        "keywords": [
            [
                "visual-language"
            ],
            [
                "3D"
            ],
            [
                "Robot",
                "Navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments. We provide code and trial video data at http://hovsg.github.io/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Code and video are available at http://hovsg.github.io/"
    },
    {
        "paper id": "2403.17862",
        "abstract url": "https://arxiv.org/abs/2403.17862",
        "title": "Proceedings Sixth Workshop on Models for Formal Analysis of Real Systems",
        "rating": -2,
        "keywords": [
            [
                "biology"
            ]
        ],
        "abstract": "This volume contains the proceedings of MARS 2024, the sixth workshop on Models for Formal Analysis of Real Systems, held as part of ETAPS 2024, the European Joint Conferences on Theory and Practice of Software. The MARS workshops bring together researchers from different communities who are developing formal models of real systems in areas where complex models occur, such as networks, cyber-physical systems, hardware/software co-design, biology, etc. The motivation and aim for MARS stem from the following two observations: (1) Large case studies are essential to show that specification formalisms and modelling techniques are applicable to real systems, whereas many research papers only consider toy examples or tiny case studies. (2) Developing an accurate model of a real system takes a large amount of time, often months or years. In most scientific papers, however, salient details of the model need to be skipped due to lack of space, and to leave room for formal verification methodologies and results. The MARS workshops aim at remedying these issues, emphasising modelling over verification, so as to retain lessons learnt from formal modelling, which are not usually discussed elsewhere.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17868",
        "abstract url": "https://arxiv.org/abs/2403.17868",
        "title": "An invitation to the sample complexity of quantum hypothesis testing",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Quantum hypothesis testing (QHT) has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of QHT, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on QHT, we characterize the sample complexity of binary QHT in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple QHT. In more detail, we prove that the sample complexity of symmetric binary QHT depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. As a counterpart of the quantum Stein's lemma, we also find that the sample complexity of asymmetric binary QHT depends logarithmically on the inverse type II error probability and inversely on the quantum relative entropy. We then provide lower and upper bounds on the sample complexity of multiple QHT, with it remaining an intriguing open question to improve these bounds. The final part of our paper outlines and reviews how sample complexity of QHT is relevant to a broad swathe of research areas and can enhance understanding of many fundamental concepts, including quantum algorithms for simulation and search, quantum learning and classification, and foundations of quantum mechanics. As such, we view our paper as an invitation to researchers coming from different communities to study and contribute to the problem of sample complexity of QHT, and we outline a number of open directions for future research.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "v2: 52 pages, 1 figure, title modified and now includes Section V on applications and further avenues of research; see independent and concurrent work of Pensia, Jog, Loh at arXiv:2403.16981"
    },
    {
        "paper id": "2403.17873",
        "abstract url": "https://arxiv.org/abs/2403.17873",
        "title": "Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach",
        "rating": -2.0,
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI"
            ],
            [
                "Workshop"
            ]
        ],
        "abstract": "Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims to bridge the gap between LLM capabilities and user perceptions, promoting the ethically responsible development and use of LLM-based technology.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Extended version of the manuscript accepted for the ACM CHI Workshop on Human-Centered Explainable AI 2024 (HCXAI24)"
    },
    {
        "paper id": "2403.17876",
        "abstract url": "https://arxiv.org/abs/2403.17876",
        "title": "MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers. Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular. Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages. Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 14 linguistically and geographically diverse languages, with digital footprints of varying sizes. Using xMIND, we systematically benchmark several state-of-the-art content-based neural news recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer scenarios, considering both monolingual and bilingual news consumption patterns. Our findings reveal that (i) current NNRs, even when based on a multilingual language model, suffer from substantial performance losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT training has limited benefits, particularly when combined with a bilingual news consumption. Our findings thus warrant a broader research effort in multilingual and cross-lingual news recommendation. The xMIND dataset is available at https://github.com/andreeaiana/xMIND.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted at the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024)"
    },
    {
        "paper id": "2403.17889",
        "abstract url": "https://arxiv.org/abs/2403.17889",
        "title": "Large scale paired antibody language models",
        "rating": -2,
        "keywords": [
            [
                "biotherapeutics"
            ]
        ],
        "abstract": "Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics. With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data. To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input. These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset. We show that our models outperform existing antibody and protein language models on a diverse range of design and regression tasks relevant to antibody engineering. This advancement marks a significant leap forward in leveraging machine learning, large scale data sets and high-performance computing for enhancing antibody design for therapeutic development.",
        "subjects": [
            "q-bio.BM"
        ],
        "comment": "14 pages, 2 figures, 6 tables, model weights available at https://zenodo.org/doi/10.5281/zenodo.10876908"
    },
    {
        "paper id": "2403.17911",
        "abstract url": "https://arxiv.org/abs/2403.17911",
        "title": "Domain-Specific Evaluation Strategies for AI in Journalism",
        "rating": -2.0,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.CY"
            ],
            [
                "Workshop"
            ]
        ],
        "abstract": "News organizations today rely on AI tools to increase efficiency and productivity across various tasks in news production and distribution. These tools are oriented towards stakeholders such as reporters, editors, and readers. However, practitioners also express reservations around adopting AI technologies into the newsroom, due to the technical and ethical challenges involved in evaluating AI technology and its return on investments. This is to some extent a result of the lack of domain-specific strategies to evaluate AI models and applications. In this paper, we consider different aspects of AI evaluation (model outputs, interaction, and ethics) that can benefit from domain-specific tailoring, and suggest examples of how journalistic considerations can lead to specialized metrics or strategies. In doing so, we lay out a potential framework to guide AI evaluation in journalism, such as seen in other disciplines (e.g. law, healthcare). We also consider directions for future work, as well as how our approach might generalize to other domains.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Accepted to the Workshop on Evaluating AI at the ACM CHI conference on Human Factors in Computing Systems"
    },
    {
        "paper id": "2403.17915",
        "abstract url": "https://arxiv.org/abs/2403.17915",
        "title": "Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos",
        "rating": -2,
        "keywords": [
            [
                "Depth"
            ],
            [
                "health",
                "surgery",
                "clinical",
                "organ"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Monocular depth estimation in endoscopy videos can enable assistive and robotic surgery to obtain better coverage of the organ and detection of various health issues. Despite promising progress on mainstream, natural image depth estimation, techniques perform poorly on endoscopy images due to a lack of strong geometric features and challenging illumination effects. In this paper, we utilize the photometric cues, i.e., the light emitted from an endoscope and reflected by the surface, to improve monocular depth estimation. We first create two novel loss functions with supervised and self-supervised variants that utilize a per-pixel shading representation. We then propose a novel depth refinement network (PPSNet) that leverages the same per-pixel shading representation. Finally, we introduce teacher-student transfer learning to produce better depth maps from both synthetic data with supervision and clinical data with self-supervision. We achieve state-of-the-art results on the C3VD dataset while estimating high-quality depth maps from clinical data. Our code, pre-trained models, and supplementary materials can be found on our project page: https://ppsnet.github.io/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "26 pages, 7 tables, 7 figures"
    },
    {
        "paper id": "2403.17920",
        "abstract url": "https://arxiv.org/abs/2403.17920",
        "title": "TC4D: Trajectory-Conditioned Text-to-4D Generation",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "synthesize",
                "text-to-video"
            ],
            [
                "Trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent techniques for text-to-4D generation synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models. However, existing representations for motion, such as deformation models or time-dependent neural representations, are limited in the amount of motion they can generate-they cannot synthesize motion extending far beyond the bounding box used for volume rendering. The lack of a more flexible motion model contributes to the gap in realism between 4D generation methods and recent, near-photorealistic video generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D generation, which factors motion into global and local components. We represent the global motion of a scene's bounding box using rigid transformation along a trajectory parameterized by a spline. We learn local deformations that conform to the global trajectory using supervision from a text-to-video model. Our approach enables the synthesis of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to the realism and amount of generated motion, which we evaluate qualitatively and through a user study. Video results can be viewed on our website: https://sherwinbahmani.github.io/tc4d.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://sherwinbahmani.github.io/tc4d"
    },
    {
        "paper id": "2403.17992",
        "abstract url": "https://arxiv.org/abs/2403.17992",
        "title": "Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration",
        "rating": -2,
        "keywords": [
            [
                "cancer",
                "disease"
            ]
        ],
        "abstract": "Advances in artificial intelligence (AI) show great potential in revealing underlying information from phonon microscopy (high-frequency ultrasound) data to identify cancerous cells. However, this technology suffers from the 'batch effect' that comes from unavoidable technical variations between each experiment, creating confounding variables that the AI model may inadvertently learn. We therefore present a multi-task conditional neural network framework to simultaneously achieve inter-batch calibration, by removing confounding variables, and accurate cell classification of time-resolved phonon-derived signals. We validate our approach by training and validating on different experimental batches, achieving a balanced precision of 89.22% and an average cross-validated precision of 89.07% for classifying background, healthy and cancerous regions. Classification can be performed in 0.5 seconds with only simple prior batch information required for multiple batch corrections. Further, we extend our model to reconstruct denoised signals, enabling physical interpretation of salient features indicating disease state including sound velocity, sound attenuation and cell-adhesion to substrate.",
        "subjects": [
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18021",
        "abstract url": "https://arxiv.org/abs/2403.18021",
        "title": "A Study on the Use of Simulation in Synthesizing Path-Following Control Policies for Autonomous Ground Robots",
        "rating": -2,
        "keywords": [
            [
                "Synthesizing"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "We report results obtained and insights gained while answering the following question: how effective is it to use a simulator to establish path following control policies for an autonomous ground robot? While the quality of the simulator conditions the answer to this question, we found that for the simulation platform used herein, producing four control policies for path planning was straightforward once a digital twin of the controlled robot was available. The control policies established in simulation and subsequently demonstrated in the real world are PID control, MPC, and two neural network (NN) based controllers. Training the two NN controllers via imitation learning was accomplished expeditiously using seven simple maneuvers: follow three circles clockwise, follow the same circles counter-clockwise, and drive straight. A test randomization process that employs random micro-simulations is used to rank the ``goodness'' of the four control policies. The policy ranking noted in simulation correlates well with the ranking observed when the control policies were tested in the real world. The simulation platform used is publicly available and BSD3-released as open source; a public Docker image is available for reproducibility studies. It contains a dynamics engine, a sensor simulator, a ROS2 bridge, and a ROS2 autonomy stack the latter employed both in the simulator and the real world experiments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures"
    },
    {
        "paper id": "2403.18038",
        "abstract url": "https://arxiv.org/abs/2403.18038",
        "title": "TGGLinesPlus: A robust topological graph-guided computer vision algorithm for line detection from images",
        "rating": -2,
        "keywords": [
            [
                "graph"
            ],
            [
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Line detection is a classic and essential problem in image processing, computer vision and machine intelligence. Line detection has many important applications, including image vectorization (e.g., document recognition and art design), indoor mapping, and important societal challenges (e.g., sea ice fracture line extraction from satellite imagery). Many line detection algorithms and methods have been developed, but robust and intuitive methods are still lacking. In this paper, we proposed and implemented a topological graph-guided algorithm, named TGGLinesPlus, for line detection. Our experiments on images from a wide range of domains have demonstrated the flexibility of our TGGLinesPlus algorithm. We also benchmarked our algorithm with five classic and state-of-the-art line detection methods and the results demonstrate the robustness of TGGLinesPlus. We hope our open-source implementation of TGGLinesPlus will inspire and pave the way for many applications where spatial science matters.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Our TGGLinesPlus Python implementation is open source. 27 pages, 8 figures and 4 tables"
    },
    {
        "paper id": "2403.18062",
        "abstract url": "https://arxiv.org/abs/2403.18062",
        "title": "ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition",
        "rating": -2,
        "keywords": [
            [
                "robotics"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information - the object's name and the intended task - to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach's decomposition and reasoning pipeline is capable of selecting the correct part in 92% of the cases and successfully grasping the object in 82% of the tasks we evaluate. Additional videos, experiments, code, and data are available on our project website: https://shapegrasp.github.io/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2403.18085",
        "abstract url": "https://arxiv.org/abs/2403.18085",
        "title": "ANOCA: AC Network-aware Optimal Curtailment Approach for Dynamic Hosting Capacity",
        "rating": -2,
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "With exponential growth in distributed energy resources (DERs) coupled with at-capacity distribution grid infrastructure, prosumers cannot always export all extra power to the grid without violating technical limits. Consequently, a slew of dynamic hosting capacity (DHC) algorithms have emerged for optimal utilization of grid infrastructure while maximizing export from DERs. Most of these DHC algorithms utilize the concept of operating envelopes (OE), where the utility gives prosumers technical power export limits, and they are free to export power within these limits. Recent studies have shown that OE-based frameworks have drawbacks, as most develop power export limits based on convex or linear grid models. As OEs must capture extreme operating conditions, both convex and linear models can violate technical limits in practice because they approximate grid physics. However, AC models are unsuitable because they may not be feasible within the whole region of OE. We propose a new two-stage optimization framework for DHC built on three-phase AC models to address the current gaps. In this approach, the prosumers first run a receding horizon multi-period optimization to identify optimal export power setpoints to communicate with the utility. The utility then performs an infeasibility-based optimization to either accept the prosumer's request or dispatch an optimal curtail signal such that overall system technical constraints are not violated. To explore various curtailment strategies, we develop an L1, L2, and Linf norm-based dispatch algorithm with an exact three-phase AC model. We test our framework on a 1420 three-phase node meshed distribution network and show that the proposed algorithm optimally curtails DERs while guaranteeing the AC feasibility of the network.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18102",
        "abstract url": "https://arxiv.org/abs/2403.18102",
        "title": "The operadic theory of convexity",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "In this article, we characterize convexity in terms of algebras over a PROP, and establish a tensor-product-like symmetric monoidal structure on the category of convex sets. Using these two structures, and the theory of $\\scr{O}$-monoidal categories, we state and prove a Grothendieck construction for lax $\\scr{O}$-monoidal functors into convex sets. We apply this construction to the categorical characterization of entropy of Baez, Fritz, and Leinster, and to the study of quantum contextuality in the framework of simplicial distributions.",
        "subjects": [
            "math.CT"
        ],
        "comment": "42 pages"
    },
    {
        "paper id": "2403.18104",
        "abstract url": "https://arxiv.org/abs/2403.18104",
        "title": "Mathematical Foundation and Corrections for Full Range Head Pose Estimation",
        "rating": -2,
        "keywords": [
            [
                "6D"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region. However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order. Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works. In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles. When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from provided code. This paper presents (1) code and algorithms for inferring coordinate system from provided source code, code for Euler angle application order and extracting precise rotation matrices and the Euler angles, (2) code and algorithms for converting poses from one rotation system to another, (3) novel formulae for 2D augmentations of the rotation matrices, and (4) derivations and code for the correct drawing routines for rotation matrices and poses. This paper also addresses the feasibility of defining rotations with right-handed coordinate system in Wikipedia and SciPy, which makes the Euler angle extraction much easier for full-range head pose research.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18114",
        "abstract url": "https://arxiv.org/abs/2403.18114",
        "title": "Segment Any Medical Model Extended",
        "rating": -2,
        "keywords": [
            [
                "navigation"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The Segment Anything Model (SAM) has drawn significant attention from researchers who work on medical image segmentation because of its generalizability. However, researchers have found that SAM may have limited performance on medical images compared to state-of-the-art non-foundation models. Regardless, the community sees potential in extending, fine-tuning, modifying, and evaluating SAM for analysis of medical imaging. An increasing number of works have been published focusing on the mentioned four directions, where variants of SAM are proposed. To this end, a unified platform helps push the boundary of the foundation model for medical images, facilitating the use, modification, and validation of SAM and its variants in medical image segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that integrates new SAM variant models, adopts faster communication protocols, accommodates new interactive modes, and allows for fine-tuning of subcomponents of the models. These features can expand the potential of foundation models like SAM, and the results can be translated to applications such as image-guided therapy, mixed reality interaction, robotic navigation, and data augmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The content of the manuscript has been presented in SPIE Medical Imaging 2024, and had been accepted to appear in the proceedings of the conference"
    },
    {
        "paper id": "2403.18134",
        "abstract url": "https://arxiv.org/abs/2403.18134",
        "title": "Integrative Graph-Transformer Framework for Histopathology Whole Slide Image Representation and Classification",
        "rating": -2,
        "keywords": [
            [
                "Graph"
            ],
            [
                "Whole Slide"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "In digital pathology, the multiple instance learning (MIL) strategy is widely used in the weakly supervised histopathology whole slide image (WSI) classification task where giga-pixel WSIs are only labeled at the slide level. However, existing attention-based MIL approaches often overlook contextual information and intrinsic spatial relationships between neighboring tissue tiles, while graph-based MIL frameworks have limited power to recognize the long-range dependencies. In this paper, we introduce the integrative graph-transformer framework that simultaneously captures the context-aware relational features and global WSI representations through a novel Graph Transformer Integration (GTI) block. Specifically, each GTI block consists of a Graph Convolutional Network (GCN) layer modeling neighboring relations at the local instance level and an efficient global attention model capturing comprehensive global information from extensive feature embeddings. Extensive experiments on three publicly available WSI datasets: TCGA-NSCLC, TCGA-RCC and BRIGHT, demonstrate the superiority of our approach over current state-of-the-art MIL methods, achieving an improvement of 1.0% to 2.6% in accuracy and 0.7%-1.6% in AUROC.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18139",
        "abstract url": "https://arxiv.org/abs/2403.18139",
        "title": "Pseudo-MRI-Guided PET Image Reconstruction Method Based on a Diffusion Probabilistic Model",
        "rating": -2,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "MRI"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Anatomically guided PET reconstruction using MRI information has been shown to have the potential to improve PET image quality. However, these improvements are limited to PET scans with paired MRI information. In this work we employed a diffusion probabilistic model (DPM) to infer T1-weighted-MRI (deep-MRI) images from FDG-PET brain images. We then use the DPM-generated T1w-MRI to guide the PET reconstruction. The model was trained with brain FDG scans, and tested in datasets containing multiple levels of counts. Deep-MRI images appeared somewhat degraded than the acquired MRI images. Regarding PET image quality, volume of interest analysis in different brain regions showed that both PET reconstructed images using the acquired and the deep-MRI images improved image quality compared to OSEM. Same conclusions were found analysing the decimated datasets. A subjective evaluation performed by two physicians confirmed that OSEM scored consistently worse than the MRI-guided PET images and no significant differences were observed between the MRI-guided PET images. This proof of concept shows that it is possible to infer DPM-based MRI imagery to guide the PET reconstruction, enabling the possibility of changing reconstruction parameters such as the strength of the prior on anatomically guided PET reconstruction in the absence of MRI.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18178",
        "abstract url": "https://arxiv.org/abs/2403.18178",
        "title": "Online Embedding Multi-Scale CLIP Features into 3D Maps",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "robot",
                "navigation"
            ]
        ],
        "abstract": "This study introduces a novel approach to online embedding of multi-scale CLIP (Contrastive Language-Image Pre-Training) features into 3D maps. By harnessing CLIP, this methodology surpasses the constraints of conventional vocabulary-limited methods and enables the incorporation of semantic information into the resultant maps. While recent approaches have explored the embedding of multi-modal features in maps, they often impose significant computational costs, lacking practicality for exploring unfamiliar environments in real time. Our approach tackles these challenges by efficiently computing and embedding multi-scale CLIP features, thereby facilitating the exploration of unfamiliar environments through real-time map generation. Moreover, the embedding CLIP features into the resultant maps makes offline retrieval via linguistic queries feasible. In essence, our approach simultaneously achieves real-time object search and mapping of unfamiliar environments. Additionally, we propose a zero-shot object-goal navigation system based on our mapping approach, and we validate its efficacy through object-goal navigation, offline object retrieval, and multi-object-goal navigation in both simulated environments and real robot experiments. The findings demonstrate that our method not only exhibits swifter performance than state-of-the-art mapping methods but also surpasses them in terms of the success rate of object-goal navigation tasks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures"
    },
    {
        "paper id": "2403.18195",
        "abstract url": "https://arxiv.org/abs/2403.18195",
        "title": "SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, determining their correctness in manual images and providing corrections when necessary. Finally, we utilize SCANet to correct the assembly results of MEPNet. Experimental results demonstrate that SCANet can identify and correct MEPNet's misassembled results, significantly improving the correctness of assembly. Our code and dataset are available at https://github.com/Yaser-wyx/SCANet.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18197",
        "abstract url": "https://arxiv.org/abs/2403.18197",
        "title": "LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight Loco-Manipulators",
        "rating": -2,
        "keywords": [
            [
                "6D"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments. Traditional designs typically rely on the robot's inherent body parts or incorporate top-mounted arms for manipulation tasks. However, these configurations may limit the robot's operational dexterity, efficiency and adaptability, particularly in cluttered or constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments. By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace. To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators. Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Project page: https://linchangyi1.github.io/LocoMan"
    },
    {
        "paper id": "2403.18201",
        "abstract url": "https://arxiv.org/abs/2403.18201",
        "title": "Few-shot Online Anomaly Detection and Segmentation",
        "rating": -2,
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "industrial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting anomaly patterns from images is a crucial artificial intelligence technique in industrial applications. Recent research in this domain has emphasized the necessity of a large volume of training data, overlooking the practical scenario where, post-deployment of the model, unlabeled data containing both normal and abnormal samples can be utilized to enhance the model's performance. Consequently, this paper focuses on addressing the challenging yet practical few-shot online anomaly detection and segmentation (FOADS) task. Under the FOADS framework, models are trained on a few-shot normal dataset, followed by inspection and improvement of their capabilities by leveraging unlabeled streaming data containing both normal and abnormal samples simultaneously. To tackle this issue, we propose modeling the feature distribution of normal images using a Neural Gas network, which offers the flexibility to adapt the topology structure to identify outliers in the data flow. In order to achieve improved performance with limited training samples, we employ multi-scale feature embedding extracted from a CNN pre-trained on ImageNet to obtain a robust representation. Furthermore, we introduce an algorithm that can incrementally update parameters without the need to store previous samples. Comprehensive experimental results demonstrate that our method can achieve substantial performance under the FOADS setting, while ensuring that the time complexity remains within an acceptable range on MVTec AD and BTAD datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18202",
        "abstract url": "https://arxiv.org/abs/2403.18202",
        "title": "TGMM: Combining Parse Tree with GPU for Scalable Multilingual and Multi-Granularity Code Clone Detection",
        "rating": -2,
        "keywords": [
            [
                "grammar"
            ]
        ],
        "abstract": "The rapid evolution of programming languages and software systems has necessitated the implementation of multilingual and scalable clone detection tools. However, it is difficult to achieve the above requirements at the same time. Most existing tools only focus on one challenge. In this work, we propose TGMM, a tree and GPU-based tool for multilingual and multi-granularity code clone detection. By generating parse trees based on user-provided grammar files, TGMM can extract code blocks at a specified granularity and detect Type-3 clones efficiently. In order to show the performance of TGMM, we compare it with seven state-of-the-art tools in terms of recall, precision, and execution time. TGMM ranks first in execution time and precision, while its recall is comparable to the others. Moreover, we analyzed the language extensibility of TGMM across 30 mainstream programming languages. Out of these, a total of 25 languages were supported, while the remaining five currently lack the necessary grammar files. Finally, we analyzed the clone characteristics of nine popular languages at five common granularities, hoping to inspire future researchers. The source code of TGMM is available at: https://github.com/TGMM24/TGMM.git.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "14 pages, 7 figures"
    },
    {
        "paper id": "2403.18211",
        "abstract url": "https://arxiv.org/abs/2403.18211",
        "title": "NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation",
        "rating": -2,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "fMRI"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions. By training with over 60,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets. Project page: https://jingyanghuo.github.io/neuropictor/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18227",
        "abstract url": "https://arxiv.org/abs/2403.18227",
        "title": "One Backpropagation in Two Tower Recommendation Models",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "Recent years have witnessed extensive researches on developing two tower recommendation models for relieving information overload. Four building modules can be identified in such models, namely, user-item encoding, negative sampling, loss computing and back-propagation updating. To the best of our knowledge, existing algorithms have researched only on the first three modules, yet neglecting the backpropagation module. They all adopt a kind of two backpropagation strategy, which are based on an implicit assumption of equally treating users and items in the training phase. In this paper, we challenge such an equal training assumption and propose a novel one backpropagation updating strategy, which keeps the normal gradient backpropagation for the item encoding tower, but cuts off the backpropagation for the user encoding tower. Instead, we propose a moving-aggregation updating strategy to update a user encoding in each training epoch. Except the proposed backpropagation updating module, we implement the other three modules with the most straightforward choices. Experiments on four public datasets validate the effectiveness and efficiency of our model in terms of improved recommendation performance and reduced computation overload over the state-of-the-art competitors.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "9 pages, 8 figures"
    },
    {
        "paper id": "2403.18249",
        "abstract url": "https://arxiv.org/abs/2403.18249",
        "title": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges",
        "rating": -2,
        "keywords": [
            [
                "attacks"
            ],
            [
                "healthcare"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18873",
        "abstract url": "https://arxiv.org/abs/2403.18873",
        "title": "Predicting risk of cardiovascular disease using retinal OCT imaging",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "disease",
                "clinical",
                "retinal"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "We investigated the potential of optical coherence tomography (OCT) as an additional imaging technique to predict future cardiovascular disease (CVD). We utilised a self-supervised deep learning approach based on Variational Autoencoders (VAE) to learn low-dimensional representations of high-dimensional 3D OCT images and to capture distinct characteristics of different retinal layers within the OCT image. A Random Forest (RF) classifier was subsequently trained using the learned latent features and participant demographic and clinical data, to differentiate between patients at risk of CVD events (MI or stroke) and non-CVD cases. Our predictive model, trained on multimodal data, was assessed based on its ability to correctly identify individuals likely to suffer from a CVD event(MI or stroke), within a 5-year interval after image acquisition. Our self-supervised VAE feature selection and multimodal Random Forest classifier differentiate between patients at risk of future CVD events and the control group with an AUC of 0.75, outperforming the clinically established QRISK3 score (AUC= 0.597). The choroidal layer visible in OCT images was identified as an important predictor of future CVD events using a novel approach to model explanability. Retinal OCT imaging provides a cost-effective and non-invasive alternative to predict the risk of cardiovascular disease and is readily accessible in optometry practices and hospitals.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "18 pages for main manuscript, 7 figures, 2 pages for appendix and preprint for a journal"
    },
    {
        "paper id": "2403.17384",
        "abstract url": "https://arxiv.org/abs/2403.17384",
        "title": "Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation",
        "rating": -2.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "forecasting",
                "satellite"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17507",
        "abstract url": "https://arxiv.org/abs/2403.17507",
        "title": "EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields",
        "rating": -2.5,
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning force fields (MLFFs) have emerged as a promising approach to bridge the accuracy of quantum mechanical methods and the efficiency of classical force fields. However, the abundance of MLFF models and the challenge of accurately predicting atomic forces pose significant obstacles in their practical application. In this paper, we propose a novel ensemble learning framework, EL-MLFFs, which leverages the stacking method to integrate predictions from diverse MLFFs and enhance force prediction accuracy. By constructing a graph representation of molecular structures and employing a graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures atomic interactions and refines force predictions. We evaluate our approach on two distinct datasets: methane molecules and methanol adsorbed on a Cu(100) surface. The results demonstrate that EL-MLFFs significantly improves force prediction accuracy compared to individual MLFFs, with the ensemble of all eight models yielding the best performance. Moreover, our ablation study highlights the crucial roles of the residual network and graph attention layers in the model's architecture. The EL-MLFFs framework offers a promising solution to the challenges of model selection and force prediction accuracy in MLFFs, paving the way for more reliable and efficient molecular simulations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 3 figures"
    },
    {
        "paper id": "2403.17549",
        "abstract url": "https://arxiv.org/abs/2403.17549",
        "title": "Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis",
        "rating": -2.5,
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "Medical",
                "health",
                "healthcare",
                "MRI",
                "tumor"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation. Despite challenges like model complexity, the applications of generative models in healthcare, including Med-PaLM 2 technology, show promising results. By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes. However, ethical considerations and collaboration among stakeholders are essential for responsible implementation. Through experiments leveraging GANs to augment brain tumor MRI datasets, the study demonstrates how generative AI can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17607",
        "abstract url": "https://arxiv.org/abs/2403.17607",
        "title": "Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs",
        "rating": -2.5,
        "keywords": [
            [
                "Radiance Fields"
            ],
            [
                "Physics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning. In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidia's H100 GPU by up to a factor 19. The code can be found at https://github.com/intel/tiny-dpcpp-nn.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17755",
        "abstract url": "https://arxiv.org/abs/2403.17755",
        "title": "DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection",
        "rating": -2.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "Healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant. Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable. This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase. DataCook operates by \"cooking\" the raw data before distribution, enabling the development of models that perform normally on this processed data. However, during the deployment phase, the original test data must be also \"cooked\" through DataCook to ensure normal model performance. This process grants copyright holders control over authorization during the deployment phase. The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to standard adversarial examples (Adv) that aim to confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations, ensuring that the data processed by DataCook remains easily understandable. We conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D data and the high-resolution variants. The outcomes indicate that DataCook effectively meets its objectives, preventing models trained on AntiAdv from analyzing unauthorized data effectively, without compromising the validity and accuracy of the data in legitimate scenarios. Code and data are available at https://github.com/MedMNIST/DataCook.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17847",
        "abstract url": "https://arxiv.org/abs/2403.17847",
        "title": "Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections",
        "rating": -2.5,
        "keywords": [
            [
                "Super-resolution"
            ],
            [
                "forecast",
                "agricultural"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to drastically geographical elevation changes and uneven distribution through the year. Thus, it is crucial to track and predict the rainfall to make the most use of it and to prevent the floods. However, climate models have limited resolution and require intensive computational power for local-scale use. Therefore, we proposed a deep convolutional neural network with skip connections, attention blocks, and auxiliary data concatenation, in order to downscale the low-resolution precipitation data into high-resolution one. Eventually, we compare with other climate downscaling methods and show better performance in metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation, structural similarity index (SSIM), and forecast indicators.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18128",
        "abstract url": "https://arxiv.org/abs/2403.18128",
        "title": "HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks",
        "rating": -2.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "medical",
                "Health",
                "healthcare",
                "diagnosis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "While electronic health records (EHRs) are widely used across various applications in healthcare, most applications use the EHRs in their raw (tabular) format. Relying on raw or simple data pre-processing can greatly limit the performance or even applicability of downstream tasks using EHRs. To address this challenge, we present HealthGAT, a novel graph attention network framework that utilizes a hierarchical approach to generate embeddings from EHR, surpassing traditional graph-based methods. Our model iteratively refines the embeddings for medical codes, resulting in improved EHR data analysis. We also introduce customized EHR-centric auxiliary pre-training tasks to leverage the rich medical knowledge embedded within the data. This approach provides a comprehensive analysis of complex medical relationships and offers significant advancement over standard data representation techniques. HealthGAT has demonstrated its effectiveness in various healthcare scenarios through comprehensive evaluations against established methodologies. Specifically, our model shows outstanding performance in node classification and downstream tasks such as predicting readmissions and diagnosis classifications. Our code is available at https://github.com/healthylaife/HealthGAT",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18209",
        "abstract url": "https://arxiv.org/abs/2403.18209",
        "title": "Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving",
        "rating": -2.5,
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the vehicle throughout the decision-making process. In addition, we develop a safe RL method with dual-constraint optimization based on the Lagrange multiplier to optimize the training process for end-to-end autonomous driving. Comprehensive experiments were conducted on the MetaDrive simulator. Experimental results demonstrate that the proposed method achieves higher safety in continuous state and action tasks, and exhibits higher exploration performance in long-distance decision-making tasks compared with state-of-the-art methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17392",
        "abstract url": "https://arxiv.org/abs/2403.17392",
        "title": "Natural-artificial hybrid swarm: Cyborg-insect group navigation in unknown obstructed soft terrain",
        "rating": -3,
        "keywords": [
            [
                "robotics",
                "robot",
                "navigation"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "Navigating multi-robot systems in complex terrains has always been a challenging task. This is due to the inherent limitations of traditional robots in collision avoidance, adaptation to unknown environments, and sustained energy efficiency. In order to overcome these limitations, this research proposes a solution by integrating living insects with miniature electronic controllers to enable robotic-like programmable control, and proposing a novel control algorithm for swarming. Although these creatures, called cyborg insects, have the ability to instinctively avoid collisions with neighbors and obstacles while adapting to complex terrains, there is a lack of literature on the control of multi-cyborg systems. This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input. In response to this issue, we propose a novel swarm navigation algorithm addressing these challenges. The effectiveness of the algorithm is demonstrated through an experimental validation in which a cyborg swarm was successfully navigated through an unknown sandy field with obstacles and hills. This research contributes to the domain of swarm robotics and showcases the potential of integrating biological organisms with robotics and control theory to create more intelligent autonomous systems with real-world applications.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17416",
        "abstract url": "https://arxiv.org/abs/2403.17416",
        "title": "AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations",
        "rating": -3,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms. However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations. While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance. Up to now, the over-correlation issue remains unexplored in RS. Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge. To this end, this paper aims to address the aforementioned gap by undertaking a comprehensive study of the over-correlation issue in graph collaborative filtering models. Firstly, we present empirical evidence to demonstrate the widespread prevalence of over-correlation in these models. Subsequently, we dive into a theoretical analysis which establishes a pivotal connection between the over-correlation and over-smoothing issues. Leveraging these insights, we introduce the Adaptive Feature De-correlation Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies correlation penalties to the feature dimensions of the representation matrix, effectively alleviating both over-correlation and over-smoothing issues. The efficacy of the proposed framework is corroborated through extensive experiments conducted with four representative graph collaborative filtering models across four publicly available datasets.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by SIGIR2024"
    },
    {
        "paper id": "2403.17441",
        "abstract url": "https://arxiv.org/abs/2403.17441",
        "title": "Adaptive LiDAR-Radar Fusion for Outdoor Odometry Across Dense Smoke Conditions",
        "rating": -3,
        "keywords": [
            [
                "point cloud"
            ],
            [
                "LiDAR",
                "Radar"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "Robust odometry estimation in perceptually degraded environments represents a key challenge in the field of robotics. In this paper, we propose a LiDAR-radar fusion method for robust odometry for adverse environment with LiDAR degeneracy. By comparing the LiDAR point cloud with the radar static point cloud obtained through preprocessing module, it is possible to identify instances of LiDAR degeneracy to overcome perceptual limits. We demonstrate the effectiveness of our method in challenging conditions such as dense smoke, showcasing its ability to reliably estimate odometry and identify/remove dynamic points prone to LiDAR degeneracy.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17458",
        "abstract url": "https://arxiv.org/abs/2403.17458",
        "title": "Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice",
        "rating": -3,
        "keywords": [
            [
                "attacks"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2403.17551",
        "abstract url": "https://arxiv.org/abs/2403.17551",
        "title": "Time-Optimal Flight with Safety Constraints and Data-driven Dynamics",
        "rating": -3,
        "keywords": [
            [
                "Flight"
            ],
            [
                "drone"
            ]
        ],
        "abstract": "Time-optimal quadrotor flight is an extremely challenging problem due to the limited control authority encountered at the limit of handling. Model Predictive Contouring Control (MPCC) has emerged as a leading model-based approach for time optimization problems such as drone racing. However, the standard MPCC formulation used in quadrotor racing introduces the notion of the gates directly in the cost function, creating a multi-objective optimization that continuously trades off between maximizing progress and tracking the path accurately. This paper introduces three key components that enhance the MPCC approach for drone racing. First and foremost, we provide safety guarantees in the form of a constraint and terminal set. The safety set is designed as a spatial constraint which prevents gate collisions while allowing for time-optimization only in the cost function. Second, we augment the existing first principles dynamics with a residual term that captures complex aerodynamic effects and thrust forces learned directly from real world data. Third, we use Trust Region Bayesian Optimization (TuRBO), a state of the art global Bayesian Optimization algorithm, to tune the hyperparameters of the MPC controller given a sparse reward based on lap time minimization. The proposed approach achieves similar lap times to the best state-of-the-art RL and outperforms the best time-optimal controller while satisfying constraints. In both simulation and real-world, our approach consistently prevents gate crashes with 100\\% success rate, while pushing the quadrotor to its physical limit reaching speeds of more than 80km/h.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "12 pages, 7 figures"
    },
    {
        "paper id": "2403.17622",
        "abstract url": "https://arxiv.org/abs/2403.17622",
        "title": "Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "LiDAR",
                "SLAM"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17631",
        "abstract url": "https://arxiv.org/abs/2403.17631",
        "title": "AniArtAvatar: Animatable 3D Art Avatar from a Single Image",
        "rating": -3,
        "keywords": [
            [
                "3D",
                "Avatar"
            ],
            [
                "diffusion",
                "synthesize"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements. Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D diffusion model to synthesize multi-view images from a single art portrait with a neutral expression. With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface. For avatar animation, we extract control points, transfer the motion with these points, and deform the implicit canonical space. Firstly, we render the front image of the avatar, extract the 2D landmarks, and project them to the 3D space using a trained SDF network. We extract 3D driving landmarks using 3DMM and transfer the motion to the avatar landmarks. To animate the avatar pose, we manually set the body height and bound the head and torso of an avatar with two cages. The head and torso can be animated by transforming the two cages. Our approach is a one-shot pipeline that can be applied to various styles. Experiments demonstrate that our method can generate high-quality 3D art avatars with desired control over different motions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17634",
        "abstract url": "https://arxiv.org/abs/2403.17634",
        "title": "Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems",
        "rating": -3,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurations. This adaptive approach selectively masks input tokens, transforming the recommendation task into an inference challenge based on varying token subsets, thereby enhancing the agent's ability to infer across diverse trajectory lengths. Furthermore, we incorporate a multi-scale segmented retention mechanism that facilitates efficient modeling of long sequences, significantly enhancing computational efficiency. Our experimental analysis, conducted on both online simulator and offline datasets, clearly demonstrates the advantages of our proposed method.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17656",
        "abstract url": "https://arxiv.org/abs/2403.17656",
        "title": "SGHormer: An Energy-Saving Graph Transformer Driven by Spikes",
        "rating": -3,
        "keywords": [
            [
                "Graph"
            ],
            [
                "biologically"
            ]
        ],
        "abstract": "Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHormer explicitly capture global structure information and recover the expressive power of spiking embeddings, respectively. In experiments, SGHormer achieves comparable performances to other full-precision GTs with extremely low computational energy consumption. The results show that SGHomer makes a remarkable progress in the field of low-energy GTs.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "9 pages, 3 figures"
    },
    {
        "paper id": "2403.17664",
        "abstract url": "https://arxiv.org/abs/2403.17664",
        "title": "DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with Space-sensitive Customization and Semantic Preservation",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model. Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17694",
        "abstract url": "https://arxiv.org/abs/2403.17694",
        "title": "AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion",
                "Synthesis"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at https://github.com/scutzzj/AniPortrait",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17794",
        "abstract url": "https://arxiv.org/abs/2403.17794",
        "title": "Fermihedral: On the Optimal Compilation for Fermion-to-Qubit Encoding",
        "rating": -3,
        "keywords": [
            [
                "depth"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "This paper introduces Fermihedral, a compiler framework focusing on discovering the optimal Fermion-to-qubit encoding for targeted Fermionic Hamiltonians. Fermion-to-qubit encoding is a crucial step in harnessing quantum computing for efficient simulation of Fermionic quantum systems. Utilizing Pauli algebra, Fermihedral redefines complex constraints and objectives of Fermion-to-qubit encoding into a Boolean Satisfiability problem which can then be solved with high-performance solvers. To accommodate larger-scale scenarios, this paper proposed two new strategies that yield approximate optimal solutions mitigating the overhead from the exponentially large number of clauses. Evaluation across diverse Fermionic systems highlights the superiority of Fermihedral, showcasing substantial reductions in implementation costs, gate counts, and circuit depth in the compiled circuits. Real-system experiments on IonQ's device affirm its effectiveness, notably enhancing simulation accuracy.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17881",
        "abstract url": "https://arxiv.org/abs/2403.17881",
        "title": "Deepfake Generation and Detection: A Benchmark and Survey",
        "rating": -3,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "attacks"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential published works. Finally, we analyze challenges and future research directions of the discussed fields.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "We closely follow the latest developments in https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection"
    },
    {
        "paper id": "2403.17933",
        "abstract url": "https://arxiv.org/abs/2403.17933",
        "title": "SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models",
        "rating": -3,
        "keywords": [
            [
                "Diffusion",
                "Synthesizing"
            ],
            [
                "vehicle"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns or increasing traffic density. Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan. It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model. Compared to nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it a more accessible option and helping with democratizing future research in this field.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18028",
        "abstract url": "https://arxiv.org/abs/2403.18028",
        "title": "Predicting Species Occurrence Patterns from Partial Observations",
        "rating": -3.0,
        "keywords": [
            [
                "biodiversity"
            ],
            [
                "satellite"
            ],
            [
                "cs.LG"
            ],
            [
                "workshop",
                "ICLR"
            ]
        ],
        "abstract": "To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial information both within a taxon (birds) and across taxa (birds and butterflies). Our approach opens new perspectives to leveraging insights from species with abundant data to other species with scarce data, by modelling the ecosystems in which they co-occur.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Tackling Climate Change with Machine Learning workshop at ICLR 2024"
    },
    {
        "paper id": "2403.18030",
        "abstract url": "https://arxiv.org/abs/2403.18030",
        "title": "EinExprs: Contraction Paths of Tensor Networks as Symbolic Expressions",
        "rating": -3,
        "keywords": [
            [
                "graph"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Tensor Networks are graph representations of summation expressions in which vertices represent tensors and edges represent tensor indices or vector spaces. In this work, we present EinExprs.jl, a Julia package for contraction path optimization that offers state-of-art optimizers. We propose a representation of the contraction path of a Tensor Network based on symbolic expressions. Using this package the user may choose among a collection of different methods such as Greedy algorithms, or an approach based on the hypergraph partitioning problem. We benchmark this library with examples obtained from the simulation of Random Quantum Circuits (RQC), a well known example where Tensor Networks provide state-of-the-art methods.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "4 pages, 5 figures, submitted to JuliaCon Proceedings 2023"
    },
    {
        "paper id": "2403.18033",
        "abstract url": "https://arxiv.org/abs/2403.18033",
        "title": "SpectralWaste Dataset: Multimodal Data for Waste Sorting Automation",
        "rating": -3,
        "keywords": [
            [
                "biodegradable"
            ],
            [
                "industrial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The increase in non-biodegradable waste is a worldwide concern. Recycling facilities play a crucial role, but their automation is hindered by the complex characteristics of waste recycling lines like clutter or object deformation. In addition, the lack of publicly available labeled data for these environments makes developing robust perception systems challenging. Our work explores the benefits of multimodal perception for object segmentation in real waste management scenarios. First, we present SpectralWaste, the first dataset collected from an operational plastic waste sorting facility that provides synchronized hyperspectral and conventional RGB images. This dataset contains labels for several categories of objects that commonly appear in sorting plants and need to be detected and separated from the main trash flow for several reasons, such as security in the management line or reuse. Additionally, we propose a pipeline employing different object segmentation architectures and evaluate the alternatives on our dataset, conducting an extensive analysis for both multimodal and unimodal alternatives. Our evaluation pays special attention to efficiency and suitability for real-time processing and demonstrates how HSI can bring a boost to RGB-only perception in these realistic industrial settings without much computational overhead.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18151",
        "abstract url": "https://arxiv.org/abs/2403.18151",
        "title": "Automated Report Generation for Lung Cytological Images Using a CNN Vision Classifier and Multiple-Transformer Text Decoders: Preliminary Study",
        "rating": -3,
        "keywords": [
            [
                "diagnosis",
                "cancer"
            ],
            [
                "grammar"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Cytology plays a crucial role in lung cancer diagnosis. Pulmonary cytology involves cell morphological characterization in the specimen and reporting the corresponding findings, which are extremely burdensome tasks. In this study, we propose a report-generation technique for lung cytology images. In total, 71 benign and 135 malignant pulmonary cytology specimens were collected. Patch images were extracted from the captured specimen images, and the findings were assigned to each image as a dataset for report generation. The proposed method consists of a vision model and a text decoder. In the former, a convolutional neural network (CNN) is used to classify a given image as benign or malignant, and the features related to the image are extracted from the intermediate layer. Independent text decoders for benign and malignant cells are prepared for text generation, and the text decoder switches according to the CNN classification results. The text decoder is configured using a Transformer that uses the features obtained from the CNN for report generation. Based on the evaluation results, the sensitivity and specificity were 100% and 96.4%, respectively, for automated benign and malignant case classification, and the saliency map indicated characteristic benign and malignant areas. The grammar and style of the generated texts were confirmed as correct and in better agreement with gold standard compared to existing LLM-based image-captioning methods and single-text-decoder ablation model. These results indicate that the proposed method is useful for pulmonary cytology classification and reporting.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2403.18172",
        "abstract url": "https://arxiv.org/abs/2403.18172",
        "title": "Vision-Based Force Estimation for Minimally Invasive Telesurgery Through Contact Detection and Local Stiffness Models",
        "rating": -3,
        "keywords": [
            [
                "robot"
            ],
            [
                "surgical",
                "clinical"
            ]
        ],
        "abstract": "In minimally invasive telesurgery, obtaining accurate force information is difficult due to the complexities of in-vivo end effector force sensing. This constrains development and implementation of haptic feedback and force-based automated performance metrics, respectively. Vision-based force sensing approaches using deep learning are a promising alternative to intrinsic end effector force sensing. However, they have limited ability to generalize to novel scenarios, and require learning on high-quality force sensor training data that can be difficult to obtain. To address these challenges, this paper presents a novel vision-based contact-conditional approach for force estimation in telesurgical environments. Our method leverages supervised learning with human labels and end effector position data to train deep neural networks. Predictions from these trained models are optionally combined with robot joint torque information to estimate forces indirectly from visual data. We benchmark our method against ground truth force sensor data and demonstrate generality by fine-tuning to novel surgical scenarios in a data-efficient manner. Our methods demonstrated greater than 90% accuracy on contact detection and less than 10% force prediction error. These results suggest potential usefulness of contact-conditional force estimation for sensory substitution haptic feedback and tissue handling skill evaluation in clinical settings.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Preprint of an article accepted in Journal of Medical Robotics Research \u00a92024 copyright World Scientific Publishing Company"
    },
    {
        "paper id": "2403.18206",
        "abstract url": "https://arxiv.org/abs/2403.18206",
        "title": "Sailing Through Point Clouds: Safe Navigation Using Point Cloud Based Control Barrier Functions",
        "rating": -3,
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "synthesizing"
            ],
            [
                "Navigation"
            ]
        ],
        "abstract": "The capability to navigate safely in an unstructured environment is crucial when deploying robotic systems in real-world scenarios. Recently, control barrier function (CBF) based approaches have been highly effective in synthesizing safety-critical controllers. In this work, we propose a novel CBF-based local planner comprised of two components: Vessel and Mariner. The Vessel is a novel scaling factor based CBF formulation that synthesizes CBFs using only point cloud data. The Mariner is a CBF-based preview control framework that is used to mitigate getting stuck in spurious equilibria during navigation. To demonstrate the efficacy of our proposed approach, we first compare the proposed point cloud based CBF formulation with other point cloud based CBF formulations. Then, we demonstrate the performance of our proposed approach and its integration with global planners using experimental studies on the Unitree B1 and Unitree Go2 quadruped robots in various environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18247",
        "abstract url": "https://arxiv.org/abs/2403.18247",
        "title": "An Experimentally Validated Feasible Quantum Protocol for Identity-Based Signature with Application to Secure Email Communication",
        "rating": -3,
        "keywords": [
            [
                "attacks"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Digital signatures are one of the simplest cryptographic building blocks that provide appealing security characteristics such as authenticity, unforgeability, and undeniability. In 1984, Shamir developed the first Identity-based signature (IBS) to simplify public key infrastructure and circumvent the need for certificates. It makes the process uncomplicated by enabling users to verify digital signatures using only the identifiers of signers, such as email, phone number, etc. Nearly all existing IBS protocols rely on several theoretical assumption-based hard problems. Unfortunately, these hard problems are unsafe and pose a hazard in the quantum realm. Thus, designing IBS algorithms that can withstand quantum attacks and ensure long-term security is an important direction for future research. Quantum cryptography (QC) is one such approach. In this paper, we propose an IBS based on QC. Our scheme's security is based on the laws of quantum mechanics. It thereby achieves long-term security and provides resistance against quantum attacks. We verify the proposed design's correctness and feasibility by simulating it in a prototype quantum device and the IBM Qiskit quantum simulator. The implementation code in qiskit with Jupyternotebook is provided in the Annexure. Moreover, we discuss the application of our design in secure email communication.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.16040",
        "abstract url": "https://arxiv.org/abs/2404.16040",
        "title": "Pilot Study to Discover Candidate Biomarkers for Autism based on Perception and Production of Facial Expressions",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "Biomarkers",
                "Facial"
            ]
        ],
        "abstract": "Purpose: Facial expression production and perception in autism spectrum disorder (ASD) suggest potential presence of behavioral biomarkers that may stratify individuals on the spectrum into prognostic or treatment subgroups. Construct validity and group discriminability have been recommended as criteria for identification of candidate stratification biomarkers. Methods: In an online pilot study of 11 children and young adults diagnosed with ASD and 11 age- and gender-matched neurotypical (NT) individuals, participants recognize and mimic static and dynamic facial expressions of 3D avatars. Webcam-based eye-tracking (ET) and facial video tracking (VT), including activation and asymmetry of action units (AUs) from the Facial Action Coding System (FACS) are collected. We assess validity of constructs for each dependent variable (DV) based on the expected response in the NT group. Then, the Boruta statistical method identifies DVs that are significant to group discriminability (ASD or NT). Results: We identify one candidate ET biomarker (percentage gaze duration to the face while mimicking static 'disgust' expression) and 14 additional DVs of interest for future study, including 4 ET DVs, 5 DVs related to VT AU activation, and 4 DVs related to AU asymmetry in VT. Based on a power analysis, we provide sample size recommendations for future studies. Conclusion: This pilot study provides a framework for ASD stratification biomarker discovery based on perception and production of facial expressions.",
        "subjects": [
            "q-bio.NC"
        ],
        "comment": "18 pages, 3 figures, 5 tables"
    },
    {
        "paper id": "2403.17745",
        "abstract url": "https://arxiv.org/abs/2403.17745",
        "title": "Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients",
        "rating": -3.5,
        "keywords": [
            [
                "healthcare",
                "Disease",
                "clinical"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information. However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental results on two real-world datasets demonstrate that RAREMed provides accurate drug sets for both rare and common disease patients, thereby mitigating unfairness in medication recommendation systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18100",
        "abstract url": "https://arxiv.org/abs/2403.18100",
        "title": "Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning",
        "rating": -3.5,
        "keywords": [
            [
                "medical"
            ],
            [
                "IoT"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This article explores how to drive intelligent iot monitoring and control through cloud computing and machine learning. As iot and the cloud continue to generate large and diverse amounts of data as sensor devices in the network, the collected data is sent to the cloud for statistical analysis, prediction, and data analysis to achieve business objectives. However, because the cloud computing model is limited by distance, it can be problematic in environments where the quality of the Internet connection is not ideal for critical operations. Therefore, edge computing, as a distributed computing architecture, moves the location of processing applications, data and services from the central node of the network to the logical edge node of the network to reduce the dependence on cloud processing and analysis of data, and achieve near-end data processing and analysis. The combination of iot and edge computing can reduce latency, improve efficiency, and enhance security, thereby driving the development of intelligent systems. The paper also introduces the development of iot monitoring and control technology, the application of edge computing in iot monitoring and control, and the role of machine learning in data analysis and fault detection. Finally, the application and effect of intelligent Internet of Things monitoring and control system in industry, agriculture, medical and other fields are demonstrated through practical cases and experimental studies.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17774",
        "abstract url": "https://arxiv.org/abs/2403.17774",
        "title": "LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous Navigation in Agriculture Fields",
        "rating": -4,
        "keywords": [
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "robotics",
                "robot",
                "Navigation"
            ],
            [
                "agricultural"
            ]
        ],
        "abstract": "Autonomous navigation is crucial for various robotics applications in agriculture. However, many existing methods depend on RTK-GPS systems, which are expensive and susceptible to poor signal coverage. This paper introduces a state-of-the-art LiDAR-based navigation system that can achieve over-canopy autonomous navigation in row-crop fields, even when the canopy fully blocks the interrow spacing. Our crop row detection algorithm can detect crop rows across diverse scenarios, encompassing various crop types, growth stages, weeds presence, and discontinuities within the crop rows. Without utilizing the global localization of the robot, our navigation system can perform autonomous navigation in these challenging scenarios, detect the end of the crop rows, and navigate to the next crop row autonomously, providing a crop-agnostic approach to navigate the whole row-crop field. This navigation system has undergone tests in various simulated agricultural fields, achieving an average of 2.98cm autonomous driving accuracy without human intervention on the custom Amiga robot. In addition, the qualitative results of our crop row detection algorithm from the actual soybean fields validate our LiDAR-based crop row detection algorithm's potential for practical agricultural applications.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 9 figures, submitted to IROS 2024"
    },
    {
        "paper id": "2403.18026",
        "abstract url": "https://arxiv.org/abs/2403.18026",
        "title": "Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network",
        "rating": -4,
        "keywords": [
            [
                "GAN"
            ],
            [
                "biological"
            ],
            [
                "image quality enhancement"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "High-quality fluorescence imaging of biological systems is limited by processes like photobleaching and phototoxicity, and also in many cases, by limited access to the latest generations of microscopes. Moreover, low temporal resolution can lead to a motion blur effect in living systems. Our work presents a deep learning (DL) generative-adversarial approach to the problem of obtaining high-quality (HQ) images based on their low-quality (LQ) equivalents. We propose a generative-adversarial network (GAN) for contrast transfer between two different separate microscopy systems: a confocal microscope (producing HQ images) and a wide-field fluorescence microscope (producing LQ images). Our model proves that such transfer is possible, allowing us to receive HQ-generated images characterized by low mean squared error (MSE) values, high structural similarity index (SSIM), and high peak signal-to-noise ratio (PSNR) values. For our best model in the case of comparing HQ-generated images and HQ-ground truth images, the median values of the metrics are 6x10-4, 0.9413, and 31.87, for MSE, SSIM, and PSNR, respectively. In contrast, in the case of comparison between LQ and HQ ground truth median values of the metrics are equal to 0.0071, 0.8304, and 21.48 for MSE, SSIM, and PSNR respectively. Therefore, we observe a significant increase ranging from 14% to 49% for SSIM and PSNR respectively. These results, together with other single-system cross-modality studies, provide proof of concept for further implementation of a cross-system biological image quality enhancement.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "15 Pages, 5 Figures, 1 Table, 3 pages Supplementary Materials"
    },
    {
        "paper id": "2404.15297",
        "abstract url": "https://arxiv.org/abs/2404.15297",
        "title": "Multi-stream Transmission for Directional Modulation Network via Distributed Multi-UAV-aided Multi-active-IRS",
        "rating": -4,
        "keywords": [
            [
                "6G"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Active intelligent reflecting surface (IRS) is a revolutionary technique for the future 6G networks. The conventional far-field single-IRS-aided directional modulation(DM) networks have only one (no direct path) or two (existing direct path) degrees of freedom (DoFs). This means that there are only one or two streams transmitted simultaneously from base station to user and will seriously limit its rate gain achieved by IRS. How to create multiple DoFs more than two for DM? In this paper, single large-scale IRS is divided to multiple small IRSs and a novel multi-IRS-aided multi-stream DM network is proposed to achieve a point-to-point multi-stream transmission by creating $K$ ($\\geq3$) DoFs, where multiple small IRSs are placed distributively via multiple unmanned aerial vehicles (UAVs). The null-space projection, zero-forcing (ZF) and phase alignment are adopted to design the transmit beamforming vector, receive beamforming vector and phase shift matrix (PSM), respectively, called NSP-ZF-PA. Here, $K$ PSMs and their corresponding beamforming vectors are independently optimized. The weighted minimum mean-square error (WMMSE) algorithm is involved in alternating iteration for the optimization variables by introducing the power constraint on IRS, named WMMSE-PC, where the majorization-minimization (MM) algorithm is used to solve the total PSM. To achieve a lower computational complexity, a maximum trace method, called Max-TR-SVD, is proposed by optimize the PSM of all IRSs. Numerical simulation results has shown that the proposed NSP-ZF-PA performs much better than Max-TR-SVD in terms of rate. In particular, the rate of NSP-ZF-PA with sixteen small IRSs is about five times that of NSP-ZF-PA with combining all small IRSs as a single large IRS. Thus, a dramatic rate enhancement may be achieved by multiple distributed IRSs.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.00693",
        "abstract url": "https://arxiv.org/abs/2405.00693",
        "title": "Large Language Models for Human-Robot Interaction: Opportunities and Risks",
        "rating": -5,
        "keywords": [
            [
                "robotics",
                "Robot"
            ],
            [
                "healthcare"
            ],
            [
                "forecast"
            ]
        ],
        "abstract": "The tremendous development in large language models (LLM) has led to a new wave of innovations and applications and yielded research results that were initially forecast to take longer. In this work, we tap into these recent developments and present a meta-study about the potential of large language models if deployed in social robots. We place particular emphasis on the applications of social robots: education, healthcare, and entertainment. Before being deployed in social robots, we also study how these language models could be safely trained to ``understand'' societal norms and issues, such as trust, bias, ethics, cognition, and teamwork. We hope this study provides a resourceful guide to other robotics researchers interested in incorporating language models in their robots.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17788",
        "abstract url": "https://arxiv.org/abs/2403.17788",
        "title": "System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners",
        "rating": -7,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Robot"
            ],
            [
                "graph"
            ],
            [
                "industrial"
            ],
            [
                "agricultural"
            ]
        ],
        "abstract": "The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications. This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation. The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud. Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration. The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization. Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration. Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17391",
        "abstract url": "https://arxiv.org/abs/2403.17391",
        "title": "Reverse Kron reduction of Multi-phase Radial Network",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider the problem of identifying the admittance matrix of a three-phase radial network from voltage and current measurements at a subset of nodes. These measurements are used to estimate a virtual network represented by the Kron reduction (Schur complement) of the full admittance matrix. We focus on recovering exactly the full admittance matrix from its Kron reduction, i.e., computing the inverse of Schur complement. The key idea is to decompose Kron reduction into a sequence of iterations that maintains an invariance structure, and exploit this structure to reverse each step of the iterative Kron reduction.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17414",
        "abstract url": "https://arxiv.org/abs/2403.17414",
        "title": "The Privacy Policy Permission Model: A Unified View of Privacy Policies",
        "rating": -10,
        "keywords": [],
        "abstract": "Organizations use privacy policies to communicate their data collection practices to their clients. A privacy policy is a set of statements that specifies how an organization gathers, uses, discloses, and maintains a client's data. However, most privacy policies lack a clear, complete explanation of how data providers' information is used. We propose a modeling methodology, called the Privacy Policy Permission Model (PPPM), that provides a uniform, easy-to-understand representation of privacy policies, which can accurately and clearly show how data is used within an organization's practice. Using this methodology, a privacy policy is captured as a diagram. The diagram is capable of highlighting inconsistencies and inaccuracies in the privacy policy. The methodology supports privacy officers in properly and clearly articulating an organization's privacy policy.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "23 pages + 2 pages references + 11 Pages Appendix, 19 figures,Published in teh Trasactions on Data Privacy in April 2021"
    },
    {
        "paper id": "2403.17417",
        "abstract url": "https://arxiv.org/abs/2403.17417",
        "title": "Cyclic pursuit formation control for arbitrary desired shapes",
        "rating": -10,
        "keywords": [],
        "abstract": "A multi-agent system comprises numerous agents that autonomously make decisions to collectively accomplish tasks, drawing significant attention for their wide-ranging applications. Within this context, formation control emerges as a prominent task, wherein agents collaboratively shape and maneuver while preserving formation integrity. Our focus centers on cyclic pursuit, a method facilitating the formation of circles, ellipses, and figure-eights under the assumption that agents can only perceive the relative positions of those preceding them. However, this method's scope has been restricted to these specific shapes, leaving the feasibility of forming other shapes uncertain. In response, our study proposes a novel method based on cyclic pursuit capable of forming a broader array of shapes, enabling agents to individually shape while pursuing preceding agents, thereby extending the repertoire of achievable formations. We present two scenarios concerning the information available to agents and devise formation control methods tailored to each scenario. Through extensive simulations, we demonstrate the efficacy of our proposed method in forming multiple shapes, including those represented as Fourier series, thereby underscoring the versatility and effectiveness of our approach.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17425",
        "abstract url": "https://arxiv.org/abs/2403.17425",
        "title": "Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model",
        "rating": -10,
        "keywords": [],
        "abstract": "In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario included as two features is not accurate enough. In this paper, we propose the Masked Multi-domain Network (MMN) to solve this problem. To achieve the accuracy requirement, we model domain-specific parameters and propose a dynamically weighted loss to account for the loss scale imbalance issue within each mini-batch. To achieve the scalability requirement, we propose a parameter sharing and composition strategy to reduce model parameters from a product space to a sum space. To achieve the convenience requirement, we propose an auto-masking strategy which can take mixed data from all the domains as input. It avoids the overhead caused by data partitioning, individual processing and separate storage. Both offline and online experimental results validate the superiority of MMN for multi-type and multi-scenario CVR prediction. MMN is now the serving model for real-time CVR prediction in UC Toutiao.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "CIKM 2023 (larger figures)"
    },
    {
        "paper id": "2403.17430",
        "abstract url": "https://arxiv.org/abs/2403.17430",
        "title": "Java Classes with \"-Er\" and \"-Utils\" Suffixes Have Higher Complexity",
        "rating": -10,
        "keywords": [],
        "abstract": "In object-oriented programming languages, a belief exists that classes with -Er/-Or and -Utils suffixes are \"code smells\" because they take over a lot of functional responsibility, turning out to be bulky and complicated, and therefore making it more difficult to maintain the code. In order to validate this intuition, we analyzed complexity and cohesion of 13,861 Java classes from 212 unique open-source GitHub repositories. We found out that average values of Cyclomatic Complexity and Cognitive Complexity metrics are at least 2.5 times higher when suffixes are present.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17437",
        "abstract url": "https://arxiv.org/abs/2403.17437",
        "title": "An Empirical Study of ChatGPT-related projects on GitHub",
        "rating": -10,
        "keywords": [],
        "abstract": "As ChatGPT possesses powerful capabilities in natural language processing and code analysis, it has received widespread attention since its launch. Developers have applied its powerful capabilities to various domains through software projects which are hosted on the largest open-source platform (GitHub) worldwide. Simultaneously, these projects have triggered extensive discussions. In order to comprehend the research content of these projects and understand the potential requirements discussed, we collected ChatGPT-related projects from the GitHub platform and utilized the LDA topic model to identify the discussion topics. Specifically, we selected 200 projects, categorizing them into three primary categories through analyzing their descriptions: ChatGPT implementation & training, ChatGPT application, ChatGPT improvement & extension. Subsequently, we employed the LDA topic model to identify 10 topics from issue texts, and compared the distribution and evolution trend of the discovered topics within the three primary project categories. Our observations include (1) The number of projects growing in a single month for the three primary project categories are closely associated with the development of ChatGPT. (2) There exist significant variations in the popularity of each topic for the three primary project categories. (3) The monthly changes in the absolute impact of each topic for the three primary project categories are diverse, which is often closely associated with the variation in the number of projects owned by that category. (4) With the passage of time, the relative impact of each topic exhibits different development trends in the three primary project categories. Based on these findings, we discuss implications for developers and users.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17440",
        "abstract url": "https://arxiv.org/abs/2403.17440",
        "title": "$(\u03c9_1, \u03c9_2)$-Temporal random hyperbolic graphs",
        "rating": -10,
        "keywords": [],
        "abstract": "We extend a recent model of temporal random hyperbolic graphs by allowing connections and disconnections to persist across network snapshots with different probabilities, $\u03c9_1$ and $\u03c9_2$. This extension, while conceptually simple, poses analytical challenges involving the Appell $F_1$ series. Despite these challenges, we are able to analyze key properties of the model, which include the distributions of contact and intercontact durations, as well as the expected time-aggregated degree. The incorporation of $\u03c9_1$ and $\u03c9_2$ enables more flexible tuning of the average contact and intercontact durations, and of the average time-aggregated degree, providing a finer control for exploring the effect of temporal network dynamics on epidemic processes. Overall, our results provide new insights into the analysis of temporal networks and contribute to a more general representation of real-world scenarios.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17469",
        "abstract url": "https://arxiv.org/abs/2403.17469",
        "title": "Geometric planted matchings beyond the Gaussian model",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider the problem of recovering an unknown matching between a set of $n$ randomly placed points in $\\mathbb{R}^d$ and random perturbations of these points. This can be seen as a model for particle tracking and more generally, entity resolution. We use matchings in random geometric graphs to derive minimax lower bounds for this problem that hold under great generality. Using these results we show that for a broad class of distributions, the order of the number of mistakes made by an estimator that minimizes the sum of squared Euclidean distances is minimax optimal when $d$ is fixed and is optimal up to $n^{o(1)}$ factors when $d = o(\\log n)$. In the high-dimensional regime we consider a setup where both initial positions and perturbations have independent sub-Gaussian coordinates. In this setup we give sufficient conditions under which the same estimator makes no mistakes with high probability. We prove an analogous result for an adapted version of this estimator that incorporates information on the covariance matrix of the perturbations.",
        "subjects": [
            "math.ST"
        ],
        "comment": "36 pages, 2 figures"
    },
    {
        "paper id": "2403.17473",
        "abstract url": "https://arxiv.org/abs/2403.17473",
        "title": "Document Set Expansion with Positive-Unlabelled Learning Using Intractable Density Estimation",
        "rating": -10,
        "keywords": [],
        "abstract": "The Document Set Expansion (DSE) task involves identifying relevant documents from large collections based on a limited set of example documents. Previous research has highlighted Positive and Unlabeled (PU) learning as a promising approach for this task. However, most PU methods rely on the unrealistic assumption of knowing the class prior for positive samples in the collection. To address this limitation, this paper introduces a novel PU learning framework that utilizes intractable density estimation models. Experiments conducted on PubMed and Covid datasets in a transductive setting showcase the effectiveness of the proposed method for DSE. Code is available from https://github.com/Beautifuldog01/Document-set-expansion-puDE.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted at LREC-COLING 2024. arXiv admin note: text overlap with arXiv:2401.11145"
    },
    {
        "paper id": "2403.17476",
        "abstract url": "https://arxiv.org/abs/2403.17476",
        "title": "A TDD Distributed MIMO Testbed Using a 1-Bit Radio-Over-Fiber Fronthaul Architecture",
        "rating": -10,
        "keywords": [],
        "abstract": "We present the uplink and downlink of a time-division duplex distributed multiple-input multiple-output (D-MIMO) testbed, based on a 1-bit radio-over-fiber architecture, which is low-cost and scalable. The proposed architecture involves a central unit (CU) that is equipped with 1-bit digital-to-analog and analog-to-digital converters, operating at 10 GS/s. The CU is connected to multiple single-antenna remote radio heads (RRHs) via optical fibers, over which a binary RF waveform is transmitted. In the uplink, a binary RF waveform is generated at the RRHs by a comparator, whose inputs are the received RF signal and a suitably designed dither signal. In the downlink, a binary RF waveform is generated at the CU via bandpass sigma-delta modulation. Our measurement results show that low error-vector magnitude (EVM) can be achieved in both the uplink and the downlink, despite 1-bit sampling at the CU. Specifically, for point-to-point over-cable transmission between a single user equipment (UE) and a CU equipped with a single RRH, we report, for a 10 MBd signal using single-carrier 16QAM modulation, an EVM of 3.3% in the downlink, and of 4.5% in the uplink. We then consider a CU connected to 3 RRHs serving over the air 2 UEs, and show that, after over-the-air reciprocity calibration, a downlink zero-forcing precoder designed on the basis of uplink channel estimates at the CU, achieves an EVM of 6.4% and 10.9% at UE 1 and UE 2, respectively. Finally, we investigate the ability of the proposed architecture to support orthogonal frequency-division multiplexing (OFDM) waveforms, and its robustness against both in-band and out-of-band interference.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted for publication in IEEE Transactions on Microwave Theory and Techniques"
    },
    {
        "paper id": "2403.17480",
        "abstract url": "https://arxiv.org/abs/2403.17480",
        "title": "Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost",
        "rating": -10,
        "keywords": [],
        "abstract": "An online non-convex optimization problem is considered where the goal is to minimize the flow time (total delay) of a set of jobs by modulating the number of active servers, but with a switching cost associated with changing the number of active servers over time. Each job can be processed by at most one fixed speed server at any time. Compared to the usual online convex optimization (OCO) problem with switching cost, the objective function considered is non-convex and more importantly, at each time, it depends on all past decisions and not just the present one. Both worst-case and stochastic inputs are considered; for both cases, competitive algorithms are derived.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17509",
        "abstract url": "https://arxiv.org/abs/2403.17509",
        "title": "Computer classification of linear codes based on lattice point enumeration and integer linear programming",
        "rating": -10,
        "keywords": [],
        "abstract": "Linear codes play a central role in coding theory and have applications in several branches of mathematics. For error correction purposes the minimum Hamming distance should be as large as possible. Linear codes related to applications in Galois Geometry often require a certain divisibility of the occurring weights. In this paper we present an algorithmic framework for the classification of linear codes over finite fields with restricted sets of weights. The underlying algorithms are based on lattice point enumeration and integer linear programming. We present new enumeration and non-existence results for projective two-weight codes, divisible codes, and additive $\\mathbb{F}_4$-codes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2403.17515",
        "abstract url": "https://arxiv.org/abs/2403.17515",
        "title": "Prediction-sharing During Training and Inference",
        "rating": -10,
        "keywords": [],
        "abstract": "Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it. Within these two settings we study optimal contract choice. More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal. Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic simulation using real loan data.",
        "subjects": [
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17524",
        "abstract url": "https://arxiv.org/abs/2403.17524",
        "title": "Provably Secure Disambiguating Neural Linguistic Steganography",
        "rating": -10,
        "keywords": [],
        "abstract": "Recent research in provably secure neural linguistic steganography has overlooked a crucial aspect: the sender must detokenize stegotexts to avoid raising suspicion from the eavesdropper. The segmentation ambiguity problem, which arises when using language models based on subwords, leads to occasional decoding failures in all neural language steganography implementations based on these models. Current solutions to this issue involve altering the probability distribution of candidate words, rendering them incompatible with provably secure steganography. We propose a novel secure disambiguation method named SyncPool, which effectively addresses the segmentation ambiguity problem. We group all tokens with prefix relationships in the candidate pool before the steganographic embedding algorithm runs to eliminate uncertainty among ambiguous tokens. To enable the receiver to synchronize the sampling process of the sender, a shared cryptographically-secure pseudorandom number generator (CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does not change the size of the candidate pool or the distribution of tokens and thus is applicable to provably secure language steganography methods. We provide theoretical proofs and experimentally demonstrate the applicability of our solution to various languages and models, showing its potential to significantly improve the reliability and security of neural linguistic steganography systems.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17531",
        "abstract url": "https://arxiv.org/abs/2403.17531",
        "title": "Design and Preliminary Evaluation of a Torso Stabiliser for Individuals with Spinal Cord Injury",
        "rating": -10,
        "keywords": [],
        "abstract": "Spinal cord injuries (SCIs) generally result in sensory and mobility impairments, with torso instability being particularly debilitating. Existing torso stabilisers are often rigid and restrictive. This paper presents an early investigation into a non-restrictive 1 degree-of-freedom (DoF) mechanical torso stabiliser inspired by devices such as centrifugal clutches and seat-belt mechanisms. Firstly, the paper presents a motion-capture (MoCap) and OpenSim-based kinematic analysis of the cable-based system to understand requisite device characteristics. The simulated evaluation resulted in the cable-based device to require 55-60cm of unrestricted travel, and to lock at a threshold cable velocity of 80-100cm/sec. Next, the developed 1-DoF device is introduced. The proposed mechanical device is transparent during activities of daily living, and transitions to compliant blocking when incipient fall is detected. Prototype behaviour was then validated using a MoCap-based kinematic analysis to verify non-restrictive movement, reliable transition to blocking, and compliance of the blocking.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "4 pages, 4 figures, 10 references. Submitted to IEEE EMBC 2024 conference"
    },
    {
        "paper id": "2403.17554",
        "abstract url": "https://arxiv.org/abs/2403.17554",
        "title": "Robust Stability for Multiagent Systems with Spatio-Temporally Correlated Packet Loss",
        "rating": -10,
        "keywords": [],
        "abstract": "A problem with considering correlations in the analysis of multiagent system with stochastic packet loss is that they induce dependencies between agents that are otherwise decoupled, preventing the application of decomposition methods required for efficient evaluation. To circumvent that issue, this paper is proposing an approach based on analysing sets of networks with independent communication links, only considering the correlations in an implicit fashion. Combining ideas from the robust stabilization of Markov jump linear systems with recently proposed techniques for analysing packet loss in multiagent systems, we obtain a linear matrix inequality based stability condition which is independent of the number of agents. The main result is that the set of stabilized probability distributions has non-empty interior such that small correlations cannot lead to instability, even though only distributions of independent links were analysed. Moreover, two examples are provided to demonstrate the applicability of the results to practically relevant scenarios.",
        "subjects": [
            "math.OC"
        ],
        "comment": "7 pages, 2 figures, 1 table"
    },
    {
        "paper id": "2403.17574",
        "abstract url": "https://arxiv.org/abs/2403.17574",
        "title": "SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless Functions",
        "rating": -10,
        "keywords": [],
        "abstract": "As an emerging cloud computing deployment paradigm, serverless computing is gaining traction due to its efficiency and ability to harness on-demand cloud resources. However, a significant hurdle remains in the form of the cold start problem, causing latency when launching new function instances from scratch. Existing solutions tend to use over-simplistic strategies for function pre-loading/unloading without full invocation pattern exploitation, rendering unsatisfactory optimization of the trade-off between cold start latency and resource waste. To bridge this gap, we propose SPES, the first differentiated scheduler for runtime cold start mitigation by optimizing serverless function provision. Our insight is that the common architecture of serverless systems prompts the concentration of certain invocation patterns, leading to predictable invocation behaviors. This allows us to categorize functions and pre-load/unload proper function instances with finer-grained strategies based on accurate invocation prediction. Experiments demonstrate the success of SPES in optimizing serverless function provision on both sides: reducing the 75th-percentile cold start rates by 49.77% and the wasted memory time by 56.43%, compared to the state-of-the-art. By mitigating the cold start issue, SPES is a promising advancement in facilitating cloud services deployed on serverless architectures.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "12 pages, accepted by ICDE 2024 (40th IEEE International Conference on Data Engineering)"
    },
    {
        "paper id": "2403.17577",
        "abstract url": "https://arxiv.org/abs/2403.17577",
        "title": "Channel-Adaptive Pilot Design for FDD-MIMO Systems Utilizing Gaussian Mixture Models",
        "rating": -10,
        "keywords": [],
        "abstract": "In this work, we propose to utilize Gaussian mixture models (GMMs) to design pilots for downlink (DL) channel estimation in frequency division duplex (FDD) systems. The GMM captures prior information during training that is leveraged to design a codebook of pilot matrices in an initial offline phase. Once shared with the mobile terminal (MT), the GMM is utilized to determine a feedback index at the MT in the online phase. This index selects a pilot matrix from a codebook, eliminating the need for online pilot optimization. The GMM is further used for DL channel estimation at the MT via observation-dependent linear minimum mean square error (LMMSE) filters, parametrized by the GMM. The analytic representation of the GMM allows adaptation to any signal-to-noise ratio (SNR) level and pilot configuration without re-training. With extensive simulations, we demonstrate the superior performance of the proposed GMM-based pilot scheme compared to state-of-the-art approaches.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17587",
        "abstract url": "https://arxiv.org/abs/2403.17587",
        "title": "Parameterized Analysis of Bribery in Challenge the Champ Tournaments",
        "rating": -10,
        "keywords": [],
        "abstract": "Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players. We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of players. On the algorithmic side, we show that the problem is fixed-parameter tractable when parameterized either by the number of different bribe values or the number of different probability values. To this end, we establish several results that are of independent interest. In particular, we show that the product knapsack problem is W[1]-hard when parameterized by the number of items in the knapsack, and that constructive bribery for cup tournaments is W[1]-hard when parameterized by the number of players. Furthermore, we present a novel way of designing mixed integer linear programs, ensuring optimal solutions where all variables are integers.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17597",
        "abstract url": "https://arxiv.org/abs/2403.17597",
        "title": "An Exact Solution for Allocating Car Parking Spaces on Campus",
        "rating": -10,
        "keywords": [],
        "abstract": "All over the world, especially in the university environment, planning managers and traffic engineers are constantly faced with the problem of inadequate allocation of car parking spaces to demanded users. Users could either prefer reserved parking spaces to unreserved parking spaces or vice versa. This makes the campus parking manager to be faced with two basic problem which are: the problem of allocating the actual number of available reserved spaces to users without any conflict over the same parking space, and the problem of determining the number of parking permit to be issued for parking lot with unreserved spaces. Hence, an optimal or available solution to the problem is required. This paper investigates a model for allocating car parking spaces, adds a constraint to address the reserved parking policy in a university environment and solves the parking allocation problem using an exact solution method. The result obtained gives the value of the objective function and the optimal allocation of users to each parking lot.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "An International Multidiscinary Conference on Research, Development and Practices in Science, Technology, Education, Arts, Management & the Social Science (iSTEAMS). Conference Centre, University of Ibandan, Nigeria. 30 May - 01 June 2013"
    },
    {
        "paper id": "2403.17598",
        "abstract url": "https://arxiv.org/abs/2403.17598",
        "title": "Ultrafast Adaptive Primary Frequency Tuning and Secondary Frequency Identification for S/S WPT system",
        "rating": -10,
        "keywords": [],
        "abstract": "Magnetic resonance wireless power transfer (WPT) technology is increasingly being adopted across diverse applications. However, its effectiveness can be significantly compromised by parameter shifts within the resonance network, owing to its high system quality factor. Such shifts are inherent and challenging to mitigate during the manufacturing process. In response, this article introduces a rapid frequency tuning approach. Leveraging switch-controlled capacitors (SCC) to adjust the resonance network and the primary side's operating frequency, alongside a current zero-crossing detection (ZCD) circuit for voltage-current phase determination, this method circumvents the need for intricate knowledge of WPT system parameters. Moreover, it obviates the necessity for inter-side communication for real-time identification of the secondary side resonance frequency. The swift response of SCC and two-step perturb-and-observe algorithm mitigate output disturbances, thereby expediting the frequency tuning process. Experimental validation on a 200W Series-Series compensated WPT (SS-WPT) system demonstrates that the proposed method achieves frequency recognition accuracy within 0.7kHz in less than 1ms, increasing system efficiency up to 9%.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "11 pages,16 figures,to be published in IEEE Transactions on Industrial Electronics"
    },
    {
        "paper id": "2403.17599",
        "abstract url": "https://arxiv.org/abs/2403.17599",
        "title": "Coimagining the Future of Voice Assistants with Cultural Sensitivity",
        "rating": -10,
        "keywords": [],
        "abstract": "Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the user experience (UX) is often limited, leading to underuse, disengagement, and abandonment. Co-designing interactions for VAs with potential end-users can be useful. Crowdsourcing this process online and anonymously may add value. However, most work has been done in the English-speaking West on dialogue data sets. We must be sensitive to cultural differences in language, social interactions, and attitudes towards technology. Our aims were to explore the value of co-designing VAs in the non-Western context of Japan and demonstrate the necessity of cultural sensitivity. We conducted an online elicitation study (N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined dialogues (N = 282) and activities (N = 73) with future VAs. We discuss the implications for coimagining interactions with future VAs, offer design guidelines for the Japanese and English-speaking US contexts, and suggest opportunities for cultural plurality in VA design and scholarship.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "21 pages"
    },
    {
        "paper id": "2403.17618",
        "abstract url": "https://arxiv.org/abs/2403.17618",
        "title": "Technical Report: Incorporating Blogs in Pollux",
        "rating": -10,
        "keywords": [],
        "abstract": "This technical report describes the incorporation of political blogs into Pollux, the Specialised Information Service (FID) for Political Science in Germany. Considering the widespread use of political blogs in political science research, we decided to include them in the Pollux search system to enhance the available information infrastructure. We describe the crawling and analyzing of the blogs and the pipeline that integrates them into the Pollux system. To demonstrate the content of the incorporated blogs, we also provide a visualization of the topics covered by the blog posts during the first three months following integration.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "6 pages, 6 figures, technical report"
    },
    {
        "paper id": "2403.17665",
        "abstract url": "https://arxiv.org/abs/2403.17665",
        "title": "When View- and Conflict-Robustness Coincide for Multiversion Concurrency Control",
        "rating": -10,
        "keywords": [],
        "abstract": "A DBMS allows trading consistency for efficiency through the allocation of isolation levels that are strictly weaker than serializability. The robustness problem asks whether, for a given set of transactions and a given allocation of isolation levels, every possible interleaved execution of those transactions that is allowed under the provided allocation, is always safe. In the literature, safe is interpreted as conflict-serializable (to which we refer here as conflict-robustness). In this paper, we study the view-robustness problem, interpreting safe as view-serializable. View-serializability is a more permissive notion that allows for a greater number of schedules to be serializable and aligns more closely with the intuitive understanding of what it means for a database to be consistent. However, view-serializability is more complex to analyze (e.g., conflict-serializability can be decided in polynomial time whereas deciding view-serializability is NP-complete). While conflict-robustness implies view-robustness, the converse does not hold in general. In this paper, we provide a sufficient condition for isolation levels guaranteeing that conflict- and view-robustness coincide and show that this condition is satisfied by the isolation levels occurring in Postgres and Oracle: read committed (RC), snapshot isolation (SI) and serializable snapshot isolation (SSI). It hence follows that for these systems, widening from conflict- to view-serializability does not allow for more sets of transactions to become robust. Interestingly, the complexity of deciding serializability within these isolation levels is still quite different. Indeed, deciding conflict-serializability for schedules allowed under RC and SI remains in polynomial time, while we show that deciding view-serializability within these isolation levels remains NP-complete.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2403.17672",
        "abstract url": "https://arxiv.org/abs/2403.17672",
        "title": "Predicting Perceived Gloss: Do Weak Labels Suffice?",
        "rating": -10,
        "keywords": [],
        "abstract": "Estimating perceptual attributes of materials directly from images is a challenging task due to their complex, not fully-understood interactions with external factors, such as geometry and lighting. Supervised deep learning models have recently been shown to outperform traditional approaches, but rely on large datasets of human-annotated images for accurate perception predictions. Obtaining reliable annotations is a costly endeavor, aggravated by the limited ability of these models to generalise to different aspects of appearance. In this work, we show how a much smaller set of human annotations (\"strong labels\") can be effectively augmented with automatically derived \"weak labels\" in the context of learning a low-dimensional image-computable gloss metric. We evaluate three alternative weak labels for predicting human gloss perception from limited annotated data. Incorporating weak labels enhances our gloss prediction beyond the current state of the art. Moreover, it enables a substantial reduction in human annotation costs without sacrificing accuracy, whether working with rendered images or real photographs.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "Computer Graphics Forum (Eurographics 2024)"
    },
    {
        "paper id": "2403.17674",
        "abstract url": "https://arxiv.org/abs/2403.17674",
        "title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games",
        "rating": -10,
        "keywords": [],
        "abstract": "Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, achieving the complementary performance in the cybersecurity games.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2403.17675",
        "abstract url": "https://arxiv.org/abs/2403.17675",
        "title": "Chattering Phenomena in Time-Optimal Control for High-Order Chain-of-Integrators Systems with Full State Constraints",
        "rating": -10,
        "keywords": [],
        "abstract": "Time-optimal control for high-order chain-of-integrators systems with full state constraints remains an open and challenging problem in the optimal control theory domain. The behaviors of optimal control in high-order problems lack precision characterization, even where the existence of the chattering phenomenon remains unknown and overlooked. This paper establishes a theoretical framework for chattering phenomena in the considered problem, providing novel findings on the uniqueness of state constraints inducing chattering, the upper bound on switching times in an unconstrained arc during chattering, and the convergence of states and costates to the chattering limit point. For the first time, this paper proves the existence of the chattering phenomenon in the considered problem. The chattering optimal control for 4th order problems with velocity constraints is precisely solved, providing an approach to plan strictly time-optimal snap-limited trajectories. Other cases of order $n\\leq4$ are proved not to allow chattering. The conclusions correct the longstanding misconception in the industry regarding the time-optimality of S-shaped trajectories with minimal switching times.",
        "subjects": [
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17676",
        "abstract url": "https://arxiv.org/abs/2403.17676",
        "title": "Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices",
        "rating": -10,
        "keywords": [],
        "abstract": "Reservoir computing is a recurrent neural network that has been applied across various domains in machine learning. The implementation of reservoir computing, however, often demands heavy computations for activating the reservoir. Configuring physical reservoir networks and harnessing the nonlinearity from the underlying devices for activation is an emergent solution to address the computational challenge. Herein, we analyze the feasibility of employing the nonlinearity from solution-processed molybdenum disulfide (MoS2) devices for reservoir activation. The devices, fabricated using liquid-phase exfoliated MoS2, exhibit a high-order nonlinearity achieved by Stark modulation of the MoS2 material. We demonstrate that this nonlinearity can be fitted and employed as the activation function to facilitate reservoir computing implementation. Notably, owing to the high-order nonlinearity, the network exhibits long-term synchronization and robust generalization abilities for approximating complex dynamical systems. Given the remarkable reservoir activation capability, coupled with the scalability of the device fabrication, our findings open the possibility for the physical realization of lightweight, efficient reservoir computing for, for instance, signal classification, motion tracking, and pattern recognition of complex time series as well as secure cryptography. As an example, we show the network can be appointed to generate chaotic random numbers for secure data encryption.",
        "subjects": [
            "physics.app-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17704",
        "abstract url": "https://arxiv.org/abs/2403.17704",
        "title": "Prioritize Team Actions: Multi-Agent Temporal Logic Task Planning with Ordering Constraints",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we investigate the problem of linear temporal logic (LTL) path planning for multi-agent systems, introducing the new concept of \\emph{ordering constraints}. Specifically, we consider a generic objective function that is defined for the path of each individual agent. The primary objective is to find a global plan for the team of agents, ensuring they collectively meet the specified LTL requirements. Simultaneously, we aim to maintain a pre-determined order in the values of the objective function for each agent, which we refer to as the ordering constraints. This new requirement stems from scenarios like security-aware planning, where relative orders outweigh absolute values in importance. We present an efficient algorithm to solve this problem, supported by proofs of correctness that demonstrate the optimality of our solution. Additionally, we provide a case study in security-aware path planning to illustrate the practicality and effectiveness of our proposed approach.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This article is withdrawn due to errors in the methodology section, specifically concerning the insufficient explanation of the data collection process. Upon review, it's clear that the data sampling methods were not adequately described, potentially leading to misinterpretations of the results"
    },
    {
        "paper id": "2403.17713",
        "abstract url": "https://arxiv.org/abs/2403.17713",
        "title": "Distance-Based Hierarchical Cutting of Complex Networks with Non-Preferential and Preferential Choice of Seeds",
        "rating": -10,
        "keywords": [],
        "abstract": "Graphs and complex networks can be successively separated into connected components associated to respective seed nodes, therefore establishing a respective hierarchical organization. In the present work, we study the properties of the hierarchical structure implied by distance-based cutting of Erd\u0151s-R\u00e9nyi, Barab\u00e1si-Albert, and a specific geometric network. Two main situations are considered regarding the choice of the seeds: non-preferential and preferential to the respective node degree. Among the obtained findings, we have the tendency of geometrical networks yielding more balanced pairs of connected components along the network progressive separation, presenting little chaining effects, followed by the Erd\u0151s-R\u00e9nyi and Barab\u00e1si-Albert types of networks. The choice of seeds preferential to the node degree tended to enhance the balance of the connected components in the case of the geometrical networks.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": "15 pages and 9 figures"
    },
    {
        "paper id": "2403.17714",
        "abstract url": "https://arxiv.org/abs/2403.17714",
        "title": "Evaluating Authoring Tools with the Explorable Authoring Requirements",
        "rating": -10,
        "keywords": [],
        "abstract": "Explorables with interactive, multimodal content, openly available on the web, are a promising medium for education. Yet authoring such explorables requires web development expertise, excluding most educators and students from the authoring and remixing process. Some tools are available to reduce this barrier of entry and others are in development, making a method to evaluate these new tools necessary. On the basis of the software quality model ISO 25010, empirical results, and domain modeling, we derive the Explorable Authoring Requirements (EAR) as a requirements catalogue explorable authoring tools should implement. We then outline a future research design to operationalize EAR.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "12 pages plus references, preprint of paper for NaKoDi2022 (adjunct to WiPSCE 2022 conference)"
    },
    {
        "paper id": "2403.17729",
        "abstract url": "https://arxiv.org/abs/2403.17729",
        "title": "EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention",
        "rating": -10,
        "keywords": [],
        "abstract": "To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.Furthermore, a phase contrastive learning task is proposed to improve the isotropy of contextual representations in EulerFormer. Our theoretical framework possesses a high degree of completeness and generality. It is more robust to semantic variations and possesses moresuperior theoretical properties in principle. Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted for publication in SIGIR'24"
    },
    {
        "paper id": "2403.17730",
        "abstract url": "https://arxiv.org/abs/2403.17730",
        "title": "On Structural Non-commutativity in Affine Feedback of SISO Nonlinear Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "The affine feedback connection of SISO nonlinear systems modeled by Chen--Fliess series is shown to be a group action on the plant which is isomorphic to the semi-direct product of shuffle and additive group of non-commutative formal power series. The additive and multiplicative feedback loops in an affine feedback connection are thus proven to be structurally non-commutative. A flip in the order of these loops results in a net additive feedback loop.",
        "subjects": [
            "math.OC"
        ],
        "comment": "submitted to $26^{th}$ International Symposium on Mathematical Theory of Networks and Systems, 2024"
    },
    {
        "paper id": "2403.17754",
        "abstract url": "https://arxiv.org/abs/2403.17754",
        "title": "Optimal Euclidean Tree Covers",
        "rating": -10,
        "keywords": [],
        "abstract": "A $(1+\\varepsilon)\\textit{-stretch tree cover}$ of a metric space is a collection of trees, where every pair of points has a $(1+\\varepsilon)$-stretch path in one of the trees. The celebrated $\\textit{Dumbbell Theorem}$ [Arya et~al. STOC'95] states that any set of $n$ points in $d$-dimensional Euclidean space admits a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d} \\cdot \\log(1/\\varepsilon))$ trees, where the $O_d$ notation suppresses terms that depend solely on the dimension~$d$. The running time of their construction is $O_d(n \\log n \\cdot \\frac{\\log(1/\\varepsilon)}{\\varepsilon^{d}} + n \\cdot \\varepsilon^{-2d})$. Since the same point may occur in multiple levels of the tree, the $\\textit{maximum degree}$ of a point in the tree cover may be as large as $\u03a9(\\log \u03a6)$, where $\u03a6$ is the aspect ratio of the input point set. In this work we present a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d+1} \\cdot \\log(1/\\varepsilon))$ trees, which is optimal (up to the $\\log(1/\\varepsilon)$ factor). Moreover, the maximum degree of points in any tree is an $\\textit{absolute constant}$ for any $d$. As a direct corollary, we obtain an optimal {routing scheme} in low-dimensional Euclidean spaces. We also present a $(1+\\varepsilon)$-stretch $\\textit{Steiner}$ tree cover (that may use Steiner points) with $O_d(\\varepsilon^{(-d+1)/{2}} \\cdot \\log(1/\\varepsilon))$ trees, which too is optimal. The running time of our two constructions is linear in the number of edges in the respective tree covers, ignoring an additive $O_d(n \\log n)$ term; this improves over the running time underlying the Dumbbell Theorem.",
        "subjects": [
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17759",
        "abstract url": "https://arxiv.org/abs/2403.17759",
        "title": "TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we present TWOLAR: a two-stage pipeline for passage reranking based on the distillation of knowledge from Large Language Models (LLM). TWOLAR introduces a new scoring strategy and a distillation process consisting in the creation of a novel and diverse training dataset. The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then reranked by exploiting the zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate the contribution of each new component we introduced. Our experimental results show that TWOLAR significantly enhances the document reranking ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate future work we release our data set, finetuned models, and code.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17767",
        "abstract url": "https://arxiv.org/abs/2403.17767",
        "title": "Asymptotic Bayes risk of semi-supervised learning with uncertain labeling",
        "rating": -10,
        "keywords": [],
        "abstract": "This article considers a semi-supervised classification setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels. Our main aim is to compute the Bayes risk for this model. We compare the behavior of the Bayes risk and the best known algorithm for this model. This comparison eventually gives new insights over the algorithm.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17784",
        "abstract url": "https://arxiv.org/abs/2403.17784",
        "title": "SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings",
        "rating": -10,
        "keywords": [],
        "abstract": "Crafting effective captions for figures is important. Readers heavily depend on these captions to grasp the figure's message. However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing. This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition. SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference. Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them. A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing. Participants' feedback further offers valuable design insights for future systems aiming to enhance caption writing.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems"
    },
    {
        "paper id": "2403.17785",
        "abstract url": "https://arxiv.org/abs/2403.17785",
        "title": "Neural Distributed Controllers with Port-Hamiltonian Structures",
        "rating": -10,
        "keywords": [],
        "abstract": "Controlling large-scale cyber-physical systems necessitates optimal distributed policies, relying solely on local real-time data and limited communication with neighboring agents. However, finding optimal controllers remains challenging, even in seemingly simple scenarios. Parameterizing these policies using Neural Networks (NNs) can deliver good performance, but their sensitivity to small input changes can destabilize the closed-loop system. This paper addresses this issue for a network of nonlinear dissipative systems. Specifically, we leverage well-established port-Hamiltonian structures to characterize deep distributed control policies with closed-loop stability guarantees and a finite $\\mathcal{L}_2$ gain, regardless of specific NN parameters. This eliminates the need to constrain the parameters during optimization and enables training with standard methods like stochastic gradient descent. A numerical study on the consensus control of Kuramoto oscillators demonstrates the effectiveness of the proposed controllers.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This paper is submitted in CDC2024 for a possible publication"
    },
    {
        "paper id": "2403.17786",
        "abstract url": "https://arxiv.org/abs/2403.17786",
        "title": "Query Refinement for Diverse Top-$k$ Selection",
        "rating": -10,
        "keywords": [],
        "abstract": "Database queries are often used to select and rank items as decision support for many applications. As automated decision-making tools become more prevalent, there is a growing recognition of the need to diversify their outcomes. In this paper, we define and study the problem of modifying the selection conditions of an ORDER BY query so that the result of the modified query closely fits some user-defined notion of diversity while simultaneously maintaining the intent of the original query. We show the hardness of this problem and propose a Mixed Integer Linear Programming (MILP) based solution. We further present optimizations designed to enhance the scalability and applicability of the solution in real-life scenarios. We investigate the performance characteristics of our algorithm and show its efficiency and the usefulness of our optimizations.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "v2 corrects author order"
    },
    {
        "paper id": "2403.17818",
        "abstract url": "https://arxiv.org/abs/2403.17818",
        "title": "CSSTs: A Dynamic Data Structure for Partial Orders in Concurrent Execution Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "Dynamic analyses are a standard approach to analyzing and testing concurrent programs. Such techniques observe program traces and analyze them to infer the presence or absence of bugs. At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\u03c3$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$. The standard data structure for this task has thus far been vector clocks. These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\u03c3$, while they cannot handle the deletion of existing orderings. In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time. For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting. Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17819",
        "abstract url": "https://arxiv.org/abs/2403.17819",
        "title": "Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)",
        "rating": -10,
        "keywords": [],
        "abstract": "Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks. In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17863",
        "abstract url": "https://arxiv.org/abs/2403.17863",
        "title": "An AI-Native Runtime for Multi-Wearable Environments",
        "rating": -10,
        "keywords": [],
        "abstract": "The miniaturization of AI accelerators is paving the way for next-generation wearable applications within wearable technologies. We introduce Mojito, an AI-native runtime with advanced MLOps designed to facilitate the development and deployment of these applications on wearable devices. It emphasizes the necessity of dynamic orchestration of distributed resources equipped with ultra-low-power AI accelerators to overcome challenges associated with unpredictable runtime environments. Through its innovative approaches, Mojito demonstrates how future wearable technologies can evolve to be more autonomous.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "7 pages, 4 figures"
    },
    {
        "paper id": "2403.17885",
        "abstract url": "https://arxiv.org/abs/2403.17885",
        "title": "Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and Transaction Time",
        "rating": -10,
        "keywords": [],
        "abstract": "The Ethereum Improvement Proposal 3675 (EIP-3675) marks a significant shift, transitioning from a Proof of Work (PoW) to a Proof of Stake (PoS) consensus mechanism. This transition resulted in a staggering 99.95% decrease in energy consumption. However, the transition prompts two critical questions: (1). How does EIP-3675 affect miners' dynamics? and (2). How do users determine priority fees, considering that paying too little may cause delays or non-inclusion, yet paying too much wastes money with little to no benefits? To address the first question, we present a comprehensive empirical study examining EIP-3675's effect on miner dynamics (i.e., miner participation, distribution, and the degree of randomness in miner selection). Our findings reveal that the transition has encouraged broader participation of miners in block append operation, resulting in a larger pool of unique miners ($\\approx50\\times$ PoW), and the change in miner distribution with the increased number of unique small category miners ($\\approx60\\times$ PoW). However, there is an unintended consequence: a reduction in the miner selection randomness, which signifies the negative impact of the transition to PoS-Ethereum on network decentralization. Regarding the second question, we employed regression-based machine learning models; the Gradient Boosting Regressor performed best in predicting priority fees, while the K-Neighbours Regressor was worst.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "This paper has been accepted as a full paper at the IEEE International Conference on Blockchain and Cryptocurrency (ICBC) 2024"
    },
    {
        "paper id": "2403.17892",
        "abstract url": "https://arxiv.org/abs/2403.17892",
        "title": "Density of group languages in shift spaces",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the density of group languages (i.e. rational languages recognized by morphisms onto finite groups) inside shift spaces. The density of a rational language can be understood as the frequency of some \"pattern\" in the shift space, for example a pattern like \"words with an even number of a given letter.\" In this paper, we handle density of group languages via ergodicity of skew products between the shift space and the recognizing group. We consider both the cases of shifts of finite type (with a suitable notion of irreducibility), and of minimal shifts. In the latter case, our main result is a closed formula for the density which holds whenever the skew product has minimal closed invariant subsets which are ergodic under the product of the original measure and the uniform probability measure on the group. The formula is derived in part from a characterization of minimal closed invariant subsets for skew products relying on notions of cocycles and coboundaries. In the case where the whole skew product itself is ergodic under the product measure, then the density is completely determined by the cardinality of the image of the language inside the recognizing group. We provide sufficient conditions for the skew product to have minimal closed invariant subsets that are ergodic under the product measure. Finally, we investigate the link between minimal closed invariant subsets, return words and bifix codes.",
        "subjects": [
            "math.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17901",
        "abstract url": "https://arxiv.org/abs/2403.17901",
        "title": "Search and Society: Reimagining Information Access for Radical Futures",
        "rating": -10,
        "keywords": [],
        "abstract": "Information retrieval (IR) technologies and research are undergoing transformative changes. It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on fairness, accountability, transparency, and ethics in IR and the rest of IR research. Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly stated sociotechnical imaginaries. The sociotechnical imaginaries that underpin the design and development of information access technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives. Our guiding future imaginaries must be informed by other academic fields, such as democratic theory and critical theory, and should be co-developed with social science scholars, legal scholars, civil rights and social justice activists, and artists, among others. In this perspective paper, we motivate why the community must consider this radical shift in how we do research and what we work on, and sketch a path forward towards this transformation.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17907",
        "abstract url": "https://arxiv.org/abs/2403.17907",
        "title": "Multi-Agent Resilient Consensus under Intermittent Faulty and Malicious Transmissions (Extended Version)",
        "rating": -10,
        "keywords": [],
        "abstract": "In this work, we consider the consensus problem in which legitimate agents share their values over an undirected communication network in the presence of malicious or faulty agents. Different from the previous works, we characterize the conditions that generalize to several scenarios such as intermittent faulty or malicious transmissions, based on trust observations. As the standard trust aggregation approach based on a constant threshold fails to distinguish intermittent malicious/faulty activity, we propose a new detection algorithm utilizing time-varying thresholds and the random trust values available to legitimate agents. Under these conditions, legitimate agents almost surely determine their trusted neighborhood correctly with geometrically decaying misclassification probabilities. We further prove that the consensus process converges almost surely even in the presence of malicious agents. We also derive the probabilistic bounds on the deviation from the nominal consensus value that would have been achieved with no malicious agents in the system. Numerical results verify the convergence among agents and exemplify the deviation under different scenarios.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Extended Version of CDC '24 submission"
    },
    {
        "paper id": "2403.17908",
        "abstract url": "https://arxiv.org/abs/2403.17908",
        "title": "Reciprocity Calibration of Dual-Antenna Repeaters",
        "rating": -10,
        "keywords": [],
        "abstract": "We present a reciprocity calibration method for dual-antenna repeaters in wireless networks. The method uses bi-directional measurements between two network nodes, A and B, where for each bi-directional measurement, the repeaters are configured in different states. The nodes A and B could be two access points in a distributed MIMO system, or they could be a base station and a mobile user terminal, for example. From the calibration measurements, the differences between the repeaters' forward and reverse gains are estimated. The repeaters are then (re-)configured to compensate for these differences such that the repeaters appear, transparently to the network, as reciprocal components of the propagation environment, enabling reciprocity-based beamforming in the network.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "IEEE Wireless Communications Letters, 2024"
    },
    {
        "paper id": "2403.17913",
        "abstract url": "https://arxiv.org/abs/2403.17913",
        "title": "Enhancing Indoor and Outdoor THz Communications with Beyond Diagonal-IRS: Optimization and Performance Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "This work investigates the application of Beyond Diagonal Intelligent Reflective Surface (BD-IRS) to enhance THz downlink communication systems, operating in a hybrid: reflective and transmissive mode, to simultaneously provide services to indoor and outdoor users. We propose an optimization framework that jointly optimizes the beamforming vectors and phase shifts in the hybrid reflective/transmissive mode, aiming to maximize the system sum rate. To tackle the challenges in solving the joint design problem, we employ the conjugate gradient method and propose an iterative algorithm that successively optimizes the hybrid beamforming vectors and the phase shifts. Through comprehensive numerical simulations, our findings demonstrate a significant improvement in rate when compared to existing benchmark schemes, including time- and frequency-divided approaches, by approximately $30.5\\%$ and $69.9\\%$ respectively and even outperforms the STAR-IRS system by $76.99\\%$. This underscores the significant influence of IRS elements on system performance relative to that of base station antennas, highlighting their pivotal role in advancing the communication system efficacy.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.17917",
        "abstract url": "https://arxiv.org/abs/2403.17917",
        "title": "Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper presents two algorithms for multi-agent dynamic coverage in spatiotemporal environments, where the coverage algorithms are informed by the method of data assimilation. In particular, we show that by considering the information assimilation algorithm, here a Numerical Gaussian Process Kalman Filter, the influence of measurements taken at one position on the uncertainty of the estimate at another location can be computed. We use this relationship to propose new coverage algorithms. Furthermore, we show that the controllers naturally extend to the multi-agent context, allowing for a distributed-control central-information paradigm for multi-agent coverage. Finally, we demonstrate the algorithms through a realistic simulation of a team of UAVs collecting wind data over a region in Austria.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 2 figures, submitted to CDC 2024"
    },
    {
        "paper id": "2403.17927",
        "abstract url": "https://arxiv.org/abs/2403.17927",
        "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
        "rating": -10,
        "keywords": [],
        "abstract": "In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "work in progress"
    },
    {
        "paper id": "2403.18041",
        "abstract url": "https://arxiv.org/abs/2403.18041",
        "title": "Learning Piecewise Residuals of Control Barrier Functions for Safety of Switching Systems using Multi-Output Gaussian Processes",
        "rating": -10,
        "keywords": [],
        "abstract": "Control barrier functions (CBFs) have recently been introduced as a systematic tool to ensure safety by establishing set invariance. When combined with a control Lyapunov function (CLF), they form a safety-critical control mechanism. However, the effectiveness of CBFs and CLFs is closely tied to the system model. In practice, model uncertainty can jeopardize safety and stability guarantees and may lead to undesirable performance. In this paper, we develop a safe learning-based control strategy for switching systems in the face of uncertainty. We focus on the case that a nominal model is available for a true underlying switching system. This uncertainty results in piecewise residuals for each switching surface, impacting the CLF and CBF constraints. We introduce a batch multi-output Gaussian process (MOGP) framework to approximate these piecewise residuals, thereby mitigating the adverse effects of uncertainty. A particular structure of the covariance function enables us to convert the MOGP-based chance constraints CLF and CBF into second-order cone constraints, which leads to a convex optimization. We analyze the feasibility of the resulting optimization and provide the necessary and sufficient conditions for feasibility. The effectiveness of the proposed strategy is validated through a simulation of a switching adaptive cruise control system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2403.09573"
    },
    {
        "paper id": "2403.18042",
        "abstract url": "https://arxiv.org/abs/2403.18042",
        "title": "Extending Network Calculus To Deal With Partially Negative And Decreasing Service Curves",
        "rating": -10,
        "keywords": [],
        "abstract": "Network Calculus (NC) is a versatile analytical methodology to efficiently compute performance bounds in networked systems. The arrival and service curve abstractions allow to model diverse and heterogeneous distributed systems. The operations to compute residual service curves and to concatenate sequences of systems enable an efficient and accurate calculation of per-flow timing guarantees. Yet, in some scenarios involving multiple concurrent flows at a system, the central notion of so-called min-plus service curves is too weak to still be able to compute a meaningful residual service curve. In these cases, one usually resorts to so-called strict service curves that enable the computation of per-flow bounds. However, strict service curves are restrictive: (1) there are service elements for which only min-plus service curves can be provided but not strict ones and (2) strict service curves generally have no concatenation property, i.e., a sequence of two strict systems does not yield a strict service curve. In this report, we extend NC to deal with systems only offering aggregate min-plus service curves to multiple flows. The key to this extension is the exploitation of minimal arrival curves, i.e., lower bounds on the arrival process. Technically speaking, we provide basic performance bounds (backlog and delay) for the case of negative service curves. We also discuss their accuracy and show them to be tight. In order to illustrate their usefulness we also present patterns of application of these new results for: (1) heterogeneous systems involving computation and communication resources and (2) finite buffers that are shared between multiple flows.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "To be published in part in RTAS 2024"
    },
    {
        "paper id": "2403.18044",
        "abstract url": "https://arxiv.org/abs/2403.18044",
        "title": "Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design",
        "rating": -10,
        "keywords": [],
        "abstract": "Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.",
        "subjects": [
            "math.OC"
        ],
        "comment": "9 pages, 6 figures, 2 tables"
    },
    {
        "paper id": "2403.18052",
        "abstract url": "https://arxiv.org/abs/2403.18052",
        "title": "R2D2 image reconstruction with model uncertainty quantification in radio astronomy",
        "rating": -10,
        "keywords": [],
        "abstract": "The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models. Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration. The resulting multiple R2D2 instances can also be leveraged to generate ``R2D2 samples'', from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality. Focusing on RI imaging, and adopting a telescope-specific approach, multiple R2D2 instances were trained to encompass the most general observation setting of the Very Large Array (VLA). Simulations and real-data experiments confirm that: (i) R2D2's image estimation capability is superior to that of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction capability (arising from series with only few DNNs) makes the computation of multiple reconstruction samples and of uncertainty maps practical even at large image dimension; (iii) it is characterized by a very low model uncertainty.",
        "subjects": [
            "astro-ph.IM"
        ],
        "comment": "submitted to IEEE EUSIPCO 2024"
    },
    {
        "paper id": "2403.18055",
        "abstract url": "https://arxiv.org/abs/2403.18055",
        "title": "Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing",
        "rating": -10,
        "keywords": [],
        "abstract": "We study in this paper boundary stabilization, in the L2 sense, of the one-dimensional Kuramoto-Sivashinsky equation subject to intermittent sensing. We assume that we measure the state of this spatio-temporal equation on a given spatial subdomain during certain intervals of time, while we measure the state on the remaining spatial subdomain during the remaining intervals of time. As a result, we assign a feedback law at the boundary of the spatial domain and force to zero the value of the state at the junction of the two subdomains. Throughout the study, the destabilizing coefficient is assumed to be space-dependent and bounded but unknown. Adaptive boundary controllers are designed under different assumptions on the forcing term. In particular, when the forcing term is null, we guarantee global exponential stability of the origin. Furthermore, when the forcing term is bounded and admits a known upper bound, we guarantee input-to-state stability, and only global uniform ultimate boundedness is guaranteed when the upper bound is unknown. Numerical simulations are performed to illustrate our results",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted to Automatica"
    },
    {
        "paper id": "2403.18059",
        "abstract url": "https://arxiv.org/abs/2403.18059",
        "title": "Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability",
        "rating": -10,
        "keywords": [],
        "abstract": "We generalize the problem of online submodular welfare maximization to incorporate a variety of new elements arising from reusability, stochastic rewards, combinatorial actions and similar features that have received significant attention in recent years. For our general formulation, we show that a non-adaptive Greedy algorithm achieves the highest possible competitive ratio against an adaptive offline benchmark in the adversarial arrival model and in the unknown IID stochastic arrival model. In addition to generalizing several previous results, this shows that, in general, adaptivity to stochastic rewards (and similar features) offers no theoretical (worst-case) benefits.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18073",
        "abstract url": "https://arxiv.org/abs/2403.18073",
        "title": "Workflow Mini-Apps: Portable, Scalable, Tunable & Faithful Representations of Scientific Workflows",
        "rating": -10,
        "keywords": [],
        "abstract": "Workflows are critical for scientific discovery. However, the sophistication, heterogeneity, and scale of workflows make building, testing, and optimizing them increasingly challenging. Furthermore, their complexity and heterogeneity make performance reproducibility hard. In this paper, we propose workflow mini-apps as a tool to address the challenges in building and testing workflows while controlling the fidelity of representing realworld workflows. Workflow mini-apps are deployed and run on various HPC systems and architectures without workflow-specific constraints. We offer insight into their design and implementation, providing an analysis of their performance and reproducibility. Workflow mini-apps thus advance the science of workflows by providing simple, portable, and managed (fidelity) representations of otherwise complex and difficult-to-control real workflows.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18079",
        "abstract url": "https://arxiv.org/abs/2403.18079",
        "title": "Paths to Equilibrium in Normal-Form Games",
        "rating": -10,
        "keywords": [],
        "abstract": "In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for mixed extensions of finite normal-form games.%",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18087",
        "abstract url": "https://arxiv.org/abs/2403.18087",
        "title": "Channel Estimation and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces",
        "rating": -10,
        "keywords": [],
        "abstract": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a new advance and generalization of the RIS technique. BD-RIS breaks through the isolation between RIS elements by creatively introducing inter-element connections, thereby enabling smarter wave manipulation and enlarging coverage. However, exploring proper channel estimation schemes suitable for BD-RIS aided communication systems still remains an open problem. In this paper, we study channel estimation and beamforming design for BD-RIS aided multi-antenna systems. We first describe the channel estimation strategy based on the least square (LS) method, derive the mean square error (MSE) of the LS estimation, and formulate the joint pilot sequence and BD-RIS design problem with unique constraints induced by BD-RIS architectures. Specifically, we propose an efficient pilot sequence and BD-RIS design which theoretically guarantees to achieve the minimum MSE. With the estimated channel, we then consider two BD-RIS scenarios and propose beamforming design algorithms. Finally, we provide simulation results to verify the effectiveness of the proposed channel estimation scheme and beamforming design algorithms. We also show that more interelement connections in BD-RIS improves the performance while increasing the training overhead for channel estimation.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "12 pages, 10 figures, submitted to IEEE journal"
    },
    {
        "paper id": "2403.18106",
        "abstract url": "https://arxiv.org/abs/2403.18106",
        "title": "Authorized Subject Headings in the Online Automatic catalog Environment An Empirical Study on a Sample of Arabic Records",
        "rating": -10,
        "keywords": [],
        "abstract": "Subject headings are very important to machine catalogs, given the importance of thematic research. This study aims to measure the quality of a group of authorized subject headings with a sample of Arabic bibliographic records on the catalog of Egyptian university libraries by identifying the most important practices, policies, procedures followed, and tools used. In addition to assessing the actual capabilities of lists, thesaurus, and guidelines that were used in establishing thematic availability points. The study used both the descriptive analytical and evaluation approaches to achieve the study objectives.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "in Arabic language"
    },
    {
        "paper id": "2403.18119",
        "abstract url": "https://arxiv.org/abs/2403.18119",
        "title": "Multiple Model Reference Adaptive Control with Blending for Non-Square Multivariable Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper we develop a multiple model reference adaptive controller (MMRAC) with blending. The systems under consideration are non-square, i.e., the number of inputs is not equal to the number of states; multi-input, linear, time-invariant with uncertain parameters that lie inside of a known, compact, and convex set. Moreover, the full state of the plant is available for feedback. A multiple model online identification scheme for the plant's state and input matrices is developed that guarantees the estimated parameters converge to the underlying plant model under the assumption of persistence of excitation. Using an exact matching condition, the parameter estimates are used in a control law such that the plant's states asymptotically track the reference signal generated by a state-space model reference. The control architecture is proven to provide boundedness of all closed-loop signals and to asymptotically drive the state tracking error to zero. Numerical simulations illustrate the stability and efficacy of the proposed MMRAC scheme.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "10 pages, 7 figures, IEEE Journal Submission"
    },
    {
        "paper id": "2403.18129",
        "abstract url": "https://arxiv.org/abs/2403.18129",
        "title": "On the Statistical Analysis of the Multipath Propagation Model Parameters for Power Line Communications",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes a fitting procedure that aims to identify the statistical properties of the parameters that describe the most widely known multipath propagation model (MPM) used in power line communication (PLC). Firstly, the MPM parameters are computed by fitting the theoretical model to a large database of single-input-single-output (SISO) experimental measurements, carried out in typical home premises. Secondly, the determined parameters are substituted back into the MPM formulation with the aim to prove their faithfulness, thus validating the proposed computation procedure. Then, the MPM parameters properties have been evaluated. In particular, the statistical behavior is established identifying the best fitting distribution by comparing the most common distributions through the use of the likelihood function. Moreover, the relationship among the different paths is highlighted in terms of statistical correlation. The identified statistical behavior for the MPM parameters confirms the assumptions of the previous works that, however, were mostly established in an heuristic way.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "9 pages, 7 figures"
    },
    {
        "paper id": "2403.18146",
        "abstract url": "https://arxiv.org/abs/2403.18146",
        "title": "Adaptive TTD Configurations for Near-Field Communications: An Unsupervised Transformer Approach",
        "rating": -10,
        "keywords": [],
        "abstract": "True-time delayers (TTDs) are popular analog devices for facilitating near-field wideband beamforming subject to the spatial-wideband effect. In this paper, an adaptive TTD configuration is proposed for short-range TTDs. Compared to the existing TTD configurations, the proposed one can effectively combat the spatial-widebandd effect for arbitrary user locations and array shapes with the aid of a switch network. A novel end-to-end deep neural network is proposed to optimize the hybrid beamforming with adaptive TTDs for maximizing spectral efficiency. 1) First, based on the U-Net architecture, a near-field channel learning module (NFC-LM) is proposed for adaptive beamformer design through extracting the latent channel response features of various users across different frequencies. In the NFC-LM, an improved cross attention (CA) is introduced to further optimize beamformer design by enhancing the latent feature connection between near-field channel and different beamformers. 2) Second, a switch multi-user transformer (S-MT) is proposed to adaptively control the connection between TTDs and phase shifters (PSs). In the S-MT, an improved multi-head attention, namely multi-user attention (MSA), is introduced to optimize the switch network through exploring the latent channel relations among various users. 3) Third, a multi feature cross attention (MCA) is introduced to simultaneously optimize the NFC-LM and S-MT by enhancing the latent feature correlation between beamformers and switch network. Numerical simulation results show that 1) the proposed adaptive TTD configuration effectively eliminates the spatial-wideband effect under uniform linear array (ULA) and uniform circular array (UCA) architectures, and 2) the proposed deep neural network can provide near optimal spectral efficiency, and solve the multi-user bemformer design and dynamical connection problem in real-time.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18149",
        "abstract url": "https://arxiv.org/abs/2403.18149",
        "title": "Code Generation for Conic Model-Predictive Control on Microcontrollers with TinyMPC",
        "rating": -10,
        "keywords": [],
        "abstract": "Conic constraints appear in many important control applications like legged locomotion, robotic manipulation, and autonomous rocket landing. However, current solvers for conic optimization problems have relatively heavy computational demands in terms of both floating-point operations and memory footprint, making them impractical for use on small embedded devices. We extend TinyMPC, an open-source, high-speed solver targeting low-power embedded control applications, to handle second-order cone constraints. We also present code-generation software to enable deployment of TinyMPC on a variety of microcontrollers. We benchmark our generated code against state-of-the-art embedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed increase over ECOS while consuming less memory. Finally, we demonstrate TinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained quadrotor with fast dynamics. TinyMPC and its code-generation tools are publicly available at https://tinympc.org.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to CDC, 2024. First two authors contributed equally"
    },
    {
        "paper id": "2403.18160",
        "abstract url": "https://arxiv.org/abs/2403.18160",
        "title": "Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure",
        "rating": -10,
        "keywords": [],
        "abstract": "Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily stemming from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a GPT-driven chatbot system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players' post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like 'Openness' and 'Agreeableness', and a preference for democratic values.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "22 pages, 6 figures, Accepted by CHI Conference on Human Factors in Computing Systems 2024"
    },
    {
        "paper id": "2403.18164",
        "abstract url": "https://arxiv.org/abs/2403.18164",
        "title": "Incentive Designs for Learning Agents to Stabilize Coupled Exogenous Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider a large population of learning agents noncooperatively selecting strategies from a common set, influencing the dynamics of an exogenous system (ES) we seek to stabilize at a desired equilibrium. Our approach is to design a dynamic payoff mechanism capable of shaping the population's strategy profile, thus affecting the ES's state, by offering incentives for specific strategies within budget limits. Employing system-theoretic passivity concepts, we establish conditions under which a payoff mechanism can be systematically constructed to ensure the global asymptotic stabilization of the ES's equilibrium. In comparison to previous approaches originally studied in the context of the so-called epidemic population games, the method proposed here allows for more realistic epidemic models and other types of ES, such as predator-prey dynamics. Stabilization is established with the support of a Lyapunov function, which provides useful bounds on the transients.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 3 figures"
    },
    {
        "paper id": "2403.18174",
        "abstract url": "https://arxiv.org/abs/2403.18174",
        "title": "Local (coarse) correlated equilibria in non-concave games",
        "rating": -10,
        "keywords": [],
        "abstract": "We investigate local notions of correlated equilibria, distributions of actions for smooth games such that players do not incur any regret against modifications of their strategies along a set of continuous vector fields. Our analysis shows that such equilibria are intrinsically linked to the projected gradient dynamics of the game. We identify the equivalent of coarse equilibria in this setting when no regret is incurred against any gradient field of a differentiable function. As a result, such equilibria are approximable when all players employ online (projected) gradient ascent with equal step-sizes as learning algorithms, and when their compact and convex action sets either (1) possess a smooth boundary, or (2) are polyhedra over which linear optimisation is ``trivial''. As a consequence, primal-dual proofs of performance guarantees for local coarse equilibria take the form of a generalised Lyapunov function for the gradient dynamics of the game. Adapting the regret matching framework to our setting, we also show that general local correlated equilibria are approximable when the set of vector fields is finite, given access to a fixed-point oracle for linear or conical combinations. For the class of affine-linear vector fields, which subsumes correlated equilibria of normal form games as a special case, such a fixed-point turns out to be the solution of a convex quadratic minimisation problem. Our results are independent of concavity assumptions on players' utilities.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "39 pages"
    },
    {
        "paper id": "2403.18200",
        "abstract url": "https://arxiv.org/abs/2403.18200",
        "title": "Fault-tolerant properties of scale-free linear protocols for synchronization of homogeneous multi-agent systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Originally, protocols were designed for multi-agent systems (MAS) using information about the network. However, in many cases there is no or only limited information available about the network. Recently, there has been a focus on scale-free synchronization of multi-agent systems (MAS). In this case, the protocol is designed without any prior information about the network. As long as the network contains a directed spanning tree, the scale-free protocol guarantees that the network achieves synchronization. If there is no directed spanning tree for the network then synchronization cannot be achieved. But what happens when these scale-free protocols are applied to such a network where the directed spanning tree no longer exists? The latter might arise if, for instance, a fault occurs in one of more crucial links. This paper establishes that the network decomposes into a number of basic bicomponents which achieves synchronization among all nodes in this basic bicomponent. On the other hand, nodes which are not part of any basic bicomponent converge to a weighted average of the synchronized trajectories of the basic bicomponents. The weights are independent of the initial conditions and are independent of the designed protocol.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "The article was submitted to IEEE Transactions on Automatic Control for review at March 27th, 2024"
    },
    {
        "paper id": "2403.18216",
        "abstract url": "https://arxiv.org/abs/2403.18216",
        "title": "Minimax Optimal Fair Classification with Bounded Demographic Disparity",
        "rating": -10,
        "keywords": [],
        "abstract": "Mitigating the disparate impact of statistical machine learning methods is crucial for ensuring fairness. While extensive research aims to reduce disparity, the effect of using a \\emph{finite dataset} -- as opposed to the entire population -- remains unclear. This paper explores the statistical foundations of fair binary classification with two protected groups, focusing on controlling demographic disparity, defined as the difference in acceptance rates between the groups. Although fairness may come at the cost of accuracy even with infinite data, we show that using a finite sample incurs additional costs due to the need to estimate group-specific acceptance thresholds. We study the minimax optimal classification error while constraining demographic disparity to a user-specified threshold. To quantify the impact of fairness constraints, we introduce a novel measure called \\emph{fairness-aware excess risk} and derive a minimax lower bound on this measure that all classifiers must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding method with an offset that we show attains the minimax lower bound. Our lower bound proofs involve several innovations. Experiments support that FairBayes-DDP+ controls disparity at the user-specified level, while being faster and having a more favorable fairness-accuracy tradeoff than several baselines.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18222",
        "abstract url": "https://arxiv.org/abs/2403.18222",
        "title": "Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies",
        "rating": -10,
        "keywords": [],
        "abstract": "Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures"
    },
    {
        "paper id": "2403.18223",
        "abstract url": "https://arxiv.org/abs/2403.18223",
        "title": "A Transformer-Based Framework for Payload Malware Detection and Classification",
        "rating": -10,
        "keywords": [],
        "abstract": "As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attention mechanism. Our proposed method uses the raw payload bytes that represent the packet contents and is deployed as man-in-the-middle. The payload bytes are used to detect malicious packets and classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23 datasets demonstrate that our transformer-based model is effective in distinguishing malicious from benign traffic in the test dataset, attaining an average accuracy of 79\\% using binary classification and 72\\% on the multi-classification experiment, both using solely payload bytes.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18226",
        "abstract url": "https://arxiv.org/abs/2403.18226",
        "title": "How is Testing Related to Single Statement Bugs?",
        "rating": -10,
        "keywords": [],
        "abstract": "In this study, we analyzed the correlation between unit test coverage and the occurrence of Single Statement Bugs (SSBs) in open-source Java projects. We analyzed data from the top 100 Maven-based projects on GitHub, which includes 7824 SSBs. Our preliminary findings suggest a weak to moderate correlation, indicating that increased test coverage is somewhat reduce the occurrence of SSBs. However, this relationship is not very strong, emphasizing the need for better tests. Our study contributes to the ongoing discussion on enhancing software quality and provides a basis for future research into effective testing practices aimed at mitigating SSBs.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18229",
        "abstract url": "https://arxiv.org/abs/2403.18229",
        "title": "A Comprehensive Overview of the Lebesgue Differentiation Theorem in Coq",
        "rating": -10,
        "keywords": [],
        "abstract": "Formalization of real analysis offers a chance to rebuild traditional proofs of important theorems as unambiguous theories that can be interactively explored. This paper provides a comprehensive overview of the Lebesgue Differentiation Theorem formalized in the Coq proof assistant, from which the first Fundamental Theorem of Calculus (FTC) for the Lebesgue integral is obtained as a corollary. Proving the first FTC in this way has the advantage of decomposing into loosely-coupled theories of moderate size and of independent interest that lend themselves well to incremental and collaborative development. We explain how we formalize all the topological constructs and all the standard lemmas needed to eventually relate the definitions of derivability and of Lebesgue integration of MathComp-Analysis, a formalization of analysis developed on top of the Mathematical Components library. In the course of this experiment, we substantially enrich MathComp-Analysis and even devise a new proof for Urysohn's lemma.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18231",
        "abstract url": "https://arxiv.org/abs/2403.18231",
        "title": "The Dimensions of the Hulls of Conorm Codes from Algebraic Geometry Codes",
        "rating": -10,
        "keywords": [],
        "abstract": "Chara et al. introduced conorm codes defined over algebraic geometry codes, but the hulls of conorm codes were not determined yet. In this paper, we study the dimension of the hull of conorm codes using the method introduced by Camps et al. For an algebraic geometry code $\\mathcal{C}:=C_\\mathscr{L}(D, G)$, we consider the divisor $\\gcd(G, H)$, where $H$ is the divisor satisfying \\[C_\\mathscr{L}(D, G)^\\perp=C_\\mathscr{L}(D, H).\\] Given an extension $F'/\\mathbb{F}_{q^t}$ of an algebraic function field $F/\\mathbb{F}_q$, we assume that the divisor $\\gcd(G, H)$ is non-special. If the degree of $\\gcd(G, H)$ is greater than $2g-2+{t\\over [F':F]}\u00b0\\text{Diff}(F'/F)$, then we have determined the exact dimension of the hull of the conorm of $\\mathcal{C}$. If not, we have determined the lower bound of the dimension of the hull of the conorm of $\\mathcal{C}$. We provide some examples for the dimension of the hull of certain conorm codes of AG codes defined over a rational function field.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.18235",
        "abstract url": "https://arxiv.org/abs/2403.18235",
        "title": "An Execution-time-certified QP Algorithm for $\\ell_1$ penalty-based Soft-constrained MPC",
        "rating": -10,
        "keywords": [],
        "abstract": "Providing an execution time certificate and handling possible infeasibility in closed-loop are two pressing requirements of Model Predictive Control (MPC). To simultaneously meet these two requirements, this paper uses an $\\ell_1$ penalty-based soft-constrained MPC formulation and innovatively transforms the resulting non-smooth QP into a box-constrained QP, which is solved by our previously proposed direct and execution-time certified algorithm with only dimension-dependent (data-independent), simple-calculated and exact number of iterations (Wu and Braatz (2023)). This approach not only overcomes the limitation of our previously proposed algorithm (Wu and Braatz (2023)), only applicable to input-constrained MPC, but also enjoys exact recovery feature (exactly recover the same solution when the original problem is feasible) of $\\ell_1$ penalty-based soft-constrained MPC formulation without suffering numerical difficulty of the resulting non-smoothness. Other various real-time QP applications, not limited to MPC, would also benefit from our QP algorithm with execution-time certificate and global feasibility.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, submitted to Automatica"
    },
    {
        "paper id": "2403.18254",
        "abstract url": "https://arxiv.org/abs/2403.18254",
        "title": "Differentially Private Distributed Nonconvex Stochastic Optimization with Quantized Communications",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes a new distributed nonconvex stochastic optimization algorithm that can achieve privacy protection, communication efficiency and convergence simultaneously. Specifically, each node adds time-varying privacy noises to its local state to avoid information leakage, and then quantizes its noise-perturbed state before transmitting to improve communication efficiency. By employing the subsampling method controlled through the sample-size parameter, the proposed algorithm reduces the impact of privacy noises, and enhances the differential privacy level. When the global cost function satisfies the Polyak-Lojasiewicz condition, the mean and high-probability convergence rate and the oracle complexity of the proposed algorithm are given. Importantly, the proposed algorithm achieves both the mean convergence and a finite cumulative differential privacy budget over infinite iterations as the sample-size goes to infinity. A numerical example of the distributed training on the \"MNIST\" dataset is given to show the effectiveness of the algorithm.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.00056",
        "abstract url": "https://arxiv.org/abs/2404.00056",
        "title": "Fingerprinting web servers through Transformer-encoded HTTP response headers",
        "rating": -10,
        "keywords": [],
        "abstract": "We explored leveraging state-of-the-art deep learning, big data, and natural language processing to enhance the detection of vulnerable web server versions. Focusing on improving accuracy and specificity over rule-based systems, we conducted experiments by sending various ambiguous and non-standard HTTP requests to 4.77 million domains and capturing HTTP response status lines. We represented these status lines through training a BPE tokenizer and RoBERTa encoder for unsupervised masked language modeling. We then dimensionality reduced and concatenated encoded response lines to represent each domain's web server. A Random Forest and multilayer perceptron (MLP) classified these web servers, and achieved 0.94 and 0.96 macro F1-score, respectively, on detecting the five most popular origin web servers. The MLP achieved a weighted F1-score of 0.55 on classifying 347 major type and minor version pairs. Analysis indicates that our test cases are meaningful discriminants of web server types. Our approach demonstrates promise as a powerful and flexible alternative to rule-based systems.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Based on a bachelor's thesis. Submission to arXiv approved by supervisor"
    },
    {
        "paper id": "2404.00057",
        "abstract url": "https://arxiv.org/abs/2404.00057",
        "title": "PerOS: Personalized Self-Adapting Operating Systems in the Cloud",
        "rating": -10,
        "keywords": [],
        "abstract": "Operating systems (OSes) are foundational to computer systems, managing hardware resources and ensuring secure environments for diverse applications. However, despite their enduring importance, the fundamental design objectives of OSes have seen minimal evolution over decades. Traditionally prioritizing aspects like speed, memory efficiency, security, and scalability, these objectives often overlook the crucial aspect of intelligence as well as personalized user experience. The lack of intelligence becomes increasingly critical amid technological revolutions, such as the remarkable advancements in machine learning (ML). Today's personal devices, evolving into intimate companions for users, pose unique challenges for traditional OSes like Linux and iOS, especially with the emergence of specialized hardware featuring heterogeneous components. Furthermore, the rise of large language models (LLMs) in ML has introduced transformative capabilities, reshaping user interactions and software development paradigms. While existing literature predominantly focuses on leveraging ML methods for system optimization or accelerating ML workloads, there is a significant gap in addressing personalized user experiences at the OS level. To tackle this challenge, this work proposes PerOS, a personalized OS ingrained with LLM capabilities. PerOS aims to provide tailored user experiences while safeguarding privacy and personal data through declarative interfaces, self-adaptive kernels, and secure data management in a scalable cloud-centric architecture; therein lies the main research question of this work: How can we develop intelligent, secure, and scalable OSes that deliver personalized experiences to thousands of users?",
        "subjects": [
            "cs.HC"
        ],
        "comment": "29 pages, 3 figures"
    },
    {
        "paper id": "2404.02167",
        "abstract url": "https://arxiv.org/abs/2404.02167",
        "title": "A remark on conditional entropy",
        "rating": -10,
        "keywords": [],
        "abstract": "The following note proves that conditional entropy of a sequence is almost time-reversal invariant, specifically they only differ by a small constant factor dependent only upon the forward and backward models that the entropies are being calculated with respect to. This gives rise to a numerical value that quantifies learnability, as well as a methodology to control for distributional shift between datasets. Rough guidelines are given for practitioners.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Preprint, 5 pages"
    },
    {
        "paper id": "2404.07959",
        "abstract url": "https://arxiv.org/abs/2404.07959",
        "title": "Damage identification of offshore jacket platforms in a digital twin framework considering optimal sensor placement",
        "rating": -10,
        "keywords": [],
        "abstract": "A new digital twin (DT) framework with optimal sensor placement (OSP) is proposed to accurately calculate the modal responses and identify the damage ratios of the offshore jacket platforms. The proposed damage identification framework consists of two models (namely one OSP model and one damage identification model). The OSP model adopts the multi-objective Lichtenberg algorithm (MOLA) to perform the sensor number/location optimization to make a good balance between the sensor cost and the modal calculation accuracy. In the damage identification model, the Markov Chain Monte Carlo (MCMC)-Bayesian method is developed to calculate the structural damage ratios based on the modal information obtained from the sensory measurements, where the uncertainties of the structural parameters are quantified. The proposed method is validated using an offshore jacket platform, and the analysis results demonstrate efficient identification of the structural damage location and severity.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2404.10635",
        "abstract url": "https://arxiv.org/abs/2404.10635",
        "title": "Compressed Federated Reinforcement Learning with a Generative Model",
        "rating": -10,
        "keywords": [],
        "abstract": "Reinforcement learning has recently gained unprecedented popularity, yet it still grapples with sample inefficiency. Addressing this challenge, federated reinforcement learning (FedRL) has emerged, wherein agents collaboratively learn a single policy by aggregating local estimations. However, this aggregation step incurs significant communication costs. In this paper, we propose CompFedRL, a communication-efficient FedRL approach incorporating both \\textit{periodic aggregation} and (direct/error-feedback) compression mechanisms. Specifically, we consider compressed federated $Q$-learning with a generative model setup, where a central server learns an optimal $Q$-function by periodically aggregating compressed $Q$-estimates from local agents. For the first time, we characterize the impact of these two mechanisms (which have remained elusive) by providing a finite-time analysis of our algorithm, demonstrating strong convergence behaviors when utilizing either direct or error-feedback compression. Our bounds indicate improved solution accuracy concerning the number of agents and other federated hyperparameters while simultaneously reducing communication costs. To corroborate our theory, we also conduct in-depth numerical experiments to verify our findings, considering Top-$K$ and Sparsified-$K$ sparsification operators.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.15296",
        "abstract url": "https://arxiv.org/abs/2404.15296",
        "title": "Maximum Discrepancy Generative Regularization and Non-Negative Matrix Factorization for Single Channel Source Separation",
        "rating": -10,
        "keywords": [],
        "abstract": "The idea of adversarial learning of regularization functionals has recently been introduced in the wider context of inverse problems. The intuition behind this method is the realization that it is not only necessary to learn the basic features that make up a class of signals one wants to represent, but also, or even more so, which features to avoid in the representation. In this paper, we will apply this approach to the training of generative models, leading to what we call Maximum Discrepancy Generative Regularization. In particular, we apply this to problem of source separation by means of Non-negative Matrix Factorization (NMF) and present a new method for the adversarial training of NMF bases. We show in numerical experiments, both for image and audio separation, that this leads to a clear improvement of the reconstructed signals, in particular in the case where little or no strong supervision data is available.",
        "subjects": [
            "math.NA"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2305.01758"
    },
    {
        "paper id": "2404.15298",
        "abstract url": "https://arxiv.org/abs/2404.15298",
        "title": "Unraveling Retraction Dynamics in COVID-19 Research: Patterns, Reasons, and Implications",
        "rating": -10,
        "keywords": [],
        "abstract": "Amid the COVID-19 pandemic, while the world sought solutions, some scholars exploited the situation for personal gains through deceptive studies and manipulated data. This paper presents the extent of 400 retracted COVID-19 papers listed by the Retraction Watch database until February 2024. The primary purpose of the research was to analyze journal quality and retraction trends. For all stakeholders involved, such as editors, relevant researchers, and policymakers, evaluating the journal's quality is crucial information since it could help them effectively stop such incidents and their negative effects in the future. The present research results imply that one-fourth of publications were retracted within the first month of their publication, followed by an additional 6\\% within six months of publication. One-third of the retractions originated from Q1 journals, with another significant portion coming from Q2 (29.8). A notable percentage of the retracted papers (23.2\\%) lacked publishing impact, signifying their publication as conference papers or in journals not indexed by Scopus. An examination of the retraction reasons reveals that one-fourth of retractions were due to numerous causes, mostly in Q2 journals, and another quarter were due to data problems, with the majority happening in Q1 publications. Elsevier retracted 31 of the papers, with the majority published in Q1, followed by Springer (11.5), predominantly in Q2. Retracted papers were mainly associated with the USA, China, and India. In the USA, retractions were primarily from Q1 journals followed by no-impact publications; in China, it was Q1 followed by Q2, and in India, it was Q2 followed by no-impact publications. The study also examined author contributions, revealing that 69.3 were male contributors, with females (30.7) mainly holding middle author positions.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "13 Pages, 9 figures"
    },
    {
        "paper id": "2405.00036",
        "abstract url": "https://arxiv.org/abs/2405.00036",
        "title": "Spatio-temporal load shifting for truly clean computing",
        "rating": -10,
        "keywords": [],
        "abstract": "Companies with datacenters are procuring significant amounts of renewable energy to reduce their carbon footprint. There is increasing interest in achieving 24/7 Carbon-Free Energy (CFE) matching in electricity usage, aiming to eliminate all carbon footprints associated with electricity consumption on an hourly basis. However, the variability of renewable energy resources poses significant challenges for achieving this goal. We explore the impact of shifting computing jobs and associated power loads both in time and between datacenter locations. We develop an optimization model to simulate a network of geographically distributed datacenters managed by a company leveraging spatio-temporal load flexibility to achieve 24/7 CFE matching. We isolate three signals relevant for informed use of load flexiblity: varying average quality of renewable energy resources, low correlation between wind power generation over long distances due to different weather conditions, and lags in solar radiation peak due to Earth's rotation. We illustrate that the location of datacenters and the time of year affect which signal drives an effective load-shaping strategy. The energy procurement and load-shifting decisions based on informed use of these signals facilitate the resource-efficiency and cost-effectiveness of clean computing -- the costs of 24/7 CFE are reduced by 1.29$\\pm$0.07 EUR/MWh for every additional percentage of flexible load. We provide practical guidelines on how companies with datacenters can leverage spatio-temporal load flexibility for truly clean computing. Our results and the open-source optimization model can also be useful for a broader variety of companies with flexible loads and an interest in eliminating their carbon footprint.",
        "subjects": [
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.00038",
        "abstract url": "https://arxiv.org/abs/2405.00038",
        "title": "Getting a Handle on Unmanaged Memory",
        "rating": -10,
        "keywords": [],
        "abstract": "The inability to relocate objects in unmanaged languages brings with it a menagerie of problems. Perhaps the most impactful is memory fragmentation, which has long plagued applications such as databases and web servers. These issues either fester or require Herculean programmer effort to address on a per-application basis because, in general, heap objects cannot be moved in unmanaged languages. In contrast, managed languages like C# cleanly address fragmentation through the use of compacting garbage collection techniques built upon heap object movement. In this work, we bridge this gap between unmanaged and managed languages through the use of handles, a level of indirection allowing heap object movement. Handles open the door to seamlessly employ runtime features from managed languages in existing, unmodified code written in unmanaged languages. We describe a new compiler and runtime system, ALASKA, that acts as a drop-in replacement for malloc. Without any programmer effort, the ALASKA compiler transforms pointer-based code to utilize handles, with optimizations to reduce performance impact. A codesigned runtime system manages this level of indirection and exploits heap object movement via an extensible service interface. We investigate the overheads of ALASKA on large benchmarks and applications spanning multiple domains. To show the power and extensibility of handles, we use ALASKA to eliminate fragmentation on the heap through compaction, reducing memory usage by up to 40% in Redis.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.01560",
        "abstract url": "https://arxiv.org/abs/2405.01560",
        "title": "Copyright related risks in the creation and use of ML/AI systems",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper summarizes the current copyright related risks that Machine Learning (ML) and Artificial Intelligence (AI) systems (including Large Language Models --LLMs) incur. These risks affect different stakeholders: owners of the copyright of the training data, the users of ML/AI systems, the creators of trained models, and the operators of AI systems. This paper also provides an overview of ongoing legal cases in the United States related to these risks.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    }
]