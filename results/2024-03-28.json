[
    {
        "paper id": "2403.19205",
        "abstract url": "https://arxiv.org/abs/2403.19205",
        "title": "From Activation to Initialization: Scaling Insights for Optimizing Neural Fields",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In the realm of computer vision, Neural Fields have gained prominence as a contemporary tool harnessing neural networks for signal representation. Despite the remarkable progress in adapting these networks to solve a variety of problems, the field still lacks a comprehensive theoretical framework. This article aims to address this gap by delving into the intricate interplay between initialization and activation, providing a foundational basis for the robust optimization of Neural Fields. Our theoretical insights reveal a deep-seated connection among network initialization, architectural choices, and the optimization process, emphasizing the need for a holistic approach when designing cutting-edge Neural Fields.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.19224",
        "abstract url": "https://arxiv.org/abs/2403.19224",
        "title": "Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition",
        "rating": 1.5,
        "keywords": [
            [
                "cs.SD"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "The mainstream paradigm of speech emotion recognition (SER) is identifying the single emotion label of the entire utterance. This line of works neglect the emotion dynamics at fine temporal granularity and mostly fail to leverage linguistic information of speech signal explicitly. In this paper, we propose Emotion Neural Transducer for fine-grained speech emotion recognition with automatic speech recognition (ASR) joint training. We first extend typical neural transducer with emotion joint network to construct emotion lattice for fine-grained SER. Then we propose lattice max pooling on the alignment lattice to facilitate distinguishing emotional and non-emotional frames. To adapt fine-grained SER to transducer inference manner, we further make blank, the special symbol of ASR, serve as underlying emotion indicator as well, yielding Factorized Emotion Neural Transducer. For typical utterance-level SER, our ENT models outperform state-of-the-art methods on IEMOCAP in low word error rate. Experiments on IEMOCAP and the latest speech emotion diarization dataset ZED also demonstrate the superiority of fine-grained emotion modeling. Our code is available at https://github.com/ECNU-Cross-Innovation-Lab/ENT.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Accepted by 49th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)"
    },
    {
        "paper id": "2403.19225",
        "abstract url": "https://arxiv.org/abs/2403.19225",
        "title": "Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Weakly-supervised action segmentation is a task of learning to partition a long video into several action segments, where training videos are only accompanied by transcripts (ordered list of actions). Most of existing methods need to infer pseudo segmentation for training by serial alignment between all frames and the transcript, which is time-consuming and hard to be parallelized while training. In this work, we aim to escape from this inefficient alignment with massive but redundant frames, and instead to directly localize a few action transitions for pseudo segmentation generation, where a transition refers to the change from an action segment to its next adjacent one in the transcript. As the true transitions are submerged in noisy boundaries due to intra-segment visual variation, we propose a novel Action-Transition-Aware Boundary Alignment (ATBA) framework to efficiently and effectively filter out noisy boundaries and detect transitions. In addition, to boost the semantic learning in the case that noise is inevitably present in the pseudo segmentation, we also introduce video-level losses to utilize the trusted video-level supervision. Extensive experiments show the effectiveness of our approach on both performance and training speed.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024"
    },
    {
        "paper id": "2403.19242",
        "abstract url": "https://arxiv.org/abs/2403.19242",
        "title": "RTracker: Recoverable Tracking via PN Tree Structured Memory",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Existing tracking methods mainly focus on learning better target representation or developing more robust prediction models to improve tracking performance. While tracking performance has significantly improved, the target loss issue occurs frequently due to tracking failures, complete occlusion, or out-of-view situations. However, considerably less attention is paid to the self-recovery issue of tracking methods, which is crucial for practical applications. To this end, we propose a recoverable tracking framework, RTracker, that uses a tree-structured memory to dynamically associate a tracker and a detector to enable self-recovery ability. Specifically, we propose a Positive-Negative Tree-structured memory to chronologically store and maintain positive and negative target samples. Upon the PN tree memory, we develop corresponding walking rules for determining the state of the target and define a set of control flows to unite the tracker and the detector in different tracking scenarios. Our core idea is to use the support samples of positive and negative target categories to establish a relative distance-based criterion for a reliable assessment of target loss. The favorable performance in comparison against the state-of-the-art methods on numerous challenging benchmarks demonstrates the effectiveness of the proposed algorithm.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "accepted by CVPR 2024"
    },
    {
        "paper id": "2403.19278",
        "abstract url": "https://arxiv.org/abs/2403.19278",
        "title": "CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Domain adaptive object detection aims to adapt detection models to domains where annotated data is unavailable. Existing methods have been proposed to address the domain gap using the semi-supervised student-teacher framework. However, a fundamental issue arises from the class imbalance in the labelled training set, which can result in inaccurate pseudo-labels. The relationship between classes, especially where one class is a majority and the other minority, has a large impact on class bias. We propose Class-Aware Teacher (CAT) to address the class bias issue in the domain adaptation setting. In our work, we approximate the class relationships with our Inter-Class Relation module (ICRm) and exploit it to reduce the bias within the model. In this way, we are able to apply augmentations to highly related classes, both inter- and intra-domain, to boost the performance of minority classes while having minimal impact on majority classes. We further reduce the bias by implementing a class-relation weight to our classification loss. Experiments conducted on various datasets and ablation studies show that our method is able to address the class bias in the domain adaptation setting. On the Cityscapes to Foggy Cityscapes dataset, we attained a 52.5 mAP, a substantial improvement over the 51.2 mAP achieved by the state-of-the-art method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted into CVPR 2024"
    },
    {
        "paper id": "2403.19366",
        "abstract url": "https://arxiv.org/abs/2403.19366",
        "title": "Infrared Small Target Detection with Scale and Location Sensitivity",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Recently, infrared small target detection (IRSTD) has been dominated by deep-learning-based methods. However, these methods mainly focus on the design of complex model structures to extract discriminative features, leaving the loss functions for IRSTD under-explored. For example, the widely used Intersection over Union (IoU) and Dice losses lack sensitivity to the scales and locations of targets, limiting the detection performance of detectors. In this paper, we focus on boosting detection performance with a more effective loss but a simpler model structure. Specifically, we first propose a novel Scale and Location Sensitive (SLS) loss to handle the limitations of existing losses: 1) for scale sensitivity, we compute a weight for the IoU loss based on target scales to help the detector distinguish targets with different scales: 2) for location sensitivity, we introduce a penalty term based on the center points of targets to help the detector localize targets more precisely. Then, we design a simple Multi-Scale Head to the plain U-Net (MSHNet). By applying SLS loss to each scale of the predictions, our MSHNet outperforms existing state-of-the-art methods by a large margin. In addition, the detection performance of existing detectors can be further improved when trained with our SLS loss, demonstrating the effectiveness and generalization of our SLS loss. The code is available at https://github.com/ying-fu/MSHNet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2403.19376",
        "abstract url": "https://arxiv.org/abs/2403.19376",
        "title": "NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "The acquisition of objects outside the Line-of-Sight of cameras is a very intriguing but also extremely challenging research topic. Recent works showed the feasibility of this idea exploiting transient imaging data produced by custom direct Time of Flight sensors. In this paper, for the first time, we tackle this problem using only data from an off-the-shelf indirect Time of Flight sensor without any further hardware requirement. We introduced a Deep Learning model able to re-frame the surfaces where light bounces happen as a virtual mirror. This modeling makes the task easier to handle and also facilitates the construction of annotated training data. From the obtained data it is possible to retrieve the depth information of the hidden scene. We also provide a first-in-its-kind synthetic dataset for the task and demonstrate the feasibility of the proposed idea over it.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Submitted to ECCV 24, 17 pages, 6 figures, 2 tables"
    },
    {
        "paper id": "2403.19527",
        "abstract url": "https://arxiv.org/abs/2403.19527",
        "title": "Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Category-level 6D object pose estimation aims to estimate the rotation, translation and size of unseen instances within specific categories. In this area, dense correspondence-based methods have achieved leading performance. However, they do not explicitly consider the local and global geometric information of different instances, resulting in poor generalization ability to unseen instances with significant shape variations. To deal with this problem, we propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learning method for category-level 6D object pose estimation (AG-Pose), which includes two key designs: (1) The first design is an Instance-Adaptive Keypoint Detection module, which can adaptively detect a set of sparse keypoints for various instances to represent their geometric structures. (2) The second design is a Geometric-Aware Feature Aggregation module, which can efficiently integrate the local and global geometric information into keypoint features. These two modules can work together to establish robust keypoint-level correspondences for unseen instances, thus enhancing the generalization ability of the model.Experimental results on CAMERA25 and REAL275 datasets show that the proposed AG-Pose outperforms state-of-the-art methods by a large margin without category-specific shape priors.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR2024"
    },
    {
        "paper id": "2403.19154",
        "abstract url": "https://arxiv.org/abs/2403.19154",
        "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \\citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \\citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \\texttt{Questioner} -- and a \\texttt{Roleplayer} whose preferences are unknown to the \\texttt{Questioner}. By asking questions, the \\texttt{Questioner} elicits preferences from the \\texttt{Roleplayer}. The \\texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \\texttt{Oracle} with access to the \\texttt{Roleplayer}'s latent preferences. After two iterations of self-improvement, the \\texttt{Questioner} asks better questions, allowing it to generate responses that are preferred over responses from the initial model on \\highlightpink{\\textbf{72\\%}} of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19158",
        "abstract url": "https://arxiv.org/abs/2403.19158",
        "title": "Uncertainty-Aware Deep Video Compression with Ensembles",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning-based video compression is a challenging task, and many previous state-of-the-art learning-based video codecs use optical flows to exploit the temporal correlation between successive frames and then compress the residual error. Although these two-stage models are end-to-end optimized, the epistemic uncertainty in the motion estimation and the aleatoric uncertainty from the quantization operation lead to errors in the intermediate representations and introduce artifacts in the reconstructed frames. This inherent flaw limits the potential for higher bit rate savings. To address this issue, we propose an uncertainty-aware video compression model that can effectively capture the predictive uncertainty with deep ensembles. Additionally, we introduce an ensemble-aware loss to encourage the diversity among ensemble members and investigate the benefits of incorporating adversarial training in the video compression task. Experimental results on 1080p sequences show that our model can effectively save bits by more than 20% compared to DVC Pro.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Published on IEEE Transactions on Multimedia"
    },
    {
        "paper id": "2403.19167",
        "abstract url": "https://arxiv.org/abs/2403.19167",
        "title": "Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. Then, we proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tasks. Code is available at \\texttt{https://github.com/LibroWu/SelF-Reasoner}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19183",
        "abstract url": "https://arxiv.org/abs/2403.19183",
        "title": "Empirical Analysis for Unsupervised Universal Dependency Parse Tree Aggregation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Dependency parsing is an essential task in NLP, and the quality of dependency parsers is crucial for many downstream tasks. Parsers' quality often varies depending on the domain and the language involved. Therefore, it is essential to combat the issue of varying quality to achieve stable performance. In various NLP tasks, aggregation methods are used for post-processing aggregation and have been shown to combat the issue of varying quality. However, aggregation methods for post-processing aggregation have not been sufficiently studied in dependency parsing tasks. In an extensive empirical study, we compare different unsupervised post-processing aggregation methods to identify the most suitable dependency tree structure aggregation method.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19207",
        "abstract url": "https://arxiv.org/abs/2403.19207",
        "title": "LV-CTC: Non-autoregressive ASR with CTC and latent variable models",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Non-autoregressive (NAR) models for automatic speech recognition (ASR) aim to achieve high accuracy and fast inference by simplifying the autoregressive (AR) generation process of conventional models. Connectionist temporal classification (CTC) is one of the key techniques used in NAR ASR models. In this paper, we propose a new model combining CTC and a latent variable model, which is one of the state-of-the-art models in the neural machine translation research field. A new neural network architecture and formulation specialized for ASR application are introduced. In the proposed model, CTC alignment is assumed to be dependent on the latent variables that are expected to capture dependencies between tokens. Experimental results on a 100 hours subset of Librispeech corpus showed the best recognition accuracy among CTC-based NAR models. On the TED-LIUM2 corpus, the best recognition accuracy is achieved including AR E2E models with faster inference speed.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19213",
        "abstract url": "https://arxiv.org/abs/2403.19213",
        "title": "Learning Multiple Representations with Inconsistency-Guided Detail Regularization for Mask-Guided Matting",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Mask-guided matting networks have achieved significant improvements and have shown great potential in practical applications in recent years. However, simply learning matting representation from synthetic and lack-of-real-world-diversity matting data, these approaches tend to overfit low-level details in wrong regions, lack generalization to objects with complex structures and real-world scenes such as shadows, as well as suffer from interference of background lines or textures. To address these challenges, in this paper, we propose a novel auxiliary learning framework for mask-guided matting models, incorporating three auxiliary tasks: semantic segmentation, edge detection, and background line detection besides matting, to learn different and effective representations from different types of data and annotations. Our framework and model introduce the following key aspects: (1) to learn real-world adaptive semantic representation for objects with diverse and complex structures under real-world scenes, we introduce extra semantic segmentation and edge detection tasks on more diverse real-world data with segmentation annotations; (2) to avoid overfitting on low-level details, we propose a module to utilize the inconsistency between learned segmentation and matting representations to regularize detail refinement; (3) we propose a novel background line detection task into our auxiliary learning framework, to suppress interference of background lines or textures. In addition, we propose a high-quality matting benchmark, Plant-Mat, to evaluate matting methods on complex structures. Extensively quantitative and qualitative results show that our approach outperforms state-of-the-art mask-guided methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19217",
        "abstract url": "https://arxiv.org/abs/2403.19217",
        "title": "Blind Identification of Binaural Room Impulse Responses from Smart Glasses",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Smart glasses are increasingly recognized as a key medium for augmented reality, offering a hands-free platform with integrated microphones and non-ear-occluding loudspeakers to seamlessly mix virtual sound sources into the real-world acoustic scene. To convincingly integrate virtual sound sources, the room acoustic rendering of the virtual sources must match the real-world acoustics. Information about a user's acoustic environment however is typically not available. This work uses a microphone array in a pair of smart glasses to blindly identify binaural room impulse responses (BRIRs) from a few seconds of speech in the real-world environment. The proposed method uses dereverberation and beamforming to generate a pseudo reference signal that is used by a multichannel Wiener filter to estimate room impulse responses which are then converted to BRIRs. The multichannel room impulse responses can be used to estimate room acoustic parameters which is shown to outperform baseline algorithms in the estimation of reverberation time and direct-to-reverberant energy ratio. Results from a listening experiment further indicate that the estimated BRIRs often reproduce the real-world room acoustics perceptually more convincingly than measured BRIRs from other rooms with similar geometry.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19219",
        "abstract url": "https://arxiv.org/abs/2403.19219",
        "title": "Collaborative Knowledge Infusion for Low-resource Stance Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Stance detection is the view towards a specific target by a given context (\\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often needed to assist stance detection models in understanding the target well and making detection correctly. However, prevailing works for knowledge-infused stance detection predominantly incorporate target knowledge from a singular source that lacks knowledge verification in limited domain knowledge. The low-resource training data further increases the challenge for the data-driven large models in this task. To address those challenges, we propose a collaborative knowledge infusion approach for low-resource stance detection tasks, employing a combination of aligned knowledge enhancement and efficient parameter learning techniques. Specifically, our stance detection approach leverages target background knowledge collaboratively from different knowledge sources with the help of knowledge alignment. Additionally, we also introduce the parameter-efficient collaborative adaptor with a staged optimization algorithm, which collaboratively addresses the challenges associated with low-resource stance detection tasks from both network structure and learning perspectives. To assess the effectiveness of our method, we conduct extensive experiments on three public stance detection datasets, including low-resource and cross-target settings. The results demonstrate significant performance improvements compared to the existing stance detection approaches.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages, 3 figures, Big Data Mining and Analysis"
    },
    {
        "paper id": "2403.19221",
        "abstract url": "https://arxiv.org/abs/2403.19221",
        "title": "Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video paragraph captioning (VPC) involves generating detailed narratives for long videos, utilizing supportive modalities such as speech and event boundaries. However, the existing models are constrained by the assumption of constant availability of a single auxiliary modality, which is impractical given the diversity and unpredictable nature of real-world scenarios. To this end, we propose a Missing-Resistant framework MR-VPC that effectively harnesses all available auxiliary inputs and maintains resilience even in the absence of certain modalities. Under this framework, we propose the Multimodal VPC (MVPC) architecture integrating video, speech, and event boundary inputs in a unified manner to process various auxiliary inputs. Moreover, to fortify the model against incomplete data, we introduce DropAM, a data augmentation strategy that randomly omits auxiliary inputs, paired with DistillAM, a regularization target that distills knowledge from teacher models trained on modality-complete data, enabling efficient learning in modality-deficient environments. Through exhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has proven to deliver superior performance on modality-complete and modality-missing test data. This work highlights the significance of developing resilient VPC models and paves the way for more adaptive, robust multimodal video understanding.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code available at https://github.com/lancopku/MR-VPC"
    },
    {
        "paper id": "2403.19238",
        "abstract url": "https://arxiv.org/abs/2403.19238",
        "title": "Taming Lookup Tables for Efficient Image Retouching",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The widespread use of high-definition screens in edge devices, such as end-user cameras, smartphones, and televisions, is spurring a significant demand for image enhancement. Existing enhancement models often optimize for high performance while falling short of reducing hardware inference time and power consumption, especially on edge devices with constrained computing and storage resources. To this end, we propose Image Color Enhancement Lookup Table (ICELUT) that adopts LUTs for extremely efficient edge inference, without any convolutional neural network (CNN). During training, we leverage pointwise (1x1) convolution to extract color information, alongside a split fully connected layer to incorporate global information. Both components are then seamlessly converted into LUTs for hardware-agnostic deployment. ICELUT achieves near-state-of-the-art performance and remarkably low power consumption. We observe that the pointwise network structure exhibits robust scalability, upkeeping the performance even with a heavily downsampled 32x32 input image. These enable ICELUT, the first-ever purely LUT-based image enhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at least one order faster than any CNN solution. Codes are available at https://github.com/Stephen0808/ICELUT.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19260",
        "abstract url": "https://arxiv.org/abs/2403.19260",
        "title": "NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "To address the global issue of hateful content proliferating in online platforms, hate speech detection (HSD) models are typically developed on datasets collected in the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on curated samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature largely overestimates real-world performance on representative data. We also propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance. Finally, we show that in this context, a human-in-the-loop approach to content moderation where humans review 1% of Nigerian tweets flagged as hateful would enable to moderate 60% of all hateful content. Taken together, these results pave the way towards robust HSD systems and a better protection of social media users from hateful content in low-resource settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19267",
        "abstract url": "https://arxiv.org/abs/2403.19267",
        "title": "MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/cocacola-lab/MineLand.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Project website: https://github.com/cocacola-lab/MineLand"
    },
    {
        "paper id": "2403.19270",
        "abstract url": "https://arxiv.org/abs/2403.19270",
        "title": "sDPO: Don't Use Your Data All at Once",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19275",
        "abstract url": "https://arxiv.org/abs/2403.19275",
        "title": "Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the agent suitable for social media, we design five basic modules for it: persona, planning, action, memory and reflection. To provide an interaction and verification environment for the agent, we build a social media simulation sandbox. In the experimental verification, automatic and human evaluations demonstrated the effectiveness of the agent we constructed.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19285",
        "abstract url": "https://arxiv.org/abs/2403.19285",
        "title": "Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task. How to select informative examples remains an open issue. Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19294",
        "abstract url": "https://arxiv.org/abs/2403.19294",
        "title": "FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Self-supervised multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results on the KITTI and Cityscapes datasets show that our method outperforms the state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19306",
        "abstract url": "https://arxiv.org/abs/2403.19306",
        "title": "Sparse Generation: Making Pseudo Labels Sparse for weakly supervision with points",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, research on point weakly supervised object detection (PWSOD) methods in the field of computer vision has attracted people's attention. However, existing pseudo labels generation methods perform poorly in a small amount of supervised annotation data and dense object detection tasks. We consider the generation of weakly supervised pseudo labels as the result of model's sparse output, and propose a method called Sparse Generation to make pseudo labels sparse. It constructs dense tensors through the relationship between data and detector model, optimizes three of its parameters, and obtains a sparse tensor via coordinated calculation, thereby indirectly obtaining higher quality pseudo labels, and solving the model's density problem in the situation of only a small amount of supervised annotation data can be used. On two broadly used open-source datasets (RSOD, SIMD) and a self-built dataset (Bullet-Hole), the experimental results showed that the proposed method has a significant advantage in terms of overall performance metrics, comparing to that state-of-the-art method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19318",
        "abstract url": "https://arxiv.org/abs/2403.19318",
        "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "30 pages"
    },
    {
        "paper id": "2403.19322",
        "abstract url": "https://arxiv.org/abs/2403.19322",
        "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The surge of Multimodal Large Language Models (MLLMs), given their prominent emergent capabilities in instruction following and reasoning, has greatly advanced the field of visual reasoning. However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play grounding of reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting. We further create P2GB, a benchmark aimed at assessing MLLMs' ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual reasoning tasks demonstrate the superiority of P2G. Noteworthy, P2G achieved comparable performance with GPT-4V on P2GB, with a 7B backbone. Our work highlights the potential of plug-and-play grounding of reasoning and opens up a promising alternative beyond model scaling.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 3 figures"
    },
    {
        "paper id": "2403.19335",
        "abstract url": "https://arxiv.org/abs/2403.19335",
        "title": "KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment analysis that is the first and largest publicly available dataset of its kind. KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from various sources and includes numerical ratings ranging from 1 to 5, providing a quantitative representation of customer attitudes. The study also pursued the automation of Kazakh sentiment classification through the development and evaluation of four machine learning models trained for both polarity classification and score classification. Experimental analysis included evaluation of the results considering both balanced and imbalanced scenarios. The most successful model attained an F1-score of 0.81 for polarity classification and 0.39 for score classification on the test sets. The dataset and fine-tuned models are open access and available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19340",
        "abstract url": "https://arxiv.org/abs/2403.19340",
        "title": "Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "To address the challenges associated with data processing at scale, we propose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline for large language models (LLMs) with a user-friendly design at its core. Easy addition of custom processors with block-based interface in Dataverse allows users to readily and efficiently use Dataverse to build their own ETL pipeline. We hope that Dataverse will serve as a vital tool for LLM development and open source the entire library to welcome community contribution. Additionally, we provide a concise, two-minute video demonstration of our system, illustrating its capabilities and implementation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19346",
        "abstract url": "https://arxiv.org/abs/2403.19346",
        "title": "Large Language Models Are Unconscious of Unreasonability in Math Problems",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors. In this paper, we study the behavior of LLMs when faced with unreasonable math problems and further explore their potential to address these problems. First, we construct the Unreasonable Math Problem (UMP) benchmark to examine the error detection ability of LLMs. Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content. In order to improve their ability of error detection and correction, we further design a strategic prompt template called Critical Calculation and Conclusion(CCC). With CCC, LLMs can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 4 figures"
    },
    {
        "paper id": "2403.19352",
        "abstract url": "https://arxiv.org/abs/2403.19352",
        "title": "A diverse Multilingual News Headlines Dataset from around the World",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Babel Briefings is a novel dataset featuring 4.7 million news headlines from August 2020 to November 2021, across 30 languages and 54 locations worldwide with English translations of all articles included. Designed for natural language processing and media studies, it serves as a high-quality dataset for training or evaluating language models as well as offering a simple, accessible collection of articles, for example, to analyze global news coverage and cultural narratives. As a simple demonstration of the analyses facilitated by this dataset, we use a basic procedure using a TF-IDF weighted similarity metric to group articles into clusters about the same event. We then visualize the \\emph{event signatures} of the event showing articles of which languages appear over time, revealing intuitive features based on the proximity of the event and unexpectedness of the event. The dataset is available on \\href{https://www.kaggle.com/datasets/felixludos/babel-briefings}{Kaggle} and \\href{https://huggingface.co/datasets/felixludos/babel-briefings}{HuggingFace} with accompanying \\href{https://github.com/felixludos/babel-briefings}{GitHub} code.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Published in NAACL 2024 Proceedings (Short Paper track)"
    },
    {
        "paper id": "2403.19354",
        "abstract url": "https://arxiv.org/abs/2403.19354",
        "title": "AIpom at SemEval-2024 Task 8: Detecting AI-produced Outputs in M4",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper describes AIpom, a system designed to detect a boundary between human-written and machine-generated text (SemEval-2024 Task 8, Subtask C: Human-Machine Mixed Text Detection). We propose a two-stage pipeline combining predictions from an instruction-tuned decoder-only model and encoder-only sequence taggers. AIpom is ranked second on the leaderboard while achieving a Mean Absolute Error of 15.94. Ablation studies confirm the benefits of pipelining encoder and decoder models, particularly in terms of improved performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "2nd place at SemEval-2024 Task 8, Subtask C, to appear in SemEval-2024 proceedings"
    },
    {
        "paper id": "2403.19365",
        "abstract url": "https://arxiv.org/abs/2403.19365",
        "title": "EthioMT: Parallel Corpus for Low-resource Ethiopian Languages",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent research in natural language processing (NLP) has achieved impressive performance in tasks such as machine translation (MT), news classification, and question-answering in high-resource languages. However, the performance of MT leaves much to be desired for low-resource languages. This is due to the smaller size of available parallel corpora in these languages, if such corpora are available at all. NLP in Ethiopian languages suffers from the same issues due to the unavailability of publicly accessible datasets for NLP tasks, including MT. To help the research community and foster research for Ethiopian languages, we introduce EthioMT -- a new parallel corpus for 15 languages. We also create a new benchmark by collecting a dataset for better-researched languages in Ethiopia. We evaluate the newly collected corpus and the benchmark dataset for 23 Ethiopian languages using transformer and fine-tuning approaches.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at The Fifth workshop on Resources for African Indigenous Languages (RAIL) 2024 ( LREC-COLING 2024)"
    },
    {
        "paper id": "2403.19386",
        "abstract url": "https://arxiv.org/abs/2403.19386",
        "title": "PointCloud-Text Matching: Benchmark Datasets and a Baseline",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages token-level and feature-level attention to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity. To handle noisy correspondence, RNCL divides negative pairs, which are much less error-prone than positive pairs, into clean and noisy subsets, and assigns them forward and reverse optimization directions respectively, thus enhancing robustness against noisy correspondence. We conduct extensive experiments on our benchmarks and demonstrate the superiority of our RoMa.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19390",
        "abstract url": "https://arxiv.org/abs/2403.19390",
        "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19399",
        "abstract url": "https://arxiv.org/abs/2403.19399",
        "title": "KazParC: Kazakh Parallel Corpus for Machine Translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19424",
        "abstract url": "https://arxiv.org/abs/2403.19424",
        "title": "The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Post-hoc explanation methods are an important tool for increasing model transparency for users. Unfortunately, the currently used methods for attributing token importance often yield diverging patterns. In this work, we study potential sources of disagreement across methods from a linguistic perspective. We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences. Token-level differences between methods are smoothed out if we compare them on the syntactic span level. We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size $k$. We systematically investigate the interaction between $k$ and spans and propose an improved configuration for selecting important tokens.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Long paper accepted to LREC-Coling 2024 main conference. Please cite the conference proceedings version when available"
    },
    {
        "paper id": "2403.19432",
        "abstract url": "https://arxiv.org/abs/2403.19432",
        "title": "Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Data accuracy is essential for scientific research and policy development. The National Violent Death Reporting System (NVDRS) data is widely used for discovering the patterns and causes of death. Recent studies suggested the annotation inconsistencies within the NVDRS and the potential impact on erroneous suicide-cause attributions. We present an empirical Natural Language Processing (NLP) approach to detect annotation inconsistencies and adopt a cross-validation-like paradigm to identify problematic instances. We analyzed 267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our results showed that incorporating the target state's data into training the suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the target state's test set and a decrease of 1.1% on other states' test set. To conclude, we demonstrated the annotation inconsistencies in NVDRS's death investigation notes, identified problematic instances, evaluated the effectiveness of correcting problematic instances, and eventually proposed an NLP improvement solution.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "19 pages, 6 figures"
    },
    {
        "paper id": "2403.19492",
        "abstract url": "https://arxiv.org/abs/2403.19492",
        "title": "Segmentation tool for images of cracks",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Safety-critical infrastructures, such as bridges, are periodically inspected to check for existing damage, such as fatigue cracks and corrosion, and to guarantee the safe use of the infrastructure. Visual inspection is the most frequent type of general inspection, despite the fact that its detection capability is rather limited, especially for fatigue cracks. Machine learning algorithms can be used for augmenting the capability of classical visual inspection of bridge structures, however, the implementation of such an algorithm requires a massive annotated training dataset, which is time-consuming to produce. This paper proposes a semi-automatic crack segmentation tool that eases the manual segmentation of cracks on images needed to create a training dataset for a machine learning algorithm. Also, it can be used to measure the geometry of the crack. This tool makes use of an image processing algorithm, which was initially developed for the analysis of vascular systems on retinal images. The algorithm relies on a multi-orientation wavelet transform, which is applied to the image to construct the so-called \"orientation scores\", i.e. a modified version of the image. Afterwards, the filtered orientation scores are used to formulate an optimal path problem that identifies the crack. The globally optimal path between manually selected crack endpoints is computed, using a state-of-the-art geometric tracking method. The pixel-wise segmentation is done afterwards using the obtained crack path. The proposed method outperforms fully automatic methods and shows potential to be an adequate alternative to the manual data annotation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19509",
        "abstract url": "https://arxiv.org/abs/2403.19509",
        "title": "Phonetic Segmentation of the UCLA Phonetics Lab Archive",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Research in speech technologies and comparative linguistics depends on access to diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of the earliest multilingual speech corpora, with long-form audio recordings and phonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently, 95 of these languages were time-aligned with word-level phonetic transcriptions (Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive, which uses the 95-language CMU re-release as our starting point. VoxAngeles also includes word- and phone-level segmentations from the original UCLA corpus, as well as phonetic measurements of word and phone durations, vowel formants, and vowel f0. This corpus enhances the usability of the original data, particularly for quantitative phonetic typology, as demonstrated through a case study of vowel intrinsic f0. We also discuss the utility of the VoxAngeles corpus for general research and pedagogy in crosslinguistic phonetics, as well as for low-resource and multilingual speech technologies. VoxAngeles is free to download and use under a CC-BY-NC 4.0 license.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.19521",
        "abstract url": "https://arxiv.org/abs/2403.19521",
        "title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like \"The capital of France is,\" task-specific attention heads extract the topic entity, such as \"France,\" from the context and pass it to subsequent MLPs to recall the required answer such as \"Paris.\" We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions. We mitigate this suppression by leveraging our interpretation to improve factual recall performance. Our interpretations have been evaluated across various language models, from the GPT-2 families to 1.3B OPT, and across tasks covering different domains of factual knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19534",
        "abstract url": "https://arxiv.org/abs/2403.19534",
        "title": "Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Prior studies have made significant progress in image inpainting guided by either text or subject image. However, the research on editing with their combined guidance is still in the early stages. To tackle this challenge, we present LAR-Gen, a novel approach for image inpainting that enables seamless inpainting of masked scene images, incorporating both the textual prompts and specified subjects. Our approach adopts a coarse-to-fine manner to ensure subject identity preservation and local semantic coherence. The process involves (i) Locate: concatenating the noise with masked scene image to achieve precise regional editing, (ii) Assign: employing decoupled cross-attention mechanism to accommodate multi-modal guidance, and (iii) Refine: using a novel RefineNet to supplement subject details. Additionally, to address the issue of scarce training data, we introduce a novel data construction pipeline. This pipeline extracts substantial pairs of data consisting of local text prompts and corresponding visual instances from a vast image dataset, leveraging publicly available large models. Extensive experiments and varied application scenarios demonstrate the superiority of LAR-Gen in terms of both identity preservation and text semantic consistency. Project page can be found at \\url{https://ali-vilab.github.io/largen-page/}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "22 pages, 14 figures"
    },
    {
        "paper id": "2403.19554",
        "abstract url": "https://arxiv.org/abs/2403.19554",
        "title": "Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In video-based emotion recognition, audio and visual modalities are often expected to have a complementary relationship, which is widely explored using cross-attention. However, they may also exhibit weak complementary relationships, resulting in poor representations of audio-visual features, thus degrading the performance of the system. To address this issue, we propose Dynamic Cross-Attention (DCA) that can dynamically select cross-attended or unattended features on the fly based on their strong or weak complementary relationship with each other, respectively. Specifically, a simple yet efficient gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit a strong complementary relationship, otherwise unattended features. We evaluate the performance of the proposed approach on the challenging RECOLA and Aff-Wild2 datasets. We also compare the proposed approach with other variants of cross-attention and show that the proposed model consistently improves the performance on both datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at IEEE ICME2024"
    },
    {
        "paper id": "2403.19559",
        "abstract url": "https://arxiv.org/abs/2403.19559",
        "title": "Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Hate speech detection models are only as good as the data they are trained on. Datasets sourced from social media suffer from systematic gaps and biases, leading to unreliable models with simplistic decision boundaries. Adversarial datasets, collected by exploiting model weaknesses, promise to fix this problem. However, adversarial data collection can be slow and costly, and individual annotators have limited creativity. In this paper, we introduce GAHD, a new German Adversarial Hate speech Dataset comprising ca.\\ 11k examples. During data collection, we explore new strategies for supporting annotators, to create more diverse adversarial examples more efficiently and provide a manual analysis of annotator disagreements for each strategy. Our experiments show that the resulting dataset is challenging even for state-of-the-art hate speech detection models, and that training on GAHD clearly improves model robustness. Further, we find that mixing multiple support strategies is most advantageous. We make GAHD publicly available at https://github.com/jagol/gahd.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at NAACL 2024 (main conference)"
    },
    {
        "paper id": "2403.19579",
        "abstract url": "https://arxiv.org/abs/2403.19579",
        "title": "The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The pursuit of learning robust representations without human supervision is a longstanding challenge. The recent advancements in self-supervised contrastive learning approaches have demonstrated high performance across various representation learning challenges. However, current methods depend on the random transformation of training examples, resulting in some cases of unrepresentative positive pairs that can have a large impact on learning. This limitation not only impedes the convergence of the learning process but the robustness of the learnt representation as well as requiring larger batch sizes to improve robustness to such bad batches. This paper attempts to alleviate the influence of false positive and false negative pairs by employing pairwise similarity calculations through the Fr\u00e9chet ResNet Distance (FRD), thereby obtaining robust representations from unlabelled data. The effectiveness of the proposed method is substantiated by empirical results, where a linear classifier trained on self-supervised contrastive representations achieved an impressive 87.74\\% top-1 accuracy on STL10 and 99.31\\% on the Flower102 dataset. These results emphasize the potential of the proposed approach in pushing the boundaries of the state-of-the-art in self-supervised contrastive learning, particularly for image classification tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 Pages, 4 figures, IEEE WCCI 2024 Conference"
    },
    {
        "paper id": "2403.19584",
        "abstract url": "https://arxiv.org/abs/2403.19584",
        "title": "Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval.Traditional methods typically employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or retrieval, which identifying locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task. This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database. It then uniquely combines query results with images itself, forming elaborate prompts customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19588",
        "abstract url": "https://arxiv.org/abs/2403.19588",
        "title": "DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation. Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs. Our code is available at https://github.com/naver-ai/rdnet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code at https://github.com/naver-ai/rdnet"
    },
    {
        "paper id": "2403.19596",
        "abstract url": "https://arxiv.org/abs/2403.19596",
        "title": "LocCa: Visual Pretraining with Location-aware Captioners",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image captioning has been shown as an effective pretraining method similar to contrastive pretraining. However, the incorporation of location-aware information into visual pretraining remains an area with limited research. In this paper, we propose a simple visual pretraining method with location-aware captioners (LocCa). LocCa uses a simple image captioner task interface, to teach a model to read out rich information, i.e. bounding box coordinates, and captions, conditioned on the image pixel input. Thanks to the multitask capabilities of an encoder-decoder architecture, we show that an image captioner can easily handle multiple tasks during pretraining. Our experiments demonstrate that LocCa outperforms standard captioners significantly on localization downstream tasks while maintaining comparable performance on holistic tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19611",
        "abstract url": "https://arxiv.org/abs/2403.19611",
        "title": "Nearest Neighbor Classication for Classical Image Upsampling",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Given a set of ordered pixel data in the form of an image, our goal is to perform upsampling on the data such that: the resulting resolution is improved by some factor, the final result passes the human test, having added new, believable, and realistic information and detail to the image, the time complexity for upscaling is relatively close to that of lossy upscaling implementations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2403.19634",
        "abstract url": "https://arxiv.org/abs/2403.19634",
        "title": "Asymmetric and trial-dependent modeling: the contribution of LIA to SdSV Challenge Task 2",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "The SdSv challenge Task 2 provided an opportunity to assess efficiency and robustness of modern text-independent speaker verification systems. But it also made it possible to test new approaches, capable of taking into account the main issues of this challenge (duration, language, ...). This paper describes the contributions of our laboratory to the speaker recognition field. These contributions highlight two other challenges in addition to short-duration and language: the mismatch between enrollment and test data and the one between subsets of the evaluation trial dataset. The proposed approaches experimentally show their relevance and efficiency on the SdSv evaluation, and could be of interest in many real-life applications.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "LIA system description for the Short Duration Speaker Verification (SdSv) challenge 2020 Task 2"
    },
    {
        "paper id": "2403.19638",
        "abstract url": "https://arxiv.org/abs/2403.19638",
        "title": "Siamese Vision Transformers are Scalable Audio-visual Learners",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Traditional audio-visual methods rely on independent audio and visual backbones, which is costly and not scalable. In this work, we investigate using an audio-visual siamese network (AVSiam) for efficient and scalable audio-visual pretraining. Our framework uses a single shared vision transformer backbone to process audio and visual inputs, improving its parameter efficiency, reducing the GPU memory footprint, and allowing us to scale our method to larger datasets and model sizes. We pretrain our model using a contrastive audio-visual matching objective with a multi-ratio random masking scheme, which enables our model to process larger audio-visual instance batches, helpful for contrastive learning. Unlike prior audio-visual methods, our method can robustly handle audio, visual, and audio-visual inputs with a single shared ViT backbone. Furthermore, despite using the shared backbone for both modalities, AVSiam achieves competitive or even better results than prior methods on AudioSet and VGGSound for audio-visual classification and retrieval. Our code is available at https://github.com/GenjiB/AVSiam",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19150",
        "abstract url": "https://arxiv.org/abs/2403.19150",
        "title": "Towards Understanding Dual BN In Hybrid Adversarial Training",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT). With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN and BN are used for adversarial and clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between adversarial and clean samples is not very large, which is counter-intuitive considering the significant influence of adversarial perturbation on the model accuracy. We further propose a two-task hypothesis which serves as the empirical foundation and a unified framework for Hybrid-AT improvement. We also investigate Dual BN in test-time and reveal that affine parameters characterize the robustness during inference. Overall, our work sheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its underlying justification.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted at TMLR"
    },
    {
        "paper id": "2403.19232",
        "abstract url": "https://arxiv.org/abs/2403.19232",
        "title": "AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search",
        "rating": 0.5,
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Training-free network architecture search (NAS) aims to discover high-performing networks with zero-cost proxies, capturing network characteristics related to the final performance. However, network rankings estimated by previous training-free NAS methods have shown weak correlations with the performance. To address this issue, we propose AZ-NAS, a novel approach that leverages the ensemble of various zero-cost proxies to enhance the correlation between a predicted ranking of networks and the ground truth substantially in terms of the performance. To achieve this, we introduce four novel zero-cost proxies that are complementary to each other, analyzing distinct traits of architectures in the views of expressivity, progressivity, trainability, and complexity. The proxy scores can be obtained simultaneously within a single forward and backward pass, making an overall NAS process highly efficient. In order to integrate the rankings predicted by our proxies effectively, we introduce a non-linear ranking aggregation method that highlights the networks highly-ranked consistently across all the proxies. Experimental results conclusively demonstrate the efficacy and efficiency of AZ-NAS, outperforming state-of-the-art methods on standard benchmarks, all while maintaining a reasonable runtime cost.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.19245",
        "abstract url": "https://arxiv.org/abs/2403.19245",
        "title": "The use of ChatGPT in higher education: The advantages and disadvantages",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Higher education scholars are interested in an artificial intelligence (AI) technology called ChatGPT, which was developed by OpenAI. Whether ChatGPT can improve learning is still a topic of debate among experts. This concise overview of the literature examines the application of ChatGPT in higher education to comprehend and produce high-level instruction. By examining the essential literature, this study seeks to provide a thorough assessment of the advantages and disadvantages of utilizing ChatGPT in higher education settings. But it's crucial to consider both the positive and negative elements. For this rapid review, the researcher searched Google Scholar, Scopus, and others between January 2023 and July 2023 for prior research from various publications. These studies were examined. The study found that employing ChatGPT in higher education is beneficial for a number of reasons. It can provide individualized instruction, and prompt feedback, facilitate access to learning, and promote student interaction. These benefits could improve the learning environment and make it more fun for academics and students. The cons of ChatGPT are equally present. These problems include the inability to comprehend emotions, the lack of social interaction chances, technological limitations, and the dangers of depending too much on ChatGPT for higher education. Higher education should combine ChatGPT with other teaching techniques to provide students and lecturers with a comprehensive education. However, it is crucial to consider the positives, negatives, and moral issues before adopting ChatGPT in the classroom.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19314",
        "abstract url": "https://arxiv.org/abs/2403.19314",
        "title": "Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction",
        "rating": 0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Scene reconstruction from multi-view images is a fundamental problem in computer vision and graphics. Recent neural implicit surface reconstruction methods have achieved high-quality results; however, editing and manipulating the 3D geometry of reconstructed scenes remains challenging due to the absence of naturally decomposed object entities and complex object/background compositions. In this paper, we present Total-Decom, a novel method for decomposed 3D reconstruction with minimal human interaction. Our approach seamlessly integrates the Segment Anything Model (SAM) with hybrid implicit-explicit neural surface representations and a mesh-based region-growing technique for accurate 3D object decomposition. Total-Decom requires minimal human annotations while providing users with real-time control over the granularity and quality of decomposition. We extensively evaluate our method on benchmark datasets and demonstrate its potential for downstream applications, such as animation and scene editing. The code is available at \\href{https://github.com/CVMI-Lab/Total-Decom.git}{https://github.com/CVMI-Lab/Total-Decom.git}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 7 figures, accepted by CVPR 2024"
    },
    {
        "paper id": "2403.19339",
        "abstract url": "https://arxiv.org/abs/2403.19339",
        "title": "An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence. In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected. For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise counterfactual examples to complement standard binary labels as annotations for a dataset. Finally we discuss the challenges in future extensions of this work.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "4 pages, 2 figures, Submitted to IJCAI 2024 Demonstration Track"
    },
    {
        "paper id": "2403.19394",
        "abstract url": "https://arxiv.org/abs/2403.19394",
        "title": "Cycling on the Freeway: The Perilous State of Open Source Neuroscience Software",
        "rating": 0.5,
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Most scientists need software to perform their research (Barker et al., 2020; Carver et al., 2022; Hettrick, 2014; Hettrick et al., 2014; Switters and Osimo, 2019), and neuroscientists are no exception. Whether we work with reaction times, electrophysiological signals, or magnetic resonance imaging data, we rely on software to acquire, analyze, and statistically evaluate the raw data we obtain - or to generate such data if we work with simulations. In recent years there has been a shift toward relying on free, open-source scientific software (FOSSS) for neuroscience data analysis (Poldrack et al., 2019), in line with the broader open science movement in academia (McKiernan et al., 2016) and wider industry trends (Eghbal, 2016). Importantly, FOSSS is typically developed by working scientists (not professional software developers) which sets up a precarious situation given the nature of the typical academic workplace (wherein academics, especially in their early careers, are on short and fixed term contracts). In this paper, we will argue that the existing ecosystem of neuroscientific open source software is brittle, and discuss why and how the neuroscience community needs to come together to ensure a healthy growth of our software landscape to the benefit of all.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19405",
        "abstract url": "https://arxiv.org/abs/2403.19405",
        "title": "Tabular Learning: Encoding for Entity and Context Embeddings",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Examining the effect of different encoding techniques on entity and context embeddings, the goal of this work is to challenge commonly used Ordinal encoding for tabular learning. Applying different preprocessing methods and network architectures over several datasets resulted in a benchmark on how the encoders influence the learning outcome of the networks. By keeping the test, validation and training data consistent, results have shown that ordinal encoding is not the most suited encoder for categorical data in terms of preprocessing the data and thereafter, classifying the target variable correctly. A better outcome was achieved, encoding the features based on string similarities by computing a similarity matrix as input for the network. This is the case for both, entity and context embeddings, where the transformer architecture showed improved performance for Ordinal and Similarity encoding with regard to multi-label classification tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19412",
        "abstract url": "https://arxiv.org/abs/2403.19412",
        "title": "A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization",
        "rating": 0.5,
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Event cameras exhibit remarkable attributes such as high dynamic range, asynchronicity, and low latency, making them highly suitable for vision tasks that involve high-speed motion in challenging lighting conditions. These cameras implicitly capture movement and depth information in events, making them appealing sensors for Camera Pose Relocalization (CPR) tasks. Nevertheless, existing CPR networks based on events neglect the pivotal fine-grained temporal information in events, resulting in unsatisfactory performance. Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices. In this paper, we introduce PEPNet, a simple and effective point-based network designed to regress six degrees of freedom (6-DOFs) event camera poses. We rethink the relationship between the event camera and CPR tasks, leveraging the raw Point Cloud directly as network input to harness the high-temporal resolution and inherent sparsity of events. PEPNet is adept at abstracting the spatial and implicit temporal features through hierarchical structure and explicit temporal features by Attentive Bi-directional Long Short-Term Memory (A-Bi-LSTM). By employing a carefully crafted lightweight design, PEPNet delivers state-of-the-art (SOTA) performance on both indoor and outdoor datasets with meager computational resources. Specifically, PEPNet attains a significant 38% and 33% performance improvement on the random split IJRR and M3ED datasets, respectively. Moreover, the lightweight design version PEPNet$_{tiny}$ accomplishes results comparable to the SOTA while employing a mere 0.5% of the parameters.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2403.19417",
        "abstract url": "https://arxiv.org/abs/2403.19417",
        "title": "OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion",
        "rating": 0.5,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities. In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment. The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task objectives. Within the CTC framework, we employ Large Language Models (LLMs) to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task. OAKINK2 datasets and models are available at https://oakink.net/v2.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "To be appeared in CVPR 2024. 26 pages"
    },
    {
        "paper id": "2403.19418",
        "abstract url": "https://arxiv.org/abs/2403.19418",
        "title": "Constants of Motion for Conserved and Non-conserved Dynamics",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion. This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators. For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped. The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\\em total} system (i.e., oscillator plus dissipative environment). For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions. In addition, a constant is identified which generalizes angular momentum for all ratios of the frequencies. The approach presented here can produce {\\em multiple} constants of motion from a {\\em single}, generic data set.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 5 figures"
    },
    {
        "paper id": "2403.19419",
        "abstract url": "https://arxiv.org/abs/2403.19419",
        "title": "Fairness in Ranking: Robustness through Randomization without the Protected Attribute",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "There has been great interest in fairness in machine learning, especially in relation to classification problems. In ranking-related problems, such as in online advertising, recommender systems, and HR automation, much work on fairness remains to be done. Two complications arise: first, the protected attribute may not be available in many applications. Second, there are multiple measures of fairness of rankings, and optimization-based methods utilizing a single measure of fairness of rankings may produce rankings that are unfair with respect to other measures. In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute. In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19421",
        "abstract url": "https://arxiv.org/abs/2403.19421",
        "title": "Scaling up ridge regression for brain encoding in a massive individual fMRI dataset",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Brain encoding with neuroimaging data is an established analysis aimed at predicting human brain activity directly from complex stimuli features such as movie frames. Typically, these features are the latent space representation from an artificial neural network, and the stimuli are image, audio, or text inputs. Ridge regression is a popular prediction model for brain encoding due to its good out-of-sample generalization performance. However, training a ridge regression model can be highly time-consuming when dealing with large-scale deep functional magnetic resonance imaging (fMRI) datasets that include many space-time samples of brain activity. This paper evaluates different parallelization techniques to reduce the training time of brain encoding with ridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI resource currently available. With multi-threading, our results show that the Intel Math Kernel Library (MKL) significantly outperforms the OpenBLAS library, being 1.9 times faster using 32 threads on a single machine. We then evaluated the Dask multi-CPU implementation of ridge regression readily available in scikit-learn (MultiOutput), and we proposed a new \"batch\" version of Dask parallelization, motivated by a time complexity analysis. In line with our theoretical analysis, MultiOutput parallelization was found to be impractical, i.e., slower than multi-threading on a single machine. In contrast, the Batch-MultiOutput regression scaled well across compute nodes and threads, providing speed-ups of up to 33 times with 8 compute nodes and 32 threads compared to a single-threaded scikit-learn execution. Batch parallelization using Dask thus emerges as a scalable approach for brain encoding with ridge regression on high-performance computing systems using scikit-learn and large fMRI datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19423",
        "abstract url": "https://arxiv.org/abs/2403.19423",
        "title": "Echo-chambers and Idea Labs: Communication Styles on Twitter",
        "rating": 0.5,
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "This paper investigates the communication styles and structures of Twitter (X) communities within the vaccination context. While mainstream research primarily focuses on the echo-chamber phenomenon, wherein certain ideas are reinforced and participants are isolated from opposing opinions, this study reveals the presence of diverse communication styles across various communities. In addition to the communities exhibiting echo-chamber behavior, this research uncovers communities with distinct communication patterns. By shedding light on the nuanced nature of communication within social networks, this study emphasizes the significance of understanding the diversity of perspectives within online communities.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19473",
        "abstract url": "https://arxiv.org/abs/2403.19473",
        "title": "Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM",
        "rating": 0.5,
        "keywords": [
            [
                "RGB-D"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Implicit neural representation (INR), in combination with geometric rendering, has recently been employed in real-time dense RGB-D SLAM. Despite active research endeavors being made, there lacks a unified protocol for fair evaluation, impeding the evolution of this area. In this work, we establish, to our knowledge, the first open-source benchmark framework to evaluate the performance of a wide spectrum of commonly used INRs and rendering functions for mapping and localization. The goal of our benchmark is to 1) gain an intuition of how different INRs and rendering functions impact mapping and localization and 2) establish a unified evaluation protocol w.r.t. the design choices that may impact the mapping and localization. With the framework, we conduct a large suite of experiments, offering various insights in choosing the INRs and geometric rendering functions: for example, the dense feature grid outperforms other INRs (e.g. tri-plane and hash grid), even when geometric and color features are jointly encoded for memory efficiency. To extend the findings into the practical scenario, a hybrid encoding strategy is proposed to bring the best of the accuracy and completion from the grid-based and decomposition-based INRs. We further propose explicit hybrid encoding for high-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts the premise on robustness and computation efficiency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.19480",
        "abstract url": "https://arxiv.org/abs/2403.19480",
        "title": "$H$-Consistency Guarantees for Regression",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a detailed study of $H$-consistency bounds for regression. We first present new theorems that generalize the tools previously given to establish $H$-consistency bounds. This generalization proves essential for analyzing $H$-consistency bounds specific to regression. Next, we prove a series of novel $H$-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set. This includes positive results for the Huber loss, all $\\ell_p$ losses, $p \\geq 1$, the squared $\u03b5$-insensitive loss, as well as a negative result for the $\u03b5$-insensitive loss used in squared Support Vector Regression (SVR). We further leverage our analysis of $H$-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5). This readily establishes novel algorithms for adversarial regression, for which we report favorable experimental results in Section 6.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19490",
        "abstract url": "https://arxiv.org/abs/2403.19490",
        "title": "Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment",
        "rating": 0.5,
        "keywords": [
            [
                "Reinforcement"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Structural model pruning is a prominent approach used for reducing the computational cost of Convolutional Neural Networks (CNNs) before their deployment on resource-constrained devices. Yet, the majority of proposed ideas require a pretrained model before pruning, which is costly to secure. In this paper, we propose a novel structural pruning approach to jointly learn the weights and structurally prune architectures of CNN models. The core element of our method is a Reinforcement Learning (RL) agent whose actions determine the pruning ratios of the CNN model's layers, and the resulting model's accuracy serves as its reward. We conduct the joint training and pruning by iteratively training the model's weights and the agent's policy, and we regularize the model's weights to align with the selected structure by the agent. The evolving model's weights result in a dynamic reward function for the agent, which prevents using prominent episodic RL methods with stationary environment assumption for our purpose. We address this challenge by designing a mechanism to model the complex changing dynamics of the reward function and provide a representation of it to the RL agent. To do so, we take a learnable embedding for each training epoch and employ a recurrent model to calculate a representation of the changing environment. We train the recurrent model and embeddings using a decoder model to reconstruct observed rewards. Such a design empowers our agent to effectively leverage episodic observations along with the environment representations to learn a proper policy to determine performant sub-networks of the CNN model. Our extensive experiments on CIFAR-10 and ImageNet using ResNets and MobileNets demonstrate the effectiveness of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024"
    },
    {
        "paper id": "2403.19494",
        "abstract url": "https://arxiv.org/abs/2403.19494",
        "title": "Regression with Multi-Expert Deferral",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set-specific. Our framework is versatile, applying to multiple experts, accommodating any bounded regression losses, addressing both instance-dependent and label-dependent costs, and supporting both single-stage and two-stage methods. A by-product is that our single-stage formulation includes the recent regression with abstention framework (Cheng et al., 2023) as a special case, where only a single expert, the squared loss and a label-independent cost are considered. Minimizing our proposed loss functions directly leads to novel algorithms for regression with deferral. We report the results of extensive experiments showing the effectiveness of our proposed algorithms.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19500",
        "abstract url": "https://arxiv.org/abs/2403.19500",
        "title": "Tensor Network-Constrained Kernel Machines as Gaussian Processes",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19507",
        "abstract url": "https://arxiv.org/abs/2403.19507",
        "title": "SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model's performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage. We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information. Our method is rigorously tested on multiple PDE datasets, including the Navier-Stokes equations and shallow water equations, showcasing the advantages of our proposed approach over conventional U-Nets with a comparable parameter budget. We further demonstrate that increasing the number of waves in SineNet while maintaining the same number of parameters leads to a monotonically improved performance. The results highlight the effectiveness of SineNet and the potential of our approach in advancing the state-of-the-art in neural PDE solver design. Our code is available as part of AIRS (https://github.com/divelab/AIRS).",
        "subjects": [
            "cs.LG"
        ],
        "comment": "The Twelfth International Conference on Learning Representations"
    },
    {
        "paper id": "2403.19517",
        "abstract url": "https://arxiv.org/abs/2403.19517",
        "title": "XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold",
        "rating": 0.5,
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We propose XScale-NVS for high-fidelity cross-scale novel view synthesis of real-world large-scale scenes. Existing representations based on explicit surface suffer from discretization resolution or UV distortion, while implicit volumetric representations lack scalability for large scenes due to the dispersed weight distribution and surface ambiguity. In light of the above challenges, we introduce hash featurized manifold, a novel hash-based featurization coupled with a deferred neural rendering framework. This approach fully unlocks the expressivity of the representation by explicitly concentrating the hash entries on the 2D manifold, thus effectively representing highly detailed contents independent of the discretization resolution. We also introduce a novel dataset, namely GigaNVS, to benchmark cross-scale, high-resolution novel view synthesis of realworld large-scale scenes. Our method significantly outperforms competing baselines on various real-world scenes, yielding an average LPIPS that is 40% lower than prior state-of-the-art on the challenging GigaNVS benchmark. Please see our project page at: xscalenvs.github.io.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024. Project page: xscalenvs.github.io/"
    },
    {
        "paper id": "2403.19522",
        "abstract url": "https://arxiv.org/abs/2403.19522",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model. We demonstrate the efficacy of Model Stock with fine-tuned models based upon pre-trained CLIP architectures, achieving remarkable performance on both ID and OOD tasks on the standard benchmarks, all while barely bringing extra computational demands. Our code and pre-trained models are available at https://github.com/naver-ai/model-stock.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Code at https://github.com/naver-ai/model-stock"
    },
    {
        "paper id": "2403.19539",
        "abstract url": "https://arxiv.org/abs/2403.19539",
        "title": "De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts",
        "rating": 0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Data-Free Knowledge Distillation (DFKD) is a promising task to train high-performance small models to enhance actual deployment without relying on the original training data. Existing methods commonly avoid relying on private data by utilizing synthetic or sampled data. However, a long-overlooked issue is that the severe distribution shifts between their substitution and original data, which manifests as huge differences in the quality of images and class proportions. The harmful shifts are essentially the confounder that significantly causes performance bottlenecks. To tackle the issue, this paper proposes a novel perspective with causal inference to disentangle the student models from the impact of such shifts. By designing a customized causal graph, we first reveal the causalities among the variables in the DFKD task. Subsequently, we propose a Knowledge Distillation Causal Intervention (KDCI) framework based on the backdoor adjustment to de-confound the confounder. KDCI can be flexibly combined with most existing state-of-the-art baselines. Experiments in combination with six representative DFKD methods demonstrate the effectiveness of our KDCI, which can obviously help existing methods under almost all settings, \\textit{e.g.}, improving the baseline by up to 15.54\\% accuracy on the CIFAR-100 dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR24"
    },
    {
        "paper id": "2403.19546",
        "abstract url": "https://arxiv.org/abs/2403.19546",
        "title": "Croissant: A Metadata Format for ML-Ready Datasets",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Preprint. Contributors listed in alphabetical order"
    },
    {
        "paper id": "2403.19591",
        "abstract url": "https://arxiv.org/abs/2403.19591",
        "title": "Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alternatives. Code is available at https:// github.com/PingchengDong/GQA-LUT.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "61st ACM/IEEE Design Automation Conference (DAC) 2024"
    },
    {
        "paper id": "2403.19625",
        "abstract url": "https://arxiv.org/abs/2403.19625",
        "title": "Top-$k$ Classification and Cardinality-Aware Prediction",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a detailed study of top-$k$ classification, the task of predicting the $k$ most probable classes for an input, extending beyond single-class prediction. We demonstrate that several prevalent surrogate loss functions in multi-class classification, such as comp-sum and constrained losses, are supported by $H$-consistency bounds with respect to the top-$k$ loss. These bounds guarantee consistency in relation to the hypothesis set $H$, providing stronger guarantees than Bayes-consistency due to their non-asymptotic and hypothesis-set specific nature. To address the trade-off between accuracy and cardinality $k$, we further introduce cardinality-aware loss functions through instance-dependent cost-sensitive learning. For these functions, we derive cost-sensitive comp-sum and constrained surrogate losses, establishing their $H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to new cardinality-aware algorithms for top-$k$ classification. We report the results of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN datasets demonstrating the effectiveness and benefit of these algorithms.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19629",
        "abstract url": "https://arxiv.org/abs/2403.19629",
        "title": "Metric Learning from Limited Pairwise Preference Comparisons",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric can be jointly identified. We present a divide-and-conquer approach that achieves this, and provide theoretical recovery guarantees and empirical validation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19647",
        "abstract url": "https://arxiv.org/abs/2403.19647",
        "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Code and data at https://github.com/saprmarks/feature-circuits. Demonstration at https://feature-circuits.xyz"
    },
    {
        "paper id": "2403.19159",
        "abstract url": "https://arxiv.org/abs/2403.19159",
        "title": "Disentangling Length from Quality in Direct Preference Optimization",
        "rating": 0,
        "keywords": [
            [
                "Reinforcement"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstrapping. We then develop a principled but simple regularization strategy that prevents length exploitation, while still maintaining improvements in model quality. We demonstrate these effects across datasets on summarization and dialogue, where we achieve up to 20\\% improvement in win rates when controlling for length, despite the GPT4 judge's well-known verbosity bias.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19160",
        "abstract url": "https://arxiv.org/abs/2403.19160",
        "title": "Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Neural rendering techniques have significantly advanced 3D human body modeling. However, previous approaches often overlook dynamics induced by factors such as motion inertia, leading to challenges in scenarios like abrupt stops after rotation, where the pose remains static while the appearance changes. This limitation arises from reliance on a single pose as conditional input, resulting in ambiguity in mapping one pose to multiple appearances. In this study, we elucidate that variations in human appearance depend not only on the current frame's pose condition but also on past pose states. Therefore, we introduce Dyco, a novel method utilizing the delta pose sequence representation for non-rigid deformations and canonical space to effectively model temporal appearance variations. To prevent a decrease in the model's generalization ability to novel poses, we further propose low-dimensional global context to reduce unnecessary inter-body part dependencies and a quantization operation to mitigate overfitting of the delta pose sequence by the model. To validate the effectiveness of our approach, we collected a novel dataset named I3D-Human, with a focus on capturing temporal changes in clothing appearance under approximate poses. Through extensive experiments on both I3D-Human and existing datasets, our approach demonstrates superior qualitative and quantitative performance. In addition, our inertia-aware 3D human method can unprecedentedly simulate appearance changes caused by inertia at different velocities.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19164",
        "abstract url": "https://arxiv.org/abs/2403.19164",
        "title": "RecDiffusion: Rectangling for Image Stitching with Diffusion Models",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel diffusion-based learning framework, \\textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched image's irregular borders to a geometrically corrected intermediary. Followed by Content Diffusion Models (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks. Code is released at https://github.com/lhaippp/RecDiffusion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19254",
        "abstract url": "https://arxiv.org/abs/2403.19254",
        "title": "Imperceptible Protection against Style Imitation from Diffusion Models",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent progress in diffusion models has profoundly enhanced the fidelity of image generation. However, this has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality. Recognizing the importance of maintaining this, we develop a visually improved protection method that preserves its protection capability. To this end, we create a perceptual map to identify areas most sensitive to human eyes. We then adjust the protection intensity guided by an instance-aware refinement. We also integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19259",
        "abstract url": "https://arxiv.org/abs/2403.19259",
        "title": "J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution",
        "rating": 0,
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We also constructed an experimental model and clarified the challenges in multimodal reference resolution tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING 2024"
    },
    {
        "paper id": "2403.19279",
        "abstract url": "https://arxiv.org/abs/2403.19279",
        "title": "Fine-Tuning Language Models with Reward Learning on Policy",
        "rating": 0,
        "keywords": [
            [
                "Reinforcement"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs. Extensive experiments on three benchmark datasets show that RLP consistently outperforms the state-of-the-art. Our code is available at \\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NAACL2024 Main Track Long Paper"
    },
    {
        "paper id": "2403.19326",
        "abstract url": "https://arxiv.org/abs/2403.19326",
        "title": "MedBN: Robust Test-Time Adaptation against Malicious Test Samples",
        "rating": 0.0,
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.LG"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Test-time adaptation (TTA) has emerged as a promising solution to address performance decay due to unforeseen distribution shifts between training and test data. While recent TTA methods excel in adapting to test data variations, such adaptability exposes a model to vulnerability against malicious examples, an aspect that has received limited attention. Previous studies have uncovered security vulnerabilities within TTA even when a small proportion of the test batch is maliciously manipulated. In response to the emerging threat, we propose median batch normalization (MedBN), leveraging the robustness of the median for statistics estimation within the batch normalization layer during test-time inference. Our method is algorithm-agnostic, thus allowing seamless integration with existing TTA frameworks. Our experimental results on benchmark datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently demonstrate that MedBN outperforms existing approaches in maintaining robust performance across different attack scenarios, encompassing both instant and cumulative attacks. Through extensive experiments, we show that our approach sustains the performance even in the absence of attacks, achieving a practical balance between robustness and performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to CVPR 2024"
    },
    {
        "paper id": "2403.19428",
        "abstract url": "https://arxiv.org/abs/2403.19428",
        "title": "Burst Super-Resolution with Diffusion Models for Improving Perceptual Quality",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image. In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry. Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity images can be reconstructed by diffusion models. However, prior SR methods using the diffusion model are not properly optimized for the burst SR task. Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR. In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the diffusion model. This reverse process from the intermediate step 1) skips diffusion steps for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures. Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics. Code: https://github.com/placerkyo/BSRD",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to IJCNN 2024 (International Joint Conference on Neural Networks)"
    },
    {
        "paper id": "2403.19443",
        "abstract url": "https://arxiv.org/abs/2403.19443",
        "title": "Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model",
        "rating": 0,
        "keywords": [
            [
                "Reinforcement"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of reward (easy), and those with small gaps (difficult). The first stage allows us to obtain a relatively optimal policy (LLM) model quickly, whereas the second stage refines LLM with online RLHF, thus mitigating the distribution shift issue associated with DPO. Experiments are conducted on two public alignment datasets, namely HH-RLHF and TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human evaluation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19454",
        "abstract url": "https://arxiv.org/abs/2403.19454",
        "title": "JDocQA: Japanese Document Question Answering Dataset for Generative Language Models",
        "rating": 0,
        "keywords": [
            [
                "Generative"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Document question answering is a task of question answering on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is known as a quite challenging task because it requires not only text understanding but also understanding of figures and tables, and hence visual question answering (VQA) methods are often examined in addition to textual approaches. We introduce Japanese Document Question Answering (JDocQA), a large-scale document-based QA dataset, essentially requiring both visual and textual information to answer questions, which comprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese. Each QA instance includes references to the document pages and bounding boxes for the answer clues. We incorporate multiple categories of questions and unanswerable questions from the document for realistic question-answering applications. We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs) and multimodal models. Incorporating unanswerable questions in finetuning may contribute to harnessing the so-called hallucination generation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LREC-COLING2024"
    },
    {
        "paper id": "2403.19456",
        "abstract url": "https://arxiv.org/abs/2403.19456",
        "title": "Break-for-Make: Modular Low-Rank Adaptations for Composable Content-Style Customization",
        "rating": 0,
        "keywords": [
            [
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Personalized generation paradigms empower designers to customize visual intellectual properties with the help of textual descriptions by tuning or adapting pre-trained text-to-image models on a few images. Recent works explore approaches for concurrently customizing both content and detailed visual style appearance. However, these existing approaches often generate images where the content and style are entangled. In this study, we reconsider the customization of content and style concepts from the perspective of parameter space construction. Unlike existing methods that utilize a shared parameter space for content and style, we propose a learning framework that separates the parameter space to facilitate individual learning of content and style, thereby enabling disentangled content and style. To achieve this goal, we introduce \"partly learnable projection\" (PLP) matrices to separate the original adapters into divided sub-parameter spaces. We propose \"break-for-make\" customization learning pipeline based on PLP, which is simple yet effective. We break the original adapters into \"up projection\" and \"down projection\", train content and style PLPs individually with the guidance of corresponding textual prompts in the separate adapters, and maintain generalization by employing a multi-correspondence projection learning strategy. Based on the adapters broken apart for separate training content and style, we then make the entity parameter space by reconstructing the content and style PLPs matrices, followed by fine-tuning the combined adapter to generate the target object with the desired appearance. Experiments on various styles, including textures, materials, and artistic style, show that our method outperforms state-of-the-art single/multiple concept learning pipelines in terms of content-style-prompt alignment.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19467",
        "abstract url": "https://arxiv.org/abs/2403.19467",
        "title": "Beyond Talking -- Generating Holistic 3D Human Dyadic Motion for Communication",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19514",
        "abstract url": "https://arxiv.org/abs/2403.19514",
        "title": "CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network",
        "rating": 0,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, incomplete multi-view clustering, which studies the challenging multi-view clustering problem on missing views, has received growing research interests. Although a series of methods have been proposed to address this issue, the following problems still exist: 1) Almost all of the existing methods are based on shallow models, which is difficult to obtain discriminative common representations. 2) These methods are generally sensitive to noise or outliers since the negative samples are treated equally as the important samples. In this paper, we propose a novel incomplete multi-view clustering network, called Cognitive Deep Incomplete Multi-view Clustering Network (CDIMC-net), to address these issues. Specifically, it captures the high-level features and local structure of each view by incorporating the view-specific deep encoders and graph embedding strategy into a framework. Moreover, based on the human cognition, i.e., learning from easy to hard, it introduces a self-paced strategy to select the most confident samples for model training, which can reduce the negative influence of outliers. Experimental results on several incomplete datasets show that CDIMC-net outperforms the state-of-the-art incomplete multi-view clustering methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IJCAI 2020"
    },
    {
        "paper id": "2403.19548",
        "abstract url": "https://arxiv.org/abs/2403.19548",
        "title": "WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models",
        "rating": 0,
        "keywords": [
            [
                "generative"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NAACL 2024 (Findings)"
    },
    {
        "paper id": "2403.19549",
        "abstract url": "https://arxiv.org/abs/2403.19549",
        "title": "GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM",
        "rating": 0,
        "keywords": [
            [
                "Point Cloud",
                "RGBD"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in RGB-only dense Simultaneous Localization and Mapping (SLAM) have predominantly utilized grid-based neural implicit encodings and/or struggle to efficiently realize global map and pose consistency. To this end, we propose an efficient RGB-only dense SLAM system using a flexible neural point cloud scene representation that adapts to keyframe poses and depth updates, without needing costly backpropagation. Another critical challenge of RGB-only SLAM is the lack of geometric priors. To alleviate this issue, with the aid of a monocular depth estimator, we introduce a novel DSPO layer for bundle adjustment which optimizes the pose and depth of keyframes along with the scale of the monocular depth. Finally, our system benefits from loop closure and online global bundle adjustment and performs either better or competitive to existing dense neural RGB SLAM methods in tracking, mapping and rendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source code will be made available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19580",
        "abstract url": "https://arxiv.org/abs/2403.19580",
        "title": "OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the current state of 3D object detection research, the severe scarcity of annotated 3D data, substantial disparities across different data modalities, and the absence of a unified architecture, have impeded the progress towards the goal of universality. In this paper, we propose \\textbf{OV-Uni3DETR}, a unified open-vocabulary 3D detector via cycle-modality propagation. Compared with existing 3D detectors, OV-Uni3DETR offers distinct advantages: 1) Open-vocabulary 3D detection: During training, it leverages various accessible data, especially extensive 2D detection images, to boost training diversity. During inference, it can detect both seen and unseen classes. 2) Modality unifying: It seamlessly accommodates input data from any given modality, effectively addressing scenarios involving disparate modalities or missing sensor information, thereby supporting test-time modality switching. 3) Scene unifying: It provides a unified multi-modal model architecture for diverse scenes collected by distinct sensors. Specifically, we propose the cycle-modality propagation, aimed at propagating knowledge bridging 2D and 3D modalities, to support the aforementioned functionalities. 2D semantic knowledge from large-vocabulary learning guides novel class discovery in the 3D domain, and 3D geometric knowledge provides localization supervision for 2D detection images. OV-Uni3DETR achieves the state-of-the-art performance on various scenarios, surpassing existing methods by more than 6\\% on average. Its performance using only RGB images is on par with or even surpasses that of previous point cloud based methods. Code and pre-trained models will be released later.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19593",
        "abstract url": "https://arxiv.org/abs/2403.19593",
        "title": "Frame by Familiar Frame: Understanding Replication in Video Diffusion Models",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Building on the momentum of image generation diffusion models, there is an increasing interest in video-based diffusion models. However, video generation poses greater challenges due to its higher-dimensional nature, the scarcity of training data, and the complex spatiotemporal relationships involved. Image generation models, due to their extensive data requirements, have already strained computational resources to their limits. There have been instances of these models reproducing elements from the training samples, leading to concerns and even legal disputes over sample replication. Video diffusion models, which operate with even more constrained datasets and are tasked with generating both spatial and temporal content, may be more prone to replicating samples from their training sets. Compounding the issue, these models are often evaluated using metrics that inadvertently reward replication. In our paper, we present a systematic investigation into the phenomenon of sample replication in video diffusion models. We scrutinize various recent diffusion models for video synthesis, assessing their tendency to replicate spatial and temporal content in both unconditional and conditional generation scenarios. Our study identifies strategies that are less likely to lead to replication. Furthermore, we propose new evaluation strategies that take replication into account, offering a more accurate measure of a model's ability to generate the original content.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19595",
        "abstract url": "https://arxiv.org/abs/2403.19595",
        "title": "Situation Awareness for Driver-Centric Driving Style Adaptation",
        "rating": 0,
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "There is evidence that the driving style of an autonomous vehicle is important to increase the acceptance and trust of the passengers. The driving situation has been found to have a significant influence on human driving behavior. However, current driving style models only partially incorporate driving environment information, limiting the alignment between an agent and the given situation. Therefore, we propose a situation-aware driving style model based on different visual feature encoders pretrained on fleet data, as well as driving behavior predictors, which are adapted to the driving style of a specific driver. Our experiments show that the proposed method outperforms static driving styles significantly and forms plausible situation clusters. Furthermore, we found that feature encoders pretrained on our dataset lead to more precise driving behavior modeling. In contrast, feature encoders pretrained supervised and unsupervised on different data sources lead to more specific situation clusters, which can be utilized to constrain and control the driving style adaptation for specific situations. Moreover, in a real-world setting, where driving style adaptation is happening iteratively, we found the MLP-based behavior predictors achieve good performance initially but suffer from catastrophic forgetting. In contrast, behavior predictors based on situationdependent statistics can learn iteratively from continuous data streams by design. Overall, our experiments show that important information for driving behavior prediction is contained within the visual feature encoder. The dataset is publicly available at huggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 6 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2403.19600",
        "abstract url": "https://arxiv.org/abs/2403.19600",
        "title": "Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model",
        "rating": 0,
        "keywords": [
            [
                "generative",
                "Diffusion",
                "Text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-to-image (T2I) generative models have recently emerged as a powerful tool, enabling the creation of photo-realistic images and giving rise to a multitude of applications. However, the effective integration of T2I models into fundamental image classification tasks remains an open question. A prevalent strategy to bolster image classification performance is through augmenting the training set with synthetic images generated by T2I models. In this study, we scrutinize the shortcomings of both current generative and conventional data augmentation techniques. Our analysis reveals that these methods struggle to produce images that are both faithful (in terms of foreground objects) and diverse (in terms of background contexts) for domain-specific concepts. To tackle this challenge, we introduce an innovative inter-class data augmentation method known as Diff-Mix (https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by performing image translations between classes. Our empirical results demonstrate that Diff-Mix achieves a better balance between faithfulness and diversity, leading to a marked improvement in performance across diverse image classification scenarios, including few-shot, conventional, and long-tail classifications for domain-specific datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19603",
        "abstract url": "https://arxiv.org/abs/2403.19603",
        "title": "Semantic Map-based Generation of Navigation Instructions",
        "rating": 0,
        "keywords": [
            [
                "Navigation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast scope for improvement. We release the code for data preparation and model training at https://github.com/chengzu-li/VLGen.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "5 pages, 2 figures, 3 tables (13 pages, 3 figures, 5 tables including references and appendices), accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.19612",
        "abstract url": "https://arxiv.org/abs/2403.19612",
        "title": "ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIST. Beyond these demonstrations, ILPO-Net's rotational invariance paves the way for other applications across multiple disciplines. Our code is publicly available at https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19615",
        "abstract url": "https://arxiv.org/abs/2403.19615",
        "title": "SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing",
        "rating": 0,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field's anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated. Our codes, data and models are available at https://github.com/zsy1987/SA-GS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://kevinsong729.github.io/project-pages/SA-GS/ Code: https://github.com/zsy1987/SA-GS"
    },
    {
        "paper id": "2403.19631",
        "abstract url": "https://arxiv.org/abs/2403.19631",
        "title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models",
        "rating": 0,
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\u00efve similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2403.19645",
        "abstract url": "https://arxiv.org/abs/2403.19645",
        "title": "GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models",
        "rating": 0,
        "keywords": [
            [
                "Generative",
                "Diffusion",
                "GAN",
                "Image Editing",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The rapid advancement in image generation models has predominantly been driven by diffusion models, which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual prompts. Despite their success, diffusion models encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched. In contrast, Generative Adversarial Networks (GANs) have been recognized for their success in disentangled edits through their interpretable latent spaces. We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained GAN models-representative of specific, controllable attributes-and transfers these directions into diffusion-based models. This novel approach not only maintains the generative quality and diversity that diffusion models are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://gantastic.github.io"
    },
    {
        "paper id": "2403.19651",
        "abstract url": "https://arxiv.org/abs/2403.19651",
        "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
        "rating": 0,
        "keywords": [
            [
                "synthesizing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via large multimodal models (LMMs) and large language models (LLMs). Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves comparable or better results on eight benchmarks of various image retrieval tasks than prior state-of-the-art (SOTA) methods. Remarkably, it outperforms previous SOTA but with a 50X smaller model size on multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2403.19653",
        "abstract url": "https://arxiv.org/abs/2403.19653",
        "title": "Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code available at https://github.com/k8xu/ImageAttribution"
    },
    {
        "paper id": "2403.19149",
        "abstract url": "https://arxiv.org/abs/2403.19149",
        "title": "Topological Cycle Graph Attention Network for Brain Functional Connectivity",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study, we introduce a novel Topological Cycle Graph Attention Network (CycGAT), designed to delineate a functional backbone within brain functional graph--key pathways essential for signal transmissio--from non-essential, redundant connections that form cycles around this core structure. We first introduce a cycle incidence matrix that establishes an independent cycle basis within a graph, mapping its relationship with edges. We propose a cycle graph convolution that leverages a cycle adjacency matrix, derived from the cycle incidence matrix, to specifically filter edge signals in a domain of cycles. Additionally, we strengthen the representation power of the cycle graph convolution by adding an attention mechanism, which is further augmented by the introduction of edge positional encodings in cycles, to enhance the topological awareness of CycGAT. We demonstrate CycGAT's localization through simulation and its efficacy on an ABCD study's fMRI data (n=8765), comparing it with baseline models. CycGAT outperforms these models, identifying a functional backbone with significantly fewer cycles, crucial for understanding neural circuits related to general intelligence. Our code will be released once accepted.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19163",
        "abstract url": "https://arxiv.org/abs/2403.19163",
        "title": "D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations",
        "rating": -0.5,
        "keywords": [
            [
                "architecture search"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit functions for single-instance signals through a Decoder-Only randomly projected Hypernetwork (D'OH). By directly changing the dimension of a latent code to approximate a target implicit neural architecture, we provide a natural way to vary the memory footprint of neural representations without the costly need for neural architecture search on a space of alternative low-rate structures.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "29 pages, 17 figures"
    },
    {
        "paper id": "2403.19211",
        "abstract url": "https://arxiv.org/abs/2403.19211",
        "title": "Dual-Personalizing Adapter for Federated Foundation Models",
        "rating": -0.5,
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-time distribution shifts. To address challenges in this new setting, we explore a simple yet effective solution to learn a comprehensive foundation model. Specifically, a dual-personalizing adapter architecture (FedDPA) is proposed, comprising a global adapter and a local adapter for addressing test-time distribution shifts and personalization, respectively. Additionally, we introduce an instance-wise dynamic weighting mechanism to optimize the balance between the global and local adapters, enhancing overall performance. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19220",
        "abstract url": "https://arxiv.org/abs/2403.19220",
        "title": "GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds",
        "rating": -0.5,
        "keywords": [
            [
                "3D",
                "voxel",
                "RGB-D"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Point clouds captured by different sensors such as RGB-D cameras and LiDAR possess non-negligible domain gaps. Most existing methods design different network architectures and train separately on point clouds from various sensors. Typically, point-based methods achieve outstanding performances on even-distributed dense point clouds from RGB-D cameras, while voxel-based methods are more efficient for large-range sparse LiDAR point clouds. In this paper, we propose geometry-to-voxel auxiliary learning to enable voxel representations to access point-level geometric information, which supports better generalisation of the voxel-based backbone with additional interpretations of multi-sensor point clouds. Specifically, we construct hierarchical geometry pools generated by a voxel-guided dynamic point network, which efficiently provide auxiliary fine-grained geometric information adapted to different stages of voxel features. We conduct experiments on joint multi-sensor datasets to demonstrate the effectiveness of GeoAuxNet. Enjoying elaborate geometric information, our method outperforms other models collectively trained on multi-sensor datasets, and achieve competitive results with the-state-of-art experts on each single dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2403.19243",
        "abstract url": "https://arxiv.org/abs/2403.19243",
        "title": "Sine Activated Low-Rank Matrices for Parameter Efficient Learning",
        "rating": -0.5,
        "keywords": [
            [
                "3D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling. This demonstrates the wide-ranging potential and efficiency of our proposed technique.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "The first two authors contributed equally"
    },
    {
        "paper id": "2403.19246",
        "abstract url": "https://arxiv.org/abs/2403.19246",
        "title": "MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph representation learning has rapidly emerged as a pivotal field of study. Despite its growing popularity, the majority of research has been confined to embedding single-layer graphs, which fall short in representing complex systems with multifaceted relationships. To bridge this gap, we introduce MPXGAT, an innovative attention-based deep learning model tailored to multiplex graph embedding. Leveraging the robustness of Graph Attention Networks (GATs), MPXGAT captures the structure of multiplex networks by harnessing both intra-layer and inter-layer connections. This exploitation facilitates accurate link prediction within and across the network's multiple layers. Our comprehensive experimental evaluation, conducted on various benchmark datasets, confirms that MPXGAT consistently outperforms state-of-the-art competing algorithms.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19289",
        "abstract url": "https://arxiv.org/abs/2403.19289",
        "title": "Graph Neural Networks for Treatment Effect Prediction",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random underlining the need for models that can generalize with limited labeled samples to reduce experimental risks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19303",
        "abstract url": "https://arxiv.org/abs/2403.19303",
        "title": "Developing generative AI chatbots conceptual framework for higher education",
        "rating": -0.5,
        "keywords": [
            [
                "generative",
                "synthesis"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "This research explores the quickly changing field of generative artificial intelligence (GAI) chatbots in higher education, an industry that is undergoing major technological changes. AI chatbots, such as ChatGPT, HuggingChat, and Google Bard, are becoming more and more common in a variety of sectors, including education. Their acceptance is still in its early phases, with a variety of prospects and obstacles. However, their potential in higher education is particularly noteworthy, providing lecturers and students with affordable, individualized support. Creating a comprehensive framework to aid the usage of generative AI chatbots in higher education institutions (HEIs) is the aim of this project. The Chukwuere Generative AI Chatbots Acceptance Model (CGAICAM) is the result of this study's synthesis of elements from well-known frameworks, including the TAM, UTAUT2, TPB, and others along with variables like optimism, innovativeness, discomfort, insecurity, and others. Using a research method that encompasses a comprehensive analysis of extant literature from databases such as IEEE, ACM, ScienceDirect, and Google Scholar, the study aims to comprehend the implications of AI Chatbots on higher education and pinpoint critical elements for their efficacious implementation. Peer-reviewed English-language publications published between 2020 and 2023 with a focus on the use of AI chatbots in higher education were the main focus of the search criteria. The results demonstrate how much AI chatbots can do to improve student engagement, streamline the educational process, and support administrative and research duties. But there are also clear difficulties, such as unfavorable student sentiments, doubts about the veracity of material produced by AI, and unease and nervousness with new technologies.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "28 pages"
    },
    {
        "paper id": "2403.19462",
        "abstract url": "https://arxiv.org/abs/2403.19462",
        "title": "Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization",
        "rating": -0.5,
        "keywords": [
            [
                "Reinforcement"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work studies a Reinforcement Learning (RL) problem in which we are given a set of trajectories collected with K baseline policies. Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space. The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space. We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound. Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary. We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19499",
        "abstract url": "https://arxiv.org/abs/2403.19499",
        "title": "Client-supervised Federated Learning: Towards One-model-for-all Personalization",
        "rating": -0.5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Personalized Federated Learning (PerFL) is a new machine learning paradigm that delivers personalized models for diverse clients under federated learning settings. Most PerFL methods require extra learning processes on a client to adapt a globally shared model to the client-specific personalized model using its own local data. However, the model adaptation process in PerFL is still an open challenge in the stage of model deployment and test time. This work tackles the challenge by proposing a novel federated learning framework to learn only one robust global model to achieve competitive performance to those personalized models on unseen/test clients in the FL system. Specifically, we design a new Client-Supervised Federated Learning (FedCS) to unravel clients' bias on instances' latent representations so that the global model can learn both client-specific and client-agnostic knowledge. Experimental study shows that the FedCS can learn a robust FL global model for the changing data distributions of unseen/test clients. The FedCS's global model can be directly deployed to the test clients while achieving comparable performance to other personalized FL methods that require model adaptation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19501",
        "abstract url": "https://arxiv.org/abs/2403.19501",
        "title": "RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method",
        "rating": -0.5,
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present RELI11D, a high-quality multimodal human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024, Project website: http://www.lidarhumanmotion.net/reli11d/"
    },
    {
        "paper id": "2403.19561",
        "abstract url": "https://arxiv.org/abs/2403.19561",
        "title": "Self-Improved Learning for Scalable Neural Combinatorial Optimization",
        "rating": -0.5,
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehensive experiments on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes in both uniform and real-world distributions demonstrate the superior scalability of our method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19572",
        "abstract url": "https://arxiv.org/abs/2403.19572",
        "title": "Swarm Characteristics Classification Using Neural Networks",
        "rating": -0.5,
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of 20 time steps, while also demonstrating graceful degradation down to 80% accuracy under 50% noise, as well as excellent scalability to swarm sizes from 10 to 100 agents. These capabilities are promising for real-time decision-making support in defense scenarios by rapidly inferring insights about swarm behavior.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19153",
        "abstract url": "https://arxiv.org/abs/2403.19153",
        "title": "Exploring Holistic HMI Design for Automated Vehicles: Insights from a Participatory Workshop to Bridge In-Vehicle and External Communication",
        "rating": -1,
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "Human-Machine Interfaces (HMIs) for automated vehicles (AVs) are typically divided into two categories: internal HMIs for interactions within the vehicle, and external HMIs for communication with other road users. In this work, we examine the prospects of bridging these two seemingly distinct domains. Through a participatory workshop with automotive user interface researchers and practitioners, we facilitated a critical exploration of holistic HMI design by having workshop participants collaboratively develop interaction scenarios involving AVs, in-vehicle users, and external road users. The discussion offers insights into the escalation of interface elements as an HMI design strategy, the direct interactions between different users, and an expanded understanding of holistic HMI design. This work reflects a collaborative effort to understand the practical aspects of this holistic design approach, offering new perspectives and encouraging further investigation into this underexplored aspect of automotive user interfaces.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19161",
        "abstract url": "https://arxiv.org/abs/2403.19161",
        "title": "Improving Vietnamese-English Medical Machine Translation",
        "rating": -1,
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Machine translation for Vietnamese-English in the medical domain is still an under-explored research area. In this paper, we introduce MedEV -- a high-quality Vietnamese-English parallel dataset constructed specifically for the medical domain, comprising approximately 360K sentence pairs. We conduct extensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo), state-of-the-art Vietnamese-English neural machine translation models and pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV dataset. Experimental results show that the best performance is achieved by fine-tuning \"vinai-translate\" for each translation direction. We publicly release our dataset to promote further research.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To appear in Proceedings of LREC-COLING 2024"
    },
    {
        "paper id": "2403.19177",
        "abstract url": "https://arxiv.org/abs/2403.19177",
        "title": "Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets",
        "rating": -1,
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Medical image segmentation presents the challenge of segmenting various-size targets, demanding the model to effectively capture both local and global information. Despite recent efforts using CNNs and ViTs to predict annotations of different scales, these approaches often struggle to effectively balance the detection of targets across varying sizes. Simply utilizing local information from CNNs and global relationships from ViTs without considering potential significant divergence in latent feature distributions may result in substantial information loss. To address this issue, in this paper, we will introduce a novel Stagger Network (SNet) and argues that a well-designed fusion structure can mitigate the divergence in latent feature distributions between CNNs and ViTs, thereby reducing information loss. Specifically, to emphasize both global dependencies and local focus, we design a Parallel Module to bridge the semantic gap. Meanwhile, we propose the Stagger Module, trying to fuse the selected features that are more semantically similar. An Information Recovery Module is further adopted to recover complementary information back to the network. As a key contribution, we theoretically analyze that the proposed parallel and stagger strategies would lead to less information loss, thus certifying the SNet's rationale. Experimental results clearly proved that the proposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse dataset where targets are in various sizes. Besides, it also demonstrates superiority on the ACDC and the MoNuSeg datasets where targets are with more consistent dimensions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19200",
        "abstract url": "https://arxiv.org/abs/2403.19200",
        "title": "Cell-Free MIMO Perceptive Mobile Networks: Cloud vs. Edge Processing",
        "rating": -1,
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "Perceptive mobile networks implement sensing and communication by reusing existing cellular infrastructure. Cell-free multiple-input multiple-output, thanks to the cooperation among distributed access points, supports the deployment of multistatic radar sensing, while providing high spectral efficiency for data communication services. To this end, the distributed access points communicate over fronthaul links with a central processing unit acting as a cloud processor. This work explores four different types of PMN uplink solutions based on Cell-free multiple-input multiple-output, in which the sensing and decoding functionalities are carried out at either cloud or edge. Accordingly, we investigate and compare joint cloud-based decoding and sensing (CDCS), hybrid cloud-based decoding and edge-based sensing (CDES), hybrid edge-based decoding and cloud-based sensing (EDCS) and edge-based decoding and sensing (EDES). In all cases, we target a unified design problem formulation whereby the fronthaul quantization of signals received in the training and data phases are jointly designed to maximize the achievable rate under sensing requirements and fronthaul capacity constraints. Via numerical results, the four implementation scenarios are compared as a function of the available fronthaul resources by highlighting the relative merits of edge- and cloud-based sensing and communications. This study provides guidelines on the optimal functional allocation in fronthaul-constrained networks implementing integrated sensing and communications.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "30 pages, 11 figures"
    },
    {
        "paper id": "2403.19203",
        "abstract url": "https://arxiv.org/abs/2403.19203",
        "title": "Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient Multi-Modal Imaging Skin Lesion Classification",
        "rating": -1,
        "keywords": [
            [
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "In this study, we introduce a multi-modal approach that efficiently integrates multi-scale clinical and dermoscopy features within a single network, thereby substantially reducing model parameters. The proposed method includes three novel fusion schemes. Firstly, unlike current methods that usually employ two individual models for for clinical and dermoscopy modalities, we verified that multimodal feature can be learned by sharing the parameters of encoder while leaving the individual modal-specific classifiers. Secondly, the shared cross-attention module can replace the individual one to efficiently interact between two modalities at multiple layers. Thirdly, different from current methods that equally optimize dermoscopy and clinical branches, inspired by prior knowledge that dermoscopy images play a more significant role than clinical images, we propose a novel biased loss. This loss guides the single-shared network to prioritize dermoscopy information over clinical information, implicitly learning a better joint feature representation for the modal-specific task. Extensive experiments on a well-recognized Seven-Point Checklist (SPC) dataset and a collected dataset demonstrate the effectiveness of our method on both CNN and Transformer structures. Furthermore, our method exhibits superiority in both accuracy and model parameters compared to currently advanced methods.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "This paper have submitted to Journal for review"
    },
    {
        "paper id": "2403.19244",
        "abstract url": "https://arxiv.org/abs/2403.19244",
        "title": "The role of chemo-mechanical modelling in the development of battery technology -- a perspective",
        "rating": -1,
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "In the race to reduce global CO2 emissions and achieve net-zero, chemomechanics must play a critical role in the technological development of current and next-generation batteries to improve their energy storage capabilities and their lifetime. Many degradation processes arise through mechanics via the development of diffusion-induced stress and volumetric strains within the various constituent materials in a battery. From particle cracking in lithium-ion batteries to lithium dendrite-based fracture of solid electrolytes in solid-state batteries, it is clear that significant barriers exist in the development of these energy storage systems, where chemomechanics plays a central part. To accelerate technological and scientific advances in this area, multi-scale and highly coupled multiphysics modelling must be carried out that includes mechanics-based phenomena. In this perspective article, we provide an introduction to chemomechanical modelling, the various physical problems that it addresses, and the issues that need to be resolved in order to expand its use within the field of battery technology.",
        "subjects": [
            "physics.chem-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19248",
        "abstract url": "https://arxiv.org/abs/2403.19248",
        "title": "Genos: General In-Network Unsupervised Intrusion Detection by Rule Extraction",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Anomaly-based network intrusion detection systems (A-NIDS) use unsupervised models to detect unforeseen attacks. However, existing A-NIDS solutions suffer from low throughput, lack of interpretability, and high maintenance costs. Recent in-network intelligence (INI) exploits programmable switches to offer line-rate deployment of NIDS. Nevertheless, current in-network NIDS are either model-specific or only apply to supervised models. In this paper, we propose Genos, a general in-network framework for unsupervised A-NIDS by rule extraction, which consists of a Model Compiler, a Model Interpreter, and a Model Debugger. Specifically, observing benign data are multimodal and usually located in multiple subspaces in the feature space, we utilize a divide-and-conquer approach for model-agnostic rule extraction. In the Model Compiler, we first propose a tree-based clustering algorithm to partition the feature space into subspaces, then design a decision boundary estimation mechanism to approximate the source model in each subspace. The Model Interpreter interprets predictions by important attributes to aid network operators in understanding the predictions. The Model Debugger conducts incremental updating to rectify errors by only fine-tuning rules on affected subspaces, thus reducing maintenance costs. We implement a prototype using physical hardware, and experiments demonstrate its superior performance of 100 Gbps throughput, great interpretability, and trivial updating overhead.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "accepted by IEEE International Conference on Computer Communications (INFOCOM 2024)"
    },
    {
        "paper id": "2403.19262",
        "abstract url": "https://arxiv.org/abs/2403.19262",
        "title": "Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning",
        "rating": -1,
        "keywords": [
            [
                "reinforcement"
            ]
        ],
        "abstract": "Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential. However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags. Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments. This paper proposes a novel self-supervised deep reinforcement learning approach that does not require labeled ground truth data. A reinforcement learning agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges. The agent learns, self-supervised, by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening. Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art supervised methods, overcoming data dependency and lack of generalizability limitations. This makes self-supervised deep reinforcement learning a promising solution for practical and scalable UWB-ranging error correction.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "11 pages, 8 figures and 4 tables"
    },
    {
        "paper id": "2403.19266",
        "abstract url": "https://arxiv.org/abs/2403.19266",
        "title": "On the Performance of Low-complexity Decoders of LDPC and Polar Codes",
        "rating": -1,
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Efficient decoding is crucial to high-throughput and low-power wireless communication scenarios. A theoretical analysis of the performance-complexity tradeoff toward low-complexity decoding is required for a better understanding of the fundamental limits in the above-mentioned scenarios. This study aims to explore the performance of decoders with complexity constraints. Specifically, we investigate the performance of LDPC codes with different numbers of belief-propagation iterations and the performance of polar codes with an SSC decoder. We found that the asymptotic error rates of both polar codes and LDPC codes are functions of complexity $T$ and code length $N$, in the form of $2^{-a2^{b\\frac{T}{N}}}$, where $a$ and $b$ are constants that depend on channel and coding schemes. Our analysis reveals the different performance-complexity tradeoffs for LDPC and polar codes. The results indicate that if one aims to further enhance the decoding efficiency for LDPC codes, the key lies in how to efficiently pass messages on the factor graph. In terms of decoding efficiency, polar codes asymptotically outperform $(J, K)$-regular LDPC codes with a code rate $R \\le 1-\\frac{J(J-1)}{2^J+(J-1)}$ in the low-complexity regime $(T \\le O(NlogN))$.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2012.13378 by other authors"
    },
    {
        "paper id": "2403.19283",
        "abstract url": "https://arxiv.org/abs/2403.19283",
        "title": "Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction",
        "rating": -1,
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs' potency across various tasks. However, applying LLMs to grammatical error correction (GEC) is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC. Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs. This indicates that for a syntax-oriented task like GEC, paying more attention to syntactic information can effectively boost LLMs' performance. Our code will be publicly available after the publication of this paper.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024 Main Conference"
    },
    {
        "paper id": "2403.19293",
        "abstract url": "https://arxiv.org/abs/2403.19293",
        "title": "Adaptive Preload Control of Cable-Driven Parallel Robots for Handling Task",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This paper presents a method for dynamic adjustment of cable preloads based on the actuation redundancy of \\acp{CDPR}, which allows increasing or decreasing the platform stiffness depending on task requirements. This is achieved by computing preload parameters with an extended nullspace formulation of the kinematics. The method facilitates the operator's ability to specify a defined preload within the operation space. The algorithms are implemented in a real-time environment, allowing for the use of optimization in hybrid position-force control. To validate the effectiveness of this approach, a simulation study is performed, and the obtained results are compared to existing methods. Furthermore, the method is investigated experimentally and compared with the conventional position-controlled operation of a cable robot. The results demonstrate the feasibility of adaptively adjusting cable preloads during platform motion and manipulation of additional objects.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to \"Annals of Scientific Society for Assembly, Handling and Industrial Robotics\" (MHI2024 conference/colloquium)"
    },
    {
        "paper id": "2403.19310",
        "abstract url": "https://arxiv.org/abs/2403.19310",
        "title": "MRNaB: Mixed Reality-based Robot Navigation Interface using Optical-see-through MR-beacon",
        "rating": -1,
        "keywords": [
            [
                "robotics",
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "Recent advancements in robotics have led to the development of numerous interfaces to enhance the intuitiveness of robot navigation. However, the reliance on traditional 2D displays imposes limitations on the simultaneous visualization of information. Mixed Reality (MR) technology addresses this issue by enhancing the dimensionality of information visualization, allowing users to perceive multiple pieces of information concurrently. This paper proposes Mixed reality-based robot navigation interface using an optical-see-through MR-beacon (MRNaB), a novel approach that incorporates an MR-beacon, situated atop the real-world environment, to function as a signal transmitter for robot navigation. This MR-beacon is designed to be persistent, eliminating the need for repeated navigation inputs for the same location. Our system is mainly constructed into four primary functions: \"Add\", \"Move\", \"Delete\", and \"Select\". These allow for the addition of a MR-beacon, location movement, its deletion, and the selection of MR-beacon for navigation purposes, respectively. The effectiveness of the proposed method was then validated through experiments by comparing it with the traditional 2D system. As the result, MRNaB was proven to increase the performance of the user when doing navigation to a certain place subjectively and objectively. For additional material, please check: https://mertcookimg.github.io/mrnab",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19316",
        "abstract url": "https://arxiv.org/abs/2403.19316",
        "title": "Hypergraph-based Multi-View Action Recognition using Event Cameras",
        "rating": -1,
        "keywords": [
            [
                "bio-inspired"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network. By treating segments as vertices and constructing hyperedges using rule-based and KNN-based strategies, a multi-view hypergraph neural network that captures relationships across viewpoint and temporal features is established. The vertex attention hypergraph propagation is also introduced for enhanced feature fusion. To prompt research in this area, we present the largest multi-view event-based action dataset $\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6 viewpoints, which surpasses existing datasets by over tenfold. Experimental results show that HyperMV significantly outperforms baselines in both cross-subject and cross-view scenarios, and also exceeds the state-of-the-arts in frame-based multi-view action recognition.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI 2024)"
    },
    {
        "paper id": "2403.19317",
        "abstract url": "https://arxiv.org/abs/2403.19317",
        "title": "Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization",
        "rating": -1,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Legal professionals face the challenge of managing an overwhelming volume of lengthy judgments, making automated legal case summarization crucial. However, prior approaches mainly focused on training and evaluating these models within the same jurisdiction. In this study, we explore the cross-jurisdictional generalizability of legal case summarization models.Specifically, we explore how to effectively summarize legal cases of a target jurisdiction where reference summaries are not available. In particular, we investigate whether supplementing models with unlabeled target jurisdiction corpus and extractive silver summaries obtained from unsupervised algorithms on target data enhances transfer performance. Our comprehensive study on three datasets from different jurisdictions highlights the role of pre-training in improving transfer performance. We shed light on the pivotal influence of jurisdictional similarity in selecting optimal source datasets for effective transfer. Furthermore, our findings underscore that incorporating unlabeled target data yields improvements in general pre-trained models, with additional gains when silver summaries are introduced. This augmentation is especially valuable when dealing with extractive datasets and scenarios featuring limited alignment between source and target jurisdictions. Our study provides key insights for developing adaptable legal case summarization systems, transcending jurisdictional boundaries.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024"
    },
    {
        "paper id": "2403.19319",
        "abstract url": "https://arxiv.org/abs/2403.19319",
        "title": "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "NeRF",
                "radiance fields"
            ],
            [
                "generative",
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present Mesh2NeRF, an approach to derive ground-truth radiance fields from textured meshes for 3D generation tasks. Many 3D generative approaches represent 3D scenes as radiance fields for training. Their ground-truth radiance fields are usually fitted from multi-view renderings from a large-scale synthetic 3D dataset, which often results in artifacts due to occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic solution to directly obtain ground-truth radiance fields from 3D meshes, characterizing the density field with an occupancy function featuring a defined surface thickness, and determining view-dependent color through a reflection function considering both the mesh and environment lighting. Mesh2NeRF extracts accurate radiance fields which provides direct supervision for training generative NeRFs and single scene representation. We validate the effectiveness of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in PSNR for view synthesis in single scene representation on the ABO dataset, a 0.69 PSNR enhancement in the single-view conditional generation of ShapeNet Cars, and notably improved mesh extraction from NeRF in the unconditional generation of Objaverse Mugs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://terencecyj.github.io/projects/Mesh2NeRF/ Video: https://youtu.be/oufv1N3f7iY"
    },
    {
        "paper id": "2403.19324",
        "abstract url": "https://arxiv.org/abs/2403.19324",
        "title": "Rapid nonlinear convex guidance via overparameterized monomial coordinates and fundamental solution expansions",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "This paper introduces a framework by which the nonlinear trajectory optimization problem is posed as a path-planning problem in a space liberated of dynamics. In this space, general state constraints for continuous and impulsive control problems are encoded as linear constraints on the native overparameterized variables. This framework is enabled by nonlinear expansion in the vicinity of a reference in terms of fundamental solutions and a minimal nonlinear basis of mixed monomials in problem initial conditions. The former can be computed using state transition tensors, differential algebra, or analytic approaches, and the latter is computed analytically. Nonlinear guidance schemes are proposed taking advantage of this framework, including a successive convex programming scheme for delta-V minimizing trajectory optimization. This work enables a stable and highly rapid nonlinear guidance implementation without the need for collocation or real-time integration.",
        "subjects": [
            "math.OC"
        ],
        "comment": "35 pages, 14 figures"
    },
    {
        "paper id": "2403.19358",
        "abstract url": "https://arxiv.org/abs/2403.19358",
        "title": "Risk prediction of pathological gambling on social media",
        "rating": -1,
        "keywords": [
            [
                "pathological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper addresses the problem of risk prediction on social media data, specifically focusing on the classification of Reddit users as having a pathological gambling disorder. To tackle this problem, this paper focuses on incorporating temporal and emotional features into the model. The preprocessing phase involves dealing with the time irregularity of posts by padding sequences. Two baseline architectures are used for preliminary evaluation: BERT classifier on concatenated posts per user and GRU with LSTM on sequential data. Experimental results demonstrate that the sequential models outperform the concatenation-based model. The results of the experiments conclude that the incorporation of a time decay layer (TD) and passing the emotion classification layer (EmoBERTa) through LSTM improves the performance significantly. Experiments concluded that the addition of a self-attention layer didn't significantly improve the performance of the model, however provided easily interpretable attention scores. The developed architecture with the inclusion of EmoBERTa and TD layers achieved a high F1 score, beating existing benchmarks on pathological gambling dataset. Future work may involve the early prediction of risk factors associated with pathological gambling disorder and testing models on other datasets. Overall, this research highlights the significance of the sequential processing of posts including temporal and emotional features to boost the predictive power, as well as adding an attention layer for interpretability.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19368",
        "abstract url": "https://arxiv.org/abs/2403.19368",
        "title": "Cloudy with a Chance of Cyberattacks: Dangling Resources Abuse on Cloud Platforms",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Recent works showed that it is feasible to hijack resources on cloud platforms. In such hijacks, attackers can take over released resources that belong to legitimate organizations. It was proposed that adversaries could abuse these resources to carry out attacks against customers of the hijacked services, e.g., through malware distribution. However, to date, no research has confirmed the existence of these attacks. We identify, for the first time, real-life hijacks of cloud resources. This yields a number of surprising and important insights. First, contrary to previous assumption that attackers primarily target IP addresses, our findings reveal that the type of resource is not the main consideration in a hijack. Attackers focus on hijacking records that allow them to determine the resource by entering freetext. The costs and overhead of hijacking such records are much lower than those of hijacking IP addresses, which are randomly selected from a large pool. Second, identifying hijacks poses a substantial challenge. Monitoring resource changes, e.g., changes in content, is insufficient, since such changes could also be legitimate. Retrospective analysis of digital assets to identify hijacks is also arduous due to the immense volume of data involved and the absence of indicators to search for. To address this challenge, we develop a novel approach that involves analyzing data from diverse sources to effectively differentiate between malicious and legitimate modifications. Our analysis has revealed 20,904 instances of hijacked resources on popular cloud platforms. While some hijacks are short-lived (up to 15 days), 1/3 persist for more than 65 days. We study how attackers abuse the hijacked resources and find that, in contrast to the threats considered in previous work, the majority of the abuse (75%) is blackhat search engine optimization.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "17 pages, 29 figures, to be published in NSDI'24: Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation"
    },
    {
        "paper id": "2403.19407",
        "abstract url": "https://arxiv.org/abs/2403.19407",
        "title": "Towards Temporally Consistent Referring Video Object Segmentation",
        "rating": -1,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Referring Video Object Segmentation (R-VOS) methods face challenges in maintaining consistent object segmentation due to temporal context variability and the presence of other visually similar objects. We propose an end-to-end R-VOS paradigm that explicitly models temporal instance consistency alongside the referring segmentation. Specifically, we introduce a novel hybrid memory that facilitates inter-frame collaboration for robust spatio-temporal matching and propagation. Features of frames with automatically generated high-quality reference masks are propagated to segment the remaining frames based on multi-granularity association to achieve temporally consistent R-VOS. Furthermore, we propose a new Mask Consistency Score (MCS) metric to evaluate the temporal consistency of video segmentation. Extensive experiments demonstrate that our approach enhances temporal consistency by a significant margin, leading to top-ranked performance on popular R-VOS benchmarks, i.e., Ref-YouTube-VOS (67.1%) and Ref-DAVIS17 (65.6%).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19414",
        "abstract url": "https://arxiv.org/abs/2403.19414",
        "title": "BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation",
        "rating": -1,
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value. Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities. While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation. To address these limitations, we propose the method Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's multi-step reasoning process and iteratively enhance this reasoning process. We employ a least-to-most prompting strategy to guide a large language model (LLM) in explicit reasoning, breaking down MDG into simpler sub-questions. These sub-questions build on answers from previous ones. Additionally, we also introduce two distinct bootstrapping techniques for prompting, which autonomously correct errors and facilitate the LLM's explicit reasoning. This approach eliminates the need for entity annotation and increases the transparency of the MDG process by explicitly generating the intermediate reasoning chain. The experimental findings on the two public datasets indicate that BP4ER outperforms state-of-the-art methods in terms of both objective and subjective evaluation metrics.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2403.19438",
        "abstract url": "https://arxiv.org/abs/2403.19438",
        "title": "SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control",
        "rating": -1,
        "keywords": [
            [
                "Generative"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Autonomous driving progress relies on large-scale annotated datasets. In this work, we explore the potential of generative models to produce vast quantities of freely-labeled data for autonomous driving applications and present SubjectDrive, the first model proven to scale generative data production in a way that could continuously improve autonomous driving applications. We investigate the impact of scaling up the quantity of generative data on the performance of downstream perception models and find that enhancing data diversity plays a crucial role in effectively scaling generative data production. Therefore, we have developed a novel model equipped with a subject control mechanism, which allows the generative model to leverage diverse external data sources for producing varied and useful data. Extensive evaluations confirm SubjectDrive's efficacy in generating scalable autonomous driving training data, marking a significant step toward revolutionizing data production methods in this field.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://subjectdrive.github.io/"
    },
    {
        "paper id": "2403.19441",
        "abstract url": "https://arxiv.org/abs/2403.19441",
        "title": "A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews",
        "rating": -1,
        "keywords": [
            [
                "diagnosis",
                "Clinical"
            ],
            [
                "cs.SD"
            ]
        ],
        "abstract": "Post-traumatic stress disorder (PTSD) is a mental disorder that can be developed after witnessing or experiencing extremely traumatic events. PTSD can affect anyone, regardless of ethnicity, or culture. An estimated one in every eleven people will experience PTSD during their lifetime. The Clinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians (PCL-C) interviews are gold standards in the diagnosis of PTSD. These questionnaires can be fooled by the subject's responses. This work proposes a deep learning-based approach that achieves state-of-the-art performances for PTSD detection using audio recordings during clinical interviews. Our approach is based on MFCC low-level features extracted from audio recordings of clinical interviews, followed by deep high-level learning using a Stochastic Transformer. Our proposed approach achieves state-of-the-art performances with an RMSE of 2.92 on the eDAIC dataset thanks to the stochastic depth, stochastic deep learning layers, and stochastic activation function.",
        "subjects": [
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19495",
        "abstract url": "https://arxiv.org/abs/2403.19495",
        "title": "CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS"
    },
    {
        "paper id": "2403.19497",
        "abstract url": "https://arxiv.org/abs/2403.19497",
        "title": "Surface-based parcellation and vertex-wise analysis of ultra high-resolution ex vivo 7 tesla MRI in neurodegenerative diseases",
        "rating": -1,
        "keywords": [
            [
                "MRI",
                "disease",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Magnetic resonance imaging (MRI) is the standard modality to understand human brain structure and function in vivo (antemortem). Decades of research in human neuroimaging has led to the widespread development of methods and tools to provide automated volume-based segmentations and surface-based parcellations which help localize brain functions to specialized anatomical regions. Recently ex vivo (postmortem) imaging of the brain has opened-up avenues to study brain structure at sub-millimeter ultra high-resolution revealing details not possible to observe with in vivo MRI. Unfortunately, there has been limited methodological development in ex vivo MRI primarily due to lack of datasets and limited centers with such imaging resources. Therefore, in this work, we present one-of-its-kind dataset of 82 ex vivo T2w whole brain hemispheres MRI at 0.3 mm isotropic resolution spanning Alzheimer's disease and related dementias. We adapted and developed a fast and easy-to-use automated surface-based pipeline to parcellate, for the first time, ultra high-resolution ex vivo brain tissue at the native subject space resolution using the Desikan-Killiany-Tourville (DKT) brain atlas. This allows us to perform vertex-wise analysis in the template space and thereby link morphometry measures with pathology measurements derived from histology. We will open-source our dataset docker container, Jupyter notebooks for ready-to-use out-of-the-box set of tools and command line options to advance ex vivo MRI clinical brain imaging research on the project webpage.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Under review at MICCAI 2024"
    },
    {
        "paper id": "2403.19510",
        "abstract url": "https://arxiv.org/abs/2403.19510",
        "title": "On the Robustness of LDP Protocols for Numerical Attributes under Data Poisoning Attacks",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "Recent studies reveal that local differential privacy (LDP) protocols are vulnerable to data poisoning attacks where an attacker can manipulate the final estimate on the server by leveraging the characteristics of LDP and sending carefully crafted data from a small fraction of controlled local clients. This vulnerability raises concerns regarding the robustness and reliability of LDP in hostile environments. In this paper, we conduct a systematic investigation of the robustness of state-of-the-art LDP protocols for numerical attributes, i.e., categorical frequency oracles (CFOs) with binning and consistency, and distribution reconstruction. We evaluate protocol robustness through an attack-driven approach and propose new metrics for cross-protocol attack gain measurement. The results indicate that Square Wave and CFO-based protocols in the Server setting are more robust against the attack compared to the CFO-based protocols in the User setting. Our evaluation also unfolds new relationships between LDP security and its inherent design choices. We found that the hash domain size in local-hashing-based LDP has a profound impact on protocol robustness beyond the well-known effect on utility. Further, we propose a zero-shot attack detection by leveraging the rich reconstructed distribution information. The experiment show that our detection significantly improves the existing methods and effectively identifies data manipulation in challenging scenarios.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19516",
        "abstract url": "https://arxiv.org/abs/2403.19516",
        "title": "Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "This paper studies the directed graph clustering problem through the lens of statistics, where we formulate clustering as estimating underlying communities in the directed stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed graph structure. In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed graph statistics: edge density and edge orientation. Building on this new formulation of directed clustering, we introduce two efficient and interpretable directed clustering algorithms, a spectral clustering algorithm and a semidefinite programming based clustering algorithm. We provide a theoretical upper bound on the number of misclustered vertices of the spectral clustering algorithm using tools from matrix perturbation theory. We compare, both quantitatively and qualitatively, our proposed algorithms with existing directed clustering methods on both synthetic and real-world data, thus providing further ground to our theoretical contributions.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19531",
        "abstract url": "https://arxiv.org/abs/2403.19531",
        "title": "SecGraph: Towards SGX-based Efficient and Confidentiality-Preserving Graph Search",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Graphs have more expressive power and are widely researched in various search demand scenarios, compared with traditional relational and XML models. Today, many graph search services have been deployed on a third-party server, which can alleviate users from the burdens of maintaining large-scale graphs and huge computation costs. Nevertheless, outsourcing graph search services to the third-party server may invade users' privacy. PeGraph was recently proposed to achieve the encrypted search over the social graph. The main idea of PeGraph is to maintain two data structures XSet and TSet motivated by the OXT technology to support encrypted conductive search. However, PeGraph still has some limitations. First, PeGraph suffers from high communication and computation costs in search operations. Second, PeGraph cannot support encrypted search over dynamic graphs. In this paper, we propose an SGX-based efficient and confidentiality-preserving graph search scheme SecGraph that can support insertion and deletion operations. We first design a new proxy-token generation method to reduce the communication cost. Then, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to reduce the computation cost. Finally, we design a new dynamic version of TSet named Twin-TSet to enable encrypted search over dynamic graphs. We have demonstrated the confidentiality preservation property of SecGraph through rigorous security analysis. Experiment results show that SecGraph yields up to 208x improvement in search time compared with PeGraph and the communication cost in PeGraph is up to 540x larger than that in SecGraph.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "This paper has been accepted by DASFAA 2024"
    },
    {
        "paper id": "2403.19545",
        "abstract url": "https://arxiv.org/abs/2403.19545",
        "title": "Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments",
        "rating": -1,
        "keywords": [
            [
                "robotics",
                "Robot"
            ]
        ],
        "abstract": "This study explores the integration of Lamarckian system into evolutionary robotics (ER), comparing it with the traditional Darwinian model across various environments. By adopting Lamarckian principles, where robots inherit learned traits, alongside Darwinian learning without inheritance, we investigate adaptation in dynamic settings. Our research, conducted in six distinct environmental setups, demonstrates that Lamarckian systems outperform Darwinian ones in adaptability and efficiency, particularly in challenging conditions. Our analysis highlights the critical role of the interplay between controller \\& morphological evolution and environment adaptation, with parent-offspring similarities and newborn \\&survivors before and after learning providing insights into the effectiveness of trait inheritance. Our findings suggest Lamarckian principles could significantly advance autonomous system design, highlighting the potential for more adaptable and robust robotic solutions in complex, real-world applications. These theoretical insights were validated using real physical robots, bridging the gap between simulation and practical application.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Nature. arXiv admin note: substantial text overlap with arXiv:2309.13099; text overlap with arXiv:2303.12594, arXiv:2309.14387"
    },
    {
        "paper id": "2403.19620",
        "abstract url": "https://arxiv.org/abs/2403.19620",
        "title": "Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models",
        "rating": -1,
        "keywords": [
            [
                "Generative"
            ]
        ],
        "abstract": "Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the images through local search based on an aesthetic measure. We evaluate the effectiveness of this approach by comparing the results produced by the automatic and collaborative interactive evolution. The results show that the proposed approach can generate highly attractive art images when the evolution is guided by collaborative human feedback.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Preprint. The Version of Record of this contribution is to be published in the proceedings of the 13th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2024"
    },
    {
        "paper id": "2403.19632",
        "abstract url": "https://arxiv.org/abs/2403.19632",
        "title": "GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present GauStudio, a novel modular framework for modeling 3D Gaussian Splatting (3DGS) to provide standardized, plug-and-play components for users to easily customize and implement a 3DGS pipeline. Supported by our framework, we propose a hybrid Gaussian representation with foreground and skyball background models. Experiments demonstrate this representation reduces artifacts in unbounded outdoor scenes and improves novel view synthesis. Finally, we propose Gaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse approach for high-fidelity mesh reconstruction from 3DGS inputs without fine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS approach enhance 3DGS modeling and rendering capabilities, enabling higher-quality novel view synthesis and surface reconstruction.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code: https://github.com/GAP-LAB-CUHK-SZ/gaustudio"
    },
    {
        "paper id": "2403.19646",
        "abstract url": "https://arxiv.org/abs/2403.19646",
        "title": "Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning",
        "rating": -1,
        "keywords": [
            [
                "Remote sensing",
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Monitoring changes in the Earth's surface is crucial for understanding natural processes and human impacts, necessitating precise and comprehensive interpretation methodologies. Remote sensing satellite imagery offers a unique perspective for monitoring these changes, leading to the emergence of remote sensing image change interpretation (RSICI) as a significant research focus. Current RSICI technology encompasses change detection and change captioning, each with its limitations in providing comprehensive interpretation. To address this, we propose an interactive Change-Agent which integrates a multi-level change interpretation (MCI) model as eyes and a large language model (LLM) as the brain. Our Change-Agent can follow user instructions to achieve comprehensive change interpretation and insightful analysis according to user instructions, such as change detection and change captioning, change object counting, change cause analysis, etc. Our proposed MCI model contains two branches of pixel-level change detection and semantic-level change captioning, in which multiple BI-temporal Iterative Interaction (BI3) layers utilize Local Perception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA) modules to enhance the model's discriminative feature representation capabilities. To train the MCI model, we build the LEVIR-MCI dataset with change masks and captions of bi-temporal images. Extensive experiments demonstrate the effectiveness of the proposed change interpretation model and highlight the promising potential of our Change-Agent in facilitating comprehensive and intelligent interpretation of surface changes. We will make our dataset and codebase of the change interpretation model and Change-Agent publicly available to facilitate future research at https://github.com/Chen-Yang-Liu/Change-Agent",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19652",
        "abstract url": "https://arxiv.org/abs/2403.19652",
        "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://sirui-xu.github.io/InterDreamer/"
    },
    {
        "paper id": "2403.19654",
        "abstract url": "https://arxiv.org/abs/2403.19654",
        "title": "RSMamba: Remote Sensing Image Classification with State Space Model",
        "rating": -1,
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Remote sensing image classification forms the foundation of various understanding tasks, serving a crucial function in remote sensing image interpretation. The recent advancements of Convolutional Neural Networks (CNNs) and Transformers have markedly enhanced classification accuracy. Nonetheless, remote sensing scene classification remains a significant challenge, especially given the complexity and diversity of remote sensing scenarios and the variability of spatiotemporal resolutions. The capacity for whole-image understanding can provide more precise semantic cues for scene discrimination. In this paper, we introduce RSMamba, a novel architecture for remote sensing image classification. RSMamba is based on the State Space Model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba. It integrates the advantages of both a global receptive field and linear modeling complexity. To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to two-dimensional image data, we propose a dynamic multi-path activation mechanism to augment Mamba's capacity to model non-causal data. Notably, RSMamba maintains the inherent modeling mechanism of the vanilla Mamba, yet exhibits superior performance across multiple remote sensing image classification datasets. This indicates that RSMamba holds significant potential to function as the backbone of future visual foundation models. The code will be available at \\url{https://github.com/KyanChen/RSMamba}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19655",
        "abstract url": "https://arxiv.org/abs/2403.19655",
        "title": "GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "voxel",
                "Radiance Fields"
            ],
            [
                "Generative",
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://gaussiancube.github.io/"
    },
    {
        "paper id": "2403.19165",
        "abstract url": "https://arxiv.org/abs/2403.19165",
        "title": "Evaluating Fair Feature Selection in Machine Learning for Healthcare",
        "rating": -1.5,
        "keywords": [
            [
                "health",
                "Healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk. We explore algorithmic fairness from the perspective of feature selection. Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups. To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups. We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error. We tested our approach on three publicly available healthcare datasets. On all three datasets, we observed improvements in fairness metrics coupled with a minimal degradation of balanced accuracy. Our approach addresses both distributive and procedural fairness within the fair machine learning context.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 7 figures"
    },
    {
        "paper id": "2403.19253",
        "abstract url": "https://arxiv.org/abs/2403.19253",
        "title": "Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning",
        "rating": -1.5,
        "keywords": [
            [
                "Reinforcement"
            ],
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Effective agent coordination is crucial in cooperative Multi-Agent Reinforcement Learning (MARL). While agent cooperation can be represented by graph structures, prevailing graph learning methods in MARL are limited. They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient graphs that foster redundant or detrimental information exchanges. Additionally, high computational demands for action-pair calculations in dense graphs impede scalability. To address these challenges, we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for MARL. The LTS-CG leverages agents' historical observations to calculate an agent-pair probability matrix, where a sparse graph is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty. The computational complexity of this procedure is only related to the number of agents. This graph learning process is further augmented by two innovative characteristics: Predict-Future, which enables agents to foresee upcoming observations, and Infer-Present, ensuring a thorough grasp of the environmental context from limited data. These features allow LTS-CG to construct temporal graphs from historical and real-time information, promoting knowledge exchange during policy learning and effective collaboration. Graph learning and agent training occur simultaneously in an end-to-end manner. Our demonstrated results on the StarCraft II benchmark underscore LTS-CG's superior performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19329",
        "abstract url": "https://arxiv.org/abs/2403.19329",
        "title": "Simulating Relational Event Histories -- Why and How",
        "rating": -1.5,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Many important social phenomena result from repeated interactions among individuals over time such as email exchanges in an organization, or face-to-face interactions in a classroom. Insights into the mechanisms underlying the dynamics of these interactions can be achieved through simulations of networks on a fine temporal granularity. In this paper, we present statistical frameworks to simulate relational event networks under dyadic and actor-oriented relational event models. These simulators have a broad applicability in temporal social network research such as model fit assessment, theory building, network intervention planning, making predictions, understanding the impact of network structures, to name a few. We show this in three extensive applications. First, it is shown why simulation-based techniques are crucial for relational event model assessment, for example to investigate how past events affect future interactions in the network. Second, we demonstrate how simulation techniques contribute to a better understanding of the longevity of network interventions. Third, we show how simulation techniques are important when building and extending theories about social phenomena such as understanding social identity dynamics using optimal distinctiveness theory.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19355",
        "abstract url": "https://arxiv.org/abs/2403.19355",
        "title": "Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients",
        "rating": -1.5,
        "keywords": [
            [
                "survival",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "For severely affected COVID-19 patients, it is crucial to identify high-risk patients and predict survival and need for intensive care (ICU). Most of the proposed models are not well reported making them less reproducible and prone to high risk of bias particularly in presence of imbalance data/class. In this study, the performances of nine machine and deep learning algorithms in combination with two widely used feature selection methods were investigated to predict last status representing mortality, ICU requirement, and ventilation days. Fivefold cross-validation was used for training and validation purposes. To minimize bias, the training and testing sets were split maintaining similar distributions. Only 10 out of 122 features were found to be useful in prediction modelling with Acute kidney injury during hospitalization feature being the most important one. The algorithms performances depend on feature numbers and data pre-processing techniques. LSTM performs the best in predicting last status and ICU requirement with 90%, 92%, 86% and 95% accuracy, sensitivity, specificity, and AUC respectively. DNN performs the best in predicting Ventilation days with 88% accuracy. Considering all the factors and limitations including absence of exact time point of clinical onset, LSTM with carefully selected features can accurately predict last status and ICU requirement. DNN performs the best in predicting Ventilation days. Appropriate machine learning algorithm with carefully selected features and balance data can accurately predict mortality, ICU requirement and ventilation support. Such model can be very useful in emergency and pandemic where prompt and precise",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Submitted to Biocybernetics and Biomedical Engineering, 22 March, 2024"
    },
    {
        "paper id": "2403.19444",
        "abstract url": "https://arxiv.org/abs/2403.19444",
        "title": "Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays",
        "rating": -1.5,
        "keywords": [
            [
                "medical",
                "Cancer",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex black-box deep learning models in real-world applications. Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use. To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process. On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection. Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 > 0.9), while also generating clinically relevant and more reliable explanations than existing techniques. We evaluate our approach against post-hoc image XAI techniques LIME and SHAP, as well as CXR-LLaVA, a recent textual XAI tool which operates in the context of question answering on chest X-rays.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 10 figures"
    },
    {
        "paper id": "2403.19570",
        "abstract url": "https://arxiv.org/abs/2403.19570",
        "title": "GrINd: Grid Interpolation Network for Scattered Observations",
        "rating": -1.5,
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting the evolution of spatiotemporal physical systems from sparse and scattered observational data poses a significant challenge in various scientific domains. Traditional methods rely on dense grid-structured data, limiting their applicability in scenarios with sparse observations. To address this challenge, we introduce GrINd (Grid Interpolation Network for Scattered Observations), a novel network architecture that leverages the high-performance of grid-based models by mapping scattered observations onto a high-resolution grid using a Fourier Interpolation Layer. In the high-resolution space, a NeuralPDE-class model predicts the system's state at future timepoints using differentiable ODE solvers and fully convolutional neural networks parametrizing the system's dynamics. We empirically evaluate GrINd on the DynaBench benchmark dataset, comprising six different physical systems observed at scattered locations, demonstrating its state-of-the-art performance compared to existing models. GrINd offers a promising approach for forecasting physical systems from sparse, scattered observational data, extending the applicability of deep learning methods to real-world scenarios with limited data availability.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19193",
        "abstract url": "https://arxiv.org/abs/2403.19193",
        "title": "Text Data-Centric Image Captioning with Interactive Prompts",
        "rating": -2,
        "keywords": [
            [
                "generative"
            ],
            [
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Supervised image captioning approaches have made great progress, but it is challenging to collect high-quality human-annotated image-text data. Recently, large-scale vision and language models (e.g., CLIP) and large-scale generative language models (e.g., GPT-2) have shown strong performances in various tasks, which also provide some new solutions for image captioning with web paired data, unpaired data or even text-only data. Among them, the mainstream solution is to project image embeddings into the text embedding space with the assistance of consistent representations between image-text pairs from the CLIP model. However, the current methods still face several challenges in adapting to the diversity of data configurations in a unified solution, accurately estimating image-text embedding bias, and correcting unsatisfactory prediction results in the inference stage. This paper proposes a new Text data-centric approach with Interactive Prompts for image Captioning, named TIPCap. 1) We consider four different settings which gradually reduce the dependence on paired data. 2) We construct a mapping module driven by multivariate Gaussian distribution to mitigate the modality gap, which is applicable to the above four different settings. 3) We propose a prompt interaction module that can incorporate optional prompt information before generating captions. Extensive experiments show that our TIPCap outperforms other weakly or unsupervised image captioning methods and achieves a new state-of-the-art performance on two widely used datasets, i.e., MS-COCO and Flickr30K.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19206",
        "abstract url": "https://arxiv.org/abs/2403.19206",
        "title": "CogniDot: Vasoactivity-based Cognitive Load Monitoring with a Miniature On-skin Sensor",
        "rating": -2,
        "keywords": [
            [
                "bio-compatible",
                "psychological"
            ]
        ],
        "abstract": "Vascular activities offer valuable signatures for psychological monitoring applications. We present CogniDot, an affordable, miniature skin sensor placed on the temporal area on the head that senses cognitive loads with a single-pixel color sensor. With its energy-efficient design, bio-compatible adhesive, and compact size (22mm diameter, 8.5mm thickness), it is ideal for long-term monitoring of mind status. We showed in detail the hardware design of our sensor. The user study results with 12 participants show that CogniDot can accurately differentiate between three levels of cognitive loads with a within-user accuracy of 97%. We also discuss its potential for broader applications.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19235",
        "abstract url": "https://arxiv.org/abs/2403.19235",
        "title": "DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation",
        "rating": -2,
        "keywords": [
            [
                "Diffusion",
                "synthesize",
                "text-to-image"
            ],
            [
                "Face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "While large-scale pre-trained text-to-image models can synthesize diverse and high-quality human-centered images, novel challenges arise with a nuanced task of \"identity fine editing\": precisely modifying specific features of a subject while maintaining its inherent identity and context. Existing personalization methods either require time-consuming optimization or learning additional encoders, adept in \"identity re-contextualization\". However, they often struggle with detailed and sensitive tasks like human face editing. To address these challenges, we introduce DreamSalon, a noise-guided, staged-editing framework, uniquely focusing on detailed image manipulations and identity-context preservation. By discerning editing and boosting stages via the frequency and gradient of predicted noises, DreamSalon first performs detailed manipulations on specific features in the editing stage, guided by high-frequency information, and then employs stochastic denoising in the boosting stage to improve image quality. For more precise editing, DreamSalon semantically mixes source and target textual prompts, guided by differences in their embedding covariances, to direct the model's focus on specific manipulation areas. Our experiments demonstrate DreamSalon's ability to efficiently and faithfully edit fine details on human faces, outperforming existing methods both qualitatively and quantitatively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19251",
        "abstract url": "https://arxiv.org/abs/2403.19251",
        "title": "Arbitrary State Transition of Open Qubit System Based on Switching Control",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "We present a switching control strategy based on Lyapunov control for arbitrary state transitions in open qubit systems. With coherent vector representation, we propose a switching control strategy, which can prevent the state of the qubit from entering invariant sets and singular value sets, effectively driving the system ultimately to a sufficiently small neighborhood of target states. In comparison to existing works, this control strategy relaxes the strict constraints on system models imposed by special target states. Furthermore, we identify conditions under which the open qubit system achieves finite-time stability (FTS) and finite-time contractive stability (FTCS), respectively. This represents a critical improvement in quantum state transitions, especially considering the asymptotic stability of arbitrary target states is unattainable in open quantum systems. The effectiveness of our proposed method is convincingly demonstrated through its application in a qubit system affected by various types of decoherence, including amplitude, dephasing and polarization decoherence.",
        "subjects": [
            "quant-ph"
        ],
        "comment": "12 pages, 7 figures"
    },
    {
        "paper id": "2403.19265",
        "abstract url": "https://arxiv.org/abs/2403.19265",
        "title": "Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "Surgical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Laparoscopic video tracking primarily focuses on two target types: surgical instruments and anatomy. The former could be used for skill assessment, while the latter is necessary for the projection of virtual overlays. Where instrument and anatomy tracking have often been considered two separate problems, in this paper, we propose a method for joint tracking of all structures simultaneously. Based on a single 2D monocular video clip, we train a neural field to represent a continuous spatiotemporal scene, used to create 3D tracks of all surfaces visible in at least one frame. Due to the small size of instruments, they generally cover a small part of the image only, resulting in decreased tracking accuracy. Therefore, we propose enhanced class weighting to improve the instrument tracks. We evaluate tracking on video clips from laparoscopic cholecystectomies, where we find mean tracking accuracies of 92.4% for anatomical structures and 87.4% for instruments. Additionally, we assess the quality of depth maps obtained from the method's scene reconstructions. We show that these pseudo-depths have comparable quality to a state-of-the-art pre-trained depth estimator. On laparoscopic videos in the SCARED dataset, the method predicts depth with an MAE of 2.9 mm and a relative error of 9.2%. These results show the feasibility of using neural fields for monocular 3D reconstruction of laparoscopic scenes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19276",
        "abstract url": "https://arxiv.org/abs/2403.19276",
        "title": "Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems",
        "rating": -2,
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "In implicit collaborative filtering, hard negative mining techniques are developed to accelerate and enhance the recommendation model learning. However, the inadvertent selection of false negatives remains a major concern in hard negative sampling, as these false negatives can provide incorrect information and mislead the model learning. To date, only a small number of studies have been committed to solve the false negative problem, primarily focusing on designing sophisticated sampling algorithms to filter false negatives. In contrast, this paper shifts its focus to refining the loss function. We find that the original Bayesian Personalized Ranking (BPR), initially designed for uniform negative sampling, is inadequate in adapting to hard sampling scenarios. Hence, we introduce an enhanced Bayesian Personalized Ranking objective, named as Hard-BPR, which is specifically crafted for dynamic hard negative sampling to mitigate the influence of false negatives. This method is simple yet efficient for real-world deployment. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness and robustness of our approach, along with the enhanced ability to distinguish false negatives.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2403.19292",
        "abstract url": "https://arxiv.org/abs/2403.19292",
        "title": "Deep Learning-based Modulation Classification of Practical OFDM Signals for Spectrum Sensing",
        "rating": -2,
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "In this study, the modulation of symbols on OFDM subcarriers is classified for transmissions following Wi-Fi~6 and 5G downlink specifications. First, our approach estimates the OFDM symbol duration and cyclic prefix length based on the cyclic autocorrelation function. We propose a feature extraction algorithm characterizing the modulation of OFDM signals, which includes removing the effects of a synchronization error. The obtained feature is converted into a 2D histogram of phase and amplitude and this histogram is taken as input to a convolutional neural network (CNN)-based classifier. The classifier does not require prior knowledge of protocol-specific information such as Wi-Fi preamble or resource allocation of 5G physical channels. The classifier's performance, evaluated using synthetic and real-world measured over-the-air (OTA) datasets, achieves a minimum accuracy of 97\\% accuracy with OTA data when SNR is above the value required for data transmission.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "9 pages, 12 figures"
    },
    {
        "paper id": "2403.19299",
        "abstract url": "https://arxiv.org/abs/2403.19299",
        "title": "Post Quantum Cryptography & its Comparison with Classical Cryptography",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Cryptography plays a pivotal role in safeguarding sensitive information and facilitating secure communication. Classical cryptography relies on mathematical computations, whereas quantum cryptography operates on the principles of quantum mechanics, offering a new frontier in secure communication. Quantum cryptographic systems introduce novel dimensions to security, capable of detecting and thwarting eavesdropping attempts. By contrasting quantum cryptography with its classical counterpart, it becomes evident how quantum mechanics revolutionizes the landscape of secure communication.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19300",
        "abstract url": "https://arxiv.org/abs/2403.19300",
        "title": "Random Multi-Type Spanning Forests for Synchronization on Sparse Graphs",
        "rating": -2,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Random diffusions are a popular tool in Monte-Carlo estimations, with well established algorithms such as Walk-on-Spheres (WoS) going back several decades. In this work, we introduce diffusion estimators for the problems of angular synchronization and smoothing on graphs, in the presence of a rotation associated to each edge. Unlike classical WoS algorithms, these estimators allow for global estimations by propagating along the branches of multi-type spanning forests, and we show that they can outperform standard numerical-linear-algebra solvers in challenging instances, depending on the topology and density of the graph.",
        "subjects": [
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19305",
        "abstract url": "https://arxiv.org/abs/2403.19305",
        "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
        "rating": -2,
        "keywords": [
            [
                "generative"
            ],
            [
                "industrial"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A \"Multi-Agent Text Evaluation framework\" where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "This paper has been ACCEPTED as a LONG PAPER presentation by DASFAA 2024 Industrial Track"
    },
    {
        "paper id": "2403.19332",
        "abstract url": "https://arxiv.org/abs/2403.19332",
        "title": "Learning a Formally Verified Control Barrier Function in Stochastic Environment",
        "rating": -2,
        "keywords": [
            [
                "synthesizing"
            ],
            [
                "autonomous driving"
            ]
        ],
        "abstract": "Safety is a fundamental requirement of control systems. Control Barrier Functions (CBFs) are proposed to ensure the safety of the control system by constructing safety filters or synthesizing control inputs. However, the safety guarantee and performance of safe controllers rely on the construction of valid CBFs. Inspired by universal approximatability, CBFs are represented by neural networks, known as neural CBFs (NCBFs). This paper presents an algorithm for synthesizing formally verified continuous-time neural Control Barrier Functions in stochastic environments in a single step. The proposed training process ensures efficacy across the entire state space with only a finite number of data points by constructing a sample-based learning framework for Stochastic Neural CBFs (SNCBFs). Our methodology eliminates the need for post hoc verification by enforcing Lipschitz bounds on the neural network, its Jacobian, and Hessian terms. We demonstrate the effectiveness of our approach through case studies on the inverted pendulum system and obstacle avoidance in autonomous driving, showcasing larger safe regions compared to baseline methods.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 3 figures"
    },
    {
        "paper id": "2403.19345",
        "abstract url": "https://arxiv.org/abs/2403.19345",
        "title": "Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. Such systems not only enhance user experience and satisfaction but also furnish opportunities for businesses and platforms to augment user engagement, sales, and advertising efficacy.This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems. It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized recommendation systems in e-commerce, including data privacy, algorithmic bias, scalability, and the cold start problem. Strategies to address these challenges are elucidated.Subsequently, the paper outlines a personalized recommendation system leveraging the BERT model and nearest neighbor algorithm, specifically tailored to address the exigencies of the eBay e-commerce platform. The efficacy of this recommendation system is substantiated through manual evaluation, and a practical application operational guide and structured output recommendation results are furnished to ensure the system's operability and scalability.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19347",
        "abstract url": "https://arxiv.org/abs/2403.19347",
        "title": "Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from extensive user sequences and stores them in the offline database. Subsequently, the deeper, trainable layers of the LLM facilitate intricate inter-behavior interactions, thereby generating comprehensive user embeddings. This separation allows the learning of high-level user representations to be independent of low-level behavior encoding, significantly reducing computational complexity. Finally, these refined user embeddings, in conjunction with correspondingly processed item embeddings, are incorporated into the CTR model to compute the CTR scores. Extensive experimental results show that BAHE reduces training time and memory by five times for CTR models using LLMs, especially with longer user sequences. BAHE has been deployed in a real-world system, allowing for daily updates of 50 million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR prediction.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2024"
    },
    {
        "paper id": "2403.19348",
        "abstract url": "https://arxiv.org/abs/2403.19348",
        "title": "Efficient Anchor Point Deployment for Low Latency Connectivity in MEC-Assisted C-V2X Scenarios",
        "rating": -2,
        "keywords": [
            [
                "Edge Computing"
            ]
        ],
        "abstract": "Next-generation cellular networks will play a key role in the evolution of different vertical industries. Low latency will be a major requirement in many related uses cases. This requirement is specially challenging in scenarios with high mobility of end devices, such as vehicular communications. The Multi-Access Edge Computing (MEC) paradigm seeks to satisfy it. In this article we propose the dynamic deployment of anchor point network functions at edge locations and the assignment of terminals to these anchor points with the joint objective of minimizing communications latency and reducing network overhead. We formally define the problem as a multi-objective optimization and also propose a novel heuristic greedy algorithm for approximating the solution. This algorithm compares favorably with baseline and state-of-the-art strategies for latency minimization while reducing the overhead caused by network reconfigurations.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Article published in IEEE Transactions on Vehicular Technology"
    },
    {
        "paper id": "2403.19359",
        "abstract url": "https://arxiv.org/abs/2403.19359",
        "title": "Coordinated Allocation of Radio Resources to Wi-Fi and Cellular Technologies in Shared Unlicensed Frequencies",
        "rating": -2,
        "keywords": [
            [
                "5G",
                "industrial"
            ]
        ],
        "abstract": "Wireless connectivity is essential for industrial production processes and workflow management. Moreover, the connectivity requirements of industrial devices, which are usually long-term investments, are diverse and require different radio interfaces. In this regard, the 3GPP has studied how to support heterogeneous radio access technologies (RATs) such as Wi-Fi and unlicensed cellular technologies in 5G core networks. In some cases, these technologies coexist in the same spectrum. Dynamic spectrum sharing (DSS), which has already been proven to increase spectrum efficiency in licensed bands, can also be applied to this scenario. In this paper, we propose two solutions for mobile network operators (MNOs) or service providers to dynamically divide (multiplex) the radio resources of a shared channel between a Wi-Fi basic service set (BSS) and one or several carriers of scheduled wireless networks, such as cellular technologies, with a configurable level of sharing granularity. These solutions do not require modifications to the current commercial off-the-shelf (COTS) end devices. We adapt the existing IEEE 802.11 procedures to notify the Wi-Fi stations that they must share channels with different access networks. We demonstrate that our dynamic sharing proposals are also advantageous over direct coexistence and evaluate each of them quantitatively and qualitatively to determine when one or the other is preferable. The evaluation is particularized for IEEE 802.11ac and long-term evolution (LTE) license assisted access (LAA), but the solutions can be easily extended to 5G new radio-unlicensed (5G NR-U) or to any other wireless technology in which the network side schedules end device transmissions.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Article published in IEEE Access"
    },
    {
        "paper id": "2403.19369",
        "abstract url": "https://arxiv.org/abs/2403.19369",
        "title": "RAIL: Robot Affordance Imagination with Large Language Models",
        "rating": -2,
        "keywords": [
            [
                "generative"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "This paper introduces an automatic affordance reasoning paradigm tailored to minimal semantic inputs, addressing the critical challenges of classifying and manipulating unseen classes of objects in household settings. Inspired by human cognitive processes, our method integrates generative language models and physics-based simulators to foster analytical thinking and creative imagination of novel affordances. Structured with a tripartite framework consisting of analysis, imagination, and evaluation, our system \"analyzes\" the requested affordance names into interaction-based definitions, \"imagines\" the virtual scenarios, and \"evaluates\" the object affordance. If an object is recognized as possessing the requested affordance, our method also predicts the optimal pose for such functionality, and how a potential user can interact with it. Tuned on only a few synthetic examples across 3 affordance classes, our pipeline achieves a very high success rate on affordance classification and functional pose prediction of 8 classes of novel objects, outperforming learning-based baselines. Validation through real robot manipulating experiments demonstrates the practical applicability of the imagined user interaction, showcasing the system's ability to independently conceptualize unseen affordances and interact with new objects and scenarios in everyday settings.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19374",
        "abstract url": "https://arxiv.org/abs/2403.19374",
        "title": "A noise-tolerant, resource-saving probabilistic binary neural network implemented by the SOT-MRAM compute-in-memory system",
        "rating": -2,
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "We report a spin-orbit torque(SOT) magnetoresistive random-access memory(MRAM)-based probabilistic binary neural network(PBNN) for resource-saving and hardware noise-tolerant computing applications. With the presence of thermal fluctuation, the non-destructive SOT-driven magnetization switching characteristics lead to a random weight matrix with controllable probability distribution. In the meanwhile, the proposed CIM architecture allows for the concurrent execution of the probabilistic vector-matrix multiplication (PVMM) and binarization. Furthermore, leveraging the effectiveness of random binary cells to propagate multi-bit probabilistic information, our SOT-MRAM-based PBNN system achieves a 97.78\\% classification accuracy under a 7.01\\% weight variation on the MNIST database through 10 sampling cycles, and the number of bit-level computation operations is reduced by a factor of 6.9 compared to that of the full-precision LeNet-5 network. Our work provides a compelling framework for the design of reliable neural networks tailored to the applications with low power consumption and limited computational resources.",
        "subjects": [
            "cs.ET"
        ],
        "comment": "5 pages, 10 figures"
    },
    {
        "paper id": "2403.19402",
        "abstract url": "https://arxiv.org/abs/2403.19402",
        "title": "V2X Enabled Emergency Vehicle Alert System",
        "rating": -2,
        "keywords": [
            [
                "time-efficient"
            ],
            [
                "Vehicle"
            ],
            [
                "health"
            ]
        ],
        "abstract": "Today's major concern in traffic management systems includes time-efficient emergency transports. The awareness of environment and vehicle information is necessary for the emergency vehicles as well as the surrounding commercial vehicles that might be driven by inexperienced drivers to act accordingly if they both interact. The information exchange should be quick and accurate along with how much interactive the alerting system is with the drivers. Therefore, technologies like V2X-based alert systems can deal with such emergency situations and hence prevent potential health or social hazards. An alerting system as a part of a smart-connected city is proposed in this paper. The Dedicated Short Range Communication (DSRC) based system has tried to cover the major domain of information about misbehaving vehicles, any pedestrians on the road, and information about the emergency vehicle itself. The commercial vehicle also will have a similar alert system as an application of V2V and V2I. Further in this paper, a realtime monitoring system was developed using grafana dashboard which will be installed in the area's base station to monitor the vehicles in that area.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19425",
        "abstract url": "https://arxiv.org/abs/2403.19425",
        "title": "A Robust Ensemble Algorithm for Ischemic Stroke Lesion Segmentation: Generalizability and Clinical Utility Beyond the ISLES Challenge",
        "rating": -2,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "biomarkers",
                "medical",
                "diagnosis",
                "MRI",
                "disease",
                "Clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Diffusion-weighted MRI (DWI) is essential for stroke diagnosis, treatment decisions, and prognosis. However, image and disease variability hinder the development of generalizable AI algorithms with clinical value. We address this gap by presenting a novel ensemble algorithm derived from the 2022 Ischemic Stroke Lesion Segmentation (ISLES) challenge. ISLES'22 provided 400 patient scans with ischemic stroke from various medical centers, facilitating the development of a wide range of cutting-edge segmentation algorithms by the research community. Through collaboration with leading teams, we combined top-performing algorithms into an ensemble model that overcomes the limitations of individual solutions. Our ensemble model achieved superior ischemic lesion detection and segmentation accuracy on our internal test set compared to individual algorithms. This accuracy generalized well across diverse image and disease variables. Furthermore, the model excelled in extracting clinical biomarkers. Notably, in a Turing-like test, neuroradiologists consistently preferred the algorithm's segmentations over manual expert efforts, highlighting increased comprehensiveness and precision. Validation using a real-world external dataset (N=1686) confirmed the model's generalizability. The algorithm's outputs also demonstrated strong correlations with clinical scores (admission NIHSS and 90-day mRS) on par with or exceeding expert-derived results, underlining its clinical relevance. This study offers two key findings. First, we present an ensemble algorithm (https://github.com/Tabrisrei/ISLES22_Ensemble) that detects and segments ischemic stroke lesions on DWI across diverse scenarios on par with expert (neuro)radiologists. Second, we show the potential for biomedical challenge outputs to extend beyond the challenge's initial objectives, demonstrating their real-world clinical applicability.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19446",
        "abstract url": "https://arxiv.org/abs/2403.19446",
        "title": "EDA-Driven Preprocessing for SAT Solving",
        "rating": -2,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "reinforcement"
            ]
        ],
        "abstract": "Effective formulation of problems into Conjunctive Normal Form (CNF) is critical in modern Boolean Satisfiability (SAT) solving for optimizing solver performance. Addressing the limitations of existing methods, our Electronic Design Automation (EDA)-driven preprocessing framework introduces a novel methodology for preparing SAT instances, leveraging both circuit and CNF formats for enhanced flexibility and efficiency. Central to our approach is the integration of a new logic synthesis technique, guided by a reinforcement learning agent, and a novel cost-customized LUT mapping strategy, enabling efficient handling of diverse SAT challenges. By transforming the SAT competition benchmarks into circuit instances, our framework demonstrates substantial performance improvements, as evidenced by a 52.42% reduction on average compared to solving directly. Moreover, our framework achieves a remarkable 96.14% runtime reduction on average for a set of logic equivalence checking problems that exhibit inherent circuit structures. These results highlight the effectiveness and versatility of our approach in handling both CNF and circuit instances. The code is available at https://github.com/cure-lab/EDA4SAT.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19449",
        "abstract url": "https://arxiv.org/abs/2403.19449",
        "title": "O-RAN for Energy-Efficient Serving Cluster Formulation in User-Centric Cell-Free MMIMO",
        "rating": -2,
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "The 6G Massive Multiple-Input Multiple-Output (MMIMO) networks can follow the so-called User-Centric Cell-Free (UCCF) architecture, where a single user is served by multiple Access Points (APs) coordinated by the Central Processing Unit (CPU). In this paper, we propose how O-RAN functionalities, i.e., rApp-xApp pair, can be used for energy-efficient Serving Cluster Formulation (SCF). Simulation studies show up to 37\\% gain in Energy Efficiency (EE) of the proposed solution over the state-of-the-art Network-Centric (NC) designs.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Accepted for presentation during The 2nd Workshop on Next-generation Open and Programmable Radio Access Networks (NG-OPERA), organized in conjunction with IEEE International Conference on Computer Communications, May 20, 2024"
    },
    {
        "paper id": "2403.19460",
        "abstract url": "https://arxiv.org/abs/2403.19460",
        "title": "RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation",
        "rating": -2,
        "keywords": [
            [
                "Point Cloud",
                "6-DOF"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from scene point cloud input. Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the target poses of objects for manipulation without any object segmentation. RiEMann learns a manipulation task from scratch with 5 to 10 demonstrations, generalizes to unseen SE(3) transformations and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose change of the target object. The scalable action space of RiEMann facilitates the addition of custom equivariant actions such as the direction of turning the faucet, which makes articulated object manipulation possible for RiEMann. In simulation and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks with a total of 25 variants and show that RiEMann outperforms baselines in both task success rates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%), and achieves a 5.4 frames per second (FPS) network inference speed. Code and video results are available at https://riemann-web.github.io/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19461",
        "abstract url": "https://arxiv.org/abs/2403.19461",
        "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
        "rating": -2,
        "keywords": [
            [
                "generative"
            ],
            [
                "Autonomous Driving"
            ]
        ],
        "abstract": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19474",
        "abstract url": "https://arxiv.org/abs/2403.19474",
        "title": "SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks",
        "rating": -2,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "robot",
                "navigation"
            ],
            [
                "Graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Scene graphs have been recently introduced into 3D spatial understanding as a comprehensive representation of the scene. The alignment between 3D scene graphs is the first step of many downstream tasks such as scene graph aided point cloud registration, mosaicking, overlap checking, and robot navigation. In this work, we treat 3D scene graph alignment as a partial graph-matching problem and propose to solve it with a graph neural network. We reuse the geometric features learned by a point cloud registration method and associate the clustered point-level geometric features with the node-level semantic feature via our designed feature fusion module. Partial matching is enabled by using a learnable method to select the top-k similar node pairs. Subsequent downstream tasks such as point cloud registration are achieved by running a pre-trained registration network within the matched regions. We further propose a point-matching rescoring method, that uses the node-wise alignment of the 3D scene graph to reweight the matching candidates from a pre-trained point cloud registration method. It reduces the false point correspondences estimated especially in low-overlapping cases. Experiments show that our method improves the alignment accuracy by 10~20% in low-overlap and random transformation scenarios and outperforms the existing work in multiple downstream tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 10 figures"
    },
    {
        "paper id": "2403.19489",
        "abstract url": "https://arxiv.org/abs/2403.19489",
        "title": "Evolving Assembly Code in an Adversarial Environment",
        "rating": -2,
        "keywords": [
            [
                "synthesize"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "In this work, we evolve assembly code for the CodeGuru competition. The competition's goal is to create a survivor -- an assembly program that runs the longest in shared memory, by resisting attacks from adversary survivors and finding their weaknesses. For evolving top-notch solvers, we specify a Backus Normal Form (BNF) for the assembly language and synthesize the code from scratch using Genetic Programming (GP). We evaluate the survivors by running CodeGuru games against human-written winning survivors. Our evolved programs found weaknesses in the programs they were trained against and utilized them. In addition, we compare our approach with a Large-Language Model, demonstrating that the latter cannot generate a survivor that can win at any competition. This work has important applications for cyber-security, as we utilize evolution to detect weaknesses in survivors. The assembly BNF is domain-independent; thus, by modifying the fitness function, it can detect code weaknesses and help fix them. Finally, the CodeGuru competition offers a novel platform for analyzing GP and code evolution in adversarial environments. To support further research in this direction, we provide a thorough qualitative analysis of the evolved survivors and the weaknesses found.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "9 pages, 5 figures, 6 listings"
    },
    {
        "paper id": "2403.19508",
        "abstract url": "https://arxiv.org/abs/2403.19508",
        "title": "Debiasing Cardiac Imaging with Controlled Latent Diffusion Models",
        "rating": -2,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Biobank",
                "medical",
                "health",
                "diagnosis",
                "mri",
                "disease",
                "Cardiac"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks using a large-cohort study, specifically, the UK Biobank. We assess our method by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demonstrate the effectiveness of the proposed approach in mitigating dataset imbalances, such as the scarcity of younger patients or individuals with normal BMI level suffering from heart failure. This work represents a major step towards the adoption of synthetic data for the development of fair and generalizable models for medical classification tasks. Notably, we conduct all our experiments using a single, consumer-level GPU to highlight the feasibility of our approach within resource-constrained environments. Our code is available at https://github.com/faildeny/debiasing-cardiac-mri.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19511",
        "abstract url": "https://arxiv.org/abs/2403.19511",
        "title": "Improving Clinical NLP Performance through Language Model-Generated Synthetic Clinical Data",
        "rating": -2,
        "keywords": [
            [
                "Generative"
            ],
            [
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Generative models have been showing potential for producing data in mass. This study explores the enhancement of clinical natural language processing performance by utilizing synthetic data generated from advanced language models. Promising results show feasible applications in such a high-stakes domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "submitted to review"
    },
    {
        "paper id": "2403.19578",
        "abstract url": "https://arxiv.org/abs/2403.19578",
        "title": "Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics",
        "rating": -2,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Robotics",
                "robot"
            ]
        ],
        "abstract": "We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos are available at https://www.robot-learning.uk/keypoint-action-tokens.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19586",
        "abstract url": "https://arxiv.org/abs/2403.19586",
        "title": "TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering",
        "rating": -2,
        "keywords": [
            [
                "Gaussian Splatting"
            ],
            [
                "medical",
                "diagnosis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art reconstruction quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code will be publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19633",
        "abstract url": "https://arxiv.org/abs/2403.19633",
        "title": "Lane-Change in Dense Traffic with Model Predictive Control and Neural Networks",
        "rating": -2,
        "keywords": [
            [
                "Generative",
                "GAN"
            ],
            [
                "vehicle"
            ]
        ],
        "abstract": "This paper presents an online smooth-path lane-change control framework. We focus on dense traffic where inter-vehicle space gaps are narrow, and cooperation with surrounding drivers is essential to achieve the lane-change maneuver. We propose a two-stage control framework that harmonizes Model Predictive Control (MPC) with Generative Adversarial Networks (GAN) by utilizing driving intentions to generate smooth lane-change maneuvers. To improve performance in practice, the system is augmented with an adaptive safety boundary and a Kalman Filter to mitigate sensor noise. Simulation studies are investigated in different levels of traffic density and cooperativeness of other drivers. The simulation results support the effectiveness, driving comfort, and safety of the proposed method.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19648",
        "abstract url": "https://arxiv.org/abs/2403.19648",
        "title": "Human-compatible driving partners through data-regularized self-play reinforcement learning",
        "rating": -2,
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "reinforcement"
            ]
        ],
        "abstract": "A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goals, with a success rate of 93%, an off-road rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in a human-like manner, as measured by their similarity to existing human driving logs. We also find that HR-PPO agents show considerable improvements on proxy measures for coordination with human driving, particularly in highly interactive scenarios. We open-source our code and trained agents at https://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent behaviors at https://sites.google.com/view/driving-partners.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19649",
        "abstract url": "https://arxiv.org/abs/2403.19649",
        "title": "GraspXL: Generating Grasping Motions for Diverse Objects at Scale",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "synthesize"
            ]
        ],
        "abstract": "Human hands possess the dexterity to interact with diverse objects such as grasping specific parts of the objects and/or approaching them from desired directions. More importantly, humans can grasp objects of any shape without object-specific skills. Recent works synthesize grasping motions following single objectives such as a desired approach heading direction or a grasping area. Moreover, they usually rely on expensive 3D hand-object data during training and inference, which limits their capability to synthesize grasping motions for unseen objects at scale. In this paper, we unify the generation of hand-object grasping motions across multiple motion objectives, diverse object shapes and dexterous hand morphologies in a policy learning framework GraspXL. The objectives are composed of the graspable area, heading direction during approach, wrist rotation, and hand position. Without requiring any 3D hand-object interaction data, our policy trained with 58 objects can robustly synthesize diverse grasping motions for more than 500k unseen objects with a success rate of 82.2%. At the same time, the policy adheres to objectives, which enables the generation of diverse grasps per object. Moreover, we show that our framework can be deployed to different dexterous hands and work with reconstructed or generated objects. We quantitatively and qualitatively evaluate our method to show the efficacy of our approach. Our model and code will be available.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Project Page: https://eth-ait.github.io/graspxl/"
    },
    {
        "paper id": "2403.19148",
        "abstract url": "https://arxiv.org/abs/2403.19148",
        "title": "GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education",
        "rating": -2.5,
        "keywords": [
            [
                "Generative"
            ],
            [
                "face"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "This study investigates the efficacy of six major Generative AI (GenAI) text detectors when confronted with machine-generated content that has been modified using techniques designed to evade detection by these tools (n=805). The results demonstrate that the detectors' already low accuracy rates (39.5%) show major reductions in accuracy (17.4%) when faced with manipulated content, with some techniques proving more effective than others in evading detection. The accuracy limitations and the potential for false accusations demonstrate that these tools cannot currently be recommended for determining whether violations of academic integrity have occurred, underscoring the challenges educators face in maintaining inclusive and fair assessment practices. However, they may have a role in supporting student learning and maintaining academic integrity when used in a non-punitive manner. These results underscore the need for a combined approach to addressing the challenges posed by GenAI in academia to promote the responsible and equitable use of these emerging technologies. The study concludes that the current limitations of AI text detectors require a critical approach for any possible implementation in HE and highlight possible alternatives to AI assessment strategies.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19334",
        "abstract url": "https://arxiv.org/abs/2403.19334",
        "title": "Test-Time Domain Generalization for Face Anti-Spoofing",
        "rating": -2.5,
        "keywords": [
            [
                "synthesize"
            ],
            [
                "attacks"
            ],
            [
                "facial",
                "Face"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Face Anti-Spoofing (FAS) is pivotal in safeguarding facial recognition systems against presentation attacks. While domain generalization (DG) methods have been developed to enhance FAS performance, they predominantly focus on learning domain-invariant features during training, which may not guarantee generalizability to unseen data that differs largely from the source distributions. Our insight is that testing data can serve as a valuable resource to enhance the generalizability beyond mere evaluation for DG FAS. In this paper, we introduce a novel Test-Time Domain Generalization (TTDG) framework for FAS, which leverages the testing data to boost the model's generalizability. Our method, consisting of Test-Time Style Projection (TTSP) and Diverse Style Shifts Simulation (DSSS), effectively projects the unseen data to the seen domain space. In particular, we first introduce the innovative TTSP to project the styles of the arbitrarily unseen samples of the testing distribution to the known source space of the training distributions. We then design the efficient DSSS to synthesize diverse style shifts via learnable style bases with two specifically designed losses in a hyperspherical feature space. Our method eliminates the need for model updates at the test time and can be seamlessly integrated into not only the CNN but also ViT backbones. Comprehensive experiments on widely used cross-domain FAS benchmarks demonstrate our method's state-of-the-art performance and effectiveness.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024"
    },
    {
        "paper id": "2403.19442",
        "abstract url": "https://arxiv.org/abs/2403.19442",
        "title": "Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting",
        "rating": -2.5,
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the evolving field of psychopathology, the accurate assessment and forecasting of data derived from Ecological Momentary Assessment (EMA) is crucial. EMA offers contextually-rich psychopathological measurements over time, that practically lead to Multivariate Time Series (MTS) data. Thus, many challenges arise in analysis from the temporal complexities inherent in emotional, behavioral, and contextual EMA data as well as their inter-dependencies. To address both of these aspects, this research investigates the performance of Recurrent and Temporal Graph Neural Networks (GNNs). Overall, GNNs, by incorporating additional information from graphs reflecting the inner relationships between the variables, notably enhance the results by decreasing the Mean Squared Error (MSE) to 0.84 compared to the baseline LSTM model at 1.02. Therefore, the effect of constructing graphs with different characteristics on GNN performance is also explored. Additionally, GNN-learned graphs, which are dynamically refined during the training process, were evaluated. Using such graphs showed a similarly good performance. Thus, graph learning proved also promising for other GNN methods, potentially refining the pre-defined graphs.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages, 3 figures, 2024 IEEE 40th International Conference on Data Engineering Workshops"
    },
    {
        "paper id": "2403.19181",
        "abstract url": "https://arxiv.org/abs/2403.19181",
        "title": "Make Large Language Model a Better Ranker",
        "rating": -3,
        "keywords": [
            [
                "generative"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda loss, an adaptation of lambda loss tailored to suit language generation tasks. Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference. Our evaluative studies reveal that ALRO outperforms existing embedding-based recommendation methods and the existing LLM-based recommendation baselines, highlighting its efficacy.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2403.19336",
        "abstract url": "https://arxiv.org/abs/2403.19336",
        "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
        "rating": -3,
        "keywords": [
            [
                "RGBD"
            ],
            [
                "Robot",
                "Navigation"
            ],
            [
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing is instance-level and attribute-level. In particular, when integrated with a large language model, IVLMap demonstrates the capability to i) transform natural language into navigation targets with instance and attribute information, enabling precise localization, and ii) accomplish zero-shot end-to-end navigation tasks based on natural language commands. Extensive navigation experiments are conducted. Simulation results illustrate that our method can achieve an average improvement of 14.4\\% in navigation accuracy. Code and demo are released at https://ivlmap.github.io/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19371",
        "abstract url": "https://arxiv.org/abs/2403.19371",
        "title": "Cell Electropermeabilization Modeling via Multiple Traces Formulation and Time Semi-Implicit Coupling",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "We simulate the electrical response of multiple disjoint biological 3D cells in the electropermeabilization process. Instead of solving the boundary value problem in the volume, we reduce it to a system of boundary integrals equations with nonlinear dynamics on the cell membranes via a coupling the local Multiple Traces Formulation with a time semi-implicit scheme. Spatially, boundary unknowns are approximated by spherical harmonics, thereby allowing for spectral convergence rates for suitable time steps. Numerical results are provided to validate our claims.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19415",
        "abstract url": "https://arxiv.org/abs/2403.19415",
        "title": "Brain-Shift: Unsupervised Pseudo-Healthy Brain Synthesis for Novel Biomarker Extraction in Chronic Subdural Hematoma",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "Synthesis"
            ],
            [
                "Biomarker",
                "surgery",
                "CT"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Chronic subdural hematoma (cSDH) is a common neurological condition characterized by the accumulation of blood between the brain and the dura mater. This accumulation of blood can exert pressure on the brain, potentially leading to fatal outcomes. Treatment options for cSDH are limited to invasive surgery or non-invasive management. Traditionally, the midline shift, hand-measured by experts from an ideal sagittal plane, and the hematoma volume have been the primary metrics for quantifying and analyzing cSDH. However, these approaches do not quantify the local 3D brain deformation caused by cSDH. We propose a novel method using anatomy-aware unsupervised diffeomorphic pseudo-healthy synthesis to generate brain deformation fields. The deformation fields derived from this process are utilized to extract biomarkers that quantify the shift in the brain due to cSDH. We use CT scans of 121 patients for training and validation of our method and find that our metrics allow the identification of patients who require surgery. Our results indicate that automatically obtained brain deformation fields might contain prognostic value for personalized cSDH treatment. Our implementation is available on: github.com/Barisimre/brain-morphing",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19435",
        "abstract url": "https://arxiv.org/abs/2403.19435",
        "title": "BAMM: Bidirectional Autoregressive Motion Model",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "generative",
                "diffusion"
            ],
            [
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19589",
        "abstract url": "https://arxiv.org/abs/2403.19589",
        "title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes",
        "rating": -3,
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "LiDAR"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D dense captioning stands as a cornerstone in achieving a comprehensive understanding of 3D scenes through natural language. It has recently witnessed remarkable achievements, particularly in indoor settings. However, the exploration of 3D dense captioning in outdoor scenes is hindered by two major challenges: 1) the \\textbf{domain gap} between indoor and outdoor scenes, such as dynamics and sparse visual inputs, makes it difficult to directly adapt existing indoor methods; 2) the \\textbf{lack of data} with comprehensive box-caption pair annotations specifically tailored for outdoor scenes. To this end, we introduce the new task of outdoor 3D dense captioning. As input, we assume a LiDAR point cloud and a set of RGB images captured by the panoramic camera rig. The expected output is a set of object boxes with captions. To tackle this task, we propose the TOD3Cap network, which leverages the BEV representation to generate object box proposals and integrates Relation Q-Former with LLaMA-Adapter to generate rich captions for these objects. We also introduce the TOD3Cap dataset, the largest one to our knowledge for 3D dense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K outdoor objects from 850 scenes. Notably, our TOD3Cap network can effectively localize and caption 3D objects in outdoor scenes, which outperforms baseline methods by a significant margin (+9.6 CiDEr@0.5IoU). Code, data, and models are publicly available at https://github.com/jxbbb/TOD3Cap.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code, data, and models are publicly available at https://github.com/jxbbb/TOD3Cap"
    },
    {
        "paper id": "2403.19607",
        "abstract url": "https://arxiv.org/abs/2403.19607",
        "title": "SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects",
        "rating": -3,
        "keywords": [
            [
                "3d",
                "RGB-D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "synthesis"
            ],
            [
                "Robotics"
            ]
        ],
        "abstract": "Acquiring accurate depth information of transparent objects using off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and Robotics. Depth estimation/completion methods are typically employed and trained on datasets with quality depth labels acquired from either simulation, additional sensors or specialized data collection setups and known 3d models. However, acquiring reliable depth information for datasets at scale is not straightforward, limiting training scalability and generalization. Neural Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide success in novel view synthesis and shape recovery. However, heuristics and controlled environments (lights, backgrounds, etc) are often required to accurately capture specular surfaces. In this paper, we propose using Visual Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to guide the NeRF reconstruction process for these objects via the simultaneous reconstruction of semantic fields and extensions to increase robustness. Our proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant performance on depth completion datasets for transparent objects and robotic grasping.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages. An accompanying video is available at https://www.youtube.com/watch?v=S4NCoUq4bmE"
    },
    {
        "paper id": "2403.19273",
        "abstract url": "https://arxiv.org/abs/2403.19273",
        "title": "A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors",
        "rating": -3.5,
        "keywords": [
            [
                "Disease",
                "face"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The development of an intelligent agricultural decision-supporting system for crop selection and disease forecasting in Bangladesh is the main objective of this work. The economy of the nation depends heavily on agriculture. However, choosing crops with better production rates and efficiently controlling crop disease are obstacles that farmers have to face. These issues are addressed in this research by utilizing machine learning methods and real-world datasets. The recommended approach uses a variety of datasets on the production of crops, soil conditions, agro-meteorological regions, crop disease, and meteorological factors. These datasets offer insightful information on disease trends, soil nutrition demand of crops, and agricultural production history. By incorporating this knowledge, the model first recommends the list of primarily selected crops based on the soil nutrition of a particular user location. Then the predictions of meteorological variables like temperature, rainfall, and humidity are made using SARIMAX models. These weather predictions are then used to forecast the possibilities of diseases for the primary crops list by utilizing the support vector classifier. Finally, the developed model makes use of the decision tree regression model to forecast crop yield and provides a final crop list along with associated possible disease forecast. Utilizing the outcome of the model, farmers may choose the best productive crops as well as prevent crop diseases and reduce output losses by taking preventive actions. Consequently, planning and decision-making processes are supported and farmers can predict possible crop yields. Overall, by offering a detailed decision support system for crop selection and disease prediction, this work can play a vital role in advancing agricultural practices in Bangladesh.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This paper was presented to the IEEE conference, \"2024 International Conference on Advances in Computing, Communication, Electrical, and Smart Systems (iCACCESS), 8-9 March, Dhaka, Bangladesh\""
    },
    {
        "paper id": "2403.19602",
        "abstract url": "https://arxiv.org/abs/2403.19602",
        "title": "Behavior Trees in Industrial Applications: A Case Study in Underground Explosive Charging",
        "rating": -4,
        "keywords": [
            [
                "face"
            ],
            [
                "Industrial"
            ]
        ],
        "abstract": "In industrial applications Finite State Machines (FSMs) are often used to implement decision making policies for autonomous systems. In recent years, the use of Behavior Trees (BT) as an alternative policy representation has gained considerable attention. The benefits of using BTs over FSMs are modularity and reusability, enabling a system that is easy to extend and modify. However, there exists few published studies on successful implementations of BTs for industrial applications. This paper contributes with the lessons learned from implementing BTs in a complex industrial use case, where a robotic system assembles explosive charges and places them in holes on the rock face. The main result of the paper is that even if it is possible to model the entire system as a BT, combining BTs with FSMs can increase the readability and maintainability of the system. The benefit of such combination is remarked especially in the use case studied in this paper, where the full system cannot run autonomously but human supervision and feedback are needed.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19178",
        "abstract url": "https://arxiv.org/abs/2403.19178",
        "title": "Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning",
        "rating": -5,
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "healthcare"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities. Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm. Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes. Federated Learning (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange. Despite the growing interest in decentralized methods, their application in FL remains underexplored. This paper presents a thorough investigation into Blockchain-based FL (BCFL), spotlighting the synergy between blockchain's security features and FL's privacy-preserving model training capabilities. First, we present the taxonomy of BCFL from three aspects, including decentralized, separate networks, and reputation-based architectures. Then, we summarize the general architecture of BCFL systems, providing a comprehensive perspective on FL architectures informed by blockchain. Afterward, we analyze the application of BCFL in healthcare, IoT, and other privacy-sensitive areas. Finally, we identify future research directions of BCFL.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "25 pages, accepted by KAIS 2024"
    },
    {
        "paper id": "2403.19484",
        "abstract url": "https://arxiv.org/abs/2403.19484",
        "title": "Improved Genetic Algorithm Based on Greedy and Simulated Annealing Ideas for Vascular Robot Ordering Strategy",
        "rating": -5,
        "keywords": [
            [
                "robotics",
                "Robot"
            ],
            [
                "Medical",
                "healthcare"
            ],
            [
                "forecasting"
            ]
        ],
        "abstract": "This study presents a comprehensive approach for optimizing the acquisition, utilization, and maintenance of ABLVR vascular robots in healthcare settings. Medical robotics, particularly in vascular treatments, necessitates precise resource allocation and optimization due to the complex nature of robot and operator maintenance. Traditional heuristic methods, though intuitive, often fail to achieve global optimization. To address these challenges, this research introduces a novel strategy, combining mathematical modeling, a hybrid genetic algorithm, and ARIMA time series forecasting. Considering the dynamic healthcare environment, our approach includes a robust resource allocation model for robotic vessels and operators. We incorporate the unique requirements of the adaptive learning process for operators and the maintenance needs of robotic components. The hybrid genetic algorithm, integrating simulated annealing and greedy approaches, efficiently solves the optimization problem. Additionally, ARIMA time series forecasting predicts the demand for vascular robots, further enhancing the adaptability of our strategy. Experimental results demonstrate the superiority of our approach in terms of optimization, transparency, and convergence speed from other state-of-the-art methods.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2403.19147",
        "abstract url": "https://arxiv.org/abs/2403.19147",
        "title": "Resilience-Oriented Operation of Micro-Grids in both Grid-Connected and Isolated Conditions within Sustainable Active Distribution Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "Due to the increasing occurrence of natural disasters, importance of maintaining sustainable energy for cities and society is felt more than ever. On the other hand, power loss reduction is a challenging issue of active distribution networks (ADNs). In this paper, a new convex optimization model is proposed with two objective functions including energy loss reduction in normal operating mode and system load shedding minimization in critical conditions after the occurrence of natural disasters. This purpose is fulfilled through optimal allocation of distributed generation (DG) units from both conventional and renewable types as well as energy storage systems (ESSs). In addition, a new formulation has been derived to form optimal micro-grids (MGs) aiming at energy loss reduction in normal operating condition and resiliency index improvement under emergency situations. The developed model is implemented in GAMS software and the studies have been tested and analyzed on the IEEE 33-bus system. The results verify the effectiveness of the proposed method in terms of energy loss reduction as well as resilience enhancement in extreme operation condition following severe disruptions in the system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19168",
        "abstract url": "https://arxiv.org/abs/2403.19168",
        "title": "Tunable Superconducting Magnetic Levitation with Self-Stability",
        "rating": -10,
        "keywords": [],
        "abstract": "Magnetic levitation based on the flux pinning nature of type II superconductors has the merit of self-stability, making it appealing for applications such as high speed bearings, maglev trains, space generators, etc. However, such levitation systems physically rely on the superconductor pre-capturing magnetic flux (i.e. field cooling process) before establishing the levitation state which is nonadjustable afterwards. Moreover, practical type II superconductors in the levitation system inevitably suffer from various sources of energy losses, leading to continuous levitation force decay. These intrinsic drawbacks make superconducting maglev inflexible and impractical for long term operation. Here we propose and demonstrate a new form of superconducting maglev which is tunable and with self-stability. The maglev system uses a closed-loop type II superconducting coil to lock flux of a magnet, establishing self-stable levitation between the two objects. A flux pump is used to modulate the total magnetic flux of the coil without breaking its superconductivity, thus flexibly tuning levitation force and height meanwhile maintaining self-stability. For the first time, we experimentally demonstrate a self-stable type II superconducting maglev system which is able to: counteract long term levitation force decay, adjust levitation force and equilibrium position, and establish levitation under zero field cooling condition. These breakthroughs may bridge the gap between demonstrations and practical applications of type II superconducting maglevs.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "15pages,5 figures"
    },
    {
        "paper id": "2403.19171",
        "abstract url": "https://arxiv.org/abs/2403.19171",
        "title": "Mining Bug Repositories for Multi-Fault Programs",
        "rating": -10,
        "keywords": [],
        "abstract": "Datasets such as Defects4J and BugsInPy that contain bugs from real-world software projects are necessary for a realistic evaluation of automated debugging tools. However these datasets largely identify only a single bug in each entry, while real-world software projects (including those used in Defects4J and BugsInPy) typically contain multiple bugs at the same time. We lift this limitation and describe an extension to these datasets in which multiple bugs are identified in individual entries. We use test case transplantation and fault location translation, in order to expose and locate the bugs, respectively. We thus provide datasets of true multi-fault versions within real-world software projects, which maintain the properties and usability of the original datasets.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19174",
        "abstract url": "https://arxiv.org/abs/2403.19174",
        "title": "Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration",
        "rating": -10,
        "keywords": [],
        "abstract": "This Research through Design paper explores how object detection may be applied to a large digital art museum collection to facilitate new ways of encountering and experiencing art. We present the design and evaluation of an interactive application called SMKExplore, which allows users to explore a museum's digital collection of paintings by browsing through objects detected in the images, as a novel form of open-ended exploration. We provide three contributions. First, we show how an object detection pipeline can be integrated into a design process for visual exploration. Second, we present the design and development of an app that enables exploration of an art museum's collection. Third, we offer reflections on future possibilities for museums and HCI researchers to incorporate object detection techniques into the digitalization of museums.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19175",
        "abstract url": "https://arxiv.org/abs/2403.19175",
        "title": "Toward Practical Benchmarks of Ising Machines: A Case Study on the Quadratic Knapsack Problem",
        "rating": -10,
        "keywords": [],
        "abstract": "Combinatorial optimization has wide applications from industry to natural science. Ising machines bring an emerging computing paradigm for efficiently solving a combinatorial optimization problem by searching a ground state of a given Ising model. Current cutting-edge Ising machines achieve fast sampling of near-optimal solutions of the max-cut problem. However, for problems with additional constraint conditions, their advantages have been hardly shown due to difficulties in handling the constraints. The performance of Ising machines on such problems heavily depends on encoding methods of constraints into penalties, but the optimal choice is non-trivial. In this work, we focus on benchmarks of Ising machines on the quadratic knapsack problem (QKP). To bring out their practical performance, we propose to exploit the problem structure upon using Ising machines. Specifically, we apply fast two-stage post-processing to the outputs of Ising machines, which makes handling the constraint easier. Simulation on medium-sized test instances shows that the proposed method substantially improves the solving performance of Ising machines and the improvement is robust to a choice of the encoding methods. We evaluate an Ising machine called Amplify Annealing Engine with the proposed method and found that it achieves comparable results with existing heuristics.",
        "subjects": [
            "cond-mat.stat-mech"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2403.19176",
        "abstract url": "https://arxiv.org/abs/2403.19176",
        "title": "Design and Evaluation of a DC Microgrid Testbed for DER Integration and Power Management",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper presents a DC microgrid testbed setup that consists of various Distributed Energy Resources (DERs) including solar Photovoltaics (PV), supercapacitors for voltage regulation, and Battery Energy Storage Systems (BESS). The DC microgrid accommodates both non-flexible and flexible loads which can be dynamically adjusted based on PV power availability. The integration of the setup with the Hyphae Autonomous Power Interchange System (APIS) framework automates energy transfer within the BESS, ensuring efficient power management and optimizing the overall efficiency of the DC microgrid. Furthermore, the setup is validated in terms of the efficacy of the proposed model via real-time simulation, facilitated by the Speedgoat baseline real-time target Hardware-in-the-Loop (HIL) machine. The results demonstrate the model's adeptness in efficiently managing power sharing, emphasizing the capabilities of the DC microgrid setup in terms of performance and reliability in dynamic energy scenarios as well as enhancing the resilience of the grid amidst PV uncertainties.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "2024 12th Workshop on Modeling and Simulation of Cyber-Physical Energy Systems (MSCPES)"
    },
    {
        "paper id": "2403.19180",
        "abstract url": "https://arxiv.org/abs/2403.19180",
        "title": "A Multi-hop Secure UWOC assisted Local Area Network for UIoT and Underwater Monitoring",
        "rating": -10,
        "keywords": [],
        "abstract": "Underwater environment is substantially less explored territory as compared to earth surface due to lack of robust underwater communication infrastructure. For Internet of Underwater things connectivity, underwater wireless optical communication can play a vital role, compared to conventional radio frequency communication, due to longer range, high data rate, low latency, and unregulated bandwidth. This study proposes underwater wireless optical communication driven local area network UWOC LAN, comprised of multiple network nodes with optical transceivers. Moreover, the temperature sensor data is encapsulated with individual authentication identity to enhance the security of the framework at the user end. The proposed system is evaluated in a specially designed water tank of 4 meters. The proposed system evaluation analysis shows that the system can transmit underwater temperature data reliably in real time. The proposed secure UWOC LAN is tested within a communication range of 16 meters by incorporating multi hop connectivity to monitor the underwater environment.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19185",
        "abstract url": "https://arxiv.org/abs/2403.19185",
        "title": "Deep CSI Compression for Dual-Polarized Massive MIMO Channels with Disentangled Representation Learning",
        "rating": -10,
        "keywords": [],
        "abstract": "Channel state information (CSI) feedback is critical for achieving the promised advantages of enhancing spectral and energy efficiencies in massive multiple-input multiple-output (MIMO) wireless communication systems. Deep learning (DL)-based methods have been proven effective in reducing the required signaling overhead for CSI feedback. In practical dual-polarized MIMO scenarios, channels in the vertical and horizontal polarization directions tend to exhibit high polarization correlation. To fully exploit the inherent propagation similarity within dual-polarized channels, we propose a disentangled representation neural network (NN) for CSI feedback, referred to as DiReNet. The proposed DiReNet disentangles dual-polarized CSI into three components: polarization-shared information, vertical polarization-specific information, and horizontal polarization-specific information. This disentanglement of dual-polarized CSI enables the minimization of information redundancy caused by the polarization correlation and improves the performance of CSI compression and recovery. Additionally, flexible quantization and network extension schemes are designed. Consequently, our method provides a pragmatic solution for CSI feedback to harness the physical MIMO polarization as a priori information. Our experimental results show that the performance of our proposed DiReNet surpasses that of existing DL-based networks, while also effectively reducing the number of network parameters by nearly one third.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19194",
        "abstract url": "https://arxiv.org/abs/2403.19194",
        "title": "Detecting and taking Project Interactions into account in Participatory Budgeting",
        "rating": -10,
        "keywords": [],
        "abstract": "The aim of this paper is to introduce models and algorithms for the Participatory Budgeting problem when projects can interact with each other. In this problem, the objective is to select a set of projects that fits in a given budget. Voters express their preferences over the projects and the goal is then to find a consensus set of projects that does not exceed the budget. Our goal is to detect such interactions thanks to the preferences expressed by the voters. Through the projects selected by the voters, we detect positive and negative interactions between the projects by identifying projects that are consistently chosen together. In presence of project interactions, it is preferable to select projects that interact positively rather than negatively, all other things being equal. We introduce desirable properties that utility functions should have in presence of project interactions and we build a utility function which fulfills the desirable properties introduced. We then give axiomatic properties of aggregation rules, and we study three classical aggregation rules: the maximization of the sum of the utilities, of the product of the utilities, or of the minimal utility. We show that in the three cases the problems solved by these rules are NP-hard, and we propose a branch and bound algorithm to solve them. We conclude the paper by experiments.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19197",
        "abstract url": "https://arxiv.org/abs/2403.19197",
        "title": "Ordering Collective Unit Tasks: from Scheduling to Computational Social Choice",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the collective schedules problem, which consists in computing a one machine schedule of a set of tasks, knowing that a set of individuals (also called voters) have preferences regarding the order of the execution of the tasks. Our aim is to return a consensus schedule. We consider the setting in which all tasks have the same length -- such a schedule can therefore also be viewed as a ranking. We study two rules, one based on a distance criterion, and another one based one a binary criterion, and we show that these rules extend classic scheduling criteria. We also consider time constraints and precedence constraints between the tasks, and focus on two cases: the preferences of the voters fulfill these constraints, or they do not fulfill these constraints (but the collective schedule should fulfill them). In each case, either we show that the problem is NP-hard, or we provide a polynomial time algorithm which solves it. We also provide an analysis of a heuristic, which appears to be a 2 approximation of the Spearman's rule.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19201",
        "abstract url": "https://arxiv.org/abs/2403.19201",
        "title": "Understanding Archives: Towards New Research Interfaces Relying on the Semantic Annotation of Documents",
        "rating": -10,
        "keywords": [],
        "abstract": "The digitisation campaigns carried out by libraries and archives in recent years have facilitated access to documents in their collections. However, exploring and exploiting these documents remain difficult tasks due to the sheer quantity of documents available for consultation. In this article, we show how the semantic annotation of the textual content of study corpora of archival documents allow to facilitate their exploitation and valorisation. First, we present a methodological framework for the construction of new interfaces based on textual semantics, then address the current technological obstacles and their potential solutions. We conclude by presenting a practical case of the application of this framework.",
        "subjects": [
            "cs.DL"
        ],
        "comment": "in French language. CiDE.23: Document et archivage: pratiques formelles et informelles, Oct 2023, Grenoble, France"
    },
    {
        "paper id": "2403.19216",
        "abstract url": "https://arxiv.org/abs/2403.19216",
        "title": "Are Large Language Models Good at Utility Judgments?",
        "rating": -10,
        "keywords": [],
        "abstract": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at \\url{https://github.com/ict-bigdatalab/utility_judgments}.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Acctepted by SIGIR2024"
    },
    {
        "paper id": "2403.19257",
        "abstract url": "https://arxiv.org/abs/2403.19257",
        "title": "UniFaaS: Programming across Distributed Cyberinfrastructure with Federated Function Serving",
        "rating": -10,
        "keywords": [],
        "abstract": "Modern scientific applications are increasingly decomposable into individual functions that may be deployed across distributed and diverse cyberinfrastructure such as supercomputers, clouds, and accelerators. Such applications call for new approaches to programming, distributed execution, and function-level management. We present UniFaaS, a parallel programming framework that relies on a federated function-as-a-service (FaaS) model to enable composition of distributed, scalable, and high-performance scientific workflows, and to support fine-grained function-level management. UniFaaS provides a unified programming interface to compose dynamic task graphs with transparent wide-area data management. UniFaaS exploits an observe-predict-decide approach to efficiently map workflow tasks to target heterogeneous and dynamic resources. We propose a dynamic heterogeneity-aware scheduling algorithm that employs a delay mechanism and a re-scheduling mechanism to accommodate dynamic resource capacity. Our experiments show that UniFaaS can efficiently execute workflows across computing resources with minimal scheduling overhead. We show that UniFaaS can improve the performance of a real-world drug screening workflow by as much as 22.99% when employing an additional 19.48% of resources and a montage workflow by 54.41% when employing an additional 47.83% of resources across multiple distributed clusters, in contrast to using a single cluster",
        "subjects": [
            "cs.DC"
        ],
        "comment": "13 pages, 13 figures, IPDPS2024"
    },
    {
        "paper id": "2403.19271",
        "abstract url": "https://arxiv.org/abs/2403.19271",
        "title": "DeepSample: DNN sampling-based testing for operational accuracy assessment",
        "rating": -10,
        "keywords": [],
        "abstract": "Deep Neural Networks (DNN) are core components for classification and regression tasks of many software systems. Companies incur in high costs for testing DNN with datasets representative of the inputs expected in operation, as these need to be manually labelled. The challenge is to select a representative set of test inputs as small as possible to reduce the labelling cost, while sufficing to yield unbiased high-confidence estimates of the expected DNN accuracy. At the same time, testers are interested in exposing as many DNN mispredictions as possible to improve the DNN, ending up in the need for techniques pursuing a threefold aim: small dataset size, trustworthy estimates, mispredictions exposure. This study presents DeepSample, a family of DNN testing techniques for cost-effective accuracy assessment based on probabilistic sampling. We investigate whether, to what extent, and under which conditions probabilistic sampling can help to tackle the outlined challenge. We implement five new sampling-based testing techniques, and perform a comprehensive comparison of such techniques and of three further state-of-the-art techniques for both DNN classification and regression tasks. Results serve as guidance for best use of sampling-based testing for faithful and high-confidence estimates of DNN accuracy in operation at low cost.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Accepted for publication at ICSE 2024, Lisbon, Portugal"
    },
    {
        "paper id": "2403.19272",
        "abstract url": "https://arxiv.org/abs/2403.19272",
        "title": "Mil2: Efficient Cloth Simulation Using Non-distance Barriers and Subspace Reuse",
        "rating": -10,
        "keywords": [],
        "abstract": "Mil2 pushes the performance of high-resolution cloth simulation, making the simulation interactive (in milliseconds) for models with one million degrees of freedom (DOFs) while keeping every triangle untangled. The guarantee of being penetration-free is inspired by the interior-point method, which converts the inequality constraints to barrier potentials. Nevertheless, we propose a major overhaul of this modality by defining a novel and simple barrier formulation which does not depend on the distance between mesh primitives. Such a non-distance barrier model allows a new way to integrate collision detection into the simulation pipeline. Another contributor to the performance boost comes from the so-called subspace reuse strategy. This is based on the observation that low-frequency strain vibrations are near orthogonal to the deformation induced by collisions or self-collisions, often of high frequency. Subspace reuse then takes care of low-frequency residuals, while high-frequency residuals can also be effectively smoothed by GPU-based iterative solvers. We show that our method outperforms existing fast cloth simulators by nearly one order while keeping the entire simulation penetration-free and producing high-equality animations of high-resolution models.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19287",
        "abstract url": "https://arxiv.org/abs/2403.19287",
        "title": "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios",
        "rating": -10,
        "keywords": [],
        "abstract": "In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "11 pages, 4 figures, issta2024 accepted"
    },
    {
        "paper id": "2403.19302",
        "abstract url": "https://arxiv.org/abs/2403.19302",
        "title": "Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators",
        "rating": -10,
        "keywords": [],
        "abstract": "CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings. In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments. Our experiments reveal the effectiveness of our proposed models on the TREC iKAT dataset.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19344",
        "abstract url": "https://arxiv.org/abs/2403.19344",
        "title": "Gain-Only Neural Operator Approximators of PDE Backstepping Controllers",
        "rating": -10,
        "keywords": [],
        "abstract": "For the recently introduced deep learning-powered approach to PDE backstepping control, we present an advancement applicable across all the results developed thus far: approximating the control gain function only (a function of one variable), rather than the entire kernel function of the backstepping transformation (a function of two variables). We introduce this idea on a couple benchmark (unstable) PDEs, hyperbolic and parabolic. We alter the approach of quantifying the effect of the approximation error by replacing a backstepping transformation that employs the approximated kernel (suitable for adaptive control) by a transformation that employs the exact kernel (suitable for gain scheduling). A major simplification in the target system arises, with the perturbation due to the approximation shifting from the domain to the boundary condition. This results in a significant difference in the Lyapunov analysis, which nevertheless results in a guarantee of the stability being retained with the simplified approximation approach. The approach of approximating only the control gain function simplifies the operator being approximated and the training of its neural approximation, with an expected reduction in the neural network size. The price for the savings in approximation is paid through a somewhat more intricate Lyapunov analysis, in higher Sobolev spaces for some PDEs, as well as some restrictions on initial conditions that result from higher Sobolev spaces. While the proposed approach appears inapplicable to uses in adaptive control, it is almost certainly applicable in gain scheduling applications of neural operator-approximated PDE backstepping controllers.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Preprint submitted to ECC 2024 (full 8-page version containing proofs)"
    },
    {
        "paper id": "2403.19353",
        "abstract url": "https://arxiv.org/abs/2403.19353",
        "title": "A Software-Defined Networking Solution for Interconnecting Network Functions in Service-Based Architectures",
        "rating": -10,
        "keywords": [],
        "abstract": "Mobile core networks handle critical control functions for delivering services in modern cellular networks. Traditional point-to-point architectures, where network functions are directly connected through standardized interfaces, are being substituted by service-based architectures (SBAs), where core functionalities are finer-grained microservices decoupled from the underlying infrastructure. In this way, network functions and services can be distributed, with scaling and fail-over mechanisms, and can be dynamically deployed, updated, or removed to support slicing. A myriad of network functions can be deployed or removed according to traffic flows, thereby increasing the complexity of connection management. In this context, 3GPP Release 16 defines the service communication proxy (SCP) as a unified communication interface for a set of network functions. In this paper, we propose a novel software-defined networking (SDN)-based solution with the same role for a service mesh architecture where network functions can be deployed anywhere in the infrastructure. We demonstrated its efficiency in comparison with alternative architectures.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Article published in IEEE Access"
    },
    {
        "paper id": "2403.19375",
        "abstract url": "https://arxiv.org/abs/2403.19375",
        "title": "Multi-Agent Team Access Monitoring: Environments that Benefit from Target Information Sharing",
        "rating": -10,
        "keywords": [],
        "abstract": "Robotic access monitoring of multiple target areas has applications including checkpoint enforcement, surveillance and containment of fire and flood hazards. Monitoring access for a single target region has been successfully modeled as a minimum-cut problem. We generalize this model to support multiple target areas using two approaches: iterating on individual targets and examining the collections of targets holistically. Through simulation we measure the performance of each approach on different scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19378",
        "abstract url": "https://arxiv.org/abs/2403.19378",
        "title": "Cleaning data with Swipe",
        "rating": -10,
        "keywords": [],
        "abstract": "The repair problem for functional dependencies is the problem where an input database needs to be modified such that all functional dependencies are satisfied and the difference with the original database is minimal. The output database is then called an optimal repair. If the allowed modifications are value updates, finding an optimal repair is NP-hard. A well-known approach to find approximations of optimal repairs builds a Chase tree in which each internal node resolves violations of one functional dependency and leaf nodes represent repairs. A key property of this approach is that controlling the branching factor of the Chase tree allows to control the trade-off between repair quality and computational efficiency. In this paper, we explore an extreme variant of this idea in which the Chase tree has only one path. To construct this path, we first create a partition of attributes such that classes can be repaired sequentially. We repair each class only once and do so by fixing the order in which dependencies are repaired. This principle is called priority repairing and we provide a simple heuristic to determine priority. The algorithms for attribute partitioning and priority repair are combined in the Swipe algorithm. An empirical study on four real-life data sets shows that Swipe is in the range of one to three orders of magnitude faster than multi-sequence Chase-based approaches, whereas the quality of repairs is comparable or better. Moreover, we provide a scalability analysis of the Swipe algorithm.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19379",
        "abstract url": "https://arxiv.org/abs/2403.19379",
        "title": "Optimal Pilot Design for OTFS in Linear Time-Varying Channels",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper investigates the positioning of the pilot symbols, as well as the power distribution between the pilot and the communication symbols in the OTFS modulation scheme. We analyze the pilot placements that minimize the mean squared error (MSE) in estimating the channel taps. In addition, we optimize the average channel capacity by adjusting the power balance. We show that this leads to a significant increase in average capacity. The results provide valuable guidance for designing the OTFS parameters to achieve maximum capacity. Numerical simulations are performed to validate the findings.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "13 pages, 8 figures, submitted to IEEE Transactions on Wireless Communications"
    },
    {
        "paper id": "2403.19381",
        "abstract url": "https://arxiv.org/abs/2403.19381",
        "title": "On Uncertainty Quantification for Near-Bayes Optimal Algorithms",
        "rating": -10,
        "keywords": [],
        "abstract": "Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19388",
        "abstract url": "https://arxiv.org/abs/2403.19388",
        "title": "Cosystolic Expansion of Sheaves on Posets with Applications to Good 2-Query LTCs and Lifted Codes",
        "rating": -10,
        "keywords": [],
        "abstract": "We study sheaves on posets, showing that cosystolic expansion of such sheaves can be derived from local expansion conditions of the sheaf and the poset (typically a high dimensional expander). When the poset at hand is a cell complex, a sheaf on it may be thought of as generalizing coefficient groups used for defining homology and cohomology, by letting the coefficient group vary along the cell complex. Previous works established local criteria for cosystolic expansion only for simplicial complexes and with respect to constant coefficients. Cosystolic expansion of sheaves is related to property testing. We use this relation and our local criterion for cosystolic expansion to give two applications to locally testable codes (LTCs). First, we show the existence of good $2$-query LTCs. These codes are related to the recent good $q$-query LTCs of Dinur et. al and Panteleev-Kalachev, being the formers' so-called line codes, but we get them from a new, more illuminating perspective, namely, by realizing them as cocycle codes of sheaves over posets. We then derive their good properties directly from our criterion for cosystolic expansion. Second, we give a local criterion for a a lifted code (with some auxiliary structure) to be locally testable. This improves on a previous work of Dikstein et. al, where it was shown that one can obtain local testability of lifted codes from a mixture of local and global conditions.",
        "subjects": [
            "math.CO"
        ],
        "comment": "This subsumes sections 1-8 of arXiv:2208.01778. Preliminary version. Final version will appear soon"
    },
    {
        "paper id": "2403.19398",
        "abstract url": "https://arxiv.org/abs/2403.19398",
        "title": "Clustering MOOC Programming Solutions to Diversify Their Presentation to Students",
        "rating": -10,
        "keywords": [],
        "abstract": "In many MOOCs, whenever a student completes a programming task, they can see previous solutions of other students to find potentially different ways of solving the problem and learn new coding constructs. However, a lot of MOOCs simply show the most recent solutions, disregarding their diversity or quality. To solve this novel problem, we adapted the existing plagiarism detection tool JPlag to Python submissions on Hyperskill, a popular MOOC platform. However, due to the tool's inner algorithm, it fully processed only 46 out of 867 studied tasks. Therefore, we developed our own tool called Rhubarb. This tool first standardizes solutions that are algorithmically the same, then calculates the structure-aware edit distance between them, and then applies clustering. Finally, it selects one example from each of the largest clusters, taking into account their code quality. Rhubarb was able to handle all 867 tasks successfully. We compared approaches on a set of 59 tasks that both tools could process. Eight experts rated the selected solutions based on diversity, code quality, and usefulness. The default platform approach of selecting recent submissions received on average 3.12 out of 5, JPlag - 3.77, Rhubarb - 3.50. Since in the real MOOC, it is imperative to process everything, we created a system that uses JPlag on the 5.3% of tasks it fully processes and Rhubarb on the remaining 94.7%.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "7 pages, 4 figures"
    },
    {
        "paper id": "2403.19401",
        "abstract url": "https://arxiv.org/abs/2403.19401",
        "title": "Hardness of Learning Boolean Functions from Label Proportions",
        "rating": -10,
        "keywords": [],
        "abstract": "In recent years the framework of learning from label proportions (LLP) has been gaining importance in machine learning. In this setting, the training examples are aggregated into subsets or bags and only the average label per bag is available for learning an example-level predictor. This generalizes traditional PAC learning which is the special case of unit-sized bags. The computational learning aspects of LLP were studied in recent works (Saket, NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for learning halfspaces in the LLP setting. In this work we focus on the intractability of LLP learning Boolean functions. Our first result shows that given a collection of bags of size at most $2$ which are consistent with an OR function, it is NP-hard to find a CNF of constantly many clauses which satisfies any constant-fraction of the bags. This is in contrast with the work of (Saket, NeurIPS'21) which gave a $(2/5)$-approximation for learning ORs using a halfspace. Thus, our result provides a separation between constant clause CNFs and halfspaces as hypotheses for LLP learning ORs. Next, we prove the hardness of satisfying more than $1/2 + o(1)$ fraction of such bags using a $t$-DNF (i.e. DNF where each term has $\\leq t$ literals) for any constant $t$. In usual PAC learning such a hardness was known (Khot-Saket, FOCS'08) only for learning noisy ORs. We also study the learnability of parities and show that it is NP-hard to satisfy more than $(q/2^{q-1} + o(1))$-fraction of $q$-sized bags which are consistent with a parity using a parity, while a random parity based algorithm achieves a $(1/2^{q-2})$-approximation.",
        "subjects": [
            "cs.CC"
        ],
        "comment": "17 pages. Conference version of this paper appeared in FSTTCS 2023"
    },
    {
        "paper id": "2403.19409",
        "abstract url": "https://arxiv.org/abs/2403.19409",
        "title": "Channel Deduction: A New Learning Framework to Acquire Channel from Outdated Samples and Coarse Estimate",
        "rating": -10,
        "keywords": [],
        "abstract": "How to reduce the pilot overhead required for channel estimation? How to deal with the channel dynamic changes and error propagation in channel prediction? To jointly address these two critical issues in next-generation transceiver design, in this paper, we propose a novel framework named channel deduction for high-dimensional channel acquisition in multiple-input multiple-output (MIMO)-orthogonal frequency division multiplexing (OFDM) systems. Specifically, it makes use of the outdated channel information of past time slots, performs coarse estimation for the current channel with a relatively small number of pilots, and then fuses these two information to obtain a complete representation of the present channel. The rationale is to align the current channel representation to both the latent channel features within the past samples and the coarse estimate of current channel at the pilots, which, in a sense, behaves as a complementary combination of estimation and prediction and thus reduces the overall overhead. To fully exploit the highly nonlinear correlations in time, space, and frequency domains, we resort to learning-based implementation approaches. By using the highly efficient complex-domain multilayer perceptron (MLP)-mixer for crossing space-frequency domain representation and the recurrence-based or attention-based mechanisms for the past-present interaction, we respectively design two different channel deduction neural networks (CDNets). We provide a general procedure of data collection, training, and deployment to standardize the application of CDNets. Comprehensive experimental evaluations in accuracy, robustness, and efficiency demonstrate the superiority of the proposed approach, which reduces the pilot overhead by up to 88.9% compared to state-of-the-art estimation approaches and enables continuous operating even under unknown user movement and error propagation.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19436",
        "abstract url": "https://arxiv.org/abs/2403.19436",
        "title": "\"At the end of the day, I am accountable\": Gig Workers' Self-Tracking for Multi-Dimensional Accountability Management",
        "rating": -10,
        "keywords": [],
        "abstract": "Tracking is inherent in and central to the gig economy. Platforms track gig workers' performance through metrics such as acceptance rate and punctuality, while gig workers themselves engage in self-tracking. Although prior research has extensively examined how gig platforms track workers through metrics -- with some studies briefly acknowledging the phenomenon of self-tracking among workers -- there is a dearth of studies that explore how and why gig workers track themselves. To address this, we conducted 25 semi-structured interviews, revealing how gig workers self-tracking to manage accountabilities to themselves and external entities across three identities: the holistic self, the entrepreneurial self, and the platformized self. We connect our findings to neoliberalism, through which we contextualize gig workers' self-accountability and the invisible labor of self-tracking. We further discuss how self-tracking mitigates information and power asymmetries in gig work and offer design implications to support gig workers' multi-dimensional self-tracking.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Accepted to CHI 2024"
    },
    {
        "paper id": "2403.19448",
        "abstract url": "https://arxiv.org/abs/2403.19448",
        "title": "Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients",
        "rating": -10,
        "keywords": [],
        "abstract": "Kakade's natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization. We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side. Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential. Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the geometry of the linear program. Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results. We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error. In particular, these general results cover the case of state-action natural policy gradients.",
        "subjects": [
            "math.OC"
        ],
        "comment": "27 pages, 4 figures, under review"
    },
    {
        "paper id": "2403.19455",
        "abstract url": "https://arxiv.org/abs/2403.19455",
        "title": "Stabilization of a Class of Large-Scale Systems of Linear Hyperbolic PDEs via Continuum Approximation of Exact Backstepping Kernels",
        "rating": -10,
        "keywords": [],
        "abstract": "We establish that stabilization of a class of linear, hyperbolic partial differential equations (PDEs) with a large (nevertheless finite) number of components, can be achieved via employment of a backstepping-based control law, which is constructed for stabilization of a continuum version (i.e., as the number of components tends to infinity) of the PDE system. This is achieved by proving that the exact backstepping kernels, constructed for stabilization of the large-scale system, can be approximated (in certain sense such that exponential stability is preserved) by the backstepping kernels constructed for stabilization of a continuum version (essentially an infinite ensemble) of the original PDE system. The proof relies on construction of a convergent sequence of backstepping kernels that is defined such that each kernel matches the exact backstepping kernels (derived based on the original, large-scale system), in a piecewise constant manner with respect to an ensemble variable; while showing that they satisfy the continuum backstepping kernel equations. We present a numerical example that reveals that complexity of computation of stabilizing backstepping kernels may not scale with the number of components of the PDE state, when the kernels are constructed on the basis of the continuum version, in contrast to the case in which they are constructed on the basis of the original, large-scale system. In addition, we formally establish the connection between the solutions to the large-scale system and its continuum counterpart. Thus, this approach can be useful for design of computationally tractable, stabilizing backstepping-based control laws for large-scale PDE systems.",
        "subjects": [
            "math.OC"
        ],
        "comment": "16 pages, 6 figures, submitted to IEEE Transactions on Automatic Control"
    },
    {
        "paper id": "2403.19457",
        "abstract url": "https://arxiv.org/abs/2403.19457",
        "title": "Transmissive RIS Transmitter Enabled Spatial Modulation for MIMO Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we propose a novel transmissive reconfigurable intelligent surface (TRIS) transmitter-enabled spatial modulation (SM) multiple-input multiple-output (MIMO) system. In the transmission phase, a column-wise activation strategy is implemented for the TRIS panel, where the specific column elements are activated per time slot. Concurrently, the receiver employs the maximum likelihood detection technique. Based on this, for the transmit signals, we derive the closed-form expressions for the upper bounds of the average bit error probability (ABEP) of the proposed scheme from different perspectives, employing both vector-based and element-based approaches. Furthermore, we provide the asymptotic closed-form expressions for the ABEP of the TRIS-SM scheme, as well as the diversity gain. To improve the performance of the proposed TRIS-SM system, we optimize ABEP with a fixed data rate. Additionally, we provide lower bounds to simplify the computational complexity of improved TRIS-SM scheme. The Monte Carlo simulation method is used to validate the theoretical derivations exhaustively. The results demonstrate that the proposed TRIS-SM scheme can achieve better ABEP performance compared to the conventional SM scheme. Furthermore, the improved TRIS-SM scheme outperforms the TRIS-SM scheme in terms of reliability.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19459",
        "abstract url": "https://arxiv.org/abs/2403.19459",
        "title": "NeuroLGP-SM: A Surrogate-assisted Neuroevolution Approach using Linear Genetic Programming",
        "rating": -10,
        "keywords": [],
        "abstract": "Evolutionary algorithms are increasingly recognised as a viable computational approach for the automated optimisation of deep neural networks (DNNs) within artificial intelligence. This method extends to the training of DNNs, an approach known as neuroevolution. However, neuroevolution is an inherently resource-intensive process, with certain studies reporting the consumption of thousands of GPU days for refining and training a single DNN network. To address the computational challenges associated with neuroevolution while still attaining good DNN accuracy, surrogate models emerge as a pragmatic solution. Despite their potential, the integration of surrogate models into neuroevolution is still in its early stages, hindered by factors such as the effective use of high-dimensional data and the representation employed in neuroevolution. In this context, we address these challenges by employing a suitable representation based on Linear Genetic Programming, denoted as NeuroLGP, and leveraging Kriging Partial Least Squares. The amalgamation of these two techniques culminates in our proposed methodology known as the NeuroLGP-Surrogate Model (NeuroLGP-SM). For comparison purposes, we also code and use a baseline approach incorporating a repair mechanism, a common practice in neuroevolution. Notably, the baseline approach surpasses the renowned VGG-16 model in accuracy. Given the computational intensity inherent in DNN operations, a singular run is typically the norm. To evaluate the efficacy of our proposed approach, we conducted 96 independent runs. Significantly, our methodologies consistently outperform the baseline, with the SM model demonstrating superior accuracy or comparable results to the NeuroLGP approach. Noteworthy is the additional advantage that the SM approach exhibits a 25% reduction in computational requirements, further emphasising its efficiency for neuroevolution.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Accepted in \"International Conference on Optimization and Learning (OLA), Dubrovnik, Croatia, 2024\", 13 pages, 4 figures, 1 table"
    },
    {
        "paper id": "2403.19470",
        "abstract url": "https://arxiv.org/abs/2403.19470",
        "title": "Deep decomposition method for the limited aperture inverse obstacle scattering problem",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we consider a deep learning approach to the limited aperture inverse obstacle scattering problem. It is well known that traditional deep learning relies solely on data, which may limit its performance for the inverse problem when only indirect observation data and a physical model are available. A fundamental question arises in light of these limitations: is it possible to enable deep learning to work on inverse problems without labeled data and to be aware of what it is learning? This work proposes a deep decomposition method (DDM) for such purposes, which does not require ground truth labels. It accomplishes this by providing physical operators associated with the scattering model to the neural network architecture. Additionally, a deep learning based data completion scheme is implemented in DDM to prevent distorting the solution of the inverse problem for limited aperture data. Furthermore, apart from addressing the ill-posedness imposed by the inverse problem itself, DDM is a physics-aware machine learning technique that can have interpretability property. The convergence result of DDM is theoretically proven. Numerical experiments are presented to demonstrate the validity of the proposed DDM even when the incident and observation apertures are extremely limited.",
        "subjects": [
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19475",
        "abstract url": "https://arxiv.org/abs/2403.19475",
        "title": "A theoretical framework for the design and analysis of computational thinking problems in education",
        "rating": -10,
        "keywords": [],
        "abstract": "The field of computational thinking education has grown in recent years as researchers and educators have sought to develop and assess students' computational thinking abilities. While much of the research in this area has focused on defining computational thinking, the competencies it involves and how to assess them in teaching and learning contexts, this work takes a different approach. We provide a more situated perspective on computational thinking, focusing on the types of problems that require computational thinking skills to be solved and the features that support these processes. We develop a framework for analysing existing computational thinking problems in an educational context. We conduct a comprehensive literature review to identify prototypical activities from areas where computational thinking is typically pursued in education. We identify the main components and characteristics of these activities, along with their influence on activating computational thinking competencies. The framework provides a catalogue of computational thinking skills that can be used to understand the relationship between problem features and competencies activated. This study contributes to the field of computational thinking education by offering a tool for evaluating and revising existing problems to activate specific skills and for assisting in designing new problems that target the development of particular competencies. The results of this study may be of interest to researchers and educators working in computational thinking education.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19506",
        "abstract url": "https://arxiv.org/abs/2403.19506",
        "title": "LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae",
        "rating": -10,
        "keywords": [],
        "abstract": "This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "3 pages, accepted to CHI2024 workshop \"Challenges and Opportunities of LLM-Based Synthetic Personae and Data in HCI\""
    },
    {
        "paper id": "2403.19526",
        "abstract url": "https://arxiv.org/abs/2403.19526",
        "title": "Logic and Languages of Higher-Dimensional Automata",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper we study finite higher-dimensional automata (HDAs) from the logical point of view. Languages of HDAs are sets of finite bounded-width interval pomsets with interfaces (iiPoms<=k) closed under order extension. We prove that languages of HDAs are MSO-definable. For the converse, we show that the order extensions of MSO-definable sets of iiPoms<=k are languages of HDAs. As a consequence, unlike the case of all pomsets, order extension of MSO-definable sets of iiPoms<=k is also MSO-definable.",
        "subjects": [
            "cs.FL"
        ],
        "comment": "Submission to DLT24, 12 pages + references + appendix"
    },
    {
        "paper id": "2403.19530",
        "abstract url": "https://arxiv.org/abs/2403.19530",
        "title": "Detecting Financial Bots on the Ethereum Blockchain",
        "rating": -10,
        "keywords": [],
        "abstract": "The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6%, while the highest-performing model for binary classification is a Random Forest with an accuracy of 83%. Our machine learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19556",
        "abstract url": "https://arxiv.org/abs/2403.19556",
        "title": "Expectation Maximization Aided Modified Weighted Sequential Energy Detector for Distributed Cooperative Spectrum Sensing",
        "rating": -10,
        "keywords": [],
        "abstract": "Distributed cooperative spectrum sensing usually involves a group of unlicensed secondary users (SUs) collaborating to detect the primary user (PU) in the channel, and thereby opportunistically utilize it without causing interference to the PU. The conventional energy detector (ED) based spectrum sensing ignores the dynamic nature of the PU by using energy statistic only from the present sensing interval for the PU detection. However, for a dynamic PU, previous studies have shown that improved detection capabilities can be achieved by aggregating both present and past energy samples in a test statistic. To this end, a weighted sequential energy detector (WSED) has been proposed, but it is based on aggregating all the collected energy samples over an observation window. For a highly dynamic PU, that involves also combining the outdated samples in the test statistic. In this paper, we propose a modified WSED (mWSED) that uses the primary user states information over the window to aggregate only the highly correlated energy samples in its test statistic. In practice, since the PU states are a priori unknown, we also develop a joint expectation-maximization and Viterbi (EM-Viterbi) algorithm based scheme to iteratively estimate the states by using the energy samples collected over the window. The estimated states are then used in mWSED to compute its test statistics, and the algorithm is referred to here as EM-mWSED. Simulation results are presented to demonstrate the states estimation performance of EM-Viterbi and the PU detection performance of EM-mWSED. The results show that, for both highly dynamic as well as slowly time-varying PU, these algorithms outperform the ED and WSED at PU detection, and their performances improve by either increasing the average number of neighbors per SU in the network, or by increasing the SNR or the number of samples per energy statistic.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19560",
        "abstract url": "https://arxiv.org/abs/2403.19560",
        "title": "Exploring Communication Dynamics: Eye-tracking Analysis in Pair Programming of Computer Science Education",
        "rating": -10,
        "keywords": [],
        "abstract": "Pair programming is widely recognized as an effective educational tool in computer science that promotes collaborative learning and mirrors real-world work dynamics. However, communication breakdowns within pairs significantly challenge this learning process. In this study, we use eye-tracking data recorded during pair programming sessions to study communication dynamics between various pair programming roles across different student, expert, and mixed group cohorts containing 19 participants. By combining eye-tracking data analysis with focus group interviews and questionnaires, we provide insights into communication's multifaceted nature in pair programming. Our findings highlight distinct eye-tracking patterns indicating changes in communication skills across group compositions, with participants prioritizing code exploration over communication, especially during challenging tasks. Further, students showed a preference for pairing with experts, emphasizing the importance of understanding group formation in pair programming scenarios. These insights emphasize the importance of understanding group dynamics and enhancing communication skills through pair programming for successful outcomes in computer science education.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19577",
        "abstract url": "https://arxiv.org/abs/2403.19577",
        "title": "A Public and Reproducible Assessment of the Topics API on Real Data",
        "rating": -10,
        "keywords": [],
        "abstract": "The Topics API for the web is Google's privacy-enhancing alternative to replace third-party cookies. Results of prior work have led to an ongoing discussion between Google and research communities about the capability of Topics to trade off both utility and privacy. The central point of contention is largely around the realism of the datasets used in these analyses and their reproducibility; researchers using data collected on a small sample of users or generating synthetic datasets, while Google's results are inferred from a private dataset. In this paper, we complement prior research by performing a reproducible assessment of the latest version of the Topics API on the largest and publicly available dataset of real browsing histories. First, we measure how unique and stable real users' interests are over time. Then, we evaluate if Topics can be used to fingerprint the users from these real browsing traces by adapting methodologies from prior privacy studies. Finally, we call on web actors to perform and enable reproducible evaluations by releasing anonymized distributions. We find that 46%, 55%, and 60% of the 1207 users in the dataset are uniquely re-identified across websites after only 1, 2, and 3 observations of their topics by advertisers, respectively. This paper shows on real data that Topics does not provide the same privacy guarantees to all users, further highlighting the need for public and reproducible evaluations of the claims made by new web proposals.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted at SecWeb 2024: Workshop on Designing Security for the Web"
    },
    {
        "paper id": "2403.19605",
        "abstract url": "https://arxiv.org/abs/2403.19605",
        "title": "Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free Prediction",
        "rating": -10,
        "keywords": [],
        "abstract": "Decision-making pipelines are generally characterized by tradeoffs among various risk functions. It is often desirable to manage such tradeoffs in a data-adaptive manner. As we demonstrate, if this is done naively, state-of-the art uncertainty quantification methods can lead to significant violations of putative risk guarantees. To address this issue, we develop methods that permit valid control of risk when threshold and tradeoff parameters are chosen adaptively. Our methodology supports monotone and nearly-monotone risks, but otherwise makes no distributional assumptions. To illustrate the benefits of our approach, we carry out numerical experiments on synthetic data and the large-scale vision dataset MS-COCO.",
        "subjects": [
            "stat.ME"
        ],
        "comment": "27 pages, 10 figures"
    },
    {
        "paper id": "2403.19616",
        "abstract url": "https://arxiv.org/abs/2403.19616",
        "title": "Feedback Optimization of Incentives for Distribution Grid Services",
        "rating": -10,
        "keywords": [],
        "abstract": "Energy prices and net power injection limitations regulate the operations in distribution grids and typically ensure that operational constraints are met. Nevertheless, unexpected or prolonged abnormal events could undermine the grid's functioning. During contingencies, customers could contribute effectively to sustaining the network by providing services. This paper proposes an incentive mechanism that promotes users' active participation by essentially altering the energy pricing rule. The incentives are modeled via a linear function whose parameters can be computed by the system operator (SO) by solving an optimization problem. Feedback-based optimization algorithms are then proposed to seek optimal incentives by leveraging measurements from the grid, even in the case when the SO does not have a full grid and customer information. Numerical simulations on a standard testbed validate the proposed approach.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2403.19622",
        "abstract url": "https://arxiv.org/abs/2403.19622",
        "title": "RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents",
        "rating": -10,
        "keywords": [],
        "abstract": "The ultimate goals of robotic learning is to acquire a comprehensive and generalizable robotic system capable of performing both seen skills within the training distribution and unseen skills in novel environments. Recent progress in utilizing language models as high-level planners has demonstrated that the complexity of tasks can be reduced through decomposing them into primitive-level plans, making it possible to generalize on novel robotic tasks in a composable manner. Despite the promising future, the community is not yet adequately prepared for composable generalization agents, particularly due to the lack of primitive-level real-world robotic datasets. In this paper, we propose a primitive-level robotic dataset, namely RH20T-P, which contains about 33000 video clips covering 44 diverse and complicated robotic tasks. Each clip is manually annotated according to a set of meticulously designed primitive skills, facilitating the future development of composable generalization agents. To validate the effectiveness of RH20T-P, we also construct a potential and scalable agent based on RH20T-P, called RA-P. Equipped with two planners specialized in task decomposition and motion planning, RA-P can adapt to novel physical skills through composable generalization. Our website and videos can be found at https://sites.google.com/view/rh20t-primitive/main. Dataset and code will be made available soon.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "24 pages, 12 figures, 6 tables"
    },
    {
        "paper id": "2403.19639",
        "abstract url": "https://arxiv.org/abs/2403.19639",
        "title": "Linear Programming in Isabelle/HOL",
        "rating": -10,
        "keywords": [],
        "abstract": "Linear programming describes the problem of optimising a linear objective function over a set of constraints on its variables. In this paper we present a solver for linear programs implemented in the proof assistant Isabelle/HOL. This allows formally proving its soundness, termination, and other properties. We base these results on a previous formalisation of the simplex algorithm which does not take optimisation problems into account. Using the weak duality theorem of linear programming we obtain an algorithm for solving linear programs. Using Isabelle's code generation mechanism we can generate an external solver for linear programs.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    }
]