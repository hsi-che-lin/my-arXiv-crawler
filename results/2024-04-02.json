[
    {
        "paper id": "2404.01751",
        "abstract url": "https://arxiv.org/abs/2404.01751",
        "title": "T-VSL: Text-Guided Visual Sound Source Localization in Mixtures",
        "rating": 2.5,
        "keywords": [
            [
                "audio-visual"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios. The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization. To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures. Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables our framework to handle a flexible number of sources and exhibits promising zero-shot transferability to unseen classes during test time. Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Tech report. Accepted in CVPR-2024"
    },
    {
        "paper id": "2404.01819",
        "abstract url": "https://arxiv.org/abs/2404.01819",
        "title": "Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection",
        "rating": 2.5,
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In this paper, we address the limitations of the DETR-based semi-supervised object detection (SSOD) framework, particularly focusing on the challenges posed by the quality of object queries. In DETR-based SSOD, the one-to-one assignment strategy provides inaccurate pseudo-labels, while the one-to-many assignments strategy leads to overlapping predictions. These issues compromise training efficiency and degrade model performance, especially in detecting small or occluded objects. We introduce Sparse Semi-DETR, a novel transformer-based, end-to-end semi-supervised object detection solution to overcome these challenges. Sparse Semi-DETR incorporates a Query Refinement Module to enhance the quality of object queries, significantly improving detection capabilities for small and partially obscured objects. Additionally, we integrate a Reliable Pseudo-Label Filtering Module that selectively filters high-quality pseudo-labels, thereby enhancing detection accuracy and consistency. On the MS-COCO and Pascal VOC object detection benchmarks, Sparse Semi-DETR achieves a significant improvement over current state-of-the-art methods that highlight Sparse Semi-DETR's effectiveness in semi-supervised object detection, particularly in challenging scenarios involving small or partially obscured objects.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024"
    },
    {
        "paper id": "2404.02098",
        "abstract url": "https://arxiv.org/abs/2404.02098",
        "title": "BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition",
        "rating": 2.5,
        "keywords": [
            [
                "audio-visual"
            ],
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Self-supervision has recently shown great promise for learning visual and auditory speech representations from unlabelled data. In this work, we propose BRAVEn, an extension to the recent RAVEn method, which learns speech representations entirely from raw audio-visual data. Our modifications to RAVEn enable BRAVEn to achieve state-of-the-art results among self-supervised methods in various settings. Moreover, we observe favourable scaling behaviour by increasing the amount of unlabelled data well beyond other self-supervised works. In particular, we achieve 20.0% / 1.7% word error rate for VSR / ASR on the LRS3 test set, with only 30 hours of labelled data and no external ASR models. Our results suggest that readily available unlabelled audio-visual data can largely replace costly transcribed data.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ICASSP 2024. Code: https://github.com/ahaliassos/raven"
    },
    {
        "paper id": "2404.02132",
        "abstract url": "https://arxiv.org/abs/2404.02132",
        "title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era",
        "rating": 2.5,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024; https://github.com/Beckschen/ViTamin"
    },
    {
        "paper id": "2404.02145",
        "abstract url": "https://arxiv.org/abs/2404.02145",
        "title": "Iterated Learning Improves Compositionality in Large Vision-Language Models",
        "rating": 2.5,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, recent investigations find that most-if not all-our state-of-the-art vision-language models struggle at compositionality. They are unable to distinguish between images of \" a girl in white facing a man in black\" and \"a girl in black facing a man in white\". Moreover, prior work suggests that compositionality doesn't arise with scale: larger model sizes or training data don't help. This paper develops a new iterated training algorithm that incentivizes compositionality. We draw on decades of cognitive science research that identifies cultural transmission-the need to teach a new generation-as a necessary inductive prior that incentivizes humans to develop compositional languages. Specifically, we reframe vision-language contrastive learning as the Lewis Signaling Game between a vision agent and a language agent, and operationalize cultural transmission by iteratively resetting one of the agent's weights during training. After every iteration, this training paradigm induces representations that become \"easier to learn\", a property of compositional languages: e.g. our model trained on CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the SugarCrepe benchmark.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2404.01911",
        "abstract url": "https://arxiv.org/abs/2404.01911",
        "title": "VLRM: Vision-Language Models act as Reward Models for Image Captioning",
        "rating": 2,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to generate longer and more comprehensive descriptions. Our model reaches impressive 0.90 R@1 CLIP Recall score on MS-COCO Carpathy Test Split. Weights are available at https://huggingface.co/sashakunitsyn/vlrm-blip2-opt-2.7b.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01716",
        "abstract url": "https://arxiv.org/abs/2404.01716",
        "title": "Effective internal language model training and fusion for factorized transducer model",
        "rating": 1.5,
        "keywords": [
            [
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T baseline enhanced with external LM fusion, the proposed model yields a 5.5% relative improvement on general-sets and an 8.9% WER reduction for rare words. The proposed model can achieve superior performance without relying on external language models, rendering it highly efficient for production use-cases. To further improve the performance, we propose a novel and memory-efficient ILM-fusion-aware minimum word error rate (MWER) training method which improves ILM integration significantly.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted to ICASSP 2024"
    },
    {
        "paper id": "2404.01725",
        "abstract url": "https://arxiv.org/abs/2404.01725",
        "title": "Disentangled Pre-training for Human-Object Interaction Detection",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Detecting human-object interaction (HOI) has long been limited by the amount of supervised data available. Recent approaches address this issue by pre-training according to pseudo-labels, which align object regions with HOI triplets parsed from image captions. However, pseudo-labeling is tricky and noisy, making HOI pre-training a complex process. Therefore, we propose an efficient disentangled pre-training method for HOI detection (DP-HOI) to address this problem. First, DP-HOI utilizes object detection and action recognition datasets to pre-train the detection and interaction decoder layers, respectively. Then, we arrange these decoder layers so that the pre-training architecture is consistent with the downstream HOI detection task. This facilitates efficient knowledge transfer. Specifically, the detection decoder identifies reliable human instances in each action recognition dataset image, generates one corresponding query, and feeds it into the interaction decoder for verb classification. Next, we combine the human instance verb predictions in the same image and impose image-level supervision. The DP-HOI structure can be easily adapted to the HOI detection task, enabling effective model parameter initialization. Therefore, it significantly enhances the performance of existing HOI detection models on a broad range of rare categories. The code and pre-trained weight are available at https://github.com/xingaoli/DP-HOI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR2024"
    },
    {
        "paper id": "2404.01740",
        "abstract url": "https://arxiv.org/abs/2404.01740",
        "title": "Weakly-supervised Audio Separation via Bi-modal Semantic Similarity",
        "rating": 1.5,
        "keywords": [
            [
                "cs.SD"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-source samples in the target modality during training. We empirically show that this is well within reach if we have access to a pretrained joint embedding model between the two modalities (i.e., CLAP). Furthermore, we propose to incorporate our framework into two fundamental scenarios to enhance separation performance. First, we show that our proposed methodology significantly improves the performance of purely unsupervised baselines by reducing the distribution shift between training and test samples. In particular, we show that our framework can achieve 71% boost in terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5% of the supervised learning performance. Second, we show that we can further improve the performance of the supervised learning itself by 17% if we augment it by our proposed weakly-supervised framework, that enables a powerful semi-supervised framework for audio separation.",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Tech report. Accepted in ICLR-2024"
    },
    {
        "paper id": "2404.01775",
        "abstract url": "https://arxiv.org/abs/2404.01775",
        "title": "A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show that poor separation between incorrectly classified ID samples vs. OOD samples is an overlooked yet important limitation of existing methods. Code: https://github.com/glhr/ood-labelnoise",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2404.01945",
        "abstract url": "https://arxiv.org/abs/2404.01945",
        "title": "Event-assisted Low-Light Video Object Segmentation",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In the realm of video object segmentation (VOS), the challenge of operating under low-light conditions persists, resulting in notably degraded image quality and compromised accuracy when comparing query and memory frames for similarity computation. Event cameras, characterized by their high dynamic range and ability to capture motion information of objects, offer promise in enhancing object visibility and aiding VOS methods under such low-light conditions. This paper introduces a pioneering framework tailored for low-light VOS, leveraging event camera data to elevate segmentation accuracy. Our approach hinges on two pivotal components: the Adaptive Cross-Modal Fusion (ACMF) module, aimed at extracting pertinent features while fusing image and event modalities to mitigate noise interference, and the Event-Guided Memory Matching (EGMM) module, designed to rectify the issue of inaccurate matching prevalent in low-light settings. Additionally, we present the creation of a synthetic LLE-DAVIS dataset and the curation of a real-world LLE-VOS dataset, encompassing frames and events. Experimental evaluations corroborate the efficacy of our method across both datasets, affirming its effectiveness in low-light scenarios.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2404.01976",
        "abstract url": "https://arxiv.org/abs/2404.01976",
        "title": "Joint-Task Regularization for Partially Labeled Multi-Task Learning",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Multi-task learning has become increasingly popular in the machine learning field, but its practicality is hindered by the need for large, labeled datasets. Most multi-task learning methods depend on fully labeled datasets wherein each input example is accompanied by ground-truth labels for all target tasks. Unfortunately, curating such datasets can be prohibitively expensive and impractical, especially for dense prediction tasks which require per-pixel labels for each image. With this in mind, we propose Joint-Task Regularization (JTR), an intuitive technique which leverages cross-task relations to simultaneously regularize all tasks in a single joint-task latent space to improve learning when data is not fully labeled for all tasks. JTR stands out from existing approaches in that it regularizes all tasks jointly rather than separately in pairs -- therefore, it achieves linear complexity relative to the number of tasks while previous methods scale quadratically. To demonstrate the validity of our approach, we extensively benchmark our method across a wide variety of partially labeled scenarios based on NYU-v2, Cityscapes, and Taskonomy.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted paper to CVPR 2024 (main conference)"
    },
    {
        "paper id": "2404.02068",
        "abstract url": "https://arxiv.org/abs/2404.02068",
        "title": "Using Interpretation Methods for Model Enhancement",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in low-resource settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings. Code is available at https://github.com/Chord-Chen-30/UIMER.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EMNLP 2023"
    },
    {
        "paper id": "2404.02117",
        "abstract url": "https://arxiv.org/abs/2404.02117",
        "title": "Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners",
        "rating": 1.5,
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model to learn new classes incrementally without forgetting when only a few samples for each class are given. FSCIL encounters two significant challenges: catastrophic forgetting and overfitting, and these challenges have driven prior studies to primarily rely on shallow models, such as ResNet-18. Even though their limited capacity can mitigate both forgetting and overfitting issues, it leads to inadequate knowledge transfer during few-shot incremental sessions. In this paper, we argue that large models such as vision and language transformers pre-trained on large datasets can be excellent few-shot incremental learners. To this end, we propose a novel FSCIL framework called PriViLege, Pre-trained Vision and Language transformers with prompting functions and knowledge distillation. Our framework effectively addresses the challenges of catastrophic forgetting and overfitting in large models through new pre-trained knowledge tuning (PKT) and two losses: entropy-based divergence loss and semantic knowledge distillation loss. Experimental results show that the proposed PriViLege significantly outperforms the existing state-of-the-art methods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and +13.36% in miniImageNet. Our implementation code is available at https://github.com/KHU-AGI/PriViLege.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2404.01645",
        "abstract url": "https://arxiv.org/abs/2404.01645",
        "title": "ContrastCAD: Contrastive Learning-based Representation Learning for Computer-Aided Design Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The success of Transformer-based models has encouraged many researchers to learn CAD models using sequence-based approaches. However, learning CAD models is still a challenge, because they can be represented as complex shapes with long construction sequences. Furthermore, the same CAD model can be expressed using different CAD construction sequences. We propose a novel contrastive learning-based approach, named ContrastCAD, that effectively captures semantic information within the construction sequences of the CAD model. ContrastCAD generates augmented views using dropout techniques without altering the shape of the CAD model. We also propose a new CAD data augmentation method, called a Random Replace and Extrude (RRE) method, to enhance the learning performance of the model when training an imbalanced training CAD dataset. Experimental results show that the proposed RRE augmentation method significantly enhances the learning performance of Transformer-based autoencoders, even for complex CAD models having very long construction sequences. The proposed ContrastCAD model is shown to be robust to permutation changes of construction sequences and performs better representation learning by generating representation spaces where similar CAD models are more closely clustered. Our codes are available at https://github.com/cm8908/ContrastCAD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01651",
        "abstract url": "https://arxiv.org/abs/2404.01651",
        "title": "NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The use of words to convey speaker's intent is traditionally distinguished from the `mention' of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NAACL 2024 (Main conference)"
    },
    {
        "paper id": "2404.01652",
        "abstract url": "https://arxiv.org/abs/2404.01652",
        "title": "Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training. Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model's performance in its original corpus and domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024 Findings"
    },
    {
        "paper id": "2404.01663",
        "abstract url": "https://arxiv.org/abs/2404.01663",
        "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01667",
        "abstract url": "https://arxiv.org/abs/2404.01667",
        "title": "METAL: Towards Multilingual Meta-Evaluation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024 findings"
    },
    {
        "paper id": "2404.01669",
        "abstract url": "https://arxiv.org/abs/2404.01669",
        "title": "How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale Twitter Study Spanning Pre-COVID and Post-COVID Era",
        "rating": 1.0,
        "keywords": [
            [
                "cs.SI"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "The debate around vaccines has been going on for decades, but the COVID-19 pandemic showed how crucial it is to understand and mitigate anti-vaccine sentiments. While the pandemic may be over, it is still important to understand how the pandemic affected the anti-vaccine discourse, and whether the arguments against non-COVID vaccines (e.g., Flu, MMR, IPV, HPV vaccines) have also changed due to the pandemic. This study attempts to answer these questions through a large-scale study of anti-vaccine posts on Twitter. Almost all prior works that utilized social media to understand anti-vaccine opinions considered only the three broad stances of Anti-Vax, Pro-Vax, and Neutral. There has not been any effort to identify the specific reasons/concerns behind the anti-vax sentiments (e.g., side-effects, conspiracy theories, political reasons) on social media at scale. In this work, we propose two novel methods for classifying tweets into 11 different anti-vax concerns -- a discriminative approach (entailment-based) and a generative approach (based on instruction tuning of LLMs) -- which outperform several strong baselines. We then apply this classifier on anti-vaccine tweets posted over a 5-year period (Jan 2018 - Jan 2023) to understand how the COVID-19 pandemic has impacted the anti-vaccine concerns among the masses. We find that the pandemic has made the anti-vaccine discourse far more complex than in the pre-COVID times, and increased the variety of concerns being voiced. Alarmingly, we find that concerns about COVID vaccines are now being projected onto the non-COVID vaccines, thus making more people hesitant in taking vaccines in the post-COVID era.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "This work has been accepted to appear at the 18th International AAAI Conference on Web and Social Media (ICWSM), 2024"
    },
    {
        "paper id": "2404.01673",
        "abstract url": "https://arxiv.org/abs/2404.01673",
        "title": "A Universal Knowledge Embedded Contrastive Learning Framework for Hyperspectral Image Classification",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Hyperspectral image (HSI) classification techniques have been intensively studied and a variety of models have been developed. However, these HSI classification models are confined to pocket models and unrealistic ways of datasets partitioning. The former limits the generalization performance of the model and the latter is partitioned leads to inflated model evaluation metrics, which results in plummeting model performance in the real world. Therefore, we propose a universal knowledge embedded contrastive learning framework (KnowCL) for supervised, unsupervised, and semisupervised HSI classification, which largely closes the gap of HSI classification models between pocket models and standard vision backbones. We present a new HSI processing pipeline in conjunction with a range of data transformation and augmentation techniques that provide diverse data representations and realistic data partitioning. The proposed framework based on this pipeline is compatible with all kinds of backbones and can fully exploit labeled and unlabeled samples with expected training time. Furthermore, we design a new loss function, which can adaptively fuse the supervised loss and unsupervised loss, enhancing the learning performance. This proposed new classification paradigm shows great potentials in exploring for HSI classification technology. The code can be accessed at https://github.com/quanweiliu/KnowCL.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01676",
        "abstract url": "https://arxiv.org/abs/2404.01676",
        "title": "Incentives in Private Collaborative Machine Learning",
        "rating": 1.0,
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce differential privacy (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to the grand coalition's posterior. We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to NeurIPS 2023"
    },
    {
        "paper id": "2404.01699",
        "abstract url": "https://arxiv.org/abs/2404.01699",
        "title": "Task Integration Distillation for Object Detectors",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Knowledge distillation is a widely adopted technique for model lightening. However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory. Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task. This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection. Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation. This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01700",
        "abstract url": "https://arxiv.org/abs/2404.01700",
        "title": "MotionChain: Conversational Motion Controllers via Multimodal Prompts",
        "rating": 1,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in language models have demonstrated their adeptness in conducting multi-turn dialogues and retaining conversational context. However, this proficiency remains largely unexplored in other multimodal generative models, particularly in human motion models. By integrating multi-turn conversations in controlling continuous virtual human movements, generative human motion models can achieve an intuitive and step-by-step process of human task execution for humanoid robotics, game agents, or other embodied systems. In this work, we present MotionChain, a conversational human motion controller to generate continuous and long-term human motion through multimodal prompts. Specifically, MotionChain consists of multi-modal tokenizers that transform various data types such as text, image, and motion, into discrete tokens, coupled with a Vision-Motion-aware Language model. By leveraging large-scale language, vision-language, and vision-motion data to assist motion-related generation tasks, MotionChain thus comprehends each instruction in multi-turn conversation and generates human motions followed by these prompts. Extensive experiments validate the efficacy of MotionChain, demonstrating state-of-the-art performance in conversational motion generation, as well as more intuitive manners of controlling and interacting with virtual humans.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 4 figures"
    },
    {
        "paper id": "2404.01701",
        "abstract url": "https://arxiv.org/abs/2404.01701",
        "title": "On the Role of Summary Content Units in Text Summarization Evaluation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs). These SCUs are concise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages? ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategies to approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively. We find that while STUs and SMUs are competitive, the best approximation quality is achieved by SGUs. We also show through a simple sentence-decomposition baseline (SSUs) that SCUs (and their approximations) offer the most value when ranking short summaries, but may not help as much when ranking systems or longer summaries.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 Pages, 3 Figures, 3 Tables, camera ready version accepted at NAACL 2024"
    },
    {
        "paper id": "2404.01705",
        "abstract url": "https://arxiv.org/abs/2404.01705",
        "title": "Samba: Semantic Segmentation of Remotely Sensed Images with State Space Model",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "High-resolution remotely sensed images poses a challenge for commonly used semantic segmentation methods such as Convolutional Neural Network (CNN) and Vision Transformer (ViT). CNN-based methods struggle with handling such high-resolution images due to their limited receptive field, while ViT faces challenges to handle long sequences. Inspired by Mamba, which adopts a State Space Model (SSM) to efficiently capture global semantic information, we propose a semantic segmentation framework for high-resolution remotely sensed images, named Samba. Samba utilizes an encoder-decoder architecture, with Samba blocks serving as the encoder for efficient multi-level semantic information extraction, and UperNet functioning as the decoder. We evaluate Samba on the LoveDA dataset, comparing its performance against top-performing CNN and ViT methods. The results reveal that Samba achieved unparalleled performance on LoveDA. This represents that the proposed Samba is an effective application of the SSM in semantic segmentation of remotely sensed images, setting a new benchmark in performance for Mamba-based techniques in this specific application. The source code and baseline implementations are available at https://github.com/zhuqinfeng1999/Samba.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01706",
        "abstract url": "https://arxiv.org/abs/2404.01706",
        "title": "Polarity Calibration for Opinion Summarization",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarity Calibration model (PoCa) on two types of opinions summarization tasks: summarizing product reviews and political opinions articles. Automatic and human evaluation demonstrate that our approach can mitigate the polarity mismatch between output summary and input text, as well as maintain the content semantic and language quality.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024"
    },
    {
        "paper id": "2404.01715",
        "abstract url": "https://arxiv.org/abs/2404.01715",
        "title": "EMONA: Event-level Moral Opinions in News Articles",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Most previous research on moral frames has focused on social media short texts, little work has explored moral sentiment within news articles. In news articles, authors often express their opinions or political stance through moral judgment towards events, specifically whether the event is right or wrong according to social moral rules. This paper initiates a new task to understand moral opinions towards events in news articles. We have created a new dataset, EMONA, and annotated event-level moral opinions in news articles. This dataset consists of 400 news articles containing over 10k sentences and 45k events, among which 9,613 events received moral foundation labels. Extracting event morality is a challenging task, as moral judgment towards events can be very implicit. Baseline models were built for event moral identification and classification. In addition, we also conduct extrinsic evaluations to integrate event-level moral opinions into three downstream tasks. The statistical analysis and experiments show that moral opinions of events can serve as informative features for identifying ideological bias or subjective events.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024"
    },
    {
        "paper id": "2404.01737",
        "abstract url": "https://arxiv.org/abs/2404.01737",
        "title": "Transfer Learning from Whisper for Microscopic Intelligibility Prediction",
        "rating": 1,
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Macroscopic intelligibility models predict the expected human word-error-rate for a given speech-in-noise stimulus. In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners' perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use transfer learning from large scale deep learning models for speech processing, whereas such methods have rarely been used for microscopic modeling. In this paper, we study the use of transfer learning from Whisper, a state-of-the-art deep learning model for automatic speech recognition, for microscopic intelligibility prediction at the level of lexical responses. Our method outperforms the considered baselines, even in a zero-shot setup, and yields a relative improvement of up to 66\\% when fine-tuned to predict listeners' responses. Our results showcase the promise of large scale deep learning based methods for microscopic intelligibility prediction.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01743",
        "abstract url": "https://arxiv.org/abs/2404.01743",
        "title": "Atom-Level Optical Chemical Structure Recognition with Limited Supervision",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Identifying the chemical structure from a graphical representation, or image, of a molecule is a challenging pattern recognition task that would greatly benefit drug development. Yet, existing methods for chemical structure recognition do not typically generalize well, and show diminished effectiveness when confronted with domains where data is sparse, or costly to generate, such as hand-drawn molecule images. To address this limitation, we propose a new chemical structure recognition tool that delivers state-of-the-art performance and can adapt to new domains with a limited number of data samples and supervision. Unlike previous approaches, our method provides atom-level localization, and can therefore segment the image into the different atoms and bonds. Our model is the first model to perform OCSR with atom-level entity detection with only SMILES supervision. Through rigorous and extensive benchmarking, we demonstrate the preeminence of our chemical structure recognition approach in terms of data efficiency, accuracy, and atom-level entity prediction.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted in IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024"
    },
    {
        "paper id": "2404.01745",
        "abstract url": "https://arxiv.org/abs/2404.01745",
        "title": "Unleash the Potential of CLIP for Video Highlight Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal and large language models (LLMs) have revolutionized the utilization of open-world knowledge, unlocking novel potentials across various tasks and applications. Among these domains, the video domain has notably benefited from their capabilities. In this paper, we present Highlight-CLIP (HL-CLIP), a method designed to excel in the video highlight detection task by leveraging the pre-trained knowledge embedded in multimodal models. By simply fine-tuning the multimodal encoder in combination with our innovative saliency pooling technique, we have achieved the state-of-the-art performance in the highlight detection task, the QVHighlight Benchmark, to the best of our knowledge.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01753",
        "abstract url": "https://arxiv.org/abs/2404.01753",
        "title": "M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01768",
        "abstract url": "https://arxiv.org/abs/2404.01768",
        "title": "Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, LLMs could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including SHAP, LIME, and BertViz, and analyse a series of example cases discussing the results. Finally, we develop a series of stereotype elicitation prompts and evaluate the presence of stereotypes in text generation tasks with popular LLMs, using one of our best performing previously presented stereotypes detectors. Our experiments yielded several key findings: i) Training stereotype detectors in a multi-dimension setting yields better results than training multiple single-dimension classifiers.ii) The integrated MGS Dataset enhances both the in-dataset and cross-dataset generalisation ability of stereotype detectors compared to using the datasets separately. iii) There is a reduction in stereotypes in the content generated by GPT Family LLMs with newer versions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under reviewed as a conference paper at COLM 2024"
    },
    {
        "paper id": "2404.01785",
        "abstract url": "https://arxiv.org/abs/2404.01785",
        "title": "Can Humans Identify Domains?",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in *TGeGUM*: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annotations, allowing us to examine the level of human disagreement and the relative difficulty of each annotation task. With a Fleiss' kappa of at most 0.53 on the sentence level and 0.66 at the prose level, it is evident that despite the ubiquity of domains in NLP, there is little human consensus on how to define them. By training classifiers to perform the same task, we find that this uncertainty also extends to NLP models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at LREC-COLING 2024"
    },
    {
        "paper id": "2404.01786",
        "abstract url": "https://arxiv.org/abs/2404.01786",
        "title": "Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k sampling, top-p sampling, contrastive searching, and locally typical searching, this work has provided valuable insights into the strengths, weaknesses, and potential applications of each method. Each text-generating method is evaluated using several standard metrics and a comparative study has been made on the performance of the approaches. Finally, some future directions of research in the field of automatic text generation are also identified.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "This report pertains to the Capstone Project done by Group 5 of the Fall batch of 2023 students at Praxis Tech School, Kolkata, India. The reports consists of 57 pages and it includes 17 figures and 8 tables. This is the preprint which will be submitted to IEEE CONIT 2024 for review"
    },
    {
        "paper id": "2404.01842",
        "abstract url": "https://arxiv.org/abs/2404.01842",
        "title": "Semi-Supervised Domain Adaptation for Wildfire Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, both the frequency and intensity of wildfires have increased worldwide, primarily due to climate change. In this paper, we propose a novel protocol for wildfire detection, leveraging semi-supervised Domain Adaptation for object detection, accompanied by a corresponding dataset designed for use by both academics and industries. Our dataset encompasses 30 times more diverse labeled scenes for the current largest benchmark wildfire dataset, HPWREN, and introduces a new labeling policy for wildfire detection. Inspired by CoordConv, we propose a robust baseline, Location-Aware Object Detection for Semi-Supervised Domain Adaptation (LADA), utilizing a teacher-student based framework capable of extracting translational variance features characteristic of wildfires. With only using 1% target domain labeled data, our framework significantly outperforms our source-only baseline by a notable margin of 3.8% in mean Average Precision on the HPWREN wildfire dataset. Our dataset is available at https://github.com/BloomBerry/LADA.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 5 figures, 22 tables"
    },
    {
        "paper id": "2404.01854",
        "abstract url": "https://arxiv.org/abs/2404.01854",
        "title": "IndoCulture: Exploring Geographically-Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Although commonsense reasoning is greatly shaped by cultural and geographical factors, previous studies on language models have predominantly centered on English cultures, potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture, aimed at understanding the influence of geographical factors on language model reasoning ability, with a specific emphasis on the diverse cultures found within eleven Indonesian provinces. In contrast to prior works that relied on templates (Yin et al., 2022) and online scrapping (Fung et al., 2024), we created IndoCulture by asking local people to manually develop the context and plausible options based on predefined topics. Evaluations of 23 language models reveal several insights: (1) even the best open-source model struggles with an accuracy of 53.2%, (2) models often provide more accurate predictions for specific provinces, such as Bali and West Java, and (3) the inclusion of location contexts enhances performance, especially in larger models like GPT-4, emphasizing the significance of geographical context in commonsense reasoning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01856",
        "abstract url": "https://arxiv.org/abs/2404.01856",
        "title": "Poro 34B and the Blessing of Multilinguality",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at https://huggingface.co/LumiOpen/Poro-34B.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01860",
        "abstract url": "https://arxiv.org/abs/2404.01860",
        "title": "Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders Learn More With Less",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents two simple improvements to the Self-Structuring AutoEncoder (Self-StrAE). Firstly, we show that including reconstruction to the vocabulary as an auxiliary objective improves representation quality. Secondly, we demonstrate that increasing the number of independent channels leads to significant improvements in embedding quality, while simultaneously reducing the number of parameters. Surprisingly, we demonstrate that this trend can be followed to the extreme, even to point of reducing the total number of non-embedding parameters to seven. Our system can be pre-trained from scratch with as little as 10M tokens of input data, and proves effective across English, Spanish and Afrikaans.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "SemEval 2024"
    },
    {
        "paper id": "2404.01882",
        "abstract url": "https://arxiv.org/abs/2404.01882",
        "title": "Scene Adaptive Sparse Transformer for Event-based Object Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks, their high computational costs still diminish the low power consumption advantage of event cameras. Image-based works attempt to reduce these costs by introducing sparse Transformers. However, they display inadequate sparsity and adaptability when applied to event-based object detection, since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers, leading to reduced performance and efficiency. Furthermore, they lack scene-specific sparsity optimization, resulting in information loss and a lower recall rate. To overcome these limitations, we propose the Scene Adaptive Sparse Transformer (SAST). SAST enables window-token co-sparsification, significantly enhancing fault tolerance and reducing computational overhead. Leveraging the innovative scoring and selection modules, along with the Masked Sparse Window Self-Attention, SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity, maintaining a remarkable balance between performance and computational cost. The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1). Code: https://github.com/Peterande/SAST",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01891",
        "abstract url": "https://arxiv.org/abs/2404.01891",
        "title": "ASTRA: An Action Spotting TRAnsformer for Soccer Videos",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce ASTRA, a Transformer-based model designed for the task of Action Spotting in soccer matches. ASTRA addresses several challenges inherent in the task and dataset, including the requirement for precise action localization, the presence of a long-tail data distribution, non-visibility in certain actions, and inherent label noise. To do so, ASTRA incorporates (a) a Transformer encoder-decoder architecture to achieve the desired output temporal resolution and to produce precise predictions, (b) a balanced mixup strategy to handle the long-tail distribution of the data, (c) an uncertainty-aware displacement head to capture the label variability, and (d) input audio signal to enhance detection of non-visible actions. Results demonstrate the effectiveness of ASTRA, achieving a tight Average-mAP of 66.82 on the test set. Moreover, in the SoccerNet 2023 Action Spotting challenge, we secure the 3rd position with an Average-mAP of 70.21 on the challenge set.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01892",
        "abstract url": "https://arxiv.org/abs/2404.01892",
        "title": "Minimize Quantization Output Error with Bias Compensation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Quantization is a promising method that reduces memory usage and computational intensity of Deep Neural Networks (DNNs), but it often leads to significant output error that hinder model deployment. In this paper, we propose Bias Compensation (BC) to minimize the output error, thus realizing ultra-low-precision quantization without model fine-tuning. Instead of optimizing the non-convex quantization process as in most previous methods, the proposed BC bypasses the step to directly minimize the quantizing output error by identifying a bias vector for compensation. We have established that the minimization of output error through BC is a convex problem and provides an efficient strategy to procure optimal solutions associated with minimal output error,without the need for training or fine-tuning. We conduct extensive experiments on Vision Transformer models and Large Language Models, and the results show that our method notably reduces quantization output error, thereby permitting ultra-low-precision post-training quantization and enhancing the task performance of models. Especially, BC improves the accuracy of ViT-B with 4-bit PTQ4ViT by 36.89% on the ImageNet-1k task, and decreases the perplexity of OPT-350M with 3-bit GPTQ by 5.97 on WikiText2.The code is in https://github.com/GongCheng1919/bias-compensation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2404.01903",
        "abstract url": "https://arxiv.org/abs/2404.01903",
        "title": "Activation Steering for Robust Type Prediction in CodeLLMs",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90% of type mispredictions. Finally, we show that steering vectors calculated from Python activations reliably correct type mispredictions in TypeScript, and vice versa. This result suggests that LLMs may be learning to transfer knowledge of types across programming languages.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "16 pages, 7 figures"
    },
    {
        "paper id": "2404.01914",
        "abstract url": "https://arxiv.org/abs/2404.01914",
        "title": "SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations. To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants. SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources. We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities. Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a novel self-distillation method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties. Our approach demonstrates competitive performance on the NER benchmark and surpasses existing methods on both MNER and GMNER benchmarks. Further analysis shows that the proposed distillation and knowledge utilization methods improve the performance of our model on various benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages, 7 figures, NAACL 2024"
    },
    {
        "paper id": "2404.01921",
        "abstract url": "https://arxiv.org/abs/2404.01921",
        "title": "A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL-24 Main"
    },
    {
        "paper id": "2404.01923",
        "abstract url": "https://arxiv.org/abs/2404.01923",
        "title": "SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates \"skeleton heuristics\", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input. Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by NAACL 2024 Findings"
    },
    {
        "paper id": "2404.01924",
        "abstract url": "https://arxiv.org/abs/2404.01924",
        "title": "Toward Efficient Visual Gyroscopes: Spherical Moments, Harmonics Filtering, and Masking Techniques for Spherical Camera Applications",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Unlike a traditional gyroscope, a visual gyroscope estimates camera rotation through images. The integration of omnidirectional cameras, offering a larger field of view compared to traditional RGB cameras, has proven to yield more accurate and robust results. However, challenges arise in situations that lack features, have substantial noise causing significant errors, and where certain features in the images lack sufficient strength, leading to less precise prediction results. Here, we address these challenges by introducing a novel visual gyroscope, which combines an analytical method with a neural network approach to provide a more efficient and accurate rotation estimation from spherical images. The presented method relies on three key contributions: an adapted analytical approach to compute the spherical moments coefficients, introduction of masks for better global feature representation, and the use of a multilayer perceptron to adaptively choose the best combination of masks and filters. Experimental results demonstrate superior performance of the proposed approach in terms of accuracy. The paper emphasizes the advantages of integrating machine learning to optimize analytical solutions, discusses limitations, and suggests directions for future research.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Submitted to 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)"
    },
    {
        "paper id": "2404.01940",
        "abstract url": "https://arxiv.org/abs/2404.01940",
        "title": "Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2404.01948",
        "abstract url": "https://arxiv.org/abs/2404.01948",
        "title": "Quantifying Noise of Dynamic Vision Sensor",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Dynamic visual sensors (DVS) are characterized by a large amount of background activity (BA) noise, which it is mixed with the original (cleaned) sensor signal. The dynamic nature of the signal and the absence in practical application of the ground truth, it clearly makes difficult to distinguish between noise and the cleaned sensor signals using standard image processing techniques. In this letter, a new technique is presented to characterise BA noise derived from the Detrended Fluctuation Analysis (DFA). The proposed technique can be used to address an existing DVS issues, which is how to quantitatively characterised noise and signal without ground truth, and how to derive an optimal denoising filter parameters. The solution of the latter problem is demonstrated for the popular real moving-car dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 4 figures, submitted to the IEEE Signal Processing Letters"
    },
    {
        "paper id": "2404.01952",
        "abstract url": "https://arxiv.org/abs/2404.01952",
        "title": "Automatic Wood Pith Detector: Local Orientation Estimation and Robust Accumulation",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "A fully automated technique for wood pith detection (APD), relying on the concentric shape of the structure of wood ring slices, is introduced. The method estimates the ring's local orientations using the 2D structure tensor and finds the pith position, optimizing a cost function designed for this problem. We also present a variant (APD-PCL), using the parallel coordinates space, that enhances the method's effectiveness when there are no clear tree ring patterns. Furthermore, refining previous work by Kurdthongmee, a YoloV8 net is trained for pith detection, producing a deep learning-based approach to the same problem (APD-DL). All methods were tested on seven datasets, including images captured under diverse conditions (controlled laboratory settings, sawmill, and forest) and featuring various tree species (Pinus taeda, Douglas fir, Abies alba, and Gleditsia triacanthos). All proposed approaches outperform existing state-of-the-art methods and can be used in CPU-based real-time applications. Additionally, we provide a novel dataset comprising images of gymnosperm and angiosperm species. Dataset and source code are available at http://github.com/hmarichal93/apd.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "18 pages, presented to ICPR 2024 conference"
    },
    {
        "paper id": "2404.01953",
        "abstract url": "https://arxiv.org/abs/2404.01953",
        "title": "Classifying Graphemes in English Words Through the Application of a Fuzzy Inference System",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In Linguistics, a grapheme is a written unit of a writing system corresponding to a phonological sound. In Natural Language Processing tasks, written language is analysed through two different mediums, word analysis, and character analysis. This paper focuses on a third approach, the analysis of graphemes. Graphemes have advantages over word and character analysis by being self-contained representations of phonetic sounds. Due to the nature of splitting a word into graphemes being based on complex, non-binary rules, the application of fuzzy logic would provide a suitable medium upon which to predict the number of graphemes in a word. This paper proposes the application of a Fuzzy Inference System to split words into their graphemes. This Fuzzy Inference System results in a correct prediction of the number of graphemes in a word 50.18% of the time, with 93.51% being within a margin of +- 1 from the correct classification. Given the variety in language, graphemes are tied with pronunciation and therefore can change depending on a regional accent/dialect, the +- 1 accuracy represents the impreciseness of grapheme classification when regional variances are accounted for. To give a baseline of comparison, a second method involving a recursive IPA mapping exercise using a pronunciation dictionary was developed to allow for comparisons to be made.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01954",
        "abstract url": "https://arxiv.org/abs/2404.01954",
        "title": "HyperCLOVA X Technical Report",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "44 pages"
    },
    {
        "paper id": "2404.01959",
        "abstract url": "https://arxiv.org/abs/2404.01959",
        "title": "Bi-LORA: A Vision-Language Approach for Synthetic Image Detection",
        "rating": 1,
        "keywords": [
            [
                "Vision-Language",
                "VLM"
            ],
            [
                "diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01961",
        "abstract url": "https://arxiv.org/abs/2404.01961",
        "title": "Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to SemEval@NAACL 2024"
    },
    {
        "paper id": "2404.01964",
        "abstract url": "https://arxiv.org/abs/2404.01964",
        "title": "CAM-Based Methods Can See through Walls",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "25 pages, 9 figures"
    },
    {
        "paper id": "2404.01988",
        "abstract url": "https://arxiv.org/abs/2404.01988",
        "title": "Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection",
        "rating": 1,
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Unsupervised Domain Adaptation (UDA) has shown significant advancements in object detection under well-lit conditions; however, its performance degrades notably in low-visibility scenarios, especially at night, posing challenges not only for its adaptability in low signal-to-noise ratio (SNR) conditions but also for the reliability and efficiency of automated vehicles. To address this problem, we propose a \\textbf{Co}operative \\textbf{S}tudents (\\textbf{CoS}) framework that innovatively employs global-local transformations (GLT) and a proxy-based target consistency (PTC) mechanism to capture the spatial consistency in day- and night-time scenarios effectively, and thus bridge the significant domain shift across contexts. Building upon this, we further devise an adaptive IoU-informed thresholding (AIT) module to gradually avoid overlooking potential true positives and enrich the latent information in the target domain. Comprehensive experiments show that CoS essentially enhanced UDA performance in low-visibility conditions and surpasses current state-of-the-art techniques, achieving an increase in mAP of 3.0\\%, 1.9\\%, and 2.5\\% on BDD100K, SHIFT, and ACDC datasets, respectively. Code is available at https://github.com/jichengyuan/Cooperitive_Students.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code is available at https://github.com/jichengyuan/Cooperitive_Students"
    },
    {
        "paper id": "2404.01992",
        "abstract url": "https://arxiv.org/abs/2404.01992",
        "title": "Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted for NAACL 2024"
    },
    {
        "paper id": "2404.02000",
        "abstract url": "https://arxiv.org/abs/2404.02000",
        "title": "Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\\%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To appear in AfricaNLP 2024"
    },
    {
        "paper id": "2404.02009",
        "abstract url": "https://arxiv.org/abs/2404.02009",
        "title": "Preuve de concept d'un bot vocal dialoguant en wolof",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal. This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal. The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech. The voicebot receives in input the customer's oral request that is then processed by a SLU system to reply to the customer's request using audio recordings. The first results of this proof-of-concept are encouraging as we achieved 22\\% of WER for the ASR task and 78\\% of F1-score on the NLU task.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "in French language"
    },
    {
        "paper id": "2404.02022",
        "abstract url": "https://arxiv.org/abs/2404.02022",
        "title": "Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02037",
        "abstract url": "https://arxiv.org/abs/2404.02037",
        "title": "MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection -- ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. Then, we experiment with different text detoxification models -- from unsupervised baselines to LLMs and fine-tuned models on the presented parallel corpora -- showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL2024"
    },
    {
        "paper id": "2404.02043",
        "abstract url": "https://arxiv.org/abs/2404.02043",
        "title": "Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference -- providing the \"recipe\" for the optimal setups.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02053",
        "abstract url": "https://arxiv.org/abs/2404.02053",
        "title": "BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments. Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextual aspects of market sentiment. The integration of advanced NLP techniques like BERTopic with traditional financial analysis methods marks a step forward in developing more sophisticated tools for understanding and predicting market behaviors.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02054",
        "abstract url": "https://arxiv.org/abs/2404.02054",
        "title": "Deconstructing In-Context Learning: Understanding Prompts via Corruption",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The ability of large language models (LLMs) to \"learn in context\" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models ($\\geq$30B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to LREC-COLING 2024 main conference. The code is available at https://github.com/text-machine-lab/Understanding_prompts_via_corruption"
    },
    {
        "paper id": "2404.02060",
        "abstract url": "https://arxiv.org/abs/2404.02060",
        "title": "Long-context LLMs Struggle with Long In-context Learning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02063",
        "abstract url": "https://arxiv.org/abs/2404.02063",
        "title": "SPMamba: State-space model is all you need in speech separation",
        "rating": 1,
        "keywords": [
            [
                "cs.SD"
            ]
        ],
        "abstract": "In speech separation, both CNN- and Transformer-based models have demonstrated robust separation capabilities, garnering significant attention within the research community. However, CNN-based methods have limited modelling capability for long-sequence audio, leading to suboptimal separation performance. Conversely, Transformer-based methods are limited in practical applications due to their high computational complexity. Notably, within computer vision, Mamba-based methods have been celebrated for their formidable performance and reduced computational requirements. In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba. We adopt the TF-GridNet model as the foundational framework and substitute its Transformer component with a bidirectional Mamba module, aiming to capture a broader range of contextual information. Our experimental results reveal an important role in the performance aspects of Mamba-based models. SPMamba demonstrates superior performance with a significant advantage over existing separation models in a dataset built on Librispeech. Notably, SPMamba achieves a substantial improvement in separation quality, with a 2.42 dB enhancement in SI-SNRi compared to the TF-GridNet. The source code for SPMamba is publicly accessible at https://github.com/JusperLee/SPMamba .",
        "subjects": [
            "cs.SD"
        ],
        "comment": "Technical Report. Work in progress. Code is available at https://github.com/JusperLee/SPMamba"
    },
    {
        "paper id": "2404.02088",
        "abstract url": "https://arxiv.org/abs/2404.02088",
        "title": "LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF layer to try to model the inter-dependencies between adjacent utterances more effectively. In the official leaderboard for the task, our architecture was ranked 8th, achieving an F1-score of 0.1759 on the leaderboard.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02103",
        "abstract url": "https://arxiv.org/abs/2404.02103",
        "title": "CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG. CLAPNQ is publicly available at https://github.com/primeqa/clapnq",
        "subjects": [
            "cs.CL"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2404.02109",
        "abstract url": "https://arxiv.org/abs/2404.02109",
        "title": "The Effects of Group Sanctions on Participation and Toxicity: Quasi-experimental Evidence from the Fediverse",
        "rating": 1.0,
        "keywords": [
            [
                "cs.SI"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Online communities often overlap and coexist, despite incongruent norms and approaches to content moderation. When communities diverge, decentralized and federated communities may pursue group-level sanctions, including defederation (disconnection) to block communication between members of specific communities. We investigate the effects of defederation in the context of the Fediverse, a set of decentralized, interconnected social networks with independent governance. Mastodon and Pleroma, the most popular software powering the Fediverse, allow administrators on one server to defederate from another. We use a difference-in-differences approach and matched controls to estimate the effects of defederation events on participation and message toxicity among affected members of the blocked and blocking servers. We find that defederation causes a drop in activity for accounts on the blocked servers, but not on the blocking servers. Also, we find no evidence of an effect of defederation on message toxicity.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "13 pages, 8 figures; Accepted to the 18th International AAAI Conference on Web and Social Media"
    },
    {
        "paper id": "2404.02124",
        "abstract url": "https://arxiv.org/abs/2404.02124",
        "title": "Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NAACL 2024 findings"
    },
    {
        "paper id": "2404.02126",
        "abstract url": "https://arxiv.org/abs/2404.02126",
        "title": "Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1--5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To be published in NAACL24 proceedings"
    },
    {
        "paper id": "2404.02127",
        "abstract url": "https://arxiv.org/abs/2404.02127",
        "title": "FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning",
        "rating": 1,
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02157",
        "abstract url": "https://arxiv.org/abs/2404.02157",
        "title": "Segment Any 3D Object with Language",
        "rating": 1,
        "keywords": [
            [
                "visual-language"
            ],
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions. Earlier works that rely on only annotated base categories for training suffer from limited generalization to unseen novel categories. Recent works mitigate poor generalizability to novel categories by generating class-agnostic masks or projecting generalized masks from 2D to 3D, but disregard semantic or geometry information, leading to sub-optimal performance. Instead, generating generalizable but semantic-related masks directly from 3D point clouds would result in superior outcomes. In this paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a semantic and geometric-aware visual-language learning framework with strong generalizability by generating semantic-related masks directly from 3D point clouds. Specifically, we propose a multimodal fusion network to incorporate multimodal semantics in both backbone and decoder. In addition, to align the 3D segmentation model with various language instructions and enhance the mask quality, we introduce three types of multimodal associations as supervision. Our SOLE outperforms previous methods by a large margin on ScanNetv2, ScanNet200, and Replica benchmarks, and the results are even close to the fully-supervised counterpart despite the absence of class annotations in the training. Furthermore, extensive qualitative results demonstrate the versatility of our SOLE to language instructions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://cvrp-sole.github.io"
    },
    {
        "paper id": "2404.01650",
        "abstract url": "https://arxiv.org/abs/2404.01650",
        "title": "Test-Time Model Adaptation with Only Forward Passes",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Only Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C. The source code will be released.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "17 pages, 3 figures, 14 tables"
    },
    {
        "paper id": "2404.01677",
        "abstract url": "https://arxiv.org/abs/2404.01677",
        "title": "Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system outperforms previous works by achieving state-of-the-art performances in complex scenarios while maintaining performances in simple scenarios. Besides, we observe that GFaiR is faithful to its reasoning process.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Coling 2024"
    },
    {
        "paper id": "2404.01692",
        "abstract url": "https://arxiv.org/abs/2404.01692",
        "title": "Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss",
        "rating": 0.5,
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In real-world scenarios, image recognition tasks, such as semantic segmentation and object detection, often pose greater challenges due to the lack of information available within low-resolution (LR) content. Image super-resolution (SR) is one of the promising solutions for addressing the challenges. However, due to the ill-posed property of SR, it is challenging for typical SR methods to restore task-relevant high-frequency contents, which may dilute the advantage of utilizing the SR method. Therefore, in this paper, we propose Super-Resolution for Image Recognition (SR4IR) that effectively guides the generation of SR images beneficial to achieving satisfactory image recognition performance when processing LR images. The critical component of our SR4IR is the task-driven perceptual (TDP) loss that enables the SR network to acquire task-specific knowledge from a network tailored for a specific task. Moreover, we propose a cross-quality patch mix and an alternate training framework that significantly enhances the efficacy of the TDP loss by addressing potential problems when employing the TDP loss. Through extensive experiments, we demonstrate that our SR4IR achieves outstanding task performance by generating SR images useful for a specific image recognition task, including semantic segmentation, object detection, and image classification. The implementation code is available at https://github.com/JaehaKim97/SR4IR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2404.01714",
        "abstract url": "https://arxiv.org/abs/2404.01714",
        "title": "Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "32 pages, 13 figures"
    },
    {
        "paper id": "2404.01730",
        "abstract url": "https://arxiv.org/abs/2404.01730",
        "title": "Asymptotics of Language Model Alignment",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\u03c6$ that results in a higher expected reward while keeping $\u03c6$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\u03c6_\u0394$ that maximizes $E_{\u03c6_\u0394} r(y)$ subject to a relative entropy constraint $KL(\u03c6_\u0394|| p) \\leq \u0394.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in terms of relative entropy. To further analyze the properties of alignment methods, we introduce two simplifying assumptions: we let the language model be memoryless, and the reward model be linear. Although these assumptions may not reflect complex real-world scenarios, they enable a precise characterization of the asymptotic behavior of both the best-of-$N$ alignment, and the KL-constrained RL method, in terms of information-theoretic quantities. We prove that the reward of the optimal KL-constrained RL solution satisfies a large deviation principle, and we fully characterize its rate function. We also show that the rate of growth of the scaled cumulants of the reward is characterized by a proper Renyi cross entropy. Finally, we show that best-of-$N$ is asymptotically equivalent to KL-constrained RL solution by proving that their expected rewards are asymptotically equal, and concluding that the two distributions must be close in KL divergence.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01794",
        "abstract url": "https://arxiv.org/abs/2404.01794",
        "title": "Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Autonomous and learning systems based on Deep Reinforcement Learning have firmly established themselves as a foundation for approaches to creating resilient and efficient Cyber-Physical Energy Systems. However, most current approaches suffer from two distinct problems: Modern model-free algorithms such as Soft Actor Critic need a high number of samples to learn a meaningful policy, as well as a fallback to ward against concept drifts (e. g., catastrophic forgetting). In this paper, we present the work in progress towards a hybrid agent architecture that combines model-based Deep Reinforcement Learning with imitation learning to overcome both problems.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Accepted as publication at MSCPES '24"
    },
    {
        "paper id": "2404.01804",
        "abstract url": "https://arxiv.org/abs/2404.01804",
        "title": "Neuromorphic Wireless Device-Edge Co-Inference via the Directed Information Bottleneck",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "An important use case of next-generation wireless systems is device-edge co-inference, where a semantic task is partitioned between a device and an edge server. The device carries out data collection and partial processing of the data, while the remote server completes the given task based on information received from the device. It is often required that processing and communication be run as efficiently as possible at the device, while more computing resources are available at the edge. To address such scenarios, we introduce a new system solution, termed neuromorphic wireless device-edge co-inference. According to it, the device runs sensing, processing, and communication units using neuromorphic hardware, while the server employs conventional radio and computing technologies. The proposed system is designed using a transmitter-centric information-theoretic criterion that targets a reduction of the communication overhead, while retaining the most relevant information for the end-to-end semantic task of interest. Numerical results on standard data sets validate the proposed architecture, and a preliminary testbed realization is reported.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2404.01805",
        "abstract url": "https://arxiv.org/abs/2404.01805",
        "title": "Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Emotion detection in textual data has received growing interest in recent years, as it is pivotal for developing empathetic human-computer interaction systems. This paper introduces a method for categorizing emotions from text, which acknowledges and differentiates between the diversified similarities and distinctions of various emotions. Initially, we establish a baseline by training a transformer-based model for standard emotion classification, achieving state-of-the-art performance. We argue that not all misclassifications are of the same importance, as there are perceptual similarities among emotional classes. We thus redefine the emotion labeling problem by shifting it from a traditional classification model to an ordinal classification one, where discrete emotions are arranged in a sequential order according to their valence levels. Finally, we propose a method that performs ordinal classification in the two-dimensional emotion space, considering both valence and arousal scales. The results show that our approach not only preserves high accuracy in emotion prediction but also significantly reduces the magnitude of errors in cases of misclassification.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01847",
        "abstract url": "https://arxiv.org/abs/2404.01847",
        "title": "Accelerating Transformer Pre-Training with 2:4 Sparsity",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Training large Transformers is slow, but recent innovations on GPU architecture gives us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of Transformers in pre-training. First, we define a \"flip rate\" to monitor the stability of a 2:4 training process. Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model's quality by a simple yet effective dense fine-tuning procedure near the end of pre-training. Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that a combination of our methods reaches the best performance on multiple Transformers among different 2:4 training methods, while actual acceleration can be observed on different shapes of Transformer block.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01853",
        "abstract url": "https://arxiv.org/abs/2404.01853",
        "title": "Pairwise Similarity Distribution Clustering for Noisy Label Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Noisy label learning aims to train deep neural networks using a large amount of samples with noisy labels, whose main challenge comes from how to deal with the inaccurate supervision caused by wrong labels. Existing works either take the label correction or sample selection paradigm to involve more samples with accurate labels into the training process. In this paper, we propose a simple yet effective sample selection algorithm, termed as Pairwise Similarity Distribution Clustering~(PSDC), to divide the training samples into one clean set and another noisy set, which can power any of the off-the-shelf semi-supervised learning regimes to further train networks for different downstream tasks. Specifically, we take the pairwise similarity between sample pairs to represent the sample structure, and the Gaussian Mixture Model~(GMM) to model the similarity distribution between sample pairs belonging to the same noisy cluster, therefore each sample can be confidently divided into the clean set or noisy set. Even under severe label noise rate, the resulting data partition mechanism has been proved to be more robust in judging the label confidence in both theory and practice. Experimental results on various benchmark datasets, such as CIFAR-10, CIFAR-100 and Clothing1M, demonstrate significant improvements over state-of-the-art methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01857",
        "abstract url": "https://arxiv.org/abs/2404.01857",
        "title": "Detecting Gender Bias in Course Evaluations",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "An outtake from the findnings of a master thesis studying gender bias in course evaluations through the lense of machine learning and nlp. We use different methods to examine and explore the data and find differences in what students write about courses depending on gender of the examiner. Data from English and Swedish courses are evaluated and compared, in order to capture more nuance in the gender bias that might be found. Here we present the results from the work so far, but this is an ongoing project and there is more work to do.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Presented at the Swedish Language Technology Conference 2022. 5 pages, 2 figures, 5 tables"
    },
    {
        "paper id": "2404.01862",
        "abstract url": "https://arxiv.org/abs/2404.01862",
        "title": "Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model",
        "rating": 0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "22 pages, 8 figures, CVPR 2024"
    },
    {
        "paper id": "2404.01877",
        "abstract url": "https://arxiv.org/abs/2404.01877",
        "title": "Procedural Fairness in Machine Learning",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fairness in machine learning (ML) has received much attention. However, existing studies have mainly focused on the distributive fairness of ML models. The other dimension of fairness, i.e., procedural fairness, has been neglected. In this paper, we first define the procedural fairness of ML models, and then give formal definitions of individual and group procedural fairness. We propose a novel metric to evaluate the group procedural fairness of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets. Our experiments reveal the relationship between procedural and distributive fairness of the ML model. Based on our analysis, we propose a method for identifying the features that lead to the procedural unfairness of the model and propose two methods to improve procedural fairness after identifying unfair features. Our experimental results demonstrate that we can accurately identify the features that lead to procedural unfairness in the ML model, and both of our proposed methods can significantly improve procedural fairness with a slight impact on model performance, while also improving distributive fairness.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2404.01930",
        "abstract url": "https://arxiv.org/abs/2404.01930",
        "title": "Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in active learning as well as many other domains. We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage. We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them. Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee. In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining active learning guarantees that do not depend on the smallest probability in the prior. Moreover, we discover a new parameter of adaptive selection policies, which we term the \"maximal gain ratio\". We show that this parameter is strictly less restrictive than the greedy approximation parameter that has been used in previous approximation guarantees, and show that it can be used to provide stronger approximation guarantees than previous results. In particular, we show that the maximal gain ratio is never larger than the greedy approximation factor of a policy, and that it can be considerably smaller. This provides a new insight into the properties that make a policy useful for adaptive combinatorial maximization.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ALT 2024"
    },
    {
        "paper id": "2404.01936",
        "abstract url": "https://arxiv.org/abs/2404.01936",
        "title": "Settling Time vs. Accuracy Tradeoffs for Clustering Big Data",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the theoretical and practical runtime limits of k-means and k-median clustering on large datasets. Since effectively all clustering methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the clustering on the compressed representation. Unfortunately, there is no universal best choice for compressing the number of points - while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow. Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the dataset size. We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time - within log-factors of the time it takes to read the data. Any approach that significantly improves on this must then resort to practical heuristics, leading us to consider the spectrum of sampling strategies across both real and artificial datasets in the static and streaming settings. Through this, we show the conditions in which coresets are necessary for preserving cluster validity as well as the settings in which faster, cruder sampling strategies are sufficient. As a result, we provide a comprehensive theoretical and practical blueprint for effective clustering regardless of data size. Our code is publicly available and has scripts to recreate the experiments.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01943",
        "abstract url": "https://arxiv.org/abs/2404.01943",
        "title": "Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation",
        "rating": 0.5,
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "3D"
            ],
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024. The code is available at https://github.com/MrZihan/HNR-VLN"
    },
    {
        "paper id": "2404.01958",
        "abstract url": "https://arxiv.org/abs/2404.01958",
        "title": "MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Human activity recognition (HAR) will be an essential function of various emerging applications. However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements. In this work, we propose MESEN, a multimodal-empowered unimodal sensing framework, to utilize unlabeled multimodal data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase. From a study on the impact of supervised multimodal fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the multimodal-aided pre-training stage. With the proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning, MESEN exploits unlabeled multimodal data to extract effective unimodal features for each modality. Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples. Extensive experiments on eight public multimodal datasets demonstrate that MESEN achieves significant performance improvements over state-of-the-art baselines in enhancing unimodal HAR by exploiting multimodal data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to the 21th ACM Conference on Embedded Networked Sensor Systems (SenSys 2023)"
    },
    {
        "paper id": "2404.01965",
        "abstract url": "https://arxiv.org/abs/2404.01965",
        "title": "Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets. However, the computational demands of DL models pose environmental and resource challenges. Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference. Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques. We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption. Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization. Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\\% in accuracy and low computational cost. Overall, our method accelerates efficient model development while enabling sustainable AI applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02039",
        "abstract url": "https://arxiv.org/abs/2404.02039",
        "title": "A Survey on Large Language Model-Based Game Agents",
        "rating": 0.5,
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting & exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02041",
        "abstract url": "https://arxiv.org/abs/2404.02041",
        "title": "SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation",
        "rating": 0.5,
        "keywords": [
            [
                "3d"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present a new self-supervised approach, SelfPose3d, for estimating 3d poses of multiple persons from multiple camera views. Unlike current state-of-the-art fully-supervised methods, our approach does not require any 2d or 3d ground-truth poses and uses only the multi-view input images from a calibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2d human pose estimator. We propose two self-supervised learning objectives: self-supervised person localization in 3d space and self-supervised 3d pose estimation. We achieve self-supervised 3d person localization by training the model on synthetically generated 3d points, serving as 3d person root positions, and on the projected root-heatmaps in all the views. We then model the 3d poses of all the localized persons with a bottleneck representation, map them onto all views obtaining 2d joints, and render them using 2d Gaussian heatmaps in an end-to-end differentiable manner. Afterwards, we use the corresponding 2d joints and heatmaps from the pseudo 2d poses for learning. To alleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptive supervision attention mechanism to guide the self-supervision. Our experiments and analysis on three public benchmark datasets, including Panoptic, Shelf, and Campus, show the effectiveness of our approach, which is comparable to fully-supervised methods. Code is available at \\url{https://github.com/CAMMA-public/SelfPose3D}",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted for CVPR 2024"
    },
    {
        "paper id": "2404.02047",
        "abstract url": "https://arxiv.org/abs/2404.02047",
        "title": "Universal representations for financial transactional data: embracing local, global, and external contexts",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\\% for the next MCC prediction task and up to 46\\% for downstream tasks from existing contrastive baselines. Incorporating external information improves the scores by an additional 20\\%.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02058",
        "abstract url": "https://arxiv.org/abs/2404.02058",
        "title": "Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02067",
        "abstract url": "https://arxiv.org/abs/2404.02067",
        "title": "Red-Teaming Segment Anything Model",
        "rating": 0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications. The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts. We not only show the effectiveness of popular white-box attacks and resistance to black-box attacks but also introduce a novel approach - Focused Iterative Gradient Attack (FIGA) that combines white-box approaches to construct an efficient attack resulting in a smaller number of modified pixels. All of our testing methods and analyses indicate a need for enhanced safety measures in foundation models for image segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024 - The 4th Workshop of Adversarial Machine Learning on Computer Vision: Robustness of Foundation Models"
    },
    {
        "paper id": "2404.02072",
        "abstract url": "https://arxiv.org/abs/2404.02072",
        "title": "EGTR: Extracting Graph from Transformer for Scene Graph Generation",
        "rating": 0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively according to the quality of the detected objects. By the relation smoothing, the model is trained according to the continuous curriculum that focuses on object detection task at the beginning of training and performs multi-task learning as the object detection performance gradually improves. Furthermore, we propose a connectivity prediction task that predicts whether a relation exists between object pairs as an auxiliary task of the relation extraction. We demonstrate the effectiveness and efficiency of our method for the Visual Genome and Open Image V6 datasets. Our code is publicly available at https://github.com/naver-ai/egtr .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2404.02108",
        "abstract url": "https://arxiv.org/abs/2404.02108",
        "title": "Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average Reward Markov Decision Processes",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present two Policy Gradient-based methods with general parameterization in the context of infinite horizon average reward Markov Decision Processes. The first approach employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{3/5})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the state of the art of the problem, which achieves a regret of $\\tilde{\\mathcal{O}}(T^{3/4})$.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "34 pages"
    },
    {
        "paper id": "2404.02112",
        "abstract url": "https://arxiv.org/abs/2404.02112",
        "title": "ImageNot: A contrast with ImageNet preserves model rankings",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects. We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet. This is true when training models from scratch or fine-tuning them. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. We further give evidence that ImageNot has a similar utility as ImageNet for transfer learning purposes. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02113",
        "abstract url": "https://arxiv.org/abs/2404.02113",
        "title": "Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL",
        "rating": 0.5,
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform poorly when restricted to one-percent tuning, whereas several algorithmic mitigations designed to maintain network plasticity perform surprising well. In addition, we find that properties designed to measure the network's ability to learn continually indeed correlate with performance under one-percent tuning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02155",
        "abstract url": "https://arxiv.org/abs/2404.02155",
        "title": "Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in Neural Radiance Fields",
        "rating": 0.5,
        "keywords": [
            [
                "3D",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance fields, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invariance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance. We revisit a few popular radiance field models and find that these systems use various heuristics to deal with issues arising from scene scaling. We test their behaviors and show our recipe to be more robust.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance"
    },
    {
        "paper id": "2404.01657",
        "abstract url": "https://arxiv.org/abs/2404.01657",
        "title": "Release of Pre-Trained Models for the Japanese Language",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI. Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, 1 figure, 5 tables, accepted for LREC-COLING 2024. Models are publicly available at https://huggingface.co/rinna"
    },
    {
        "paper id": "2404.01690",
        "abstract url": "https://arxiv.org/abs/2404.01690",
        "title": "RefQSR: Reference-based Quantization for Image Super-Resolution Networks",
        "rating": 0,
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Single image super-resolution (SISR) aims to reconstruct a high-resolution image from its low-resolution observation. Recent deep learning-based SISR models show high performance at the expense of increased computational costs, limiting their use in resource-constrained environments. As a promising solution for computationally efficient network design, network quantization has been extensively studied. However, existing quantization methods developed for SISR have yet to effectively exploit image self-similarity, which is a new direction for exploration in this study. We introduce a novel method called reference-based quantization for image super-resolution (RefQSR) that applies high-bit quantization to several representative patches and uses them as references for low-bit quantization of the rest of the patches in an image. To this end, we design dedicated patch clustering and reference-based quantization modules and integrate them into existing SISR network quantization methods. The experimental results demonstrate the effectiveness of RefQSR on various SISR networks and quantization methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IEEE Transactions on Image Processing (TIP)"
    },
    {
        "paper id": "2404.01703",
        "abstract url": "https://arxiv.org/abs/2404.01703",
        "title": "Boosting Visual Recognition for Autonomous Driving in Real-world Degradations with Deep Channel Prior",
        "rating": 0,
        "keywords": [
            [
                "Autonomous Driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The environmental perception of autonomous vehicles in normal conditions have achieved considerable success in the past decade. However, various unfavourable conditions such as fog, low-light, and motion blur will degrade image quality and pose tremendous threats to the safety of autonomous driving. That is, when applied to degraded images, state-of-the-art visual models often suffer performance decline due to the feature content loss and artifact interference caused by statistical and structural properties disruption of captured images. To address this problem, this work proposes a novel Deep Channel Prior (DCP) for degraded visual recognition. Specifically, we observe that, in the deep representation space of pre-trained models, the channel correlations of degraded features with the same degradation type have uniform distribution even if they have different content and semantics, which can facilitate the mapping relationship learning between degraded and clear representations in high-sparsity feature space. Based on this, a novel plug-and-play Unsupervised Feature Enhancement Module (UFEM) is proposed to achieve unsupervised feature correction, where the multi-adversarial mechanism is introduced in the first stage of UFEM to achieve the latent content restoration and artifact removal in high-sparsity feature space. Then, the generated features are transferred to the second stage for global correlation modulation under the guidance of DCP to obtain high-quality and recognition-friendly features. Evaluations of three tasks and eight benchmark datasets demonstrate that our proposed method can comprehensively improve the performance of pre-trained models in real degradation conditions. The source code is available at https://github.com/liyuhang166/Deep_Channel_Prior",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01709",
        "abstract url": "https://arxiv.org/abs/2404.01709",
        "title": "Upsample Guidance: Scale Up Diffusion Models without Training",
        "rating": 0,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion models. We also observed that the proper selection of guidance scale can improve image quality, fidelity, and prompt alignment.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "15 pages, 15 Figures"
    },
    {
        "paper id": "2404.01717",
        "abstract url": "https://arxiv.org/abs/2404.01717",
        "title": "AddSR: Accelerating Diffusion-based Blind Super-Resolution with Adversarial Diffusion Distillation",
        "rating": 0,
        "keywords": [
            [
                "Diffusion",
                "text-to-image",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Blind super-resolution methods based on stable diffusion showcase formidable generative capabilities in reconstructing clear high-resolution images with intricate details from low-resolution inputs. However, their practical applicability is often hampered by poor efficiency, stemming from the requirement of thousands or hundreds of sampling steps. Inspired by the efficient text-to-image approach adversarial diffusion distillation (ADD), we design AddSR to address this issue by incorporating the ideas of both distillation and ControlNet. Specifically, we first propose a prediction-based self-refinement strategy to provide high-frequency information in the student model output with marginal additional time cost. Furthermore, we refine the training process by employing HR images, rather than LR images, to regulate the teacher model, providing a more robust constraint for distillation. Second, we introduce a timestep-adapting loss to address the perception-distortion imbalance problem introduced by ADD. Extensive experiments demonstrate our AddSR generates better restoration results, while achieving faster speed than previous SD-based state-of-the-art models (e.g., 7x faster than SeeSR).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01720",
        "abstract url": "https://arxiv.org/abs/2404.01720",
        "title": "Self-Improvement Programming for Temporal Knowledge Graph Question Answering",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given. Then, it aligns these drafts to TKGs with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by LREC-COLING 2024 (long paper)"
    },
    {
        "paper id": "2404.01722",
        "abstract url": "https://arxiv.org/abs/2404.01722",
        "title": "Sentence-level Media Bias Analysis with Event Relation Graph",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Media outlets are becoming more partisan and polarized nowadays. In this paper, we identify media bias at the sentence level, and pinpoint bias sentences that intend to sway readers' opinions. As bias sentences are often expressed in a neutral and factual way, considering broader context outside a sentence can help reveal the bias. In particular, we observe that events in a bias sentence need to be understood in associations with other events in the document. Therefore, we propose to construct an event relation graph to explicitly reason about event-event relations for sentence-level bias identification. The designed event relation graph consists of events as nodes and four common types of event relations: coreference, temporal, causal, and subevent relations. Then, we incorporate event relation graph for bias sentences identification in two steps: an event-aware language model is built to inject the events and event relations knowledge into the basic language model via soft labels; further, a relation-aware graph attention network is designed to update sentence embedding with events and event relations information based on hard labels. Experiments on two benchmark datasets demonstrate that our approach with the aid of event relation graph improves both precision and recall of bias sentence identification.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NAACL 2024"
    },
    {
        "paper id": "2404.01758",
        "abstract url": "https://arxiv.org/abs/2404.01758",
        "title": "GEARS: Local Geometry-aware Hand-object Interaction Synthesis",
        "rating": 0,
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating realistic hand motion sequences in interaction with objects has gained increasing attention with the growing interest in digital humans. Prior work has illustrated the effectiveness of employing occupancy-based or distance-based virtual sensors to extract hand-object interaction features. Nonetheless, these methods show limited generalizability across object categories, shapes and sizes. We hypothesize that this is due to two reasons: 1) the limited expressiveness of employed virtual sensors, and 2) scarcity of available training data. To tackle this challenge, we introduce a novel joint-centered sensor designed to reason about local object geometry near potential interaction regions. The sensor queries for object surface points in the neighbourhood of each hand joint. As an important step towards mitigating the learning complexity, we transform the points from global frame to hand template frame and use a shared module to process sensor features of each individual joint. This is followed by a spatio-temporal transformer network aimed at capturing correlation among the joints in different dimensions. Moreover, we devise simple heuristic rules to augment the limited training sequences with vast static hand grasping samples. This leads to a broader spectrum of grasping types observed during training, in turn enhancing our model's generalization capability. We evaluate on two public datasets, GRAB and InterCap, where our method shows superiority over baselines both quantitatively and perceptually.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01767",
        "abstract url": "https://arxiv.org/abs/2404.01767",
        "title": "Class-Incremental Few-Shot Event Detection",
        "rating": 0,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism. Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of Prompt-KD.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01816",
        "abstract url": "https://arxiv.org/abs/2404.01816",
        "title": "Rethinking Annotator Simulation: Realistic Evaluation of Whole-Body PET Lesion Interactive Segmentation Methods",
        "rating": 0,
        "keywords": [
            [
                "robot"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Interactive segmentation plays a crucial role in accelerating the annotation, particularly in domains requiring specialized expertise such as nuclear medicine. For example, annotating lesions in whole-body Positron Emission Tomography (PET) images can require over an hour per volume. While previous works evaluate interactive segmentation models through either real user studies or simulated annotators, both approaches present challenges. Real user studies are expensive and often limited in scale, while simulated annotators, also known as robot users, tend to overestimate model performance due to their idealized nature. To address these limitations, we introduce four evaluation metrics that quantify the user shift between real and simulated annotators. In an initial user study involving four annotators, we assess existing robot users using our proposed metrics and find that robot users significantly deviate in performance and annotation behavior compared to real annotators. Based on these findings, we propose a more realistic robot user that reduces the user shift by incorporating human factors such as click variation and inter-annotator disagreement. We validate our robot user in a second user study, involving four other annotators, and show it consistently reduces the simulated-to-real user shift compared to traditional robot users. By employing our robot user, we can conduct more large-scale and cost-efficient evaluations of interactive segmentation models, while preserving the fidelity of real user studies. Our implementation is based on MONAI Label and will be made publicly available.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "10 pages, 5 figures, 1 table"
    },
    {
        "paper id": "2404.01843",
        "abstract url": "https://arxiv.org/abs/2404.01843",
        "title": "Sketch3D: Style-Consistent Guidance for Sketch-to-3D Generation",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, image-to-3D approaches have achieved significant results with a natural image as input. However, it is not always possible to access these enriched color input samples in practical applications, where only sketches are available. Existing sketch-to-3D researches suffer from limitations in broad applications due to the challenges of lacking color information and multi-view content. To overcome them, this paper proposes a novel generation paradigm Sketch3D to generate realistic 3D assets with shape aligned with the input sketch and color matching the textual description. Concretely, Sketch3D first instantiates the given sketch in the reference image through the shape-preserving generation process. Second, the reference image is leveraged to deduce a coarse 3D Gaussian prior, and multi-view style-consistent guidance images are generated based on the renderings of the 3D Gaussians. Finally, three strategies are designed to optimize 3D Gaussians, i.e., structural optimization via a distribution transfer mechanism, color optimization with a straightforward MSE loss and sketch similarity optimization with a CLIP-based geometric similarity loss. Extensive visual comparisons and quantitative analysis illustrate the advantage of our Sketch3D in generating realistic 3D assets while preserving consistency with the input.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01869",
        "abstract url": "https://arxiv.org/abs/2404.01869",
        "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey",
        "rating": 0,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on genuine reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "26 pages, 2 figures"
    },
    {
        "paper id": "2404.01932",
        "abstract url": "https://arxiv.org/abs/2404.01932",
        "title": "Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks",
        "rating": 0,
        "keywords": [
            [
                "vision-language"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 5 figures, 2 tables, conference"
    },
    {
        "paper id": "2404.01941",
        "abstract url": "https://arxiv.org/abs/2404.01941",
        "title": "LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human pose and shape (HPS) estimation with lensless imaging is not only beneficial to privacy protection but also can be used in covert surveillance scenarios due to the small size and simple structure of this device. However, this task presents significant challenges due to the inherent ambiguity of the captured measurements and lacks effective methods for directly estimating human pose and shape from lensless data. In this paper, we propose the first end-to-end framework to recover 3D human poses and shapes from lensless measurements to our knowledge. We specifically design a multi-scale lensless feature decoder to decode the lensless measurements through the optically encoded mask for efficient feature extraction. We also propose a double-head auxiliary supervision mechanism to improve the estimation accuracy of human limb ends. Besides, we establish a lensless imaging system and verify the effectiveness of our method on various datasets acquired by our lensless imaging system.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01984",
        "abstract url": "https://arxiv.org/abs/2404.01984",
        "title": "Fashion Style Editing with Generative Human Prior",
        "rating": 0,
        "keywords": [
            [
                "Image editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image editing has been a long-standing challenge in the research community with its far-reaching impact on numerous applications. Recently, text-driven methods started to deliver promising results in domains like human faces, but their applications to more complex domains have been relatively limited. In this work, we explore the task of fashion style editing, where we aim to manipulate the fashion style of human imagery using text descriptions. Specifically, we leverage a generative human prior and achieve fashion style editing by navigating its learned latent space. We first verify that the existing text-driven editing methods fall short for our problem due to their overly simplified guidance signal, and propose two directions to reinforce the guidance: textual augmentation and visual referencing. Combined with our empirical findings on the latent space structure, our Fashion Style Editing framework (FaSE) successfully projects abstract fashion concepts onto human images and introduces exciting new applications to the field.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2404.01994",
        "abstract url": "https://arxiv.org/abs/2404.01994",
        "title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
        "rating": 0,
        "keywords": [
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by LREC-COLING 2024"
    },
    {
        "paper id": "2404.01995",
        "abstract url": "https://arxiv.org/abs/2404.01995",
        "title": "A discussion about violin reduction: geometric analysis of contour lines and channel of minima",
        "rating": 0,
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Some early violins have been reduced during their history to fit imposed morphological standards, while more recent ones have been built directly to these standards. We can observe differences between reduced and unreduced instruments, particularly in their contour lines and channel of minima. In a recent preliminary work, we computed and highlighted those two features for two instruments using triangular 3D meshes acquired by photogrammetry, whose fidelity has been assessed and validated with sub-millimetre accuracy. We propose here an extension to a corpus of 38 violins, violas and cellos, and introduce improved procedures, leading to a stronger discussion of the geometric analysis. We first recall the material we are working with. We then discuss how to derive the best reference plane for the violin alignment, which is crucial for the computation of contour lines and channel of minima. Finally, we show how to compute efficiently both characteristics and we illustrate our results with a few examples.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Paper accepted (before reviewing) for the Florence Heri-Tech 2024 Conference"
    },
    {
        "paper id": "2404.02013",
        "abstract url": "https://arxiv.org/abs/2404.02013",
        "title": "Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces",
        "rating": 0,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Online gender-based harassment is a widespread issue limiting the free expression and participation of women and marginalized genders in digital spaces. Detecting such abusive content can enable platforms to curb this menace. We participated in the Gendered Abuse Detection in Indic Languages shared task at ICON2023 that provided datasets of annotated Twitter posts in English, Hindi and Tamil for building classifiers to identify gendered abuse. Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM networks that can effectively model semantic and sequential patterns in textual data. The CNN captures localized features indicative of abusive language through its convolution filters applied on embedded input text. To determine context-based offensiveness, the BiLSTM analyzes this sequence for dependencies among words and phrases. Multiple variations were trained using FastText and GloVe word embeddings for each language dataset comprising over 7,600 crowdsourced annotations across labels for explicit abuse, targeted minority attacks and general offences. The validation scores showed strong performance across f1-measures, especially for English 0.84. Our experiments reveal how customizing embeddings and model hyperparameters can improve detection capability. The proposed architecture ranked 1st in the competition, proving its ability to handle real-world noisy text with code-switching. This technique has a promising scope as platforms aim to combat cyber harassment facing Indic language internet users. Our Code is at https://github.com/advaithavetagiri/CNLP-NITS-PP",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02046",
        "abstract url": "https://arxiv.org/abs/2404.02046",
        "title": "Causality-based Transfer of Driving Scenarios to Unseen Intersections",
        "rating": 0,
        "keywords": [
            [
                "automated driving",
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Scenario-based testing of automated driving functions has become a promising method to reduce time and cost compared to real-world testing. In scenario-based testing automated functions are evaluated in a set of pre-defined scenarios. These scenarios provide information about vehicle behaviors, environmental conditions, or road characteristics using parameters. To create realistic scenarios, parameters and parameter dependencies have to be fitted utilizing real-world data. However, due to the large variety of intersections and movement constellations found in reality, data may not be available for certain scenarios. This paper proposes a methodology to systematically analyze relations between parameters of scenarios. Bayesian networks are utilized to analyze causal dependencies in order to decrease the amount of required data and to transfer causal patterns creating unseen scenarios. Thereby, infrastructural influences on movement patterns are investigated to generate realistic scenarios on unobserved intersections. For evaluation, scenarios and underlying parameters are extracted from the inD dataset. Movement patterns are estimated, transferred and checked against recorded data from those initially unseen intersections.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "6 pages, 8 figures, 1 table, Accepted to be published as part of the 35th IEEE Intelligent Vehicles Symposium, June 2 - 5, 2024, Korea"
    },
    {
        "paper id": "2404.02052",
        "abstract url": "https://arxiv.org/abs/2404.02052",
        "title": "Noise Masking Attacks and Defenses for Pretrained Speech Models",
        "rating": 0.0,
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond ASR models, to attack pretrained speech encoders. Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "accepted to ICASSP 2024"
    },
    {
        "paper id": "2404.02065",
        "abstract url": "https://arxiv.org/abs/2404.02065",
        "title": "Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation",
        "rating": 0,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semi-supervised semantic segmentation relieves the reliance on large-scale labeled data by leveraging unlabeled data. Recent semi-supervised semantic segmentation approaches mainly resort to pseudo-labeling methods to exploit unlabeled data. However, unreliable pseudo-labeling can undermine the semi-supervision processes. In this paper, we propose an algorithm called Multi-Level Label Correction (MLLC), which aims to use graph neural networks to capture structural relationships in Semantic-Level Graphs (SLGs) and Class-Level Graphs (CLGs) to rectify erroneous pseudo-labels. Specifically, SLGs represent semantic affinities between pairs of pixel features, and CLGs describe classification consistencies between pairs of pixel labels. With the support of proximate pattern information from graphs, MLLC can rectify incorrectly predicted pseudo-labels and can facilitate discriminative feature representations. We design an end-to-end network to train and perform this effective label corrections mechanism. Experiments demonstrate that MLLC can significantly improve supervised baselines and outperforms state-of-the-art approaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets. Specifically, MLLC improves the supervised baseline by at least 5% and 2% with DeepLabV2 and DeepLabV3+ respectively under different partition protocols.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 8 figures. IEEE Transactions on Multimedia, 2024"
    },
    {
        "paper id": "2404.02115",
        "abstract url": "https://arxiv.org/abs/2404.02115",
        "title": "GINopic: Topic Modeling with Graph Isomorphism Network",
        "rating": 0,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted as a long paper for NAACL 2024 main conference"
    },
    {
        "paper id": "2404.01642",
        "abstract url": "https://arxiv.org/abs/2404.01642",
        "title": "ADVREPAIR:Provable Repair of Adversarial Attack",
        "rating": -0.5,
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to adversarial attacks poses serious safety risks. Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of adversarial attack mechanisms, while adversarial training, leveraging a large number of adversarial samples to enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a novel approach for provable repair of adversarial attacks using limited data. By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood. Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against adversarial attacks to generalize to other inputs. ADVREPAIR demonstrates superior efficiency, scalability and repair success rate. Different from existing DNN repair methods, our repair can generalize to general inputs, thereby improving the robustness of the neural network globally, which indicates a significant breakthrough in the generalization capability of ADVREPAIR.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01686",
        "abstract url": "https://arxiv.org/abs/2404.01686",
        "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
        "rating": -0.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "robot",
                "navigation"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2404.01695",
        "abstract url": "https://arxiv.org/abs/2404.01695",
        "title": "Selective Temporal Knowledge Graph Reasoning",
        "rating": -0.5,
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts in the form of (subject, relation, object, timestamp), has attracted much attention recently. TKG reasoning aims to predict future facts based on given historical ones. However, existing TKG reasoning models are unable to abstain from predictions they are uncertain, which will inevitably bring risks in real-world applications. Thus, in this paper, we propose an abstention mechanism for TKG reasoning, which helps the existing models make selective, instead of indiscriminate, predictions. Specifically, we develop a confidence estimator, called Confidence Estimator with History (CEHis), to enable the existing TKG reasoning models to first estimate their confidence in making predictions, and then abstain from those with low confidence. To do so, CEHis takes two kinds of information into consideration, namely, the certainty of the current prediction and the accuracy of historical predictions. Experiments with representative TKG reasoning models on two benchmark datasets demonstrate the effectiveness of the proposed CEHis.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01712",
        "abstract url": "https://arxiv.org/abs/2404.01712",
        "title": "Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics",
        "rating": -0.5,
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the proposed method significantly reduces the unlearning runtime. Experimental studies demonstrate that the proposed scheme surpasses existing results by orders of magnitude in terms of time and memory costs, while also enhancing accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "24 pages, 8 figures"
    },
    {
        "paper id": "2404.01727",
        "abstract url": "https://arxiv.org/abs/2404.01727",
        "title": "Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge",
        "rating": -0.5,
        "keywords": [
            [
                "6-DoF"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We focus on the generalization ability of the 6-DoF grasp detection method in this paper. While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures. To enhance the grasp detection methods' generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation to objects with significant shape and structure differences. More specifically, we employ the physical constraint regularization during the training phase to guide the model towards predicting grasps that comply with the physical rule on grasping. For the unstable grasp poses predicted on novel objects, we design a contact-score joint optimization using the projection contact map to refine these poses in cluttered scenarios. Extensive experiments conducted on the GraspNet-1billion benchmark demonstrate a substantial performance gain on the novel object set and the real-world grasping experiments also demonstrate the effectiveness of our generalizing 6-DoF grasp detection method.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2404.01769",
        "abstract url": "https://arxiv.org/abs/2404.01769",
        "title": "Unifying Qualitative and Quantitative Safety Verification of DNN-Controlled Systems",
        "rating": -0.5,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapid advance of deep reinforcement learning techniques enables the oversight of safety-critical systems through the utilization of Deep Neural Networks (DNNs). This underscores the pressing need to promptly establish certified safety guarantees for such DNN-controlled systems. Most of the existing verification approaches rely on qualitative approaches, predominantly employing reachability analysis. However, qualitative verification proves inadequate for DNN-controlled systems as their behaviors exhibit stochastic tendencies when operating in open and adversarial environments. In this paper, we propose a novel framework for unifying both qualitative and quantitative safety verification problems of DNN-controlled systems. This is achieved by formulating the verification tasks as the synthesis of valid neural barrier certificates (NBCs). Initially, the framework seeks to establish almost-sure safety guarantees through qualitative verification. In cases where qualitative verification fails, our quantitative verification method is invoked, yielding precise lower and upper bounds on probabilistic safety across both infinite and finite time horizons. To facilitate the synthesis of NBCs, we introduce their $k$-inductive variants. We also devise a simulation-guided approach for training NBCs, aiming to achieve tightness in computing precise certified lower and upper bounds. We prototype our approach into a tool called $\\textsf{UniQQ}$ and showcase its efficacy on four classic DNN-controlled systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This work is a technical report for the paper with the same name to appear in the 36th International Conference on Computer Aided Verification (CAV 2024)"
    },
    {
        "paper id": "2404.01822",
        "abstract url": "https://arxiv.org/abs/2404.01822",
        "title": "A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection",
        "rating": -0.5,
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learners trained with our proposed few-shot subgraph sampling outperform standard community models in the inductive setup. We make our code publicly available.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "To be published at Findings of NAACL 2024"
    },
    {
        "paper id": "2404.01828",
        "abstract url": "https://arxiv.org/abs/2404.01828",
        "title": "Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay",
        "rating": -0.5,
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks have demonstrated susceptibility to adversarial attacks. Adversarial defense techniques often focus on one-shot setting to maintain robustness against attack. However, new attacks can emerge in sequences in real-world deployment scenarios. As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks. In this paper, we discuss for the first time the concept of continual adversarial defense under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic \\& Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks. (2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics for further replay constraints and potential future attacks. (3) A straightforward regularizer mitigates the 'plasticity-stability' trade-off by aligning model output between new and old tasks. Experiment results demonstrate that AIR can approximate or even exceed the empirical performance upper bounds achieved by Joint Training.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01933",
        "abstract url": "https://arxiv.org/abs/2404.01933",
        "title": "PREGO: online mistake detection in PRocedural EGOcentric videos",
        "rating": -0.5,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Promptly identifying procedural errors from egocentric videos in an online setting is highly challenging and valuable for detecting mistakes as soon as they happen. This capability has a wide range of applications across various fields, such as manufacturing and healthcare. The nature of procedural mistakes is open-set since novel types of failures might occur, which calls for one-class classifiers trained on correctly executed procedures. However, no technique can currently detect open-set procedural mistakes online. We propose PREGO, the first online one-class classification model for mistake detection in PRocedural EGOcentric videos. PREGO is based on an online action recognition component to model the current action, and a symbolic reasoning module to predict the next actions. Mistake detection is performed by comparing the recognized current action with the expected future one. We evaluate PREGO on two procedural egocentric video datasets, Assembly101 and Epic-tent, which we adapt for online benchmarking of procedural mistake detection to establish suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets, respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2404.01998",
        "abstract url": "https://arxiv.org/abs/2404.01998",
        "title": "Specularity Factorization for Low-Light Enhancement",
        "rating": -0.5,
        "keywords": [
            [
                "dehazing",
                "Low-Light Enhancement"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition. Our model-driven {\\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned. The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion. Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision. Our system improves the state-of-the-art performance on standard benchmarks and achieves better generalization on multiple other datasets. We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and dehazing with negligible overhead thereby highlighting the multi-domain and multi-task generalizability of our proposed RSFNet. The code and data is released for reproducibility on the project homepage.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024, Pages: 8(main)+4(references)+17(supp) = 29"
    },
    {
        "paper id": "2404.02003",
        "abstract url": "https://arxiv.org/abs/2404.02003",
        "title": "AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design",
        "rating": -0.5,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical. Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02078",
        "abstract url": "https://arxiv.org/abs/2404.02078",
        "title": "Advancing LLM Reasoning Generalists with Preference Trees",
        "rating": -0.5,
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Models and data are available at https://github.com/OpenBMB/Eurus"
    },
    {
        "paper id": "2404.01643",
        "abstract url": "https://arxiv.org/abs/2404.01643",
        "title": "A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection",
        "rating": -1,
        "keywords": [
            [
                "CT"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models. (2) CT-scan contains large number of out-of-distribution (OOD) slices. The crucial features may only be present in specific spatial regions and slices of the entire CT scan. How can we effectively figure out where these are located? To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan. It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally. Meanwhile, we proposed Kernel-Density-based slice Sampling (KDS) method to improve the stability when training and inference stage, therefore speeding up the rate of convergence and boosting performance. As a result, the experiments demonstrate the promising performance of our model using a simple EfficientNet-2D (E2D) model, even with only 1% of the training data. The efficacy of our approach has been validated on the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop, in conjunction with CVPR 2024. Our source code will be made available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Submitted to DEF-AI-MIA workshop. arXiv admin note: text overlap with arXiv:2403.11230"
    },
    {
        "paper id": "2404.01654",
        "abstract url": "https://arxiv.org/abs/2404.01654",
        "title": "AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in Parkinson's Disease",
        "rating": -1,
        "keywords": [
            [
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Parkinson's Disease (PD) is the second most common neurodegenerative disorder. The existing assessment method for PD is usually the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to assess the severity of various types of motor symptoms and disease progression. However, manual assessment suffers from high subjectivity, lack of consistency, and high cost and low efficiency of manual communication. We want to use a computer vision based solution to capture human pose images based on a camera, reconstruct and perform motion analysis using algorithms, and extract the features of the amount of motion through feature engineering. The proposed approach can be deployed on different smartphones, and the video recording and artificial intelligence analysis can be done quickly and easily through our APP.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Technical report for AI WALKUP, an APP winning 3rd Prize of 2022 HUST GS AI Innovation and Design Competition"
    },
    {
        "paper id": "2404.01655",
        "abstract url": "https://arxiv.org/abs/2404.01655",
        "title": "FashionEngine: Interactive Generation and Editing of 3D Clothed Humans",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present FashionEngine, an interactive 3D human generation and editing system that allows us to design 3D digital humans in a way that aligns with how humans interact with the world, such as natural languages, visual perceptions, and hand-drawing. FashionEngine automates the 3D human production with three key components: 1) A pre-trained 3D human diffusion model that learns to model 3D humans in a semantic UV latent space from 2D image training data, which provides strong priors for diverse generation and editing tasks. 2) Multimodality-UV Space encoding the texture appearance, shape topology, and textual semantics of human clothing in a canonical UV-aligned space, which faithfully aligns the user multimodal inputs with the implicit UV latent space for controllable 3D human editing. The multimodality-UV space is shared across different user inputs, such as texts, images, and sketches, which enables various joint multimodal editing tasks. 3) Multimodality-UV Aligned Sampler learns to sample high-quality and diverse 3D humans from the diffusion prior for multimodal user inputs. Extensive experiments validate FashionEngine's state-of-the-art performance for conditional generation/editing tasks. In addition, we present an interactive user interface for our FashionEngine that enables both conditional and unconditional generation tasks, and editing tasks including pose/view/shape control, text-, image-, and sketch-driven 3D human editing and 3D virtual try-on, in a unified framework. Our project page is at: https://taohuumd.github.io/projects/FashionEngine.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://taohuumd.github.io/projects/FashionEngine"
    },
    {
        "paper id": "2404.01656",
        "abstract url": "https://arxiv.org/abs/2404.01656",
        "title": "Supporting Mitosis Detection AI Training with Inter-Observer Eye-Gaze Consistencies",
        "rating": -1,
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The expansion of artificial intelligence (AI) in pathology tasks has intensified the demand for doctors' annotations in AI development. However, collecting high-quality annotations from doctors is costly and time-consuming, creating a bottleneck in AI progress. This study investigates eye-tracking as a cost-effective technology to collect doctors' behavioral data for AI training with a focus on the pathology task of mitosis detection. One major challenge in using eye-gaze data is the low signal-to-noise ratio, which hinders the extraction of meaningful information. We tackled this by levering the properties of inter-observer eye-gaze consistencies and creating eye-gaze labels from consistent eye-fixations shared by a group of observers. Our study involved 14 non-medical participants, from whom we collected eye-gaze data and generated eye-gaze labels based on varying group sizes. We assessed the efficacy of such eye-gaze labels by training Convolutional Neural Networks (CNNs) and comparing their performance to those trained with ground truth annotations and a heuristic-based baseline. Results indicated that CNNs trained with our eye-gaze labels closely followed the performance of ground-truth-based CNNs, and significantly outperformed the baseline. Although primarily focused on mitosis, we envision that insights from this study can be generalized to other medical imaging tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IEEE International Conference on Healthcare Informatics 2024"
    },
    {
        "paper id": "2404.01661",
        "abstract url": "https://arxiv.org/abs/2404.01661",
        "title": "Interaction-Aware Vehicle Motion Planning with Collision Avoidance Constraints in Highway Traffic",
        "rating": -1,
        "keywords": [
            [
                "trajectory",
                "Vehicle"
            ]
        ],
        "abstract": "This paper proposes collision-free optimal trajectory planning for autonomous vehicles in highway traffic, where vehicles need to deal with the interaction among each other. To address this issue, a novel optimal control framework is suggested, which couples the trajectory of surrounding vehicles with collision avoidance constraints. Additionally, we describe a trajectory optimization technique under state constraints, utilizing a planner based on Pontryagin's Minimum Principle, capable of numerically solving collision avoidance scenarios with surrounding vehicles. Simulation results demonstrate the effectiveness of the proposed approach regarding interaction-based motion planning for different scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01671",
        "abstract url": "https://arxiv.org/abs/2404.01671",
        "title": "Automating Vessel Segmentation in the Heart and Brain: A Trend to Develop Multi-Modality and Label-Efficient Deep Learning Techniques",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "healthcare",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Cardio-cerebrovascular diseases are the leading causes of mortality worldwide, whose accurate blood vessel segmentation is significant for both scientific research and clinical usage. However, segmenting cardio-cerebrovascular structures from medical images is very challenging due to the presence of thin or blurred vascular shapes, imbalanced distribution of vessel and non-vessel pixels, and interference from imaging artifacts. These difficulties make manual or semi-manual segmentation methods highly time-consuming, labor-intensive, and prone to errors with interobserver variability, where different experts may produce different segmentations from a variety of modalities. Consequently, there is a growing interest in developing automated algorithms. This paper provides an up-to-date survey of deep learning techniques, for cardio-cerebrovascular segmentation. It analyzes the research landscape, surveys recent approaches, and discusses challenges such as the scarcity of accurately annotated data and variability. This paper also illustrates the urgent needs for developing multi-modality label-efficient deep learning techniques. To the best of our knowledge, this paper is the first comprehensive survey of deep learning approaches that effectively segment vessels in both the heart and brain. It aims to advance automated segmentation techniques for cardio-cerebrovascular diseases, benefiting researchers and healthcare professionals.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "13 pages with 4 figures , and 2 table"
    },
    {
        "paper id": "2404.01672",
        "abstract url": "https://arxiv.org/abs/2404.01672",
        "title": "The Meta Distribution of the SIR in Joint Communication and Sensing Networks",
        "rating": -1,
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "In this paper, we introduce a novel mathematical framework for assessing the performance of joint communication and sensing (JCAS) in wireless networks, employing stochastic geometry as an analytical tool. We focus on deriving the meta distribution of the signal-to-interference ratio (SIR) for JCAS networks. This approach enables a fine-grained quantification of individual user or radar performance intrinsic to these networks. Our work involves the modeling of JCAS networks and the derivation of mathematical expressions for the JCAS SIR meta distribution. Through simulations, we validate both our theoretical analysis and illustrate how the JCAS SIR meta distribution varies with the network deployment density.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01679",
        "abstract url": "https://arxiv.org/abs/2404.01679",
        "title": "Event Detection from Social Media for Epidemic Prediction",
        "rating": -1,
        "keywords": [
            [
                "disease"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting sharp increases in the extracted events by our framework can provide warnings 4-9 weeks earlier than the WHO epidemic declaration for Monkeypox. This utility of our framework lays the foundations for better preparedness against emerging epidemics.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at NAACL 2024"
    },
    {
        "paper id": "2404.01702",
        "abstract url": "https://arxiv.org/abs/2404.01702",
        "title": "Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot. This includes combining data from several modalities together with the context of the situation and background knowledge. Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data. In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup). We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations. Then we implement and evaluate the model on the real setup. In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification. For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "8 pages, 8 figures"
    },
    {
        "paper id": "2404.01713",
        "abstract url": "https://arxiv.org/abs/2404.01713",
        "title": "Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G",
        "rating": -1,
        "keywords": [
            [
                "6G",
                "IoT"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01744",
        "abstract url": "https://arxiv.org/abs/2404.01744",
        "title": "Octopus v2: On-device language model for super agent",
        "rating": -1,
        "keywords": [
            [
                "face"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisites for real-world applications.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01746",
        "abstract url": "https://arxiv.org/abs/2404.01746",
        "title": "Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation",
        "rating": -1,
        "keywords": [
            [
                "trajectory",
                "vehicle"
            ]
        ],
        "abstract": "Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios. Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making. These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control. However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods. This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity. We demonstrate that these refined networks maintain the problem-solving efficacy of larger models while significantly accelerating optimization. Specifically, in the domain of interaction-aware trajectory planning for autonomous vehicles, we illustrate that training a smaller prediction network using knowledge distillation speeds up optimization without sacrificing accuracy.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01748",
        "abstract url": "https://arxiv.org/abs/2404.01748",
        "title": "Global Mapping of Exposure and Physical Vulnerability Dynamics in Least Developed Countries using Remote Sensing and Machine Learning",
        "rating": -1.0,
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "As the world marked the midterm of the Sendai Framework for Disaster Risk Reduction 2015-2030, many countries are still struggling to monitor their climate and disaster risk because of the expensive large-scale survey of the distribution of exposure and physical vulnerability and, hence, are not on track in reducing risks amidst the intensifying effects of climate change. We present an ongoing effort in mapping this vital information using machine learning and time-series remote sensing from publicly available Sentinel-1 SAR GRD and Sentinel-2 Harmonized MSI. We introduce the development of \"OpenSendaiBench\" consisting of 47 countries wherein most are least developed (LDCs), trained ResNet-50 deep learning models, and demonstrated the region of Dhaka, Bangladesh by mapping the distribution of its informal constructions. As a pioneering effort in auditing global disaster risk over time, this paper aims to advance the area of large-scale risk quantification in informing our collective long-term efforts in reducing climate and disaster risk.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This is the camera-ready paper for the accepted poster at the 2nd Machine Learning for Remote Sensing Workshop, 12th International Conference on Learning Representations (ICLR) in Vienna, Austria, on the 11th of May 2024. Access the poster here: https://zenodo.org/doi/10.5281/zenodo.10903886 Watch the video version of our poster here: https://youtu.be/N6ithJeCF4M"
    },
    {
        "paper id": "2404.01765",
        "abstract url": "https://arxiv.org/abs/2404.01765",
        "title": "Guidelines for Cerebrovascular Segmentation: Managing Imperfect Annotations in the context of Semi-Supervised Learning",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Segmentation in medical imaging is an essential and often preliminary task in the image processing chain, driving numerous efforts towards the design of robust segmentation algorithms. Supervised learning methods achieve excellent performances when fed with a sufficient amount of labeled data. However, such labels are typically highly time-consuming, error-prone and expensive to produce. Alternatively, semi-supervised learning approaches leverage both labeled and unlabeled data, and are very useful when only a small fraction of the dataset is labeled. They are particularly useful for cerebrovascular segmentation, given that labeling a single volume requires several hours for an expert. In addition to the challenge posed by insufficient annotations, there are concerns regarding annotation consistency. The task of annotating the cerebrovascular tree is inherently ambiguous. Due to the discrete nature of images, the borders and extremities of vessels are often unclear. Consequently, annotations heavily rely on the expert subjectivity and on the underlying clinical objective. These discrepancies significantly increase the complexity of the segmentation task for the model and consequently impair the results. Consequently, it becomes imperative to provide clinicians with precise guidelines to improve the annotation process and construct more uniform datasets. In this article, we investigate the data dependency of deep learning methods within the context of imperfect data and semi-supervised learning, for cerebrovascular segmentation. Specifically, this study compares various state-of-the-art semi-supervised methods based on unsupervised regularization and evaluates their performance in diverse quantity and quality data scenarios. Based on these experiments, we provide guidelines for the annotation and training of cerebrovascular segmentation models.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01799",
        "abstract url": "https://arxiv.org/abs/2404.01799",
        "title": "PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency",
        "rating": -1,
        "keywords": [
            [
                "quality assessment"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses the aforementioned limitations, presenting a new direction for LLM benchmark research. Second, we implement PATCH by measuring GPT-4 and Gemini-Pro-Vision's proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on existing benchmarking practices. Third, we release 4 datasets to support measuring and comparing LLM proficiency in grade school mathematics and science against human populations.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01803",
        "abstract url": "https://arxiv.org/abs/2404.01803",
        "title": "Systematic Solutions to Login and Authentication Security: A Dual-Password Login-Authentication Mechanism",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Credential theft and remote attacks are the most serious threats to authentication mechanisms. The crux of the problems is that we cannot control such behaviors. However, if a password does not contain user's secrets, stealing it is useless. If unauthorized inputs are disabled, the remote attacks can be invalidated. Thereby, credential secrets and input fields to our accounts can be controlled. Rather than encrypting passwords, we design a dual-password login-authentication mechanism, where a user-selected secret-free login password is converted into an untypable authentication password. Subsequently, the authenticatable functionality of the login password and the typable functionality of the authentication password may be disabled or invalidated so that the credential theft and remote attacks can be prevented. Thus, the usability-security trade-off and password reuse are resolved; local storage of authentication passwords is no longer necessary. More importantly, the password converter acts as an open hash algorithm, meaning that its intermediate elements can be used to define a truly unique identity of the login process to implement a novel dual-identity authentication. Particularly, the elements are concealed, inaccessible, and independent of any personal information, and therefore can be used to define a perfect unforgeable process identifier to identify and disable the unauthorized inputs.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "11 pages, 3 figures, 27 conferences"
    },
    {
        "paper id": "2404.01810",
        "abstract url": "https://arxiv.org/abs/2404.01810",
        "title": "Surface Reconstruction from Gaussian Splatting via Novel Stereo Views",
        "rating": -1,
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "RGB-D",
                "depth"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge. We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements' locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods. We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: https://gs2mesh.github.io/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://gs2mesh.github.io/"
    },
    {
        "paper id": "2404.01817",
        "abstract url": "https://arxiv.org/abs/2404.01817",
        "title": "Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration",
        "rating": -1,
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "The NeuroEvolution of Augmenting Topologies (NEAT) algorithm has received considerable recognition in the field of neuroevolution. Its effectiveness is derived from initiating with simple networks and incrementally evolving both their topologies and weights. Although its capability across various challenges is evident, the algorithm's computational efficiency remains an impediment, limiting its scalability potential. In response, this paper introduces a tensorization method for the NEAT algorithm, enabling the transformation of its diverse network topologies and associated operations into uniformly shaped tensors for computation. This advancement facilitates the execution of the NEAT algorithm in a parallelized manner across the entire population. Furthermore, we develop TensorNEAT, a library that implements the tensorized NEAT algorithm and its variants, such as CPPN and HyperNEAT. Building upon JAX, TensorNEAT promotes efficient parallel computations via automated function vectorization and hardware acceleration. Moreover, the TensorNEAT library supports various benchmark environments including Gym, Brax, and gymnax. Through evaluations across a spectrum of robotics control environments in Brax, TensorNEAT achieves up to 500x speedups compared to the existing implementations such as NEAT-Python. Source codes are available at: https://github.com/EMI-Group/tensorneat.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Genetic and Evolutionary Computation Conference (GECCO '24)"
    },
    {
        "paper id": "2404.01829",
        "abstract url": "https://arxiv.org/abs/2404.01829",
        "title": "Synthesizing Control Lyapunov-Value Functions for High-Dimensional Systems Using System Decomposition and Admissible Control Sets",
        "rating": -1,
        "keywords": [
            [
                "Synthesizing"
            ]
        ],
        "abstract": "Control Lyapunov functions (CLFs) play a vital role in modern control applications, but finding them remains a problem. Recently, the control Lyapunov-value function (CLVF) and robust CLVF have been proposed as solutions for nonlinear time-invariant systems with bounded control and disturbance. However, the CLVF suffers from the ''curse of dimensionality,'' which hinders its application to practical high-dimensional systems. In this paper, we propose a method to decompose systems of a particular coupled nonlinear structure, in order to solve for the CLVF in each low-dimensional subsystem. We then reconstruct the full-dimensional CLVF and provide sufficient conditions for when this reconstruction is exact. Moreover, a point-wise optimal controller can be obtained using a quadratic program. We also show that when the exact reconstruction is impossible, the subsystems' CLVFs and their ``admissible control sets'' can be used to generate a Lipschitz continuous CLF. We provide several numerical examples to validate the theory and show computational efficiency.",
        "subjects": [
            "math.OC"
        ],
        "comment": "7 pages, 4 figures, submitted to 63rd Conference on Decision and Control"
    },
    {
        "paper id": "2404.01833",
        "abstract url": "https://arxiv.org/abs/2404.01833",
        "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
        "rating": -1,
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as \"jailbreaks\", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01836",
        "abstract url": "https://arxiv.org/abs/2404.01836",
        "title": "CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS",
        "rating": -1,
        "keywords": [
            [
                "automated driving"
            ]
        ],
        "abstract": "Future mobility systems and their components are increasingly defined by their software. The complexity of these cooperative intelligent transport systems (C-ITS) and the everchanging requirements posed at the software require continual software updates. The dynamic nature of the system and the practically innumerable scenarios in which different software components work together necessitate efficient and automated development and testing procedures that use simulations as one core methodology. The availability of such simulation architectures is a common interest among many stakeholders, especially in the field of automated driving. That is why we propose CARLOS - an open, modular, and scalable simulation framework for the development and testing of software in C-ITS that leverages the rich CARLA and ROS ecosystems. We provide core building blocks for this framework and explain how it can be used and extended by the community. Its architecture builds upon modern microservice and DevOps principles such as containerization and continuous integration. In our paper, we motivate the architecture by describing important design principles and showcasing three major use cases - software prototyping, data-driven development, and automated testing. We make CARLOS and example implementations of the three use cases publicly available at github.com/ika-rwthaachen/carlos",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 5 figures, 1 table; Accepted to be published as part of the 35th IEEE Intelligent Vehicles Symposium (IV), Jeju Island, Korea, June 2-5, 2024"
    },
    {
        "paper id": "2404.01863",
        "abstract url": "https://arxiv.org/abs/2404.01863",
        "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models",
        "rating": -1.0,
        "keywords": [
            [
                "depth"
            ],
            [
                "Text-to-Image"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text prompts. We demonstrate that incorporating the confidence-calibrated rewards in fine-tuning effectively reduces overoptimization, resulting in twice as many wins in human evaluation for text-image alignment compared against the baseline reward models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICLR 2024"
    },
    {
        "paper id": "2404.01867",
        "abstract url": "https://arxiv.org/abs/2404.01867",
        "title": "Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation",
        "rating": -1,
        "keywords": [
            [
                "robotics",
                "Robot"
            ]
        ],
        "abstract": "Efficiently tackling multiple tasks within complex environment, such as those found in robot manipulation, remains an ongoing challenge in robotics and an opportunity for data-driven solutions, such as reinforcement learning (RL). Model-based RL, by building a dynamic model of the robot, enables data reuse and transfer learning between tasks with the same robot and similar environment. Furthermore, data gathering in robotics is expensive and we must rely on data efficient approaches such as model-based RL, where policy learning is mostly conducted on cheaper simulations based on the learned model. Therefore, the quality of the model is fundamental for the performance of the posterior tasks. In this work, we focus on improving the quality of the model and maintaining the data efficiency by performing active learning of the dynamic model during a preliminary exploration phase based on maximize information gathering. We employ Bayesian neural network models to represent, in a probabilistic way, both the belief and information encoded in the dynamic model during exploration. With our presented strategies we manage to actively estimate the novelty of each transition, using this as the exploration reward. In this work, we compare several Bayesian inference methods for neural networks, some of which have never been used in a robotics context, and evaluate them in a realistic robot manipulation setup. Our experiments show the advantages of our Bayesian model-based RL approach, with similar quality in the results than relevant alternatives with much lower requirements regarding robot execution steps. Unlike related previous studies that focused the validation solely on toy problems, our research takes a step towards more realistic setups, tackling robotic arm end-tasks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01871",
        "abstract url": "https://arxiv.org/abs/2404.01871",
        "title": "On the reduction of Linear Parameter-Varying State-Space models",
        "rating": -1,
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "This paper presents an overview and comparative study of the state of the art in State-Order Reduction (SOR) and Scheduling Dimension Reduction (SDR) for Linear Parameter-Varying (LPV) State-Space (SS) models, comparing and benchmarking their capabilities, limitations and performance. The use case chosen for these studies is an interconnected network of nonlinear coupled mass spring damper systems with three different configurations, where some spring coefficients are described by arbitrary user-defined static nonlinear functions. For SOR, the following methods are compared: Linear Time-Invariant (LTI), LPV and LFR-based balanced reductions, moment matching and parameter-varying oblique projection. For SDR, the following methods are compared: Principal Component Analysis (PCA), trajectory PCA, Kernel PCA and LTI balanced truncation, autoencoders and deep neural network. The comparison reveals the most suitable reduction methods for the different benchmark configurations, from which we provide use case SOR and SDR guidelines that can be used to choose the best reduction method for a given LPV-SS model.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01878",
        "abstract url": "https://arxiv.org/abs/2404.01878",
        "title": "Real, fake and synthetic faces - does the coin have three sides?",
        "rating": -1,
        "keywords": [
            [
                "facial",
                "face"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the ever-growing power of generative artificial intelligence, deepfake and artificially generated (synthetic) media have continued to spread online, which creates various ethical and moral concerns regarding their usage. To tackle this, we thus present a novel exploration of the trends and patterns observed in real, deepfake and synthetic facial images. The proposed analysis is done in two parts: firstly, we incorporate eight deep learning models and analyze their performances in distinguishing between the three classes of images. Next, we look to further delve into the similarities and differences between these three sets of images by investigating their image properties both in the context of the entire image as well as in the context of specific regions within the image. ANOVA test was also performed and provided further clarity amongst the patterns associated between the images of the three classes. From our findings, we observe that the investigated deeplearning models found it easier to detect synthetic facial images, with the ViT Patch-16 model performing best on this task with a class-averaged sensitivity, specificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%, respectively. This observation was supported by further analysis of various image properties. We saw noticeable differences across the three category of images. This analysis can help us build better algorithms for facial image generation, and also shows that synthetic, deepfake and real face images are indeed three different classes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01889",
        "abstract url": "https://arxiv.org/abs/2404.01889",
        "title": "RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement",
        "rating": -1,
        "keywords": [
            [
                "Image Enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP embeddings of backlit and well-lit images from training data, we compute the residual vector in the embedding space as a simple difference between the mean embeddings of the well-lit and backlit images. This vector then guides the enhancement network during training, pushing a backlit image towards the space of well-lit images. This approach further dramatically reduces training time, stabilizes training and produces high quality enhanced images without artifacts, both in supervised and unsupervised training regimes. Additionally, we show that residual vectors can be interpreted, revealing biases in training data, and thereby enabling potential bias correction.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01900",
        "abstract url": "https://arxiv.org/abs/2404.01900",
        "title": "Automatic Derivation of an Optimal Task Frame for Learning and Controlling Contact-Rich Tasks",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This study investigates learning from demonstration (LfD) for contact-rich tasks. The procedure for choosing a task frame to express the learned signals for the motion and interaction wrench is often omitted or using expert insight. This article presents a procedure to derive the optimal task frame from motion and wrench data recorded during the demonstration. The procedure is based on two principles that are hypothesized to underpin the control configuration targeted by an expert, and assumes task frame origins and orientations that are fixed to either the world or the robot tool. It is rooted in screw theory, is entirely probabilistic and does not involve any hyperparameters. The procedure was validated by demonstrating several tasks, including surface following and manipulation of articulated objects, showing good agreement between the obtained and the assumed expert task frames. To validate the performance of the learned tasks by a UR10e robot, a constraint-based controller was designed based on the derived task frames and the learned data expressed therein. These experiments showed the effectiveness and versatility of the proposed approach. The task frame derivation approach fills a gap in the state of the art of LfD, bringing LfD for contact-rich tasks closer to practical application.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01909",
        "abstract url": "https://arxiv.org/abs/2404.01909",
        "title": "A Temporal Graph Model to Study the Dynamics of Collective Behavior and Performance in Team Sports: An Application to Basketball",
        "rating": -1,
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "In this study, a temporal graph model is designed to model the behavior of collective sports teams based on the networks of player interactions. The main motivation for the model is to integrate the temporal dimension into the analysis of players' passing networks in order to gain deeper insights into the dynamics of system behavior, particularly how a system exploits the degeneracy property to self-regulate. First, the temporal graph model and the entropy measures used to assess the complexity of the dynamics of the network structure are introduced and illustrated. Second, an experiment using basketball data is conducted to investigate the relationship between the complexity level and team performance. This is accomplished by examining the correlations between the entropy measures in a team's behavior and the team's final performance, as well as the link between the relative score compared to that of the opponent and the entropy in the team's behavior. Results indicate positive correlations between entropy measures and final team performance, and threshold values of relative score associated with changes in team behavior -- thereby revealing common and unique team signatures. From a complexity science perspective, the model proves useful for identifying key performance factors in team sports and for studying the effects of given constraints on the exploitation of degeneracy to organize team behavior through various network structures. Future research can easily extend the model and apply it to other types of social networks.",
        "subjects": [
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01910",
        "abstract url": "https://arxiv.org/abs/2404.01910",
        "title": "Multicore DRAM Bank-& Row-Conflict Bomb for Timing Attacks in Mixed-Criticality Systems",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "With the increasing use of multicore platforms to realize mixed-criticality systems, understanding the underlying shared resources, such as the memory hierarchy shared among cores, and achieving isolation between co-executing tasks running on the same platform with different criticality levels becomes relevant. In addition to safety considerations, a malicious entity can exploit shared resources to create timing attacks on critical applications. In this paper, we focus on understanding the shared DRAM dual in-line memory module and created a timing attack, that we named the \"bank & row conflict bomb\", to target a victim task in a multicore platform. We also created a \"navigate\" algorithm to understand how victim requests are managed by the Memory Controller and provide valuable inputs for designing the bank & row conflict bomb. We performed experimental tests on a 2nd Gen Intel Xeon Processor with an 8GB DDR4-2666 DRAM module to show that such an attack can produce a significant increase in the execution time of the victim task by about 150%, motivating the need for proper countermeasures to help ensure the safety and security of critical applications.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "To appear in the proceedings of the 27th IEEE international Symposium on Real-time Distributed Computing (ISORC)"
    },
    {
        "paper id": "2404.01929",
        "abstract url": "https://arxiv.org/abs/2404.01929",
        "title": "Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA -- A Semi-Supervised Video Object Detection Method",
        "rating": -1,
        "keywords": [
            [
                "Cancer"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "This study aims to establish a computer-aided diagnostic system for lung lesions using bronchoscope endobronchial ultrasound (EBUS) to assist physicians in identifying lesion areas. During EBUS-transbronchial needle aspiration (EBUS-TBNA) procedures, physicians rely on grayscale ultrasound images to determine the location of lesions. However, these images often contain significant noise and can be influenced by surrounding tissues or blood vessels, making interpretation challenging. Previous research has lacked the application of object detection models to EBUS-TBNA, and there has been no well-defined solution for annotating the EBUS-TBNA dataset. In related studies on ultrasound images, although models have been successful in capturing target regions for their respective tasks, their training and predictions have been based on two-dimensional images, limiting their ability to leverage temporal features for improved predictions. This study introduces a three-dimensional image-based object detection model. It utilizes an attention mechanism to capture temporal correlations and we will implements a filtering mechanism to select relevant information from previous frames. Subsequently, a teacher-student model training approach is employed to optimize the model further, leveraging unlabeled data. To mitigate the impact of poor-quality pseudo-labels on the student model, we will add a special Gaussian Mixture Model (GMM) to ensure the quality of pseudo-labels.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01934",
        "abstract url": "https://arxiv.org/abs/2404.01934",
        "title": "Towards a Completeness Argumentation for Scenario Concepts",
        "rating": -1,
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "Scenario-based testing has become a promising approach to overcome the complexity of real-world traffic for safety assurance of automated vehicles. Within scenario-based testing, a system under test is confronted with a set of predefined scenarios. This set shall ensure more efficient testing of an automated vehicle operating in an open context compared to real-world testing. However, the question arises if a scenario catalog can cover the open context sufficiently to allow an argumentation for sufficiently safe driving functions and how this can be proven. Within this paper, a methodology is proposed to argue a sufficient completeness of a scenario concept using a goal structured notation. Thereby, the distinction between completeness and coverage is discussed. For both, methods are proposed for a streamlined argumentation and regarding evidence. These methods are applied to a scenario concept and the inD dataset to prove the usability.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "6 pages, 5 figures, 1 table; Accepted to be published as part of the 35th IEEE Intelligent Vehicles Symposium, June 2 - 5, 2024, Korea"
    },
    {
        "paper id": "2404.01946",
        "abstract url": "https://arxiv.org/abs/2404.01946",
        "title": "Synthetic Data for Robust Stroke Segmentation",
        "rating": -1,
        "keywords": [
            [
                "medical",
                "clinical",
                "pathological"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning-based semantic segmentation in neuroimaging currently requires high-resolution scans and extensive annotated datasets, posing significant barriers to clinical applicability. We present a novel synthetic framework for the task of lesion segmentation, extending the capabilities of the established SynthSeg approach to accommodate large heterogeneous pathologies with lesion-specific augmentation strategies. Our method trains deep learning models, demonstrated here with the UNet architecture, using label maps derived from healthy and stroke datasets, facilitating the segmentation of both healthy tissue and pathological lesions without sequence-specific training data. Evaluated against in-domain and out-of-domain (OOD) datasets, our framework demonstrates robust performance, rivaling current methods within the training domain and significantly outperforming them on OOD data. This contribution holds promise for advancing medical imaging analysis in clinical settings, especially for stroke pathology, by enabling reliable segmentation across varied imaging sequences with reduced dependency on large annotated corpora. Code and weights available at https://github.com/liamchalcroft/SynthStroke.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01949",
        "abstract url": "https://arxiv.org/abs/2404.01949",
        "title": "Heuristic Optimization of Amplifier Reconfiguration Process for Autonomous Driving Optical Networks",
        "rating": -1,
        "keywords": [
            [
                "Autonomous Driving"
            ]
        ],
        "abstract": "We propose a heuristic-based optimization scheme for reliable optical amplifier reconfiguration process in ADON. In the experiment on a commercial testbed, the scheme prevents a 0.48-dB Q-factor degradation and outperforms 97.3% random solutions.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01986",
        "abstract url": "https://arxiv.org/abs/2404.01986",
        "title": "Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues",
        "rating": -1,
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the person's gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5% to 91.2%); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system's ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01991",
        "abstract url": "https://arxiv.org/abs/2404.01991",
        "title": "Kallaama: A Transcribed Speech Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal",
        "rating": -1,
        "keywords": [
            [
                "agricultural"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for speech technologies developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. However, such technologies are keys to the protection, promotion and teaching of these languages. Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer. These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country. However, they remain under-resourced in terms of machine-readable data that can be used for automatic processing and language technologies, all the more so in the agricultural sector. We release a transcribed speech dataset containing 125 hours of recordings, about agriculture, in each of the above-mentioned languages. These resources are specifically designed for Automatic Speech Recognition purpose, including traditional approaches. To build such technologies, we provide textual corpora in Wolof and Pulaar, and a pronunciation lexicon containing 49,132 entries from the Wolof dataset.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To appear in RAIL 2024"
    },
    {
        "paper id": "2404.02010",
        "abstract url": "https://arxiv.org/abs/2404.02010",
        "title": "Resource-Aware Collaborative Monte Carlo Localization with Distribution Compression",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Global localization is essential in enabling robot autonomy, and collaborative localization is key for multi-robot systems. In this paper, we address the task of collaborative global localization under computational and communication constraints. We propose a method which reduces the amount of information exchanged and the computational cost. We also analyze, implement and open-source seminal approaches, which we believe to be a valuable contribution to the community. We exploit techniques for distribution compression in near-linear time, with error guarantees. We evaluate our approach and the implemented baselines on multiple challenging scenarios, simulated and real-world. Our approach can run online on an onboard computer. We release an open-source C++/ROS2 implementation of our approach, as well as the baselines",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Under review for IROS 2024"
    },
    {
        "paper id": "2404.02018",
        "abstract url": "https://arxiv.org/abs/2404.02018",
        "title": "Large Language Models for Orchestrating Bimanual Robots",
        "rating": -1,
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have taken control of a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge for the first time by an LLM, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. In the simulated environment, the LABOR agent is evaluated through several everyday tasks on the NICOL humanoid robot. Reported success rates indicate that overall coordination efficiency is close to optimal performance, while the analysis of failure causes, classified into spatial and temporal coordination and skill selection, shows that these vary over tasks. The project website can be found at http://labor-agent.github.io",
        "subjects": [
            "cs.RO"
        ],
        "comment": "The project website can be found at http://labor-agent.github.io"
    },
    {
        "paper id": "2404.02059",
        "abstract url": "https://arxiv.org/abs/2404.02059",
        "title": "IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT",
        "rating": -1,
        "keywords": [
            [
                "Parameter-efficient",
                "PEFT",
                "efficient Fine-tuning"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation. IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training. Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that \"parameter efficiency represents overall efficiency\". TPME provides more comprehensive insights into practical efficiency comparisons between different methods. Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN. We release our codes and other materials at https://github.com/jjGenAILab/IISAN.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by SIGIR2024"
    },
    {
        "paper id": "2404.02062",
        "abstract url": "https://arxiv.org/abs/2404.02062",
        "title": "Digital Forgetting in Large Language Models: A Survey of Unlearning Methods",
        "rating": -1,
        "keywords": [
            [
                "Unlearning"
            ]
        ],
        "abstract": "The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, we introduce the approaches to digital forgetting in LLMs, among which unlearning methodologies stand out as the state of the art. Fourth, we provide a detailed taxonomy of machine unlearning methods for LLMs, and we survey and compare current approaches. Fifth, we detail datasets, models and metrics used for the evaluation of forgetting, retaining and runtime. Sixth, we discuss challenges in the area. Finally, we provide some concluding remarks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "70 pages"
    },
    {
        "paper id": "2404.02082",
        "abstract url": "https://arxiv.org/abs/2404.02082",
        "title": "WcDT: World-centric Diffusion Transformer for Traffic Scene Generation",
        "rating": -1,
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "autonomous driving",
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce a novel approach for autonomous driving trajectory generation by harnessing the complementary strengths of diffusion probabilistic models (a.k.a., diffusion models) and transformers. Our proposed framework, termed the \"World-Centric Diffusion Transformer\" (WcDT), optimizes the entire trajectory generation process, from feature extraction to model inference. To enhance the scene diversity and stochasticity, the historical trajectory data is first preprocessed and encoded into latent space using Denoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with Transformer (DiT) blocks. Then, the latent features, historical trajectories, HD map features, and historical traffic signal information are fused with various transformer-based encoders. The encoded traffic scenes are then decoded by a trajectory decoder to generate multimodal future trajectories. Comprehensive experimental results show that the proposed approach exhibits superior performance in generating both realistic and diverse trajectories, showing its potential for integration into automatic driving simulation systems.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 6 figures"
    },
    {
        "paper id": "2404.02084",
        "abstract url": "https://arxiv.org/abs/2404.02084",
        "title": "Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images",
        "rating": -1,
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets. To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and self-supervised multi-task learning. Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain. Feature-fusion network and self-supervised multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability. In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks. Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "17 pages, 11 figures"
    },
    {
        "paper id": "2404.02101",
        "abstract url": "https://arxiv.org/abs/2404.02101",
        "title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation",
        "rating": -1,
        "keywords": [
            [
                "Text-to-Video"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Controllability plays a crucial role in video generation since it allows users to create desired content. However, existing models largely overlooked the precise control of camera pose that serves as a cinematic language to express deeper narrative nuances. To alleviate this issue, we introduce CameraCtrl, enabling accurate camera pose control for text-to-video(T2V) models. After precisely parameterizing the camera trajectory, a plug-and-play camera module is then trained on a T2V model, leaving others untouched. Additionally, a comprehensive study on the effect of various datasets is also conducted, suggesting that videos with diverse camera distribution and similar appearances indeed enhance controllability and generalization. Experimental results demonstrate the effectiveness of CameraCtrl in achieving precise and domain-adaptive camera control, marking a step forward in the pursuit of dynamic and customized video storytelling from textual and camera pose inputs. Our project website is at: https://hehao13.github.io/projects-CameraCtrl/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://hehao13.github.io/projects-CameraCtrl/ Code: https://github.com/hehao13/CameraCtrl"
    },
    {
        "paper id": "2404.02125",
        "abstract url": "https://arxiv.org/abs/2404.02125",
        "title": "3D Congealing: 3D-Aware Image Alignment in the Wild",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "image editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images capturing semantically similar objects. Given a collection of unlabeled Internet images, our goal is to associate the shared semantic parts from the inputs and aggregate the knowledge from 2D images to a shared 3D canonical space. We introduce a general framework that tackles the task without assuming shape templates, poses, or any camera parameters. At its core is a canonical 3D representation that encapsulates geometric and semantic information. The framework optimizes for the canonical representation together with the pose for each input image, and a per-image coordinate map that warps 2D pixel coordinates to the 3D canonical frame to account for the shape matching. The optimization procedure fuses prior knowledge from a pre-trained image generative model and semantic information from input images. The former provides strong knowledge guidance for this under-constraint task, while the latter provides the necessary information to mitigate the training data bias from the pre-trained model. Our framework can be used for various tasks such as correspondence matching, pose estimation, and image editing, achieving strong results on real-world image datasets under challenging illumination conditions and on in-the-wild online image collections.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://ai.stanford.edu/~yzzhang/projects/3d-congealing/"
    },
    {
        "paper id": "2404.02135",
        "abstract url": "https://arxiv.org/abs/2404.02135",
        "title": "ResNet with Integrated Convolutional Block Attention Module for Ship Classification Using Transfer Learning on Optical Satellite Imagery",
        "rating": -1,
        "keywords": [
            [
                "remote sensing",
                "Satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This study proposes a novel transfer learning framework for effective ship classification using high-resolution optical remote sensing satellite imagery. The framework is based on the deep convolutional neural network model ResNet50 and incorporates the Convolutional Block Attention Module (CBAM) to enhance performance. CBAM enables the model to attend to salient features in the images, allowing it to better discriminate between subtle differences between ships and backgrounds. Furthermore, this study adopts a transfer learning approach tailored for accurately classifying diverse types of ships by fine-tuning a pre-trained model for the specific task. Experimental results demonstrate the efficacy of the proposed framework in ship classification using optical remote sensing imagery, achieving a high classification accuracy of 94% across 5 classes, outperforming existing methods. This research holds potential applications in maritime surveillance and management, illegal fishing detection, and maritime traffic monitoring.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02138",
        "abstract url": "https://arxiv.org/abs/2404.02138",
        "title": "Topic-based Watermarks for LLM-Generated Text",
        "rating": -1,
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a \"topic-based watermarking algorithm\" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM. Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM. Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02148",
        "abstract url": "https://arxiv.org/abs/2404.02148",
        "title": "Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models",
        "rating": -1,
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models which are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to the scarcity of synchronized multi-view video data, it is impractical to adapt this paradigm to 4D generation directly. Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models that can provide satisfactory dynamic and geometric priors respectively. In this paper, we present Diffusion$^2$, a novel framework for dynamic 3D content creation that leverages the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view and multi-frame images which can be employed to optimize continuous 4D representation. Specifically, we design a simple yet effective denoising strategy via score composition of video and multi-view diffusion models based on the probability structure of the images to be generated. Owing to the high parallelism of the image generation and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes. Furthermore, our method circumvents the reliance on 4D data, thereby having the potential to benefit from the scalability of the foundation video and multi-view diffusion models. Extensive experiments demonstrate the efficacy of our proposed framework and its capability to flexibly adapt to various types of prompts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Technical Report"
    },
    {
        "paper id": "2404.02151",
        "abstract url": "https://arxiv.org/abs/2404.02151",
        "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
        "rating": -1,
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token \"Sure\"), potentially with multiple restarts. In this way, we achieve nearly 100\\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02154",
        "abstract url": "https://arxiv.org/abs/2404.02154",
        "title": "Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration",
        "rating": -1,
        "keywords": [
            [
                "Image Restoration",
                "dehazing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation. The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives. We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks. Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training. This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights. Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours. To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD having 2M image samples. We validate our DyNet for image denoising, deraining, and dehazing in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models. The source codes and trained models are available at https://github.com/akshaydudhane16/DyNet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01646",
        "abstract url": "https://arxiv.org/abs/2404.01646",
        "title": "Transformer meets wcDTW to improve real-time battery bids: A new approach to scenario selection",
        "rating": -1.5,
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Stochastic battery bidding in real-time energy markets is a nuanced process, with its efficacy depending on the accuracy of forecasts and the representative scenarios chosen for optimization. In this paper, we introduce a pioneering methodology that amalgamates Transformer-based forecasting with weighted constrained Dynamic Time Warping (wcDTW) to refine scenario selection. Our approach harnesses the predictive capabilities of Transformers to foresee Energy prices, while wcDTW ensures the selection of pertinent historical scenarios by maintaining the coherence between multiple uncertain products. Through extensive simulations in the PJM market for July 2023, our method exhibited a 10% increase in revenue compared to the conventional method, highlighting its potential to revolutionize battery bidding strategies in real-time markets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01872",
        "abstract url": "https://arxiv.org/abs/2404.01872",
        "title": "Fast and Adaptive Questionnaires for Voting Advice Applications",
        "rating": -1.5,
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires. To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire. However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%. To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters. Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations. Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter's current position in the latent space and the remaining unanswered questions. We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system's predictive accuracy. Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of recommendations, achieving 74% accuracy after asking the same number of questions as in the condensed version.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages plus appendix, submitted to ECML/PKDD 2024"
    },
    {
        "paper id": "2404.01981",
        "abstract url": "https://arxiv.org/abs/2404.01981",
        "title": "Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials",
        "rating": -1.5,
        "keywords": [
            [
                "health",
                "Clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge. In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders. We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial. Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages. Our results demonstrate that tested models can effectively generalize to clinical speakers, with less than 2.7% EER for European Languages and 8.26% EER for Arabic. This represents a significant step in developing more versatile and efficient speaker verification systems for cognitive and mental health clinical trials that can be used across a wide range of languages and dialects, substantially reducing the effort required to develop speaker verification systems for multiple languages. We also evaluate how speech tasks and number of speakers involved in the trial influence the performance and show that the type of speech tasks impacts the model performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02152",
        "abstract url": "https://arxiv.org/abs/2404.02152",
        "title": "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image",
        "rating": -1.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to CVPR 2024. Project page: https://zju3dv.github.io/geneavatar/"
    },
    {
        "paper id": "2404.01640",
        "abstract url": "https://arxiv.org/abs/2404.01640",
        "title": "Deterministic Search on Complete Bipartite Graphs by Continuous Time Quantum Walk",
        "rating": -2,
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "This paper presents a deterministic search algorithm on complete bipartite graphs. Our algorithm adopts the simple form of alternating iterations of an oracle and a continuous-time quantum walk operator, which is a generalization of Grover's search algorithm. We address the most general case of multiple marked states, so there is a problem of estimating the number of marked states. To this end, we construct a quantum counting algorithm based on the spectrum structure of the search operator. To implement the continuous-time quantum walk operator, we perform Hamiltonian simulation in the quantum circuit model. We achieve simulation in constant time, that is, the complexity of the quantum circuit does not scale with the evolution time. Besides, deterministic search serves as a simple tool for perfect state transfer (PST). As an application, we explore the problem of PST on complete bipartite graphs.",
        "subjects": [
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01647",
        "abstract url": "https://arxiv.org/abs/2404.01647",
        "title": "EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis",
        "rating": -2,
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. We recommend watching the project website: https://tanshuai0219.github.io/EDTalk/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "22 pages, 15 figures"
    },
    {
        "paper id": "2404.01674",
        "abstract url": "https://arxiv.org/abs/2404.01674",
        "title": "PRISM-TopoMap: Online Topological Mapping with Place Recognition and Scan Matching",
        "rating": -2,
        "keywords": [
            [
                "robot",
                "navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Mapping is one of the crucial tasks enabling autonomous navigation of a mobile robot. Conventional mapping methods output dense geometric map representation, e.g. an occupancy grid, which is not trivial to keep consistent for the prolonged runs covering large environments. Meanwhile, capturing the topological structure of the workspace enables fast path planning, is less prone to odometry error accumulation and does not consume much memory. Following this idea, this paper introduces PRISM-TopoMap -- a topological mapping method that maintains a graph of locally aligned locations not relying on global metric coordinates. The proposed method involves learnable multimodal place recognition paired with the scan matching pipeline for localization and loop closure in the graph of locations. The latter is updated online and the robot is localized in a proper node at each time step. We conduct a broad experimental evaluation of the suggested approach in a range of photo-realistic environments and on a real robot (wheeled differential driven Husky robot), and compare it to state of the art. The results of the empirical evaluation confirm that PRISM-Topomap consistently outperforms competitors across several measures of mapping and navigation efficiency and performs well on a real robot. The code of PRISM-Topomap is open-sourced and available at https://github.com/kirillMouraviev/prism-topomap.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This is a pre-print of the paper submitted to an IROS 2024 conference"
    },
    {
        "paper id": "2404.01710",
        "abstract url": "https://arxiv.org/abs/2404.01710",
        "title": "Practical Persistent Multi-Word Compare-and-Swap Algorithms for Many-Core CPUs",
        "rating": -2,
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "In the last decade, academic and industrial researchers have focused on persistent memory because of the development of the first practical product, Intel Optane. One of the main challenges of persistent memory programming is to guarantee consistent durability over separate memory addresses, and Wang et al. proposed a persistent multi-word compare-and-swap (PMwCAS) algorithm to solve this problem. However, their algorithm contains redundant compare-and-swap (CAS) and cache flush instructions and does not achieve sufficient performance on many-core CPUs. This paper proposes a new algorithm to improve performance on many-core CPUs by removing useless CAS/flush instructions from PMwCAS operations. We also exclude dirty flags, which help ensure consistent durability in the original algorithm, from our algorithm using PMwCAS descriptors as write-ahead logs. Experimental results show that the proposed method is up to ten times faster than the original algorithm and suggests several productive uses of PMwCAS operations.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "8 pages, 14 figures"
    },
    {
        "paper id": "2404.01723",
        "abstract url": "https://arxiv.org/abs/2404.01723",
        "title": "Contextual Embedding Learning to Enhance 2D Networks for Volumetric Image Segmentation",
        "rating": -2,
        "keywords": [
            [
                "3D"
            ],
            [
                "medical",
                "surgery",
                "diagnosis",
                "MRI",
                "CT"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The segmentation of organs in volumetric medical images plays an important role in computer-aided diagnosis and treatment/surgery planning. Conventional 2D convolutional neural networks (CNNs) can hardly exploit the spatial correlation of volumetric data. Current 3D CNNs have the advantage to extract more powerful volumetric representations but they usually suffer from occupying excessive memory and computation nevertheless. In this study we aim to enhance the 2D networks with contextual information for better volumetric image segmentation. Accordingly, we propose a contextual embedding learning approach to facilitate 2D CNNs capturing spatial information properly. Our approach leverages the learned embedding and the slice-wisely neighboring matching as a soft cue to guide the network. In such a way, the contextual information can be transferred slice-by-slice thus boosting the volumetric representation of the network. Experiments on challenging prostate MRI dataset (PROMISE12) and abdominal CT dataset (CHAOS) show that our contextual embedding learning can effectively leverage the inter-slice context and improve segmentation performance. The proposed approach is a plug-and-play, and memory-efficient solution to enhance the 2D networks for volumetric segmentation. The code will be publicly available.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "13 pages, 8 figures"
    },
    {
        "paper id": "2404.01726",
        "abstract url": "https://arxiv.org/abs/2404.01726",
        "title": "A Stability-Based Abstraction Framework for Reach-Avoid Control of Stochastic Dynamical Systems with Unknown Noise Distributions",
        "rating": -2,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Finite-state abstractions are widely studied for the automated synthesis of correct-by-construction controllers for stochastic dynamical systems. However, existing abstraction methods often lead to prohibitively large finite-state models. To address this issue, we propose a novel abstraction scheme for stochastic linear systems that exploits the system's stability to obtain significantly smaller abstract models. As a unique feature, we first stabilize the open-loop dynamics using a linear feedback gain. We then use a model-based approach to abstract a known part of the stabilized dynamics while using a data-driven method to account for the stochastic uncertainty. We formalize abstractions as Markov decision processes (MDPs) with intervals of transition probabilities. By stabilizing the dynamics, we can further constrain the control input modeled in the abstraction, which leads to smaller abstract models while retaining the correctness of controllers. Moreover, when the stabilizing feedback controller is aligned with the property of interest, then a good trade-off is achieved between the reduction in the abstraction size and the performance loss. The experiments show that our approach can reduce the size of the graph of abstractions by up to 90% with negligible performance loss.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted for presentation at ECC 2024"
    },
    {
        "paper id": "2404.01735",
        "abstract url": "https://arxiv.org/abs/2404.01735",
        "title": "CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Product bundling has been a prevailing marketing strategy that is beneficial in the online shopping scenario. Effective product bundling methods depend on high-quality item representations, which need to capture both the individual items' semantics and cross-item relations. However, previous item representation learning methods, either feature fusion or graph learning, suffer from inadequate cross-modal alignment and struggle to capture the cross-item relations for cold-start items. Multimodal pre-train models could be the potential solutions given their promising performance on various multimodal downstream tasks. However, the cross-item relations have been under-explored in the current multimodal pre-train models. To bridge this gap, we propose a novel and simple framework Cross-Item Relational Pre-training (CIRP) for item representation learning in product bundling. Specifically, we employ a multimodal encoder to generate image and text representations. Then we leverage both the cross-item contrastive loss (CIC) and individual item's image-text contrastive loss (ITC) as the pre-train objectives. Our method seeks to integrate cross-item relation modeling capability into the multimodal encoder, while preserving the in-depth aligned multimodal semantics. Therefore, even for cold-start items that have no relations, their representations are still relation-aware. Furthermore, to eliminate the potential noise and reduce the computational cost, we harness a relation pruning module to remove the noisy and redundant relations. We apply the item representations extracted by CIRP to the product bundling model ItemKNN, and experiments on three e-commerce datasets demonstrate that CIRP outperforms various leading representation learning methods.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "arXiv preprint, 10 pages, 4 figures, 6 tables"
    },
    {
        "paper id": "2404.01750",
        "abstract url": "https://arxiv.org/abs/2404.01750",
        "title": "Exploring Latent Pathways: Enhancing the Interpretability of Autonomous Driving with a Variational Autoencoder",
        "rating": -2,
        "keywords": [
            [
                "Autonomous Driving"
            ],
            [
                "bio-inspired"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Autonomous driving presents a complex challenge, which is usually addressed with artificial intelligence models that are end-to-end or modular in nature. Within the landscape of modular approaches, a bio-inspired neural circuit policy model has emerged as an innovative control module, offering a compact and inherently interpretable system to infer a steering wheel command from abstract visual features. Here, we take a leap forward by integrating a variational autoencoder with the neural circuit policy controller, forming a solution that directly generates steering commands from input camera images. By substituting the traditional convolutional neural network approach to feature extraction with a variational autoencoder, we enhance the system's interpretability, enabling a more transparent and understandable decision-making process. In addition to the architectural shift toward a variational autoencoder, this study introduces the automatic latent perturbation tool, a novel contribution designed to probe and elucidate the latent features within the variational autoencoder. The automatic latent perturbation tool automates the interpretability process, offering granular insights into how specific latent variables influence the overall model's behavior. Through a series of numerical experiments, we demonstrate the interpretative power of the variational autoencoder-neural circuit policy model and the utility of the automatic latent perturbation tool in making the inner workings of autonomous driving systems more transparent.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Submitted to 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)"
    },
    {
        "paper id": "2404.01752",
        "abstract url": "https://arxiv.org/abs/2404.01752",
        "title": "Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space",
        "rating": -2,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths. The difficulty of the problem arises from two primary factors. First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially. Second, the continuous space presents potentially infinite states and actions. For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples. SI-CPP exhibits improved scalability while SI-CCBS produces higher-quality solutions compared to the state-of-the-art planners for continuous space. Compared to the most scalable existing algorithm, SI-CPP achieves a success rate that is up to 94% higher with 100 robots while maintaining solution quality (i.e., flowtime, the sum of travel times of all robots) without significant compromise. SI-CPP also decreases the makespan up to 45%. SI-CCBS decreases the flowtime by 9% compared to the competitor, albeit exhibiting a 14% lower success rate.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01757",
        "abstract url": "https://arxiv.org/abs/2404.01757",
        "title": "Analyzing the Single Event Upset Vulnerability of Binarized Neural Networks on SRAM FPGAs",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "autonomous driving"
            ]
        ],
        "abstract": "Neural Networks (NNs) are increasingly used in the last decade in several demanding applications, such as object detection and classification, autonomous driving, etc. Among different computing platforms for implementing NNs, FPGAs have multiple advantages due to design flexibility and high performance-to-watt ratio. Moreover, approximation techniques, such as quantization, have been introduced, which reduce the computational and storage requirements, thus enabling the integration of larger NNs into FPGA devices. On the other hand, FPGAs are sensitive to radiation-induced Single Event Upsets (SEUs). In this work, we perform an in-depth reliability analysis in an FPGA-based Binarized Fully Connected Neural Network (BNN) accelerator running a statistical fault injection campaign. The BNN benchmark has been produced by FINN, an open-source framework that provides an end-to-end flow from abstract level to design, making it easy to design customized FPGA NN accelerators, while it also supports various approximation techniques. The campaign includes the injection of faults in the configuration memory of a state-of-the-art Xilinx Ultrascale+ FPGA running the BNN, as well an exhaustive fault injection in the user flip flops. We have analyzed the fault injection results characterizing the SEU vulnerability of the circuit per network layer, per clock cycle, and register. In general, the results show that the BNNs are inherently resilient to soft errors, since a low portion of SEUs in the configuration memory and the flip flops, cause system crashes or misclassification errors.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "7 pages, 5 figures, 4 tables. 2021 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT). IEEE, 2021"
    },
    {
        "paper id": "2404.01781",
        "abstract url": "https://arxiv.org/abs/2404.01781",
        "title": "An evaluation of CFEAR Radar Odometry",
        "rating": -2,
        "keywords": [
            [
                "Radar"
            ],
            [
                "Robotics"
            ]
        ],
        "abstract": "This article describes the method CFEAR Radar odometry, submitted to a competition at the Radar in Robotics workshop, ICRA 2024. CFEAR is an efficient and accurate method for spinning 2D radar odometry that generalizes well across environments. This article presents an overview of the odometry pipeline with new experiments on the public Boreas dataset. We show that a real-time capable configuration of CFEAR -- with its original parameter se -- yields surprisingly low drift in the Boreas dataset. Additionally, we discuss an improved implementation and solving strategy that enables the most accurate configuration to run in real-time with improved robustness, reaching as low as 0.66% translation drift at a frame rate of 68 Hz. A recent release of the source code is available to the community https://github.com/dan11003/CFEAR_Radarodometry_code_public, and we publish the evaluation from this article https://github.com/dan11003/cfear_2024_workshop.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Will be updated with data from Boreas test sequences at a later stage"
    },
    {
        "paper id": "2404.01790",
        "abstract url": "https://arxiv.org/abs/2404.01790",
        "title": "Super-Resolution Analysis for Landfill Waste Classification",
        "rating": -2,
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "health"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Illegal landfills are a critical issue due to their environmental, economic, and public health impacts. This study leverages aerial imagery for environmental crime monitoring. While advances in artificial intelligence and computer vision hold promise, the challenge lies in training models with high-resolution literature datasets and adapting them to open-access low-resolution images. Considering the substantial quality differences and limited annotation, this research explores the adaptability of models across these domains. Motivated by the necessity for a comprehensive evaluation of waste detection algorithms, it advocates cross-domain classification and super-resolution enhancement to analyze the impact of different image resolutions on waste classification as an evaluation to combat the proliferation of illegal landfills. We observed performance improvements by enhancing image quality but noted an influence on model sensitivity, necessitating careful threshold fine-tuning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This article has been accepted by the Symposium on Intelligent Data Analysis (IDA 2024)"
    },
    {
        "paper id": "2404.01801",
        "abstract url": "https://arxiv.org/abs/2404.01801",
        "title": "EventSleep: Sleep Activity Recognition with Event Cameras",
        "rating": -2,
        "keywords": [
            [
                "infrared"
            ],
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Event cameras are a promising technology for activity recognition in dark environments due to their unique properties. However, real event camera datasets under low-lighting conditions are still scarce, which also limits the number of approaches to solve these kind of problems, hindering the potential of this technology in many applications. We present EventSleep, a new dataset and methodology to address this gap and study the suitability of event cameras for a very relevant medical application: sleep monitoring for sleep disorders analysis. The dataset contains synchronized event and infrared recordings emulating common movements that happen during the sleep, resulting in a new challenging and unique dataset for activity recognition in dark environments. Our novel pipeline is able to achieve high accuracy under these challenging conditions and incorporates a Bayesian approach (Laplace ensembles) to increase the robustness in the predictions, which is fundamental for medical applications. Our work is the first application of Bayesian neural networks for event cameras, the first use of Laplace ensembles in a realistic problem, and also demonstrates for the first time the potential of event cameras in a new application domain: to enhance current sleep evaluation procedures. Our activity recognition results highlight the potential of event cameras under dark conditions, and its capacity and robustness for sleep activity recognition, and open problems as the adaptation of event data pre-processing techniques to dark environments.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01812",
        "abstract url": "https://arxiv.org/abs/2404.01812",
        "title": "Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions",
        "rating": -2,
        "keywords": [
            [
                "3D",
                "depth",
                "NeRF"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth reconstruction of the object surface (F-score) and (iii) 71% in the task success rate of manipulating objects a-priori unseen orientations/stable configurations in the scene; over current methods. The project page can be found here: https://actnerf.github.io.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2404.01845",
        "abstract url": "https://arxiv.org/abs/2404.01845",
        "title": "Unmasking the Nuances of Loneliness: Using Digital Biomarkers to Understand Social and Emotional Loneliness in College Students",
        "rating": -2,
        "keywords": [
            [
                "Biomarkers",
                "health"
            ]
        ],
        "abstract": "Background: Loneliness among students is increasing across the world, with potential consequences for mental health and academic success. To address this growing problem, accurate methods of detection are needed to identify loneliness and to differentiate social and emotional loneliness so that intervention can be personalized to individual need. Passive sensing technology provides a unique technique to capture behavioral patterns linked with distinct loneliness forms, allowing for more nuanced understanding and interventions for loneliness. Methods: To differentiate between social and emotional loneliness using digital biomarkers, our study included statistical tests, machine learning for predictive modeling, and SHAP values for feature importance analysis, revealing important factors in loneliness classification. Results: Our analysis revealed significant behavioral differences between socially and emotionally lonely groups, particularly in terms of phone usage and location-based features , with machine learning models demonstrating substantial predictive power in classifying loneliness levels. The XGBoost model, in particular, showed high accuracy and was effective in identifying key digital biomarkers, including phone usage duration and location-based features, as significant predictors of loneliness categories. Conclusion: This study underscores the potential of passive sensing data, combined with machine learning techniques, to provide insights into the behavioral manifestations of social and emotional loneliness among students. The identification of key digital biomarkers paves the way for targeted interventions aimed at mitigating loneliness in this population.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01849",
        "abstract url": "https://arxiv.org/abs/2404.01849",
        "title": "EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking",
        "rating": -2,
        "keywords": [
            [
                "depth"
            ],
            [
                "vehicle"
            ]
        ],
        "abstract": "As electric vehicle (EV) numbers rise, concerns about the capacity of current charging and power grid infrastructure grow, necessitating the development of smart charging solutions. While many smart charging simulators have been developed in recent years, only a few support the development of Reinforcement Learning (RL) algorithms in the form of a Gym environment, and those that do usually lack depth in modeling Vehicle-to-Grid (V2G) scenarios. To address the aforementioned issues, this paper introduces the EV2Gym, a realistic simulator platform for the development and assessment of small and large-scale smart charging algorithms within a standardized platform. The proposed simulator is populated with comprehensive EV, charging station, power transformer, and EV behavior models validated using real data. EV2Gym has a highly customizable interface empowering users to choose from pre-designed case studies or craft their own customized scenarios to suit their specific requirements. Moreover, it incorporates a diverse array of RL, mathematical programming, and heuristic algorithms to speed up the development and benchmarking of new solutions. By offering a unified and standardized platform, EV2Gym aims to provide researchers and practitioners with a robust environment for advancing and assessing smart charging algorithms.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "10 pages, 9 figures, and 6 tables"
    },
    {
        "paper id": "2404.01850",
        "abstract url": "https://arxiv.org/abs/2404.01850",
        "title": "Intelligent Reflecting Surfaces assisted Laser-based Optical Wireless Communication Networks",
        "rating": -2,
        "keywords": [
            [
                "face"
            ]
        ],
        "abstract": "The increasing demand for wireless networks of higher capacity requires key-enabling technologies. Optical wireless communication (OWC) arises as a complementary technology to radio frequency (RF) systems that can support high aggregate data rates. However, OWC systems face some challenges including beam-blockage. Intelligent reflecting surfaces (IRSs) can offer alternative pathways for the optical signal, ensuring continuous connectivity. In this work, we investigate the potential of using IRS in an indoor OWC network. In particular, we define a system model of indoor OWC that employs IRS in conjunction with angle diversity transmitters (ADT) using vertical-cavity surface-emitting laser (VCSEL) arrays. The VCSEL beam is narrow, directed, and easy to block, however, it can deliver high data rates under eye safety regulations. Simulation results show that the deployment of IRS can significantly improve the achievable data rates of Laser-based OWC systems.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "5 pages, 3 figures"
    },
    {
        "paper id": "2404.01855",
        "abstract url": "https://arxiv.org/abs/2404.01855",
        "title": "Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation",
        "rating": -2,
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions. We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01875",
        "abstract url": "https://arxiv.org/abs/2404.01875",
        "title": "Satellite Federated Edge Learning: Architecture Design and Convergence Analysis",
        "rating": -2,
        "keywords": [
            [
                "remote sensing",
                "Satellite"
            ]
        ],
        "abstract": "The proliferation of low-earth-orbit (LEO) satellite networks leads to the generation of vast volumes of remote sensing data which is traditionally transferred to the ground server for centralized processing, raising privacy and bandwidth concerns. Federated edge learning (FEEL), as a distributed machine learning approach, has the potential to address these challenges by sharing only model parameters instead of raw data. Although promising, the dynamics of LEO networks, characterized by the high mobility of satellites and short ground-to-satellite link (GSL) duration, pose unique challenges for FEEL. Notably, frequent model transmission between the satellites and ground incurs prolonged waiting time and large transmission latency. This paper introduces a novel FEEL algorithm, named FEDMEGA, tailored to LEO mega-constellation networks. By integrating inter-satellite links (ISL) for intra-orbit model aggregation, the proposed algorithm significantly reduces the usage of low data rate and intermittent GSL. Our proposed method includes a ring all-reduce based intra-orbit aggregation mechanism, coupled with a network flow-based transmission scheme for global model aggregation, which enhances transmission efficiency. Theoretical convergence analysis is provided to characterize the algorithm performance. Extensive simulations show that our FEDMEGA algorithm outperforms existing satellite FEEL algorithms, exhibiting an approximate 30% improvement in convergence rate.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "16 pages, 15 figures"
    },
    {
        "paper id": "2404.01885",
        "abstract url": "https://arxiv.org/abs/2404.01885",
        "title": "Enhancing Robot Navigation Efficiency Using Cellular Automata with Active Cells",
        "rating": -2,
        "keywords": [
            [
                "trajectory"
            ],
            [
                "robotics",
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "Enhancing robot navigation efficiency is a crucial objective in modern robotics. Robots relying on external navigation systems are often susceptible to electromagnetic interference (EMI) and encounter environmental disturbances, resulting in orientation errors within their surroundings. Therefore, the study employed an internal navigation system to enhance robot navigation efficacy under interference conditions, based on the analysis of the internal parameters and the external signals. This article presents details of the robot's autonomous operation, which allows for setting the robot's trajectory using an embedded map. The robot's navigation process involves counting the number of wheel revolutions as well as adjusting wheel orientation after each straight path section. In this article, an autonomous robot navigation system has been presented that leverages an embedded control navigation map utilising cellular automata with active cells which can effectively navigate in an environment containing various types of obstacles. By analysing the neighbouring cells of the active cell, the cellular environment determines which cell should become active during the robot's next movement step. This approach ensures the robot's independence from external control inputs. Furthermore, the accuracy and speed of the robot's movement have been further enhanced using a hexagonal mosaic for navigation surface mapping. This concept of utilising on cellular automata with active cells has been extended to the navigation of a group of robots on a shared navigation surface, taking into account the intersections of the robots' trajectories over time. To achieve this, a distance control module has been used that records the travelled trajectories in terms of wheel turns and revolutions.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01904",
        "abstract url": "https://arxiv.org/abs/2404.01904",
        "title": "Construction of quantum codes from $(\u03b3,\u0394)$-cyclic codes",
        "rating": -2,
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Let $\\mathbb{F}_q$ be the finite field of $q=p^m$ elements where $p$ is a prime and $m$ is a positive integer. This paper considers $(\u03b3,\u0394)$-cyclic codes over a class of finite commutative non-chain rings $\\mathscr{R}_{q,s}=\\mathbb{F}_q[v_1,v_2,\\dots,v_s]/\\langle v_i-v_i^2,v_iv_j=v_jv_i=0\\rangle$ where $\u03b3$ is an automorphism of $\\mathscr{R}_{q,s}$, $\u0394$ is a $\u03b3$-derivation of $\\mathscr{R}_{q,s}$ and $1\\leq i\\neq j\\leq s$ for a positive integer $s$. Here, we show that a $(\u03b3,\u0394)$-cyclic code of length $n$ over $\\mathscr{R}_{q,s}$ is the direct sum of $(\u03b8,\\Im)$-cyclic codes of length $n$ over $\\mathbb{F}_q$, where $\u03b8$ is an automorphism of $\\mathbb{F}_q$ and $\\Im$ is a $\u03b8$-derivation of $\\mathbb{F}_q$. Further, necessary and sufficient conditions for both $(\u03b3,\u0394)$-cyclic and $(\u03b8,\\Im)$-cyclic codes to contain their Euclidean duals are established. Finally, we obtain many quantum codes by applying the dual containing criterion on the Gray images of these codes. The obtained codes have better parameters than those available in the literature.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01907",
        "abstract url": "https://arxiv.org/abs/2404.01907",
        "title": "Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack",
        "rating": -2,
        "keywords": [
            [
                "Attack"
            ],
            [
                "face"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection models can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content. Furthermore, we explore the prospect of improving the model's robustness over iterative adversarial learning. Although some improvements in model robustness are observed, practical applications still face significant challenges. These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by COLING 2024"
    },
    {
        "paper id": "2404.01999",
        "abstract url": "https://arxiv.org/abs/2404.01999",
        "title": "Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning",
        "rating": -2,
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "Reinforcement learning (RL) is a flexible and efficient method for programming micro-robots in complex environments. Here we investigate whether reinforcement learning can provide insights into biological systems when trained to perform chemotaxis. Namely, whether we can learn about how intelligent agents process given information in order to swim towards a target. We run simulations covering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints on biological swimmers, namely Brownian motion, lead to regions where reinforcement learners' training fails. We find that the RL agents can perform chemotaxis as soon as it is physically possible and, in some cases, even before the active swimming overpowers the stochastic environment. We study the efficiency of the emergent policy and identify convergence in agent size and swim speeds. Finally, we study the strategy adopted by the reinforcement learning algorithm to explain how the agents perform their tasks. To this end, we identify three emerging dominant strategies and several rare approaches taken. These strategies, whilst producing almost identical trajectories in simulation, are distinct and give insight into the possible mechanisms behind which biological agents explore their environment and respond to changing conditions.",
        "subjects": [
            "physics.bio-ph"
        ],
        "comment": "12 pages, 6 figures"
    },
    {
        "paper id": "2404.02111",
        "abstract url": "https://arxiv.org/abs/2404.02111",
        "title": "Risk-Aware Real-Time Task Allocation for Stochastic Multi-Agent Systems under STL Specifications",
        "rating": -2,
        "keywords": [
            [
                "synthesis"
            ],
            [
                "autonomous driving"
            ]
        ],
        "abstract": "This paper addresses the control synthesis of heterogeneous stochastic linear multi-agent systems with real-time allocation of signal temporal logic (STL) specifications. Based on previous work, we decompose specifications into sub-specifications on the individual agent level. To leverage the efficiency of task allocation, a heuristic filter evaluates potential task allocation based on STL robustness. Subsequently, an auctioning algorithm determines the definite allocation of specifications. Finally, a control strategy is synthesized for each agent-specification pair using tube-based Model Predictive Control (MPC), ensuring provable probabilistic satisfaction. We demonstrate the efficacy of the proposed methods using a multi-bus scenario that highlights a promising extension to autonomous driving applications like crossing an intersection.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2404.02141",
        "abstract url": "https://arxiv.org/abs/2404.02141",
        "title": "Robustly estimating heterogeneity in factorial data using Rashomon Partitions",
        "rating": -2,
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the space of covariates using a tree-like geometry. RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates. This prior is the $\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS. We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS. Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques. We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance.",
        "subjects": [
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02143",
        "abstract url": "https://arxiv.org/abs/2404.02143",
        "title": "Multiparametric quantification and visualization of liver fat using ultrasound",
        "rating": -2,
        "keywords": [
            [
                "MRI",
                "clinical"
            ]
        ],
        "abstract": "Objectives- Several ultrasound measures have shown promise for assessment of steatosis compared to traditional B-scan, however clinicians may be required to integrate information across the parameters. Here, we propose an integrated multiparametric approach, enabling simple clinical assessment of key information from combined ultrasound parameters. Methods- We have measured 13 parameters related to ultrasound and shear wave elastography. These were measured in 30 human subjects under a study of liver fat. The 13 individual measures are assessed for their predictive value using independent magnetic resonance imaging-derived proton density fat fraction (MRI-PDFF) measurements as a reference standard. In addition, a comprehensive and fine-grain analysis is made of all possible combinations of sub-sets of these parameters to determine if any subset can be efficiently combined to predict fat fraction. Results- We found that as few as four key parameters related to ultrasound propagation are sufficient to generate a linear multiparametric parameter with a correlation against MRI-PDFF values of greater than 0.93. This optimal combination was found to have a classification area under the curve (AUC) approaching 1.0 when applying a threshold for separating steatosis grade zero from higher classes. Furthermore, a strategy is developed for applying local estimates of fat content as a color overlay to produce a visual impression of the extent and distribution of fat within the liver. Conclusion- In principle, this approach can be applied to most clinical ultrasound systems to provide the clinician and patient with a rapid and inexpensive estimate of liver fat content.",
        "subjects": [
            "physics.med-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01925",
        "abstract url": "https://arxiv.org/abs/2404.01925",
        "title": "Improving Bird's Eye View Semantic Segmentation by Task Decomposition",
        "rating": -2.5,
        "keywords": [
            [
                "depth"
            ],
            [
                "autonomous driving"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Semantic segmentation in bird's eye view (BEV) plays a crucial role in autonomous driving. Previous methods usually follow an end-to-end pipeline, directly predicting the BEV segmentation map from monocular RGB inputs. However, the challenge arises when the RGB inputs and BEV targets from distinct perspectives, making the direct point-to-point predicting hard to optimize. In this paper, we decompose the original BEV segmentation task into two stages, namely BEV map reconstruction and RGB-BEV feature alignment. In the first stage, we train a BEV autoencoder to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns. The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level. Our approach simplifies the complexity of combining perception and generation into distinct steps, equipping the model to handle intricate and challenging scenes effectively. Besides, we propose to transform the BEV segmentation map from the Cartesian to the polar coordinate system to establish the column-wise correspondence between RGB images and BEV maps. Moreover, our method requires neither multi-scale features nor camera intrinsic parameters for depth estimation and saves computational overhead. Extensive experiments on nuScenes and Argoverse show the effectiveness and efficiency of our method. Code is available at https://github.com/happytianhao/TaDe.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024"
    },
    {
        "paper id": "2404.02106",
        "abstract url": "https://arxiv.org/abs/2404.02106",
        "title": "Neural Ordinary Differential Equation based Sequential Image Registration for Dynamic Characterization",
        "rating": -2.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "trajectory"
            ],
            [
                "biological",
                "medical",
                "MRI",
                "clinical",
                "cardiac",
                "organ"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Deformable image registration (DIR) is crucial in medical image analysis, enabling the exploration of biological dynamics such as organ motions and longitudinal changes in imaging. Leveraging Neural Ordinary Differential Equations (ODE) for registration, this extension work discusses how this framework can aid in the characterization of sequential biological processes. Utilizing the Neural ODE's ability to model state derivatives with neural networks, our Neural Ordinary Differential Equation Optimization-based (NODEO) framework considers voxels as particles within a dynamic system, defining deformation fields through the integration of neural differential equations. This method learns dynamics directly from data, bypassing the need for physical priors, making it exceptionally suitable for medical scenarios where such priors are unavailable or inapplicable. Consequently, the framework can discern underlying dynamics and use sequence data to regularize the transformation trajectory. We evaluated our framework on two clinical datasets: one for cardiac motion tracking and another for longitudinal brain MRI analysis. Demonstrating its efficacy in both 2D and 3D imaging scenarios, our framework offers flexibility and model agnosticism, capable of managing image sequences and facilitating label propagation throughout these sequences. This study provides a comprehensive understanding of how the Neural ODE-based framework uniquely benefits the image registration challenge.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Journal extension of NODEO: A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration, CVPR 2022"
    },
    {
        "paper id": "2404.01689",
        "abstract url": "https://arxiv.org/abs/2404.01689",
        "title": "A Lightweight Security Solution for Mitigation of Hatchetman Attack in RPL-based 6LoWPAN",
        "rating": -3,
        "keywords": [
            [
                "Attack"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "In recent times, the Internet of Things (IoT) has a significant rise in industries, and we live in the era of Industry 4.0, where each device is connected to the Internet from small to big. These devices are Artificial Intelligence (AI) enabled and are capable of perspective analytics. By 2023, it's anticipated that over 14 billion smart devices will be available on the Internet. These applications operate in a wireless environment where memory, power, and other resource limitations apply to the nodes. In addition, the conventional routing method is ineffective in networks with limited resource devices, lossy links, and slow data rates. Routing Protocol for Low Power and Lossy Networks (RPL), a new routing protocol for such networks, was proposed by the IETF's ROLL group. RPL operates in two modes: Storing and Non-Storing. In Storing mode, each node have the information to reach to other node. In Non-Storing mode, the routing information lies with the root node only. The attacker may exploit the Non-Storing feature of the RPL. When the root node transmits User Datagram Protocol~(UDP) or control message packet to the child nodes, the routing information is stored in the extended header of the IPv6 packet. The attacker may modify the address from the source routing header which leads to Denial of Service (DoS) attack. This attack is RPL specific which is known as Hatchetman attack. This paper shows significant degradation in terms of network performance when an attacker exploits this feature. We also propose a lightweight mitigation of Hatchetman attack using game theoretic approach to detect the Hatchetman attack in IoT.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01887",
        "abstract url": "https://arxiv.org/abs/2404.01887",
        "title": "3D Scene Generation from Scene Graphs and Self-Attention",
        "rating": -3,
        "keywords": [
            [
                "3D"
            ],
            [
                "Synthesizing"
            ],
            [
                "navigation"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Synthesizing realistic and diverse indoor 3D scene layouts in a controllable fashion opens up applications in simulated navigation and virtual reality. As concise and robust representations of a scene, scene graphs have proven to be well-suited as the semantic control on the generated layout. We present a variant of the conditional variational autoencoder (cVAE) model to synthesize 3D scenes from scene graphs and floor plans. We exploit the properties of self-attention layers to capture high-level relationships between objects in a scene, and use these as the building blocks of our model. Our model, leverages graph transformers to estimate the size, dimension and orientation of the objects in a room while satisfying relationships in the given scene graph. Our experiments shows self-attention layers leads to sparser (HOW MUCH) and more diverse scenes (HOW MUCH)\\. Included in this work, we publish the first large-scale dataset for conditioned scene generation from scene graphs, containing over XXX rooms (of floor plans and scene graphs).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02029",
        "abstract url": "https://arxiv.org/abs/2404.02029",
        "title": "Enhancing Portfolio Optimization with Transformer-GAN Integration: A Novel Approach in the Black-Litterman Framework",
        "rating": -3,
        "keywords": [
            [
                "GAN"
            ],
            [
                "forecasting"
            ]
        ],
        "abstract": "This study presents an innovative approach to portfolio optimization by integrating Transformer models with Generative Adversarial Networks (GANs) within the Black-Litterman (BL) framework. Capitalizing on Transformers' ability to discern long-range dependencies and GANs' proficiency in generating accurate predictive models, our method enhances the generation of refined predictive views for BL portfolio allocations. This fusion of our model with BL's structured method for merging objective views with market equilibrium offers a potent tool for modern portfolio management, outperforming traditional forecasting methods. Our integrated approach not only demonstrates the potential to improve investment decision-making but also contributes a new approach to capture the complexities of financial markets for robust portfolio optimization.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01693",
        "abstract url": "https://arxiv.org/abs/2404.01693",
        "title": "HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning",
        "rating": -3.5,
        "keywords": [
            [
                "3D"
            ],
            [
                "graph"
            ],
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Understanding and leveraging the 3D structures of proteins is central to a variety of biological and drug discovery tasks. While deep learning has been applied successfully for structure-based protein function prediction tasks, current methods usually employ distinct training for each task. However, each of the tasks is of small size, and such a single-task strategy hinders the models' performance and generalization ability. As some labeled 3D protein datasets are biologically related, combining multi-source datasets for larger-scale multi-task learning is one way to overcome this problem. In this paper, we propose a neural network model to address multiple tasks jointly upon the input of 3D protein structures. In particular, we first construct a standard structure-based multi-task benchmark called Protein-MT, consisting of 6 biologically relevant tasks, including affinity prediction and property prediction, integrated from 4 public datasets. Then, we develop a novel graph neural network for multi-task learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is E(3) equivariant and able to capture heterogeneous relationships between different atoms. Besides, HeMeNet can achieve task-specific learning via the task-aware readout mechanism. Extensive evaluations on our benchmark verify the effectiveness of multi-task learning, and our model generally surpasses state-of-the-art models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01975",
        "abstract url": "https://arxiv.org/abs/2404.01975",
        "title": "DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation",
        "rating": -3.5,
        "keywords": [
            [
                "depth"
            ],
            [
                "Graph"
            ],
            [
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Air quality estimation can provide air quality for target regions without air quality stations, which is useful for the public. Existing air quality estimation methods divide the study area into disjointed grid regions, and apply 2D convolution to model the spatial dependencies of adjacent grid regions based on the first law of geography, failing to model the spatial dependencies of distant grid regions. To this end, we propose a Dual-view Supergrid-aware Graph Neural Network (DSGNN) for regional air quality estimation, which can model the spatial dependencies of distant grid regions from dual views (i.e., satellite-derived aerosol optical depth (AOD) and meteorology). Specifically, images are utilized to represent the regional data (i.e., AOD data and meteorology data). The dual-view supergrid learning module is introduced to generate supergrids in a parameterized way. Based on the dual-view supergrids, the dual-view implicit correlation encoding module is introduced to learn the correlations between pairwise supergrids. In addition, the dual-view message passing network is introduced to implement the information interaction on the supergrid graphs and images. Extensive experiments on two real-world datasets demonstrate that DSGNN achieves the state-of-the-art performances on the air quality estimation task, outperforming the best baseline by an average of 19.64% in MAE.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Submitted to TKDE, 12 pages and 8 figures"
    },
    {
        "paper id": "2404.01683",
        "abstract url": "https://arxiv.org/abs/2404.01683",
        "title": "Guided-Mutation Genetic Algorithm for Mobile IoT Network Relay",
        "rating": -4,
        "keywords": [
            [
                "healthcare"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "The Internet of Things (IoT) is a communication scheme which allows various objects to exchange several types of information, enabling functions such as home automation, production management, healthcare, etc. Moreover, energy-harvesting (EH) technology is considered for IoT environment in order to reduce the need for management and enhance maintainability. However, since environments considering outdoor elements such as pedestrians, vehicles and drones have been on the rise recently, it is important to consider mobility when designing an IoT network management scheme. In order to handle this challenge, prior research has made an attempt to solve this problem via variational autoencoder (VAE) and backward-pass rate evaluation method. In this article, we propose a guided-mutation genetic algorithm (GMGA) to derive a sub-optimal relaying topology for IoT systems considering energy-harvesting. Furthermore, we propose a mobility-aware iterative relaying topology algorithm, which calculates the sub-optimal relaying topology of current time frame using the topology result of the previous one. Simulation results verify that our proposed scheme effectively solves formulated IoT network problems compared to other conventional schemes, and also effectively handles IoT environments in terms of mobility.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "12 pages, 8 figures;Submitted to IEEE Access on 2 April 2024"
    },
    {
        "paper id": "2404.01897",
        "abstract url": "https://arxiv.org/abs/2404.01897",
        "title": "Continuous Spiking Graph Neural Networks",
        "rating": -4,
        "keywords": [
            [
                "diffusion"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02150",
        "abstract url": "https://arxiv.org/abs/2404.02150",
        "title": "From Seaweed to Security: The Emergence of Alginate in Compromising IoT Fingerprint Sensors",
        "rating": -4,
        "keywords": [
            [
                "biometric"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "The increasing integration of capacitive fingerprint recognition sensors in IoT devices presents new challenges in digital forensics, particularly in the context of advanced fingerprint spoofing. Previous research has highlighted the effectiveness of materials such as latex and silicone in deceiving biometric systems. In this study, we introduce Alginate, a biopolymer derived from brown seaweed, as a novel material with the potential for spoofing IoT-specific capacitive fingerprint sensors. Our research uses Alginate and cutting-edge image recognition techniques to unveil a nuanced IoT vulnerability that raises significant security and privacy concerns. Our proof-of-concept experiments employed authentic fingerprint molds to create Alginate replicas, which exhibited remarkable visual and tactile similarities to real fingerprints. The conductivity and resistivity properties of Alginate, closely resembling human skin, make it a subject of interest in the digital forensics field, especially regarding its ability to spoof IoT device sensors. This study calls upon the digital forensics community to develop advanced anti-spoofing strategies to protect the evolving IoT infrastructure against such sophisticated threats.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01644",
        "abstract url": "https://arxiv.org/abs/2404.01644",
        "title": "InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis",
        "rating": -10,
        "keywords": [],
        "abstract": "The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users' analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient identification, verification, and interpretation of insights within the current chat-based interfaces of LLMs. In this paper, we first conduct a formative study with eight experienced data analysts to understand their general workflow and pain points during LLM-powered data analysis. Then, we propose an LLM-based multi-agent framework to automatically extract, associate, and organize insights along with the analysis process. Based on this, we introduce InsightLens, an interactive system that visualizes the intricate conversational contexts from multiple aspects to facilitate insight discovery and exploration. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users' manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01670",
        "abstract url": "https://arxiv.org/abs/2404.01670",
        "title": "Locally tabular products of modal logics",
        "rating": -10,
        "keywords": [],
        "abstract": "In the product $L_1\\times L_2$ of two Kripke complete consistent logics, local tabularity of $L_1$ and $L_2$ is necessary for local tabularity of $L_1\\times L_2$. However, it is not sufficient: the product of two locally tabular logics can be not locally tabular. We provide extra semantic and axiomatic conditions which give criteria of local tabularity of the product of two locally tabular logics. Then we apply them to identify new families of locally tabular products.",
        "subjects": [
            "math.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01685",
        "abstract url": "https://arxiv.org/abs/2404.01685",
        "title": "A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling",
        "rating": -10,
        "keywords": [],
        "abstract": "Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70.84% accuracy for CIFAR100) with less than 10M parameters and up to 3.45x speed-up of searching time, thereby making it suitable for embedded applications.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "3 pages, 3 figures"
    },
    {
        "paper id": "2404.01697",
        "abstract url": "https://arxiv.org/abs/2404.01697",
        "title": "Preventing Model Collapse in Gaussian Process Latent Variable Models",
        "rating": -10,
        "keywords": [],
        "abstract": "Gaussian process latent variable models (GPLVMs) are a versatile family of unsupervised learning models, commonly used for dimensionality reduction. However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data. This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM. Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hyperparameters, projection variance, and latent representations within the variational inference framework. The proposed GPLVM, named advisedRFLVM, is evaluated across diverse datasets and consistently outperforms various salient competing models, including state-of-the-art variational autoencoders (VAEs) and GPLVM variants, in terms of informative latent representations and missing data imputation.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01704",
        "abstract url": "https://arxiv.org/abs/2404.01704",
        "title": "Application of S-band for Protection in Multi-band Flexible-Grid Optical Networks",
        "rating": -10,
        "keywords": [],
        "abstract": "The core network is experiencing bandwidth capacity constraints as internet traffic grows. As a result, the notion of a Multi-band flexible-grid optical network was established to increase the lifespan of an optical core network. In this paper, we use the C+L band for working traffic transmission and the S-band for protection against failure. Furthermore, we compare the proposed method with the existing ones.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "First Draft"
    },
    {
        "paper id": "2404.01741",
        "abstract url": "https://arxiv.org/abs/2404.01741",
        "title": "Intrusion Tolerance for Networked Systems through Two-Level Feedback Control",
        "rating": -10,
        "keywords": [],
        "abstract": "We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem. On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor. The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem. Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems. We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them. We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions. The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Preprint to appear in the conference proceedings of the 54th IEEE/IFIP Dependable Systems and Networks Conference (DSN'24)"
    },
    {
        "paper id": "2404.01754",
        "abstract url": "https://arxiv.org/abs/2404.01754",
        "title": "Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments",
        "rating": -10,
        "keywords": [],
        "abstract": "Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model based approaches, have gained notable recognition for their potential to fix introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the LLM. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. The evaluation on Defects4DS and another well-investigated ITSP dataset reveals that PaR achieves a new state-of-the-art performance, demonstrating impressive improvements of 19.94% and 15.2% in repair rate compared to prior state-of-the-art LLM- and symbolic-based approaches, respectively",
        "subjects": [
            "cs.SE"
        ],
        "comment": "On-going work"
    },
    {
        "paper id": "2404.01760",
        "abstract url": "https://arxiv.org/abs/2404.01760",
        "title": "Security for adversarial wiretap channels",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider the wiretap channel, where the individual channel uses have memory or are influenced by an adversary. We analyze the explicit and computationally efficient construction of information-theoretically secure coding schemes which use the inverse of an extractor and an error-correcting code. These schemes are known to achieve secrecy capacity on a large class of memoryless wiretap channels. We show that this also holds for certain channel types with memory. In particular, they can achieve secrecy capacity on channels where an adversary can pick a sequence of ``states'' governing the channel's behavior, as long as, given every possible state, the channel is strongly symmetric.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2404.01761",
        "abstract url": "https://arxiv.org/abs/2404.01761",
        "title": "A Formal Proof of R(4,5)=25",
        "rating": -10,
        "keywords": [],
        "abstract": "In 1995, McKay and Radziszowski proved that the Ramsey number R(4,5) is equal to 25. Their proof relies on a combination of high-level arguments and computational steps. The authors have performed the computational parts of the proof with different implementations in order to reduce the possibility of an error in their programs. In this work, we prove this theorem in the interactive theorem prover HOL4 limiting the uncertainty to the small HOL4 kernel. Instead of verifying their algorithms directly, we rely on a verified SAT solver to prove gluing lemmas. To reduce the number of such lemmas and thus make the computational part of the proof feasible, we implement a generalization algorithm and verify that its output covers all the possible cases.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01780",
        "abstract url": "https://arxiv.org/abs/2404.01780",
        "title": "CSST Strong Lensing Preparation: a Framework for Detecting Strong Lenses in the Multi-color Imaging Survey by the China Survey Space Telescope (CSST)",
        "rating": -10,
        "keywords": [],
        "abstract": "Strong gravitational lensing is a powerful tool for investigating dark matter and dark energy properties. With the advent of large-scale sky surveys, we can discover strong lensing systems on an unprecedented scale, which requires efficient tools to extract them from billions of astronomical objects. The existing mainstream lens-finding tools are based on machine learning algorithms and applied to cut-out-centered galaxies. However, according to the design and survey strategy of optical surveys by CSST, preparing cutouts with multiple bands requires considerable efforts. To overcome these challenges, we have developed a framework based on a hierarchical visual Transformer with a sliding window technique to search for strong lensing systems within entire images. Moreover, given that multi-color images of strong lensing systems can provide insights into their physical characteristics, our framework is specifically crafted to identify strong lensing systems in images with any number of channels. As evaluated using CSST mock data based on an Semi-Analytic Model named CosmoDC2, our framework achieves precision and recall rates of 0.98 and 0.90, respectively. To evaluate the effectiveness of our method in real observations, we have applied it to a subset of images from the DESI Legacy Imaging Surveys and media images from Euclid Early Release Observations. 61 new strong lensing system candidates are discovered by our method. However, we also identified false positives arising primarily from the simplified galaxy morphology assumptions within the simulation. This underscores the practical limitations of our approach while simultaneously highlighting potential avenues for future improvements.",
        "subjects": [
            "astro-ph.IM"
        ],
        "comment": "The paper is accepted by the AJ. The complete code could be downloaded with DOI of: 10.12149/101393. Comments are welcome"
    },
    {
        "paper id": "2404.01784",
        "abstract url": "https://arxiv.org/abs/2404.01784",
        "title": "Learning-Based Joint Beamforming and Antenna Movement Design for Movable Antenna Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we investigate a multi-receiver communication system enabled by movable antennas (MAs). Specifically, the transmit beamforming and the double-side antenna movement at the transceiver are jointly designed to maximize the sum-rate of all receivers under imperfect channel state information (CSI). Since the formulated problem is non-convex with highly coupled variables, conventional optimization methods cannot solve it efficiently. To address these challenges, an effective learning-based algorithm is proposed, namely heterogeneous multi-agent deep deterministic policy gradient (MADDPG), which incorporates two agents to learn policies for beamforming and movement of MAs, respectively. Based on the offline learning under numerous imperfect CSI, the proposed heterogeneous MADDPG can output the solutions for transmit beamforming and antenna movement in real time. Simulation results validate the effectiveness of the proposed algorithm, and the MA can significantly improve the sum-rate performance of multiple receivers compared to other benchmark schemes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "13 pages, 5 figures"
    },
    {
        "paper id": "2404.01789",
        "abstract url": "https://arxiv.org/abs/2404.01789",
        "title": "A Feature Dataset of Microservices-based Systems",
        "rating": -10,
        "keywords": [],
        "abstract": "Microservice architecture has become a dominant architectural style in the service-oriented software industry. Poor practices in the design and development of microservices are called microservice bad smells. In microservice bad smells research, the detection of these bad smells relies on feature data from microservices. However, there is a lack of an appropriate open-source microservice feature dataset. The availability of such datasets may contribute to the detection of microservice bad smells unexpectedly. To address this research gap, this paper collects a number of open-source microservice systems utilizing Spring Cloud. Additionally, feature metrics are established based on the architecture and interactions of Spring Boot style microservices. And an extraction program is developed. The program is then applied to the collected open-source microservice systems, extracting the necessary information, and undergoing manual verification to create an open-source feature dataset specific to microservice systems using Spring Cloud. The dataset is made available through a CSV file. We believe that both the extraction program and the dataset have the potential to contribute to the study of micro-service bad smells.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01796",
        "abstract url": "https://arxiv.org/abs/2404.01796",
        "title": "Open Experimental Measurements of Sub-6GHz Reconfigurable Intelligent Surfaces",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we present two datasets that we make publicly available for research. The data is collected in a testbed comprised of a custom-made Reconfigurable Intelligent Surface (RIS) prototype and two regular OFDM transceivers within an anechoic chamber. First, we discuss the details of the testbed and equipment used, including insights about the design and implementation of our RIS prototype. We further present the methodology we employ to gather measurement samples, which consists of letting the RIS electronically steer the signal reflections from an OFDM transmitter toward a specific location. To this end, we evaluate a suitably designed configuration codebook and collect measurement samples of the received power with an OFDM receiver. Finally, we present the resulting datasets, their format, and examples of exploiting this data for research purposes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01798",
        "abstract url": "https://arxiv.org/abs/2404.01798",
        "title": "On the Algorithmic Recovering of Coefficients in Linearizable Differential Equations",
        "rating": -10,
        "keywords": [],
        "abstract": "We investigate the problem of recovering coefficients in scalar nonlinear ordinary differential equations that can be exactly linearized. This contribution builds upon prior work by Lyakhov, Gerdt, and Michels, which focused on obtaining a linearizability certificate through point transformations. Our focus is on quasi-linear equations, specifically those solved for the highest derivative with a rational dependence on the variables involved. Our novel algorithm for coefficient recovery relies on basic operations on Lie algebras, such as computing the derived algebra and the dimension of the symmetry algebra. This algorithmic approach is efficient, although finding the linearization transformation necessitates computing at least one solution of the corresponding Bluman-Kumei equation system.",
        "subjects": [
            "cs.SC"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2404.01800",
        "abstract url": "https://arxiv.org/abs/2404.01800",
        "title": "Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest",
        "rating": -10,
        "keywords": [],
        "abstract": "Scientific articles play a crucial role in advancing knowledge and informing research directions. One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works. This article introduces the innovative use of large language models, particularly ChatGPT, for comprehensive sentiment analysis of citations within scientific articles. By leveraging advanced natural language processing (NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works. Furthermore, ChatGPT's capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation. This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and promoting integrity in scholarly research.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01808",
        "abstract url": "https://arxiv.org/abs/2404.01808",
        "title": "Software-Defined Cryptography: A Design Feature of Cryptographic Agility",
        "rating": -10,
        "keywords": [],
        "abstract": "Cryptographic agility, or crypto-agility, is a design feature that enables agile updates to new cryptographic algorithms and standards without the need to modify or replace the surrounding infrastructure. This paper examines the prerequisites for crypto-agility and proposes its desired design feature. More specifically, we investigate the design characteristics of widely deployed cybersecurity paradigms, i.e., zero trust, and apply its design feature to crypto-agility, achieving greater visibility and automation in cryptographic management.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01814",
        "abstract url": "https://arxiv.org/abs/2404.01814",
        "title": "A neural network-based approach to hybrid systems identification for control",
        "rating": -10,
        "keywords": [],
        "abstract": "We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design. We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network's parameters, thereby enabling the use of derivative-based training procedures. We show that a careful choice of our NN's weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP). Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization. In addition to being well-suited for optimal control design, numerical simulations illustrate that our NN-based technique enjoys very similar performance to state-of-the-art system identification methodologies for hybrid systems and it is competitive on nonlinear benchmarks.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01815",
        "abstract url": "https://arxiv.org/abs/2404.01815",
        "title": "Neuromorphic Split Computing with Wake-Up Radios: Architecture and Design via Digital Twinning",
        "rating": -10,
        "keywords": [],
        "abstract": "Neuromorphic computing leverages the sparsity of temporal data to reduce processing energy by activating a small subset of neurons and synapses at each time step. When deployed for split computing in edge-based systems, remote neuromorphic processing units (NPUs) can reduce the communication power budget by communicating asynchronously using sparse impulse radio (IR) waveforms. This way, the input signal sparsity translates directly into energy savings both in terms of computation and communication. However, with IR transmission, the main contributor to the overall energy consumption remains the power required to maintain the main radio on. This work proposes a novel architecture that integrates a wake-up radio mechanism within a split computing system consisting of remote, wirelessly connected, NPUs. A key challenge in the design of a wake-up radio-based neuromorphic split computing system is the selection of thresholds for sensing, wake-up signal detection, and decision making. To address this problem, as a second contribution, this work proposes a novel methodology that leverages the use of a digital twin (DT), i.e., a simulator, of the physical system, coupled with a sequential statistical testing approach known as Learn Then Test (LTT) to provide theoretical reliability guarantees. The proposed DT-LTT methodology is broadly applicable to other design problems, and is showcased here for neuromorphic communications. Experimental results validate the design and the analysis, confirming the theoretical reliability guarantees and illustrating trade-offs among reliability, energy consumption, and informativeness of the decisions.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2404.01818",
        "abstract url": "https://arxiv.org/abs/2404.01818",
        "title": "Analyzing the inter-domain vs intra-domain knowledge flows",
        "rating": -10,
        "keywords": [],
        "abstract": "Similar to how innovations often find success in fields other than their original domains, in this study we explore whether the same holds true for scientific discoveries. We investigate the flow of knowledge across scientific disciplines, focusing on connections between citing and cited publications. Specifically, we analyze the connections among cited publications from 2015 indexed in the Web of Science and their citing counterparts to measure rates of knowledge dissemination within and across different fields. Our study aims to address key research questions concerning the disparities between inter- and intra-domain knowledge dissemination rates, the correlation between knowledge dissemination types and scholarly impact, as well as the evolution of knowledge dissemination patterns over time. These findings deepen our understanding of knowledge flows and offer practical insights with significant implications for evaluative bibliometrics.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01820",
        "abstract url": "https://arxiv.org/abs/2404.01820",
        "title": "Model Predictive Control of District Heating Grids Using Stabilizing Terminal Ingredients",
        "rating": -10,
        "keywords": [],
        "abstract": "The transformation of fossil fuel-based district heating grids (DHGs) to CO$_2$-neutral DHGs requires the development of novel operating strategies. Model predictive control (MPC) is a promising approach, as knowledge about future heat demand and heat supply can be incorporated into the control, operating constraints can be ensured and the stability of the closed-loop system can be guaranteed. In this paper, we employ MPC for DHGs to control the system mass flows and injected heat flows. Following common practice, we derive terminal ingredients to stabilize given steady state temperatures and storage masses in the DHG. To apply MPC with terminal ingredients, it is crucial that the system under control is stabilizable. By exploiting the particular system structure, we give a sufficient condition for the stabilizability in terms of the grid topology and hence, for the applicability of the MPC scheme to DHGs. Furthermore, we demonstrate the practicability of the application of MPC to an exemplary DHG in a numerical case study.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted at European Control Conference 2024 in Stockholm"
    },
    {
        "paper id": "2404.01830",
        "abstract url": "https://arxiv.org/abs/2404.01830",
        "title": "Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy",
        "rating": -10,
        "keywords": [],
        "abstract": "We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01832",
        "abstract url": "https://arxiv.org/abs/2404.01832",
        "title": "When does Subagging Work?",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning. First, we give sufficient conditions for pointwise consistency of trees. We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance. While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally. Second, we compare the performance of subagging to that of trees across different numbers of splits. We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits. However, (3) a single tree grown at optimal size can outperform subagging if the size of its individual trees is not optimally chosen. This last result goes against common practice of growing large randomized trees to eliminate bias and then averaging to reduce variance.",
        "subjects": [
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01839",
        "abstract url": "https://arxiv.org/abs/2404.01839",
        "title": "Hypergraphs of girth 5 and 6 and coding theory",
        "rating": -10,
        "keywords": [],
        "abstract": "In this paper, we study the maximum number of edges in an $N$-vertex $r$-uniform hypergraph with girth $g$ where $g \\in \\{5,6 \\}$. Writing $\\textrm{ex}_r ( N, \\mathcal{C}_{<g} )$ for this maximum, it is shown that $\\textrm{ex}_r ( N , \\mathcal{C}_{ < 5} ) = \u03a9_r ( N^{3/2 - o(1)} )$ for $r \\in \\{4,5,6 \\}$. We address an unproved claim from [31] asserting a technique of Ruzsa can be used to show that this lower bound holds for all $r \\geq 3$. We carefully explain one of the main obstacles that was overlooked at the time the claim from [31] was made, and show that this obstacle can be overcome when $r\\in \\{4,5,6\\}$. We use constructions from coding theory to prove nontrivial lower bounds that hold for all $r \\geq 3$. Finally, we use a recent result of Conlon, Fox, Sudakov, and Zhao to show that the sphere packing bound from coding theory may be improved when upper bounding the size of linear $q$-ary codes of distance $6$.",
        "subjects": [
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01848",
        "abstract url": "https://arxiv.org/abs/2404.01848",
        "title": "\"That's Not Good Science!\": An Argument for the Thoughtful Use of Formative Situations in Research through Design",
        "rating": -10,
        "keywords": [],
        "abstract": "Most currently accepted approaches to evaluating Research through Design (RtD) presume that design prototypes are finalized and ready for robust testing in laboratory or in-the-wild settings. However, it is also valuable to assess designs at intermediate phases with mid-fidelity prototypes, not just to inform an ongoing design process, but also to glean knowledge of broader use to the research community. We propose 'formative situations' as a frame for examining mid-fidelity prototypes-in-process in this way. We articulate a set of criteria to help the community better assess the rigor of formative situations, in the service of opening conversation about establishing formative situations as a valuable contribution type within the RtD community.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "8 pages, 1 figure"
    },
    {
        "paper id": "2404.01858",
        "abstract url": "https://arxiv.org/abs/2404.01858",
        "title": "Keeping Behavioral Programs Alive: Specifying and Executing Liveness Requirements",
        "rating": -10,
        "keywords": [],
        "abstract": "One of the benefits of using executable specifications such as Behavioral Programming (BP) is the ability to align the system implementation with its requirements. This is facilitated in BP by a protocol that allows independent implementation modules that specify what the system may, must, and must not do. By that, each module can enforce a single system requirement, including negative specifications such as \"don't do X after Y.\" The existing BP protocol, however, allows only the enforcement of safety requirements and does not support the execution of liveness properties such as \"do X at least three times.\" To model liveness requirements in BP directly and independently, we propose idioms for tagging states with \"must-finish,\" indicating that tasks are yet to be completed. We show that this idiom allows a direct specification of known requirements patterns from the literature. We also offer semantics and two execution mechanisms, one based on a translation to B\u00fcchi automata and the other based on a Markov decision process (MDP). The latter approach offers the possibility of utilizing deep reinforcement learning (DRL) algorithms, which bear the potential to handle large software systems effectively. This paper presents a qualitative and quantitative assessment of the proposed approach using a proof-of-concept tool. A formal analysis of the MDP-based execution mechanism is given in an appendix.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Accepted to the 32nd IEEE International Requirements Engineering 2024 conference (RE'24)"
    },
    {
        "paper id": "2404.01861",
        "abstract url": "https://arxiv.org/abs/2404.01861",
        "title": "Integrating SystemC-AMS Power Modeling with a RISC-V ISS for Virtual Prototyping of Battery-operated Embedded Devices",
        "rating": -10,
        "keywords": [],
        "abstract": "RISC-V cores have gained a lot of popularity over the last few years. However, being quite a recent and novel technology, there is still a gap in the availability of comprehensive simulation frameworks for RISC-V that cover both the functional and extra-functional aspects. This gap hinders progress in the field, as fast yet accurate system-level simulation is crucial for Design Space Exploration (DSE). This work presents an open-source framework designed to tackle this challenge, integrating functional RISC-V simulation (achieved with GVSoC) with SystemC-AMS (used to model extra-functional aspects, in detail power storage and distribution). The combination of GVSoC and SystemC-AMS in a single simulation framework allows to perform a DSE that is dependent on the mutual impact between functional and extra-functional aspects. In our experiments, we validate the framework's effectiveness by creating a virtual prototype of a compact, battery-powered embedded system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "4 pages, 4 figures, to be published in Computing Frontiers Workshops and Special Sessions (CF '24 Companion), May 7--9, 2024, Ischia, Italy"
    },
    {
        "paper id": "2404.01883",
        "abstract url": "https://arxiv.org/abs/2404.01883",
        "title": "Adversarial Combinatorial Bandits with Switching Costs",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the problem of adversarial combinatorial bandit with a switching cost $\u03bb$ for a switch of each selected arm in each round, considering both the bandit feedback and semi-bandit feedback settings. In the oblivious adversarial case with $K$ base arms and time horizon $T$, we derive lower bounds for the minimax regret and design algorithms to approach them. To prove these lower bounds, we design stochastic loss sequences for both feedback settings, building on an idea from previous work in Dekel et al. (2014). The lower bound for bandit feedback is $ \\tilde\u03a9\\big( (\u03bbK)^{\\frac{1}{3}} (TI)^{\\frac{2}{3}}\\big)$ while that for semi-bandit feedback is $ \\tilde\u03a9\\big( (\u03bbK I)^{\\frac{1}{3}} T^{\\frac{2}{3}}\\big)$ where $I$ is the number of base arms in the combinatorial arm played in each round. To approach these lower bounds, we design algorithms that operate in batches by dividing the time horizon into batches to restrict the number of switches between actions. For the bandit feedback setting, where only the total loss of the combinatorial arm is observed, we introduce the Batched-Exp2 algorithm which achieves a regret upper bound of $\\tilde{O}\\big((\u03bbK)^{\\frac{1}{3}}T^{\\frac{2}{3}}I^{\\frac{4}{3}}\\big)$ as $T$ tends to infinity. In the semi-bandit feedback setting, where all losses for the combinatorial arm are observed, we propose the Batched-BROAD algorithm which achieves a regret upper bound of $\\tilde{O}\\big( (\u03bbK)^{\\frac{1}{3}} (TI)^{\\frac{2}{3}}\\big)$.",
        "subjects": [
            "stat.ML"
        ],
        "comment": "The work has been accepted in IEEE Transactions on Information Theory. https://ieeexplore.ieee.org/document/10487974"
    },
    {
        "paper id": "2404.01886",
        "abstract url": "https://arxiv.org/abs/2404.01886",
        "title": "Can My Microservice Tolerate an Unreliable Database? Resilience Testing with Fault Injection and Visualization",
        "rating": -10,
        "keywords": [],
        "abstract": "In microservice applications, ensuring resilience during database or service disruptions constitutes a significant challenge. While several tools address resilience testing for service failures, there is a notable gap in tools specifically designed for resilience testing of database failures. To bridge this gap, we have developed an extension for fault injection in database clients, which we integrated into Filibuster, an existing tool for fault injection in services within microservice applications. Our tool systematically simulates database disruptions, thereby enabling comprehensive testing and evaluation of application resilience. It is versatile, supporting a range of both SQL and NoSQL database systems, such as Redis, Apache Cassandra, CockroachDB, PostgreSQL, and DynamoDB. A defining feature is its integration during the development phase, complemented by an IntelliJ IDE plugin, which offers developers visual feedback on the types, locations, and impacts of injected faults. A video demonstration of the tool's capabilities is accessible at https://youtu.be/bvaUVCy1m1s.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01901",
        "abstract url": "https://arxiv.org/abs/2404.01901",
        "title": "Learning-based model augmentation with LFRs",
        "rating": -10,
        "keywords": [],
        "abstract": "Artificial neural networks (ANN) have proven to be effective in dealing with the identification nonlinear models for highly complex systems. To still make use of the prior information available from baseline models derived from, e.g., first-principles (FP), methods have been developed that integrate the prior knowledge into the identification algorithm for the ANN in a variety of methods. These methods have shown better estimation speeds and/or accuracy on unseen data. Among these methods are model augmentation structures. A variety of these structures have been considered in literature, there is however no unifying theory to these. In this paper, we propose a flexible linear-fractional-representation (LFR) based model augmentation structure. This model structure is able to represent many common model augmentation structures, thus unifying them under the proposed model structure. Furthermore, we introduce an identification algorithm capable of estimating the proposed model augmentation structure. The performance and generalization capabilities of the identification algorithm and the augmentation structure is demonstrated on a hardening mass-spring-damper simulation example.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted for CDC 2024"
    },
    {
        "paper id": "2404.01908",
        "abstract url": "https://arxiv.org/abs/2404.01908",
        "title": "Optimizing Offload Performance in Heterogeneous MPSoCs",
        "rating": -10,
        "keywords": [],
        "abstract": "Heterogeneous multi-core architectures combine a few \"host\" cores, optimized for single-thread performance, with many small energy-efficient \"accelerator\" cores for data-parallel processing, on a single chip. Offloading a computation to the many-core acceleration fabric introduces a communication and synchronization cost which reduces the speedup attainable on the accelerator, particularly for small and fine-grained parallel tasks. We demonstrate that by co-designing the hardware and offload routines, we can increase the speedup of an offloaded DAXPY kernel by as much as 47.9%. Furthermore, we show that it is possible to accurately model the runtime of an offloaded application, accounting for the offload overheads, with as low as 1% MAPE error, enabling optimal offload decisions under offload execution time constraints.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "2 pages, 1 figure. Accepted for publication in the DATE24 conference proceedings"
    },
    {
        "paper id": "2404.01931",
        "abstract url": "https://arxiv.org/abs/2404.01931",
        "title": "Fluid Implicit Particle Simulation for CPU and GPU",
        "rating": -10,
        "keywords": [],
        "abstract": "One of the current challenges in physically-based simulations, and, more specifically, fluid simulations, is to produce visually appealing results at interactive rates, capable of being used in multiple forms of media. In recent times, a lot of effort has been made with regards to this with the use of multi-core architectures, as many of the computations involved in the algorithms for these simulations are very well suited for these architectures. Although there is a considerable amount of works regarding acceleration techniques in this field, there is yet room to further explore and analyze some of them. To investigate this problem, we surveyed the topic of fluid simulations and some of the recent contributions towards this field. Additionally, we implemented two versions of a fluid simulation algorithm, one on the CPU and the other on the GPU using NVIDIA's CUDA framework, with the intent of gaining a better understanding of the effort needed to move these simulations to a multi-core architecture and the performance gains that we get with it.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01950",
        "abstract url": "https://arxiv.org/abs/2404.01950",
        "title": "An Exploratory Study of the Relationship between SATD and Other Software Development Activities",
        "rating": -10,
        "keywords": [],
        "abstract": "Technical Debt is a common issue that arises when short-term gains are prioritized over long-term costs, leading to a degradation in the quality of the code. Self-Admitted Technical Debt (SATD) is a specific type of Technical Debt that involves documenting code to remind developers of its debt. Previous research has explored various aspects of SATD, including detection methods, distribution, and its impact on software quality. To better understand SATD, one comprehension technique is to examine its co-occurrence with other activities, such as refactoring and bug fixing. This study investigates the relationship between removing and adding SATD and activities such as refactoring, bug fixing, adding new features, and testing. To do so, we analyzed 77 open-source Java projects using TODO/FIXME/XXX removal or addition in inline comments as indicators of SATD. We examined the co-occurrence of SATD with each activity in each project through chi-square and odds ratio evaluations. Our results show that SATD removal occurs simultaneously with refactoring in 95% of projects, while its addition occurs in 89% of projects. Furthermore, we found that three types of refactoring - \"move class\", \"remove method\", and \"move attribute\" - occur more frequently in the presence of SATD. However, their distribution is similar in projects with and without SATD.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "6 pages, DOI: 10.1109/ICCKE60553.2023.10326279"
    },
    {
        "paper id": "2404.01985",
        "abstract url": "https://arxiv.org/abs/2404.01985",
        "title": "The open access coverage of OpenAlex, Scopus and Web of Science",
        "rating": -10,
        "keywords": [],
        "abstract": "Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.01997",
        "abstract url": "https://arxiv.org/abs/2404.01997",
        "title": "Cash or Non-Cash? Unveiling Ideators' Incentive Preferences in Crowdsourcing Contests",
        "rating": -10,
        "keywords": [],
        "abstract": "Even though research has repeatedly shown that non-cash incentives can be effective, cash incentives are the de facto standard in crowdsourcing contests. In this multi-study research, we quantify ideators' preferences for non-cash incentives and investigate how allowing ideators to self-select their preferred incentive -- offering ideators a choice between cash and non-cash incentives -- affects their creative performance. We further explore whether the market context of the organization hosting the contest -- social (non-profit) or monetary (for-profit) -- moderates incentive preferences and their effectiveness. We find that individuals exhibit heterogeneous incentive preferences and often prefer non-cash incentives, even in for-profit contexts. Offering ideators a choice of incentives can enhance creative performance. Market context moderates the effect of incentives, such that ideators who receive non-cash incentives in for-profit contexts tend to exert less effort. We show that heterogeneity of ideators' preferences (and the ability to satisfy diverse preferences with suitably diverse incentive options) is a critical boundary condition to realizing benefits from offering ideators a choice of incentives. We provide managers with guidance to design effective incentives by improving incentive-preference fit for ideators.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Journal of Management Information Systems, forthcoming 2024"
    },
    {
        "paper id": "2404.02014",
        "abstract url": "https://arxiv.org/abs/2404.02014",
        "title": "On the Effect of Quantization on Dynamic Mode Decomposition",
        "rating": -10,
        "keywords": [],
        "abstract": "Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for estimating the Koopman Operator.This paper investigates how the estimation process is affected when the data is quantized. Specifically, we examine the fundamental connection between estimates of the operator obtained from unquantized data and those from quantized data. Furthermore, using the law of large numbers, we demonstrate that, under a large data regime, the quantized estimate can be considered a regularized version of the unquantized estimate. This key theoretical finding paves the way to accurately recover the unquantized estimate from quantized data. We also explore the relationship between the two estimates in the finite data regime. The theory is validated through repeated numerical experiments conducted on three different dynamical systems.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2404.02015",
        "abstract url": "https://arxiv.org/abs/2404.02015",
        "title": "MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving",
        "rating": -10,
        "keywords": [],
        "abstract": "Large language models (LLMs) have demon- strated remarkable performance, and organiza- tions are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally for- mulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal coloca- tions and maximize utilization. MuxServe de- signs a unified resource manager to enable flexi- ble and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99\\%$ SLO attainment.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02023",
        "abstract url": "https://arxiv.org/abs/2404.02023",
        "title": "On the Regret of Recursive Methods for Discrete-Time Adaptive Control with Matched Uncertainty",
        "rating": -10,
        "keywords": [],
        "abstract": "Continuous-time adaptive controllers for systems with a matched uncertainty often comprise an online parameter estimator and a corresponding parameterized controller to cancel the uncertainty. However, such methods are often unimplementable, as they depend on an unobserved estimation error. We consider the equivalent discrete-time setting with a causal information structure. We propose a novel, online proximal point method-based adaptive controller, that under a weak persistence of excitation (PE) condition is asymptotically stable and achieves finite regret, scaling only with the time required to fulfill the PE condition. We show the same also for the widely-used recursive least squares with exponential forgetting controller under a stronger PE condition.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02040",
        "abstract url": "https://arxiv.org/abs/2404.02040",
        "title": "Transformers as Transducers",
        "rating": -10,
        "keywords": [],
        "abstract": "We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of transductions. We do so using variants of RASP, a programming language designed to help people \"think like transformers,\" as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions. Finally, we show that masked average-hard attention transformers can simulate S-RASP. A corollary of our results is a new proof that transformer decoders are Turing-complete.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02056",
        "abstract url": "https://arxiv.org/abs/2404.02056",
        "title": "Multitask-based Evaluation of Open-Source LLM on Software Vulnerability",
        "rating": -10,
        "keywords": [],
        "abstract": "This paper proposes a pipeline for quantitatively evaluating interactive LLMs using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of LLMs based on this dataset. We find that the existing state-of-the-art methods are generally superior to LLMs in software vulnerability detection. Although LLMs improve accuracy when providing context information, they still have limitations in accurately predicting severity ratings for certain CWE types. In addition, LLMs demonstrate some ability to locate vulnerabilities for certain CWE types, but their performance varies among different CWE types. Finally, LLMs show uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in a few-shot setting. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights for further enhancing LLMs' software vulnerability handling capabilities.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02077",
        "abstract url": "https://arxiv.org/abs/2404.02077",
        "title": "Energy-Optimized Planning in Non-Uniform Wind Fields with Fixed-Wing Aerial Vehicles",
        "rating": -10,
        "keywords": [],
        "abstract": "Fixed-wing small uncrewed aerial vehicles (sUAVs) possess the capability to remain airborne for extended durations and traverse vast distances. However, their operation is susceptible to wind conditions, particularly in regions of complex terrain where high wind speeds may push the aircraft beyond its operational limitations, potentially raising safety concerns. Moreover, wind impacts the energy required to follow a path, especially in locations where the wind direction and speed are not favorable. Incorporating wind information into mission planning is essential to ensure both safety and energy efficiency. In this paper, we propose a sampling-based planner using the kinematic Dubins aircraft paths with respect to the ground, to plan energy-efficient paths in non-uniform wind fields. We study the planner characteristics with synthetic and real-world wind data and compare its performance against baseline cost and path formulations. We demonstrate that the energy-optimized planner effectively utilizes updrafts to minimize energy consumption, albeit at the expense of increased travel time. The ground-relative path formulation facilitates the generation of safe trajectories onboard sUAVs within reasonable computational timeframes.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2404.02081",
        "abstract url": "https://arxiv.org/abs/2404.02081",
        "title": "Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows",
        "rating": -10,
        "keywords": [],
        "abstract": "Explainable AI (XAI) tools represent a turn to more human-centered and human-in-the-loop AI approaches that emphasize user needs and perspectives in machine learning model development workflows. However, while the majority of ML resources available today are developed for Python computational environments such as JupyterLab and Jupyter Notebook, the same has not been true of interactive XAI systems, which are often still implemented as standalone interfaces. In this paper, we address this mismatch by identifying three design patterns for embedding front-end XAI interfaces into Jupyter, namely: 1) One-way communication from Python to JavaScript, 2) Two-way data synchronization, and 3) Bi-directional callbacks. We also provide an open-source toolkit, bonXAI, that demonstrates how each design pattern might be used to build interactive XAI tools for a Pytorch text classification workflow. Finally, we conclude with a discussion of best practices and open questions. Our aims for this paper are to discuss how interactive XAI tools might be developed for computational notebooks, and how they can better integrate into existing model development workflows to support more collaborative, human-centered AI.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "16 pages, 5 figures, ACM CHI 2024 Workshop on Human-Notebook Interactions"
    },
    {
        "paper id": "2404.02090",
        "abstract url": "https://arxiv.org/abs/2404.02090",
        "title": "Already Moderate Population Sizes Provably Yield Strong Robustness to Noise",
        "rating": -10,
        "keywords": [],
        "abstract": "Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations. In this first mathematical runtime analysis of the $(1+\u03bb)$ and $(1,\u03bb)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\u03bb$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy offspring. We are optimistic that the technical lemmas resulting from this insight will find applications also in future mathematical runtime analyses of evolutionary algorithms.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Full version of the same-titled paper accepted at GECCO 2024"
    },
    {
        "paper id": "2404.02142",
        "abstract url": "https://arxiv.org/abs/2404.02142",
        "title": "Priority-Neutral Matching Lattices Are Not Distributive",
        "rating": -10,
        "keywords": [],
        "abstract": "Stable matchings are a cornerstone of market design, with numerous practical deployments backed by a rich, theoretically-tractable structure. However, in school-choice problems, stable matchings are not Pareto optimal for the students. Priority-neutral matchings, introduced by Reny (AER, 2022), generalizes the set of stable matchings by allowing for certain priority violations, and there is always a Pareto optimal priority-neutral matching. Moreover, like stable matchings, the set of priority-neutral matchings forms a lattice. We study the structure of the priority-neutral lattice. Unfortunately, we show that much of the simplicity of the stable matching lattice does not hold for the priority-neutral lattice. In particular, we show that the priority-neutral lattice need not be distributive. Moreover, we show that the greatest lower bound of two matchings in the priority-neutral lattice need not be their student-by-student minimum, answering an open question. This show that many widely-used properties of stable matchings fail for priority-neutral matchings; in particular, the set of priority-neutral matchings cannot be represented by via a partial ordering on a set of rotations. However, by proving a novel structural property of the set of priority-neutral matchings, we also show that not every lattice arises as a priority-neutral lattice, which suggests that the exact nature of the family of priority-neutral lattices may be subtle.",
        "subjects": [
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2404.02147",
        "abstract url": "https://arxiv.org/abs/2404.02147",
        "title": "Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools",
        "rating": -10,
        "keywords": [],
        "abstract": "Human-centered AI (HCAI), rather than replacing the human, puts the human user in the driver's seat of so-called human-centered AI-infused tools (HCAI tools): interactive software tools that amplify, augment, empower, and enhance human performance using AI models; often novel generative or foundation AI ones. In this paper, we discuss how interactive visualization can be a key enabling technology for creating such human-centered AI tools. Visualization has already been shown to be a fundamental component in explainable AI models, and coupling this with data-driven, semantic, and unified interaction feedback loops will enable a human-centered approach to integrating AI models in the loop with human users. We present several examples of our past and current work on such HCAI tools, including for creative writing, temporal prediction, and user experience analysis. We then draw parallels between these tools to suggest common themes on how interactive visualization can support the design of future HCAI tools.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    }
]