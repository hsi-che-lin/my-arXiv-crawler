[
    {
        "paper id": "2404.18174",
        "abstract url": "https://arxiv.org/abs/2404.18174",
        "title": "Mamba-FETrack: Frame-Event Tracking via State Space Model",
        "rating": "2",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream). Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets. However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism. This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking. Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams. Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network. The fused features will be fed into the tracking head for target object localization. Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker. Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking. The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "In Peer Review"
    },
    {
        "paper id": "2404.18311",
        "abstract url": "https://arxiv.org/abs/2404.18311",
        "title": "Towards Incremental Learning in Large Language Models: A Critical Review",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Incremental learning is the ability of systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data changes frequently or is limited. This review provides a comprehensive analysis of incremental learning in Large Language Models. It synthesizes the state-of-the-art incremental learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for incremental learning by describing specific achievements from these related topics and their critical factors. An important finding is that many of these approaches do not update the core model, and none of them update incrementally in real-time. The paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of incremental learning and its implications for designing and developing LLM-based learning systems.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18180",
        "abstract url": "https://arxiv.org/abs/2404.18180",
        "title": "EkoHate: Abusive Language and Hate Speech Detection for Code-switched Political Discussions on Nigerian Twitter",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Nigerians have a notable online presence and actively discuss political and topical matters. This was particularly evident throughout the 2023 general election, where Twitter was used for campaigning, fact-checking and verification, and even positive and negative discourse. However, little or none has been done in the detection of abusive language and hate speech in Nigeria. In this paper, we curated code-switched Twitter data directed at three musketeers of the governorship election on the most populous and economically vibrant state in Nigeria; Lagos state, with the view to detect offensive speech in political discussions. We developed EkoHate -- an abusive language and hate speech dataset for political discussions between the three candidates and their followers using a binary (normal vs offensive) and fine-grained four-label annotation scheme. We analysed our dataset and provided an empirical evaluation of state-of-the-art methods across both supervised and cross-lingual transfer learning settings. In the supervised setting, our evaluation results in both binary and four-label annotation schemes show that we can achieve 95.1 and 70.3 F1 points respectively. Furthermore, we show that our dataset adequately transfers very well to three publicly available offensive datasets (OLID, HateUS2020, and FountaHate), generalizing to political discussions in other regions like the US.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "AfricaNLP workshop @ ICLR2024 and WOAH @ NAACL2024"
    },
    {
        "paper id": "2404.18191",
        "abstract url": "https://arxiv.org/abs/2404.18191",
        "title": "Exploring the Robustness of In-Context Learning with Noisy Labels",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Recently, the mysterious In-Context Learning (ICL) ability exhibited by Transformer architectures, especially in large language models (LLMs), has sparked significant research interest. However, the resilience of Transformers' in-context learning capabilities in the presence of noisy samples, prevalent in both training corpora and prompt demonstrations, remains underexplored. In this paper, inspired by prior research that studies ICL ability using simple function classes, we take a closer look at this problem by investigating the robustness of Transformers against noisy labels. Specifically, we first conduct a thorough evaluation and analysis of the robustness of Transformers against noisy labels during in-context learning and show that they exhibit notable resilience against diverse types of noise in demonstration labels. Furthermore, we delve deeper into this problem by exploring whether introducing noise into the training set, akin to a form of data augmentation, enhances such robustness during inference, and find that such noise can indeed improve the robustness of ICL. Overall, our fruitful analysis and findings provide a comprehensive understanding of the resilience of Transformer models against label noises during ICL and provide valuable insights into the research on Transformers in natural language processing. Our code is available at https://github.com/InezYu0928/in-context-learning.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CR",
            "cs.LG",
            "math.OC"
        ],
        "comment": "ICLR 2024 Workshop on Reliable and Responsible Foundation Models"
    },
    {
        "paper id": "2404.18260",
        "abstract url": "https://arxiv.org/abs/2404.18260",
        "title": "Align, Minimize and Diversify: A Source-Free Unsupervised Domain Adaptation Method for Handwritten Text Recognition",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "This paper serves to introduce the Align, Minimize and Diversify (AMD) method, a Source-Free Unsupervised Domain Adaptation approach for Handwritten Text Recognition (HTR). This framework decouples the adaptation process from the source data, thus not only sidestepping the resource-intensive retraining process but also making it possible to leverage the wealth of pre-trained knowledge encoded in modern Deep Learning architectures. Our method explicitly eliminates the need to revisit the source data during adaptation by incorporating three distinct regularization terms: the Align term, which reduces the feature distribution discrepancy between source and target data, ensuring the transferability of the pre-trained representation; the Minimize term, which encourages the model to make assertive predictions, pushing the outputs towards one-hot-like distributions in order to minimize prediction uncertainty, and finally, the Diversify term, which safeguards against the degeneracy in predictions by promoting varied and distinctive sequences throughout the target data, preventing informational collapse. Experimental results from several benchmarks demonstrated the effectiveness and robustness of AMD, showing it to be competitive and often outperforming DA methods in HTR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Submitted to ECCV 2024"
    },
    {
        "paper id": "2404.18316",
        "abstract url": "https://arxiv.org/abs/2404.18316",
        "title": "Position paper: Do not explain (vision models) without context",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Does the stethoscope in the picture make the adjacent person a doctor or a patient? This, of course, depends on the contextual relationship of the two objects. If it is obvious, why don not explanation methods for vision models use contextual information? In this paper, we (1) review the most popular methods of explaining computer vision models by pointing out that they do not take into account context information, (2) provide examples of real-world use cases where spatial context plays a significant role, (3) propose new research directions that may lead to better use of context information in explaining computer vision models, (4) argue that a change in approach to explanations is needed from 'where' to 'how'.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted for ICML 2024"
    },
    {
        "paper id": "2404.18399",
        "abstract url": "https://arxiv.org/abs/2404.18399",
        "title": "Semantic Line Combination Detector",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "A novel algorithm, called semantic line combination detector (SLCD), to find an optimal combination of semantic lines is proposed in this paper. It processes all lines in each line combination at once to assess the overall harmony of the lines. First, we generate various line combinations from reliable lines. Second, we estimate the score of each line combination and determine the best one. Experimental results demonstrate that the proposed SLCD outperforms existing semantic line detectors on various datasets. Moreover, it is shown that SLCD can be applied effectively to three vision tasks of vanishing point detection, symmetry axis detection, and composition-based image retrieval. Our codes are available at https://github.com/Jinwon-Ko/SLCD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024 accepted"
    },
    {
        "paper id": "2405.01588",
        "abstract url": "https://arxiv.org/abs/2405.01588",
        "title": "Towards Unbiased Evaluation of Detecting Unanswerable Questions in EHRSQL",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Incorporating unanswerable questions into EHR QA systems is crucial for testing the trustworthiness of a system, as providing non-existent responses can mislead doctors in their diagnoses. The EHRSQL dataset stands out as a promising benchmark because it is the only dataset that incorporates unanswerable questions in the EHR QA system alongside practical questions. However, in this work, we identify a data bias in these unanswerable questions; they can often be discerned simply by filtering with specific N-gram patterns. Such biases jeopardize the authenticity and reliability of QA system evaluations. To tackle this problem, we propose a simple debiasing method of adjusting the split between the validation and test sets to neutralize the undue influence of N-gram filtering. By experimenting on the MIMIC-III dataset, we demonstrate both the existing data bias in EHRSQL and the effectiveness of our data split strategy in mitigating this bias.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "DPFM Workshop, ICLR 2024"
    },
    {
        "paper id": "2404.18071",
        "abstract url": "https://arxiv.org/abs/2404.18071",
        "title": "Can Perplexity Predict Fine-Tuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent language models use subwording mechanisms to handle Out-of-Vocabulary(OOV) words seen during test time and, their generation capacity is generally measured using perplexity, an intrinsic metric. It is known that increasing the subword granularity results in a decrease of perplexity value. However, the study of how subwording affects the understanding capacity of language models has been very few and only limited to a handful of languages. To reduce this gap we used 6 different tokenization schemes to pretrain relatively small language models in Nepali and used the representations learned to finetune on several downstream tasks. Although byte-level BPE algorithm has been used in recent models like GPT, RoBERTa we show that on average they are sub-optimal in comparison to algorithms such as SentencePiece in finetuning performances for Nepali. Additionally, similar recent studies have focused on the Bert-based language model. We, however, pretrain and finetune sequential transformer-based language models.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2404.18072",
        "abstract url": "https://arxiv.org/abs/2404.18072",
        "title": "Contextual Spelling Correction with Language Model for Low-resource Setting",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The task of Spell Correction(SC) in low-resource languages presents a significant challenge due to the availability of only a limited corpus of data and no annotated spelling correction datasets. To tackle these challenges a small-scale word-based transformer LM is trained to provide the SC model with contextual understanding. Further, the probabilistic error rules are extracted from the corpus in an unsupervised way to model the tendency of error happening(error model). Then the combination of LM and error model is used to develop the SC model through the well-known noisy channel framework. The effectiveness of this approach is demonstrated through experiments on the Nepali language where there is access to just an unprocessed corpus of textual data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2404.18081",
        "abstract url": "https://arxiv.org/abs/2404.18081",
        "title": "ComposerX: Multi-Agent Symbolic Music Composition with LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs' potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18085",
        "abstract url": "https://arxiv.org/abs/2404.18085",
        "title": "CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned Large Language Model",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Domain-Specific Chinese Relation Extraction (DSCRE) aims to extract relations between entities from domain-specific Chinese text. Despite the rapid development of PLMs in recent years, especially LLMs, DSCRE still faces three core challenges: complex network structure design, poor awareness, and high consumption of fine-tuning. Given the impressive performance of large language models (LLMs) in natural language processing, we propose a new framework called CRE-LLM. This framework is based on fine-tuning open-source LLMs, such as Llama-2, ChatGLM2, and Baichuan2. CRE-LLM enhances the logic-awareness and generative capabilities of the model by constructing an appropriate prompt and utilizing open-source LLMs for instruction-supervised fine-tuning. And then it directly extracts the relations of the given entities in the input textual data, which improving the CRE approach. To demonstrate the effectiveness of the proposed framework, we conducted extensive experiments on two domain-specific CRE datasets, FinRE and SanWen. The experimental results show that CRE-LLM is significantly superior and robust, achieving state-of-the-art (SOTA) performance on the FinRE dataset. This paper introduces a novel approach to domain-specific relation extraction (DSCRE) tasks that are semantically more complex by combining LLMs with triples. Our code is publicly available.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "preprint"
    },
    {
        "paper id": "2404.18094",
        "abstract url": "https://arxiv.org/abs/2404.18094",
        "title": "USAT: A Universal Speaker-Adaptive Text-to-Speech Approach",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.CL",
            "eess.AS"
        ],
        "comment": "15 pages, 13 figures. Copyright has been transferred to IEEE"
    },
    {
        "paper id": "2404.18106",
        "abstract url": "https://arxiv.org/abs/2404.18106",
        "title": "Semi-supervised Text-based Person Search",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-based person search (TBPS) aims to retrieve images of a specific person from a large image gallery based on a natural language description. Existing methods rely on massive annotated image-text data to achieve satisfactory performance in fully-supervised learning. It poses a significant challenge in practice, as acquiring person images from surveillance videos is relatively easy, while obtaining annotated texts is challenging. The paper undertakes a pioneering initiative to explore TBPS under the semi-supervised setting, where only a limited number of person images are annotated with textual descriptions while the majority of images lack annotations. We present a two-stage basic solution based on generation-then-retrieval for semi-supervised TBPS. The generation stage enriches annotated data by applying an image captioning model to generate pseudo-texts for unannotated images. Later, the retrieval stage performs fully-supervised retrieval learning using the augmented data. Significantly, considering the noise interference of the pseudo-texts on retrieval learning, we propose a noise-robust retrieval framework that enhances the ability of the retrieval model to handle noisy data. The framework integrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refine the model architecture, and Noise-Guided Progressive Training (NP-Train) to enhance the training process. PC-Mask performs masking on the input data at both the patch-level and the channel-level to prevent overfitting noisy supervision. NP-Train introduces a progressive training schedule based on the noise level of pseudo-texts to facilitate noise-robust learning. Extensive experiments on multiple TBPS benchmarks show that the proposed framework achieves promising performance under the semi-supervised setting.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2404.18112",
        "abstract url": "https://arxiv.org/abs/2404.18112",
        "title": "Garbage Segmentation and Attribute Analysis by Robotic Dogs",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Efficient waste management and recycling heavily rely on garbage exploration and identification. In this study, we propose GSA2Seg (Garbage Segmentation and Attribute Analysis), a novel visual approach that utilizes quadruped robotic dogs as autonomous agents to address waste management and recycling challenges in diverse indoor and outdoor environments. Equipped with advanced visual perception system, including visual sensors and instance segmentators, the robotic dogs adeptly navigate their surroundings, diligently searching for common garbage items. Inspired by open-vocabulary algorithms, we introduce an innovative method for object attribute analysis. By combining garbage segmentation and attribute analysis techniques, the robotic dogs accurately determine the state of the trash, including its position and placement properties. This information enhances the robotic arm's grasping capabilities, facilitating successful garbage retrieval. Additionally, we contribute an image dataset, named GSA2D, to support evaluation. Through extensive experiments on GSA2D, this paper provides a comprehensive analysis of GSA2Seg's effectiveness. Dataset available: \\href{https://www.kaggle.com/datasets/hellob/gsa2d-2024}{https://www.kaggle.com/datasets/hellob/gsa2d-2024}.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18114",
        "abstract url": "https://arxiv.org/abs/2404.18114",
        "title": "Deep Boosting Learning: A Brand-new Cooperative Approach for Image-Text Matching",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image-text matching remains a challenging task due to heterogeneous semantic diversity across modalities and insufficient distance separability within triplets. Different from previous approaches focusing on enhancing multi-modal representations or exploiting cross-modal correspondence for more accurate retrieval, in this paper we aim to leverage the knowledge transfer between peer branches in a boosting manner to seek a more powerful matching model. Specifically, we propose a brand-new Deep Boosting Learning (DBL) algorithm, where an anchor branch is first trained to provide insights into the data properties, with a target branch gaining more advanced knowledge to develop optimal features and distance metrics. Concretely, an anchor branch initially learns the absolute or relative distance between positive and negative pairs, providing a foundational understanding of the particular network and data distribution. Building upon this knowledge, a target branch is concurrently tasked with more adaptive margin constraints to further enlarge the relative distance between matched and unmatched samples. Extensive experiments validate that our DBL can achieve impressive and consistent improvements based on various recent state-of-the-art models in the image-text matching field, and outperform related popular cooperative strategies, e.g., Conventional Distillation, Mutual Learning, and Contrastive Learning. Beyond the above, we confirm that DBL can be seamlessly integrated into their training scenarios and achieve superior performance under the same computational costs, demonstrating the flexibility and broad applicability of our proposed method. Our code is publicly available at: https://github.com/Paranioar/DBL.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": "12 pages, 9 figures, Accepted by TIP2024"
    },
    {
        "paper id": "2404.18130",
        "abstract url": "https://arxiv.org/abs/2404.18130",
        "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18143",
        "abstract url": "https://arxiv.org/abs/2404.18143",
        "title": "Tracking Transforming Objects: A Benchmark",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Tracking transforming objects holds significant importance in various fields due to the dynamic nature of many real-world scenarios. By enabling systems accurately represent transforming objects over time, tracking transforming objects facilitates advancements in areas such as autonomous systems, human-computer interaction, and security applications. Moreover, understanding the behavior of transforming objects provides valuable insights into complex interactions or processes, contributing to the development of intelligent systems capable of robust and adaptive perception in dynamic environments. However, current research in the field mainly focuses on tracking generic objects. In this study, we bridge this gap by collecting a novel dedicated Dataset for Tracking Transforming Objects, called DTTO, which contains 100 sequences, amounting to approximately 9.3K frames. We provide carefully hand-annotated bounding boxes for each frame within these sequences, making DTTO the pioneering benchmark dedicated to tracking transforming objects. We thoroughly evaluate 20 state-of-the-art trackers on the benchmark, aiming to comprehend the performance of existing methods and provide a comparison for future research on DTTO. With the release of DTTO, our goal is to facilitate further research and applications related to tracking transforming objects.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18154",
        "abstract url": "https://arxiv.org/abs/2404.18154",
        "title": "Explaining vague language",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Why is language vague? Vagueness may be explained and rationalized if it can be shown that vague language is more useful to speaker and hearer than precise language. In a well-known paper, Lipman proposes a game-theoretic account of vagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot be strictly better than precision at equilibrium. More recently, \u00c9gr\u00e9, Spector, Mortier and Verheyen have put forward a Bayesian account of vagueness establishing that using vague words can be strictly more informative than using precise words. This paper proposes to compare both results and to explain why they are not in contradiction. Lipman's definition of vagueness relies exclusively on a property of signaling strategies, without making any assumptions about the lexicon, whereas \u00c9gr\u00e9 et al.'s involves a layer of semantic content. We argue that the semantic account of vagueness is needed, and more adequate and explanatory of vagueness.",
        "subjects": [
            "cs.CL",
            "cs.GT",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18161",
        "abstract url": "https://arxiv.org/abs/2404.18161",
        "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Continual learning (CL) remains one of the long-standing challenges for deep neural networks due to catastrophic forgetting of previously acquired knowledge. Although rehearsal-based approaches have been fairly successful in mitigating catastrophic forgetting, they suffer from overfitting on buffered samples and prior information loss, hindering generalization under low-buffer regimes. Inspired by how humans learn using strong inductive biases, we propose IMEX-Reg to improve the generalization performance of experience rehearsal in CL under low buffer regimes. Specifically, we employ a two-pronged implicit-explicit regularization approach using contrastive representation learning (CRL) and consistency regularization. To further leverage the global relationship between representations learned using CRL, we propose a regularization strategy to guide the classifier toward the activation correlations in the unit hypersphere of the CRL. Our results show that IMEX-Reg significantly improves generalization performance and outperforms rehearsal-based approaches in several CL scenarios. It is also robust to natural and adversarial corruptions with less task-recency bias. Additionally, we provide theoretical insights to support our design decisions further.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "Published in Transactions on Machine Learning Research"
    },
    {
        "paper id": "2404.18185",
        "abstract url": "https://arxiv.org/abs/2404.18185",
        "title": "Ranked List Truncation for Large Language Model-based Re-Ranking",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We study ranked list truncation (RLT) from a novel \"retrieve-then-re-rank\" perspective, where we optimize re-ranking by truncating the retrieved list (i.e., trim re-ranking candidates). RLT is crucial for re-ranking as it can improve re-ranking efficiency by sending variable-length candidate lists to a re-ranker on a per-query basis. It also has the potential to improve re-ranking effectiveness. Despite its importance, there is limited research into applying RLT methods to this new perspective. To address this research gap, we reproduce existing RLT methods in the context of re-ranking, especially newly emerged large language model (LLM)-based re-ranking. In particular, we examine to what extent established findings on RLT for retrieval are generalizable to the \"retrieve-then-re-rank\" setup from three perspectives: (i) assessing RLT methods in the context of LLM-based re-ranking with lexical first-stage retrieval, (ii) investigating the impact of different types of first-stage retrievers on RLT methods, and (iii) investigating the impact of different types of re-rankers on RLT methods. We perform experiments on the TREC 2019 and 2020 deep learning tracks, investigating 8 RLT methods for pipelines involving 3 retrievers and 2 re-rankers. We reach new insights into RLT methods in the context of re-ranking.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted for publication as a long paper at SIGIR 2024"
    },
    {
        "paper id": "2404.18212",
        "abstract url": "https://arxiv.org/abs/2404.18212",
        "title": "Paint by Inpaint: Learning to Add Image Objects by Removing Them First",
        "rating": "1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "diffusion",
                "Inpaint",
                "Image editing"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Image editing has advanced significantly with the introduction of text-conditioned diffusion models. Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge. We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to the utilization of segmentation mask datasets alongside inpainting models that inpaint within these masks. Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions. Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images. Unlike other editing datasets, ours features natural target images instead of synthetic ones; moreover, it maintains consistency between source and target by construction. Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions. We show that the trained model surpasses existing ones both qualitatively and quantitatively, and release the large-scale dataset alongside the trained models for the community.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18216",
        "abstract url": "https://arxiv.org/abs/2404.18216",
        "title": "L3Cube-MahaNews: News-based Short Text and Long Document Classification Datasets in Marathi",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The availability of text or topic classification datasets in the low-resource Marathi language is limited, typically consisting of fewer than 4 target labels, with some achieving nearly perfect accuracy. In this work, we introduce L3Cube-MahaNews, a Marathi text classification corpus that focuses on News headlines and articles. This corpus stands out as the largest supervised Marathi Corpus, containing over 1.05L records classified into a diverse range of 12 categories. To accommodate different document lengths, MahaNews comprises three supervised datasets specifically designed for short text, long documents, and medium paragraphs. The consistent labeling across these datasets facilitates document length-based analysis. We provide detailed data statistics and baseline results on these datasets using state-of-the-art pre-trained BERT models. We conduct a comparative analysis between monolingual and multilingual BERT models, including MahaBERT, IndicBERT, and MuRIL. The monolingual MahaBERT model outperforms all others on every dataset. These resources also serve as Marathi topic classification datasets or models and are publicly available at https://github.com/l3cube-pune/MarathiNLP .",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted at SPELLL 2023"
    },
    {
        "paper id": "2404.18228",
        "abstract url": "https://arxiv.org/abs/2404.18228",
        "title": "TextGram: Towards a better domain-adaptive pretraining",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "For green AI, it is crucial to measure and reduce the carbon footprint emitted during the training of large language models. In NLP, performing pre-training on Transformer models requires significant computational resources. This pre-training involves using a large amount of text data to gain prior knowledge for performing downstream tasks. Thus, it is important that we select the correct data in the form of domain-specific data from this vast corpus to achieve optimum results aligned with our domain-specific tasks. While training on large unsupervised data is expensive, it can be optimized by performing a data selection step before pretraining. Selecting important data reduces the space overhead and the substantial amount of time required to pre-train the model while maintaining constant accuracy. We investigate the existing selection strategies and propose our own domain-adaptive data selection method - TextGram - that effectively selects essential data from large corpora. We compare and evaluate the results of finetuned models for text classification task with and without data selection. We show that the proposed strategy works better compared to other selection methods.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted at SPELLL 2023"
    },
    {
        "paper id": "2404.18231",
        "abstract url": "https://arxiv.org/abs/2404.18231",
        "title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2404.18243",
        "abstract url": "https://arxiv.org/abs/2404.18243",
        "title": "LEGENT: Open Platform for Embodied Agents",
        "rating": "1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "3D"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in physical environments. Existing integrations often feature limited open sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich, interactive 3D environment with communicable and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Demo Paper"
    },
    {
        "paper id": "2404.18246",
        "abstract url": "https://arxiv.org/abs/2404.18246",
        "title": "AdaFSNet: Time Series Classification Based on Convolutional Network with a Adaptive and Effective Kernel Size Configuration",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Time series classification is one of the most critical and challenging problems in data mining, existing widely in various fields and holding significant research importance. Despite extensive research and notable achievements with successful real-world applications, addressing the challenge of capturing the appropriate receptive field (RF) size from one-dimensional or multi-dimensional time series of varying lengths remains a persistent issue, which greatly impacts performance and varies considerably across different datasets. In this paper, we propose an Adaptive and Effective Full-Scope Convolutional Neural Network (AdaFSNet) to enhance the accuracy of time series classification. This network includes two Dense Blocks. Particularly, it can dynamically choose a range of kernel sizes that effectively encompass the optimal RF size for various datasets by incorporating multiple prime numbers corresponding to the time series length. We also design a TargetDrop block, which can reduce redundancy while extracting a more effective RF. To assess the effectiveness of the AdaFSNet network, comprehensive experiments were conducted using the UCR and UEA datasets, which include one-dimensional and multi-dimensional time series data, respectively. Our model surpassed baseline models in terms of classification accuracy, underscoring the AdaFSNet network's efficiency and effectiveness in handling time series classification tasks.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "Accepted by IJCNN 2024"
    },
    {
        "paper id": "2404.18257",
        "abstract url": "https://arxiv.org/abs/2404.18257",
        "title": "Mapping 'when'-clauses in Latin American and Caribbean languages: an experiment in subtoken-based typology",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Languages can encode temporal subordination lexically, via subordinating conjunctions, and morphologically, by marking the relation on the predicate. Systematic cross-linguistic variation among the former can be studied using well-established token-based typological approaches to token-aligned parallel corpora. Variation among different morphological means is instead much harder to tackle and therefore more poorly understood, despite being predominant in several language groups. This paper explores variation in the expression of generic temporal subordination ('when'-clauses) among the languages of Latin America and the Caribbean, where morphological marking is particularly common. It presents probabilistic semantic maps computed on the basis of the languages of the region, thus avoiding bias towards the many world's languages that exclusively use lexified connectors, incorporating associations between character $n$-grams and English $when$. The approach allows capturing morphological clause-linkage devices in addition to lexified connectors, paving the way for larger-scale, strategy-agnostic analyses of typological variation in temporal subordination.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": "10 pages, 6 figures. To be published in the 2024 Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)"
    },
    {
        "paper id": "2404.18264",
        "abstract url": "https://arxiv.org/abs/2404.18264",
        "title": "Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Nigerian Pidgin is an English-derived contact language and is traditionally an oral language, spoken by approximately 100 million people. No orthographic standard has yet been adopted, and thus the few available Pidgin datasets that exist are characterised by noise in the form of orthographic variations. This contributes to under-performance of models in critical NLP tasks. The current work is the first to describe various types of orthographic variations commonly found in Nigerian Pidgin texts, and model this orthographic variation. The variations identified in the dataset form the basis of a phonetic-theoretic framework for word editing, which is used to generate orthographic variations to augment training data. We test the effect of this data augmentation on two critical NLP tasks: machine translation and sentiment analysis. The proposed variation generation framework augments the training data with new orthographic variants which are relevant for the test set but did not occur in the training set originally. Our results demonstrate the positive effect of augmenting the training data with a combination of real texts from other corpora as well as synthesized orthographic variation, resulting in performance improvements of 2.1 points in sentiment analysis and 1.4 BLEU points in translation to English.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted to LREC-COLING 2024 Main Conference"
    },
    {
        "paper id": "2404.18271",
        "abstract url": "https://arxiv.org/abs/2404.18271",
        "title": "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning",
        "rating": "1",
        "keywords": [
            [
                "Parameter-Efficient",
                "efficient fine-tuning"
            ],
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Text-rich graphs, which exhibit rich textual information on nodes and edges, are prevalent across a wide range of real-world business applications. Large Language Models (LLMs) have demonstrated remarkable abilities in understanding text, which also introduced the potential for more expressive modeling in text-rich graphs. Despite these capabilities, efficiently applying LLMs to representation learning on graphs presents significant challenges. Recently, parameter-efficient fine-tuning methods for LLMs have enabled efficient new task generalization with minimal time and memory consumption. Inspired by this, we introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel approach for efficient graph representation learning with LLMs on text-rich graphs. Specifically, we utilize a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt. This prompt is then inserted at the beginning of the text sequence. To improve the quality of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting the next token in the node text. Compared with existing joint GNN and LMs, our method directly generate the node embeddings from large language models with an affordable fine-tuning cost. We validate our approach through comprehensive experiments conducted on 8 different text-rich graphs, observing an average improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction evaluations. Our results demonstrate the efficacy and efficiency of our model, showing that it can be smoothly integrated with various large language models, including OPT, LLaMA and Falcon.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18276",
        "abstract url": "https://arxiv.org/abs/2404.18276",
        "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems. In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical. This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs. Our framework combines the Large Language Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations. By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer & Bender, 2023].",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "41 pages"
    },
    {
        "paper id": "2404.18286",
        "abstract url": "https://arxiv.org/abs/2404.18286",
        "title": "Comparing LLM prompting with Cross-lingual transfer performance on Indigenous and Low-resource Brazilian Languages",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models are transforming NLP for a variety of tasks. However, how LLMs perform NLP tasks for low-resource languages (LRLs) is less explored. In line with the goals of the AmericasNLP workshop, we focus on 12 LRLs from Brazil, 2 LRLs from Africa and 2 high-resource languages (HRLs) (e.g., English and Brazilian Portuguese). Our results indicate that the LLMs perform worse for the part of speech (POS) labeling of LRLs in comparison to HRLs. We explain the reasons behind this failure and provide an error analysis through examples observed in our data set.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to the Americas NLP Workshop at NAACL 2024 (https://turing.iimas.unam.mx/americasnlp/2024_workshop.html)"
    },
    {
        "paper id": "2404.18294",
        "abstract url": "https://arxiv.org/abs/2404.18294",
        "title": "PSTAIC regularization for 2D spatiotemporal image reconstruction",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "We propose a model for restoration of spatio-temporal TIRF images based on infimal decomposition regularization model named STAIC proposed earlier. We propose to strengthen the STAIC algorithm by enabling it to estimate the relative weights in the regularization term by incorporating it as part of the optimization problem. We also design an iterative scheme which alternatively minimizes the weight and image sub-problems. We demonstrate the restoration quality of this regularization scheme against other restoration models enabled by similar weight estimation schemes.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2404.05070"
    },
    {
        "paper id": "2404.18327",
        "abstract url": "https://arxiv.org/abs/2404.18327",
        "title": "MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion Recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents a novel approach to processing multimodal data for dynamic emotion recognition, named as the Multimodal Masked Autoencoder for Dynamic Emotion Recognition (MultiMAE-DER). The MultiMAE-DER leverages the closely correlated representation information within spatiotemporal sequences across visual and audio modalities. By utilizing a pre-trained masked autoencoder model, the MultiMAEDER is accomplished through simple, straightforward finetuning. The performance of the MultiMAE-DER is enhanced by optimizing six fusion strategies for multimodal input sequences. These strategies address dynamic feature correlations within cross-domain data across spatial, temporal, and spatiotemporal sequences. In comparison to state-of-the-art multimodal supervised learning models for dynamic emotion recognition, MultiMAE-DER enhances the weighted average recall (WAR) by 4.41% on the RAVDESS dataset and by 2.06% on the CREMAD. Furthermore, when compared with the state-of-the-art model of multimodal self-supervised learning, MultiMAE-DER achieves a 1.86% higher WAR on the IEMOCAP dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ICPRS 2024"
    },
    {
        "paper id": "2404.18355",
        "abstract url": "https://arxiv.org/abs/2404.18355",
        "title": "Pi\u00e8ces de viole des Cinq Livres and their statistical signatures: the musical work of Marin Marais and Jordi Savall",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This study analyzes the spectrum of audio signals related to the work of \"Pi\u00e8ces de viole des Cinq Livres\" based on the collaborative work between Marin Marais and Jordi Savall for the underlying musical information. In particular, we explore the identification of possible statistical signatures related to this musical work. Based on the complex systems approach, we compute the spectrum of audio signals, analyze and identify their best-fit statistical distributions, and plot their relative frequencies using the scientific pitch notation. Findings suggest that the collection of frequency components related to the spectrum of each of the books that form this audio work show highly skewed and associated statistical distributions. Therefore, the most frequent statistical distribution that best describes the collection of these audio data and may be associated with a singular statistical signature is the exponential.",
        "subjects": [
            "cs.SD",
            "eess.AS",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18397",
        "abstract url": "https://arxiv.org/abs/2404.18397",
        "title": "ViOCRVQA: Novel Benchmark Dataset and Vision Reader for Visual Question Answering by Understanding Vietnamese Text in Images",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Optical Character Recognition - Visual Question Answering (OCR-VQA) is the task of answering text information contained in images that have just been significantly developed in the English language in recent years. However, there are limited studies of this task in low-resource languages such as Vietnamese. To this end, we introduce a novel dataset, ViOCRVQA (Vietnamese Optical Character Recognition - Visual Question Answering dataset), consisting of 28,000+ images and 120,000+ question-answer pairs. In this dataset, all the images contain text and questions about the information relevant to the text in the images. We deploy ideas from state-of-the-art methods proposed for English to conduct experiments on our dataset, revealing the challenges and difficulties inherent in a Vietnamese dataset. Furthermore, we introduce a novel approach, called VisionReader, which achieved 0.4116 in EM and 0.6990 in the F1-score on the test set. Through the results, we found that the OCR system plays a very important role in VQA models on the ViOCRVQA dataset. In addition, the objects in the image also play a role in improving model performance. We open access to our dataset at link (https://github.com/qhnhynmm/ViOCRVQA.git) for further research in OCR-VQA task in Vietnamese.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18398",
        "abstract url": "https://arxiv.org/abs/2404.18398",
        "title": "MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Emotional Text-to-Speech (E-TTS) synthesis has gained significant attention in recent years due to its potential to enhance human-computer interaction. However, current E-TTS approaches often struggle to capture the complexity of human emotions, primarily relying on oversimplified emotional labels or single-modality inputs. To address these limitations, we propose the Multimodal Emotional Text-to-Speech System (MM-TTS), a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech. MM-TTS consists of two key components: (1) the Emotion Prompt Alignment Module (EP-Align), which employs contrastive learning to align emotional features across text, audio, and visual modalities, ensuring a coherent fusion of multimodal information; and (2) the Emotion Embedding-Induced TTS (EMI-TTS), which integrates the aligned emotional embeddings with state-of-the-art TTS models to synthesize speech that accurately reflects the intended emotions. Extensive evaluations across diverse datasets demonstrate the superior performance of MM-TTS compared to traditional E-TTS models. Objective metrics, including Word Error Rate (WER) and Character Error Rate (CER), show significant improvements on ESD dataset, with MM-TTS achieving scores of 7.35% and 3.07%, respectively. Subjective assessments further validate that MM-TTS generates speech with emotional fidelity and naturalness comparable to human speech. Our code and pre-trained models are publicly available at https://anonymous.4open.science/r/MMTTS-D214",
        "subjects": [
            "cs.CL",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18410",
        "abstract url": "https://arxiv.org/abs/2404.18410",
        "title": "Mixture-of-Instructions: Comprehensive Alignment of a Large Language Model through the Mixture of Diverse System Prompting Instructions",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. However, AI-driven products that leverage language models usually necessitate a fusion of these abilities to function effectively in real-world scenarios. Moreover, the considerable computational resources required for proper alignment of LLMs underscore the need for a more robust, efficient, and encompassing approach to multi-task alignment, ensuring improved generative performance. In response to these challenges, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction concatenation combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18413",
        "abstract url": "https://arxiv.org/abs/2404.18413",
        "title": "3AM: An Ambiguity-Aware Multi-Modal Machine Translation Dataset",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal machine translation (MMT) is a challenging task that seeks to improve translation quality by incorporating visual information. However, recent studies have indicated that the visual information provided by existing MMT datasets is insufficient, causing models to disregard it and overestimate their capabilities. This issue presents a significant obstacle to the development of MMT research. This paper presents a novel solution to this issue by introducing 3AM, an ambiguity-aware MMT dataset comprising 26,000 parallel sentence pairs in English and Chinese, each with corresponding images. Our dataset is specifically designed to include more ambiguity and a greater variety of both captions and images than other MMT datasets. We utilize a word sense disambiguation model to select ambiguous data from vision-and-language datasets, resulting in a more challenging dataset. We further benchmark several state-of-the-art MMT models on our proposed dataset. Experimental results show that MMT models trained on our dataset exhibit a greater ability to exploit visual information than those trained on other MMT datasets. Our work provides a valuable resource for researchers in the field of multimodal learning and encourages further exploration in this area. The data, code and scripts are freely available at https://github.com/MaxyLee/3AM.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18423",
        "abstract url": "https://arxiv.org/abs/2404.18423",
        "title": "Unsupervised Dynamics Prediction with Object-Centric Kinematics",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Human perception involves discerning complex multi-object scenes into time-static object appearance (ie, size, shape, color) and time-varying object motion (ie, location, velocity, acceleration). This innate ability to unconsciously understand the environment is the motivation behind the success of dynamics modeling. Object-centric representations have emerged as a promising tool for dynamics prediction, yet they primarily focus on the objects' appearance, often overlooking other crucial attributes. In this paper, we propose Object-Centric Kinematics (OCK), a framework for dynamics prediction leveraging object-centric representations. Our model utilizes a novel component named object kinematics, which comprises low-level structured states of objects' position, velocity, and acceleration. The object kinematics are obtained via either implicit or explicit approaches, enabling comprehensive spatiotemporal object reasoning, and integrated through various transformer mechanisms, facilitating effective object-centric dynamics modeling. Our model demonstrates superior performance when handling objects and backgrounds in complex scenes characterized by a wide range of object attributes and dynamic movements. Moreover, our model demonstrates generalization capabilities across diverse synthetic environments, highlighting its potential for broad applicability in vision-related tasks.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "15 pages, 6 figures, 4 tables"
    },
    {
        "paper id": "2404.19527",
        "abstract url": "https://arxiv.org/abs/2404.19527",
        "title": "Revealing the Two Sides of Data Augmentation: An Asymmetric Distillation-based Win-Win Solution for Open-Set Recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we reveal the two sides of data augmentation: enhancements in closed-set recognition correlate with a significant decrease in open-set recognition. Through empirical investigation, we find that multi-sample-based augmentations would contribute to reducing feature discrimination, thereby diminishing the open-set criteria. Although knowledge distillation could impair the feature via imitation, the mixed feature with ambiguous semantics hinders the distillation. To this end, we propose an asymmetric distillation framework by feeding teacher model extra raw data to enlarge the benefit of teacher. Moreover, a joint mutual information loss and a selective relabel strategy are utilized to alleviate the influence of hard mixed samples. Our method successfully mitigates the decline in open-set and outperforms SOTAs by 2%~3% AUROC on the Tiny-ImageNet dataset and experiments on large-scale dataset ImageNet-21K demonstrate the generalization of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.01584",
        "abstract url": "https://arxiv.org/abs/2405.01584",
        "title": "Lightweight Conceptual Dictionary Learning for Text Classification Using Information Compression",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We propose a novel, lightweight supervised dictionary learning framework for text classification based on data compression and representation. This two-phase algorithm initially employs the Lempel-Ziv-Welch (LZW) algorithm to construct a dictionary from text datasets, focusing on the conceptual significance of dictionary elements. Subsequently, dictionaries are refined considering label data, optimizing dictionary atoms to enhance discriminative power based on mutual information and class distribution. This process generates discriminative numerical representations, facilitating the training of simple classifiers such as SVMs and neural networks. We evaluate our algorithm's information-theoretic performance using information bottleneck principles and introduce the information plane area rank (IPAR) as a novel metric to quantify the information-theoretic performance. Tested on six benchmark text datasets, our algorithm competes closely with top models, especially in limited-vocabulary contexts, using significantly fewer parameters. \\review{Our algorithm closely matches top-performing models, deviating by only ~2\\% on limited-vocabulary datasets, using just 10\\% of their parameters. However, it falls short on diverse-vocabulary datasets, likely due to the LZW algorithm's constraints with low-repetition data. This contrast highlights its efficiency and limitations across different dataset types.",
        "subjects": [
            "cs.CL",
            "cs.LG",
            "eess.SP"
        ],
        "comment": "12 pages, TKDE format"
    },
    {
        "paper id": "2405.01586",
        "abstract url": "https://arxiv.org/abs/2405.01586",
        "title": "Transfer Learning and Transformer Architecture for Financial Sentiment Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Financial sentiment analysis allows financial institutions like Banks and Insurance Companies to better manage the credit scoring of their customers in a better way. Financial domain uses specialized mechanisms which makes sentiment analysis difficult. In this paper, we propose a pre-trained language model which can help to solve this problem with fewer labelled data. We extend on the principles of Transfer learning and Transformation architecture principles and also take into consideration recent outbreak of pandemics like COVID. We apply the sentiment analysis to two different sets of data. We also take smaller training set and fine tune the same as part of the model.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 9 figures"
    },
    {
        "paper id": "2405.01587",
        "abstract url": "https://arxiv.org/abs/2405.01587",
        "title": "Improve Academic Query Resolution through BERT-based Question Extraction from Images",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18075",
        "abstract url": "https://arxiv.org/abs/2404.18075",
        "title": "Comparing E-bike and Conventional Bicycle Use Patterns in a Public Bike Share System: A Case Study of Richmond, VA",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The results show that pedelecs are generally associated with longer trip distances, shorter trip times, higher speeds, and lower rates of uphill elevation change. The origin-destination analysis considering the business, mixed use, residential, and other uses shows extremely similar trends, with a large number of trips staying within either business or residential locations or mixed use. The roadway use analysis shows that pedelecs are used farther outside of the city than bikes.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18134",
        "abstract url": "https://arxiv.org/abs/2404.18134",
        "title": "Enhancing Fairness in Neural Networks Using FairVIC",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Mitigating bias in automated decision-making systems, specifically deep learning models, is a critical challenge in achieving fairness. This complexity stems from factors such as nuanced definitions of fairness, unique biases in each dataset, and the trade-off between fairness and model accuracy. To address such issues, we introduce FairVIC, an innovative approach designed to enhance fairness in neural networks by addressing inherent biases at the training stage. FairVIC differs from traditional approaches that typically address biases at the data preprocessing stage. Instead, it integrates variance, invariance and covariance into the loss function to minimise the model's dependency on protected characteristics for making predictions, thus promoting fairness. Our experimentation and evaluation consists of training neural networks on three datasets known for their biases, comparing our results to state-of-the-art algorithms, evaluating on different sizes of model architectures, and carrying out sensitivity analysis to examine the fairness-accuracy trade-off. Through our implementation of FairVIC, we observed a significant improvement in fairness across all metrics tested, without compromising the model's accuracy to a detrimental extent. Our findings suggest that FairVIC presents a straightforward, out-of-the-box solution for the development of fairer deep learning models, thereby offering a generalisable solution applicable across many tasks and datasets.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CY",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18148",
        "abstract url": "https://arxiv.org/abs/2404.18148",
        "title": "Decentralized Peer Review in Open Science: A Mechanism Proposal",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Peer review is a laborious, yet essential, part of academic publishing with crucial impact on the scientific endeavor. The current lack of incentives and transparency harms the credibility of this process. Researchers are neither rewarded for superior nor penalized for bad reviews. Additionally, confidential reports cause a loss of insights and make the review process vulnerable to scientific misconduct. We propose a community-owned and -governed system that 1) remunerates reviewers for their efforts, 2) publishes the (anonymized) reports for scrutiny by the community, 3) tracks reputation of reviewers and 4) provides digital certificates. Automated by transparent smart-contract blockchain technology, the system aims to increase quality and speed of peer review while lowering the chance and impact of erroneous judgements.",
        "subjects": [
            "cs.GT",
            "cs.CY",
            "econ.GN"
        ],
        "comment": "14 pages, 1 figure"
    },
    {
        "paper id": "2404.18150",
        "abstract url": "https://arxiv.org/abs/2404.18150",
        "title": "RadSimReal: Bridging the Gap Between Synthetic and Real Data in Radar Object Detection With Simulation",
        "rating": "0.5",
        "keywords": [
            [
                "autonomous driving",
                "Radar"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However, obtaining annotated datasets from real radar images, crucial for training these networks, is challenging, especially in scenarios with long-range detection and adverse weather and lighting conditions where radar performance excels. To address this challenge, we present RadSimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions, all without the need for real data collection. Remarkably, our findings demonstrate that training object detection models on RadSimReal data and subsequently evaluating them on real-world data produce performance levels comparable to models trained and tested on real data from the same dataset, and even achieves better performance when testing across different real datasets. RadSimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar suppliers, and has faster run-time. This innovative tool has the potential to advance the development of computer vision algorithms for radar-based autonomous driving applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2404.18159",
        "abstract url": "https://arxiv.org/abs/2404.18159",
        "title": "Evaluating ROCKET and Catch22 features for calf behaviour classification from accelerometer data using Machine Learning models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Monitoring calf behaviour continuously would be beneficial to identify routine practices (e.g., weaning, dehorning, etc.) that impact calf welfare in dairy farms. In that regard, accelerometer data collected from neck collars can be used along with Machine Learning models to classify calf behaviour automatically. Hand-crafted features are commonly used in Machine Learning models, while ROCKET and Catch22 features are specifically designed for time-series classification problems in related fields. This study aims to compare the performance of ROCKET and Catch22 features to Hand-Crafted features. 30 Irish Holstein Friesian and Jersey pre-weaned calves were monitored using accelerometer sensors allowing for 27.4 hours of annotated behaviors. Additional time-series were computed from the raw X, Y and Z-axis and split into 3-second time windows. ROCKET, Catch22 and Hand-Crafted features were calculated for each time window, and the dataset was then split into the train, validation and test sets. Each set of features was used to train three Machine Learning models (Random Forest, eXtreme Gradient Boosting, and RidgeClassifierCV) to classify six behaviours indicative of pre-weaned calf welfare (drinking milk, grooming, lying, running, walking and other). Models were tuned with the validation set, and the performance of each feature-model combination was evaluated with the test set. The best performance across the three models was obtained with ROCKET [average balanced accuracy +/- standard deviation] (0.70 +/- 0.07), followed by Catch22 (0.69 +/- 0.05), surpassing Hand-Crafted (0.65 +/- 0.034). The best balanced accuracy (0.77) was obtained with ROCKET and RidgeClassifierCV, followed by Catch22 and Random Forest (0.73). Thus, tailoring these approaches for specific behaviours and contexts will be crucial in advancing precision livestock farming and enhancing animal welfare on a larger scale.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": "45 pages, 8 figures, 11 tables (3 in the Appendix), Journal paper"
    },
    {
        "paper id": "2404.18183",
        "abstract url": "https://arxiv.org/abs/2404.18183",
        "title": "Innovative Application of Artificial Intelligence Technology in Bank Credit Risk Management",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the rapid growth of technology, especially the widespread application of artificial intelligence (AI) technology, the risk management level of commercial banks is constantly reaching new heights. In the current wave of digitalization, AI has become a key driving force for the strategic transformation of financial institutions, especially the banking industry. For commercial banks, the stability and safety of asset quality are crucial, which directly relates to the long-term stable growth of the bank. Among them, credit risk management is particularly core because it involves the flow of a large amount of funds and the accuracy of credit decisions. Therefore, establishing a scientific and effective credit risk decision-making mechanism is of great strategic significance for commercial banks. In this context, the innovative application of AI technology has brought revolutionary changes to bank credit risk management. Through deep learning and big data analysis, AI can accurately evaluate the credit status of borrowers, timely identify potential risks, and provide banks with more accurate and comprehensive credit decision support. At the same time, AI can also achieve realtime monitoring and early warning, helping banks intervene before risks occur and reduce losses.",
        "subjects": [
            "q-fin.RM",
            "cs.AI"
        ],
        "comment": "6 pages, 1 figure, 2 tables"
    },
    {
        "paper id": "2404.18190",
        "abstract url": "https://arxiv.org/abs/2404.18190",
        "title": "Naive Bayes Classifiers and One-hot Encoding of Categorical Variables",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates the consequences of encoding a $K$-valued categorical variable incorrectly as $K$ bits via one-hot encoding, when using a Na\u00efve Bayes classifier. This gives rise to a product-of-Bernoullis (PoB) assumption, rather than the correct categorical Na\u00efve Bayes classifier. The differences between the two classifiers are analysed mathematically and experimentally. In our experiments using probability vectors drawn from a Dirichlet distribution, the two classifiers are found to agree on the maximum a posteriori class label for most cases, although the posterior probabilities are usually greater for the PoB case.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "7 pages, 3 figures"
    },
    {
        "paper id": "2404.18202",
        "abstract url": "https://arxiv.org/abs/2404.18202",
        "title": "WorldGPT: Empowering LLM as Multimodal World Model",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "World models are progressively being employed across diverse fields, extending from basic environment simulation to complex scenario construction. However, existing models are mainly trained on domain-specific states and actions, and confined to single-modality state representations. In this paper, We introduce WorldGPT, a generalist world model built upon Multimodal Large Language Model (MLLM). WorldGPT acquires an understanding of world dynamics through analyzing millions of videos across various domains. To further enhance WorldGPT's capability in specialized scenarios and long-term tasks, we have integrated it with a novel cognitive architecture that combines memory offloading, knowledge retrieval, and context reflection. As for evaluation, we build WorldNet, a multimodal state transition prediction benchmark encompassing varied real-life scenarios. Conducting evaluations on WorldNet directly demonstrates WorldGPT's capability to accurately model state transition patterns, affirming its effectiveness in understanding and predicting the dynamics of complex scenarios. We further explore WorldGPT's emerging potential in serving as a world simulator, helping multimodal agents generalize to unfamiliar domains through efficiently synthesising multimodal instruction instances which are proved to be as reliable as authentic data for fine-tuning purposes. The project is available on \\url{https://github.com/DCDmllm/WorldGPT}.",
        "subjects": [
            "cs.AI",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18233",
        "abstract url": "https://arxiv.org/abs/2404.18233",
        "title": "A Note on Asynchronous Challenges: Unveiling Formulaic Bias and Data Loss in the Hayashi-Yoshida Estimator",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Hayashi-Yoshida (\\HY)-estimator exhibits an intrinsic, telescoping property that leads to an often overlooked computational bias, which we denote,formulaic or intrinsic bias. This formulaic bias results in data loss by cancelling out potentially relevant data points, the nonextant data points. This paper attempts to formalize and quantify the data loss arising from this bias. In particular, we highlight the existence of nonextant data points via a concrete example, and prove necessary and sufficient conditions for the telescoping property to induce this type of formulaic bias.Since this type of bias is nonexistent when inputs, i.e., observation times, $\u03a0^{(1)} :=(t_i^{(1)})_{i=0,1,\\ldots}$ and $\u03a0^{(2)} :=(t_j^{(2)})_{j=0,1,\\ldots}$, are synchronous, we introduce the (a,b)-asynchronous adversary. This adversary generates inputs $\u03a0^{(1)}$ and $\u03a0^{(2)}$ according to two independent homogenous Poisson processes with rates a>0 and b>0, respectively. We address the foundational questions regarding cumulative minimal (or least) average data point loss, and determine the values for a and b. We prove that for equal rates a=b, the minimal average cumulative data loss over both inputs is attained and amounts to 25\\%. We present an algorithm, which is based on our theorem, for computing the exact number of nonextant data points given inputs $\u03a0^{(1)}$ and $\u03a0^{(2)}$, and suggest alternative methods. Finally, we use simulated data to empirically compare the (cumulative) average data loss of the (\\HY)-estimator.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.CO",
            "math.PR"
        ],
        "comment": "submitted. 15 pages, 1 appendix, 4 figures, 1 table, 1 algo"
    },
    {
        "paper id": "2404.18247",
        "abstract url": "https://arxiv.org/abs/2404.18247",
        "title": "Classical integrability in the presence of a cosmological constant: analytic and machine learning results",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the integrability of two-dimensional theories that are obtained by a dimensional reduction of certain four-dimensional gravitational theories describing the coupling of Maxwell fields and neutral scalar fields to gravity in the presence of a potential for the neutral scalar fields. By focusing on a certain solution subspace, we show that a subset of the equations of motion in two dimensions are the compatibility conditions for a modified version of the Breitenlohner-Maison linear system. Subsequently, we study the Liouville integrability of the 2D models encoding the chosen 4D solution subspace from a one-dimensional point of view by constructing Lax pair matrices. In this endeavour, we successfully employ a linear neural network to search for Lax pair matrices for these models, thereby illustrating how machine learning approaches can be effectively implemented to augment the identification of integrable structures in classical systems.",
        "subjects": [
            "hep-th",
            "cs.LG",
            "math-ph"
        ],
        "comment": "32 pages, 7 figures"
    },
    {
        "paper id": "2404.18253",
        "abstract url": "https://arxiv.org/abs/2404.18253",
        "title": "Efficient Remote Sensing with Harmonized Transfer Learning and Modality Alignment",
        "rating": "0.5",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "Remote Sensing"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "With the rise of Visual and Language Pretraining (VLP), an increasing number of downstream tasks are adopting the paradigm of pretraining followed by fine-tuning. Although this paradigm has demonstrated potential in various multimodal downstream tasks, its implementation in the remote sensing domain encounters some obstacles. Specifically, the tendency for same-modality embeddings to cluster together impedes efficient transfer learning. To tackle this issue, we review the aim of multimodal transfer learning for downstream tasks from a unified perspective, and rethink the optimization process based on three distinct objectives. We propose \"Harmonized Transfer Learning and Modality Alignment (HarMA)\", a method that simultaneously satisfies task constraints, modality alignment, and single-modality uniform alignment, while minimizing training overhead through parameter-efficient fine-tuning. Remarkably, without the need for external data for training, HarMA achieves state-of-the-art performance in two popular multimodal retrieval tasks in the field of remote sensing. Our experiments reveal that HarMA achieves competitive and even superior performance to fully fine-tuned models with only minimal adjustable parameters. Due to its simplicity, HarMA can be integrated into almost all existing multimodal pretraining models. We hope this method can facilitate the efficient application of large models to a wide range of downstream tasks while significantly reducing the resource consumption. Code is available at https://github.com/seekerhuang/HarMA.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Accepted by the Twelfth International Conference on Learning Representations (ICLR) Workshop"
    },
    {
        "paper id": "2404.18262",
        "abstract url": "https://arxiv.org/abs/2404.18262",
        "title": "Generating Situated Reflection Triggers about Alternative Solution Paths: A Case Study of Generative AI for Computer-Supported Collaborative Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "An advantage of Large Language Models (LLMs) is their contextualization capability - providing different responses based on student inputs like solution strategy or prior discussion, to potentially better engage students than standard feedback. We present a design and evaluation of a proof-of-concept LLM application to offer students dynamic and contextualized feedback. Specifically, we augment an Online Programming Exercise bot for a college-level Cloud Computing course with ChatGPT, which offers students contextualized reflection triggers during a collaborative query optimization task in database design. We demonstrate that LLMs can be used to generate highly situated reflection triggers that incorporate details of the collaborative discussion happening in context. We discuss in depth the exploration of the design space of the triggers and their correspondence with the learning objectives as well as the impact on student learning in a pilot study with 34 students.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18267",
        "abstract url": "https://arxiv.org/abs/2404.18267",
        "title": "LINOCS: Lookahead Inference of Networked Operators for Continuous Stability",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Identifying latent interactions within complex systems is key to unlocking deeper insights into their operational dynamics, including how their elements affect each other and contribute to the overall system behavior. For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizing the interactions among populations is key for understanding complex ecosystems. Such systems, often modeled as dynamical systems, typically exhibit noisy high-dimensional and non-stationary temporal behavior that renders their identification challenging. Existing dynamical system identification methods often yield operators that accurately capture short-term behavior but fail to predict long-term trends, suggesting an incomplete capture of the underlying process. Methods that consider extended forecasts (e.g., recurrent neural networks) lack explicit representations of element-wise interactions and require substantial training data, thereby failing to capture interpretable network operators. Here we introduce Lookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS), a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data. LINOCS integrates several multi-step predictions with adaptive weights during training to recover dynamical operators that can yield accurate long-term predictions. We demonstrate LINOCS' ability to recover the ground truth dynamical operators underlying synthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems' decomposition, and regularized linear time-varying systems) as well as its capability to produce meaningful operators with robust reconstructions through various real-world examples.",
        "subjects": [
            "eess.SY",
            "cs.LG",
            "q-bio.QM"
        ],
        "comment": "under review"
    },
    {
        "paper id": "2404.18270",
        "abstract url": "https://arxiv.org/abs/2404.18270",
        "title": "Pragmatic Formal Verification of Sequential Error Detection and Correction Codes (ECCs) used in Safety-Critical Design",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Error Detection and Correction Codes (ECCs) are often used in digital designs to protect data integrity. Especially in safety-critical systems such as automotive electronics, ECCs are widely used and the verification of such complex logic becomes more critical considering the ISO 26262 safety standards. Exhaustive verification of ECC using formal methods has been a challenge given the high number of data bits to protect. As an example, for an ECC of 128 data bits with a possibility to detect up to four-bit errors, the combination of bit errors is given by 128C1 + 128C2 + 128C3 + 128C4 = 1.1 * 10^7. This vast analysis space often leads to bounded proof results. Moreover, the complexity and state-space increase further if the ECC has sequential encoding and decoding stages. To overcome such problems and sign-off the design with confidence within reasonable proof time, we present a pragmatic formal verification approach of complex ECC cores with several complexity reduction techniques and know-how that were learnt during the course of verification. We discuss using the linearity of the syndrome generator as a helper assertion, using the abstract model as glue logic to compare the RTL with the sequential version of the circuit, k-induction-based model checking and using mathematical relations captured as properties to simplify the verification in order to get an unbounded proof result within 24 hours of proof runtime.",
        "subjects": [
            "cs.AI",
            "cs.LO"
        ],
        "comment": "Published in DVCon U.S. 2023"
    },
    {
        "paper id": "2404.18353",
        "abstract url": "https://arxiv.org/abs/2404.18353",
        "title": "Do Neutral Prompts Produce Insecure Code? FormAI-v2 Dataset: Labelling Vulnerabilities in Code Generated by Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This study provides a comparative analysis of state-of-the-art large language models (LLMs), analyzing how likely they generate vulnerabilities when writing simple C programs using a neutral zero-shot prompt. We address a significant gap in the literature concerning the security properties of code produced by these models without specific directives. N. Tihanyi et al. introduced the FormAI dataset at PROMISE '23, containing 112,000 GPT-3.5-generated C programs, with over 51.24% identified as vulnerable. We expand that work by introducing the FormAI-v2 dataset comprising 265,000 compilable C programs generated using various LLMs, including robust models such as Google's GEMINI-pro, OpenAI's GPT-4, and TII's 180 billion-parameter Falcon, to Meta's specialized 13 billion-parameter CodeLLama2 and various other compact models. Each program in the dataset is labelled based on the vulnerabilities detected in its source code through formal verification using the Efficient SMT-based Context-Bounded Model Checker (ESBMC). This technique eliminates false positives by delivering a counterexample and ensures the exclusion of false negatives by completing the verification process. Our study reveals that at least 63.47% of the generated programs are vulnerable. The differences between the models are minor, as they all display similar coding errors with slight variations. Our research highlights that while LLMs offer promising capabilities for code generation, deploying their output in a production environment requires risk assessment and validation.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18414",
        "abstract url": "https://arxiv.org/abs/2404.18414",
        "title": "Learning a Sparse Neural Network using IHT",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The core of a good model is in its ability to focus only on important information that reflects the basic patterns and consistencies, thus pulling out a clear, noise-free signal from the dataset. This necessitates using a simplified model defined by fewer parameters. The importance of theoretical foundations becomes clear in this context, as this paper relies on established results from the domain of advanced sparse optimization, particularly those addressing nonlinear differentiable functions. The need for such theoretical foundations is further highlighted by the trend that as computational power for training NNs increases, so does the complexity of the models in terms of a higher number of parameters. In practical scenarios, these large models are often simplified to more manageable versions with fewer parameters. Understanding why these simplified models with less number of parameters remain effective raises a crucial question. Understanding why these simplified models with fewer parameters remain effective raises an important question. This leads to the broader question of whether there is a theoretical framework that can clearly explain these empirical observations. Recent developments, such as establishing necessary conditions for the convergence of iterative hard thresholding (IHT) to a sparse local minimum (a sparse method analogous to gradient descent) are promising. The remarkable capacity of the IHT algorithm to accurately identify and learn the locations of nonzero parameters underscores its practical effectiveness and utility. This paper aims to investigate whether the theoretical prerequisites for such convergence are applicable in the realm of neural network (NN) training by providing justification for all the necessary conditions for convergence. Then, these conditions are validated by experiments on a single-layer NN, using the IRIS dataset as a testbed.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18136",
        "abstract url": "https://arxiv.org/abs/2404.18136",
        "title": "SafePaint: Anti-forensic Image Inpainting with Domain Adaptation",
        "rating": "0",
        "keywords": [
            [
                "Inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing image inpainting methods have achieved remarkable accomplishments in generating visually appealing results, often accompanied by a trend toward creating more intricate structural textures. However, while these models excel at creating more realistic image content, they often leave noticeable traces of tampering, posing a significant threat to security. In this work, we take the anti-forensic capabilities into consideration, firstly proposing an end-to-end training framework for anti-forensic image inpainting named SafePaint. Specifically, we innovatively formulated image inpainting as two major tasks: semantically plausible content completion and region-wise optimization. The former is similar to current inpainting methods that aim to restore the missing regions of corrupted images. The latter, through domain adaptation, endeavors to reconcile the discrepancies between the inpainted region and the unaltered area to achieve anti-forensic goals. Through comprehensive theoretical analysis, we validate the effectiveness of domain adaptation for anti-forensic performance. Furthermore, we meticulously crafted a region-wise separated attention (RWSA) module, which not only aligns with our objective of anti-forensics but also enhances the performance of the model. Extensive qualitative and quantitative evaluations show our approach achieves comparable results to existing image inpainting methods while offering anti-forensic capabilities not available in other methods.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18155",
        "abstract url": "https://arxiv.org/abs/2404.18155",
        "title": "ShapeMoir\u00e9: Channel-Wise Shape-Guided Network for Image Demoir\u00e9ing",
        "rating": "0",
        "keywords": [
            [
                "RGB-D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Photographing optoelectronic displays often introduces unwanted moir\u00e9 patterns due to analog signal interference between the pixel grids of the display and the camera sensor arrays. This work identifies two problems that are largely ignored by existing image demoir\u00e9ing approaches: 1) moir\u00e9 patterns vary across different channels (RGB); 2) repetitive patterns are constantly observed. However, employing conventional convolutional (CNN) layers cannot address these problems. Instead, this paper presents the use of our recently proposed Shape concept. It was originally employed to model consistent features from fragmented regions, particularly when identical or similar objects coexist in an RGB-D image. Interestingly, we find that the Shape information effectively captures the moir\u00e9 patterns in artifact images. Motivated by this discovery, we propose a ShapeMoir\u00e9 method to aid in image demoir\u00e9ing. Beyond modeling shape features at the patch-level, we further extend this to the global image-level and design a novel Shape-Architecture. Consequently, our proposed method, equipped with both ShapeConv and Shape-Architecture, can be seamlessly integrated into existing approaches without introducing additional parameters or computation overhead during inference. We conduct extensive experiments on four widely used datasets, and the results demonstrate that our ShapeMoir\u00e9 achieves state-of-the-art performance, particularly in terms of the PSNR metric. We then apply our method across four popular architectures to showcase its generalization capabilities. Moreover, our ShapeMoir\u00e9 is robust and viable under real-world demoir\u00e9ing scenarios involving smartphone photographs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2404.18156",
        "abstract url": "https://arxiv.org/abs/2404.18156",
        "title": "Event-based Video Frame Interpolation with Edge Guided Motion Refinement",
        "rating": "0",
        "keywords": [
            [
                "event cameras"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video frame interpolation, the process of synthesizing intermediate frames between sequential video frames, has made remarkable progress with the use of event cameras. These sensors, with microsecond-level temporal resolution, fill information gaps between frames by providing precise motion cues. However, contemporary Event-Based Video Frame Interpolation (E-VFI) techniques often neglect the fact that event data primarily supply high-confidence features at scene edges during multi-modal feature fusion, thereby diminishing the role of event signals in optical flow (OF) estimation and warping refinement. To address this overlooked aspect, we introduce an end-to-end E-VFI learning method (referred to as EGMR) to efficiently utilize edge features from event signals for motion flow and warping enhancement. Our method incorporates an Edge Guided Attentive (EGA) module, which rectifies estimated video motion through attentive aggregation based on the local correlation of multi-modal features in a coarse-to-fine strategy. Moreover, given that event data can provide accurate visual references at scene edges between consecutive frames, we introduce a learned visibility map derived from event data to adaptively mitigate the occlusion problem in the warping refinement process. Extensive experiments on both synthetic and real datasets show the effectiveness of the proposed approach, demonstrating its potential for higher quality video frame interpolation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18206",
        "abstract url": "https://arxiv.org/abs/2404.18206",
        "title": "Enhancing Action Recognition from Low-Quality Skeleton Data via Part-Level Knowledge Distillation",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Skeleton"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Skeleton-based action recognition is vital for comprehending human-centric videos and has applications in diverse domains. One of the challenges of skeleton-based action recognition is dealing with low-quality data, such as skeletons that have missing or inaccurate joints. This paper addresses the issue of enhancing action recognition using low-quality skeletons through a general knowledge distillation framework. The proposed framework employs a teacher-student model setup, where a teacher model trained on high-quality skeletons guides the learning of a student model that handles low-quality skeletons. To bridge the gap between heterogeneous high-quality and lowquality skeletons, we present a novel part-based skeleton matching strategy, which exploits shared body parts to facilitate local action pattern learning. An action-specific part matrix is developed to emphasize critical parts for different actions, enabling the student model to distill discriminative part-level knowledge. A novel part-level multi-sample contrastive loss achieves knowledge transfer from multiple high-quality skeletons to low-quality ones, which enables the proposed knowledge distillation framework to include training low-quality skeletons that lack corresponding high-quality matches. Comprehensive experiments conducted on the NTU-RGB+D, Penn Action, and SYSU 3D HOI datasets demonstrate the effectiveness of the proposed knowledge distillation framework.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18239",
        "abstract url": "https://arxiv.org/abs/2404.18239",
        "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
        "rating": "0",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility out of the scope of unlearning. While interest in studying LLM unlearning is growing,the impact of the optimizer choice for LLM unlearning remains under-explored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between {second-order optimization} and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order unlearning framework, termed SOUL, built upon the second-order clipped stochastic optimization (Sophia)-based LLM training method. SOUL extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, suggesting the promise of second-order optimization in providing a scalable and easily implementable solution for LLM unlearning.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18245",
        "abstract url": "https://arxiv.org/abs/2404.18245",
        "title": "FAD-SAR: A Novel Fishing Activity Detection System via Synthetic Aperture Radar Images Based on Deep Learning Method",
        "rating": "0",
        "keywords": [
            [
                "Radar"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Illegal, unreported, and unregulated (IUU) fishing seriously affects various aspects of human life. However, current methods for detecting and monitoring IUU activities at sea have limitations. While Synthetic Aperture Radar (SAR) can complement existing vessel detection systems, extracting useful information from SAR images using traditional methods, especially for IUU fishing identification, poses challenges. This paper proposes a deep learning-based system for detecting fishing activities. We implemented this system on the xView3 dataset using six classical object detection models: Faster R-CNN, Cascade R-CNN, SSD, RetinaNet, FSAF, and FCOS. We applied improvement methods to enhance the performance of the Faster R-CNN model. Specifically, training the Faster R-CNN model using Online Hard Example Mining (OHEM) strategy improved the Avg-F1 value from 0.212 to 0.216, representing a 1.96% improvement.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18252",
        "abstract url": "https://arxiv.org/abs/2404.18252",
        "title": "Fisher Information Improved Training-Free Conditional Diffusion Model",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, the diffusion model with the training-free methods has succeeded in conditional image generation tasks. However, there is an efficiency problem because it requires calculating the gradient with high computational cost, and previous methods make strong assumptions to solve it, sacrificing generalization. In this work, we propose the Fisher information guided diffusion model (FIGD). Concretely, we introduce the Fisher information to estimate the gradient without making any additional assumptions to reduce computation cost. Meanwhile, we demonstrate that the Fisher information ensures the generalization of FIGD and provides new insights for training-free methods based on the information theory. The experimental results demonstrate that FIGD could achieve different conditional generations more quickly while maintaining high quality.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18343",
        "abstract url": "https://arxiv.org/abs/2404.18343",
        "title": "G-Refine: A General Quality Refiner for Text-to-Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the evolution of Text-to-Image (T2I) models, the quality defects of AI-Generated Images (AIGIs) pose a significant barrier to their widespread adoption. In terms of both perception and alignment, existing models cannot always guarantee high-quality results. To mitigate this limitation, we introduce G-Refine, a general image quality refiner designed to enhance low-quality images without compromising the integrity of high-quality ones. The model is composed of three interconnected modules: a perception quality indicator, an alignment quality indicator, and a general quality enhancement module. Based on the mechanisms of the Human Visual System (HVS) and syntax trees, the first two indicators can respectively identify the perception and alignment deficiencies, and the last module can apply targeted quality enhancement accordingly. Extensive experimentation reveals that when compared to alternative optimization methods, AIGIs after G-Refine outperform in 10+ quality metrics across 4 databases. This improvement significantly contributes to the practical application of contemporary T2I models, paving the way for their broader adoption. The code will be released on https://github.com/Q-Future/Q-Refine.",
        "subjects": [
            "cs.MM",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18359",
        "abstract url": "https://arxiv.org/abs/2404.18359",
        "title": "FoundaBench: Evaluating Chinese Fundamental Knowledge Capabilities of Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In the burgeoning field of large language models (LLMs), the assessment of fundamental knowledge remains a critical challenge, particularly for models tailored to Chinese language and culture. This paper introduces FoundaBench, a pioneering benchmark designed to rigorously evaluate the fundamental knowledge capabilities of Chinese LLMs. FoundaBench encompasses a diverse array of 3354 multiple-choice questions across common sense and K-12 educational subjects, meticulously curated to reflect the breadth and depth of everyday and academic knowledge. We present an extensive evaluation of 12 state-of-the-art LLMs using FoundaBench, employing both traditional assessment methods and our CircularEval protocol to mitigate potential biases in model responses. Our results highlight the superior performance of models pre-trained on Chinese corpora, and reveal a significant disparity between models' reasoning and memory recall capabilities. The insights gleaned from FoundaBench evaluations set a new standard for understanding the fundamental knowledge of LLMs, providing a robust framework for future advancements in the field.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18371",
        "abstract url": "https://arxiv.org/abs/2404.18371",
        "title": "QANA: LLM-based Question Generation and Network Analysis for Zero-shot Key Point Analysis and Beyond",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The proliferation of social media has led to information overload and increased interest in opinion mining. We propose \"Question-Answering Network Analysis\" (QANA), a novel opinion mining framework that utilizes Large Language Models (LLMs) to generate questions from users' comments, constructs a bipartite graph based on the comments' answerability to the questions, and applies centrality measures to examine the importance of opinions. We investigate the impact of question generation styles, LLM selections, and the choice of embedding model on the quality of the constructed QA networks by comparing them with annotated Key Point Analysis datasets. QANA achieves comparable performance to previous state-of-the-art supervised models in a zero-shot manner for Key Point Matching task, also reducing the computational cost from quadratic to linear. For Key Point Generation, questions with high PageRank or degree centrality align well with manually annotated key points. Notably, QANA enables analysts to assess the importance of key points from various aspects according to their selection of centrality measure. QANA's primary contribution lies in its flexibility to extract key points from a wide range of perspectives, which enhances the quality and impartiality of opinion mining.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under review as a conference paper at COLM 2024"
    },
    {
        "paper id": "2404.18400",
        "abstract url": "https://arxiv.org/abs/2404.18400",
        "title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "skeletons"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely high-dimensional combinatorial and nonlinear hypothesis spaces. Traditional methods of equation discovery largely focus on extracting equations from data alone, often neglecting the rich domain-specific prior knowledge that scientists typically depend on. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data in an efficient manner. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeletons, drawing from its physical understanding, which are then optimized against data to estimate skeleton parameters. We demonstrate LLM-SR's effectiveness across three diverse scientific domains, where it discovers physically accurate equations that provide significantly better fits to in-domain and out-of-domain data compared to the well-established equation discovery baselines",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18411",
        "abstract url": "https://arxiv.org/abs/2404.18411",
        "title": "Multi-modal Perception Dataset of In-water Objects for Autonomous Surface Vehicles",
        "rating": "0",
        "keywords": [
            [
                "robotics",
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces the first publicly accessible multi-modal perception dataset for autonomous maritime navigation, focusing on in-water obstacles within the aquatic environment to enhance situational awareness for Autonomous Surface Vehicles (ASVs). This dataset, consisting of diverse objects encountered under varying environmental conditions, aims to bridge the research gap in marine robotics by providing a multi-modal, annotated, and ego-centric perception dataset, for object detection and classification. We also show the applicability of the proposed dataset's framework using deep learning-based open-source perception algorithms that have shown success. We expect that our dataset will contribute to development of the marine autonomy pipeline and marine (field) robotics. Please note this is a work-in-progress paper about our on-going research that we plan to release in full via future publication.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2024"
    },
    {
        "paper id": "2405.00732",
        "abstract url": "https://arxiv.org/abs/2405.00732",
        "title": "LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report",
        "rating": "0",
        "keywords": [
            [
                "Parameter Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18074",
        "abstract url": "https://arxiv.org/abs/2404.18074",
        "title": "MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Autonomous virtual agents are often limited by their singular mode of interaction with real-world environments, restricting their versatility. To address this, we propose the Multi-Modal Agent Collaboration framework (MMAC-Copilot), a framework utilizes the collective expertise of diverse agents to enhance interaction ability with operating systems. The framework introduces a team collaboration chain, enabling each participating agent to contribute insights based on their specific domain knowledge, effectively reducing the hallucination associated with knowledge domain gaps. To evaluate the performance of MMAC-Copilot, we conducted experiments using both the GAIA benchmark and our newly introduced Visual Interaction Benchmark (VIBench). VIBench focuses on non-API-interactable applications across various domains, including 3D gaming, recreation, and office scenarios. MMAC-Copilot achieved exceptional performance on GAIA, with an average improvement of 6.8\\% over existing leading systems. Furthermore, it demonstrated remarkable capability on VIBench, particularly in managing various methods of interaction within systems and applications. These results underscore MMAC-Copilot's potential in advancing the field of autonomous virtual agents through its innovative approach to agent collaboration.",
        "subjects": [
            "cs.AI",
            "cs.HC"
        ],
        "comment": "In processing"
    },
    {
        "paper id": "2404.18135",
        "abstract url": "https://arxiv.org/abs/2404.18135",
        "title": "Dexterous Grasp Transformer",
        "rating": "-0.5",
        "keywords": [
            [
                "point cloud"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "In this work, we propose a novel discriminative framework for dexterous grasp generation, named Dexterous Grasp TRansformer (DGTR), capable of predicting a diverse set of feasible grasp poses by processing the object point cloud with only one forward pass. We formulate dexterous grasp generation as a set prediction task and design a transformer-based grasping model for it. However, we identify that this set prediction paradigm encounters several optimization challenges in the field of dexterous grasping and results in restricted performance. To address these issues, we propose progressive strategies for both the training and testing phases. First, the dynamic-static matching training (DSMT) strategy is presented to enhance the optimization stability during the training phase. Second, we introduce the adversarial-balanced test-time adaptation (AB-TTA) with a pair of adversarial losses to improve grasping quality during the testing phase. Experimental results on the DexGraspNet dataset demonstrate the capability of DGTR to predict dexterous grasp poses with both high quality and diversity. Notably, while keeping high quality, the diversity of grasp poses predicted by DGTR significantly outperforms previous works in multiple metrics without any data pre-processing. Codes are available at https://github.com/iSEE-Laboratory/DGTR .",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted to CVPR 2024"
    },
    {
        "paper id": "2404.18197",
        "abstract url": "https://arxiv.org/abs/2404.18197",
        "title": "A General Causal Inference Framework for Cross-Sectional Observational Data",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Causal inference methods for observational data are highly regarded due to their wide applicability. While there are already numerous methods available for de-confounding bias, these methods generally assume that covariates consist solely of confounders or make naive assumptions about the covariates. Such assumptions face challenges in both theory and practice, particularly when dealing with high-dimensional covariates. Relaxing these naive assumptions and identifying the confounding covariates that truly require correction can effectively enhance the practical significance of these methods. Therefore, this paper proposes a General Causal Inference (GCI) framework specifically designed for cross-sectional observational data, which precisely identifies the key confounding covariates and provides corresponding identification algorithm. Specifically, based on progressive derivations of the Markov property on Directed Acyclic Graph, we conclude that the key confounding covariates are equivalent to the common root ancestors of the treatment and the outcome variable. Building upon this conclusion, the GCI framework is composed of a novel Ancestor Set Identification (ASI) algorithm and de-confounding inference methods. Firstly, the ASI algorithm is theoretically supported by the conditional independence properties and causal asymmetry between variables, enabling the identification of key confounding covariates. Subsequently, the identified confounding covariates are used in the de-confounding inference methods to obtain unbiased causal effect estimation, which can support informed decision-making. Extensive experiments on synthetic datasets demonstrate that the GCI framework can effectively identify the critical confounding covariates and significantly improve the precision, stability, and interpretability of causal inference in observational studies.",
        "subjects": [
            "stat.ME",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "19 pages, 7 figures"
    },
    {
        "paper id": "2404.18211",
        "abstract url": "https://arxiv.org/abs/2404.18211",
        "title": "A survey of dynamic graph neural networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) have emerged as a powerful tool for effectively mining and learning from graph-structured data, with applications spanning numerous domains. However, most research focuses on static graphs, neglecting the dynamic nature of real-world networks where topologies and attributes evolve over time. By integrating sequence modeling modules into traditional GNN architectures, dynamic GNNs aim to bridge this gap, capturing the inherent temporal dependencies of dynamic graphs for a more authentic depiction of complex networks. This paper provides a comprehensive review of the fundamental concepts, key techniques, and state-of-the-art dynamic GNN models. We present the mainstream dynamic GNN models in detail and categorize models based on how temporal information is incorporated. We also discuss large-scale dynamic GNNs and pre-training techniques. Although dynamic GNNs have shown superior performance, challenges remain in scalability, handling heterogeneous information, and lack of diverse graph datasets. The paper also discusses possible future directions, such as adaptive and memory-enhanced models, inductive learning, and theoretical analysis.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18287",
        "abstract url": "https://arxiv.org/abs/2404.18287",
        "title": "Joint Energy and Latency Optimization in Federated Learning over Cell-Free Massive MIMO Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) is a distributed learning paradigm wherein users exchange FL models with a server instead of raw datasets, thereby preserving data privacy and reducing communication overhead. However, the increased number of FL users may hinder completing large-scale FL over wireless networks due to high imposed latency. Cell-free massive multiple-input multiple-output~(CFmMIMO) is a promising architecture for implementing FL because it serves many users on the same time/frequency resources. While CFmMIMO enhances energy efficiency through spatial multiplexing and collaborative beamforming, it remains crucial to meticulously allocate uplink transmission powers to the FL users. In this paper, we propose an uplink power allocation scheme in FL over CFmMIMO by considering the effect of each user's power on the energy and latency of other users to jointly minimize the users' uplink energy and the latency of FL training. The proposed solution algorithm is based on the coordinate gradient descent method. Numerical results show that our proposed method outperforms the well-known max-sum rate by increasing up to~$27$\\% and max-min energy efficiency of the Dinkelbach method by increasing up to~$21$\\% in terms of test accuracy while having limited uplink energy and latency budget for FL over CFmMIMO.",
        "subjects": [
            "cs.LG",
            "cs.IT",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18308",
        "abstract url": "https://arxiv.org/abs/2404.18308",
        "title": "Near-Term Enforcement of AI Chip Export Controls Using A Minimal Firmware-Based Design for Offline Licensing",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Offline licensing is a technical mechanism for compute governance that could be used to prevent unregulated training of potentially dangerous frontier AI models. The mechanism works by disabling AI chips unless they have an up-to-date license from a regulator. In this report, we present a technical design for a minimal version of offline licensing that could be delivered via a firmware update. Existing AI chips could potentially support offline licensing within a year if they have the following (relatively common) hardware security features: firmware verification, firmware rollback protection, and secure non-volatile memory. Public documentation suggests that NVIDIA's H100 AI chip already has these security features. Without additional hardware modifications, the system is susceptible to physical hardware attacks. However, these attacks might require expensive equipment and could be difficult to reliably apply to thousands of AI chips. A firmware-based offline licensing design shares the same legal requirements and license approval mechanism as a hardware-based solution. Implementing a firmware-based solution now could accelerate the eventual deployment of a more secure hardware-based solution in the future. For AI chip manufacturers, implementing this security mechanism might allow chips to be sold to customers that would otherwise be prohibited by export restrictions. For governments, it may be important to be able to prevent unsafe or malicious actors from training frontier AI models in the next few years. Based on this initial analysis, firmware-based offline licensing could partially solve urgent security and trade problems and is technically feasible for AI chips that have common hardware security features.",
        "subjects": [
            "cs.CR",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18326",
        "abstract url": "https://arxiv.org/abs/2404.18326",
        "title": "SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement Learning Policies",
        "rating": "-0.5",
        "keywords": [
            [
                "automated driving"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "While Deep Reinforcement Learning (DRL) has emerged as a promising solution for intricate control tasks, the lack of explainability of the learned policies impedes its uptake in safety-critical applications, such as automated driving systems (ADS). Counterfactual (CF) explanations have recently gained prominence for their ability to interpret black-box Deep Learning (DL) models. CF examples are associated with minimal changes in the input, resulting in a complementary output by the DL model. Finding such alternations, particularly for high-dimensional visual inputs, poses significant challenges. Besides, the temporal dependency introduced by the reliance of the DRL agent action on a history of past state observations further complicates the generation of CF examples. To address these challenges, we propose using a saliency map to identify the most influential input pixels across the sequence of past observed states by the agent. Then, we feed this map to a deep generative model, enabling the generation of plausible CFs with constrained modifications centred on the salient regions. We evaluate the effectiveness of our framework in diverse domains, including ADS, Atari Pong, Pacman and space-invaders games, using traditional performance metrics such as validity, proximity and sparsity. Experimental results demonstrate that this framework generates more informative and plausible CFs than the state-of-the-art for a wide range of environments and DRL agents. In order to foster research in this area, we have made our datasets and codes publicly available at https://github.com/Amir-Samadi/SAFE-RL.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18385",
        "abstract url": "https://arxiv.org/abs/2404.18385",
        "title": "Equivalence: An analysis of artists' roles with Image Generative AI from Conceptual Art perspective through an interactive installation design practice",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Over the past year, the emergence of advanced text-to-image Generative AI models has significantly impacted the art world, challenging traditional notions of creativity and the role of artists. This study explores how artists interact with these technologies, using a 5P model (Purpose, People, Process, Product, and Press) based on Rhodes' creativity framework to compare the artistic processes behind Conceptual Art and Image Generative AI. To exemplify this framework, a practical case study titled \"Equivalence\", a multi-screen interactive installation that converts users' speech input into continuously evolving paintings developed based on Stable Diffusion and NLP algorithms, was developed. Through comprehensive analysis and the case study, this work aims to broaden our understanding of artists' roles and foster a deeper appreciation for the creative aspects inherent in artwork created with Image Generative AI.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.19649",
        "abstract url": "https://arxiv.org/abs/2404.19649",
        "title": "Landmark Alternating Diffusion",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Alternating Diffusion (AD) is a commonly applied diffusion-based sensor fusion algorithm. While it has been successfully applied to various problems, its computational burden remains a limitation. Inspired by the landmark diffusion idea considered in the Robust and Scalable Embedding via Landmark Diffusion (ROSELAND), we propose a variation of AD, called Landmark AD (LAD), which captures the essence of AD while offering superior computational efficiency. We provide a series of theoretical analyses of LAD under the manifold setup and apply it to the automatic sleep stage annotation problem with two electroencephalogram channels to demonstrate its application.",
        "subjects": [
            "cs.LG",
            "math.ST",
            "physics.data-an",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.00066",
        "abstract url": "https://arxiv.org/abs/2405.00066",
        "title": "Research and application of artificial intelligence based webshell detection model: A literature review",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Webshell, as the \"culprit\" behind numerous network attacks, is one of the research hotspots in the field of cybersecurity. However, the complexity, stealthiness, and confusing nature of webshells pose significant challenges to the corresponding detection schemes. With the rise of Artificial Intelligence (AI) technology, researchers have started to apply different intelligent algorithms and neural network architectures to the task of webshell detection. However, the related research still lacks a systematic and standardized methodological process, which is confusing and redundant. Therefore, following the development timeline, we carefully summarize the progress of relevant research in this field, dividing it into three stages: Start Stage, Initial Development Stage, and In-depth Development Stage. We further elaborate on the main characteristics and core algorithms of each stage. In addition, we analyze the pain points and challenges that still exist in this field and predict the future development trend of this field from our point of view. To the best of our knowledge, this is the first review that details the research related to AI-based webshell detection. It is also hoped that this paper can provide detailed technical information for more researchers interested in AI-based webshell detection tasks.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "21 pages, 6 figures"
    },
    {
        "paper id": "2404.18083",
        "abstract url": "https://arxiv.org/abs/2404.18083",
        "title": "Online,Target-Free LiDAR-Camera Extrinsic Calibration via Cross-Modal Mask Matching",
        "rating": "-1",
        "keywords": [
            [
                "LiDAR"
            ],
            [
                "robotics"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in intelligent vehicles. Offline, target-based approaches have long been the preferred choice in this field. However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. In contrast, online, target-free approaches provide greater adaptability yet typically lack robustness, primarily due to the challenges in cross-modal feature matching. Therefore, in this article, we unleash the full potential of large vision models (LVMs), which are emerging as a significant trend in the fields of computer vision and robotics, especially for embodied artificial intelligence, to achieve robust and accurate online, target-free LCEC across a variety of challenging scenarios. Our main contributions are threefold: we introduce a novel framework known as MIAS-LCEC, provide an open-source versatile calibration toolbox with an interactive visualization interface, and publish three real-world datasets captured from various indoor and outdoor environments. The cornerstone of our framework and toolbox is the cross-modal mask matching (C3M) algorithm, developed based on a state-of-the-art (SoTA) LVM and capable of generating sufficient and reliable matches. Extensive experiments conducted on these real-world datasets demonstrate the robustness of our approach and its superior performance compared to SoTA methods, particularly for the solid-state LiDARs with super-wide fields of view.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18084",
        "abstract url": "https://arxiv.org/abs/2404.18084",
        "title": "Age-minimal Multicast by Graph Attention Reinforcement Learning",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Age of Information (AoI) is an emerging metric used to assess the timeliness of information, gaining research interest in real-time multicast applications such as video streaming and metaverse platforms. In this paper, we consider a dynamic multicast network with energy constraints, where our objective is to minimize the expected time-average AoI through energy-constrained multicast routing and scheduling. The inherent complexity of the problem, given the NP-hardness and intertwined scheduling and routing decisions, makes existing approaches inapplicable. To address these challenges, we decompose the original problem into two subtasks, each amenable to reinforcement learning (RL) methods. Subsequently, we propose an innovative framework based on graph attention networks (GATs) to effectively capture graph information with superior generalization capabilities. To validate our framework, we conduct experiments on three datasets including a real-world dataset called AS-733, and show that our proposed scheme reduces the average weighted AoI by 62.9% and reduces the energy consumption by at most 72.5% compared to baselines.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18096",
        "abstract url": "https://arxiv.org/abs/2404.18096",
        "title": "Snake with Shifted Window: Learning to Adapt Vessel Pattern for OCTA Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "pathological",
                "retinal"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Segmenting specific targets or structures in optical coherence tomography angiography (OCTA) images is fundamental for conducting further pathological studies. The retinal vascular layers are rich and intricate, and such vascular with complex shapes can be captured by the widely-studied OCTA images. In this paper, we thus study how to use OCTA images with projection vascular layers to segment retinal structures. To this end, we propose the SSW-OCTA model, which integrates the advantages of deformable convolutions suited for tubular structures and the swin-transformer for global feature extraction, adapting to the characteristics of OCTA modality images. Our model underwent testing and comparison on the OCTA-500 dataset, achieving state-of-the-art performance. The code is available at: https://github.com/ShellRedia/Snake-SWin-OCTA.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18109",
        "abstract url": "https://arxiv.org/abs/2404.18109",
        "title": "Finding Beautiful and Happy Images for Mental Health and Well-being Applications",
        "rating": "-1",
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper explores how artificial intelligence (AI) technology can contribute to achieve progress on good health and well-being, one of the United Nations' 17 Sustainable Development Goals. It is estimated that one in ten of the global population lived with a mental disorder. Inspired by studies showing that engaging and viewing beautiful natural images can make people feel happier and less stressful, lead to higher emotional well-being, and can even have therapeutic values, we explore how AI can help to promote mental health by developing automatic algorithms for finding beautiful and happy images. We first construct a large image database consisting of nearly 20K very high resolution colour photographs of natural scenes where each image is labelled with beautifulness and happiness scores by about 10 observers. Statistics of the database shows that there is a good correlation between the beautifulness and happiness scores which provides anecdotal evidence to corroborate that engaging beautiful natural images can potentially benefit mental well-being. Building on this unique database, the very first of its kind, we have developed a deep learning based model for automatically predicting the beautifulness and happiness scores of natural images. Experimental results are presented to show that it is possible to develop AI algorithms to automatically assess an image's beautifulness and happiness values which can in turn be used to develop applications for promoting mental health and well-being.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18126",
        "abstract url": "https://arxiv.org/abs/2404.18126",
        "title": "Testing $C_k$-freeness in bounded-arboricity graphs",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We study the problem of testing $C_k$-freeness ($k$-cycle-freeness) for fixed constant $k > 3$ in graphs with bounded arboricity (but unbounded degrees). In particular, we are interested in one-sided error algorithms, so that they must detect a copy of $C_k$ with high constant probability when the graph is $\u03b5$-far from $C_k$-free. We next state our results for constant arboricity and constant $\u03b5$ with a focus on the dependence on the number of graph vertices, $n$. The query complexity of all our algorithms grows polynomially with $1/\u03b5$. (1) As opposed to the case of $k=3$, where the complexity of testing $C_3$-freeness grows with the arboricity of the graph but not with the size of the graph (Levi, ICALP 2021) this is no longer the case already for $k=4$. We show that $\u03a9(n^{1/4})$ queries are necessary for testing $C_4$-freeness, and that $\\widetilde{O}(n^{1/4})$ are sufficient. The same bounds hold for $C_5$. (2) For every fixed $k \\geq 6$, any one-sided error algorithm for testing $C_k$-freeness must perform $\u03a9(n^{1/3})$ queries. (3) For $k=6$ we give a testing algorithm whose query complexity is $\\widetilde{O}(n^{1/2})$. (4) For any fixed $k$, the query complexity of testing $C_k$-freeness is upper bounded by ${O}(n^{1-1/\\lfloor k/2\\rfloor})$. Our $\u03a9(n^{1/4})$ lower bound for testing $C_4$-freeness in constant arboricity graphs provides a negative answer to an open problem posed by (Goldreich, 2021).",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18151",
        "abstract url": "https://arxiv.org/abs/2404.18151",
        "title": "Decidability of Graph Neural Networks via Logical Characterizations",
        "rating": "-1",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ]
        ],
        "abstract": "We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving \"Presburger quantifiers\". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18152",
        "abstract url": "https://arxiv.org/abs/2404.18152",
        "title": "Masked Attention as a Mechanism for Improving Interpretability of Vision Transformers",
        "rating": "-1",
        "keywords": [
            [
                "whole-slide",
                "cancer"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Vision Transformers are at the heart of the current surge of interest in foundation models for histopathology. They process images by breaking them into smaller patches following a regular grid, regardless of their content. Yet, not all parts of an image are equally relevant for its understanding. This is particularly true in computational pathology where background is completely non-informative and may introduce artefacts that could mislead predictions. To address this issue, we propose a novel method that explicitly masks background in Vision Transformers' attention mechanism. This ensures tokens corresponding to background patches do not contribute to the final image representation, thereby improving model robustness and interpretability. We validate our approach using prostate cancer grading from whole-slide images as a case study. Our results demonstrate that it achieves comparable performance with plain self-attention while providing more accurate and clinically meaningful attention heatmaps.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "Accepted at MIDL 2024"
    },
    {
        "paper id": "2404.18178",
        "abstract url": "https://arxiv.org/abs/2404.18178",
        "title": "Assessing Image Quality Using a Simple Generative Representation",
        "rating": "-1",
        "keywords": [
            [
                "quality assessment"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Perceptual image quality assessment (IQA) is the task of predicting the visual quality of an image as perceived by a human observer. Current state-of-the-art techniques are based on deep representations trained in discriminative manner. Such representations may ignore visually important features, if they are not predictive of class labels. Recent generative models successfully learn low-dimensional representations using auto-encoding and have been argued to preserve better visual features. Here we leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18186",
        "abstract url": "https://arxiv.org/abs/2404.18186",
        "title": "Static Application Security Testing (SAST) Tools for Smart Contracts: How Far Are We?",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "In recent years, the importance of smart contract security has been heightened by the increasing number of attacks against them. To address this issue, a multitude of static application security testing (SAST) tools have been proposed for detecting vulnerabilities in smart contracts. However, objectively comparing these tools to determine their effectiveness remains challenging. Existing studies often fall short due to the taxonomies and benchmarks only covering a coarse and potentially outdated set of vulnerability types, which leads to evaluations that are not entirely comprehensive and may display bias. In this paper, we fill this gap by proposing an up-to-date and fine-grained taxonomy that includes 45 unique vulnerability types for smart contracts. Taking it as a baseline, we develop an extensive benchmark that covers 40 distinct types and includes a diverse range of code characteristics, vulnerability patterns, and application scenarios. Based on them, we evaluated 8 SAST tools using this benchmark, which comprises 788 smart contract files and 10,394 vulnerabilities. Our results reveal that the existing SAST tools fail to detect around 50% of vulnerabilities in our benchmark and suffer from high false positives, with precision not surpassing 10%. We also discover that by combining the results of multiple tools, the false negative rate can be reduced effectively, at the expense of flagging 36.77 percentage points more functions. Nevertheless, many vulnerabilities, especially those beyond Access Control and Reentrancy vulnerabilities, remain undetected. We finally highlight the valuable insights from our study, hoping to provide guidance on tool development, enhancement, evaluation, and selection for developers, researchers, and practitioners.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "to appear at FSE 2024"
    },
    {
        "paper id": "2404.18198",
        "abstract url": "https://arxiv.org/abs/2404.18198",
        "title": "Permutation-equivariant quantum convolutional neural networks",
        "rating": "-1",
        "keywords": [
            [
                "quantum"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The Symmetric group $S_{n}$ manifests itself in large classes of quantum systems as the invariance of certain characteristics of a quantum state with respect to permuting the qubits. The subgroups of $S_{n}$ arise, among many other contexts, to describe label symmetry of classical images with respect to spatial transformations, e.g. reflection or rotation. Equipped with the formalism of geometric quantum machine learning, in this work we propose the architectures of equivariant quantum convolutional neural networks (EQCNNs) adherent to $S_{n}$ and its subgroups. We demonstrate that a careful choice of pixel-to-qubit embedding order can facilitate easy construction of EQCNNs for small subgroups of $S_{n}$. Our novel EQCNN architecture corresponding to the full permutation group $S_{n}$ is built by applying all possible QCNNs with equal probability, which can also be conceptualized as a dropout strategy in quantum neural networks. For subgroups of $S_{n}$, our numerical results using MNIST datasets show better classification accuracy than non-equivariant QCNNs. The $S_{n}$-equivariant QCNN architecture shows significantly improved training and test performance than non-equivariant QCNN for classification of connected and non-connected graphs. When trained with sufficiently large number of data, the $S_{n}$-equivariant QCNN shows better average performance compared to $S_{n}$-equivariant QNN . These results contribute towards building powerful quantum machine learning architectures in permutation-symmetric systems.",
        "subjects": [
            "quant-ph",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "13 pages, 10 figures"
    },
    {
        "paper id": "2404.18199",
        "abstract url": "https://arxiv.org/abs/2404.18199",
        "title": "Rethinking Attention Gated with Hybrid Dual Pyramid Transformer-CNN for Generalized Segmentation in Medical Imaging",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Inspired by the success of Transformers in Computer vision, Transformers have been widely investigated for medical imaging segmentation. However, most of Transformer architecture are using the recent transformer architectures as encoder or as parallel encoder with the CNN encoder. In this paper, we introduce a novel hybrid CNN-Transformer segmentation architecture (PAG-TransYnet) designed for efficiently building a strong CNN-Transformer encoder. Our approach exploits attention gates within a Dual Pyramid hybrid encoder. The contributions of this methodology can be summarized into three key aspects: (i) the utilization of Pyramid input for highlighting the prominent features at different scales, (ii) the incorporation of a PVT transformer to capture long-range dependencies across various resolutions, and (iii) the implementation of a Dual-Attention Gate mechanism for effectively fusing prominent features from both CNN and Transformer branches. Through comprehensive evaluation across different segmentation tasks including: abdominal multi-organs segmentation, infection segmentation (Covid-19 and Bone Metastasis), microscopic tissues segmentation (Gland and Nucleus). The proposed approach demonstrates state-of-the-art performance and exhibits remarkable generalization capabilities. This research represents a significant advancement towards addressing the pressing need for efficient and adaptable segmentation solutions in medical imaging applications.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18201",
        "abstract url": "https://arxiv.org/abs/2404.18201",
        "title": "What Foundation Models can Bring for Robot Learning in Manipulation : A Survey",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "The realization of universal robots is an ultimate goal of researchers. However, a key hurdle in achieving this goal lies in the robots' ability to manipulate objects in their unstructured surrounding environments according to different tasks. The learning-based approach is considered an effective way to address generalization. The impressive performance of foundation models in the fields of computer vision and natural language suggests the potential of embedding foundation models into manipulation tasks as a viable path toward achieving general manipulation capability. However, we believe achieving general manipulation capability requires an overarching framework akin to auto driving. This framework should encompass multiple functional modules, with different foundation models assuming distinct roles in facilitating general manipulation capability. This survey focuses on the contributions of foundation models to robot learning for manipulation. We propose a comprehensive framework and detail how foundation models can address challenges in each module of the framework. What's more, we examine current approaches, outline challenges, suggest future research directions, and identify potential risks associated with integrating foundation models into this domain.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18213",
        "abstract url": "https://arxiv.org/abs/2404.18213",
        "title": "S$^2$Mamba: A Spatial-spectral State Space Model for Hyperspectral Image Classification",
        "rating": "-1",
        "keywords": [
            [
                "Hyperspectral Image"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Land cover analysis using hyperspectral images (HSI) remains an open problem due to their low spatial resolution and complex spectral information. Recent studies are primarily dedicated to designing Transformer-based architectures for spatial-spectral long-range dependencies modeling, which is computationally expensive with quadratic complexity. Selective structured state space model (Mamba), which is efficient for modeling long-range dependencies with linear complexity, has recently shown promising progress. However, its potential in hyperspectral image processing that requires handling numerous spectral bands has not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a spatial-spectral state space model for hyperspectral image classification, to excavate spatial-spectral contextual features, resulting in more efficient and accurate land cover analysis. In S$^2$Mamba, two selective structured state space models through different dimensions are designed for feature extraction, one for spatial, and the other for spectral, along with a spatial-spectral mixture gate for optimal fusion. More specifically, S$^2$Mamba first captures spatial contextual relations by interacting each pixel with its adjacent through a Patch Cross Scanning module and then explores semantic information from continuous spectral bands through a Bi-directional Spectral Scanning module. Considering the distinct expertise of the two attributes in homogenous and complicated texture scenes, we realize the Spatial-spectral Mixture Gate by a group of learnable matrices, allowing for the adaptive incorporation of representations learned across different dimensions. Extensive experiments conducted on HSI classification benchmarks demonstrate the superiority and prospect of S$^2$Mamba. The code will be available at: https://github.com/PURE-melo/S2Mamba.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "13 pages, 9 figures"
    },
    {
        "paper id": "2404.18221",
        "abstract url": "https://arxiv.org/abs/2404.18221",
        "title": "Automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Automatic design is a promising approach to realizing robot swarms. Given a mission to be performed by the swarm, an automatic method produces the required control software for the individual robots. Automatic design has concentrated on missions that a swarm can execute independently, interacting only with a static environment and without the involvement of other active entities. In this paper, we investigate the design of robot swarms that perform their mission by interacting with other robots that populate their environment. We frame our research within robot shepherding: the problem of using a small group of robots, the shepherds, to coordinate a relatively larger group, the sheep. In our study, the group of shepherds is the swarm that is automatically designed, and the sheep are pre-programmed robots that populate its environment. We use automatic modular design and neuroevolution to produce the control software for the swarm of shepherds to coordinate the sheep. We show that automatic design can leverage mission-specific interaction strategies to enable an effective coordination between the two groups.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "2024 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
        "paper id": "2404.18223",
        "abstract url": "https://arxiv.org/abs/2404.18223",
        "title": "On the suitability of single-edge notch tension (SENT) testing for assessing hydrogen-assisted cracking susceptibility",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "Combined experiments and computational modelling are used to increase understanding of the suitability of the Single-Edge Notch Tension (SENT) test for assessing hydrogen embrittlement susceptibility. The SENT tests were designed to provide the mode I threshold stress intensity factor ($K_{\\text{th}}$) for hydrogen-assisted cracking of a C110 steel in two corrosive environments. These were accompanied by hydrogen permeation experiments to relate the environments to the absorbed hydrogen concentrations. A coupled phase-field-based deformation-diffusion-fracture model is then employed to simulate the SENT tests, predicting $K_{\\text{th}}$ in good agreement with the experimental results and providing insights into the hydrogen absorption-diffusion-cracking interactions. The suitability of SENT testing and its optimal characteristics (e.g., test duration) are discussed in terms of the various simultaneous active time-dependent phenomena, triaxiality dependencies, and regimes of hydrogen embrittlement susceptibility.",
        "subjects": [
            "cs.CE",
            "cond-mat.mtrl-sci",
            "physics.app-ph",
            "physics.chem-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18235",
        "abstract url": "https://arxiv.org/abs/2404.18235",
        "title": "Flood Data Analysis on SpaceNet 8 Using Apache Sedona",
        "rating": "-1",
        "keywords": [
            [
                "remote sensing",
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the escalating frequency of floods posing persistent threats to human life and property, satellite remote sensing has emerged as an indispensable tool for monitoring flood hazards. SpaceNet8 offers a unique opportunity to leverage cutting-edge artificial intelligence technologies to assess these hazards. A significant contribution of this research is its application of Apache Sedona, an advanced platform specifically designed for the efficient and distributed processing of large-scale geospatial data. This platform aims to enhance the efficiency of error analysis, a critical aspect of improving flood damage detection accuracy. Based on Apache Sedona, we introduce a novel approach that addresses the challenges associated with inaccuracies in flood damage detection. This approach involves the retrieval of cases from historical flood events, the adaptation of these cases to current scenarios, and the revision of the model based on clustering algorithms to refine its performance. Through the replication of both the SpaceNet8 baseline and its top-performing models, we embark on a comprehensive error analysis. This analysis reveals several main sources of inaccuracies. To address these issues, we employ data visual interpretation and histogram equalization techniques, resulting in significant improvements in model metrics. After these enhancements, our indicators show a notable improvement, with precision up by 5%, F1 score by 2.6%, and IoU by 4.5%. This work highlights the importance of advanced geospatial data processing tools, such as Apache Sedona. By improving the accuracy and efficiency of flood detection, this research contributes to safeguarding public safety and strengthening infrastructure resilience in flood-prone areas, making it a valuable addition to the field of remote sensing and disaster management.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18255",
        "abstract url": "https://arxiv.org/abs/2404.18255",
        "title": "PatentGPT: A Large Language Model for Intellectual Property",
        "rating": "-1",
        "keywords": [
            [
                "Patent"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, large language models(LLMs) have attracted significant attention due to their exceptional performance across a multitude of natural language process tasks, and have been widely applied in various fields. However, the application of large language models in the Intellectual Property (IP) domain is challenging due to the strong need for specialized knowledge, privacy protection, processing of extremely long text in this field. In this technical report, we present for the first time a low-cost, standardized procedure for training IP-oriented LLMs, meeting the unique requirements of the IP domain. Using this standard process, we have trained the PatentGPT series models based on open-source pretrained models. By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4, indicating the effectiveness of the proposed training procedure and the expertise of the PatentGPT models in the IP domain. Remarkably, our model surpassed GPT-4 on the 2019 China Patent Agent Qualification Examination, scoring 65 and matching human expert levels. Additionally, the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks, potentially serving as an alternative to GPT-4 within the IP domain.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "19 pages, 9 figures"
    },
    {
        "paper id": "2404.18279",
        "abstract url": "https://arxiv.org/abs/2404.18279",
        "title": "Out-of-distribution Detection in Medical Image Analysis: A survey",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Computer-aided diagnostics has benefited from the development of deep learning-based computer vision techniques in these years. Traditional supervised deep learning methods assume that the test sample is drawn from the identical distribution as the training data. However, it is possible to encounter out-of-distribution samples in real-world clinical scenarios, which may cause silent failure in deep learning-based medical image analysis tasks. Recently, research has explored various out-of-distribution (OOD) detection situations and techniques to enable a trustworthy medical AI system. In this survey, we systematically review the recent advances in OOD detection in medical image analysis. We first explore several factors that may cause a distributional shift when using a deep-learning-based model in clinic scenarios, with three different types of distributional shift well defined on top of these factors. Then a framework is suggested to categorize and feature existing solutions, while the previous studies are reviewed based on the methodology taxonomy. Our discussion also includes evaluation protocols and metrics, as well as the challenge and a research direction lack of exploration.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "23 pages, 3 figures"
    },
    {
        "paper id": "2404.18284",
        "abstract url": "https://arxiv.org/abs/2404.18284",
        "title": "S3-SLAM: Sparse Tri-plane Encoding for Neural Implicit SLAM",
        "rating": "-1",
        "keywords": [
            [
                "NeRF",
                "Radiance Fields"
            ],
            [
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the emergence of Neural Radiance Fields (NeRF), neural implicit representations have gained widespread applications across various domains, including simultaneous localization and mapping. However, current neural implicit SLAM faces a challenging trade-off problem between performance and the number of parameters. To address this problem, we propose sparse tri-plane encoding, which efficiently achieves scene reconstruction at resolutions up to 512 using only 2~4% of the commonly used tri-plane parameters (reduced from 100MB to 2~4MB). On this basis, we design S3-SLAM to achieve rapid and high-quality tracking and mapping through sparsifying plane parameters and integrating orthogonal features of tri-plane. Furthermore, we develop hierarchical bundle adjustment to achieve globally consistent geometric structures and reconstruct high-resolution appearance. Experimental results demonstrate that our approach achieves competitive tracking and scene reconstruction with minimal parameters on three datasets. Source code will soon be available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18337",
        "abstract url": "https://arxiv.org/abs/2404.18337",
        "title": "Additive Spanner Lower Bounds with Optimal Inner Graph Structure",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "We construct $n$-node graphs on which any $O(n)$-size spanner has additive error at least $+\u03a9(n^{3/17})$, improving on the previous best lower bound of $\u03a9(n^{1/7})$ [Bodwin-Hoppenworth FOCS '22]. Our construction completes the first two steps of a particular three-step research program, introduced in prior work and overviewed here, aimed at producing tight bounds for the problem by aligning aspects of the upper and lower bound constructions. More specifically, we develop techniques that enable the use of inner graphs in the lower bound framework whose technical properties are provably tight with the corresponding assumptions made in the upper bounds. As an additional application of our techniques, we improve the corresponding lower bound for $O(n)$-size additive emulators to $+\u03a9(n^{1/14})$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "ICALP 2024"
    },
    {
        "paper id": "2404.18352",
        "abstract url": "https://arxiv.org/abs/2404.18352",
        "title": "Post-hoc and manifold explanations analysis of facial expression data based on deep learning",
        "rating": "-1",
        "keywords": [
            [
                "biological",
                "facial",
                "psychological"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The complex information processing system of humans generates a lot of objective and subjective evaluations, making the exploration of human cognitive products of great cutting-edge theoretical value. In recent years, deep learning technologies, which are inspired by biological brain mechanisms, have made significant strides in the application of psychological or cognitive scientific research, particularly in the memorization and recognition of facial data. This paper investigates through experimental research how neural networks process and store facial expression data and associate these data with a range of psychological attributes produced by humans. Researchers utilized deep learning model VGG16, demonstrating that neural networks can learn and reproduce key features of facial data, thereby storing image memories. Moreover, the experimental results reveal the potential of deep learning models in understanding human emotions and cognitive processes and establish a manifold visualization interpretation of cognitive products or psychological attributes from a non-Euclidean space perspective, offering new insights into enhancing the explainability of AI. This study not only advances the application of AI technology in the field of psychology but also provides a new psychological theoretical understanding the information processing of the AI. The code is available in here: https://github.com/NKUShaw/Psychoinformatics.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "19PAGES"
    },
    {
        "paper id": "2404.18356",
        "abstract url": "https://arxiv.org/abs/2404.18356",
        "title": "FEDQ-Trust: Efficient Data-Driven Trust Prediction for Mobile Edge-Based IoT Systems",
        "rating": "-1",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "We introduce FEDQ-Trust, an innovative data-driven trust prediction approach designed for mobile edge-based Internet of Things (IoT) environments. The decentralized nature of mobile edge environments introduces challenges due to variations in data distribution, impacting the accuracy and training efficiency of existing distributed data-driven trust prediction models. FEDQ-Trust effectively tackles the statistical heterogeneity challenges by integrating Federated Expectation-Maximization with Deep Q Networks. Federated Expectation-Maximization's robust handling of statistical heterogeneity significantly enhances trust prediction accuracy. Meanwhile, Deep Q Networks streamlines the model training process, efficiently reducing the number of training clients while maintaining model performance. We conducted a suite of experiments within simulated MEC-based IoT settings by leveraging two real-world IoT datasets. The experimental results demonstrate that our model achieved a significant convergence time reduction of 97% to 99% while ensuring a notable improvement of 8% to 14% in accuracy compared to state-of-the-art models.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "14 pages, 6 figures, submitted to IEEE Transactions on Services Computing"
    },
    {
        "paper id": "2404.18357",
        "abstract url": "https://arxiv.org/abs/2404.18357",
        "title": "Display in the Air: Balancing Distraction and Workload in AR Glasses Interfaces for Driving Navigation",
        "rating": "-1",
        "keywords": [
            [
                "Navigation"
            ]
        ],
        "abstract": "Augmented Reality (AR) navigation via Head-Mounted Displays (HMDs), particularly AR glasses, is revolutionizing the driving experience by integrating real-time routing information into the driver's field of view. Despite the potential of AR glasses, the question of how to display navigation information on the interface of these devices remains a valuable yet relatively unexplored research area. This study employs a mixed-method approach involving 32 participants, combining qualitative feedback from semi-structured interviews with quantitative data from usability questionnaires in both simulated and real-world scenarios. Highlighting the necessity of real-world testing, the research evaluates the impact of five icon placements on the efficiency and effectiveness of information perception in both environments. The experiment results indicate a preference for non-central icon placements, especially bottom-center in real world, which mostly balances distraction and workload for the driver. Moreover, these findings contribute to the formulation of four specific design implications for augmented reality interfaces and systems. This research advances the understanding of AR glasses in driving assistance and sets the stage for further developments in this emerging technology field.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18374",
        "abstract url": "https://arxiv.org/abs/2404.18374",
        "title": "Trajectory Optimization for Adaptive Informative Path Planning with Multimodal Sensing",
        "rating": "-1",
        "keywords": [
            [
                "Trajectory"
            ]
        ],
        "abstract": "We consider the problem of an autonomous agent equipped with multiple sensors, each with different sensing precision and energy costs. The agent's goal is to explore the environment and gather information subject to its resource constraints in unknown, partially observable environments. The challenge lies in reasoning about the effects of sensing and movement while respecting the agent's resource and dynamic constraints. We formulate the problem as a trajectory optimization problem and solve it using a projection-based trajectory optimization approach where the objective is to reduce the variance of the Gaussian process world belief. Our approach outperforms previous approaches in long horizon trajectories by achieving an overall variance reduction of up to 85% and reducing the root-mean square error in the environment belief by 50%. This approach was developed in support of rover path planning for the NASA VIPER Mission.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "IEEE International Conference on Control, Decision and Information Technologies"
    },
    {
        "paper id": "2404.18375",
        "abstract url": "https://arxiv.org/abs/2404.18375",
        "title": "Field Notes on Deploying Research Robots in Public Spaces",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Human-robot interaction requires to be studied in the wild. In the summers of 2022 and 2023, we deployed two trash barrel service robots through the wizard-of-oz protocol in public spaces to study human-robot interactions in urban settings. We deployed the robots at two different public plazas in downtown Manhattan and Brooklyn for a collective of 20 hours of field time. To date, relatively few long-term human-robot interaction studies have been conducted in shared public spaces. To support researchers aiming to fill this gap, we would like to share some of our insights and learned lessons that would benefit both researchers and practitioners on how to deploy robots in public spaces. We share best practices and lessons learned with the HRI research community to encourage more in-the-wild research of robots in public spaces and call for the community to share their lessons learned to a GitHub repository.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "CHI LBW 2024"
    },
    {
        "paper id": "2404.18381",
        "abstract url": "https://arxiv.org/abs/2404.18381",
        "title": "Object Registration in Neural Fields",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "6-DoF"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Neural fields provide a continuous scene representation of 3D geometry and appearance in a way which has great promise for robotics applications. One functionality that unlocks unique use-cases for neural fields in robotics is object 6-DoF registration. In this paper, we provide an expanded analysis of the recent Reg-NF neural field registration method and its use-cases within a robotics context. We showcase the scenario of determining the 6-DoF pose of known objects within a scene using scene and object neural field models. We show how this may be used to better represent objects within imperfectly modelled scenes and generate new scenes by substituting object neural field models into the scene.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "Accepted to ICRA 2024 RoboNeRF workshop. 5 pages, 10 figures. arXiv admin note: substantial text overlap with arXiv:2402.09722"
    },
    {
        "paper id": "2404.18383",
        "abstract url": "https://arxiv.org/abs/2404.18383",
        "title": "A Framework for Learning and Reusing Robotic Skills",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "In this paper, we present our work in progress towards creating a library of motion primitives. This library facilitates easier and more intuitive learning and reusing of robotic skills. Users can teach robots complex skills through Learning from Demonstration, which is automatically segmented into primitives and stored in clusters of similar skills. We propose a novel multimodal segmentation method as well as a novel trajectory clustering method. Then, when needed for reuse, we transform primitives into new environments using trajectory editing. We present simulated results for our framework with demonstrations taken on real-world robots.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "4 pages, 4 figures. Accepted for publication as work-in-progress paper at Ubiquitous Robots (UR) 2024. Code available here: https://github.com/brenhertel/Probabilistic-Segmentation and here: https://github.com/brenhertel/Elastic-Clustering"
    },
    {
        "paper id": "2404.18384",
        "abstract url": "https://arxiv.org/abs/2404.18384",
        "title": "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions",
        "rating": "-1",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Language models can hallucinate when performing complex and detailed mathematical reasoning. Physics provides a rich domain for assessing mathematical reasoning capabilities where physical context imbues the use of symbols which needs to satisfy complex semantics (\\textit{e.g.,} units, tensorial order), leading to instances where inference may be algebraically coherent, yet unphysical. In this work, we assess the ability of Language Models (LMs) to perform fine-grained mathematical and physical reasoning using a curated dataset encompassing multiple notations and Physics subdomains. We improve zero-shot scores using synthetic in-context examples, and demonstrate non-linear degradation of derivation quality with perturbation strength via the progressive omission of supporting premises. We find that the models' mathematical reasoning is not physics-informed in this setting, where physical context is predominantly ignored in favour of reverse-engineering solutions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18396",
        "abstract url": "https://arxiv.org/abs/2404.18396",
        "title": "DRAM-Profiler: An Experimental DRAM RowHammer Vulnerability Profiling Mechanism",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "RowHammer stands out as a prominent example, potentially the pioneering one, showcasing how a failure mechanism at the circuit level can give rise to a significant and pervasive security vulnerability within systems. Prior research has approached RowHammer attacks within a static threat model framework. Nonetheless, it warrants consideration within a more nuanced and dynamic model. This paper presents a low-overhead DRAM RowHammer vulnerability profiling technique termed DRAM-Profiler, which utilizes innovative test vectors for categorizing memory cells into distinct security levels. The proposed test vectors intentionally weaken the spatial correlation between the aggressors and victim rows before an attack for evaluation, thus aiding designers in mitigating RowHammer vulnerabilities in the mapping phase. While there has been no previous research showcasing the impact of such profiling to our knowledge, our study methodically assesses 128 commercial DDR4 DRAM products. The results uncover the significant variability among chips from different manufacturers in the type and quantity of RowHammer attacks that can be exploited by adversaries.",
        "subjects": [
            "cs.CR",
            "cs.AR"
        ],
        "comment": "6 pages, 6 figures"
    },
    {
        "paper id": "2404.18401",
        "abstract url": "https://arxiv.org/abs/2404.18401",
        "title": "Spectral-Spatial Mamba for Hyperspectral Image Classification",
        "rating": "-1",
        "keywords": [
            [
                "Hyperspectral Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, deep learning models have achieved excellent performance in hyperspectral image (HSI) classification. Among the many deep models, Transformer has gradually attracted interest for its excellence in modeling the long-range dependencies of spatial-spectral features in HSI. However, Transformer has the problem of quadratic computational complexity due to the self-attention mechanism, which is heavier than other models and thus has limited adoption in HSI processing. Fortunately, the recently emerging state space model-based Mamba shows great computational efficiency while achieving the modeling power of Transformers. Therefore, in this paper, we make a preliminary attempt to apply the Mamba to HSI classification, leading to the proposed spectral-spatial Mamba (SS-Mamba). Specifically, the proposed SS-Mamba mainly consists of spectral-spatial token generation module and several stacked spectral-spatial Mamba blocks. Firstly, the token generation module converts any given HSI cube to spatial and spectral tokens as sequences. And then these tokens are sent to stacked spectral-spatial mamba blocks (SS-MB). Each SS-MB block consists of two basic mamba blocks and a spectral-spatial feature enhancement module. The spatial and spectral tokens are processed separately by the two basic mamba blocks, respectively. Besides, the feature enhancement module modulates spatial and spectral tokens using HSI sample's center region information. In this way, the spectral and spatial tokens cooperate with each other and achieve information fusion within each block. The experimental results conducted on widely used HSI datasets reveal that the proposed model achieves competitive results compared with the state-of-the-art methods. The Mamba-based method opens a new window for HSI classification.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2404.18416",
        "abstract url": "https://arxiv.org/abs/2404.18416",
        "title": "Capabilities of Gemini Models in Medicine",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "health"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health & medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18418",
        "abstract url": "https://arxiv.org/abs/2404.18418",
        "title": "Decomposition Model Assisted Energy-Saving Design in Radio Access Network",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The continuous emergence of novel services and massive connections involve huge energy consumption towards ultra-dense radio access networks. Moreover, there exist much more number of controllable parameters that can be adjusted to reduce the energy consumption from a network-wide perspective. However, a network-level energy-saving intent usually contains multiple network objectives and constraints. Therefore, it is critical to decompose a network-level energy-saving intent into multiple levels of configurated operations from a top-down refinement perspective. In this work, we utilize a softgoal interdependency graph decomposition model to assist energy-saving scheme design. Meanwhile, we propose an energy-saving approach based on deep Q-network, which achieve a better trade-off among the energy consumption, the throughput, and the first packet delay. In addition, we illustrate how the decomposition model can assist in making energy-saving decisions. Evaluation results demonstrate the performance gain of the proposed scheme in accelerating the model training process.",
        "subjects": [
            "cs.NI",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18419",
        "abstract url": "https://arxiv.org/abs/2404.18419",
        "title": "Research on Intelligent Aided Diagnosis System of Medical Image Based on Computer Deep Learning",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "Diagnosis",
                "clinical",
                "tumor"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This paper combines Struts and Hibernate two architectures together, using DAO (Data Access Object) to store and access data. Then a set of dual-mode humidity medical image library suitable for deep network is established, and a dual-mode medical image assisted diagnosis method based on the image is proposed. Through the test of various feature extraction methods, the optimal operating characteristic under curve product (AUROC) is 0.9985, the recall rate is 0.9814, and the accuracy is 0.9833. This method can be applied to clinical diagnosis, and it is a practical method. Any outpatient doctor can register quickly through the system, or log in to the platform to upload the image to obtain more accurate images. Through the system, each outpatient physician can quickly register or log in to the platform for image uploading, thus obtaining more accurate images. The segmentation of images can guide doctors in clinical departments. Then the image is analyzed to determine the location and nature of the tumor, so as to make targeted treatment.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.01585",
        "abstract url": "https://arxiv.org/abs/2405.01585",
        "title": "Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications",
        "rating": "-1",
        "keywords": [
            [
                "Tabular"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In recent times Large Language Models have exhibited tremendous capabilities, especially in the areas of mathematics, code generation and general-purpose reasoning. However for specialized domains especially in applications that require parsing and analyzing large chunks of numeric or tabular data even state-of-the-art (SOTA) models struggle. In this paper, we introduce a new approach to solving domain-specific tabular data analysis tasks by presenting a unique RAG workflow that mitigates the scalability issues of existing tabular LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel approach to fine-tune embedding models for tabular Retrieval-Augmentation Generation (RAG) applications. Embedding models form a crucial component in the RAG workflow and even current SOTA embedding models struggle as they are predominantly trained on textual datasets and thus underperform in scenarios involving complex tabular data. The evaluation results showcase that our approach not only outperforms current SOTA embedding models in this domain but also does so with a notably smaller and more efficient model structure.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.IR"
        ],
        "comment": "11 pages, 5 figures"
    },
    {
        "paper id": "2404.18214",
        "abstract url": "https://arxiv.org/abs/2404.18214",
        "title": "Contrastive Learning Method for Sequential Recommendation based on Multi-Intention Disentanglement",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Sequential recommendation is one of the important branches of recommender system, aiming to achieve personalized recommended items for the future through the analysis and prediction of users' ordered historical interactive behaviors. However, along with the growth of the user volume and the increasingly rich behavioral information, how to understand and disentangle the user's interactive multi-intention effectively also poses challenges to behavior prediction and sequential recommendation. In light of these challenges, we propose a Contrastive Learning sequential recommendation method based on Multi-Intention Disentanglement (MIDCL). In our work, intentions are recognized as dynamic and diverse, and user behaviors are often driven by current multi-intentions, which means that the model needs to not only mine the most relevant implicit intention for each user, but also impair the influence from irrelevant intentions. Therefore, we choose Variational Auto-Encoder (VAE) to realize the disentanglement of users' multi-intentions. We propose two types of contrastive learning paradigms for finding the most relevant user's interactive intention, and maximizing the mutual information of positive sample pairs, respectively. Experimental results show that MIDCL not only has significant superiority over most existing baseline methods, but also brings a more interpretable case to the research about intention-based prediction and recommendation.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18251",
        "abstract url": "https://arxiv.org/abs/2404.18251",
        "title": "Machine Learning for Blockchain Data Analysis: Progress and Opportunities",
        "rating": "-1.5",
        "keywords": [
            [
                "crime"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Blockchain technology has rapidly emerged to mainstream attention, while its publicly accessible, heterogeneous, massive-volume, and temporal data are reminiscent of the complex dynamics encountered during the last decade of big data. Unlike any prior data source, blockchain datasets encompass multiple layers of interactions across real-world entities, e.g., human users, autonomous programs, and smart contracts. Furthermore, blockchain's integration with cryptocurrencies has introduced financial aspects of unprecedented scale and complexity such as decentralized finance, stablecoins, non-fungible tokens, and central bank digital currencies. These unique characteristics present both opportunities and challenges for machine learning on blockchain data. On one hand, we examine the state-of-the-art solutions, applications, and future directions associated with leveraging machine learning for blockchain data analysis critical for the improvement of blockchain technology such as e-crime detection and trends prediction. On the other hand, we shed light on the pivotal role of blockchain by providing vast datasets and tools that can catalyze the growth of the evolving machine learning ecosystem. This paper serves as a comprehensive resource for researchers, practitioners, and policymakers, offering a roadmap for navigating this dynamic and transformative field.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18273",
        "abstract url": "https://arxiv.org/abs/2404.18273",
        "title": "Kernel Corrector LSTM",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Forecasting methods are affected by data quality issues in two ways: 1. they are hard to predict, and 2. they may affect the model negatively when it is updated with new data. The latter issue is usually addressed by pre-processing the data to remove those issues. An alternative approach has recently been proposed, Corrector LSTM (cLSTM), which is a Read \\& Write Machine Learning (RW-ML) algorithm that changes the data while learning to improve its predictions. Despite promising results being reported, cLSTM is computationally expensive, as it uses a meta-learner to monitor the hidden states of the LSTM. We propose a new RW-ML algorithm, Kernel Corrector LSTM (KcLSTM), that replaces the meta-learner of cLSTM with a simpler method: Kernel Smoothing. We empirically evaluate the forecasting accuracy and the training time of the new algorithm and compare it with cLSTM and LSTM. Results indicate that it is able to decrease the training time while maintaining a competitive forecasting accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 4 figures, IDA 2024"
    },
    {
        "paper id": "2404.18296",
        "abstract url": "https://arxiv.org/abs/2404.18296",
        "title": "Using Deep Q-Learning to Dynamically Toggle between Push/Pull Actions in Computational Trust Mechanisms",
        "rating": "-1.5",
        "keywords": [
            [
                "biologically"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent work on decentralized computational trust models for open Multi Agent Systems has resulted in the development of CA, a biologically inspired model which focuses on the trustee's perspective. This new model addresses a serious unresolved problem in existing trust and reputation models, namely the inability to handle constantly changing behaviors and agents' continuous entry and exit from the system. In previous work, we compared CA to FIRE, a well-known trust and reputation model, and found that CA is superior when the trustor population changes, whereas FIRE is more resilient to the trustee population changes. Thus, in this paper, we investigate how the trustors can detect the presence of several dynamic factors in their environment and then decide which trust model to employ in order to maximize utility. We frame this problem as a machine learning problem in a partially observable environment, where the presence of several dynamic factors is not known to the trustor and we describe how an adaptable trustor can rely on a few measurable features so as to assess the current state of the environment and then use Deep Q Learning (DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt to a changing environment. We ran a series of simulation experiments to compare the performance of the adaptable trustor with the performance of trustors using only one model (FIRE or CA) and we show that an adaptable agent is indeed capable of learning when to use each model and, thus, perform consistently in dynamic environments.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18304",
        "abstract url": "https://arxiv.org/abs/2404.18304",
        "title": "Retrieval-Oriented Knowledge for Click-Through Rate Prediction",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Click-through rate (CTR) prediction plays an important role in personalized recommendations. Recently, sample-level retrieval-based models (e.g., RIM) have achieved remarkable performance by retrieving and aggregating relevant samples. However, their inefficiency at the inference stage makes them impractical for industrial applications. To overcome this issue, this paper proposes a universal plug-and-play Retrieval-Oriented Knowledge (ROK) framework. Specifically, a knowledge base, consisting of a retrieval-oriented embedding layer and a knowledge encoder, is designed to preserve and imitate the retrieved & aggregated representations in a decomposition-reconstruction paradigm. Knowledge distillation and contrastive learning methods are utilized to optimize the knowledge base, and the learned retrieval-enhanced representations can be integrated with arbitrary CTR models in both instance-wise and feature-wise manners. Extensive experiments on three large-scale datasets show that ROK achieves competitive performance with the retrieval-based CTR models while reserving superior inference efficiency and model compatibility.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18314",
        "abstract url": "https://arxiv.org/abs/2404.18314",
        "title": "DIRESA, a distance-preserving nonlinear dimension reduction technique based on regularized autoencoders",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In meteorology, finding similar weather patterns or analogs in historical datasets can be useful for data assimilation, forecasting, and postprocessing. In climate science, analogs in historical and climate projection data are used for attribution and impact studies. However, most of the time, those large weather and climate datasets are nearline. They must be downloaded, which takes a lot of bandwidth and disk space, before the computationally expensive search can be executed. We propose a dimension reduction technique based on autoencoder (AE) neural networks to compress those datasets and perform the search in an interpretable, compressed latent space. A distance-regularized Siamese twin autoencoder (DIRESA) architecture is designed to preserve distance in latent space while capturing the nonlinearities in the datasets. Using conceptual climate models of different complexities, we show that the latent components thus obtained provide physical insight into the dominant modes of variability in the system. Compressing datasets with DIRESA reduces the online storage and keeps the latent components uncorrelated, while the distance (ordering) preservation and reconstruction fidelity robustly outperform Principal Component Analysis (PCA) and other dimension reduction techniques such as UMAP or variational autoencoders.",
        "subjects": [
            "cs.LG",
            "nlin.CD",
            "physics.ao-ph"
        ],
        "comment": "16 pages, 10 figures, 4 tables; 7 pages of Supporting Information"
    },
    {
        "paper id": "2404.18362",
        "abstract url": "https://arxiv.org/abs/2404.18362",
        "title": "Physics-informed Convolutional Neural Network for Microgrid Economic Dispatch",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The variability of renewable energy generation and the unpredictability of electricity demand create a need for real-time economic dispatch (ED) of assets in microgrids. However, solving numerical optimization problems in real-time can be incredibly challenging. This study proposes using a convolutional neural network (CNN) based on deep learning to address these challenges. Compared to traditional methods, CNN is more efficient, delivers more dependable results, and has a shorter response time when dealing with uncertainties. While CNN has shown promising results, it does not extract explainable knowledge from the data. To address this limitation, a physics-inspired CNN model is developed by incorporating constraints of the ED problem into the CNN training to ensure that the model follows physical laws while fitting the data. The proposed method can significantly accelerate real-time economic dispatch of microgrids without compromising the accuracy of numerical optimization techniques. The effectiveness of the proposed data-driven approach for optimal allocation of microgrid resources in real-time is verified through a comprehensive comparison with conventional numerical optimization approaches.",
        "subjects": [
            "eess.SY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18404",
        "abstract url": "https://arxiv.org/abs/2404.18404",
        "title": "Deep generative modelling of canonical ensemble with differentiable thermal properties",
        "rating": "-1.5",
        "keywords": [
            [
                "thermal"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a variational modelling method with differentiable temperature for canonical ensembles. Using a deep generative model, the free energy is estimated and minimized simultaneously in a continuous temperature range. At optimal, this generative model is a Boltzmann distribution with temperature dependence. The training process requires no dataset, and works with arbitrary explicit density generative models. We applied our method to study the phase transitions (PT) in the Ising and XY models, and showed that the direct-sampling simulation of our model is as accurate as the Markov Chain Monte Carlo (MCMC) simulation, but more efficient. Moreover, our method can give thermodynamic quantities as differentiable functions of temperature akin to an analytical solution. The free energy aligns closely with the exact one to the second-order derivative, so this inclusion of temperature dependence enables the otherwise biased variational model to capture the subtle thermal effects at the PTs. These findings shed light on the direct simulation of physical systems using deep generative models",
        "subjects": [
            "cond-mat.stat-mech",
            "cs.LG"
        ],
        "comment": "Main text: 4.5 pages, 2 figures. Supplement: 9 pages"
    },
    {
        "paper id": "2404.18960",
        "abstract url": "https://arxiv.org/abs/2404.18960",
        "title": "Leak Proof CMap; a framework for training and evaluation of cell line agnostic L1000 similarity methods",
        "rating": "-1.5",
        "keywords": [
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Connectivity Map (CMap) is a large publicly available database of cellular transcriptomic responses to chemical and genetic perturbations built using a standardized acquisition protocol known as the L1000 technique. Databases such as CMap provide an exciting opportunity to enrich drug discovery efforts, providing a 'known' phenotypic landscape to explore and enabling the development of state of the art techniques for enhanced information extraction and better informed decisions. Whilst multiple methods for measuring phenotypic similarity and interrogating profiles have been developed, the field is severely lacking standardized benchmarks using appropriate data splitting for training and unbiased evaluation of machine learning methods. To address this, we have developed 'Leak Proof CMap' and exemplified its application to a set of common transcriptomic and generic phenotypic similarity methods along with an exemplar triplet loss-based method. Benchmarking in three critical performance areas (compactness, distinctness, and uniqueness) is conducted using carefully crafted data splits ensuring no similar cell lines or treatments with shared or closely matching responses or mechanisms of action are present in training, validation, or test sets. This enables testing of models with unseen samples akin to exploring treatments with novel modes of action in novel patient derived cell lines. With a carefully crafted benchmark and data splitting regime in place, the tooling now exists to create performant phenotypic similarity methods for use in personalized medicine (novel cell lines) and to better augment high throughput phenotypic screening technologies with the L1000 transcriptomic technology.",
        "subjects": [
            "q-bio.QM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18105",
        "abstract url": "https://arxiv.org/abs/2404.18105",
        "title": "Tightly-Coupled VLP/INS Integrated Navigation by Inclination Estimation and Blockage Handling",
        "rating": "-2",
        "keywords": [
            [
                "robot",
                "Navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Visible Light Positioning (VLP) has emerged as a promising technology capable of delivering indoor localization with high accuracy. In VLP systems that use Photodiodes (PDs) as light receivers, the Received Signal Strength (RSS) is affected by the incidence angle of light, making the inclination of PDs a critical parameter in the positioning model. Currently, most studies assume the inclination to be constant, limiting the applications and positioning accuracy. Additionally, light blockages may severely interfere with the RSS measurements but the literature has not explored blockage detection in real-world experiments. To address these problems, we propose a tightly coupled VLP/INS (Inertial Navigation System) integrated navigation system that uses graph optimization to account for varying PD inclinations and VLP blockages. We also discussed the possibility of simultaneously estimating the robot's pose and the locations of some unknown LEDs. Simulations and two groups of real-world experiments demonstrate the efficiency of our approach, achieving an average positioning accuracy of 10 cm during movement and inclination accuracy within 1 degree despite inclination changes and blockages.",
        "subjects": [
            "cs.RO",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18166",
        "abstract url": "https://arxiv.org/abs/2404.18166",
        "title": "Behavior-Contextualized Item Preference Modeling for Multi-Behavior Recommendation",
        "rating": "-2",
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "In recommender systems, multi-behavior methods have demonstrated their effectiveness in mitigating issues like data sparsity, a common challenge in traditional single-behavior recommendation approaches. These methods typically infer user preferences from various auxiliary behaviors and apply them to the target behavior for recommendations. However, this direct transfer can introduce noise to the target behavior in recommendation, due to variations in user attention across different behaviors. To address this issue, this paper introduces a novel approach, Behavior-Contextualized Item Preference Modeling (BCIPM), for multi-behavior recommendation. Our proposed Behavior-Contextualized Item Preference Network discerns and learns users' specific item preferences within each behavior. It then considers only those preferences relevant to the target behavior for final recommendations, significantly reducing noise from auxiliary behaviors. These auxiliary behaviors are utilized solely for training the network parameters, thereby refining the learning process without compromising the accuracy of the target behavior recommendations. To further enhance the effectiveness of BCIPM, we adopt a strategy of pre-training the initial embeddings. This step is crucial for enriching the item-aware preferences, particularly in scenarios where data related to the target behavior is sparse. Comprehensive experiments conducted on four real-world datasets demonstrate BCIPM's superior performance compared to several leading state-of-the-art models, validating the robustness and efficiency of our proposed approach.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "This paper has been accepted by SIGIR 2024"
    },
    {
        "paper id": "2404.18170",
        "abstract url": "https://arxiv.org/abs/2404.18170",
        "title": "Bridging Worlds: Achieving Language Interoperability between Julia and Python in Scientific Computing",
        "rating": "-2",
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "In the realm of scientific computing, both Julia and Python have established themselves as powerful tools. Within the context of High Energy Physics (HEP) data analysis, Python has been traditionally favored, yet there exists a compelling case for migrating legacy software to Julia. This article focuses on language interoperability, specifically exploring how Awkward Array data structures can bridge the gap between Julia and Python. The talk offers insights into key considerations such as memory management, data buffer copies, and dependency handling. It delves into the performance enhancements achieved by invoking Julia from Python and vice versa, particularly for intensive array-oriented calculations involving large-scale, though not excessively dimensional, arrays of HEP data. The advantages and challenges inherent in achieving interoperability between Julia and Python in the domain of scientific computing are discussed.",
        "subjects": [
            "cs.PL",
            "physics.data-an"
        ],
        "comment": "8 pages, 1 figure, ACAT2024 workshop"
    },
    {
        "paper id": "2404.18203",
        "abstract url": "https://arxiv.org/abs/2404.18203",
        "title": "LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision. To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds. To compensate for the loss of perception in the 3D domain, structural features are extracted as well. These quality logits and structural features are then combined and regressed into quality scores. Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model understanding and assessment accuracy. We hope our contributions can inspire subsequent investigations into the fusion of LMMs with PCQA, fostering advancements in 3D visual quality analysis and beyond.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18226",
        "abstract url": "https://arxiv.org/abs/2404.18226",
        "title": "A quantum compiler design method by using linear combinations of permutations",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "A matrix can be converted into a doubly stochastic matrix by using two diagonal matrices. And a doubly stochastic matrix can be written as a sum of permutation matrices. In this paper, we describe a method to write a given generic matrix in terms of quantum gates based on the block encoding. In particular, we first show how to convert a matrix into doubly stochastic matrices and by using Birkhoff's algorithm, we express that matrix in terms of a linear combination of permutations which can be mapped to quantum circuits. We then discuss a few optimization techniques that can be applied in a possibly future quantum compiler software based on the method described here.",
        "subjects": [
            "quant-ph",
            "cs.PL"
        ],
        "comment": "related code https://github.com/adaskin/stochastic-matrix-into-permutations/"
    },
    {
        "paper id": "2404.18291",
        "abstract url": "https://arxiv.org/abs/2404.18291",
        "title": "Panoptic Segmentation and Labelling of Lumbar Spine Vertebrae using Modified Attention Unet",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "diagnosis",
                "MRI"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Segmentation and labeling of vertebrae in MRI images of the spine are critical for the diagnosis of illnesses and abnormalities. These steps are indispensable as MRI technology provides detailed information about the tissue structure of the spine. Both supervised and unsupervised segmentation methods exist, yet acquiring sufficient data remains challenging for achieving high accuracy. In this study, we propose an enhancing approach based on modified attention U-Net architecture for panoptic segmentation of 3D sliced MRI data of the lumbar spine. Our method achieves an impressive accuracy of 99.5\\% by incorporating novel masking logic, thus significantly advancing the state-of-the-art in vertebral segmentation and labeling. This contributes to more precise and reliable diagnosis and treatment planning.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "9 pages, 10 figures"
    },
    {
        "paper id": "2404.18297",
        "abstract url": "https://arxiv.org/abs/2404.18297",
        "title": "Coordination Capacity for Classical-Quantum Correlations",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Network coordination is considered in three basic settings, characterizing the generation of separable and classical-quantum correlations among multiple parties. First, we consider the simulation of a classical-quantum state between two nodes with rate-limited common randomness (CR) and communication. Furthermore, we study the preparation of a separable state between multiple nodes with rate-limited CR and no communication. At last, we consider a broadcast setting, where a sender and two receivers simulate a classical-quantum-quantum state using rate-limited CR and communication. We establish the optimal tradeoff between communication and CR rates in each setting.",
        "subjects": [
            "quant-ph",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18300",
        "abstract url": "https://arxiv.org/abs/2404.18300",
        "title": "VoroTO: Multiscale Topology Optimization of Voronoi Structures using Surrogate Neural Networks",
        "rating": "-2",
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "Cellular structures found in nature exhibit remarkable properties such as high strength, high energy absorption, excellent thermal/acoustic insulation, and fluid transfusion. Many of these structures are Voronoi-like; therefore researchers have proposed Voronoi multi-scale designs for a wide variety of engineering applications. However, designing such structures can be computationally prohibitive due to the multi-scale nature of the underlying analysis and optimization. In this work, we propose the use of a neural network (NN) to carry out efficient topology optimization (TO) of multi-scale Voronoi structures. The NN is first trained using Voronoi parameters (cell site locations, thickness, orientation, and anisotropy) to predict the homogenized constitutive properties. This network is then integrated into a conventional TO framework to minimize structural compliance subject to a volume constraint. Special considerations are given for ensuring positive definiteness of the constitutive matrix and promoting macroscale connectivity. Several numerical examples are provided to showcase the proposed method.",
        "subjects": [
            "cs.CE",
            "math.NA"
        ],
        "comment": "Submitted to Engineering with Computers"
    },
    {
        "paper id": "2404.18302",
        "abstract url": "https://arxiv.org/abs/2404.18302",
        "title": "GNarsil: Splitting Stabilizers into Gauges",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum subsystem codes have been shown to improve error-correction performance, ease the implementation of logical operations on codes, and make stabilizer measurements easier by decomposing stabilizers into smaller-weight gauge operators. In this paper, we present two algorithms that produce new subsystem codes from a \"seed\" CSS code. They replace some stabilizers of a given CSS code with smaller-weight gauge operators that split the remaining stabilizers, while being compatible with the logical Pauli operators of the code. The algorithms recover the well-known Bacon-Shor code computationally as well as produce a new $\\left[\\left[ 9,1,2,2 \\right]\\right]$ rotated surface subsystem code with weight-$3$ gauges and weight-$4$ stabilizers. We illustrate using a $\\left[\\left[ 100,25,3 \\right]\\right]$ subsystem hypergraph product (SHP) code that the algorithms can produce more efficient gauge operators than the closed-form expressions of the SHP construction. However, we observe that the stabilizers of the lifted product quantum LDPC codes are more challenging to split into small-weight gauge operators. Hence, we introduce the subsystem lifted product (SLP) code construction and develop a new $\\left[\\left[ 775, 124, 20 \\right]\\right]$ code from Tanner's classical quasi-cyclic LDPC code. The code has high-weight stabilizers but all gauge operators that split stabilizers have weight $5$, except one. In contrast, the LP stabilizer code from Tanner's code has parameters $\\left[\\left[ 1054, 124, 20 \\right]\\right]$. This serves as a novel example of new subsystem codes that outperform stabilizer versions of them. Finally, based on our experiments, we share some general insights about non-locality's effects on the performance of splitting stabilizers into small-weight gauges.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": "8 Pages, 3 Figures"
    },
    {
        "paper id": "2404.18305",
        "abstract url": "https://arxiv.org/abs/2404.18305",
        "title": "Data-Driven Dynamic State Estimation of Photovoltaic Systems via Sparse Regression Unscented Kalman Filter",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Dynamic state estimation (DSE) is vital in modern power systems with numerous inverter-based distributed energy resources including solar and wind, ensuring real-time accuracy for tracking system variables and optimizing grid stability. This paper proposes a data-driven DSE approach designed for photovoltaic (PV) energy conversion systems (single stage and two stage) that are subjected to both process and measurement noise. The proposed framework follows a two-phase methodology encompassing ``data-driven model identification\" and ``state-estimation.\" In the initial model identification phase, state feedback is gathered to elucidate the dynamics of the photovoltaic systems using nonlinear sparse regression technique. Following the identification of the PV dynamics, the nonlinear data-driven model will be utilized to estimate the dynamics of the PV system for monitoring and protection purposes. To account for incomplete measurements, inherent uncertainties, and noise, we employ an ``unscented Kalman filter,\" which facilitates state estimation by processing the noisy output data. Ultimately, the paper substantiates the efficacy of the proposed sparse regression-based unscented Kalman filter through simulation results, providing a comparative analysis with a physics-based DSE.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18312",
        "abstract url": "https://arxiv.org/abs/2404.18312",
        "title": "Application of Iterative LQR on a Mobile Robot With Simple Dynamics",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "The aim in this paper is to apply the iLQR, iterative Linear Quadratic Regulator, to control the movement of a mobile robot following an already defined trajectory. This control strategy has proven its utility for nonlinear systems. As follows up, this work intends to concertize this statement and to evaluate the extent to which the performance is comparatively improved against the ordinary, non-iterative LQR. The method is applied to a differential robot with non-holonomic constraints. The mathematical equations, resulting description and the implementation of this method are explicitly explained, and the simulation studies are conducted in the Matlab and Simulink environment.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18319",
        "abstract url": "https://arxiv.org/abs/2404.18319",
        "title": "User Welfare Optimization in Recommender Systems with Competing Content Creators",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Driven by the new economic opportunities created by the creator economy, an increasing number of content creators rely on and compete for revenue generated from online content recommendation platforms. This burgeoning competition reshapes the dynamics of content distribution and profoundly impacts long-term user welfare on the platform. However, the absence of a comprehensive picture of global user preference distribution often traps the competition, especially the creators, in states that yield sub-optimal user welfare. To encourage creators to best serve a broad user population with relevant content, it becomes the platform's responsibility to leverage its information advantage regarding user preference distribution to accurately signal creators. In this study, we perform system-side user welfare optimization under a competitive game setting among content creators. We propose an algorithmic solution for the platform, which dynamically computes a sequence of weights for each user based on their satisfaction of the recommended content. These weights are then utilized to design mechanisms that adjust the recommendation policy or the post-recommendation rewards, thereby influencing creators' content production strategies. To validate the effectiveness of our proposed method, we report our findings from a series of experiments, including: 1. a proof-of-concept negative example illustrating how creators' strategies converge towards sub-optimal states without platform intervention; 2. offline experiments employing our proposed intervention mechanisms on diverse datasets; and 3. results from a three-week online experiment conducted on a leading short-video recommendation platform.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18363",
        "abstract url": "https://arxiv.org/abs/2404.18363",
        "title": "Reactive Composition of UAV Delivery Services in Urban Environments",
        "rating": "-2",
        "keywords": [
            [
                "UAV",
                "drone"
            ]
        ],
        "abstract": "We propose a novel failure-aware reactive UAV delivery service composition framework. A skyway network infrastructure is presented for the effective provisioning of services in urban areas. We present a formal drone delivery service model and a system architecture for reactive drone delivery services. We develop radius-based, cell density-based, and two-phased algorithms to reduce the search space and perform reactive service compositions when a service failure occurs. We conduct a set of experiments with a real drone dataset to demonstrate the effectiveness of our proposed approach.",
        "subjects": [
            "cs.RO",
            "cs.DC"
        ],
        "comment": "14 pages, 18 figures. This is an accepted paper and it is going to appear in IEEE Transactions on Intelligent Transportation Systems (T-ITS)"
    },
    {
        "paper id": "2404.18388",
        "abstract url": "https://arxiv.org/abs/2404.18388",
        "title": "SPECIAL: Synopsis Assisted Secure Collaborative Analytics",
        "rating": "-2",
        "keywords": [
            [
                "SQL"
            ]
        ],
        "abstract": "Secure collaborative analytics (SCA) enable the processing of analytical SQL queries across multiple owners' data, even when direct data sharing is not feasible. Although essential for strong privacy, the large overhead from data-oblivious primitives in traditional SCA has hindered its practical adoption. Recent SCA variants that permit controlled leakages under differential privacy (DP) show a better balance between privacy and efficiency. However, they still face significant challenges, such as potentially unbounded privacy loss, suboptimal query planning, and lossy processing. To address these challenges, we introduce SPECIAL, the first SCA system that simultaneously ensures bounded privacy loss, advanced query planning, and lossless processing. SPECIAL employs a novel synopsis-assisted secure processing model, where a one-time privacy cost is spent to acquire private synopses (table statistics) from owner data. These synopses then allow SPECIAL to estimate (compaction) sizes for secure operations (e.g., filter, join) and index encrypted data without extra privacy loss. Crucially, these estimates and indexes can be prepared before runtime, thereby facilitating efficient query planning and accurate cost estimations. Moreover, by using one-sided noise mechanisms and private upper bound techniques, SPECIAL ensures strict lossless processing for complex queries (e.g., multi-join). Through a comprehensive benchmark, we show that SPECIAL significantly outperforms cutting-edge SCAs, with up to 80X faster query times and over 900X smaller memory for complex queries. Moreover, it also achieves up to an 89X reduction in privacy loss under continual processing.",
        "subjects": [
            "cs.CR",
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18409",
        "abstract url": "https://arxiv.org/abs/2404.18409",
        "title": "PKU-AIGIQA-4K: A Perceptual Quality Assessment Database for Both Text-to-Image and Image-to-Image AI-Generated Images",
        "rating": "-2",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, image generation technology has rapidly advanced, resulting in the creation of a vast array of AI-generated images (AIGIs). However, the quality of these AIGIs is highly inconsistent, with low-quality AIGIs severely impairing the visual experience of users. Due to the widespread application of AIGIs, the AI-generated image quality assessment (AIGIQA), aimed at evaluating the quality of AIGIs from the perspective of human perception, has garnered increasing interest among scholars. Nonetheless, current research has not yet fully explored this field. We have observed that existing databases are limited to images generated from single scenario settings. Databases such as AGIQA-1K, AGIQA-3K, and AIGCIQA2023, for example, only include images generated by text-to-image generative models. This oversight highlights a critical gap in the current research landscape, underscoring the need for dedicated databases catering to image-to-image scenarios, as well as more comprehensive databases that encompass a broader range of AI-generated image scenarios. Addressing these issues, we have established a large scale perceptual quality assessment database for both text-to-image and image-to-image AIGIs, named PKU-AIGIQA-4K. We then conduct a well-organized subjective experiment to collect quality labels for AIGIs and perform a comprehensive analysis of the PKU-AIGIQA-4K database. Regarding the use of image prompts during the training process, we propose three image quality assessment (IQA) methods based on pre-trained models that include a no-reference method NR-AIGCIQA, a full-reference method FR-AIGCIQA, and a partial-reference method PR-AIGCIQA. Finally, leveraging the PKU-AIGIQA-4K database, we conduct extensive benchmark experiments and compare the performance of the proposed methods and the current IQA methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages. arXiv admin note: substantial text overlap with arXiv:2311.15556"
    },
    {
        "paper id": "2405.01509",
        "abstract url": "https://arxiv.org/abs/2405.01509",
        "title": "Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models",
        "rating": "-2",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "watermarking"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In the rapidly evolving domain of artificial intelligence, safeguarding the intellectual property of Large Language Models (LLMs) is increasingly crucial. Current watermarking techniques against model extraction attacks, which rely on signal insertion in model logits or post-processing of generated text, remain largely heuristic. We propose a novel method for embedding learnable linguistic watermarks in LLMs, aimed at tracing and preventing model extraction attacks. Our approach subtly modifies the LLM's output distribution by introducing controlled noise into token frequency distributions, embedding an statistically identifiable controllable watermark.We leverage statistical hypothesis testing and information theory, particularly focusing on Kullback-Leibler Divergence, to differentiate between original and modified distributions effectively. Our watermarking method strikes a delicate well balance between robustness and output quality, maintaining low false positive/negative rates and preserving the LLM's original performance.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "not decided"
    },
    {
        "paper id": "2404.18077",
        "abstract url": "https://arxiv.org/abs/2404.18077",
        "title": "Generative AI for Low-Carbon Artificial Intelligence of Things",
        "rating": "-2.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "By integrating Artificial Intelligence (AI) with the Internet of Things (IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields. However, AIoT is facing the challenges of energy consumption and carbon emissions due to the continuous advancement of mobile technology. Fortunately, Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT due to its excellent reasoning and generation capabilities. In this article, we explore the potential of GAI for carbon emissions reduction and propose a novel GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main impacts that cause carbon emissions in AIoT, and then introduce GAI techniques and their relations to carbon emissions. We then explore the application prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon emissions of network components. Subsequently, we propose a Large Language Model (LLM)-enabled carbon emission optimization framework, in which we design pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more accurate and reliable optimization problems. Furthermore, we utilize Generative Diffusion Models (GDMs) to identify optimal strategies for carbon emission reduction. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we insightfully provide open research directions for low-carbon AIoT.",
        "subjects": [
            "cs.NI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18101",
        "abstract url": "https://arxiv.org/abs/2404.18101",
        "title": "Advancing Supervised Learning with the Wave Loss Function: A Robust and Smooth Approach",
        "rating": "-2.5",
        "keywords": [
            [
                "SVM"
            ],
            [
                "biomedical",
                "Disease"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Loss function plays a vital role in supervised learning frameworks. The selection of the appropriate loss function holds the potential to have a substantial impact on the proficiency attained by the acquired model. The training of supervised learning algorithms inherently adheres to predetermined loss functions during the optimization process. In this paper, we present a novel contribution to the realm of supervised machine learning: an asymmetric loss function named wave loss. It exhibits robustness against outliers, insensitivity to noise, boundedness, and a crucial smoothness property. Theoretically, we establish that the proposed wave loss function manifests the essential characteristic of being classification-calibrated. Leveraging this breakthrough, we incorporate the proposed wave loss function into the least squares setting of support vector machines (SVM) and twin support vector machines (TSVM), resulting in two robust and smooth models termed Wave-SVM and Wave-TSVM, respectively. To address the optimization problem inherent in Wave-SVM, we utilize the adaptive moment estimation (Adam) algorithm. It is noteworthy that this paper marks the first instance of the Adam algorithm application to solve an SVM model. Further, we devise an iterative algorithm to solve the optimization problems of Wave-TSVM. To empirically showcase the effectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on benchmark UCI and KEEL datasets (with and without feature noise) from diverse domains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical domain, we evaluate it on the Alzheimer Disease Neuroimaging Initiative (ADNI) dataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM and Wave-TSVM in achieving superior prediction accuracy against the baseline models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18209",
        "abstract url": "https://arxiv.org/abs/2404.18209",
        "title": "4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on Relational DBs",
        "rating": "-2.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing. This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes. As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics. To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs. Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks. From a delivery standpoint, we operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer. We conclude by presenting evaluations using 4DBInfer, the results of which highlight the importance of considering each such dimension in the design of RDB predictive models, as well as the limitations of more naive approaches such as simply joining adjacent tables. Our source code is released at https://github.com/awslabs/multi-table-benchmark .",
        "subjects": [
            "cs.LG",
            "cs.DB"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2404.18955",
        "abstract url": "https://arxiv.org/abs/2404.18955",
        "title": "GARA: A novel approach to Improve Genetic Algorithms' Accuracy and Efficiency by Utilizing Relationships among Genes",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "biophysical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Genetic algorithms have played an important role in engineering optimization. Traditional GAs treat each gene separately. However, biophysical studies of gene regulatory networks revealed direct associations between different genes. It inspires us to propose an improvement to GA in this paper, Gene Regulatory Genetic Algorithm (GRGA), which, to our best knowledge, is the first time to utilize relationships among genes for improving GA's accuracy and efficiency. We design a directed multipartite graph encapsulating the solution space, called RGGR, where each node corresponds to a gene in the solution and the edge represents the relationship between adjacent nodes. The edge's weight reflects the relationship degree and is updated based on the idea that the edges' weights in a complete chain as candidate solution with acceptable or unacceptable performance should be strengthened or reduced, respectively. The obtained RGGR is then employed to determine appropriate loci of crossover and mutation operators, thereby directing the evolutionary process toward faster and better convergence. We analyze and validate our proposed GRGA approach in a single-objective multimodal optimization problem, and further test it on three types of applications, including feature selection, text summarization, and dimensionality reduction. Results illustrate that our GARA is effective and promising.",
        "subjects": [
            "cs.NE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18089",
        "abstract url": "https://arxiv.org/abs/2404.18089",
        "title": "ATR-Mapping: Asymmetric Topological Representation based Mapping Framework for Multi-Robot Environment Exploration",
        "rating": "-3",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "robotics",
                "Robot"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "In recent years, the widespread application of multi-robot systems in areas such as power inspection, autonomous vehicle fleets has made multi-robot technology a research hotspot in the field of robotics. This paper investigates multi-robot cooperative exploration in unknown environments, proposing a training framework and decision strategy based on multi-agent reinforcement learning. Specifically we propose a Asymmetric Topological Representation based mapping framework (ATR-Mapping), combining the advantages of methods based on raw grid maps and methods based on topology, the structural information from the raw grid maps is extracted and combined with a topological graph constructed based on geometric distance information for decision-making. Leveraging this topological graph representation, we employs a decision network based on topological graph matching to assign corresponding boundary points to each robot as long-term target points for decision-making. We conducts testing and application of the proposed algorithms in real world scenarios using the Gazebo and Gibson simulation environments. It validates that the proposed method, when compared to existing methods, achieves a certain degree of performance improvement.",
        "subjects": [
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18090",
        "abstract url": "https://arxiv.org/abs/2404.18090",
        "title": "A Novel Classification of Attacks on Blockchain Layers: Vulnerabilities, Attacks, Mitigations, and Research Directions",
        "rating": "-3",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "The widespread adoption of blockchain technology has amplified the spectrum of potential threats to its integrity and security. The ongoing quest to exploit vulnerabilities emphasizes how critical it is to expand on current research initiatives. Thus, using a methodology based on discrete blockchain layers, our survey study aims to broaden the existing body of knowledge by thoroughly discussing both new and known attack vectors inside the blockchain ecosystem. This survey proposes a novel classification of blockchain attacks and an in-depth investigation of blockchain data security. In particular, the paper provides a thorough discussion of the attack techniques and vulnerabilities that are specific to each tier, along with a detailed look at mitigating techniques. We reveal the deep dynamics of these security concerns by closely investigating the fundamental causes of attacks at various blockchain tiers. We clarify mitigation methods for known vulnerabilities and offer new information on recently developed attack vectors. We also discuss the implications of quantum computing in blockchain and the weaknesses in the current technology that can be exploited in the future. Our study advances the field of blockchain security and privacy research while also contributing to our understanding of blockchain vulnerabilities and attacks. This survey paper is a useful tool for readers who want to learn more about the intricacies of blockchain security. It also invites researchers to help strengthen blockchain privacy and security, paving the way for further developments in this dynamic and ever-evolving field.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18104",
        "abstract url": "https://arxiv.org/abs/2404.18104",
        "title": "The power of shallow-depth Toffoli and qudit quantum circuits",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "The relevance of shallow-depth quantum circuits has recently increased, mainly due to their applicability to near-term devices. In this context, one of the main goals of quantum circuit complexity is to find problems that can be solved by quantum shallow circuits but require more computational resources classically. Our first contribution in this work is to prove new separations between classical and quantum constant-depth circuits. Firstly, we show a separation between constant-depth quantum circuits with quantum advice $\\mathsf{QNC}^0/\\mathsf{qpoly}$, and $\\mathsf{AC}^0[p]$, which is the class of classical constant-depth circuits with unbounded-fan in and $\\pmod{p}$ gates. In addition, we show a separation between $\\mathsf{QAC}^0$, which additionally has Toffoli gates with unbounded control, and $\\mathsf{AC}^0[p]$. This establishes the first such separation for a shallow-depth quantum class that does not involve quantum fan-out gates. Secondly, we consider $\\mathsf{QNC}^0$ circuits with infinite-size gate sets. We show that these circuits, along with (classical or quantum) prime modular gates, can implement threshold gates, showing that $\\mathsf{QNC}^0[p]=\\mathsf{QTC}^0$. Finally, we also show that in the infinite-size gateset case, these quantum circuit classes for higher-dimensional Hilbert spaces do not offer any advantage to standard qubit implementations.",
        "subjects": [
            "quant-ph",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18142",
        "abstract url": "https://arxiv.org/abs/2404.18142",
        "title": "Revisiting Majumdar-Ghosh spin chain model and Max-cut problem using variational quantum algorithms",
        "rating": "-3",
        "keywords": [
            [
                "graph"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "In this work, energy levels of the Majumdar-Ghosh model (MGM) are analyzed up to 15 spins chain in the noisy intermediate-scale quantum framework using noisy simulations. This is a useful model whose exact solution is known for a particular choice of interaction coefficients. We have solved this model for interaction coefficients other than that required for the exactly solvable conditions as this solution can be of help in understanding the quantum phase transitions in complex spin chain models. The solutions are obtained using quantum approximate optimization algorithms (QAOA), and variational quantum eigensolver (VQE). To obtain the solutions, the one-dimensional lattice network is mapped to a Hamiltonian that corresponds to the required interaction coefficients among spins. Then, the ground states energy eigenvalue of this Hamiltonian is found using QAOA and VQE. Further, the validity of the Lieb-Schultz-Mattis theorem in the context of MGM is established by employing variational quantum deflation to find the first excited energy of MGM. Solution for an unweighted Max-cut graph for 17 nodes is also obtained using QAOA and VQE to know which one of these two techniques performs better in a combinatorial optimization problem. Since the variational quantum algorithms used here to revisit the Max-cut problem and MGM are hybrid algorithms, they require classical optimization. Consequently, the results obtained using different types of classical optimizers are compared to reveal that the QNSPSA optimizer improves the convergence of QAOA in comparison to the SPSA optimizer. However, VQE with EfficientSU2 ansatz using the SPSA optimizer yields the best results.",
        "subjects": [
            "quant-ph",
            "cs.DS"
        ],
        "comment": "Merits and demerits of using different variational quantum algorithms have been studied for Majumdar-Ghosh model and Max-cut problem"
    },
    {
        "paper id": "2404.18149",
        "abstract url": "https://arxiv.org/abs/2404.18149",
        "title": "Compressed Deepfake Video Detection Based on 3D Spatiotemporal Trajectories",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Deepfake"
            ],
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The misuse of deepfake technology by malicious actors poses a potential threat to nations, societies, and individuals. However, existing methods for detecting deepfakes primarily focus on uncompressed videos, such as noise characteristics, local textures, or frequency statistics. When applied to compressed videos, these methods experience a decrease in detection performance and are less suitable for real-world scenarios. In this paper, we propose a deepfake video detection method based on 3D spatiotemporal trajectories. Specifically, we utilize a robust 3D model to construct spatiotemporal motion features, integrating feature details from both 2D and 3D frames to mitigate the influence of large head rotation angles or insufficient lighting within frames. Furthermore, we separate facial expressions from head movements and design a sequential analysis method based on phase space motion trajectories to explore the feature differences between genuine and fake faces in deepfake videos. We conduct extensive experiments to validate the performance of our proposed method on several compressed deepfake benchmarks. The robustness of the well-designed features is verified by calculating the consistent distribution of facial landmarks before and after video compression.Our method yields satisfactory results and showcases its potential for practical applications.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18208",
        "abstract url": "https://arxiv.org/abs/2404.18208",
        "title": "ROS 2 on a Chip, Achieving Brain-Like Speeds and Efficiency in Robotic Networking",
        "rating": "-3",
        "keywords": [
            [
                "robotics",
                "Robot"
            ],
            [
                "FPGA"
            ]
        ],
        "abstract": "The Robot Operating System (ROS) pubsub model played a pivotal role in developing sophisticated robotic applications. However, the complexities and real-time demands of modern robotics necessitate more efficient communication solutions that are deterministic and isochronous. This article introduces a groundbreaking approach: embedding ROS 2 message-passing infrastructure directly onto a specialized hardware chip, significantly enhancing speed and efficiency in robotic communications. Our FPGA prototypes of the chip design can send or receive packages in less than 2.5 microseconds, accelerating networking communications by more than 62x on average and improving energy consumption by more than 500x when compared to traditional ROS 2 software implementations on modern CPUs. Additionally, it dramatically reduces maximum latency in ROS 2 networking communication by more than 30,000x. In situations of peak latency, our design guarantees an isochronous response within 11 microseconds, a stark improvement over the potential hundreds of milliseconds reported by modern CPU systems under similar conditions.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18225",
        "abstract url": "https://arxiv.org/abs/2404.18225",
        "title": "Quadruped robot traversing 3D complex environments with limited perception",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "lidar"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Traversing 3-D complex environments has always been a significant challenge for legged locomotion. Existing methods typically rely on external sensors such as vision and lidar to preemptively react to obstacles by acquiring environmental information. However, in scenarios like nighttime or dense forests, external sensors often fail to function properly, necessitating robots to rely on proprioceptive sensors to perceive diverse obstacles in the environment and respond promptly. This task is undeniably challenging. Our research finds that methods based on collision detection can enhance a robot's perception of environmental obstacles. In this work, we propose an end-to-end learning-based quadruped robot motion controller that relies solely on proprioceptive sensing. This controller can accurately detect, localize, and agilely respond to collisions in unknown and complex 3D environments, thereby improving the robot's traversability in complex environments. We demonstrate in both simulation and real-world experiments that our method enables quadruped robots to successfully traverse challenging obstacles in various complex environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 8 figures,submitted to iros2024"
    },
    {
        "paper id": "2404.18321",
        "abstract url": "https://arxiv.org/abs/2404.18321",
        "title": "Riemannian Optimization for Active Mapping with Robot Teams",
        "rating": "-3",
        "keywords": [
            [
                "RGB-D"
            ],
            [
                "Robot"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Autonomous exploration of unknown environments using a team of mobile robots demands distributed perception and planning strategies to enable efficient and scalable performance. Ideally, each robot should update its map and plan its motion not only relying on its own observations, but also considering the observations of its peers. Centralized solutions to multi-robot coordination are susceptible to central node failure and require a sophisticated communication infrastructure for reliable operation. Current decentralized active mapping methods consider simplistic robot models with linear-Gaussian observations and Euclidean robot states. In this work, we present a distributed multi-robot mapping and planning method, called Riemannian Optimization for Active Mapping (ROAM). We formulate an optimization problem over a graph with node variables belonging to a Riemannian manifold and a consensus constraint requiring feasible solutions to agree on the node variables. We develop a distributed Riemannian optimization algorithm that relies only on one-hop communication to solve the problem with consensus and optimality guarantees. We show that multi-robot active mapping can be achieved via two applications of our distributed Riemannian optimization over different manifolds: distributed estimation of a 3-D semantic map and distributed planning of SE(3) trajectories that minimize map uncertainty. We demonstrate the performance of ROAM in simulation and real-world experiments using a team of robots with RGB-D cameras.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18331",
        "abstract url": "https://arxiv.org/abs/2404.18331",
        "title": "Multi-Robot Object SLAM using Distributed Variational Inference",
        "rating": "-3",
        "keywords": [
            [
                "trajectory",
                "SLAM"
            ],
            [
                "Robot"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Multi-robot simultaneous localization and mapping (SLAM) enables a robot team to achieve coordinated tasks relying on a common map. However, centralized processing of robot observations is undesirable because it creates a single point of failure and requires pre-existing infrastructure and significant multi-hop communication throughput. This paper formulates multi-robot object SLAM as a variational inference problem over a communication graph. We impose a consensus constraint on the objects maintained by different nodes to ensure agreement on a common map. To solve the problem, we develop a distributed mirror descent algorithm with a regularization term enforcing consensus. Using Gaussian distributions in the algorithm, we derive a distributed multi-state constraint Kalman filter (MSCKF) for multi-robot object SLAM. Experiments on real and simulated data show that our method improves the trajectory and object estimates, compared to individual-robot SLAM, while achieving better scaling to large robot teams, compared to centralized multi-robot SLAM. Code is available at https://github.com/intrepidChw/distributed_msckf.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18386",
        "abstract url": "https://arxiv.org/abs/2404.18386",
        "title": "Network Intent Decomposition and Optimization for Energy-Aware Radio Access Network",
        "rating": "-3",
        "keywords": [
            [
                "Graph"
            ],
            [
                "6G"
            ]
        ],
        "abstract": "With recent advancements in the sixth generation (6G) communication technologies, more vertical industries have encountered diverse network services. How to reduce energy consumption is critical to meet the expectation of the quality of diverse network services. In particular, the number of base stations in 6G is huge with coupled adjustable network parameters. However, the problem is complex with multiple network objectives and parameters. Network intents are difficult to map to individual network elements and require enhanced automation capabilities. In this paper, we present a network intent decomposition and optimization mechanism in an energy-aware radio access network scenario. By characterizing the intent ontology with a standard template, we present a generic network intent representation framework. Then we propose a novel intent modeling method using Knowledge Acquisition in automated Specification language, which can model the network ontology. To clarify the number and types of network objectives and energy-saving operations, we develop a Softgoal Interdependency Graph-based network intent decomposition model, and thus, a network intent decomposition algorithm is presented. Simulation results demonstrate that the proposed algorithm outperforms without conflict analysis in intent decomposition time. Moreover, we design a deep Q-network-assisted intent optimization scheme to validate the performance gain.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18407",
        "abstract url": "https://arxiv.org/abs/2404.18407",
        "title": "ICMarks: A Robust Watermarking Framework for Integrated Circuit Physical Design IP Protection",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Watermarking"
            ]
        ],
        "abstract": "Physical design watermarking on contemporary integrated circuit (IC) layout encodes signatures without considering the dense connections and design constraints, which could lead to performance degradation on the watermarked products. This paper presents ICMarks, a quality-preserving and robust watermarking framework for modern IC physical design. ICMarks embeds unique watermark signatures during the physical design's placement stage, thereby authenticating the IC layout ownership. ICMarks's novelty lies in (i) strategically identifying a region of cells to watermark with minimal impact on the layout performance and (ii) a two-level watermarking framework for augmented robustness toward potential removal and forging attacks. Extensive evaluations on benchmarks of different design objectives and sizes validate that ICMarks incurs no wirelength and timing metrics degradation, while successfully proving ownership. Furthermore, we demonstrate ICMarks is robust against two major watermarking attack categories, namely, watermark removal and forging attacks; even if the adversaries have prior knowledge of the watermarking schemes, the signatures cannot be removed without significantly undermining the layout quality.",
        "subjects": [
            "cs.CR",
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18144",
        "abstract url": "https://arxiv.org/abs/2404.18144",
        "title": "Generative AI for Visualization: State of the Art and Future Directions",
        "rating": "-3.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "graph"
            ],
            [
                "tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design. Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations. Concurrently, recent major breakthroughs in GenAI like diffusion model and large language model have also drastically increase the potential of GenAI4VIS. From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research. Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction. For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI and generative algorithms. By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18219",
        "abstract url": "https://arxiv.org/abs/2404.18219",
        "title": "BUFF: Boosted Decision Tree based Ultra-Fast Flow matching",
        "rating": "-3.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "Tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Tabular data stands out as one of the most frequently encountered types in high energy physics. Unlike commonly homogeneous data such as pixelated images, simulating high-dimensional tabular data and accurately capturing their correlations are often quite challenging, even with the most advanced architectures. Based on the findings that tree-based models surpass the performance of deep learning models for tasks specific to tabular data, we adopt the very recent generative modeling class named conditional flow matching and employ different techniques to integrate the usage of Gradient Boosted Trees. The performances are evaluated for various tasks on different analysis level with several public datasets. We demonstrate the training and inference time of most high-level simulation tasks can achieve speedup by orders of magnitude. The application can be extended to low-level feature simulation and conditioned generations with competitive performance.",
        "subjects": [
            "physics.ins-det",
            "cs.LG",
            "hep-ex",
            "hep-ph",
            "physics.data-an"
        ],
        "comment": "9 pages, 10 figures, 1 additional figure in appendix"
    },
    {
        "paper id": "2404.18328",
        "abstract url": "https://arxiv.org/abs/2404.18328",
        "title": "Multi-stage Attack Detection and Prediction Using Graph Neural Networks: An IoT Feasibility Study",
        "rating": "-3.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Attack"
            ],
            [
                "IoT"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the ever-increasing reliance on digital networks for various aspects of modern life, ensuring their security has become a critical challenge. Intrusion Detection Systems play a crucial role in ensuring network security, actively identifying and mitigating malicious behaviours. However, the relentless advancement of cyber-threats has rendered traditional/classical approaches insufficient in addressing the sophistication and complexity of attacks. This paper proposes a novel 3-stage intrusion detection system inspired by a simplified version of the Lockheed Martin cyber kill chain to detect advanced multi-step attacks. The proposed approach consists of three models, each responsible for detecting a group of attacks with common characteristics. The detection outcome of the first two stages is used to conduct a feasibility study on the possibility of predicting attacks in the third stage. Using the ToN IoT dataset, we achieved an average of 94% F1-Score among different stages, outperforming the benchmark approaches based on Random-forest model. Finally, we comment on the feasibility of this approach to be integrated in a real-world system and propose various possible future work.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.02336",
        "abstract url": "https://arxiv.org/abs/2405.02336",
        "title": "Artificial General Intelligence (AGI)-Native Wireless Systems: A Journey Beyond 6G",
        "rating": "-3.5",
        "keywords": [
            [
                "6G"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Building future wireless systems that support services like digital twins (DTs) is challenging to achieve through advances to conventional technologies like meta-surfaces. While artificial intelligence (AI)-native networks promise to overcome some limitations of wireless technologies, developments still rely on AI tools like neural networks. Such tools struggle to cope with the non-trivial challenges of the network environment and the growing demands of emerging use cases. In this paper, we revisit the concept of AI-native wireless systems, equipping them with the common sense necessary to transform them into artificial general intelligence (AGI)-native systems. These systems acquire common sense by exploiting different cognitive abilities such as perception, analogy, and reasoning, that enable them to generalize and deal with unforeseen scenarios. Towards developing the components of such a system, we start by showing how the perception module can be built through abstracting real-world elements into generalizable representations. These representations are then used to create a world model, founded on principles of causality and hyper-dimensional (HD) computing, that aligns with intuitive physics and enables analogical reasoning, that define common sense. Then, we explain how methods such as integrated information theory play a role in the proposed intent-driven and objective-driven planning methods that maneuver the AGI-native network to take actions. Next, we discuss how an AGI-native network can enable use cases related to human and autonomous agents: a) analogical reasoning for next-generation DTs, b) synchronized and resilient experiences for cognitive avatars, and c) brain-level metaverse experiences like holographic teleportation. Finally, we conclude with a set of recommendations to build AGI-native systems. Ultimately, we envision this paper as a roadmap for the beyond 6G era.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18162",
        "abstract url": "https://arxiv.org/abs/2404.18162",
        "title": "fMRI Exploration of Visual Quality Assessment",
        "rating": "-4",
        "keywords": [
            [
                "fMRI"
            ],
            [
                "Quality Assessment"
            ]
        ],
        "abstract": "Despite significant strides in visual quality assessment, the neural mechanisms underlying visual quality perception remain insufficiently explored. This study employed fMRI to examine brain activity during image quality assessment and identify differences in human processing of images with varying quality. Fourteen healthy participants underwent tasks assessing both image quality and content classification while undergoing functional MRI scans. The collected behavioral data was statistically analyzed, and univariate and functional connectivity analyses were conducted on the imaging data. The findings revealed that quality assessment is a more complex task than content classification, involving enhanced activation in high-level cognitive brain regions for fine-grained visual analysis. Moreover, the research showed the brain's adaptability to different visual inputs, adopting different strategies depending on the input's quality. In response to high-quality images, the brain primarily uses specialized visual areas for precise analysis, whereas with low-quality images, it recruits additional resources including higher-order visual cortices and related cognitive and attentional networks to decode and recognize complex, ambiguous signals effectively. This study pioneers the intersection of neuroscience and image quality research, providing empirical evidence through fMRI linking image quality to neural processing. It contributes novel insights into the human visual system's response to diverse image qualities, thereby paving the way for advancements in objective image quality assessment algorithms.",
        "subjects": [
            "cs.MM",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18192",
        "abstract url": "https://arxiv.org/abs/2404.18192",
        "title": "Block-Map-Based Localization in Large-Scale Environment",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "SLAM"
            ],
            [
                "robot",
                "navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments. Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services. To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps. Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information. Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose. Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking. Comparison experiments are performed on publicly available large-scale datasets. Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT and M2DGR.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 4 figures, 4 tables, published to ICRA 2024"
    },
    {
        "paper id": "2404.18369",
        "abstract url": "https://arxiv.org/abs/2404.18369",
        "title": "A SAT Scalpel for Lattice Surgery: Representation and Synthesis of Subroutines for Surface-Code Fault-Tolerant Quantum Computing",
        "rating": "-4",
        "keywords": [
            [
                "Surgery"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum error correction is necessary for large-scale quantum computing. A promising quantum error correcting code is the surface code. For this code, fault-tolerant quantum computing (FTQC) can be performed via lattice surgery, i.e., splitting and merging patches of code. Given the frequent use of certain lattice-surgery subroutines (LaS), it becomes crucial to optimize their design in order to minimize the overall spacetime volume of FTQC. In this study, we define the variables to represent LaS and the constraints on these variables. Leveraging this formulation, we develop a synthesizer for LaS, LaSsynth, that encodes a LaS construction problem into a SAT instance, subsequently querying SAT solvers for a solution. Starting from a baseline design, we can gradually invoke the solver with shrinking spacetime volume to derive more compact designs. Due to our foundational formulation and the use of SAT solvers, LaSsynth can exhaustively explore the design space, yielding optimal designs in volume. For example, it achieves 8% and 18% volume reduction respectively over two states-of-the-art human designs for the 15-to-1 T-factory, a bottleneck in FTQC.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": "To appear in 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)"
    },
    {
        "paper id": "2404.18373",
        "abstract url": "https://arxiv.org/abs/2404.18373",
        "title": "6G comprehensive intelligence: network operations and optimization based on Large Language Models",
        "rating": "-4",
        "keywords": [
            [
                "health"
            ],
            [
                "6G",
                "Industrial",
                "IoT"
            ]
        ],
        "abstract": "The sixth generation mobile communication standard (6G) can promote the development of Industrial Internet and Internet of Things (IoT). To achieve comprehensive intelligent development of the network and provide customers with higher quality personalized services. This paper proposes a network performance optimization and intelligent operation network architecture based on Large Language Model (LLM), aiming to build a comprehensive intelligent 6G network system. The Large Language Model, with more parameters and stronger learning ability, can more accurately capture patterns and features in data, which can achieve more accurate content output and high intelligence and provide strong support for related research such as network data security, privacy protection, and health assessment. This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application value, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "8 pages, 5 figures, 15 preferences"
    },
    {
        "paper id": "2404.18394",
        "abstract url": "https://arxiv.org/abs/2404.18394",
        "title": "Reconstructing Satellites in 3D from Amateur Telescope Images",
        "rating": "-4",
        "keywords": [
            [
                "3D",
                "Gaussian splatting"
            ],
            [
                "satellite"
            ],
            [
                "image restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper proposes a framework for the 3D reconstruction of satellites in low-Earth orbit, utilizing videos captured by small amateur telescopes. The video data obtained from these telescopes differ significantly from data for standard 3D reconstruction tasks, characterized by intense motion blur, atmospheric turbulence, pervasive background light pollution, extended focal length and constrained observational perspectives. To address these challenges, our approach begins with a comprehensive pre-processing workflow that encompasses deep learning-based image restoration, feature point extraction and camera pose initialization. We proceed with the application of an improved 3D Gaussian splatting algorithm for reconstructing the 3D model. Our technique supports simultaneous 3D Gaussian training and pose estimation, enabling the robust generation of intricate 3D point clouds from sparse, noisy data. The procedure is further bolstered by a post-editing phase designed to eliminate noise points inconsistent with our prior knowledge of a satellite's geometric constraints. We validate our approach using both synthetic datasets and actual observations of China's Space Station, showcasing its significant advantages over existing methods in reconstructing 3D space objects from ground-based observations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18395",
        "abstract url": "https://arxiv.org/abs/2404.18395",
        "title": "Mesh-based Photorealistic and Real-time 3D Mapping for Robust Visual Perception of Autonomous Underwater Vehicle",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "Vehicle",
                "SLAM"
            ],
            [
                "image enhancement"
            ]
        ],
        "abstract": "This paper proposes a photorealistic real-time dense 3D mapping system that utilizes a learning-based image enhancement method and mesh-based map representation. Due to the characteristics of the underwater environment, where problems such as hazing and low contrast occur, it is hard to apply conventional simultaneous localization and mapping (SLAM) methods. Furthermore, for sensitive tasks like inspecting cracks, photorealistic mapping is very important. However, the behavior of Autonomous Underwater Vehicle (AUV) is computationally constrained. In this paper, we utilize a neural network-based image enhancement method to improve pose estimation and mapping quality and apply a sliding window-based mesh expansion method to enable lightweight, fast, and photorealistic mapping. To validate our results, we utilize real-world and indoor synthetic datasets. We performed qualitative validation with the real-world dataset and quantitative validation by modeling images from the indoor synthetic dataset as underwater scenes.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 15 figures, IEEE ICRA Workshop on Field Robotics 2024"
    },
    {
        "paper id": "2404.18082",
        "abstract url": "https://arxiv.org/abs/2404.18082",
        "title": "Cyber Security in Containerization Platforms: A Comparative Study of Security Challenges, Measures and Best Practices",
        "rating": "-10",
        "keywords": [],
        "abstract": "The paper reviews the comparative study of security measures, challenges, and best practices with a view to enhancing cyber safety in containerized platforms. This review is intended to give insight into the enhanced security posture of containerized environments, with a view to examining safety vulnerabilities in containerization platforms, exploring strategies for increasing containers isolation and assessing how encryption techniques play an important role in providing secure applications. The paper also provides practical guidance for organizations seeking to strengthen their cyber security defenses in the containerization area platforms.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18088",
        "abstract url": "https://arxiv.org/abs/2404.18088",
        "title": "On completely regular self-dual codes with covering radius $\u03c1=2$",
        "rating": "-10",
        "keywords": [],
        "abstract": "We give a complete classification of completely regular self-dual codes with covering radius $\u03c1= 2$. More precisely, we prove that there are two sporadic such codes, of length $8$, and an infinite family, of length $4$. We provide a description of all such codes and give the intersection arrays for all of them.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18098",
        "abstract url": "https://arxiv.org/abs/2404.18098",
        "title": "Parameterized Dynamic Logic -- Towards A Cyclic Logical Framework for Program Verification via Operational Semantics",
        "rating": "-10",
        "keywords": [],
        "abstract": "Dynamic logic and its variations, because of their good expressive forms capturing program specifications clearly by isolating programs from logical formulas, have been used as a formalism in program reasoning for decades and have many applications in different areas. The program models of traditional dynamic logics are in explicit forms. With a clearly-defined syntactic structure, compositional verification is made possible, in which a deduction step transfers proving a program into proving its sub-programs. This structure-based reasoning forms the basis of many dynamic logics and popular Hoare-style logics. However, structural rules induce a major drawback that for different target programs, different rules have to be proposed to adapt different program structures. Moreover, there exist programs that does not support (or not entirely support) a structure-based reasoning. In this paper, we propose a parameterized `dynamic-logic-like' logic called DLp with general forms of program models and formulas, and propose a cyclic proof system for this logic. Program reasoning in DLp is directly based on symbolic executions of programs according to their operational semantics. This reduces the burden of designing a large set of rules when specializing a logic theory to a specific domain, and facilitates verifying programs without a suitable structure for direct reasoning. Without reasoning by dissolving program structures, DLp can cause an infinite proof structure. To solve this, we build a cyclic preproof structure for the proof system of DLp and prove its soundness. Case studies are analyzed to show how DLp works for reasoning about different types of programs.",
        "subjects": [
            "cs.LO",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18117",
        "abstract url": "https://arxiv.org/abs/2404.18117",
        "title": "A Basis-preserving Algorithm for Computing the Bezout Matrix of Newton Polynomials",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper tackles the problem of constructing Bezout matrices for Newton polynomials in a basis-preserving approach that operates directly with the given Newton basis, thus avoiding the need for transformation from Newton basis to monomial basis. This approach significantly reduces the computational cost and also mitigates numerical instability caused by basis transformation. For this purpose, we investigate the internal structure of Bezout matrices in Newton basis and design a basis-preserving algorithm that generates the Bezout matrix in the specified basis used to formulate the input polynomials. Furthermore, we show an application of the proposed algorithm on constructing confederate resultant matrices for Newton polynomials. Experimental results demonstrate that the proposed methods perform superior to the basis-transformation-based ones.",
        "subjects": [
            "cs.SC",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18118",
        "abstract url": "https://arxiv.org/abs/2404.18118",
        "title": "Finite-time Safety and Reach-avoid Verification of Stochastic Discrete-time Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper studies finite-time safety and reach-avoid verification for stochastic discrete-time dynamical systems. The aim is to ascertain lower and upper bounds of the probability that, within a predefined finite-time horizon, a system starting from an initial state in a safe set will either exit the safe set (safety verification) or reach a target set while remaining within the safe set until the first encounter with the target (reach-avoid verification). We introduce novel barrier-like sufficient conditions for characterizing these bounds, which either complement existing ones or fill gaps. Finally, we demonstrate the efficacy of these conditions on two examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18125",
        "abstract url": "https://arxiv.org/abs/2404.18125",
        "title": "A Large-Scale Empirical Study of COVID-19 Contact Tracing Mobile App Reviews",
        "rating": "-10",
        "keywords": [],
        "abstract": "Since the beginning of 2020, the novel coronavirus has begun to sweep across the globe. Given the prevalence of smartphones everywhere, many countries across continents also developed COVID-19 contract tracing apps that users can install to get a warning of potential contacts with infected people. Unlike regular apps that undergo detailed requirement analysis, carefully designed development, rigorous testing, contact tracing apps were deployed after rapid development. Therefore such apps may not reach expectations for all end users. Users share their opinions and experience of the usage of the apps in the app store. This paper aims to understand the types of topics users discuss in the reviews of the COVID-19 contact tracing apps across the continents by analyzing the app reviews. We collected all the reviews of 35 COVID-19 contact tracing apps developed by 34 countries across the globe. We group the app reviews into the following geographical regions: Asia, Europe, North America, Latin America, Africa, Middle East, and Australasia (Australia and NZ). We run topic modeling on the app reviews of each region. We analyze the produced topics and their evolution over time by categorizing them into hierarchies and computing the ratings of reviews related to the topics. While privacy could be a concern with such apps, we only find privacy-related topics in Australasia, North America, and Middle East. Topics related to usability and performance of the apps are prevalent across all regions. Users frequently complained about the lack of features, user interface and the negative impact of such apps on their mobile batteries. Still, we also find that many users praised the apps because they helped them stay aware of the potential danger of getting infected. The finding of this study is expected to help app developers utilize their resources to address the reported issues in a prioritized way.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18132",
        "abstract url": "https://arxiv.org/abs/2404.18132",
        "title": "Allocating Mixed Goods with Customized Fairness and Indivisibility Ratio",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the problem of fairly allocating a combination of divisible and indivisible goods. While fairness criteria like envy-freeness (EF) and proportionality (PROP) can always be achieved for divisible goods, only their relaxed versions, such as the ''up to one'' relaxations EF1 and PROP1, can be satisfied when the goods are indivisible. The ''up to one'' relaxations require the fairness conditions to be satisfied provided that one good can be completely eliminated or added in the comparison. In this work, we bridge the gap between the two extremes and propose ''up to a fraction'' relaxations for the allocation of mixed divisible and indivisible goods. The fraction is determined based on the proportion of indivisible goods, which we call the indivisibility ratio. The new concepts also introduce asymmetric conditions that are customized for individuals with varying indivisibility ratios. We provide both upper and lower bounds on the fractions of the modified item in order to satisfy the fairness criterion. Our results are tight up to a constant for EF and asymptotically tight for PROP.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "Appears in the 33rd International Joint Conference on Artificial Intelligence (IJCAI), 2024"
    },
    {
        "paper id": "2404.18133",
        "abstract url": "https://arxiv.org/abs/2404.18133",
        "title": "Fair Division of Indivisible Goods with Comparison-Based Queries",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the problem of fairly allocating $m$ indivisible goods to $n$ agents, where agents may have different preferences over the goods. In the traditional setting, agents' valuations are provided as inputs to the algorithm. In this paper, we study a new comparison-based query model where the algorithm presents two bundles of goods to an agent and the agent responds by telling the algorithm which bundle she prefers. We investigate the query complexity for computing allocations with several fairness notions including proportionality up to one good (PROP1), envy-freeness up to one good (EF1), and maximin share (MMS). Our main result is an algorithm that computes an allocation satisfying both PROP1 and $\\frac12$-MMS within $O(\\log m)$ queries with a constant number of $n$ agents. For identical and additive valuation, we present an algorithm for computing an EF1 allocation within $O(\\log m)$ queries with a constant number of $n$ agents. To complement the positive results, we show that the lower bound of the query complexity for any of the three fairness notions is $\u03a9(\\log m)$ even with two agents.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18176",
        "abstract url": "https://arxiv.org/abs/2404.18176",
        "title": "Auto-Optimized Maximum Torque Per Ampere Control of IPMSM Using Dual Control for Exploration and Exploitation",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, a maximum torque per ampere (MTPA) control strategy for the interior permanent magnet synchronous motor (IPMSM) using dual control for exploration and exploitation (DCEE). In the proposed method, the permanent magnet flux and the difference between the $d$- and $q$-axis inductance are identified by multiple estimators using the recursive least square method. The initial values of the estimated parameters in different estimators are different. By using multiple estimators, exploration of the operational environment to reduce knowledge uncertainty can be realized. Compared to those MTPA control strategies based on the extremum-seeking method, the proposed method has better dynamic performance when speed or load varies. The effectiveness of the proposed method is verified by simulations.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18181",
        "abstract url": "https://arxiv.org/abs/2404.18181",
        "title": "Learning to Move Objects with Fluid Streams in a Differentiable Simulation",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce a method for manipulating objects in three-dimensional space using controlled fluid streams. To achieve this, we train a neural network controller in a differentiable simulation and evaluate it in a simulated environment consisting of an 8x8 grid of vertical emitters. By carrying out various horizontal displacement tasks such as moving objects to specific positions while reacting to external perturbations, we demonstrate that a controller, trained with a limited number of iterations, can generalise to longer episodes and learn the complex dynamics of fluid-solid interactions. Importantly, our approach requires only the observation of the manipulated object's state, paving the way for the development of physical systems that enable contactless manipulation of objects using air streams.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures"
    },
    {
        "paper id": "2404.18187",
        "abstract url": "https://arxiv.org/abs/2404.18187",
        "title": "Joint Spectrum Partitioning and Power Allocation for Energy Efficient Semi-Integrated Sensing and Communications",
        "rating": "-10",
        "keywords": [],
        "abstract": "With spectrum resources becoming congested and the emergence of sensing-enabled wireless applications, conventional resource allocation methods need a revamp to support communications-only, sensing-only, and integrated sensing and communication (ISaC) services together. In this letter, we propose two joint spectrum partitioning (SP) and power allocation (PA) schemes to maximize the aggregate sensing and communication performance as well as corresponding energy efficiency (EE) of a semi-ISaC system that supports all three services in a unified manner. The proposed framework captures the priority of the distinct services, impact of target clutters, power budget and bandwidth constraints, and sensing and communication quality-of-service (QoS) requirements. We reveal that the former problem is jointly convex and the latter is a non-convex problem that can be solved optimally by exploiting fractional and parametric programming techniques. Numerical results verify the effectiveness of proposed schemes and extract novel insights related to the impact of the priority and QoS requirements of distinct services on the performance of semi-ISaC networks.",
        "subjects": [
            "cs.IT",
            "cs.NI"
        ],
        "comment": "Accepted in IEEE Communications Letters"
    },
    {
        "paper id": "2404.18196",
        "abstract url": "https://arxiv.org/abs/2404.18196",
        "title": "Towards Intent-based User Interfaces: Charting the Design Space of Intent-AI Interactions Across Task Types",
        "rating": "-10",
        "keywords": [],
        "abstract": "Technological advances continue to redefine the dynamics of human-machine interactions, particularly in task execution. This proposal responds to the advancements in Generative AI by outlining a research plan that probes intent-AI interaction across a diverse set of tasks: fixed-scope content curation task, atomic creative tasks, and complex and interdependent tasks. This exploration aims to inform and contribute to the development of Intent-based User Interface (IUI). The study is structured in three phases: examining fixed-scope tasks through news headline generation, exploring atomic creative tasks via analogy generation, and delving into complex tasks through exploratory visual data analysis. Future work will focus on improving IUIs to better provide suggestions to encourage experienced users to express broad and exploratory intents, and detailed and structured guidance for novice users to iterate on analysis intents for high quality outputs.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18205",
        "abstract url": "https://arxiv.org/abs/2404.18205",
        "title": "LEGO-like Small-Model Constructions for \u00c5qvist's Logics",
        "rating": "-10",
        "keywords": [],
        "abstract": "\u00c5qvist's logics (E, F, F+(CM), and G) are among the best-known systems in the long tradition of preference-based approaches for modeling conditional obligation. While the general semantics of preference models align well with philosophical intuitions, more constructive characterizations are needed to assess computational complexity and facilitate automated deduction. Existing small model constructions from conditional logics (due to Friedman and Halpern) are applicable only to F+(CM) and G, while recently developed proof-theoretic characterizations leave unresolved the exact complexity of theoremhood in logic F. In this paper, we introduce alternative small model constructions, obtained uniformly for all four \u00c5qvist's logics. Our constructions propose alternative semantical characterizations and imply co-NP-completeness of theoremhood. Furthermore, they can be naturally encoded in classical propositional logic for automated deduction.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18210",
        "abstract url": "https://arxiv.org/abs/2404.18210",
        "title": "Distributed Dissipativity-Based Controller and Topology Co-Design for DC Microgrids",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a new distributed control approach for the voltage regulation problem in DC microgrids (MGs) comprised of interconnected distributed generators (DGs), distribution lines, and loads. First, we describe the closed-loop DC MG with a distributed controller as a networked system in which the DGs and lines are two subsystems interconnected via a static interconnection matrix. This matrix demonstrates the distributed controller gains as well as the communication topology of DC MG. To design the distributed controller gains, we use the dissipative properties of the subsystems and formulate a global linear matrix inequality (LMI) problem. To support the feasibility of global control design, we propose a local controller for each DG and line subsystem by formulating a local LMI problem. In contrast to existing controllers that segregate communication topology and controller gains, our proposed controller simultaneously designs both communication topology and distributed controllers. The proposed controller is also compositional, meaning that when a new subsystem is added to or removed from networked systems, the controller for the new subsystem is designed solely based on the dynamics of the new subsystem and the dissipativity information of its coupled subsystems. This ensures that the overall system remains stable during plug-and-play (PnP) operation.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18249",
        "abstract url": "https://arxiv.org/abs/2404.18249",
        "title": "Tenspiler: A Verified Lifting-Based Compiler for Tensor Operations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Tensor processing infrastructures such as deep learning frameworks and specialized hardware accelerators have revolutionized how computationally intensive code from domains such as deep learning and image processing is executed and optimized. These infrastructures provide powerful and expressive abstractions while ensuring high performance. However, to utilize them, code must be written specifically using the APIs / ISAs of such software frameworks or hardware accelerators. Importantly, given the fast pace of innovation in these domains, code written today quickly becomes legacy as new frameworks and accelerators are developed, and migrating such legacy code manually is a considerable effort. To enable developers in leveraging such DSLs while preserving their current programming paradigm, we introduce Tenspiler, a verified lifting-based compiler that uses program synthesis to translate sequential programs written in general-purpose programming languages (e.g., C++ or Python code) into tensor operations. Central to Tenspiler is our carefully crafted yet simple intermediate language, named TensIR, that expresses tensor operations. TensIR enables efficient lifting, verification, and code generation. Currently, Tenspiler already supports \\textbf{six} DSLs, spanning a broad spectrum of software and hardware environments. Furthermore, we show that new backends can be easily supported by Tenspiler by adding simple pattern-matching rules for TensIR. Using 10 real-world code benchmark suites, our experimental evaluation shows that by translating code to be executed on \\textbf{6} different software frameworks and hardware devices, Tenspiler offers on average 105$\\times$ kernel and 9.65$\\times$ end-to-end execution time improvement over the fully-optimized sequential implementation of the same benchmarks.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18254",
        "abstract url": "https://arxiv.org/abs/2404.18254",
        "title": "Robust Resource Sharing in Network Slicing via Hypothesis Testing",
        "rating": "-10",
        "keywords": [],
        "abstract": "In network slicing, the network operator needs to satisfy the service level agreements of multiple slices at the same time and on the same physical infrastructure. To do so with reduced provisioned resources, the operator may consider resource sharing mechanisms. However, each slice then becomes susceptible to traffic surges in other slices which degrades performance isolation. To maintain both high efficiency and high isolation, we propose the introduction of hypothesis testing in resource sharing. Our approach comprises two phases. In the trial phase, the operator obtains a stochastic model for each slice that describes its normal behavior, provisions resources and then signs the service level agreements. In the regular phase, whenever there is resource contention, hypothesis testing is conducted to check which slices follow their normal behavior. Slices that fail the test are excluded from resource sharing to protect the well-behaved ones. We test our approach on a mobile traffic dataset. Results show that our approach fortifies the service level agreements against unexpected traffic patterns and achieves high efficiency via resource sharing. Overall, our approach provides an appealing tradeoff between efficiency and isolation.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18280",
        "abstract url": "https://arxiv.org/abs/2404.18280",
        "title": "Tracy, Traces, and Transducers: Computable Counterexamples and Explanations for HyperLTL Model-Checking",
        "rating": "-10",
        "keywords": [],
        "abstract": "HyperLTL model-checking enables the automated verification of information-flow properties for security-critical systems. However, it only provides a binary answer. Here, we introduce two paradigms to compute counterexamples and explanations for HyperLTL model-checking, thereby considerably increasing its usefulness. Both paradigms are based on the maxim ``counterexamples/explanations are Skolem functions for the existentially quantified trace variables''. Our first paradigm is complete (everything can be explained), but restricted to ultimately periodic system traces. The second paradigm works with (Turing machine) computable Skolem functions and is therefore much more general, but also shown incomplete (not everything can computably be explained). Finally, we prove that it is decidable whether a given finite transition system and a formula have computable Skolem functions witnessing that the system satisfies the formula. Our algorithm also computes transducers implementing computable Skolem functions, if they exist.",
        "subjects": [
            "cs.LO",
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18282",
        "abstract url": "https://arxiv.org/abs/2404.18282",
        "title": "Monitoring Real-Time Systems under Parametric Delay",
        "rating": "-10",
        "keywords": [],
        "abstract": "Online monitoring of embedded real-time systems can be achieved by reduction of an adequate property language, like Metric Interval Temporal Logic, to timed automata and symbolic execution of the resulting automata on the trace observed from the system. This direct construction however only is faithful if observation of the trace is immediate in the sense that the monitor can assign exact time stamps to the actions it observes, which is rarely true in practice due to the substantial and fluctuating parametric delays introduced by the circuitry connecting the observed system to its monitoring device. We present a purely zone-based online monitoring procedure and its implementation which handle such parametric delays exactly without recurrence to costly verification procedures for parametric timed automata.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18301",
        "abstract url": "https://arxiv.org/abs/2404.18301",
        "title": "Finding Understanding and Support: Navigating Online Communities to Share and Connect at the intersection of Abuse and Foster Care Experiences",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many children in foster care experience trauma that is rooted in unstable family relationships. Other members of the foster care system like foster parents and social workers face secondary trauma. Drawing on 10 years of Reddit data, we used a mixed methods approach to analyze how different members of the foster care system find support and similar experiences at the intersection of two Reddit communities - foster care, and abuse. Users who cross this boundary focus on trauma experiences specific to different roles in foster care. While representing a small number of users, boundary crossing users contribute heavily to both communities, and, compared to matching users, receive higher scores and more replies. We explore the roles boundary crossing users have both in the online community and in the context of foster care. Finally, we present design recommendations that would support trauma survivors find communities more suited to their personal experiences.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18310",
        "abstract url": "https://arxiv.org/abs/2404.18310",
        "title": "Multiport Network Modeling for Reconfigurable Intelligent Surfaces: Numerical Validation with a Full-Wave PEEC Simulator",
        "rating": "-10",
        "keywords": [],
        "abstract": "Reconfigurable Intelligent Surface (RIS) modeling and optimization are a crucial steps in developing the next generation of wireless communications. To this aim, the availability of accurate electromagnetic (EM) models is of paramount important for the design of RIS-assisted communication links. In this work, we validate a widely-used analytical multiport network for RISs by means of a well-established full-wave numerical method based on the Partial Elements Equivalent Circuit (PEEC) approach. Numerical results show good agreement between the two methods, thus demonstrating i) the considered multiport network model being effective and ii) the PEEC method being appropriate for EM modeling of RIS-assisted wireless links.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18313",
        "abstract url": "https://arxiv.org/abs/2404.18313",
        "title": "Multi-Link Operation and Wireless Digital Twin to Support Enhanced Roaming in Next-Gen Wi-Fi",
        "rating": "-10",
        "keywords": [],
        "abstract": "The next generation of Wi-Fi is meant to achieve ultra-high reliability for wireless communication. Several approaches are available to this extent, some of which are being considered for inclusion in standards specifications, including coordination of access points to reduce interference. In this paper, we propose a centralized architecture based on digital twins, called WiTwin, with the aim of supporting wireless stations in selecting the optimal association according to a set of parameters. Unlike prior works, we assume that Wi-Fi 7 features like multi-link operation (MLO) are available. Moreover, one of the main goals of this architecture is to preserve communication quality in the presence of mobility, by helping stations to perform reassociation at the right time and in the best way.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "preprint, 4 pages"
    },
    {
        "paper id": "2404.18315",
        "abstract url": "https://arxiv.org/abs/2404.18315",
        "title": "Design and Optimization of Reconfigurable Intelligent Surfaces Using the PEEC Method",
        "rating": "-10",
        "keywords": [],
        "abstract": "The design and optimization of Reconfigurable Intelligent Surfaces (RISs) are key challenges for future wireless communication systems. RISs are devices that can manipulate electromagnetic (EM) waves in a programmable way, thus enhancing the performance and efficiency of wireless links. To achieve this goal, it is essential to have reliable EM models that can capture the behavior of RISs in different scenarios. This work demonstrates that the Partial Elements Equivalent Circuit (PEEC) method is a powerful tool for EM analysis of RIS-aided wireless links. It might also be integrated with optimization algorithms in order to optimize wireless communication networks.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18322",
        "abstract url": "https://arxiv.org/abs/2404.18322",
        "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "The growing demand for Large Language Models (LLMs) across diverse applications has prompted a paradigm shift in the design of deep learning serving systems. Deploying LLMs, especially in multi-tenant environments, presents considerable challenges due to their high computational and memory demands. We present BlockLLM, a serving system that exploits the potential of sharing components among fine-tuned LLM models to offer an efficient and flexible solution for LLM workloads. BlockLLM partitions the models into finer-grained blocks to enable the reuse of model components and independent provisioning to improve the computation efficiency. BlockLLM consists of an offline block zoo, for storing the blocks, and an online system to serve the requests through chains of blocks. It offers multi-fold flexibility: (1) Adaptive assembly of block chains on-the-fly is achieved with the help of equivalence evaluation among blocks in the zoo. (2) We enable per-block batch size and configure best-effort KV cache coordination at individual block level. (3) We adopt speculative execution and locality-aware block placement to mitigate the communication costs from dynamic block resource allocation. Our evaluation demonstrates that BlockLLM reduces memory and storage footprints and improves computation efficiency, outperforming existing serving approach in 95\\%ile latency and GPU utilization by 33.5\\% and 20.1\\%, respectively.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18344",
        "abstract url": "https://arxiv.org/abs/2404.18344",
        "title": "Some Computational Results on Koszul-Vinberg Cochain Complexes",
        "rating": "-10",
        "keywords": [],
        "abstract": "An affine connection is said to be flat if its curvature tensor vanishes identically. Koszul-Vinberg (KV for abbreviation) cohomology has been invoked to study the deformation theory of flat and torsion-free affine connections on tangent bundle. In this Note, we compute explicitly the differentials of various specific KV cochains, and study their relation to classical objects in information geometry, including deformations associated with projective and dual-projective transformations of a flat and torsion-free affine connection. As an application, we also give a simple yet non-trivial example of a KV algebra of which second cohomology group does not vanish.",
        "subjects": [
            "math.DG",
            "cs.IT"
        ],
        "comment": "9 pages, 0 figue"
    },
    {
        "paper id": "2404.18350",
        "abstract url": "https://arxiv.org/abs/2404.18350",
        "title": "L-DIT: A dApp for Live Detectability, Identifiability and Trackability for ASOs on the Behavioral Dynamics Blockchain",
        "rating": "-10",
        "keywords": [],
        "abstract": "As the number of Anthropogenic Space Objects (ASOs) grows, there is an urgent need to ensure space safety, security, and sustainability (S3) for long-term space use. Currently, no globally effective method can quantify the safety, security, and Sustainability of all ASOs in orbit. Existing methods such as the Space Sustainability Rating (SSR) rely on volunteering private information to provide sustainability ratings. However, the need for such sensitive data might prove to be a barrier to adoption for space entities. For effective comparison of ASOs, the rating mechanism should apply to all ASOs, even retroactively, so that the sustainability of a single ASO can be assessed holistically. Lastly, geopolitical boundaries and alignments play a crucial and limiting role in a volunteered rating system, limiting the space safety, security, and sustainability. This work presents a Live Detectability, Identifiability, and Trackability (L-DIT) score through a distributed app (dApp) built on top of the Behavioral Dynamics blockchain (BDB). The BDB chain is a space situational awareness (SSA) chain that provides verified and cross-checked ASO data from multiple sources. This unique combination of consensus-based information from BDB and permissionless access to data allows the DIT scoring method presented here to be applied to all ASOs. While the underlying BDB chain collects, filters, and validates SSA data from various open (and closed if available) sources, the L-DIT dApp consumes the data from the chain to provide L-DIT score that can contribute towards an operator's, manufacturer's, or owner's sustainability practices. Our dApp provides data for all ASOs, allowing their sustainability score to be compared against other ASOs, regardless of geopolitical alignments, providing business value to entities such as space insurance providers and enabling compliance validation and enforcement.",
        "subjects": [
            "cs.CR",
            "astro-ph.IM"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18361",
        "abstract url": "https://arxiv.org/abs/2404.18361",
        "title": "Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design",
        "rating": "-10",
        "keywords": [],
        "abstract": "NVIDIA's Multi-Instance GPU (MIG) technology enables partitioning GPU computing power and memory into separate hardware instances, providing complete isolation including compute resources, caches, and memory. However, prior work identifies that MIG does not extend to partitioning the last-level TLB (i.e., L3 TLB), which remains shared among all instances. To enhance TLB reach, NVIDIA GPUs reorganized the TLB structure with 16 sub-entries in each L3 TLB entry that have a one-to-one mapping to the address translations for 16 pages of size 64KB located within the same 1MB aligned range. Our comprehensive investigation of address translation efficiency in MIG identifies two main issues caused by L3 TLB sharing interference: (i) it results in performance degradation for co-running applications, and (ii) TLB sub-entries are not fully utilized before eviction. Based on this observation, we propose STAR to improve the utilization of TLB sub-entries through dynamic sharing of TLB entries across multiple base addresses. STAR evaluates TLB entries based on their sub-entry utilization to optimize address translation storage, dynamically adjusting between a shared and non-shared status to cater to current demand. We show that STAR improves overall performance by an average of 30.2% across various multi-tenant workloads.",
        "subjects": [
            "cs.DC",
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18365",
        "abstract url": "https://arxiv.org/abs/2404.18365",
        "title": "\"What Keeps People Secure is That They Met The Security Team\": Deconstructing Drivers And Goals of Organizational Security Awareness",
        "rating": "-10",
        "keywords": [],
        "abstract": "Security awareness campaigns in organizations now collectively cost billions of dollars annually. There is increasing focus on ensuring certain security behaviors among employees. On the surface, this would imply a user-centered view of security in organizations. Despite this, the basis of what security awareness managers do and what decides this are unclear. We conducted n=15 semi-structured interviews with full-time security awareness managers, with experience across various national and international companies in European countries, with thousands of employees. Through thematic analysis, we identify that success in awareness management is fragile while having the potential to improve; there are a range of restrictions, and mismatched drivers and goals for security awareness, affecting how it is structured, delivered, measured, and improved. We find that security awareness as a practice is underspecified, and split between messaging around secure behaviors and connecting to employees, with a lack of recognition for the measures that awareness managers regard as important. We discuss ways forward, including alternative indicators of success, and security usability advocacy for employees.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18392",
        "abstract url": "https://arxiv.org/abs/2404.18392",
        "title": "Dflow, a Python framework for constructing cloud-native AI-for-Science workflows",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the AI-for-science era, scientific computing scenarios such as concurrent learning and high-throughput computing demand a new generation of infrastructure that supports scalable computing resources and automated workflow management on both cloud and high-performance supercomputers. Here we introduce Dflow, an open-source Python toolkit designed for scientists to construct workflows with simple programming interfaces. It enables complex process control and task scheduling across a distributed, heterogeneous infrastructure, leveraging containers and Kubernetes for flexibility. Dflow is highly observable and can scale to thousands of concurrent nodes per workflow, enhancing the efficiency of complex scientific computing tasks. The basic unit in Dflow, known as an Operation (OP), is reusable and independent of the underlying infrastructure or context. Dozens of workflow projects have been developed based on Dflow, spanning a wide range of projects. We anticipate that the reusability of Dflow and its components will encourage more scientists to publish their workflows and OP components. These components, in turn, can be adapted and reused in various contexts, fostering greater collaboration and innovation in the scientific community.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18405",
        "abstract url": "https://arxiv.org/abs/2404.18405",
        "title": "Understanding and Shaping Human-Technology Assemblages in the Age of Generative AI",
        "rating": "-10",
        "keywords": [],
        "abstract": "Generative AI capabilities are rapidly transforming how we perceive, interact with, and relate to machines. This one-day workshop invites HCI researchers, designers, and practitioners to imaginatively inhabit and explore the possible futures that might emerge from humans combining generative AI capabilities into everyday technologies at massive scale. Workshop participants will craft stories, visualisations, and prototypes through scenario-based design to investigate these possible futures, resulting in the production of an open-annotated scenario library and a journal or interactions article to disseminate the findings. We aim to gather the DIS community knowledge to explore, understand and shape the relations this new interaction paradigm is forging between humans, their technologies and the environment in safe, sustainable, enriching, and responsible ways.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2404.18406",
        "abstract url": "https://arxiv.org/abs/2404.18406",
        "title": "Movable Antenna-Enhanced Wireless Powered Mobile Edge Computing Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we propose a movable antenna (MA) enhanced scheme for wireless powered mobile edge computing (WP-MEC) system, where the hybrid access point (HAP) equipped with multiple MAs first emits wireless energy to charge wireless devices (WDs), and then receives the offloaded tasks from the WDs for edge computing. The MAs deployed at the HAP enhance the spatial degrees of freedom (DoFs) by flexibly adjusting the positions of MAs within an available region, thereby improving the efficiency of both downlink wireless energy transfer (WPT) and uplink task offloading. To balance the performance enhancement against the implementation intricacy, we further propose three types of MA positioning configurations, i.e., dynamic MA positioning, semi-dynamic MA positioning, and static MA positioning. In addition, the non-linear power conversion of energy harvesting (EH) circuits at the WDs and the finite computing capability at the edge server are taken into account. Our objective is to maximize the sum computational rate (SCR) by jointly optimizing the time allocation, positions of MAs, energy beamforming matrix, receive combing vectors, and offloading strategies of WDs. To solve the non-convex problems, efficient alternating optimization (AO) frameworks are proposed. Moreover, we propose a hybrid algorithm of particle swarm optimization with variable local search (PSO-VLS) to solve the sub-problem of MA positioning. Numerical results validate the superiority of exploiting MAs over the fixed-position antennas (FPAs) for enhancing the SCR performance of WP-MEC systems.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "13 pages, 10 figures. Submitted for possible publication"
    },
    {
        "paper id": "2404.18417",
        "abstract url": "https://arxiv.org/abs/2404.18417",
        "title": "Domain Reasoning in TopKAT",
        "rating": "-10",
        "keywords": [],
        "abstract": "TopKAT is the algebraic theory of Kleene algebra with tests (KAT) extended with a top element. Compared to KAT, one pleasant feature of TopKAT is that, in relational models, the top element allows us to express the domain and codomain of a relation. This enables several applications in program logics, such as proving under-approximate specifications or reachability properties of imperative programs. However, while TopKAT inherits many pleasant features of KATs, such as having a decidable equational theory, it is incomplete with respect to relational models. In other words, there are properties that hold true of all relational TopKATs but cannot be proved with the axioms of TopKAT. This issue is potentially worrisome for program-logic applications, in which relational models play a key role. In this paper, we further investigate the completeness properties of TopKAT with respect to relational models. We show that TopKAT is complete with respect to (co)domain comparison of KAT terms, but incomplete when comparing the (co)domain of arbitrary TopKAT terms. Since the encoding of under-approximate specifications in TopKAT hinges on this type of formula, the aforementioned incompleteness results have a limited impact when using TopKAT to reason about such specifications.",
        "subjects": [
            "cs.PL",
            "cs.LO"
        ],
        "comment": "A version of this article is accepted at ICALP 2024"
    },
    {
        "paper id": "2404.18424",
        "abstract url": "https://arxiv.org/abs/2404.18424",
        "title": "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval",
        "rating": "-10",
        "keywords": [],
        "abstract": "The current use of large language models (LLMs) for zero-shot document ranking follows one of two ways: 1) prompt-based re-ranking methods, which require no further training but are feasible for only re-ranking a handful of candidate documents due to the associated computational costs; and 2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated to the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM. Our experimental evaluation on the BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    }
]