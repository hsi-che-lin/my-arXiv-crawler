[
    {
        "paper id": "2405.12139",
        "abstract url": "https://arxiv.org/abs/2405.12139",
        "title": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM",
        "rating": "2.5",
        "keywords": [
            [
                "Visual Language"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object. By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality. Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance. Moreover, coordinating human annotators for high-quality annotations is laborious and time-consuming. To address these challenges, we introduce DTLLM-VLT, which automatically generates extensive and multi-granularity text to enhance environmental diversity. (1) DTLLM-VLT generates scientific and multi-granularity text descriptions using a cohesive prompt framework. Its succinct and highly adaptable design allows seamless integration into various visual tracking benchmarks. (2) We select three prominent benchmarks to deploy our approach: short-term tracking, long-term tracking, and global instance tracking. We offer four granularity combinations for these benchmarks, considering the extent and density of semantic information, thereby showcasing the practicality and versatility of DTLLM-VLT. (3) We conduct comparative experiments on VLT benchmarks with different text granularities, evaluating and analyzing the impact of diverse text on tracking performance. Conclusionally, this work leverages LLM to provide multi-granularity semantic information for VLT task from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers. In the future, we believe this work can be extended to more datasets to support vision datasets understanding.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR Workshop 2024, Oral Presentation"
    },
    {
        "paper id": "2405.11822",
        "abstract url": "https://arxiv.org/abs/2405.11822",
        "title": "FeTT: Continual Class Incremental Learning via Feature Transformation Tuning",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Continual learning (CL) aims to extend deep models from static and enclosed environments to dynamic and complex scenarios, enabling systems to continuously acquire new knowledge of novel categories without forgetting previously learned knowledge. Recent CL models have gradually shifted towards the utilization of pre-trained models (PTMs) with parameter-efficient fine-tuning (PEFT) strategies. However, continual fine-tuning still presents a serious challenge of catastrophic forgetting due to the absence of previous task data. Additionally, the fine-tune-then-frozen mechanism suffers from performance limitations due to feature channels suppression and insufficient training data in the first CL task. To this end, this paper proposes feature transformation tuning (FeTT) model to non-parametrically fine-tune backbone features across all tasks, which not only operates independently of CL training data but also smooths feature channels to prevent excessive suppression. Then, the extended ensemble strategy incorporating different PTMs with FeTT model facilitates further performance improvement. We further elaborate on the discussions of the fine-tune-then-frozen paradigm and the FeTT model from the perspectives of discrepancy in class marginal distributions and feature channels. Extensive experiments on CL benchmarks validate the effectiveness of our proposed method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11831",
        "abstract url": "https://arxiv.org/abs/2405.11831",
        "title": "SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model",
        "rating": "2",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.LG",
                "eess.AS"
            ]
        ],
        "abstract": "Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities. However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency. Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities. Given these advantages, we explore the potential of SSM-based models in audio tasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning. SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively. We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification. Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k. These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications.",
        "subjects": [
            "eess.AS",
            "cs.LG"
        ],
        "comment": "Code at https://github.com/SiavashShams/ssamba"
    },
    {
        "paper id": "2405.11850",
        "abstract url": "https://arxiv.org/abs/2405.11850",
        "title": "Rethinking Overlooked Aspects in Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in large vision-language models (LVLMs), such as GPT4-V and LLaVA, have been substantial. LLaVA's modular architecture, in particular, offers a blend of simplicity and efficiency. Recent works mainly focus on introducing more pre-training and instruction tuning data to improve model's performance. This paper delves into the often-neglected aspects of data efficiency during pre-training and the selection process for instruction tuning datasets. Our research indicates that merely increasing the size of pre-training data does not guarantee improved performance and may, in fact, lead to its degradation. Furthermore, we have established a pipeline to pinpoint the most efficient instruction tuning (SFT) dataset, implying that not all SFT data utilized in existing studies are necessary. The primary objective of this paper is not to introduce a state-of-the-art model, but rather to serve as a roadmap for future research, aiming to optimize data usage during pre-training and fine-tuning processes to enhance the performance of vision-language models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11870",
        "abstract url": "https://arxiv.org/abs/2405.11870",
        "title": "Intuitive Fine-Tuning: Towards Unifying SFT and RLHF into a Single Process",
        "rating": "2",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences. Although SFT advances in training efficiency, RLHF delivers better alignment, thus they are often combined. However, common practices simply apply them sequentially without unifying their optimization targets, resulting in a trade-off between fitting different objectives, and ignoring the opportunities to bridge the paradigm gap and take the strength from both. To obtain a unified understanding, we interpret SFT and RLHF using two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework. This modeling shows that SFT is only a specialized case of RLHF with inferior estimation and optimization. RLHF evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers. Therefore, SFT overestimates the ability of model, leading to inferior optimization. Building on this view, we introduce Intuitive Fine-tuning (IFT) to integrate SFT and RLHF into a single process. IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, while using a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical alignment methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12130",
        "abstract url": "https://arxiv.org/abs/2405.12130",
        "title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Work in Progress"
    },
    {
        "paper id": "2405.11891",
        "abstract url": "https://arxiv.org/abs/2405.11891",
        "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics (TDD), a \\textcolor{black}{simple yet effective} approach to unveil and manipulate the role of prompts in generating LLM outputs. TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs. Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "ICLR 2024"
    },
    {
        "paper id": "2405.11905",
        "abstract url": "https://arxiv.org/abs/2405.11905",
        "title": "CSTA: CNN-based Spatiotemporal Attention for Video Summarization",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Video summarization aims to generate a concise representation of a video, capturing its essential content and key moments while reducing its overall length. Although several methods employ attention mechanisms to handle long-term dependencies, they often fail to capture the visual significance inherent in frames. To address this limitation, we propose a CNN-based SpatioTemporal Attention (CSTA) method that stacks each feature of frames from a single video to form image-like frame representations and applies 2D CNN to these frame features. Our methodology relies on CNN to comprehend the inter and intra-frame relations and to find crucial attributes in videos by exploiting its ability to learn absolute positions within images. In contrast to previous work compromising efficiency by designing additional modules to focus on spatial importance, CSTA requires minimal computational overhead as it uses CNN as a sliding window. Extensive experiments on two benchmark datasets (SumMe and TVSum) demonstrate that our proposed approach achieves state-of-the-art performance with fewer MACs compared to previous methods. Codes are available at https://github.com/thswodnjs3/CSTA.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2405.11792",
        "abstract url": "https://arxiv.org/abs/2405.11792",
        "title": "Source Localization by Multidimensional Steered Response Power Mapping with Sparse Bayesian Learning",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "We propose an advance Steered Response Power (SRP) method for localizing multiple sources. While conventional SRP performs well in adverse conditions, it remains to struggle in scenarios with closely neighboring sources, resulting in ambiguous SRP maps. We address this issue by applying sparsity optimization in SRP to obtain high-resolution maps. Our approach represents SRP maps as multidimensional matrices to preserve time-frequency information and further improve performance in unfavorable conditions. We use multi-dictionary Sparse Bayesian Learning to localize sources without needing prior knowledge of their quantity. We validate our method through practical experiments with a 16-channel planar microphone array and compare against three other SRP and sparsity-based methods. Our multidimensional SRP approach outperforms conventional SRP and the current state-of-the-art sparse SRP methods for localizing closely spaced sources in a reverberant room.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11804",
        "abstract url": "https://arxiv.org/abs/2405.11804",
        "title": "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in machine translation (MT) have significantly enhanced translation quality across various domains. However, the translation of literary texts remains a formidable challenge due to their complex language, figurative expressions, and cultural nuances. In this work, we introduce a novel multi-agent framework based on large language models (LLMs) for literary translation, implemented as a company called TransAgents, which mirrors traditional translation publication process by leveraging the collective capabilities of multiple agents, to address the intricate demands of translating literary works. To evaluate the effectiveness of our system, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP). MHP assesses translations from the perspective of monolingual readers of the target language, while BLP uses advanced LLMs to compare translations directly with the original texts. Empirical findings indicate that despite lower d-BLEU scores, translations from TransAgents are preferred by both human evaluators and LLMs over human-written references, particularly in genres requiring domain-specific knowledge. We also highlight the strengths and limitations of TransAgents through case studies and suggests directions for future research.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "work in progress"
    },
    {
        "paper id": "2405.11809",
        "abstract url": "https://arxiv.org/abs/2405.11809",
        "title": "Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy. These methods attempt to improve accuracy by introducing new modules or integrating traditional methods. However, the improvements are only modest. In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy. As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices. Our proposed method involves three key steps. Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions. Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model. Finally, we systematically prune the lightweight model to obtain the final model. Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "International Conference on Robotics and Automation (ICRA) 2024"
    },
    {
        "paper id": "2405.11819",
        "abstract url": "https://arxiv.org/abs/2405.11819",
        "title": "Beyond MLE: Investigating SEARNN for Low-Resourced Neural Machine Translation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Structured prediction tasks, like machine translation, involve learning functions that map structured inputs to structured outputs. Recurrent Neural Networks (RNNs) have historically been a popular choice for such tasks, including in natural language processing (NLP) applications. However, training RNNs using Maximum Likelihood Estimation (MLE) has its limitations, including exposure bias and a mismatch between training and testing metrics. SEARNN, based on the learning to search (L2S) framework, has been proposed as an alternative to MLE for RNN training. This project explored the potential of SEARNN to improve machine translation for low-resourced African languages -- a challenging task characterized by limited training data availability and the morphological complexity of the languages. Through experiments conducted on translation for English to Igbo, French to \\ewe, and French to \\ghomala directions, this project evaluated the efficacy of SEARNN over MLE in addressing the unique challenges posed by these languages. With an average BLEU score improvement of $5.4$\\% over the MLE objective, we proved that SEARNN is indeed a viable algorithm to effectively train RNNs on machine translation for low-resourced languages.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "In fulfillment of the 2024 practical coursework of IFT6132 course: https://www-labs.iro.umontreal.ca/~slacoste/teaching/ift6132/W24/"
    },
    {
        "paper id": "2405.11837",
        "abstract url": "https://arxiv.org/abs/2405.11837",
        "title": "Improving the Explain-Any-Concept by Introducing Nonlinearity to the Trainable Surrogate Model",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the evolving field of Explainable AI (XAI), interpreting the decisions of deep neural networks (DNNs) in computer vision tasks is an important process. While pixel-based XAI methods focus on identifying significant pixels, existing concept-based XAI methods use pre-defined or human-annotated concepts. The recently proposed Segment Anything Model (SAM) achieved a significant step forward to prepare automatic concept sets via comprehensive instance segmentation. Building upon this, the Explain Any Concept (EAC) model emerged as a flexible method for explaining DNN decisions. EAC model is based on using a surrogate model which has one trainable linear layer to simulate the target model. In this paper, by introducing an additional nonlinear layer to the original surrogate model, we show that we can improve the performance of the EAC model. We compare our proposed approach to the original EAC model and report improvements obtained on both ImageNet and MS COCO datasets.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "This paper is accepted for publication at IEEE SIU conference, 2024"
    },
    {
        "paper id": "2405.11862",
        "abstract url": "https://arxiv.org/abs/2405.11862",
        "title": "SEMv3: A Fast and Robust Approach to Table Separation Line Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Table structure recognition (TSR) aims to parse the inherent structure of a table from its input image. The `\"split-and-merge\" paradigm is a pivotal approach to parse table structure, where the table separation line detection is crucial. However, challenges such as wireless and deformed tables make it demanding. In this paper, we adhere to the \"split-and-merge\" paradigm and propose SEMv3 (SEM: Split, Embed and Merge), a method that is both fast and robust for detecting table separation lines. During the split stage, we introduce a Keypoint Offset Regression (KOR) module, which effectively detects table separation lines by directly regressing the offset of each line relative to its keypoint proposals. Moreover, in the merge stage, we define a series of merge actions to efficiently describe the table structure based on table grids. Extensive ablation studies demonstrate that our proposed KOR module can detect table separation lines quickly and accurately. Furthermore, on public datasets (e.g. WTW, ICDAR-2019 cTDaR Historical and iFLYTAB), SEMv3 achieves state-of-the-art (SOTA) performance. The code is available at https://github.com/Chunchunwumu/SEMv3.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 6 figures, 5 tables. Accepted by IJCAI2024 main track"
    },
    {
        "paper id": "2405.11865",
        "abstract url": "https://arxiv.org/abs/2405.11865",
        "title": "CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models. However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set. We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work. We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted to LREC-COLING 2024"
    },
    {
        "paper id": "2405.11874",
        "abstract url": "https://arxiv.org/abs/2405.11874",
        "title": "xFinder: Robust and Pinpoint Answer Extraction for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance. Particularly, the emergence of subjective or non-subjective cheating phenomena, such as test set leakage and prompt format overfitting, poses significant challenges to the reliable evaluation of LLMs. Since evaluation frameworks often utilize Regular Expression (RegEx) for answer extraction, some models may adjust their responses to comply with specific formats that are easily extractable by RegEx. Nevertheless, the key answer extraction module based on RegEx frequently suffers from extraction errors. This paper conducts a comprehensive analysis of the entire LLM evaluation chain, demonstrating that optimizing the key answer extraction module can improve extraction accuracy, reduce LLMs' reliance on specific answer formats, and enhance the reliability of LLM evaluation. To address these issues, we propose xFinder, a model specifically designed for key answer extraction. As part of this process, we create a specialized dataset, the Key Answer Finder (KAF) dataset, to ensure effective model training and evaluation. Through generalization testing and evaluation in real-world scenarios, the results demonstrate that the smallest xFinder model with only 500 million parameters achieves an average answer extraction accuracy of 93.42%. In contrast, RegEx accuracy in the best evaluation framework is 74.38%. xFinder exhibits stronger robustness and higher accuracy compared to existing evaluation frameworks. All resources for xFinder are available at \\url{https://github.com/IAAR-Shanghai/xFinder}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "34 Pages"
    },
    {
        "paper id": "2405.11877",
        "abstract url": "https://arxiv.org/abs/2405.11877",
        "title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at https://github.com/Eduard6421/RONLI.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at ACL 2024 (Main)"
    },
    {
        "paper id": "2405.11880",
        "abstract url": "https://arxiv.org/abs/2405.11880",
        "title": "Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "In this study, we propose an axiomatic system to define and quantify the precise memorization and in-context reasoning effects used by the large language model (LLM) for language generation. These effects are formulated as non-linear interactions between tokens/words encoded by the LLM. Specifically, the axiomatic system enables us to categorize the memorization effects into foundational memorization effects and chaotic memorization effects, and further classify in-context reasoning effects into enhanced inference patterns, eliminated inference patterns, and reversed inference patterns. Besides, the decomposed effects satisfy the sparsity property and the universal matching property, which mathematically guarantee that the LLM's confidence score can be faithfully decomposed into the memorization effects and in-context reasoning effects. Experiments show that the clear disentanglement of memorization effects and in-context reasoning effects enables a straightforward examination of detailed inference patterns encoded by LLMs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11894",
        "abstract url": "https://arxiv.org/abs/2405.11894",
        "title": "Refining Coded Image in Human Vision Layer Using CNN-Based Post-Processing",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Scalable image coding for both humans and machines is a technique that has gained a lot of attention recently. This technology enables the hierarchical decoding of images for human vision and image recognition models. It is a highly effective method when images need to serve both purposes. However, no research has yet incorporated the post-processing commonly used in popular image compression schemes into scalable image coding method for humans and machines. In this paper, we propose a method to enhance the quality of decoded images for humans by integrating post-processing into scalable coding scheme. Experimental results show that the post-processing improves compression performance. Furthermore, the effectiveness of the proposed method is validated through comparisons with traditional methods.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11897",
        "abstract url": "https://arxiv.org/abs/2405.11897",
        "title": "CReMa: Crisis Response through Computational Identification and Matching of Cross-Lingual Requests and Offers Shared on Social Media",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "During times of crisis, social media platforms play a vital role in facilitating communication and coordinating resources. Amidst chaos and uncertainty, communities often rely on these platforms to share urgent pleas for help, extend support, and organize relief efforts. However, the sheer volume of conversations during such periods, which can escalate to unprecedented levels, necessitates the automated identification and matching of requests and offers to streamline relief operations. This study addresses the challenge of efficiently identifying and matching assistance requests and offers on social media platforms during emergencies. We propose CReMa (Crisis Response Matcher), a systematic approach that integrates textual, temporal, and spatial features for multi-lingual request-offer matching. By leveraging CrisisTransformers, a set of pre-trained models specific to crises, and a cross-lingual embedding space, our methodology enhances the identification and matching tasks while outperforming strong baselines such as RoBERTa, MPNet, and BERTweet, in classification tasks, and Universal Sentence Encoder, Sentence Transformers in crisis embeddings generation tasks. We introduce a novel multi-lingual dataset that simulates scenarios of help-seeking and offering assistance on social media across the 16 most commonly used languages in Australia. We conduct comprehensive cross-lingual experiments across these 16 languages, also while examining trade-offs between multiple vector search strategies and accuracy. Additionally, we analyze a million-scale geotagged global dataset to comprehend patterns in relation to seeking help and offering assistance on social media. Overall, these contributions advance the field of crisis informatics and provide benchmarks for future research in the area.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2405.11912",
        "abstract url": "https://arxiv.org/abs/2405.11912",
        "title": "ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02% compared to vanilla interactive data annotation methods.",
        "subjects": [
            "cs.CL",
            "cs.HC"
        ],
        "comment": "Accepted to ACL 2024"
    },
    {
        "paper id": "2405.11919",
        "abstract url": "https://arxiv.org/abs/2405.11919",
        "title": "On Efficient and Statistical Quality Estimation for Data Annotation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Annotated datasets are an essential ingredient to train, evaluate, compare and productionalize supervised machine learning models. It is therefore imperative that annotations are of high quality. For their creation, good quality management and thereby reliable quality estimates are needed. Then, if quality is insufficient during the annotation process, rectifying measures can be taken to improve it. Quality estimation is often performed by having experts manually label instances as correct or incorrect. But checking all annotated instances tends to be expensive. Therefore, in practice, usually only subsets are inspected; sizes are chosen mostly without justification or regard to statistical power and more often than not, are relatively small. Basing estimates on small sample sizes, however, can lead to imprecise values for the error rate. Using unnecessarily large sample sizes costs money that could be better spent, for instance on more annotations. Therefore, we first describe in detail how to use confidence intervals for finding the minimal sample size needed to estimate the annotation error rate. Then, we propose applying acceptance sampling as an alternative to error rate estimation We show that acceptance sampling can reduce the required sample sizes up to 50% while providing the same statistical guarantees.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Accepted to ACL 2024"
    },
    {
        "paper id": "2405.11937",
        "abstract url": "https://arxiv.org/abs/2405.11937",
        "title": "Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in machine translation (MT), particularly for domain adaptation and low-resource languages. We implement the self-improvement process by fine-tuning the model on its MBR-decoded forward translations. By employing COMET as the MBR utility metric, we aim to achieve the reranking of translations that better aligns with human preferences. The paper explores the iterative application of this approach and the potential need for language-specific MBR utility metrics. The results demonstrate significant enhancements in translation quality for all examined language pairs, including successful application to domain-adapted models and generalisation to low-resource settings. This highlights the potential of COMET-guided MBR for efficient MT self-improvement in various scenarios.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "EAMT 2024"
    },
    {
        "paper id": "2405.11942",
        "abstract url": "https://arxiv.org/abs/2405.11942",
        "title": "FAME-MT Dataset: Formality Awareness Made Easy for Machine Translation Purposes",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "People use language for various purposes. Apart from sharing information, individuals may use it to express emotions or to show respect for another person. In this paper, we focus on the formality level of machine-generated translations and present FAME-MT -- a dataset consisting of 11.2 million translations between 15 European source languages and 8 European target languages classified to formal and informal classes according to target sentence formality. This dataset can be used to fine-tune machine translation models to ensure a given formality level for each European target language considered. We describe the dataset creation procedure, the analysis of the dataset's quality showing that FAME-MT is a reliable source of language register information, and we present a publicly available proof-of-concept machine translation model that uses the dataset to steer the formality level of the translation. Currently, it is the largest dataset of formality annotations, with examples expressed in 112 European language pairs. The dataset is published online: https://github.com/laniqo-public/fame-mt/ .",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at EAMT 2024"
    },
    {
        "paper id": "2405.11966",
        "abstract url": "https://arxiv.org/abs/2405.11966",
        "title": "Multiple-Choice Questions are Efficient and Robust LLM Evaluators",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructed by collecting answers and incorrect predictions on GSM8K and MATH from over 50 open-source models. Through extensive experiments, we show that LLMs' performance on the MC versions of these two popular benchmarks is strongly correlated with their performance on the original versions, and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30. Following a similar procedure, we also introduce PythonIO, a new program output prediction MC dataset constructed from two other popular LLM evaluation benchmarks HumanEval and MBPP. Our data and code are available at https://github.com/Geralt-Targaryen/MC-Evaluation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "data at https://github.com/Geralt-Targaryen/MC-Evaluation"
    },
    {
        "paper id": "2405.11971",
        "abstract url": "https://arxiv.org/abs/2405.11971",
        "title": "Data Augmentation for Text-based Person Retrieval Using Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-based Person Retrieval (TPR) aims to retrieve person images that match the description given a text query. The performance improvement of the TPR model relies on high-quality data for supervised training. However, it is difficult to construct a large-scale, high-quality TPR dataset due to expensive annotation and privacy protection. Recently, Large Language Models (LLMs) have approached or even surpassed human performance on many NLP tasks, creating the possibility to expand high-quality TPR datasets. This paper proposes an LLM-based Data Augmentation (LLM-DA) method for TPR. LLM-DA uses LLMs to rewrite the text in the current TPR dataset, achieving high-quality expansion of the dataset concisely and efficiently. These rewritten texts are able to increase the diversity of vocabulary and sentence structure while retaining the original key concepts and semantic information. In order to alleviate the hallucinations of LLMs, LLM-DA introduces a Text Faithfulness Filter (TFF) to filter out unfaithful rewritten text. To balance the contributions of original text and augmented text, a Balanced Sampling Strategy (BSS) is proposed to control the proportion of original text and augmented text used for training. LLM-DA is a plug-and-play method that can be easily integrated into various TPR models. Comprehensive experiments on three TPR benchmarks show that LLM-DA can improve the retrieval performance of current TPR models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11978",
        "abstract url": "https://arxiv.org/abs/2405.11978",
        "title": "SM-DTW: Stability Modulated Dynamic Time Warping for signature verification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Building upon findings in computational model of handwriting learning and execution, we introduce the concept of stability to explain the difference between the actual movements performed during multiple execution of the subject's signature, and conjecture that the most stable parts of the signature should play a paramount role in evaluating the similarity between a questioned signature and the reference ones during signature verification. We then introduce the Stability Modulated Dynamic Time Warping algorithm for incorporating the stability regions, i.e. the most similar parts between two signatures, into the distance measure between a pair of signatures computed by the Dynamic Time Warping for signature verification. Experiments were conducted on two datasets largely adopted for performance evaluation. Experimental results show that the proposed algorithm improves the performance of the baseline system and compares favourably with other top performing signature verification systems.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11985",
        "abstract url": "https://arxiv.org/abs/2405.11985",
        "title": "MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-Centric Visual Question Answering (TEC-VQA) in its proper format not only facilitates human-machine interaction in text-centric visual environments but also serves as a de facto gold proxy to evaluate AI models in the domain of text-centric scene understanding. However, most TEC-VQA benchmarks have focused on high-resource languages like English and Chinese. Despite pioneering works to expand multilingual QA pairs in non-text-centric VQA datasets using translation engines, the translation-based protocol encounters a substantial ``Visual-textual misalignment'' problem when applied to TEC-VQA. Specifically, it prioritizes the text in question-answer pairs while disregarding the visual text present in images. Furthermore, it does not adequately tackle challenges related to nuanced meaning, contextual distortion, language bias, and question-type diversity. In this work, we address the task of multilingual TEC-VQA and provide a benchmark with high-quality human expert annotations in 9 diverse languages, called MTVQA. To our knowledge, MTVQA is the first multilingual TEC-VQA benchmark to provide human expert annotations for text-centric scenarios. Further, by evaluating several state-of-the-art Multimodal Large Language Models (MLLMs), including GPT-4V, on our MTVQA dataset, it is evident that there is still room for performance improvement, underscoring the value of our dataset. We hope this dataset will provide researchers with fresh perspectives and inspiration within the community. The MTVQA dataset will be available at https://huggingface.co/datasets/ByteDance/MTVQA.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12031",
        "abstract url": "https://arxiv.org/abs/2405.12031",
        "title": "Neighborhood Attention Transformer with Progressive Channel Fusion for Speaker Verification",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Transformer-based architectures for speaker verification typically require more training data than ECAPA-TDNN. Therefore, recent work has generally been trained on VoxCeleb1&2. We propose a backbone network based on self-attention, which can achieve competitive results when trained on VoxCeleb2 alone. The network alternates between neighborhood attention and global attention to capture local and global features, then aggregates features of different hierarchical levels, and finally performs attentive statistical pooling. Additionally, we employ a progressive channel fusion strategy to expand the receptive field in the channel dimension as the network deepens. We trained the proposed PCF-NAT model on VoxCeleb2 and evaluated it on VoxCeleb1 and the validation sets of VoxSRC. The EER and minDCF of the shallow PCF-NAT are on average more than 20% lower than those of similarly sized ECAPA-TDNN. Deep PCF-NAT achieves an EER lower than 0.5% on VoxCeleb1-O.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "8 pages, 2 figures, 3 tables"
    },
    {
        "paper id": "2405.12055",
        "abstract url": "https://arxiv.org/abs/2405.12055",
        "title": "Unveiling factors influencing judgment variation in Sentiment Analysis with Natural Language Processing and Statistics",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "TripAdvisor reviews and comparable data sources play an important role in many tasks in Natural Language Processing (NLP), providing a data basis for the identification and classification of subjective judgments, such as hotel or restaurant reviews, into positive or negative polarities. This study explores three important factors influencing variation in crowdsourced polarity judgments, focusing on TripAdvisor reviews in Spanish. Three hypotheses are tested: the role of Part Of Speech (POS), the impact of sentiment words such as \"tasty\", and the influence of neutral words like \"ok\" on judgment variation. The study's methodology employs one-word titles, demonstrating their efficacy in studying polarity variation of words. Statistical tests on mean equality are performed on word groups of our interest. The results of this study reveal that adjectives in one-word titles tend to result in lower judgment variation compared to other word types or POS. Sentiment words contribute to lower judgment variation as well, emphasizing the significance of sentiment words in research on polarity judgments, and neutral words are associated with higher judgment variation as expected. However, these effects cannot be always reproduced in longer titles, which suggests that longer titles do not represent the best data source for testing the ambiguity of single words due to the influence on word polarity by other words like negation in longer titles. This empirical investigation contributes valuable insights into the factors influencing polarity variation of words, providing a foundation for NLP practitioners that aim to capture and predict polarity judgments in Spanish and for researchers that aim to understand factors influencing judgment variation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted manuscript to be published in PLoS One"
    },
    {
        "paper id": "2405.12059",
        "abstract url": "https://arxiv.org/abs/2405.12059",
        "title": "STYLE: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Equipping a conversational search engine with strategies regarding when to ask clarification questions is becoming increasingly important across various domains. Attributing to the context understanding capability of LLMs and their access to domain-specific sources of knowledge, LLM-based clarification strategies feature rapid transfer to various domains in a post-hoc manner. However, they still struggle to deliver promising performance on unseen domains, struggling to achieve effective domain transferability. We take the first step to investigate this issue and existing methods tend to produce one-size-fits-all strategies across diverse domains, limiting their search effectiveness. In response, we introduce a novel method, called Style, to achieve effective domain transferability. Our experimental results indicate that Style bears strong domain transferability, resulting in an average search performance improvement of ~10% on four unseen domains.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to Findings of ACL 2024"
    },
    {
        "paper id": "2405.12063",
        "abstract url": "https://arxiv.org/abs/2405.12063",
        "title": "CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are increasingly used to meet user information needs, but their effectiveness in dealing with user queries that contain various types of ambiguity remains unknown, ultimately risking user trust and satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct ~12K high-quality data to assess the strengths, weaknesses, and potential risks of various off-the-shelf LLMs. Our findings indicate the limited practical utility of current LLMs in identifying and clarifying ambiguous user queries, even enhanced by chain-of-thought (CoT) and few-shot prompting. These techniques may result in overconfidence in LLMs and yield only marginal enhancements in identifying ambiguity. Furthermore, current LLMs fall short in generating high-quality clarifying questions due to a lack of conflict resolution and inaccurate utilization of inherent knowledge. In this paper, CLAMBER presents a guidance and promotes further research on proactive and trustworthy LLMs. Our dataset is available at https://github.com/zt991211/CLAMBER",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to ACL 2024"
    },
    {
        "paper id": "2405.12081",
        "abstract url": "https://arxiv.org/abs/2405.12081",
        "title": "Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "To obtain high-quality annotations under limited budget, semi-automatic annotation methods are commonly used, where a portion of the data is annotated by experts and a model is then trained to complete the annotations for the remaining data. However, these methods mainly focus on selecting informative data for expert annotations to improve the model predictive ability (i.e., triage-to-human data), while the rest of the data is indiscriminately assigned to model annotation (i.e., triage-to-model data). This may lead to inefficiencies in budget allocation for annotations, as easy data that the model could accurately annotate may be unnecessarily assigned to the expert, and hard data may be misclassified by the model. As a result, the overall annotation quality may be compromised. To address this issue, we propose a selective annotation framework called SANT. It effectively takes advantage of both the triage-to-human and triage-to-model data through the proposed error-aware triage and bi-weighting mechanisms. As such, informative or hard data is assigned to the expert for annotation, while easy data is handled by the model. Experimental results show that SANT consistently outperforms other baselines, leading to higher-quality annotation through its proper allocation of data to both expert and model workers. We provide pioneering work on data annotation within budget constraints, establishing a landmark for future triage-based annotation studies.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "18 pages, 4 figures"
    },
    {
        "paper id": "2405.12084",
        "abstract url": "https://arxiv.org/abs/2405.12084",
        "title": "Distributional Semantics, Holism, and the Instability of Meaning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Current language models are built on the so-called distributional semantic approach to linguistic meaning that has the distributional hypothesis at its core. The distributional hypothesis involves a holistic conception of word meaning: the meaning of a word depends upon its relations to other words in the model. A standard objection to meaning holism is the charge of instability: any change in the meaning properties of a linguistic system (a human speaker, for example) would lead to many changes or possibly a complete change in the entire system. When the systems in question are trying to communicate with each other, it has been argued that instability of this kind makes communication impossible (Fodor and Lepore 1992, 1996, 1999). In this article, we examine whether the instability objection poses a problem for distributional models of meaning. First, we distinguish between distinct forms of instability that these models could exhibit, and we argue that only one such form is relevant for understanding the relation between instability and communication: what we call differential instability. Differential instability is variation in the relative distances between points in a space, rather than variation in the absolute position of those points. We distinguish differential and absolute instability by constructing two of our own models, a toy model constructed from the text of two novels, and a more sophisticated model constructed using the Word2vec algorithm from a combination of Wikipedia and SEP articles. We demonstrate the two forms of instability by showing how these models change as the corpora they are constructed from increase in size.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12100",
        "abstract url": "https://arxiv.org/abs/2405.12100",
        "title": "DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Math world problems correction(MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems. In this paper, leveraging the advancements in large language models (LLMs), we address two key objectives:(1) Distinguishing between mathematical reasoning and error correction; (2) Exploring strategies to enhance the error correction capabilities of LLMs in mathematics to solve MWPC task. We noticed that, in real-time education,assisting students in recognizing their mistakes is more crucial than simply providing correct answers. However, current research tends to prioritize obtaining accurate solutions to math problems rather than correcting potentially incorrect ones. Therefore, we modify the research paradigm, demonstrating that improving mathematical reasoning abilities does not equate to mastery in error correction. Meanwhile, we propose a novel method called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in error correction. In experiments, DOP has shown outstanding performance, highlighting its significant impact. We argue that in mathematical education, the demand for outstanding correctors surpasses that for proficient reasoners. Codes and data are available on https://github.com/ChenhaoEcnuCS/Reason-Correct.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12105",
        "abstract url": "https://arxiv.org/abs/2405.12105",
        "title": "Sheet Music Transformer ++: End-to-End Full-Page Optical Music Recognition for Pianoform Sheet Music",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Optical Music Recognition is a field that has progressed significantly, bringing accurate systems that transcribe effectively music scores into digital formats. Despite this, there are still several limitations that hinder OMR from achieving its full potential. Specifically, state of the art OMR still depends on multi-stage pipelines for performing full-page transcription, as well as it has only been demonstrated in monophonic cases, leaving behind very relevant engravings. In this work, we present the Sheet Music Transformer++, an end-to-end model that is able to transcribe full-page polyphonic music scores without the need of a previous Layout Analysis step. This is done thanks to an extensive curriculum learning-based pretraining with synthetic data generation. We conduct several experiments on a full-page extension of a public polyphonic transcription dataset. The experimental outcomes confirm that the model is competent at transcribing full-page pianoform scores, marking a noteworthy milestone in end-to-end OMR transcription.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12107",
        "abstract url": "https://arxiv.org/abs/2405.12107",
        "title": "Imp: Highly Capable Large Multimodal Models for Mobile Devices",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "By harnessing the capabilities of large language models (LLMs), recent large multimodal models (LMMs) have shown remarkable versatility in open-world multimodal understanding. Nevertheless, they are usually parameter-heavy and computation-intensive, thus hindering their applicability in resource-constrained scenarios. To this end, several lightweight LMMs have been proposed successively to maximize the capabilities under constrained scale (e.g., 3B). Despite the encouraging results achieved by these methods, most of them only focus on one or two aspects of the design space, and the key design choices that influence model capability have not yet been thoroughly investigated. In this paper, we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data. Based on our findings, we obtain Imp -- a family of highly capable LMMs at the 2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existing lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs at the 13B scale. With low-bit quantization and resolution reduction techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile chip with a high inference speed of about 13 tokens/s.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "19 pages, 6 figures"
    },
    {
        "paper id": "2405.12109",
        "abstract url": "https://arxiv.org/abs/2405.12109",
        "title": "Linguistic Structure from a Bottleneck on Sequential Information Processing",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Human language is a unique form of communication in the natural world, distinguished by its structured nature. Most fundamentally, it is systematic, meaning that signals can be broken down into component parts that are individually meaningful -- roughly, words -- which are combined in a regular way to form sentences. Furthermore, the way in which these parts are combined maintains a kind of locality: words are usually concatenated together, and they form contiguous phrases, keeping related parts of sentences close to each other. We address the challenge of understanding how these basic properties of language arise from broader principles of efficient communication under information processing constraints. Here we show that natural-language-like systematicity arises from minimization of excess entropy, a measure of statistical complexity that represents the minimum amount of information necessary for predicting the future of a sequence based on its past. In simulations, we show that codes that minimize excess entropy factorize their source distributions into approximately independent components, and then express those components systematically and locally. Next, in a series of massively cross-linguistic corpus studies, we show that human languages are structured to have low excess entropy at the level of phonology, morphology, syntax, and semantics. Our result suggests that human language performs a sequential generalization of Independent Components Analysis on the statistical distribution over meanings that need to be expressed. It establishes a link between the statistical and algebraic structure of human language, and reinforces the idea that the structure of human language may have evolved to minimize cognitive load while maximizing communicative expressiveness.",
        "subjects": [
            "cs.CL",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12147",
        "abstract url": "https://arxiv.org/abs/2405.12147",
        "title": "Eliciting Problem Specifications via Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Cognitive systems generally require a human to translate a problem definition into some specification that the cognitive system can use to attempt to solve the problem or perform the task. In this paper, we illustrate that large language models (LLMs) can be utilized to map a problem class, defined in natural language, into a semi-formal specification that can then be utilized by an existing reasoning and learning system to solve instances from the problem class. We present the design of LLM-enabled cognitive task analyst agent(s). Implemented with LLM agents, this system produces a definition of problem spaces for tasks specified in natural language. LLM prompts are derived from the definition of problem spaces in the AI literature and general problem-solving strategies (Polya's How to Solve It). A cognitive system can then use the problem-space specification, applying domain-general problem solving strategies (\"weak methods\" such as search), to solve multiple instances of problems from the problem class. This result, while preliminary, suggests the potential for speeding cognitive systems research via disintermediation of problem formulation while also retaining core capabilities of cognitive systems, such as robust inference and online learning.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "18 pages, Appendix. Submitted to Advances in Cognitive Systems 2024"
    },
    {
        "paper id": "2405.12163",
        "abstract url": "https://arxiv.org/abs/2405.12163",
        "title": "Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid advancement of large language models has given rise to a plethora of applications across a myriad of real-world tasks, mainly centered on aligning with human intent. However, the complexities inherent in human intent necessitate a dependence on labor-intensive and time-consuming human evaluation. To alleviate this constraint, we delve into the paradigm of employing open-source large language models as evaluators, aligning with the prevailing trend of utilizing GPT-4. Particularly, we present a step-by-step evaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained \\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through bran\\textbf{C}hing and bridging. Specifically, the branching operation dissects the evaluation task into various dimensions and granularities, thereby alleviating the challenges associated with evaluation. Concurrently, the bridging operation amalgamates diverse training datasets, augmenting the variety of evaluation tasks. In experimental trials, our 7B model consistently outperforms open-source larger-scale evaluation models across various widely adopted benchmarks in terms of both \\textit{Agreement} and \\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ the fine-grained correction capabilities induced by the evaluation model to refine multiple model responses, and the results show that the refinement elevates the quality of responses, leading to an improvement of 1-2 points on the MT-Bench. Our code is available at Github\\footnote{\\url{https://github.com/dropreg/Fennec}}.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12172",
        "abstract url": "https://arxiv.org/abs/2405.12172",
        "title": "Wireless vs. Traditional Ultrasound Assessed Knee Cartilage Outcomes Utilizing Automated Gain and Normalization Techniques",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Advancements in wireless ultrasound technology allow for point of care cartilage imaging, yet validation against traditional ultrasound units remains to be established for knee cartilage outcomes. Therefore, the purpose of our study was to establish the agreement of articular cartilage thickness and echo-intensity measures between traditional and wireless ultrasound units utilizing automatic-gain and normalization techniques. We used traditional and wireless ultrasound to assess the femoral cartilage via transverse suprapatellar scans with the knee in maximum flexion in 71 female NCAA Division I athletes (age: 20.0$\\pm$1.3 years, height: 171.7$\\pm$8.7 cm, mass: 69.4$\\pm$11.0 kg). Wireless ultrasound images (auto-gain and standard gain) were compared to traditional ultrasound images (standard gain) before and after normalization. Ultrasound image pixel values were algebraically scaled to normalize imaging parameter differences between units. Mean thickness and echo-intensity of the global and sub-regions of interest were measured for unnormalized and normalized images. Intraclass correlation coefficients ($ICC_{2,k}$) for absolute agreement, standard error of the measurement, and minimum detectable difference were calculated between the traditional and wireless ultrasound units across both gain parameters and normalization. Cartilage thickness demonstrated good to excellent agreement for all regions ($ICC_{2,k} = 0.83 {\\text -} 0.95$) regardless of gain and normalization. However, mean echo-intensity demonstrated poor to moderate agreement in all regions regardless of gain and normalization ($ICC_{2,k} = 0.23 {\\text -} 0.68 $). While there was a high level of agreement between units when assessing cartilage thickness, further research in ultrasound beam forming may lead to improvements in agreement of cartilage echo-intensity between ultrasound units.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "19 Pages, 1 figure, 3 tables"
    },
    {
        "paper id": "2405.12175",
        "abstract url": "https://arxiv.org/abs/2405.12175",
        "title": "Enhancing Explainable AI: A Hybrid Approach Combining GradCAM and LRP for CNN Interpretability",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a new technique that explains the output of a CNN-based model using a combination of GradCAM and LRP methods. Both of these methods produce visual explanations by highlighting input regions that are important for predictions. In the new method, the explanation produced by GradCAM is first processed to remove noises. The processed output is then multiplied elementwise with the output of LRP. Finally, a Gaussian blur is applied on the product. We compared the proposed method with GradCAM and LRP on the metrics of Faithfulness, Robustness, Complexity, Localisation and Randomisation. It was observed that this method performs better on Complexity than both GradCAM and LRP and is better than atleast one of them in the other metrics.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12206",
        "abstract url": "https://arxiv.org/abs/2405.12206",
        "title": "Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Scientist learn early on how to cite scientific sources to support their claims. Sometimes, however, scientists have challenges determining where a citation should be situated -- or, even worse, fail to cite a source altogether. Automatically detecting sentences that need a citation (i.e., citation worthiness) could solve both of these issues, leading to more robust and well-constructed scientific arguments. Previous researchers have applied machine learning to this task but have used small datasets and models that do not take advantage of recent algorithmic developments such as attention mechanisms in deep learning. We hypothesize that we can develop significantly accurate deep learning architectures that learn from large supervised datasets constructed from open access publications. In this work, we propose a Bidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism and contextual information to detect sentences that need citations. We also produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset, which is orders of magnitude larger than previous datasets. Our experiments show that our architecture achieves state of the art performance on the standard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance ($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer learning across these datasets. We further use interpretable models to illuminate how specific language is used to promote and inhibit citations. We discover that sections and surrounding sentences are crucial for our improved predictions. We further examined purported mispredictions of the model, and uncovered systematic human mistakes in citation behavior and source data. This opens the door for our model to check documents during pre-submission and pre-archival procedures. We make this new dataset, the code, and a web-based tool available to the community.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12209",
        "abstract url": "https://arxiv.org/abs/2405.12209",
        "title": "MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in large language models (LLMs) have showcased significant improvements in mathematics. However, traditional math benchmarks like GSM8k offer a unidimensional perspective, falling short in providing a holistic assessment of the LLMs' math capabilities. To address this gap, we introduce MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large language models. MathBench spans a wide range of mathematical disciplines, offering a detailed evaluation of both theoretical understanding and practical problem-solving skills. The benchmark progresses through five distinct stages, from basic arithmetic to college mathematics, and is structured to evaluate models at various depths of knowledge. Each stage includes theoretical questions and application problems, allowing us to measure a model's mathematical proficiency and its ability to apply concepts in practical scenarios. MathBench aims to enhance the evaluation of LLMs' mathematical abilities, providing a nuanced view of their knowledge understanding levels and problem solving skills in a bilingual context. The project is released at https://github.com/open-compass/MathBench .",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Project: https://github.com/open-compass/MathBench"
    },
    {
        "paper id": "2405.12299",
        "abstract url": "https://arxiv.org/abs/2405.12299",
        "title": "Perturbing the Gradient for Alleviating Meta Overfitting",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The reason for Meta Overfitting can be attributed to two factors: Mutual Non-exclusivity and the Lack of diversity, consequent to which a single global function can fit the support set data of all the meta-training tasks and fail to generalize to new unseen tasks. This issue is evidenced by low error rates on the meta-training tasks, but high error rates on new tasks. However, there can be a number of novel solutions to this problem keeping in mind any of the two objectives to be attained, i.e. to increase diversity in the tasks and to reduce the confidence of the model for some of the tasks. In light of the above, this paper proposes a number of solutions to tackle meta-overfitting on few-shot learning settings, such as few-shot sinusoid regression and few shot classification. Our proposed approaches demonstrate improved generalization performance compared to state-of-the-art baselines for learning in a non-mutually exclusive task setting. Overall, this paper aims to provide insights into tackling overfitting in meta-learning and to advance the field towards more robust and generalizable models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12413",
        "abstract url": "https://arxiv.org/abs/2405.12413",
        "title": "Targeted Multilingual Adaptation for Low-resource Language Families",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The \"massively-multilingual\" training of multilingual models is known to limit their utility in any one language, and they perform particularly poorly on low-resource languages. However, there is evidence that low-resource languages can benefit from targeted multilinguality, where the model is trained on closely related languages. To test this approach more rigorously, we systematically study best practices for adapting a pre-trained model to a language family. Focusing on the Uralic family as a test case, we adapt XLM-R under various configurations to model 15 languages; we then evaluate the performance of each experimental setting on two downstream tasks and 11 evaluation languages. Our adapted models significantly outperform mono- and multilingual baselines. Furthermore, a regression analysis of hyperparameter effects reveals that adapted vocabulary size is relatively unimportant for low-resource languages, and that low-resource languages can be aggressively up-sampled during training at little detriment to performance in high-resource languages. These results introduce new best practices for performing language adaptation in a targeted setting.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12434",
        "abstract url": "https://arxiv.org/abs/2405.12434",
        "title": "Resolving Word Vagueness with Scenario-guided Adapter for Natural Language Inference",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Natural Language Inference (NLI) is a crucial task in natural language processing that involves determining the relationship between two sentences, typically referred to as the premise and the hypothesis. However, traditional NLI models solely rely on the semantic information inherent in independent sentences and lack relevant situational visual information, which can hinder a complete understanding of the intended meaning of the sentences due to the ambiguity and vagueness of language. To address this challenge, we propose an innovative ScenaFuse adapter that simultaneously integrates large-scale pre-trained linguistic knowledge and relevant visual information for NLI tasks. Specifically, we first design an image-sentence interaction module to incorporate visuals into the attention mechanism of the pre-trained model, allowing the two modalities to interact comprehensively. Furthermore, we introduce an image-sentence fusion module that can adaptively integrate visual information from images and semantic information from sentences. By incorporating relevant visual information and leveraging linguistic knowledge, our approach bridges the gap between language and vision, leading to improved understanding and inference capabilities in NLI tasks. Extensive benchmark experiments demonstrate that our proposed ScenaFuse, a scenario-guided approach, consistently boosts NLI performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "IJCAI24"
    },
    {
        "paper id": "2405.12438",
        "abstract url": "https://arxiv.org/abs/2405.12438",
        "title": "CoCo Matrix: Taxonomy of Cognitive Contributions in Co-writing with Intelligent Agents",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, there has been a growing interest in employing intelligent agents in writing. Previous work emphasizes the evaluation of the quality of end product-whether it was coherent and polished, overlooking the journey that led to the product, which is an invaluable dimension of the creative process. To understand how to recognize human efforts in co-writing with intelligent writing systems, we adapt Flower and Hayes' cognitive process theory of writing and propose CoCo Matrix, a two-dimensional taxonomy of entropy and information gain, to depict the new human-agent co-writing model. We define four quadrants and situate thirty-four published systems within the taxonomy. Our research found that low entropy and high information gain systems are under-explored, yet offer promising future directions in writing tasks that benefit from the agent's divergent planning and the human's focused translation. CoCo Matrix, not only categorizes different writing systems but also deepens our understanding of the cognitive processes in human-agent co-writing. By analyzing minimal changes in the writing process, CoCo Matrix serves as a proxy for the writer's mental model, allowing writers to reflect on their contributions. This reflection is facilitated through the measured metrics of information gain and entropy, which provide insights irrespective of the writing system used.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12461",
        "abstract url": "https://arxiv.org/abs/2405.12461",
        "title": "WorldAfford: Affordance Grounding based on Natural Language Instructions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Affordance grounding aims to localize the interaction regions for the manipulated objects in the scene image according to given instructions. A critical challenge in affordance grounding is that the embodied agent should understand human instructions and analyze which tools in the environment can be used, as well as how to use these tools to accomplish the instructions. Most recent works primarily supports simple action labels as input instructions for localizing affordance regions, failing to capture complex human objectives. Moreover, these approaches typically identify affordance regions of only a single object in object-centric images, ignoring the object context and struggling to localize affordance regions of multiple objects in complex scenes for practical applications. To address this concern, for the first time, we introduce a new task of affordance grounding based on natural language instructions, extending it from previously using simple labels for complex human instructions. For this new task, we propose a new framework, WorldAfford. We design a novel Affordance Reasoning Chain-of-Thought Prompting to reason about affordance knowledge from LLMs more precisely and logically. Subsequently, we use SAM and CLIP to localize the objects related to the affordance knowledge in the image. We identify the affordance regions of the objects through an affordance region localization module. To benchmark this new task and validate our framework, an affordance grounding dataset, LLMaFF, is constructed. We conduct extensive experiments to verify that WorldAfford performs state-of-the-art on both the previous AGD20K and the new LLMaFF dataset. In particular, WorldAfford can localize the affordance regions of multiple objects and provide an alternative when objects in the environment cannot fully match the given instruction.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12468",
        "abstract url": "https://arxiv.org/abs/2405.12468",
        "title": "Leveraging Diverse Data Generation for Adaptable Zero-Shot Dialogue State Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This work demonstrates that substantial gains in zero-shot dialogue state tracking (DST) accuracy can be achieved by increasing the diversity of training data using synthetic data generation techniques. Current DST training resources are severely limited in the number of application domains and slot types they cover due to the high costs of data collection, resulting in limited adaptability to new domains. The presented work overcomes this challenge using a novel, fully automatic data generation approach to create synthetic zero-shot DST training resources. Unlike previous approaches for generating DST data, the presented approach generates entirely new application domains to generate dialogues, complete with silver dialogue state annotations and slot descriptions. This approach is used to create the D0T dataset for training zero-shot DST models, which covers an unprecedented 1,000+ domains. Experiments performed on the MultiWOZ benchmark indicate that training models on diverse synthetic data yields a performance improvement of +6.7% Joint Goal Accuracy, achieving results competitive with much larger models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12496",
        "abstract url": "https://arxiv.org/abs/2405.12496",
        "title": "A Survey of Integrating Wireless Technology into Active Noise Control",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Active Noise Control (ANC) is a widely adopted technology for reducing environmental noise across various scenarios. This paper focuses on enhancing noise reduction performance, particularly through the refinement of signal quality fed into ANC systems. We discuss the main wireless technique integrated into the ANC system, equipped with some innovative algorithms, in diverse environments. Instead of using microphone arrays, which increase the computation complexity of the ANC system, to isolate multiple noise sources to improve noise reduction performance, the application of the wireless technique avoids extra computation demand. Wireless transmissions of reference, error, and control signals are also applied to improve the convergence performance of the ANC system. Furthermore, this paper lists some wireless ANC applications, such as earbuds, headphones, windows, and headrests, underscoring their adaptability and efficiency in various settings.",
        "subjects": [
            "eess.AS",
            "cs.NI",
            "cs.SD",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11788",
        "abstract url": "https://arxiv.org/abs/2405.11788",
        "title": "TinyLLaVA Factory: A Modularized Codebase for Small-scale Large Multimodal Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present TinyLLaVA Factory, an open-source modular codebase for small-scale large multimodal models (LMMs) with a focus on simplicity of code implementations, extensibility of new features, and reproducibility of training results. Following the design philosophy of the factory pattern in software engineering, TinyLLaVA Factory modularizes the entire system into interchangeable components, with each component integrating a suite of cutting-edge models and methods, meanwhile leaving room for extensions to more features. In addition to allowing users to customize their own LMMs, TinyLLaVA Factory provides popular training recipes to let users pretrain and finetune their models with less coding effort. Empirical experiments validate the effectiveness of our codebase. The goal of TinyLLaVA Factory is to assist researchers and practitioners in exploring the wide landscape of designing and training small-scale LMMs with affordable computational resources.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Our codebase is made public at https://github.com/TinyLLaVA/TinyLLaVA_Factory with documentation available at https://tinyllava-factory.readthedocs.io/en/latest/"
    },
    {
        "paper id": "2405.11789",
        "abstract url": "https://arxiv.org/abs/2405.11789",
        "title": "The Grandparent Scam: A Systems Perspective Case Study On Elder Fraud And The Concept Of Human Layering",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "In April 2024, an 81-year-old Ohio man was charged with murder, assault, and kidnapping. The man believed that he was protecting his family from scammers threatening harm. What he did not realize was that the 61-year-old Uber driver he killed, was also a victim of the same scammers. This case study examines some common variants of the Grandparent Scam from a systems perspective and how weaponization of conscience is used in these scams. Additionally, this study examines the parallels between layering in money laundering and human layering in the execution of these scams.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11802",
        "abstract url": "https://arxiv.org/abs/2405.11802",
        "title": "Counterfactual Explanation-Based Badminton Motion Guidance Generation Using Wearable Sensors",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This study proposes a framework for enhancing the stroke quality of badminton players by generating personalized motion guides, utilizing a multimodal wearable dataset. These guides are based on counterfactual algorithms and aim to reduce the performance gap between novice and expert players. Our approach provides joint-level guidance through visualizable data to assist players in improving their movements without requiring expert knowledge. The method was evaluated against a traditional algorithm using metrics to assess validity, proximity, and plausibility, including arithmetic measures and motion-specific evaluation metrics. Our evaluation demonstrates that the proposed framework can generate motions that maintain the essence of original movements while enhancing stroke quality, providing closer guidance than direct expert motion replication. The results highlight the potential of our approach for creating personalized sports motion guides by generating counterfactual motion guidance for arbitrary input motion samples of badminton strokes.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "ICRA Wearable Workshop 2024 - 1st Workshop on Advancing Wearable Devices and Applications through Novel Design, Sensing, Actuation, and AI"
    },
    {
        "paper id": "2405.11816",
        "abstract url": "https://arxiv.org/abs/2405.11816",
        "title": "Transfer Learning for CSI-based Positioning with Multi-environment Meta-learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Utilizing deep learning (DL) techniques for radio-based positioning of user equipment (UE) through channel state information (CSI) fingerprints has demonstrated significant potential. DL models can extract complex characteristics from the CSI fingerprints of a particular environment and accurately predict the position of a UE. Nonetheless, the effectiveness of the DL model trained on CSI fingerprints is highly dependent on the particular training environment, limiting the trained model's applicability across different environments. This paper proposes a novel DL model structure consisting of two parts, where the first part aims at identifying features that are independent from any specific environment, while the second part combines those features in an environment specific way with the goal of positioning. To train such a two-part model, we propose the multi-environment meta-learning (MEML) approach for the first part to facilitate training across various environments, while the second part of the model is trained solely on data from a specific environment. Our findings indicate that employing the MEML approach for initializing the weights of the DL model for a new unseen environment significantly boosts the accuracy of UE positioning in the new target environment as well the reliability of its uncertainty estimation. This method outperforms traditional transfer learning methods, whether direct transfer learning (DTL) between environments or completely training from scratch with data from a new environment. The proposed approach is verified with real measurements for both line-of-sight (LOS) and non-LOS (NLOS) environments.",
        "subjects": [
            "eess.SP",
            "cs.AI"
        ],
        "comment": "Submitted for IEEE journal publication. 10 pages, 13 figures, 7 tables"
    },
    {
        "paper id": "2405.11841",
        "abstract url": "https://arxiv.org/abs/2405.11841",
        "title": "Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Facing the current debate on whether Large Language Models (LLMs) attain near-human intelligence levels (Mitchell & Krakauer, 2023; Bubeck et al., 2023; Kosinski, 2023; Shiffrin & Mitchell, 2023; Ullman, 2023), the current study introduces a benchmark for evaluating social intelligence, one of the most distinctive aspects of human cognition. We developed a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP). Our approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns. Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities. Notably, GPT models demonstrated social intelligence only at the most basic order (order = 0), in stark contrast to human social intelligence (order >= 2). Further examination indicated a propensity of LLMs to rely on pattern recognition for shortcuts, casting doubt on their possession of authentic human-level social intelligence. Our codes, dataset, appendix and human data are released at https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Also published in Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci), 2024"
    },
    {
        "paper id": "2405.11867",
        "abstract url": "https://arxiv.org/abs/2405.11867",
        "title": "Depth Prompting for Sensor-Agnostic Depth Estimation",
        "rating": "0.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Dense depth maps have been used as a key element of visual perception tasks. There have been tremendous efforts to enhance the depth quality, ranging from optimization-based to learning-based methods. Despite the remarkable progress for a long time, their applicability in the real world is limited due to systematic measurement biases such as density, sensing pattern, and scan range. It is well-known that the biases make it difficult for these methods to achieve their generalization. We observe that learning a joint representation for input modalities (e.g., images and depth), which most recent methods adopt, is sensitive to the biases. In this work, we disentangle those modalities to mitigate the biases with prompt engineering. For this, we design a novel depth prompt module to allow the desirable feature representation according to new depth distributions from either sensor types or scene configurations. Our depth prompt can be embedded into foundation models for monocular depth estimation. Through this embedding process, our method helps the pretrained model to be free from restraint of depth scan range and to provide absolute scale depth maps. We demonstrate the effectiveness of our method through extensive evaluations. Source code is publicly available at https://github.com/JinhwiPark/DepthPrompting .",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "comment": "Accepted at CVPR 2024"
    },
    {
        "paper id": "2405.11895",
        "abstract url": "https://arxiv.org/abs/2405.11895",
        "title": "Sparse Attention-driven Quality Prediction for Production Process Optimization in Digital Twins",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the process industry, optimizing production lines for long-term efficiency requires real-time monitoring and analysis of operation states to fine-tune production line parameters. However, the complexity in operational logic and the intricate coupling of production process parameters make it difficult to develop an accurate mathematical model for the entire process, thus hindering the deployment of efficient optimization mechanisms. In view of these difficulties, we propose to deploy a digital twin of the production line by digitally abstracting its physical layout and operational logic. By iteratively mapping the real-world data reflecting equipment operation status and product quality inspection in the digital twin, we adopt a quality prediction model for production process based on self-attention-enabled temporal convolutional neural networks. This model enables the data-driven state evolution of the digital twin. The digital twin takes a role of aggregating the information of actual operating conditions and the results of quality-sensitive analysis, which facilitates the optimization of process production quality with virtual-reality evolution under multi-dimensional constraints. Leveraging the digital twin model as an information-flow carrier, we extract temporal features from key process indicators and establish a production process quality prediction model based on the proposed composite neural network. Our operation experiments on a specific tobacco shredding line demonstrate that the proposed digital twin-based production process optimization method fosters seamless integration between virtual and real production lines. This integration achieves an average operating status prediction accuracy of over 98\\% and near-optimal production process control.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11907",
        "abstract url": "https://arxiv.org/abs/2405.11907",
        "title": "Ensemble and Mixture-of-Experts DeepONets For Operator Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a novel deep operator network (DeepONet) architecture for operator learning, the ensemble DeepONet, that allows for enriching the trunk network of a single DeepONet with multiple distinct trunk networks. This trunk enrichment allows for greater expressivity and generalization capabilities over a range of operator learning problems. We also present a spatial mixture-of-experts (MoE) DeepONet trunk network architecture that utilizes a partition-of-unity (PoU) approximation to promote spatial locality and model sparsity in the operator learning problem. We first prove that both the ensemble and PoU-MoE DeepONets are universal approximators. We then demonstrate that ensemble DeepONets containing a trunk ensemble of a standard trunk, the PoU-MoE trunk, and/or a proper orthogonal decomposition (POD) trunk can achieve 2-4x lower relative $\\ell_2$ errors than standard DeepONets and POD-DeepONets on both standard and challenging new operator learning problems involving partial differential equations (PDEs) in two and three dimensions. Our new PoU-MoE formulation provides a natural way to incorporate spatial locality and model sparsity into any neural network architecture, while our new ensemble DeepONet provides a powerful and general framework for incorporating basis enrichment in scientific machine learning architectures for operator learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11913",
        "abstract url": "https://arxiv.org/abs/2405.11913",
        "title": "Diff-BGM: A Diffusion Model for Video Background Music Generation",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "When editing a video, a piece of attractive background music is indispensable. However, video background music generation tasks face several challenges, for example, the lack of suitable training datasets, and the difficulties in flexibly controlling the music generation process and sequentially aligning the video and music. In this work, we first propose a high-quality music-video dataset BGM909 with detailed annotation and shot detection to provide multi-modal information about the video and music. We then present evaluation metrics to assess music quality, including music diversity and alignment between music and video with retrieval precision metrics. Finally, we propose the Diff-BGM framework to automatically generate the background music for a given video, which uses different signals to control different aspects of the music during the generation process, i.e., uses dynamic video features to control music rhythm and semantic features to control the melody and atmosphere. We propose to align the video and music sequentially by introducing a segment-aware cross-attention layer. Experiments verify the effectiveness of our proposed method. The code and models are available at https://github.com/sizhelee/Diff-BGM.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR 2024(Poster)"
    },
    {
        "paper id": "2405.11982",
        "abstract url": "https://arxiv.org/abs/2405.11982",
        "title": "Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep reinforcement learning (DRL) algorithms can suffer from modeling errors between the simulation and the real world. Many studies use adversarial learning to generate perturbation during training process to model the discrepancy and improve the robustness of DRL. However, most of these approaches use a fixed parameter to control the intensity of the adversarial perturbation, which can lead to a trade-off between average performance and robustness. In fact, finding the optimal parameter of the perturbation is challenging, as excessive perturbations may destabilize training and compromise agent performance, while insufficient perturbations may not impart enough information to enhance robustness. To keep the training stable while improving robustness, we propose a simple but effective method, namely, Adaptive Adversarial Perturbation (A2P), which can dynamically select appropriate adversarial perturbations for each sample. Specifically, we propose an adaptive adversarial coefficient framework to adjust the effect of the adversarial perturbation during training. By designing a metric for the current intensity of the perturbation, our method can calculate the suitable perturbation levels based on the current relative performance. The appealing feature of our method is that it is simple to deploy in real-world applications and does not require accessing the simulator in advance. The experiments in MuJoCo show that our method can improve the training stability and learn a robust policy when migrated to different test environments. The code is available at https://github.com/Lqm00/A2P-SAC.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12001",
        "abstract url": "https://arxiv.org/abs/2405.12001",
        "title": "Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques. Previous context-based approaches predominantly rely on the intuition that maximizing the mutual information between the task and the task representation ($I(Z;M)$) can lead to performance improvements. Despite achieving attractive results, the theoretical justification of performance improvement for such intuition has been lacking. Motivated by the return discrepancy scheme in the model-based RL field, we find that maximizing $I(Z;M)$ can be interpreted as consistently raising the lower bound of the expected return for a given policy conditioning on the optimal task representation. However, this optimization process ignores the task representation shift between two consecutive updates, which may lead to performance improvement collapse. To address this problem, we turn to use the framework of performance difference bound to consider the impacts of task representation shift explicitly. We demonstrate that by reining the task representation shift, it is possible to achieve monotonic performance improvements, thereby showcasing the advantage against previous approaches. To make it practical, we design an easy yet highly effective algorithm RETRO (\\underline{RE}ining \\underline{T}ask \\underline{R}epresentation shift in context-based \\underline{O}ffline meta reinforcement learning) with only adding one line of code compared to the backbone. Empirical results validate its state-of-the-art (SOTA) asymptotic performance, training stability and training-time consumption on MuJoCo and MetaWorld benchmarks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12016",
        "abstract url": "https://arxiv.org/abs/2405.12016",
        "title": "Strategy-Proof Auctions through Conformal Prediction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Auctions are key for maximizing sellers' revenue and ensuring truthful bidding among buyers. Recently, an approach known as differentiable economics based on deep learning shows promise in learning optimal auction mechanisms for multiple items and participants. However, this approach has no guarantee of strategy-proofness at test time. Strategy-proofness is crucial as it ensures that buyers are incentivized to bid their true valuations, leading to optimal and fair auction outcomes without the risk of manipulation. Building on conformal prediction, we introduce a novel approach to achieve strategy-proofness with rigorous statistical guarantees. The key novelties of our method are: (i) the formulation of a regret prediction model, used to quantify at test time violations of strategy-proofness; and (ii) an auction acceptance rule that leverages the predicted regret to ensure that for a new auction, the data-driven mechanism meets the strategy-proofness requirement with high probability (e.g., 99\\%). Numerical experiments demonstrate the necessity for rigorous guarantees, the validity of our theoretical results, and the applicability of our proposed method.",
        "subjects": [
            "cs.GT",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12040",
        "abstract url": "https://arxiv.org/abs/2405.12040",
        "title": "Reputation Transfer in the Twitter Diaspora",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Social media platforms have witnessed a dynamic landscape of user migration in recent years, fueled by changes in ownership, policy, and user preferences. This paper explores the phenomenon of user migration from established platforms like X/Twitter to emerging alternatives such as Threads, Mastodon, and Truth Social. Leveraging a large dataset from X/Twitter, we investigate the extent of user departure from X/Twitter and the destinations they migrate to. Additionally, we examine whether a user's reputation on one platform correlates with their reputation on another, shedding light on the transferability of digital reputation across social media ecosystems. Overall, we find that users with a large following on X/Twitter are more likely to migrate to another platform; and that their reputation on X/Twitter is highly correlated with reputations on Threads, but not Mastodon or Truth Social.",
        "subjects": [
            "cs.SI",
            "cs.HC"
        ],
        "comment": "8 pages, 6 figures"
    },
    {
        "paper id": "2405.12043",
        "abstract url": "https://arxiv.org/abs/2405.12043",
        "title": "The Incoherency Risk in the EU's New Cyber Security Policies",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The European Union (EU) has been pursuing new cyber security policies in recent years. This paper presents a short evaluation of four such policies. The focus is on potential incoherency, meaning a lack of integration, divergence between the member states, institutional dysfunction, and other related problems that should be at least partially avoidable by sound policy-making. According to the evaluation, the four policies have substantially increased the complexity of the EU's cyber security framework. In addition, there are potential problems with trust, divergence between industry sectors and different technologies, bureaucratic conflicts, and technical issues, among other things. With these insights, the paper not only contributes to the study of EU policies but also advances the understanding of cyber security policies in general.",
        "subjects": [
            "cs.CR",
            "cs.CY"
        ],
        "comment": "Submitted"
    },
    {
        "paper id": "2405.12052",
        "abstract url": "https://arxiv.org/abs/2405.12052",
        "title": "Parallelization of the K-Means Algorithm with Applications to Big Data Clustering",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The K-Means clustering using LLoyd's algorithm is an iterative approach to partition the given dataset into K different clusters. The algorithm assigns each point to the cluster based on the following objective function \\[\\ \\min \u03a3_{i=1}^{n}||x_i-\u03bc_{x_i}||^2\\] The serial algorithm involves iterative steps where we compute the distance of each datapoint from the centroids and assign the datapoint to the nearest centroid. This approach is essentially known as the expectation-maximization step. Clustering involves extensive computations to calculate distances at each iteration, which increases as the number of data points increases. This provides scope for parallelism. However, we must ensure that in a parallel process, each thread has access to the updated centroid value and no racing condition exists on any centroid values. We will compare two different approaches in this project. The first approach is an OpenMP flat synchronous method where all processes are run in parallel, and we use synchronization to ensure safe updates of clusters. The second approach we adopt is a GPU based parallelization approach using OpenACC wherein we will try to make use of GPU architecture to parallelize chunks of the algorithm to observe decreased computation time. We will analyze metrics such as speed up, efficiency,time taken with varying data points, and number of processes to compare the two approaches and understand the relative performance improvement we can get.",
        "subjects": [
            "cs.DC",
            "cs.LG"
        ],
        "comment": "7 Pages, 5 tables, 12 figures"
    },
    {
        "paper id": "2405.12087",
        "abstract url": "https://arxiv.org/abs/2405.12087",
        "title": "Channel Balance Interpolation in the Lightning Network via Machine Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Bitcoin Lightning Network is a Layer 2 payment protocol that addresses Bitcoin's scalability by facilitating quick and cost effective transactions through payment channels. This research explores the feasibility of using machine learning models to interpolate channel balances within the network, which can be used for optimizing the network's pathfinding algorithms. While there has been much exploration in balance probing and multipath payment protocols, predicting channel balances using solely node and channel features remains an uncharted area. This paper evaluates the performance of several machine learning models against two heuristic baselines and investigates the predictive capabilities of various features. Our model performs favorably in experimental evaluation, outperforming by 10% against an equal split baseline where both edges are assigned half of the channel capacity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12167",
        "abstract url": "https://arxiv.org/abs/2405.12167",
        "title": "Open-Source Assessments of AI Capabilities: The Proliferation of AI Analysis Tools, Replicating Competitor Models, and the Zhousidun Dataset",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The integration of artificial intelligence (AI) into military capabilities has become a norm for major military power across the globe. Understanding how these AI models operate is essential for maintaining strategic advantages and ensuring security. This paper demonstrates an open-source methodology for analyzing military AI models through a detailed examination of the Zhousidun dataset, a Chinese-originated dataset that exhaustively labels critical components on American and Allied destroyers. By demonstrating the replication of a state-of-the-art computer vision model on this dataset, we illustrate how open-source tools can be leveraged to assess and understand key military AI capabilities. This methodology offers a robust framework for evaluating the performance and potential of AI-enabled military capabilities, thus enhancing the accuracy and reliability of strategic assessments.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12179",
        "abstract url": "https://arxiv.org/abs/2405.12179",
        "title": "Building Temporal Kernels with Orthogonal Polynomials",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We introduce a class of models named PLEIADES (PoLynomial Expansion In Adaptive Distributed Event-based Systems), which contains temporal convolution kernels generated from orthogonal polynomial basis functions. We focus on interfacing these networks with event-based data to perform online spatiotemporal classification and detection with low latency. By virtue of using structured temporal kernels and event-based data, we have the freedom to vary the sample rate of the data along with the discretization step-size of the network without additional finetuning. We experimented with three event-based benchmarks and obtained state-of-the-art results on all three by large margins with significantly smaller memory and compute costs. We achieved: 1) 99.59% accuracy with 192K parameters on the DVS128 hand gesture recognition dataset and 100% with a small additional output filter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "15 pages, 3 figures"
    },
    {
        "paper id": "2405.12186",
        "abstract url": "https://arxiv.org/abs/2405.12186",
        "title": "Training Data Attribution via Approximate Unrolled Differentiation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many training data attribution (TDA) methods aim to estimate how a model's behavior would change if one or more data points were removed from the training set. Methods based on implicit differentiation, such as influence functions, can be made computationally efficient, but fail to account for underspecification, the implicit bias of the optimization algorithm, or multi-stage training pipelines. By contrast, methods based on unrolling address these issues but face scalability challenges. In this work, we connect the implicit-differentiation-based and unrolling-based approaches and combine their benefits by introducing Source, an approximate unrolling-based TDA method that is computed using an influence-function-like formula. While being computationally efficient compared to unrolling-based approaches, Source is suitable in cases where implicit-differentiation-based approaches struggle, such as in non-converged models and multi-stage training pipelines. Empirically, Source outperforms existing TDA techniques in counterfactual prediction, especially in settings where implicit-differentiation-based approaches fall short.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12200",
        "abstract url": "https://arxiv.org/abs/2405.12200",
        "title": "Multi-View Attentive Contextualization for Multi-View 3D Object Detection",
        "rating": "0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "We present Multi-View Attentive Contextualization (MvACon), a simple yet effective method for improving 2D-to-3D feature lifting in query-based multi-view 3D (MV3D) object detection. Despite remarkable progress witnessed in the field of query-based MV3D object detection, prior art often suffers from either the lack of exploiting high-resolution 2D features in dense attention-based lifting, due to high computational costs, or from insufficiently dense grounding of 3D queries to multi-scale 2D features in sparse attention-based lifting. Our proposed MvACon hits the two birds with one stone using a representationally dense yet computationally sparse attentive feature contextualization scheme that is agnostic to specific 2D-to-3D feature lifting approaches. In experiments, the proposed MvACon is thoroughly tested on the nuScenes benchmark, using both the BEVFormer and its recent 3D deformable attention (DFA3D) variant, as well as the PETR, showing consistent detection performance improvement, especially in enhancing performance in location, orientation, and velocity prediction. It is also tested on the Waymo-mini benchmark using BEVFormer with similar improvement. We qualitatively and quantitatively show that global cluster-based contexts effectively encode dense scene-level contexts for MV3D object detection. The promising results of our proposed MvACon reinforces the adage in computer vision -- ``(contextualized) feature matters\".",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR2024"
    },
    {
        "paper id": "2405.12203",
        "abstract url": "https://arxiv.org/abs/2405.12203",
        "title": "Accelerating Relative Entropy Coding with Space Partitioning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver. Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster algorithms are limited to very specific settings. This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios. We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications. Notably, our method successfully handles REC tasks with $D_{\\text{KL}}[Q||P]$ about three times what previous methods can manage and reduces the compression rate by approximately 5-15\\% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10 compared to previous methods, significantly improving the practicality of REC for neural compression.",
        "subjects": [
            "cs.IT",
            "cs.LG"
        ],
        "comment": "28 pages, 9 figures"
    },
    {
        "paper id": "2405.12205",
        "abstract url": "https://arxiv.org/abs/2405.12205",
        "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Preprint. Under review"
    },
    {
        "paper id": "2405.12207",
        "abstract url": "https://arxiv.org/abs/2405.12207",
        "title": "Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Clustering-based nearest neighbor search is a simple yet effective method in which data points are partitioned into geometric shards to form an index, and only a few shards are searched during query processing to find an approximate set of top-$k$ vectors. Even though the search efficacy is heavily influenced by the algorithm that identifies the set of shards to probe, it has received little attention in the literature. This work attempts to bridge that gap by studying the problem of routing in clustering-based maximum inner product search (MIPS). We begin by unpacking existing routing protocols and notice the surprising contribution of optimism. We then take a page from the sequential decision making literature and formalize that insight following the principle of ``optimism in the face of uncertainty.'' In particular, we present a new framework that incorporates the moments of the distribution of inner products within each shard to optimistically estimate the maximum inner product. We then present a simple instance of our algorithm that uses only the first two moments to reach the same accuracy as state-of-the-art routers such as \\scann by probing up to $50%$ fewer points on a suite of benchmark MIPS datasets. Our algorithm is also space-efficient: we design a sketch of the second moment whose size is independent of the number of points and in practice requires storing only $O(1)$ additional vectors per shard.",
        "subjects": [
            "cs.LG",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12211",
        "abstract url": "https://arxiv.org/abs/2405.12211",
        "title": "Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion",
                "synthesis",
                "Video Editing",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing. However, leveraging such pretrained models for video editing is considered a major challenge. Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features. These methods, however, struggle with strong nonrigid motion. In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images. Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices. Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pretrained T2I diffusion model to process both spatial and spatiotemporal slices. Our method generates videos that retain the structure and motion of the original video while adhering to the target text. Through extensive experiments, we demonstrate Slicedit's ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing competing methods. Webpage: https://matankleiner.github.io/slicedit/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ICML 2024. Code and examples are available at https://matankleiner.github.io/slicedit/"
    },
    {
        "paper id": "2405.12259",
        "abstract url": "https://arxiv.org/abs/2405.12259",
        "title": "Generalization Ability of Feature-based Performance Prediction Models: A Statistical Analysis across Benchmarks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study examines the generalization ability of algorithm performance prediction models across various benchmark suites. Comparing the statistical similarity between the problem collections with the accuracy of performance prediction models that are based on exploratory landscape analysis features, we observe that there is a positive correlation between these two measures. Specifically, when the high-dimensional feature value distributions between training and testing suites lack statistical significance, the model tends to generalize well, in the sense that the testing errors are in the same range as the training errors. Two experiments validate these findings: one involving the standard benchmark suites, the BBOB and CEC collections, and another using five collections of affine combinations of BBOB problem instances.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": "To appear in the Proc. of the 2024 IEEE World Congress on Computational - Congress on Evolutionary Computation"
    },
    {
        "paper id": "2405.12261",
        "abstract url": "https://arxiv.org/abs/2405.12261",
        "title": "EXACT: Towards a platform for empirically benchmarking Machine Learning model explanation methods",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The evolving landscape of explainable artificial intelligence (XAI) aims to improve the interpretability of intricate machine learning (ML) models, yet faces challenges in formalisation and empirical validation, being an inherently unsupervised process. In this paper, we bring together various benchmark datasets and novel performance metrics in an initial benchmarking platform, the Explainable AI Comparison Toolkit (EXACT), providing a standardised foundation for evaluating XAI methods. Our datasets incorporate ground truth explanations for class-conditional features, and leveraging novel quantitative metrics, this platform assesses the performance of post-hoc XAI methods in the quality of the explanations they produce. Our recent findings have highlighted the limitations of popular XAI methods, as they often struggle to surpass random baselines, attributing significance to irrelevant features. Moreover, we show the variability in explanations derived from different equally performing model architectures. This initial benchmarking platform therefore aims to allow XAI researchers to test and assure the high quality of their newly developed methods.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12264",
        "abstract url": "https://arxiv.org/abs/2405.12264",
        "title": "Directed Metric Structures arising in Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models are transformer neural networks which are trained to produce a probability distribution on the possible next words to given texts in a corpus, in such a way that the most likely word predicted is the actual word in the training text. In this paper we find what is the mathematical structure defined by such conditional probability distributions of text extensions. Changing the view point from probabilities to -log probabilities we observe that the subtext order is completely encoded in a metric structure defined on the space of texts $\\mathcal{L}$, by -log probabilities. We then construct a metric polyhedron $P(\\mathcal{L})$ and an isometric embedding (called Yoneda embedding) of $\\mathcal{L}$ into $P(\\mathcal{L})$ such that texts map to generators of certain special extremal rays. We explain that $P(\\mathcal{L})$ is a $(\\min,+)$ (tropical) linear span of these extremal ray generators. The generators also satisfy a system of $(\\min+)$ linear equations. We then show that $P(\\mathcal{L})$ is compatible with adding more text and from this we derive an approximation of a text vector as a Boltzmann weighted linear combination of the vectors for words in that text. We then prove a duality theorem showing that texts extensions and text restrictions give isometric polyhedra (even though they look a priory very different). Moreover we prove that $P(\\mathcal{L})$ is the lattice closure of (a version of) the so called, Isbell completion of $\\mathcal{L}$ which turns out to be the $(\\max,+)$ span of the text extremal ray generators. All constructions have interpretations in category theory but we don't use category theory explicitly. The categorical interpretations are briefly explained in an appendix. In the final appendix we describe how the syntax to semantics problem could fit in a general well known mathematical duality.",
        "subjects": [
            "cs.LG",
            "math.CT",
            "math.MG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12300",
        "abstract url": "https://arxiv.org/abs/2405.12300",
        "title": "Integration of Scanning Probe Microscope with High-Performance Computing: fixed-policy and reward-driven workflows implementation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapid development of computation power and machine learning algorithms has paved the way for automating scientific discovery with a scanning probe microscope (SPM). The key elements towards operationalization of automated SPM are the interface to enable SPM control from Python codes, availability of high computing power, and development of workflows for scientific discovery. Here we build a Python interface library that enables controlling an SPM from either a local computer or a remote high-performance computer (HPC), which satisfies the high computation power need of machine learning algorithms in autonomous workflows. We further introduce a general platform to abstract the operations of SPM in scientific discovery into fixed-policy or reward-driven workflows. Our work provides a full infrastructure to build automated SPM workflows for both routine operations and autonomous scientific discovery with machine learning.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.LG"
        ],
        "comment": "16 pages, 7 figures"
    },
    {
        "paper id": "2405.12326",
        "abstract url": "https://arxiv.org/abs/2405.12326",
        "title": "Overlap Number of Balls Model-Agnostic CounterFactuals (ONB-MACF): A Data-Morphology-based Counterfactual Generation Method for Trustworthy Artificial Intelligence",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Explainable Artificial Intelligence (XAI) is a pivotal research domain aimed at understanding the operational mechanisms of AI systems, particularly those considered ``black boxes'' due to their complex, opaque nature. XAI seeks to make these AI systems more understandable and trustworthy, providing insight into their decision-making processes. By producing clear and comprehensible explanations, XAI enables users, practitioners, and stakeholders to trust a model's decisions. This work analyses the value of data morphology strategies in generating counterfactual explanations. It introduces the Overlap Number of Balls Model-Agnostic CounterFactuals (ONB-MACF) method, a model-agnostic counterfactual generator that leverages data morphology to estimate a model's decision boundaries. The ONB-MACF method constructs hyperspheres in the data space whose covered points share a class, mapping the decision boundary. Counterfactuals are then generated by incrementally adjusting an instance's attributes towards the nearest alternate-class hypersphere, crossing the decision boundary with minimal modifications. By design, the ONB-MACF method generates feasible and sparse counterfactuals that follow the data distribution. Our comprehensive benchmark from a double perspective (quantitative and qualitative) shows that the ONB-MACF method outperforms existing state-of-the-art counterfactual generation methods across multiple quality metrics on diverse tabular datasets. This supports our hypothesis, showcasing the potential of data-morphology-based explainability strategies for trustworthy AI.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "21 pages, 6 figures. Submitted to Information Sciences"
    },
    {
        "paper id": "2405.12355",
        "abstract url": "https://arxiv.org/abs/2405.12355",
        "title": "Investigating the Impact of Choice on Deep Reinforcement Learning for Space Controls",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "For many space applications, traditional control methods are often used during operation. However, as the number of space assets continues to grow, autonomous operation can enable rapid development of control methods for different space related tasks. One method of developing autonomous control is Reinforcement Learning (RL), which has become increasingly popular after demonstrating promising performance and success across many complex tasks. While it is common for RL agents to learn bounded continuous control values, this may not be realistic or practical for many space tasks that traditionally prefer an on/off approach for control. This paper analyzes using discrete action spaces, where the agent must choose from a predefined list of actions. The experiments explore how the number of choices provided to the agents affects their measured performance during and after training. This analysis is conducted for an inspection task, where the agent must circumnavigate an object to inspect points on its surface, and a docking task, where the agent must move into proximity of another spacecraft and \"dock\" with a low relative speed. A common objective of both tasks, and most space tasks in general, is to minimize fuel usage, which motivates the agent to regularly choose an action that uses no fuel. Our results show that a limited number of discrete choices leads to optimal performance for the inspection task, while continuous control leads to optimal performance for the docking task.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12356",
        "abstract url": "https://arxiv.org/abs/2405.12356",
        "title": "Coarse-graining conformational dynamics with multi-dimensional generalized Langevin equation: how, when, and why",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A data-driven ab initio generalized Langevin equation (AIGLE) approach is developed to learn and simulate high-dimensional, heterogeneous, coarse-grained conformational dynamics. Constrained by the fluctuation-dissipation theorem, the approach can build coarse-grained models in dynamical consistency with all-atom molecular dynamics. We also propose practical criteria for AIGLE to enforce long-term dynamical consistency. Case studies of a toy polymer, with 20 coarse-grained sites, and the alanine dipeptide, with two dihedral angles, elucidate why one should adopt AIGLE or its Markovian limit for modeling coarse-grained conformational dynamics in practice.",
        "subjects": [
            "physics.bio-ph",
            "cs.LG",
            "physics.chem-ph",
            "physics.data-an"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12384",
        "abstract url": "https://arxiv.org/abs/2405.12384",
        "title": "Vulnerability Detection with Deep Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning has been shown to be a promising tool in detecting software vulnerabilities. In this work, we train neural networks with program slices extracted from the source code of C/C++ programs to detect software vulnerabilities. The program slices capture the syntax and semantic characteristics of vulnerability-related program constructs, including API function call, array usage, pointer usage, and arithmetic expression. To achieve a strong prediction model for both vulnerable code and non-vulnerable code, we compare different types of training data, different optimizers, and different types of neural networks. Our result shows that combining different types of characteristics of source code and using a balanced number of vulnerable program slices and non-vulnerable program slices produce a balanced accuracy in predicting both vulnerable code and non-vulnerable code. Among different neural networks, BGRU with the ADAM optimizer performs the best in detecting software vulnerabilities with an accuracy of 92.49%.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12386",
        "abstract url": "https://arxiv.org/abs/2405.12386",
        "title": "Particle swarm optimization with Applications to Maximum Likelihood Estimation and Penalized Negative Binomial Regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "General purpose optimization routines such as nlminb, optim (R) or nlmixed (SAS) are frequently used to estimate model parameters in nonstandard distributions. This paper presents Particle Swarm Optimization (PSO), as an alternative to many of the current algorithms used in statistics. We find that PSO can not only reproduce the same results as the above routines, it can also produce results that are more optimal or when others cannot converge. In the latter case, it can also identify the source of the problem or problems. We highlight advantages of using PSO using four examples, where: (1) some parameters in a generalized distribution are unidentified using PSO when it is not apparent or computationally manifested using routines in R or SAS; (2) PSO can produce estimation results for the log-binomial regressions when current routines may not; (3) PSO provides flexibility in the link function for binomial regression with LASSO penalty, which is unsupported by standard packages like GLM and GENMOD in Stata and SAS, respectively, and (4) PSO provides superior MLE estimates for an EE-IW distribution compared with those from the traditional statistical methods that rely on moments.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.AP",
            "stat.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12390",
        "abstract url": "https://arxiv.org/abs/2405.12390",
        "title": "A Metric-based Principal Curve Approach for Learning One-dimensional Manifold",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Principal curve is a well-known statistical method oriented in manifold learning using concepts from differential geometry. In this paper, we propose a novel metric-based principal curve (MPC) method that learns one-dimensional manifold of spatial data. Synthetic datasets Real applications using MNIST dataset show that our method can learn the one-dimensional manifold well in terms of the shape.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12412",
        "abstract url": "https://arxiv.org/abs/2405.12412",
        "title": "On Measuring Calibration of Discrete Probabilistic Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As machine learning systems become increasingly integrated into real-world applications, accurately representing uncertainty is crucial for enhancing their safety, robustness, and reliability. Training neural networks to fit high-dimensional probability distributions via maximum likelihood has become an effective method for uncertainty quantification. However, such models often exhibit poor calibration, leading to overconfident predictions. Traditional metrics like Expected Calibration Error (ECE) and Negative Log Likelihood (NLL) have limitations, including biases and parametric assumptions. This paper proposes a new approach using conditional kernel mean embeddings to measure calibration discrepancies without these biases and assumptions. Preliminary experiments on synthetic data demonstrate the method's potential, with future work planned for more complex applications.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12433",
        "abstract url": "https://arxiv.org/abs/2405.12433",
        "title": "LLM+Reasoning+Planning for supporting incomplete user queries in presence of APIs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent availability of Large Language Models (LLMs) has led to the development of numerous LLM-based approaches aimed at providing natural language interfaces for various end-user tasks. These end-user tasks in turn can typically be accomplished by orchestrating a given set of APIs. In practice, natural language task requests (user queries) are often incomplete, i.e., they may not contain all the information required by the APIs. While LLMs excel at natural language processing (NLP) tasks, they frequently hallucinate on missing information or struggle with orchestrating the APIs. The key idea behind our proposed approach is to leverage logical reasoning and classical AI planning along with an LLM for accurately answering user queries including identification and gathering of any missing information in these queries. Our approach uses an LLM and ASP (Answer Set Programming) solver to translate a user query to a representation in Planning Domain Definition Language (PDDL) via an intermediate representation in ASP. We introduce a special API \"get_info_api\" for gathering missing information. We model all the APIs as PDDL actions in a way that supports dataflow between the APIs. Our approach then uses a classical AI planner to generate an orchestration of API calls (including calls to get_info_api) to answer the user query. Our evaluation results show that our approach significantly outperforms a pure LLM based approach by achieving over 95\\% success rate in most cases on a dataset containing complete and incomplete single goal and multi-goal queries where the multi-goal queries may or may not require dataflow among the APIs.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "9 pages main content, 2 pages references, 12 pages appendix, 5 figures"
    },
    {
        "paper id": "2405.12439",
        "abstract url": "https://arxiv.org/abs/2405.12439",
        "title": "No-Regret M${}^{\\natural}$-Concave Function Maximization: Stochastic Bandit Algorithms and NP-Hardness of Adversarial Full-Information Setting",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "M${}^{\\natural}$-concave functions, a.k.a. gross substitute valuation functions, play a fundamental role in many fields, including discrete mathematics and economics. In practice, perfect knowledge of M${}^{\\natural}$-concave functions is often unavailable a priori, and we can optimize them only interactively based on some feedback. Motivated by such situations, we study online M${}^{\\natural}$-concave function maximization problems, which are interactive versions of the problem studied by Murota and Shioura (1999). For the stochastic bandit setting, we present $O(T^{-1/2})$-simple regret and $O(T^{2/3})$-regret algorithms under $T$ times access to unbiased noisy value oracles of M${}^{\\natural}$-concave functions. A key to proving these results is the robustness of the greedy algorithm to local errors in M${}^{\\natural}$-concave function maximization, which is one of our main technical results. While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting. We prove that, even with full-information feedback, no algorithms that run in polynomial time per round can achieve $O(T^{1-c})$ regret for any constant $c > 0$ unless $\\mathsf{P} = \\mathsf{NP}$. Our proof is based on a reduction from the matroid intersection problem for three matroids, which would be a novel idea in the context of online learning.",
        "subjects": [
            "cs.LG",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12443",
        "abstract url": "https://arxiv.org/abs/2405.12443",
        "title": "FFCL: Forward-Forward Net with Cortical Loops, Training and Inference on Edge Without Backpropagation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Forward-Forward Learning (FFL) algorithm is a recently proposed solution for training neural networks without needing memory-intensive backpropagation. During training, labels accompany input data, classifying them as positive or negative inputs. Each layer learns its response to these inputs independently. In this study, we enhance the FFL with the following contributions: 1) We optimize label processing by segregating label and feature forwarding between layers, enhancing learning performance. 2) By revising label integration, we enhance the inference process, reduce computational complexity, and improve performance. 3) We introduce feedback loops akin to cortical loops in the brain, where information cycles through and returns to earlier neurons, enabling layers to combine complex features from previous layers with lower-level features, enhancing learning efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted at the Great Lakes Symposium on VLSI 2024"
    },
    {
        "paper id": "2405.12450",
        "abstract url": "https://arxiv.org/abs/2405.12450",
        "title": "PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": "Accepted to be published in the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge)"
    },
    {
        "paper id": "2405.12458",
        "abstract url": "https://arxiv.org/abs/2405.12458",
        "title": "Studying Up Public Sector AI: How Networks of Power Relations Shape Agency Decisions Around AI Design and Use",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As public sector agencies rapidly introduce new AI tools in high-stakes domains like social services, it becomes critical to understand how decisions to adopt these tools are made in practice. We borrow from the anthropological practice to ``study up'' those in positions of power, and reorient our study of public sector AI around those who have the power and responsibility to make decisions about the role that AI tools will play in their agency. Through semi-structured interviews and design activities with 16 agency decision-makers, we examine how decisions about AI design and adoption are influenced by their interactions with and assumptions about other actors within these agencies (e.g., frontline workers and agency leaders), as well as those above (legal systems and contracted companies), and below (impacted communities). By centering these networks of power relations, our findings shed light on how infrastructural, legal, and social factors create barriers and disincentives to the involvement of a broader range of stakeholders in decisions about AI design and adoption. Agency decision-makers desired more practical support for stakeholder involvement around public sector AI to help overcome the knowledge and power differentials they perceived between them and other stakeholders (e.g., frontline workers and impacted community members). Building on these findings, we discuss implications for future research and policy around actualizing participatory AI approaches in public sector contexts.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12490",
        "abstract url": "https://arxiv.org/abs/2405.12490",
        "title": "Customize Your Own Paired Data via Few-shot Way",
        "rating": "0.5",
        "keywords": [
            [
                "diffusion",
                "image editing"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Existing solutions to image editing tasks suffer from several issues. Though achieving remarkably satisfying generated results, some supervised methods require huge amounts of paired training data, which greatly limits their usages. The other unsupervised methods take full advantage of large-scale pre-trained priors, thus being strictly restricted to the domains where the priors are trained on and behaving badly in out-of-distribution cases. The task we focus on is how to enable the users to customize their desired effects through only few image pairs. In our proposed framework, a novel few-shot learning mechanism based on the directional transformations among samples is introduced and expands the learnable space exponentially. Adopting a diffusion model pipeline, we redesign the condition calculating modules in our model and apply several technical improvements. Experimental results demonstrate the capabilities of our method in various cases.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by AI4CC CVPR2024 WorkShop"
    },
    {
        "paper id": "2405.12493",
        "abstract url": "https://arxiv.org/abs/2405.12493",
        "title": "Visualizing, Rethinking, and Mining the Loss Landscape of Deep Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The loss landscape of deep neural networks (DNNs) is commonly considered complex and wildly fluctuated. However, an interesting observation is that the loss surfaces plotted along Gaussian noise directions are almost v-basin ones with the perturbed model lying on the basin. This motivates us to rethink whether the 1D or 2D subspace could cover more complex local geometry structures, and how to mine the corresponding perturbation directions. This paper systematically and gradually categorizes the 1D curves from simple to complex, including v-basin, v-side, w-basin, w-peak, and vvv-basin curves. Notably, the latter two types are already hard to obtain via the intuitive construction of specific perturbation directions, and we need to propose proper mining algorithms to plot the corresponding 1D curves. Combining these 1D directions, various types of 2D surfaces are visualized such as the saddle surfaces and the bottom of a bottle of wine that are only shown by demo functions in previous works. Finally, we propose theoretical insights from the lens of the Hessian matrix to explain the observed several interesting phenomena.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11794",
        "abstract url": "https://arxiv.org/abs/2405.11794",
        "title": "ViViD: Video Virtual Try-on using Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "synthesis",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video virtual try-on aims to transfer a clothing item onto the video of a target person. Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results. In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on. Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism. To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis. Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date. Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results. The dataset, codes, and weights will be publicly available. Project page: https://becauseimbatman0.github.io/ViViD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11801",
        "abstract url": "https://arxiv.org/abs/2405.11801",
        "title": "LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Graph clustering is a fundamental problem in machine learning. Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers. Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number. We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information). In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features. In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results. By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster structure. DSI is also theoretically presented as a new graph clustering objective, not requiring the predefined cluster number. Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution. Extensive empirical results on real graphs show the superiority of our approach.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by ICML24, 26 pages"
    },
    {
        "paper id": "2405.11823",
        "abstract url": "https://arxiv.org/abs/2405.11823",
        "title": "Stereo-Knowledge Distillation from dpMV to Dual Pixels for Light Field Video Reconstruction",
        "rating": "0",
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "3D"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Dual pixels contain disparity cues arising from the defocus blur. This disparity information is useful for many vision tasks ranging from autonomous driving to 3D creative realism. However, directly estimating disparity from dual pixels is less accurate. This work hypothesizes that distilling high-precision dark stereo knowledge, implicitly or explicitly, to efficient dual-pixel student networks enables faithful reconstructions. This dark knowledge distillation should also alleviate stereo-synchronization setup and calibration costs while dramatically increasing parameter and inference time efficiency. We collect the first and largest 3-view dual-pixel video dataset, dpMV, to validate our explicit dark knowledge distillation hypothesis. We show that these methods outperform purely monocular solutions, especially in challenging foreground-background separation regions using faithful guidance from dual pixels. Finally, we demonstrate an unconventional use case unlocked by dpMV and implicit dark knowledge distillation from an ensemble of teachers for Light Field (LF) video reconstruction. Our LF video reconstruction method is the fastest and most temporally consistent to date. It remains competitive in reconstruction fidelity while offering many other essential properties like high parameter efficiency, implicit disocclusion handling, zero-shot cross-dataset transfer, geometrically consistent inference on higher spatial-angular resolutions, and adaptive baseline control. All source code is available at the anonymous repository https://github.com/Aryan-Garg.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "International Conference of Computational Photography (ICCP 2024), 11 pages and 12 figures"
    },
    {
        "paper id": "2405.11829",
        "abstract url": "https://arxiv.org/abs/2405.11829",
        "title": "Adversarially Diversified Rehearsal Memory (ADRM): Mitigating Memory Overfitting Challenge in Continual Learning",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Continual learning focuses on learning non-stationary data distribution without forgetting previous knowledge. Rehearsal-based approaches are commonly used to combat catastrophic forgetting. However, these approaches suffer from a problem called \"rehearsal memory overfitting, \" where the model becomes too specialized on limited memory samples and loses its ability to generalize effectively. As a result, the effectiveness of the rehearsal memory progressively decays, ultimately resulting in catastrophic forgetting of the learned tasks. We introduce the Adversarially Diversified Rehearsal Memory (ADRM) to address the memory overfitting challenge. This novel method is designed to enrich memory sample diversity and bolster resistance against natural and adversarial noise disruptions. ADRM employs the FGSM attacks to introduce adversarially modified memory samples, achieving two primary objectives: enhancing memory diversity and fostering a robust response to continual feature drifts in memory samples. Our contributions are as follows: Firstly, ADRM addresses overfitting in rehearsal memory by employing FGSM to diversify and increase the complexity of the memory buffer. Secondly, we demonstrate that ADRM mitigates memory overfitting and significantly improves the robustness of CL models, which is crucial for safety-critical applications. Finally, our detailed analysis of features and visualization demonstrates that ADRM mitigates feature drifts in CL memory samples, significantly reducing catastrophic forgetting and resulting in a more resilient CL model. Additionally, our in-depth t-SNE visualizations of feature distribution and the quantification of the feature similarity further enrich our understanding of feature representation in existing CL approaches. Our code is publically available at https://github.com/hikmatkhan/ADRM.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11852",
        "abstract url": "https://arxiv.org/abs/2405.11852",
        "title": "Evolving Storytelling: Benchmarks and Methods for New Character Customization with Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks. However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data. Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results. To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story. The refined dataset involves refined text prompts and eliminates character leakage. Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics. EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details. Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models. In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11904",
        "abstract url": "https://arxiv.org/abs/2405.11904",
        "title": "A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Text classifiers are vulnerable to adversarial examples -- correctly-classified examples that are deliberately transformed to be misclassified while satisfying acceptability constraints. The conventional approach to finding adversarial examples is to define and solve a combinatorial optimisation problem over a space of allowable transformations. While effective, this approach is slow and limited by the choice of transformations. An alternate approach is to directly generate adversarial examples by fine-tuning a pre-trained language model, as is commonly done for other text-to-text tasks. This approach promises to be much quicker and more expressive, but is relatively unexplored. For this reason, in this work we train an encoder-decoder paraphrase model to generate a diverse range of adversarial examples. For training, we adopt a reinforcement learning algorithm and propose a constraint-enforcing reward that promotes the generation of valid adversarial examples. Experimental results over two text classification datasets show that our model has achieved a higher success rate than the original paraphrase model, and overall has proved more effective than other competitive attacks. Finally, we show how key design choices impact the generated examples and discuss the strengths and weaknesses of the proposed approach.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11993",
        "abstract url": "https://arxiv.org/abs/2405.11993",
        "title": "GGAvatar: Geometric Adjustment of Gaussian Head Avatar",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Avatar"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula's limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 5 figures"
    },
    {
        "paper id": "2405.12006",
        "abstract url": "https://arxiv.org/abs/2405.12006",
        "title": "Depth Reconstruction with Neural Signed Distance Fields in Structured Light Systems",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Depth",
                "radiance fields",
                "Signed Distance Fields",
                "SDF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce a novel depth estimation technique for multi-frame structured light setups using neural implicit representations of 3D space. Our approach employs a neural signed distance field (SDF), trained through self-supervised differentiable rendering. Unlike passive vision, where joint estimation of radiance and geometry fields is necessary, we capitalize on known radiance fields from projected patterns in structured light systems. This enables isolated optimization of the geometry field, ensuring convergence and network efficacy with fixed device positioning. To enhance geometric fidelity, we incorporate an additional color loss based on object surfaces during training. Real-world experiments demonstrate our method's superiority in geometric performance for few-shot scenarios, while achieving comparable results with increased pattern availability.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 8 figures, accepted by 3DV 2024"
    },
    {
        "paper id": "2405.12035",
        "abstract url": "https://arxiv.org/abs/2405.12035",
        "title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems. LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially. Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12057",
        "abstract url": "https://arxiv.org/abs/2405.12057",
        "title": "NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work we present a novel multi-view photometric stereo (PS) method. Like many works in 3D reconstruction we are leveraging neural shape representations and learnt renderers. However, our work differs from the state-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we explicity leverage per-pixel intensity renderings rather than relying mainly on estimated normals. We model point light attenuation and explicitly raytrace cast shadows in order to best approximate each points incoming radiance. This is used as input to a fully neural material renderer that uses minimal prior assumptions and it is jointly optimised with the surface. Finally, estimated normal and segmentation maps can also incorporated in order to maximise the surface accuracy. Our method is among the first to outperform the classical approach of DiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away with approximate 400x400 resolution. Moreover, we show robustness to poor normals in low light count scenario, achieving 0.27mm Chamfer distance when pixel rendering is used instead of estimated normals.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12069",
        "abstract url": "https://arxiv.org/abs/2405.12069",
        "title": "Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://gaussian-head-shoulders.netlify.app/"
    },
    {
        "paper id": "2405.12221",
        "abstract url": "https://arxiv.org/abs/2405.12221",
        "title": "Images that Sound: Composing Images and Sounds on a Single Canvas",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "synthesize",
                "text-to-image"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Project site: https://ificl.github.io/images-that-sound/"
    },
    {
        "paper id": "2405.12255",
        "abstract url": "https://arxiv.org/abs/2405.12255",
        "title": "Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography",
        "rating": "0",
        "keywords": [
            [
                "Vision Language",
                "VLM"
            ],
            [
                "Diagnosis",
                "cancer"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The lack of large and diverse training data on Computer-Aided Diagnosis (CAD) in breast cancer detection has been one of the concerns that impedes the adoption of the system. Recently, pre-training with large-scale image text datasets via Vision-Language models (VLM) (\\eg CLIP) partially addresses the issue of robustness and data efficiency in computer vision (CV). This paper proposes Mammo-CLIP, the first VLM pre-trained on a substantial amount of screening mammogram-report pairs, addressing the challenges of dataset diversity and size. Our experiments on two public datasets demonstrate strong performance in classifying and localizing various mammographic attributes crucial for breast cancer detection, showcasing data efficiency and robustness similar to CLIP in CV. We also propose Mammo-FActOR, a novel feature attribution method, to provide spatial interpretation of representation with sentence-level granularity within mammography reports. Code is available publicly: \\url{https://github.com/batmanlab/Mammo-CLIP}.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "MICCAI 2024, early accept, top 11%"
    },
    {
        "paper id": "2405.12363",
        "abstract url": "https://arxiv.org/abs/2405.12363",
        "title": "Question-Based Retrieval using Atomic Units for Enterprise RAG",
        "rating": "0",
        "keywords": [
            [
                "synthesizer"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Enterprise retrieval augmented generation (RAG) offers a highly flexible framework for combining powerful large language models (LLMs) with internal, possibly temporally changing, documents. In RAG, documents are first chunked. Relevant chunks are then retrieved for a specific user query, which are passed as context to a synthesizer LLM to generate the query response. However, the retrieval step can limit performance, as incorrect chunks can lead the synthesizer LLM to generate a false response. This work proposes a zero-shot adaptation of standard dense retrieval steps for more accurate chunk recall. Specifically, a chunk is first decomposed into atomic statements. A set of synthetic questions are then generated on these atoms (with the chunk as the context). Dense retrieval involves finding the closest set of synthetic questions, and associated chunks, to the user query. It is found that retrieval with the atoms leads to higher recall than retrieval with chunks. Further performance gain is observed with retrieval using the synthetic questions generated over the atoms. Higher recall at the retrieval step enables higher performance of the enterprise LLM using the RAG pipeline.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages, 2 figures, 3 tables"
    },
    {
        "paper id": "2405.12398",
        "abstract url": "https://arxiv.org/abs/2405.12398",
        "title": "ASMR: Activation-sharing Multi-resolution Coordinate Networks For Efficient Inference",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Coordinate network or implicit neural representation (INR) is a fast-emerging method for encoding natural signals (such as images and videos) with the benefits of a compact neural representation. While numerous methods have been proposed to increase the encoding capabilities of an INR, an often overlooked aspect is the inference efficiency, usually measured in multiply-accumulate (MAC) count. This is particularly critical in use cases where inference throughput is greatly limited by hardware constraints. To this end, we propose the Activation-Sharing Multi-Resolution (ASMR) coordinate network that combines multi-resolution coordinate decomposition with hierarchical modulations. Specifically, an ASMR model enables the sharing of activations across grids of the data. This largely decouples its inference cost from its depth which is directly correlated to its reconstruction capability, and renders a near O(1) inference complexity irrespective of the number of layers. Experiments show that ASMR can reduce the MAC of a vanilla SIREN model by up to 500x while achieving an even higher reconstruction quality than its SIREN baseline.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICLR 2024 (v3: 21 pages, 11 figures, Project Page: https://github.com/stevolopolis/asmr.git)"
    },
    {
        "paper id": "2405.12399",
        "abstract url": "https://arxiv.org/abs/2405.12399",
        "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "25 pages, 11 figures, 10 tables"
    },
    {
        "paper id": "2405.12419",
        "abstract url": "https://arxiv.org/abs/2405.12419",
        "title": "GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised Point Cloud Learning in 3D",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We introduce a pioneering approach to self-supervised learning for point clouds, employing a geometrically informed mask selection strategy called GeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE). Unlike the conventional method of random masking, our technique utilizes a teacher-student model to focus on intricate areas within the data, guiding the model's focus toward regions with higher geometric complexity. This strategy is grounded in the hypothesis that concentrating on harder patches yields a more robust feature representation, as evidenced by the improved performance on downstream tasks. Our method also presents a complete-to-partial feature-level knowledge distillation technique designed to guide the prediction of geometric complexity utilizing a comprehensive context from feature-level information. Extensive experiments confirm our method's superiority over State-Of-The-Art (SOTA) baselines, demonstrating marked improvements in classification, and few-shot tasks.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12421",
        "abstract url": "https://arxiv.org/abs/2405.12421",
        "title": "A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human Feedback (RLHF) are pivotal methodologies in reward learning, which involve inferring and shaping the underlying reward function of sequential decision-making problems based on observed human demonstrations and feedback. Most prior work in reward learning has relied on prior knowledge or assumptions about decision or preference models, potentially leading to robustness issues. In response, this paper introduces a novel linear programming (LP) framework tailored for offline reward learning. Utilizing pre-collected trajectories without online exploration, this framework estimates a feasible reward set from the primal-dual optimality conditions of a suitably designed LP, and offers an optimality guarantee with provable sample efficiency. Our LP framework also enables aligning the reward functions with human feedback, such as pairwise trajectory comparison data, while maintaining computational tractability and sample efficiency. We demonstrate that our framework potentially achieves better performance compared to the conventional maximum likelihood estimation (MLE) approach through analytical examples and numerical experiments.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "ICML 2024"
    },
    {
        "paper id": "2405.12456",
        "abstract url": "https://arxiv.org/abs/2405.12456",
        "title": "Mutual Information Analysis in Multimodal Learning Systems",
        "rating": "0",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "3D"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In recent years, there has been a significant increase in applications of multimodal signal processing and analysis, largely driven by the increased availability of multimodal datasets and the rapid progress in multimodal learning systems. Well-known examples include autonomous vehicles, audiovisual generative systems, vision-language systems, and so on. Such systems integrate multiple signal modalities: text, speech, images, video, LiDAR, etc., to perform various tasks. A key issue for understanding such systems is the relationship between various modalities and how it impacts task performance. In this paper, we employ the concept of mutual information (MI) to gain insight into this issue. Taking advantage of the recent progress in entropy modeling and estimation, we develop a system called InfoMeter to estimate MI between modalities in a multimodal learning system. We then apply InfoMeter to analyze a multimodal 3D object detection system over a large-scale dataset for autonomous driving. Our experiments on this system suggest that a lower MI between modalities is beneficial for detection accuracy. This new insight may facilitate improvements in the development of future multimodal learning systems.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "6 pages, 7 figures, IEEE MIPR 2024"
    },
    {
        "paper id": "2405.11784",
        "abstract url": "https://arxiv.org/abs/2405.11784",
        "title": "Reward-Punishment Reinforcement Learning with Maximum Entropy",
        "rating": "-0.5",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We introduce the ``soft Deep MaxPain'' (softDMP) algorithm, which integrates the optimization of long-term policy entropy into reward-punishment reinforcement learning objectives. Our motivation is to facilitate a smoother variation of operators utilized in the updating of action values beyond traditional ``max'' and ``min'' operators, where the goal is enhancing sample efficiency and robustness. We also address two unresolved issues from the previous Deep MaxPain method. Firstly, we investigate how the negated (``flipped'') pain-seeking sub-policy, derived from the punishment action value, collaborates with the ``min'' operator to effectively learn the punishment module and how softDMP's smooth learning operator provides insights into the ``flipping'' trick. Secondly, we tackle the challenge of data collection for learning the punishment module to mitigate inconsistencies arising from the involvement of the ``flipped'' sub-policy (pain-avoidance sub-policy) in the unified behavior policy. We empirically explore the first issue in two discrete Markov Decision Process (MDP) environments, elucidating the crucial advancements of the DMP approach and the necessity for soft treatments on the hard operators. For the second issue, we propose a probabilistic classifier based on the ratio of the pain-seeking sub-policy to the sum of the pain-seeking and goal-reaching sub-policies. This classifier assigns roll-outs to separate replay buffers for updating reward and punishment action-value functions, respectively. Our framework demonstrates superior performance in Turtlebot 3's maze navigation tasks under the ROS Gazebo simulation.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "comment": "IJCNN2024"
    },
    {
        "paper id": "2405.11800",
        "abstract url": "https://arxiv.org/abs/2405.11800",
        "title": "Generative AI in Higher Education: A Global Perspective of Institutional Adoption Policies and Guidelines",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Integrating generative AI (GAI) into higher education is crucial for preparing a future generation of GAI-literate students. Yet a thorough understanding of the global institutional adoption policy remains absent, with most of the prior studies focused on the Global North and the promises and challenges of GAI, lacking a theoretical lens. This study utilizes the Diffusion of Innovations Theory to examine GAI adoption strategies in higher education across 40 universities from six global regions. It explores the characteristics of GAI innovation, including compatibility, trialability, and observability, and analyses the communication channels and roles and responsibilities outlined in university policies and guidelines. The findings reveal a proactive approach by universities towards GAI integration, emphasizing academic integrity, teaching and learning enhancement, and equity. Despite a cautious yet optimistic stance, a comprehensive policy framework is needed to evaluate the impacts of GAI integration and establish effective communication strategies that foster broader stakeholder engagement. The study highlights the importance of clear roles and responsibilities among faculty, students, and administrators for successful GAI integration, supporting a collaborative model for navigating the complexities of GAI in education. This study contributes insights for policymakers in crafting detailed strategies for its integration.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11811",
        "abstract url": "https://arxiv.org/abs/2405.11811",
        "title": "FedCAda: Adaptive Client-Side Optimization for Accelerated and Stable Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) has emerged as a prominent approach for collaborative training of machine learning models across distributed clients while preserving data privacy. However, the quest to balance acceleration and stability becomes a significant challenge in FL, especially on the client-side. In this paper, we introduce FedCAda, an innovative federated client adaptive algorithm designed to tackle this challenge. FedCAda leverages the Adam algorithm to adjust the correction process of the first moment estimate $m$ and the second moment estimate $v$ on the client-side and aggregate adaptive algorithm parameters on the server-side, aiming to accelerate convergence speed and communication efficiency while ensuring stability and performance. Additionally, we investigate several algorithms incorporating different adjustment functions. This comparative analysis revealed that due to the limited information contained within client models from other clients during the initial stages of federated learning, more substantial constraints need to be imposed on the parameters of the adaptive algorithm. As federated learning progresses and clients gather more global information, FedCAda gradually diminishes the impact on adaptive parameters. These findings provide insights for enhancing the robustness and efficiency of algorithmic improvements. Through extensive experiments on computer vision (CV) and natural language processing (NLP) datasets, we demonstrate that FedCAda outperforms the state-of-the-art methods in terms of adaptability, convergence, stability, and overall performance. This work contributes to adaptive algorithms for federated learning, encouraging further exploration.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11868",
        "abstract url": "https://arxiv.org/abs/2405.11868",
        "title": "Towards Graph Contrastive Learning: A Survey and Beyond",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "In recent years, deep learning on graphs has achieved remarkable success in various domains. However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature. To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress. SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data. While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature. Thus, this survey aims to fill this gap by offering a dedicated survey on GCL. We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives. Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios. We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CE",
            "cs.IR",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11881",
        "abstract url": "https://arxiv.org/abs/2405.11881",
        "title": "Out-of-Distribution Detection with a Single Unconditional Diffusion Model",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Out-of-distribution (OOD) detection is a critical task in machine learning that seeks to identify abnormal samples. Traditionally, unsupervised methods utilize a deep generative model for OOD detection. However, such approaches necessitate a different model when evaluating abnormality against a new distribution. With the emergence of foundational generative models, this paper explores whether a single generalist model can also perform OOD detection across diverse tasks. To that end, we introduce our method, Diffusion Paths, (DiffPath) in this work. DiffPath proposes to utilize a single diffusion model originally trained to perform unconditional generation for OOD detection. Specifically, we introduce a novel technique of measuring the rate-of-change and curvature of the diffusion paths connecting samples to the standard normal. Extensive experiments show that with a single model, DiffPath outperforms prior work on a variety of OOD tasks involving different distributions. Our code is publicly available at https://github.com/clear-nus/diffpath.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11884",
        "abstract url": "https://arxiv.org/abs/2405.11884",
        "title": "Vertical Federated Learning Hybrid Local Pre-training",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Vertical Federated Learning (VFL), which has a broad range of real-world applications, has received much attention in both academia and industry. Enterprises aspire to exploit more valuable features of the same users from diverse departments to boost their model prediction skills. VFL addresses this demand and concurrently secures individual parties from exposing their raw data. However, conventional VFL encounters a bottleneck as it only leverages aligned samples, whose size shrinks with more parties involved, resulting in data scarcity and the waste of unaligned data. To address this problem, we propose a novel VFL Hybrid Local Pre-training (VFLHLP) approach. VFLHLP first pre-trains local networks on the local data of participating parties. Then it utilizes these pre-trained networks to adjust the sub-model for the labeled party or enhance representation learning for other parties during downstream federated learning on aligned data, boosting the performance of federated models. The experimental results on real-world advertising datasets, demonstrate that our approach achieves the best performance over baseline methods by large margins. The ablation study further illustrates the contribution of each technique in VFLHLP to its overall performance.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11911",
        "abstract url": "https://arxiv.org/abs/2405.11911",
        "title": "PULL: PU-Learning-based Accurate Link Prediction",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Given an edge-incomplete graph, how can we accurately find the missing links? The link prediction in edge-incomplete graphs aims to discover the missing relations between entities when their relationships are represented as a graph. Edge-incomplete graphs are prevalent in real-world due to practical limitations, such as not checking all users when adding friends in a social network. Addressing the problem is crucial for various tasks, including recommending friends in social networks and finding references in citation networks. However, previous approaches rely heavily on the given edge-incomplete (observed) graph, making it challenging to consider the missing (unobserved) links during training. In this paper, we propose PULL (PU-Learning-based Link predictor), an accurate link prediction method based on the positive-unlabeled (PU) learning. PULL treats the observed edges in the training graph as positive examples, and the unconnected node pairs as unlabeled ones. PULL effectively prevents the link predictor from overfitting to the observed graph by proposing latent variables for every edge, and leveraging the expected graph structure with respect to the variables. Extensive experiments on five real-world datasets show that PULL consistently outperforms the baselines for predicting links in edge-incomplete graphs.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.SI"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2405.11916",
        "abstract url": "https://arxiv.org/abs/2405.11916",
        "title": "Information Leakage from Embedding in Large Language Models",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The widespread adoption of large language models (LLMs) has raised concerns regarding data privacy. This study aims to investigate the potential for privacy invasion through input reconstruction attacks, in which a malicious model provider could potentially recover user inputs from embeddings. We first propose two base methods to reconstruct original texts from a model's hidden states. We find that these two methods are effective in attacking the embeddings from shallow layers, but their effectiveness decreases when attacking embeddings from deeper layers. To address this issue, we then present Embed Parrot, a Transformer-based method, to reconstruct input from embeddings in deep layers. Our analysis reveals that Embed Parrot effectively reconstructs original inputs from the hidden states of ChatGLM-6B and Llama2-7B, showcasing stable performance across various token lengths and data distributions. To mitigate the risk of privacy breaches, we introduce a defense mechanism to deter exploitation of the embedding reconstruction process. Our findings emphasize the importance of safeguarding user privacy in distributed learning systems and contribute valuable insights to enhance the security protocols within such environments.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11930",
        "abstract url": "https://arxiv.org/abs/2405.11930",
        "title": "Data Contamination Calibration for Black-box LLMs",
        "rating": "-0.5",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) -- from machine learning community -- by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11951",
        "abstract url": "https://arxiv.org/abs/2405.11951",
        "title": "Distinguished In Uniform: Self Attention Vs. Virtual Nodes",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph Transformers (GTs) such as SAN and GPS are graph processing models that combine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were shown to be universal function approximators, with two reservations: 1. The initial node features must be augmented with certain positional encodings. 2. The approximation is non-uniform: Graphs of different sizes may require a different approximating network. We first clarify that this form of universality is not unique to GTs: Using the same positional encodings, also pure MPGNNs and even 2-layer MLPs are non-uniform universal approximators. We then consider uniform expressivity: The target function is to be approximated by a single network for graphs of all sizes. There, we compare GTs to the more efficient MPGNN + Virtual Node architecture. The essential difference between the two model definitions is in their global computation method -- Self-Attention Vs Virtual Node. We prove that none of the models is a uniform-universal approximator, before proving our main result: Neither model's uniform expressivity subsumes the other's. We demonstrate the theory with experiments on synthetic data. We further augment our study with real-world datasets, observing mixed results which indicate no clear ranking in practice as well.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11968",
        "abstract url": "https://arxiv.org/abs/2405.11968",
        "title": "Conditional Shift-Robust Conformal Prediction for Graph Neural Network",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) have emerged as potent tools for predicting outcomes in graph-structured data. Despite their efficacy, a significant drawback of GNNs lies in their limited ability to provide robust uncertainty estimates, posing challenges to their reliability in contexts where errors carry significant consequences. Moreover, GNNs typically excel in in-distribution settings, assuming that training and test data follow identical distributions: a condition often unmet in real-world graph data scenarios. In this article, we leverage conformal prediction, a widely recognized statistical technique for quantifying uncertainty by transforming predictive model outputs into prediction sets, to address uncertainty quantification in GNN predictions amidst conditional shift \\footnote{Representing the change in conditional probability distribution $P(label |input)$ from source domain to target domain.} in graph-based semi-supervised learning (SSL). Additionally, we propose a novel loss function aimed at refining model predictions by minimizing conditional shift in latent stages. Termed Conditional Shift Robust (CondSR) conformal prediction for GNNs, our approach CondSR is model-agnostic and adaptable to various classification models. We validate the effectiveness of our method on standard graph benchmark datasets, integrating it with state-of-the-art GNNs in node classification tasks. The code implementation is publicly available for further exploration and experimentation.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "11 pages, 4 figures"
    },
    {
        "paper id": "2405.12046",
        "abstract url": "https://arxiv.org/abs/2405.12046",
        "title": "Energy-Efficient Federated Edge Learning with Streaming Data: A Lyapunov Optimization Approach",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) has received significant attention in recent years for its advantages in efficient training of machine learning models across distributed clients without disclosing user-sensitive data. Specifically, in federated edge learning (FEEL) systems, the time-varying nature of wireless channels introduces inevitable system dynamics in the communication process, thereby affecting training latency and energy consumption. In this work, we further consider a streaming data scenario where new training data samples are randomly generated over time at edge devices. Our goal is to develop a dynamic scheduling and resource allocation algorithm to address the inherent randomness in data arrivals and resource availability under long-term energy constraints. To achieve this, we formulate a stochastic network optimization problem and use the Lyapunov drift-plus-penalty framework to obtain a dynamic resource management design. Our proposed algorithm makes adaptive decisions on device scheduling, computational capacity adjustment, and allocation of bandwidth and transmit power in every round. We provide convergence analysis for the considered setting with heterogeneous data and time-varying objective functions, which supports the rationale behind our proposed scheduling design. The effectiveness of our scheme is verified through simulation results, demonstrating improved learning performance and energy efficiency as compared to baseline schemes.",
        "subjects": [
            "cs.LG",
            "cs.DC",
            "cs.IT",
            "eess.SP"
        ],
        "comment": "Submitted to IEEE journals for possible publication"
    },
    {
        "paper id": "2405.12094",
        "abstract url": "https://arxiv.org/abs/2405.12094",
        "title": "Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?",
        "rating": "-0.5",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL), yet it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba in offline RL (dubbed DeMa) from the aspect of data structures and network architectures with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements due to the fact that DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify that the hidden attention mechanism is key to its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations from eight Atari games demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous state-of-the-art methods, outdoing Decision Transformer (DT) by 80\\% with 30\\% fewer parameters, and exceeds DT in MuJoCo with only a quarter of the parameters.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "20 pages, 8 figures"
    },
    {
        "paper id": "2405.12096",
        "abstract url": "https://arxiv.org/abs/2405.12096",
        "title": "PATE: Proximity-Aware Time series anomaly Evaluation",
        "rating": "-0.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Evaluating anomaly detection algorithms in time series data is critical as inaccuracies can lead to flawed decision-making in various domains where real-time analytics and data-driven strategies are essential. Traditional performance metrics assume iid data and fail to capture the complex temporal dynamics and specific characteristics of time series anomalies, such as early and delayed detections. We introduce Proximity-Aware Time series anomaly Evaluation (PATE), a novel evaluation metric that incorporates the temporal relationship between prediction and anomaly intervals. PATE uses proximity-based weighting considering buffer zones around anomaly intervals, enabling a more detailed and informed assessment of a detection. Using these weights, PATE computes a weighted version of the area under the Precision and Recall curve. Our experiments with synthetic and real-world datasets show the superiority of PATE in providing more sensible and accurate evaluations than other evaluation metrics. We also tested several state-of-the-art anomaly detectors across various benchmark datasets using the PATE evaluation scheme. The results show that a common metric like Point-Adjusted F1 Score fails to characterize the detection performances well, and that PATE is able to provide a more fair model comparison. By introducing PATE, we redefine the understanding of model efficacy that steers future studies toward developing more effective and accurate detection models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2024), Research Track. (Preprint version)"
    },
    {
        "paper id": "2405.12183",
        "abstract url": "https://arxiv.org/abs/2405.12183",
        "title": "Multi-order Graph Clustering with Adaptive Node-level Weight Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Current graph clustering methods emphasize individual node and edge con nections, while ignoring higher-order organization at the level of motif. Re cently, higher-order graph clustering approaches have been designed by motif based hypergraphs. However, these approaches often suffer from hypergraph fragmentation issue seriously, which degrades the clustering performance greatly. Moreover, real-world graphs usually contain diverse motifs, with nodes participating in multiple motifs. A key challenge is how to achieve precise clustering results by integrating information from multiple motifs at the node level. In this paper, we propose a multi-order graph clustering model (MOGC) to integrate multiple higher-order structures and edge connections at node level. MOGC employs an adaptive weight learning mechanism to au tomatically adjust the contributions of different motifs for each node. This not only tackles hypergraph fragmentation issue but enhances clustering accuracy. MOGC is efficiently solved by an alternating minimization algo rithm. Experiments on seven real-world datasets illustrate the effectiveness of MOGC.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12213",
        "abstract url": "https://arxiv.org/abs/2405.12213",
        "title": "Octo: An Open-Source Generalist Robot Policy",
        "rating": "-0.5",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.",
        "subjects": [
            "cs.RO",
            "cs.LG"
        ],
        "comment": "Project website: https://octo-models.github.io"
    },
    {
        "paper id": "2405.12262",
        "abstract url": "https://arxiv.org/abs/2405.12262",
        "title": "Prompt Learning for Generalized Vehicle Routing",
        "rating": "-0.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Neural combinatorial optimization (NCO) is a promising learning-based approach to solving various vehicle routing problems without much manual algorithm design. However, the current NCO methods mainly focus on the in-distribution performance, while the real-world problem instances usually come from different distributions. A costly fine-tuning approach or generalized model retraining from scratch could be needed to tackle the out-of-distribution instances. Unlike the existing methods, this work investigates an efficient prompt learning approach in NCO for cross-distribution adaptation. To be concrete, we propose a novel prompt learning method to facilitate fast zero-shot adaptation of a pre-trained model to solve routing problem instances from different distributions. The proposed model learns a set of prompts among various distributions and then selects the best-matched one to prompt a pre-trained attention model for each problem instance. Extensive experiments show that the proposed prompt learning approach facilitates the fast adaptation of pre-trained routing models. It also outperforms existing generalized models on both in-distribution prediction and zero-shot generalization to a diverse set of new tasks. Our code implementation is available online https://github.com/FeiLiu36/PromptVRP.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12354",
        "abstract url": "https://arxiv.org/abs/2405.12354",
        "title": "A Study on Optimization Techniques for Variational Quantum Circuits in Reinforcement Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "parameter efficiency"
            ],
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Quantum Computing aims to streamline machine learning, making it more effective with fewer trainable parameters. This reduction of parameters can speed up the learning process and reduce the use of computational resources. However, in the current phase of quantum computing development, known as the noisy intermediate-scale quantum era (NISQ), learning is difficult due to a limited number of qubits and widespread quantum noise. To overcome these challenges, researchers are focusing on variational quantum circuits (VQCs). VQCs are hybrid algorithms that merge a quantum circuit, which can be adjusted through parameters, with traditional classical optimization techniques. These circuits require only few qubits for effective learning. Recent studies have presented new ways of applying VQCs to reinforcement learning, showing promising results that warrant further exploration. This study investigates the effects of various techniques -- data re-uploading, input scaling, output scaling -- and introduces exponential learning rate decay in the quantum proximal policy optimization algorithm's actor-VQC. We assess these methods in the popular Frozen Lake and Cart Pole environments. Our focus is on their ability to reduce the number of parameters in the VQC without losing effectiveness. Our findings indicate that data re-uploading and an exponential learning rate decay significantly enhance hyperparameter stability and overall performance. While input scaling does not improve parameter efficiency, output scaling effectively manages greediness, leading to increased learning speed and robustness.",
        "subjects": [
            "quant-ph",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at QSW 2024"
    },
    {
        "paper id": "2405.12380",
        "abstract url": "https://arxiv.org/abs/2405.12380",
        "title": "Large scale scattering using fast solvers based on neural operators",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We extend a recently proposed machine-learning-based iterative solver, i.e. the hybrid iterative transferable solver (HINTS), to solve the scattering problem described by the Helmholtz equation in an exterior domain with a complex absorbing boundary condition. The HINTS method combines neural operators (NOs) with standard iterative solvers, e.g. Jacobi and Gauss-Seidel (GS), to achieve better performance by leveraging the spectral bias of neural networks. In HINTS, some iterations of the conventional iterative method are replaced by inferences of the pre-trained NO. In this work, we employ HINTS to solve the scattering problem for both 2D and 3D problems, where the standard iterative solver fails. We consider square and triangular scatterers of various sizes in 2D, and a cube and a model submarine in 3D. We explore and illustrate the extrapolation capability of HINTS in handling diverse geometries of the scatterer, which is achieved by training the NO on non-scattering scenarios and then deploying it in HINTS to solve scattering problems. The accurate results demonstrate that the NO in HINTS method remains effective without retraining or fine-tuning it whenever a new scatterer is given. Taken together, our results highlight the adaptability and versatility of the extended HINTS methodology in addressing diverse scattering problems.",
        "subjects": [
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12391",
        "abstract url": "https://arxiv.org/abs/2405.12391",
        "title": "Automated Anomaly Detection on European XFEL Klystrons",
        "rating": "-0.5",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "High-power multi-beam klystrons represent a key component to amplify RF to generate the accelerating field of the superconducting radio frequency (SRF) cavities at European XFEL. Exchanging these high-power components takes time and effort, thus it is necessary to minimize maintenance and downtime and at the same time maximize the device's operation. In an attempt to explore the behavior of klystrons using machine learning, we completed a series of experiments on our klystrons to determine various operational modes and conduct feature extraction and dimensionality reduction to extract the most valuable information about a normal operation. To analyze recorded data we used state-of-the-art data-driven learning techniques and recognized the most promising components that might help us better understand klystron operational states and identify early on possible faults or anomalies.",
        "subjects": [
            "physics.acc-ph",
            "cs.AI"
        ],
        "comment": "4 pages, 4 figures, 15, 15TH International Particle Accelerator Conference"
    },
    {
        "paper id": "2405.12459",
        "abstract url": "https://arxiv.org/abs/2405.12459",
        "title": "PLM4Traj: Cognizing Movement Patterns and Travel Purposes from Trajectories with Pre-trained Language Models",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Spatio-temporal trajectories play a vital role in various spatio-temporal data mining tasks. Developing a versatile trajectory learning approach that can adapt to different tasks while ensuring high accuracy is crucial. This requires effectively extracting movement patterns and travel purposes embedded in trajectories. However, this task is challenging due to limitations in the size and quality of available trajectory datasets. On the other hand, pre-trained language models (PLMs) have shown great success in adapting to different tasks by training on large-scale, high-quality corpus datasets. Given the similarities between trajectories and sentences, there is potential in leveraging PLMs to enhance the development of a versatile and effective trajectory learning method. Nevertheless, vanilla PLMs are not tailored to handle the unique spatio-temporal features present in trajectories and lack the capability to extract movement patterns and travel purposes from them. To overcome these obstacles, we propose a model called PLM4Traj that effectively utilizes PLMs to model trajectories. PLM4Traj leverages the strengths of PLMs to create a versatile trajectory learning approach while addressing the limitations of vanilla PLMs in modeling trajectories. Firstly, PLM4Traj incorporates a novel trajectory semantic embedder that enables PLMs to process spatio-temporal features in trajectories and extract movement patterns and travel purposes from them. Secondly, PLM4Traj introduces a novel trajectory prompt that integrates movement patterns and travel purposes into PLMs, while also allowing the model to adapt to various tasks. Extensive experiments conducted on two real-world datasets and two representative tasks demonstrate that PLM4Traj successfully achieves its design goals. Codes are available at https://github.com/Zeru19/PLM4Traj.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12463",
        "abstract url": "https://arxiv.org/abs/2405.12463",
        "title": "Stochastic Learning of Computational Resource Usage as Graph Structured Multimarginal Schr\u00f6dinger Bridge",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We propose to learn the time-varying stochastic computational resource usage of software as a graph structured Schr\u00f6dinger bridge problem. In general, learning the computational resource usage from data is challenging because resources such as the number of CPU instructions and the number of last level cache requests are both time-varying and statistically correlated. Our proposed method enables learning the joint time-varying stochasticity in computational resource usage from the measured profile snapshots in a nonparametric manner. The method can be used to predict the most-likely time-varying distribution of computational resource availability at a desired time. We provide detailed algorithms for stochastic learning in both single and multi-core cases, discuss the convergence guarantees, computational complexities, and demonstrate their practical use in two case studies: a single-core nonlinear model predictive controller, and a synthetic multi-core software.",
        "subjects": [
            "math.OC",
            "cs.AI",
            "cs.LG",
            "eess.SY",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12474",
        "abstract url": "https://arxiv.org/abs/2405.12474",
        "title": "How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert prohibitive computations, numerous polynomial filters have been proposed. However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily. Addressing this gap, we demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses. Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph. We integrate this heterophily basis with the homophily basis to construct a universal polynomial basis UniBasis, which devises a polynomial filter based graph neural network - UniFilter. It optimizes the convolution and propagation in GNN, thus effectively limiting over-smoothing and alleviating over-squashing. Our extensive experiments, conducted on a diverse range of real-world and synthetic datasets with varying degrees of heterophily, support the superiority of UniFilter. These results not only demonstrate the universality of UniBasis but also highlight its proficiency in graph explanation.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2311.18177"
    },
    {
        "paper id": "2405.12489",
        "abstract url": "https://arxiv.org/abs/2405.12489",
        "title": "Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Exploring the loss landscape offers insights into the inherent principles of deep neural networks (DNNs). Recent work suggests an additional asymmetry of the valley beyond the flat and sharp ones, yet without thoroughly examining its causes or implications. Our study methodically explores the factors affecting the symmetry of DNN valleys, encompassing (1) the dataset, network architecture, initialization, and hyperparameters that influence the convergence point; and (2) the magnitude and direction of the noise for 1D visualization. Our major observation shows that the {\\it degree of sign consistency} between the noise and the convergence point is a critical indicator of valley symmetry. Theoretical insights from the aspects of ReLU activation and softmax function could explain the interesting phenomenon. Our discovery propels novel understanding and applications in the scenario of Model Fusion: (1) the efficacy of interpolating separate models significantly correlates with their sign consistency ratio, and (2) imposing sign alignment during federated learning emerges as an innovative approach for model parameter alignment.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11783",
        "abstract url": "https://arxiv.org/abs/2405.11783",
        "title": "Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing",
        "rating": "-1",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 150 hypothetical MOF structures consisting of 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $H_{2}$ uptake values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 85.7% and 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "quant-ph"
        ],
        "comment": "45 pages, 7 figures, 6 supplementary figures, 1 table, 1 supplementary table"
    },
    {
        "paper id": "2405.11791",
        "abstract url": "https://arxiv.org/abs/2405.11791",
        "title": "CaseGNN++: Graph Contrastive Learning for Legal Case Retrieval with Graph Augmentation",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Legal case retrieval (LCR) is a specialised information retrieval task that aims to find relevant cases to a given query case. LCR holds pivotal significance in facilitating legal practitioners in finding precedents. Most of existing LCR methods are based on traditional lexical models and language models, which have gained promising performance in retrieval. However, the domain-specific structural information inherent in legal documents is yet to be exploited to further improve the performance. Our previous work CaseGNN successfully harnesses text-attributed graphs and graph neural networks to address the problem of legal structural information neglect. Nonetheless, there remain two aspects for further investigation: (1) The underutilization of rich edge information within text-attributed case graphs limits CaseGNN to generate informative case representation. (2) The inadequacy of labelled data in legal datasets hinders the training of CaseGNN model. In this paper, CaseGNN++, which is extended from CaseGNN, is proposed to simultaneously leverage the edge information and additional label data to discover the latent potential of LCR models. Specifically, an edge feature-based graph attention layer (EUGAT) is proposed to comprehensively update node and edge features during graph modelling, resulting in a full utilisation of structural information of legal cases. Moreover, a novel graph contrastive learning objective with graph augmentation is developed in CaseGNN++ to provide additional training signals, thereby enhancing the legal comprehension capabilities of CaseGNN++ model. Extensive experiments on two benchmark datasets from COLIEE 2022 and COLIEE 2023 demonstrate that CaseGNN++ not only significantly improves CaseGNN but also achieves supreme performance compared to state-of-the-art LCR methods. Code has been released on https://github.com/yanran-tang/CaseGNN.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2312.11229"
    },
    {
        "paper id": "2405.11793",
        "abstract url": "https://arxiv.org/abs/2405.11793",
        "title": "MM-Retinal: Knowledge-Enhanced Foundational Pretraining with Fundus Image-Text Expertise",
        "rating": "-1",
        "keywords": [
            [
                "Retinal"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Current fundus image analysis models are predominantly built for specific tasks relying on individual datasets. The learning process is usually based on data-driven paradigm without prior knowledge, resulting in poor transferability and generalizability. To address this issue, we propose MM-Retinal, a multi-modal dataset that encompasses high-quality image-text pairs collected from professional fundus diagram books. Moreover, enabled by MM-Retinal, we present a novel Knowledge-enhanced foundational pretraining model which incorporates Fundus Image-Text expertise, called KeepFIT. It is designed with image similarity-guided text revision and mixed training strategy to infuse expert knowledge. Our proposed fundus foundation model achieves state-of-the-art performance across six unseen downstream tasks and holds excellent generalization ability in zero-shot and few-shot scenarios. MM-Retinal and KeepFIT are available at https://github.com/lxirich/MM-Retinal.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Early Accepted by The International Conference on Medical Image Computing and Computer Assisted Intervention(MICCAI)2024"
    },
    {
        "paper id": "2405.11798",
        "abstract url": "https://arxiv.org/abs/2405.11798",
        "title": "Self-Supervised Learning of Visual Servoing for Low-Rigidity Robots Considering Temporal Body Changes",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "In this study, we investigate object grasping by visual servoing in a low-rigidity robot. It is difficult for a low-rigidity robot to handle its own body as intended compared to a rigid robot, and calibration between vision and body takes some time. In addition, the robot must constantly adapt to changes in its body, such as the change in camera position and change in joints due to aging. Therefore, we develop a method for a low-rigidity robot to autonomously learn visual servoing of its body. We also develop a mechanism that can adaptively change its visual servoing according to temporal body changes. We apply our method to a low-rigidity 6-axis arm, MyCobot, and confirm its effectiveness by conducting object grasping experiments based on visual servoing.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at IEEE Robotics and Automation Letters"
    },
    {
        "paper id": "2405.11817",
        "abstract url": "https://arxiv.org/abs/2405.11817",
        "title": "Systematic Review on Healthcare Systems Engineering utilizing ChatGPT",
        "rating": "-1",
        "keywords": [
            [
                "Healthcare"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents an analytical framework for conducting academic reviews in the field of Healthcare Systems Engineering, employing ChatGPT, a state-of-the-art tool among recent language models. We utilized 9,809 abstract paragraphs from conference presentations to systematically review the field. The framework comprises distinct analytical processes, each employing tailored prompts and the systematic use of the ChatGPT API. Through this framework, we organized the target field into 11 topic categories and conducted a comprehensive analysis covering quantitative yearly trends and detailed sub-categories. This effort explores the potential for leveraging ChatGPT to alleviate the burden of academic reviews. Furthermore, it provides valuable insights into the dynamic landscape of Healthcare Systems Engineering research.",
        "subjects": [
            "cs.ET",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11846",
        "abstract url": "https://arxiv.org/abs/2405.11846",
        "title": "EPPS: Advanced Polyp Segmentation via Edge Information Injection and Selective Feature Decoupling",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "cancer"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate segmentation of polyps in colonoscopy images is essential for early-stage diagnosis and management of colorectal cancer. Despite advancements in deep learning for polyp segmentation, enduring limitations persist. The edges of polyps are typically ambiguous, making them difficult to discern from the background, and the model performance is often compromised by the influence of irrelevant or unimportant features. To alleviate these challenges, we propose a novel model named Edge-Prioritized Polyp Segmentation (EPPS). Specifically, we incorporate an Edge Mapping Engine (EME) aimed at accurately extracting the edges of polyps. Subsequently, an Edge Information Injector (EII) is devised to augment the mask prediction by injecting the captured edge information into Decoder blocks. Furthermore, we introduce a component called Selective Feature Decoupler (SFD) to suppress the influence of noise and extraneous features on the model. Extensive experiments on 3 widely used polyp segmentation benchmarks demonstrate the superior performance of our method compared with other state-of-the-art approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11855",
        "abstract url": "https://arxiv.org/abs/2405.11855",
        "title": "Salience-guided Ground Factor for Robust Localization of Delivery Robots in Complex Urban Environments",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "In urban environments for delivery robots, particularly in areas such as campuses and towns, many custom features defy standard road semantic categorizations. Addressing this challenge, our paper introduces a method leveraging Salient Object Detection (SOD) to extract these unique features, employing them as pivotal factors for enhanced robot loop closure and localization. Traditional geometric feature-based localization is hampered by fluctuating illumination and appearance changes. Our preference for SOD over semantic segmentation sidesteps the intricacies of classifying a myriad of non-standardized urban features. To achieve consistent ground features, the Motion Compensate IPM (MC-IPM) technique is implemented, capitalizing on motion for distortion compensation and subsequently selecting the most pertinent salient ground features through moment computations. For thorough evaluation, we validated the saliency detection and localization performances to the real urban scenarios. Project page: https://sites.google.com/view/salient-ground-feature/home.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 9 figures, 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)"
    },
    {
        "paper id": "2405.11856",
        "abstract url": "https://arxiv.org/abs/2405.11856",
        "title": "Modeling and simulation of a mechanism for suppressing the flipping problem of a jumping robot",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "In order to solve the problem of stable jumping of micro robot, we design a special mechanism: elastic passive joint (EPJ). EPJ can assist in achieving smooth jumping through the opening-closing process when the robot jumps. First, we introduce the composition and operation principle of EPJ, and perform a dynamic modeling of the robot's jumping process. Then, in order to verify the effectiveness of EPJ in controlling the robot's smooth jump, we design a simulation experiment based on MATLAB. Through comparative experiments, it was proved that EPJ can greatly adjust the angular velocity of the robot and increase the jump distance of the robot. Finally, we analyze each parameter in EPJ and performs parameter optimization. After optimization, EPJ achieves a completely flip-free jump of the robot, laying an important foundation for improving the mobility of micro-robot.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11883",
        "abstract url": "https://arxiv.org/abs/2405.11883",
        "title": "Asynchronous MIMO-OFDM Massive Unsourced Random Access with Codeword Collisions",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "This paper investigates asynchronous MIMO massive unsourced random access in an orthogonal frequency division multiplexing (OFDM) system over frequency-selective fading channels, with the presence of both timing and carrier frequency offsets (TO and CFO) and non-negligible codeword collisions. The proposed coding framework segregates the data into two components, namely, preamble and coding parts, with the former being tree-coded and the latter LDPC-coded. By leveraging the dual sparsity of the equivalent channel across both codeword and delay domains (CD and DD), we develop a message passing-based sparse Bayesian learning algorithm, combined with belief propagation and mean field, to iteratively estimate DD channel responses, TO, and delay profiles. Furthermore, we establish a novel graph-based algorithm to iteratively separate the superimposed channels and compensate for the phase rotations. Additionally, the proposed algorithm is applied to the flat fading scenario to estimate both TO and CFO, where the channel and offset estimation is enhanced by leveraging the geometric characteristics of the signal constellation. Simulations reveal that the proposed algorithm achieves superior performance and substantial complexity reduction in both channel and offset estimation compared to the codebook enlarging-based counterparts, and enhanced data recovery performances compared to state-of-the-art URA schemes.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "13 pages, 12 figures, submitted to the IEEE for possible publication"
    },
    {
        "paper id": "2405.11903",
        "abstract url": "https://arxiv.org/abs/2405.11903",
        "title": "A comprehensive overview of deep learning techniques for 3D point cloud classification and semantic segmentation",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Point cloud analysis has a wide range of applications in many areas such as computer vision, robotic manipulation, and autonomous driving. While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unordered, irregular and noisy 3D points. To stimulate future research, this paper analyzes recent progress in deep learning methods employed for point cloud processing and presents challenges and potential directions to advance this field. It serves as a comprehensive review on two major tasks in 3D point cloud processing-- namely, 3D shape classification and semantic segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Published in Springer Nature (Machine Vision and Applications)"
    },
    {
        "paper id": "2405.11914",
        "abstract url": "https://arxiv.org/abs/2405.11914",
        "title": "PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single Highly-Ambiguous RGB Images",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating 3D shapes from single RGB images is essential in various applications such as robotics. Current approaches typically target images containing clear and complete visual descriptions of the object, without considering common realistic cases where observations of objects that are largely occluded or truncated. We thus propose a transformer-based autoregressive model to generate the probabilistic distribution of 3D shapes conditioned on an RGB image containing potentially highly ambiguous observations of the object. To handle realistic scenarios such as occlusion or field-of-view truncation, we create simulated image-to-shape training pairs that enable improved fine-tuning for real-world scenarios. We then adopt cross-attention to effectively identify the most relevant region of interest from the input image for shape generation. This enables inference of sampled shapes with reasonable diversity and strong alignment with the input image. We train and test our model on our synthetic data then fine-tune and test it on real-world data. Experiments demonstrate that our model outperforms state of the art in both scenarios",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2405.11921",
        "abstract url": "https://arxiv.org/abs/2405.11921",
        "title": "MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: https://mirror-gaussian.github.io/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11935",
        "abstract url": "https://arxiv.org/abs/2405.11935",
        "title": "A Flat Dual-Polarized Millimeter-Wave Luneburg Lens Antenna Using Transformation Optics with Reduced Anisotropy and Impedance Mismatch",
        "rating": "-1",
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "In this paper, a compact wideband dual-polarized Luneburg lens antenna (LLA) with reduced anisotropy and improved impedance matching is proposed in Ka band with a wide 2D beamscanning capability. Based on transformation optics, the spherical Luneburg lens is compressed into a cylindrical one, while the merits of high gain, broad band, wide scanning, and free polarization are preserved. A trigonometric function is employed to the material property of the flattened Luneburg lens with reduced anisotropy, thus effectively alleviates the strong reflection, the high sidelobes and back radiation with a free cost on the antenna weight and volume. Furthermore, a light thin wideband 7-by-1 metasurface phased array is studied as the primary feed for the LLA. The proposed metantenna, shorted for metamaterial-based antenna, has a high potential for B5G, future wireless communication and radar sensing as an onboard system.",
        "subjects": [
            "eess.SY",
            "physics.app-ph",
            "physics.optics"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11941",
        "abstract url": "https://arxiv.org/abs/2405.11941",
        "title": "Biomedical Entity Linking for Dutch: Fine-tuning a Self-alignment BERT Model on an Automatically Generated Wikipedia Corpus",
        "rating": "-1",
        "keywords": [
            [
                "Biomedical",
                "health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Biomedical entity linking, a main component in automatic information extraction from health-related texts, plays a pivotal role in connecting textual entities (such as diseases, drugs and body parts mentioned by patients) to their corresponding concepts in a structured biomedical knowledge base. The task remains challenging despite recent developments in natural language processing. This paper presents the first evaluated biomedical entity linking model for the Dutch language. We use MedRoBERTa.nl as base model and perform second-phase pretraining through self-alignment on a Dutch biomedical ontology extracted from the UMLS and Dutch SNOMED. We derive a corpus from Wikipedia of ontology-linked Dutch biomedical entities in context and fine-tune our model on this dataset. We evaluate our model on the Dutch portion of the Mantra GSC-corpus and achieve 54.7% classification accuracy and 69.8% 1-distance accuracy. We then perform a case study on a collection of unlabeled, patient-support forum data and show that our model is hampered by the limited quality of the preceding entity recognition step. Manual evaluation of small sample indicates that of the correctly extracted entities, around 65% is linked to the correct concept in the ontology. Our results indicate that biomedical entity linking in a language other than English remains challenging, but our Dutch model can be used to for high-level analysis of patient-generated text.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Published in the CL4Health workshop on Patient-oriented language processing @ LREC-COLING 2024"
    },
    {
        "paper id": "2405.11950",
        "abstract url": "https://arxiv.org/abs/2405.11950",
        "title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles",
        "rating": "-1",
        "keywords": [
            [
                "BioLaySumm"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics. Few-shot learning notably improved the models' ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed. Out of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by approx. 5.5 percentage points and was only approx 1.5 percentage points behind the first place.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "4 pages, 6 figure, 3 tables, submitted to: BIONLP 2024 and Shared Tasks @ ACL 2024"
    },
    {
        "paper id": "2405.11983",
        "abstract url": "https://arxiv.org/abs/2405.11983",
        "title": "A review on the use of large language models as virtual tutors",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Transformer architectures contribute to managing long-term dependencies for Natural Language Processing, representing one of the most recent changes in the field. These architectures are the basis of the innovative, cutting-edge Large Language Models (LLMs) that have produced a huge buzz in several fields and industrial sectors, among the ones education stands out. Accordingly, these generative Artificial Intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents, along with network infrastructure, towards high-quality learning. Given the popularity of LLMs, this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan. To the best of our knowledge, this is the first review of educational applications (e.g., student assessment) of LLMs. As expected, the most common role of these systems is as virtual tutors for automatic question generation. Moreover, the most popular models are GTP-3 and BERT. However, due to the continuous launch of new generative models, new works are expected to be published shortly.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11988",
        "abstract url": "https://arxiv.org/abs/2405.11988",
        "title": "DuckDB-SGX2: The Good, The Bad and The Ugly within Confidential Analytical Query Processing",
        "rating": "-1",
        "keywords": [
            [
                "flight"
            ]
        ],
        "abstract": "We provide an evaluation of an analytical workload in a confidential computing environment, combining DuckDB with two technologies: modular columnar encryption in Parquet files (data at rest) and the newest version of the Intel SGX Trusted Execution Environment (TEE), providing a hardware enclave where data in flight can be (more) securely decrypted and processed. One finding is that the \"performance tax\" for such confidential analytical processing is acceptable compared to not using these technologies. We eventually manage to run TPC-H SF30 with under 2x overhead compared to non-encrypted, non-enclave execution; we show that, specifically, columnar compression and encryption are a good combination. Our second finding consists of dos and don'ts to tune DuckDB to work effectively in this environment. There are various performance hazards: potentially 5x higher cache miss costs due to memory encryption inside the enclave, NUMA penalties, and highly elevated cost of swapping pages in and out of the enclave -- which is also triggered indirectly by using a non-SGX-aware malloc library.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11995",
        "abstract url": "https://arxiv.org/abs/2405.11995",
        "title": "Safe by Design Autonomous Driving Systems",
        "rating": "-1",
        "keywords": [
            [
                "Autonomous Driving"
            ]
        ],
        "abstract": "Developing safe autonomous driving systems is a major scientific and technical challenge. Existing AI-based end-to-end solutions do not offer the necessary safety guarantees, while traditional systems engineering approaches are defeated by the complexity of the problem. Currently, there is an increasing interest in hybrid design solutions, integrating machine learning components, when necessary, while using model-based components for goal management and planning. We study a method for building safe by design autonomous driving systems, based on the assumption that the capability to drive boils down to the coordinated execution of a given set of driving operations. The assumption is substantiated by a compositionality result considering that autopilots are dynamic systems receiving a small number of types of vistas as input, each vista defining a free space in its neighborhood. It is shown that safe driving for each type of vista in the corresponding free space, implies safe driving for any possible scenario under some easy-to-check conditions concerning the transition between vistas. The designed autopilot comprises distinct control policies one per type of vista, articulated in two consecutive phases. The first phase consists of carefully managing a potentially risky situation by virtually reducing speed, while the second phase consists of exiting the situation by accelerating. The autopilots designed use for their predictions simple functions characterizing the acceleration and deceleration capabilities of the vehicles. They cover the main driving operations, including entering a main road, overtaking, crossing intersections protected by traffic lights or signals, and driving on freeways. The results presented reinforce the case for hybrid solutions that incorporate mathematically elegant and robust decision methods that are safe by design.",
        "subjects": [
            "cs.MA"
        ],
        "comment": "32 pages, 13 figures"
    },
    {
        "paper id": "2405.12003",
        "abstract url": "https://arxiv.org/abs/2405.12003",
        "title": "Mamba-in-Mamba: Centralized Mamba-Cross-Scan in Tokenized Mamba Model for Hyperspectral Image Classification",
        "rating": "-1",
        "keywords": [
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Hyperspectral image (HSI) classification is pivotal in the remote sensing (RS) field, particularly with the advancement of deep learning techniques. Sequential models, adapted from the natural language processing (NLP) field such as Recurrent Neural Networks (RNNs) and Transformers, have been tailored to this task, offering a unique viewpoint. However, several challenges persist 1) RNNs struggle with centric feature aggregation and are sensitive to interfering pixels, 2) Transformers require significant computational resources and often underperform with limited HSI training samples, and 3) Current scanning methods for converting images into sequence-data are simplistic and inefficient. In response, this study introduces the innovative Mamba-in-Mamba (MiM) architecture for HSI classification, the first attempt of deploying State Space Model (SSM) in this task. The MiM model includes 1) A novel centralized Mamba-Cross-Scan (MCS) mechanism for transforming images into sequence-data, 2) A Tokenized Mamba (T-Mamba) encoder that incorporates a Gaussian Decay Mask (GDM), a Semantic Token Learner (STL), and a Semantic Token Fuser (STF) for enhanced feature generation and concentration, and 3) A Weighted MCS Fusion (WMF) module coupled with a Multi-Scale Loss Design to improve decoding efficiency. Experimental results from three public HSI datasets with fixed and disjoint training-testing samples demonstrate that our method outperforms existing baselines and state-of-the-art approaches, highlighting its efficacy and potential in HSI applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "19 pages, 16 figures,"
    },
    {
        "paper id": "2405.12010",
        "abstract url": "https://arxiv.org/abs/2405.12010",
        "title": "DarkDNS: Revisiting the Value of Rapid Zone Update",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Malicious actors exploit the DNS namespace to launch spam campaigns, phishing attacks, malware, and other harmful activities. Combating these threats requires visibility into domain existence, ownership and nameservice activity that the DNS protocol does not itself provide. To facilitate visibility and security-related study of the expanding gTLD namespace, ICANN introduced the Centralized Zone Data Service (CZDS) that shares daily zone file snapshots of new gTLD zones. However, a remarkably high concentration of malicious activity is associated with domains that do not live long enough make it into these daily snapshots. Using public and private sources of newly observed domains to identify this activity, we discover that even with the best available data there is a considerable visibility gap. We find that the daily snapshots miss at least 1% of newly registered and short-lived domains, which are almost always registered with malicious intent. In reducing this critical visibility gap using public sources of data, we demonstrate how more timely access to TLD zone changes can help better prevent abuse. We hope that this work sparks a discussion in the community on how to effectively and safely revive the concept of sharing Rapid Zone Updates for security research.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12018",
        "abstract url": "https://arxiv.org/abs/2405.12018",
        "title": "Continuous Sign Language Recognition with Adapted Conformer via Unsupervised Pretraining",
        "rating": "-1",
        "keywords": [
            [
                "Sign Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Conventional Deep Learning frameworks for continuous sign language recognition (CSLR) are comprised of a single or multi-modal feature extractor, a sequence-learning module, and a decoder for outputting the glosses. The sequence learning module is a crucial part wherein transformers have demonstrated their efficacy in the sequence-to-sequence tasks. Analyzing the research progress in the field of Natural Language Processing and Speech Recognition, a rapid introduction of various transformer variants is observed. However, in the realm of sign language, experimentation in the sequence learning component is limited. In this work, the state-of-the-art Conformer model for Speech Recognition is adapted for CSLR and the proposed model is termed ConSignformer. This marks the first instance of employing Conformer for a vision-based task. ConSignformer has bimodal pipeline of CNN as feature extractor and Conformer for sequence learning. For improved context learning we also introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRA into the model, it becomes more adept at learning and utilizing complex relationships within the data. To further enhance the Conformer model, unsupervised pretraining called Regressional Feature Extraction is conducted on a curated sign language dataset. The pretrained Conformer is then fine-tuned for the downstream recognition task. The experimental results confirm the effectiveness of the adopted pretraining strategy and demonstrate how CMRA contributes to the recognition process. Remarkably, leveraging a Conformer-based backbone, our model achieves state-of-the-art performance on the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12021",
        "abstract url": "https://arxiv.org/abs/2405.12021",
        "title": "Can AI Relate: Testing Large Language Model Response for Mental Health Support",
        "rating": "-1",
        "keywords": [
            [
                "Health",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are already being piloted for clinical use in hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed deployment use case is psychotherapy, where a LLM-powered chatbot can treat a patient undergoing a mental health crisis. Deployment of LLMs for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care. However, recent high-profile failures, like damaging dieting advice offered by the Tessa chatbot to patients with eating disorders, have led to doubt about their reliability in high-stakes and safety-critical settings. In this work, we develop an evaluation framework for determining whether LLM response is a viable and ethical path forward for the automation of mental health treatment. Using human evaluation with trained clinicians and automatic quality-of-care metrics grounded in psychology research, we compare the responses provided by peer-to-peer responders to those provided by a state-of-the-art LLM. We show that LLMs like GPT-4 use implicit and explicit cues to infer patient demographics like race. We then show that there are statistically significant discrepancies between patient subgroups: Responses to Black posters consistently have lower empathy than for any other demographic group (2%-13% lower than the control group). Promisingly, we do find that the manner in which responses are generated significantly impacts the quality of the response. We conclude by proposing safety guidelines for the potential deployment of LLMs for mental health response.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2405.12070",
        "abstract url": "https://arxiv.org/abs/2405.12070",
        "title": "AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Image understanding is a foundational task in computer vision, with recent applications emerging in soccer posture analysis. However, existing publicly available datasets lack comprehensive information, notably in the form of posture sequences and 2D pose annotations. Moreover, current analysis models often rely on interpretable linear models (e.g., PCA and regression), limiting their capacity to capture non-linear spatiotemporal relationships in complex and diverse scenarios. To address these gaps, we introduce the 3D Shot Posture (3DSP) dataset in soccer broadcast videos, which represents the most extensive sports image dataset with 2D pose annotations to our knowledge. Additionally, we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear approach for embedding pose sequences. Furthermore, we propose AutoSoccerPose, a pipeline aimed at semi-automating 2D and 3D pose estimation and posture analysis. While achieving full automation proved challenging, we provide a foundational baseline, extending its utility beyond the scope of annotated data. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present posture analysis results based on 3DSP. The dataset, code, and models are available at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12098",
        "abstract url": "https://arxiv.org/abs/2405.12098",
        "title": "Using Unsupervised Learning to Explore Robot-Pedestrian Interactions in Urban Environments",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "This study identifies a gap in data-driven approaches to robot-centric pedestrian interactions and proposes a corresponding pipeline. The pipeline utilizes unsupervised learning techniques to identify patterns in interaction data of urban environments, specifically focusing on conflict scenarios. Analyzed features include the robot's and pedestrian's speed and contextual parameters such as proximity to intersections. They are extracted and reduced in dimensionality using Principal Component Analysis (PCA). Finally, K-means clustering is employed to uncover underlying patterns in the interaction data. A use case application of the pipeline is presented, utilizing real-world robot mission data from a mid-sized German city. The results indicate the need for enriching interaction representations with contextual information to enable fine-grained analysis and reasoning. Nevertheless, they also highlight the need for expanding the data set and incorporating additional contextual factors to enhance the robots situational awareness and interaction quality.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12104",
        "abstract url": "https://arxiv.org/abs/2405.12104",
        "title": "Deciding branching hyperproperties for real time systems",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Security properties of real-time systems often involve reasoning about hyper-properties, as opposed to properties of single executions or trees of executions. These hyper-properties need to additionally be expressive enough to reason about real-time constraints. Examples of such properties include information flow, side channel attacks and service-level agreements. In this paper we study computational problems related to a branching-time, hyper-property extension of metric temporal logic (MTL) that we call HCMTL*. We consider both the interval-based and point-based semantics of this logic. The verification problem that we consider is to determine if a given HCMTL* formula $\\varphi$ is true in a system represented by a timed automaton. We show that this problem is undecidable. We then show that the verification problem is decidable if we consider executions upto a fixed time horizon $T$. Our decidability result relies on reducing the verification problem to the truth of an MSO formula over reals with a bounded time interval.",
        "subjects": [
            "cs.CR",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12110",
        "abstract url": "https://arxiv.org/abs/2405.12110",
        "title": "CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "radiance fields"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting the reconstruction quality. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields with the same sparse views of a scene, we observe that the two radiance fields exhibit \\textit{point disagreement} and \\textit{rendering disagreement} that can unsupervisedly predict reconstruction quality, stemming from the sampling implementation in densification. We further quantify the point disagreement and rendering disagreement by evaluating the registration between Gaussians' point representations and calculating differences in their rendered pixels. The empirical study demonstrates the negative correlation between the two disagreements and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (\\romannumeral1) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (\\romannumeral2) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurately rendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://jiaw-z.github.io/CoR-GS/"
    },
    {
        "paper id": "2405.12114",
        "abstract url": "https://arxiv.org/abs/2405.12114",
        "title": "A New Cross-Space Total Variation Regularization Model for Color Image Restoration with Quaternion Blur Operator",
        "rating": "-1",
        "keywords": [
            [
                "Image Restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The cross-channel deblurring problem in color image processing is difficult to solve due to the complex coupling and structural blurring of color pixels. Until now, there are few efficient algorithms that can reduce color infection in deblurring process. To solve this challenging problem, we present a novel cross-space total variation (CSTV) regularization model for color image deblurring by introducing a quaternion blur operator and a cross-color space regularization functional. The existence and uniqueness of the solution is proved and a new L-curve method is proposed to find a sweet balance of regularization functionals on different color spaces. The Euler-Lagrange equation is derived to show that CSTV has taken into account the coupling of all color channels and the local smoothing within each color channel. A quaternion operator splitting method is firstly proposed to enhance the ability of color infection reduction of the CSTV regularization model. This strategy also applies to the well-known color deblurring models. Numerical experiments on color image databases illustrate the efficiency and manoeuvrability of the new model and algorithms. The color images restored by them successfully maintain the color and spatial information and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000 than the restorations of the-state-of-the-art methods.",
        "subjects": [
            "cs.CV",
            "math.NA"
        ],
        "comment": "15pages,10figures"
    },
    {
        "paper id": "2405.12119",
        "abstract url": "https://arxiv.org/abs/2405.12119",
        "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation",
        "rating": "-1",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are revolutionizing conversational recommender systems by adeptly indexing item content, understanding complex conversational contexts, and generating relevant item titles. However, controlling the distribution of recommended items remains a challenge. This leads to suboptimal performance due to the failure to capture rapidly changing data distributions, such as item popularity, on targeted conversational recommendation platforms. In conversational recommendation, LLMs recommend items by generating the titles (as multiple tokens) autoregressively, making it difficult to obtain and control the recommendations over all items. Thus, we propose a Reindex-Then-Adapt (RTA) framework, which converts multi-token item titles into single tokens within LLMs, and then adjusts the probability distributions over these single-token item titles accordingly. The RTA framework marries the benefits of both LLMs and traditional recommender systems (RecSys): understanding complex queries as LLMs do; while efficiently controlling the recommended item distributions in conversational recommendations as traditional RecSys do. Our framework demonstrates improved accuracy metrics across three different conversational recommendation datasets and two adaptation settings",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12125",
        "abstract url": "https://arxiv.org/abs/2405.12125",
        "title": "Design, Control, and Motion-Planning for a Root-Perching Rotor-Distributed Manipulator",
        "rating": "-1",
        "keywords": [
            [
                "flight"
            ]
        ],
        "abstract": "Manipulation performance improvement is crucial for aerial robots. For aerial manipulators, the baselink position and attitude errors directly affect the precision at the end effector. To address this stability problem, fixed-body approaches such as perching on the environment using the rotor suction force are useful. Additionally, conventional arm-equipped multirotors, called rotor-concentrated manipulators (RCMs), find it difficult to generate a large wrench at the end effector due to joint torque limitations. Using distributed rotors to each link, the thrust can support each link weight, decreasing the arm joints' torque. Based on this approach, rotor-distributed manipulators (RDMs) can increase feasible wrench and reachability of the end-effector. This paper introduces a minimal configuration of a rotor-distributed manipulator that can perch on surfaces, especially ceilings, using a part of their body. First, we design a minimal rotor-distributed arm considering the flight and end-effector performance. Second, a flight controller is proposed for this minimal RDM along with a perching controller adaptable for various types of aerial robots. Third, we propose a motion planning method based on inverse kinematics (IK), considering specific constraints to the proposed RDMs such as perching force. Finally, we evaluate flight and perching motions and \\revise{confirm} that the proposed manipulator can significantly improve the manipulation performance.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "IEEE Transactions on Robotics (2023)"
    },
    {
        "paper id": "2405.12126",
        "abstract url": "https://arxiv.org/abs/2405.12126",
        "title": "Alzheimer's Magnetic Resonance Imaging Classification Using Deep and Meta-Learning Models",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "healthcare",
                "MRI",
                "disease"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning, a cutting-edge machine learning approach, outperforms traditional machine learning in identifying intricate structures in complex high-dimensional data, particularly in the domain of healthcare. This study focuses on classifying Magnetic Resonance Imaging (MRI) data for Alzheimer's disease (AD) by leveraging deep learning techniques characterized by state-of-the-art CNNs. Brain imaging techniques such as MRI have enabled the measurement of pathophysiological brain changes related to Alzheimer's disease. Alzheimer's disease is the leading cause of dementia in the elderly, and it is an irreversible brain illness that causes gradual cognitive function disorder. In this paper, we train some benchmark deep models individually for the approach of the solution and later use an ensembling approach to combine the effect of multiple CNNs towards the observation of higher recall and accuracy. Here, the model's effectiveness is evaluated using various methods, including stacking, majority voting, and the combination of models with high recall values. The majority voting performs better than the alternative modelling approach as the majority voting approach typically reduces the variance in the predictions. We report a test accuracy of 90% with a precision score of 0.90 and a recall score of 0.89 in our proposed approach. In future, this study can be extended to incorporate other types of medical data, including signals, images, and other data. The same or alternative datasets can be used with additional classifiers, neural networks, and AI techniques to enhance Alzheimer's detection.",
        "subjects": [
            "cs.CV",
            "cs.ET",
            "cs.LG",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12150",
        "abstract url": "https://arxiv.org/abs/2405.12150",
        "title": "Bangladeshi Native Vehicle Detection in Wild",
        "rating": "-1",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "navigation"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems. To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh. 17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images. Each image width is set to at least 1280px. The dataset's average vehicle bounding box-to-image ratio is 4.7036. This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios. In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use. The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union (IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "comment": "13 pages, 8 figures"
    },
    {
        "paper id": "2405.12174",
        "abstract url": "https://arxiv.org/abs/2405.12174",
        "title": "CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "CT"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Text-to-Table aims to generate structured tables to convey the key information from unstructured documents. Existing text-to-table datasets are typically oriented English, limiting the research in non-English languages. Meanwhile, the emergence of large language models (LLMs) has shown great success as general task solvers in multi-lingual settings (e.g., ChatGPT), theoretically enabling text-to-table in other languages. In this paper, we propose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this task. Our preliminary analysis of English text-to-table datasets highlights two key factors for dataset construction: data diversity and data hallucination. Inspired by this, the CT-Eval dataset selects a popular Chinese multidisciplinary online encyclopedia as the source and covers 28 domains to ensure data diversity. To minimize data hallucination, we first train an LLM to judge and filter out the task samples with hallucination, then employ human annotators to clean the hallucinations in the validation and testing sets. After this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we evaluate the performance of open-source and closed-source LLMs. Our results reveal that zero-shot LLMs (including GPT-4) still have a significant performance gap compared with human judgment. Furthermore, after fine-tuning, open-source LLMs can significantly improve their text-to-table ability, outperforming GPT-4 by a large margin. In short, CT-Eval not only helps researchers evaluate and quickly understand the Chinese text-to-table ability of existing LLMs but also serves as a valuable resource to significantly improve the text-to-table performance of LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2405.12197",
        "abstract url": "https://arxiv.org/abs/2405.12197",
        "title": "Automated Hardware Logic Obfuscation Framework Using GPT",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Obfuscation stands as a promising solution for safeguarding hardware intellectual property (IP) against a spectrum of threats including reverse engineering, IP piracy, and tampering. In this paper, we introduce Obfus-chat, a novel framework leveraging Generative Pre-trained Transformer (GPT) models to automate the obfuscation process. The proposed framework accepts hardware design netlists and key sizes as inputs, and autonomously generates obfuscated code tailored to enhance security. To evaluate the effectiveness of our approach, we employ the Trust-Hub Obfuscation Benchmark for comparative analysis. We employed SAT attacks to assess the security of the design, along with functional verification procedures to ensure that the obfuscated design remains consistent with the original. Our results demonstrate the efficacy and efficiency of the proposed framework in fortifying hardware IP against potential threats, thus providing a valuable contribution to the field of hardware security.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12217",
        "abstract url": "https://arxiv.org/abs/2405.12217",
        "title": "Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Recent studies indicate that large multimodal models (LMMs) are highly robust against natural distribution shifts, often surpassing previous baselines. Despite this, domain-specific adaptation is still necessary, particularly in specialized areas like healthcare. Due to the impracticality of fine-tuning LMMs given their vast parameter space, this work investigates in-context learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We find that the success of ICL heavily relies on the choice of demonstration, mirroring challenges seen in large language models but introducing unique complexities for LMMs facing distribution shifts. Our study addresses this by evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context examples through a nearest example search based on feature similarity. We uncover that its effectiveness is limited by the deficiencies of pre-trained vision encoders under distribution shift scenarios. To address these challenges, we propose InvariantSelectPR, a novel method leveraging Class-conditioned Contrastive Invariance (CCI) for more robust demonstration selection. Specifically, CCI enhances pre-trained vision encoders by improving their discriminative capabilities across different classes and ensuring invariance to domain-specific variations. This enhancement allows the encoders to effectively identify and retrieve the most informative examples, which are then used to guide LMMs in adapting to new query samples under varying distributions. Our experiments show that InvariantSelectPR substantially improves the adaptability of LMMs, achieving significant performance gains on benchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on Camelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the baseline zero-shot performance.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "17 pages, 7 figures, 7 tables"
    },
    {
        "paper id": "2405.12218",
        "abstract url": "https://arxiv.org/abs/2405.12218",
        "title": "Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://mvsgaussian.github.io/"
    },
    {
        "paper id": "2405.12293",
        "abstract url": "https://arxiv.org/abs/2405.12293",
        "title": "Exact Random Graph Matching with Multiple Graphs",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "This work studies fundamental limits for recovering the underlying correspondence among multiple correlated random graphs. We identify a necessary condition for any algorithm to correctly match all nodes across all graphs, and propose two algorithms for which the same condition is also sufficient. The first algorithm employs global information to simultaneously match all the graphs, whereas the second algorithm first partially matches the graphs pairwise and then combines the partial matchings by transitivity. Both algorithms work down to the information theoretic threshold. Our analysis reveals a scenario where exact matching between two graphs alone is impossible, but leveraging more than two graphs allows exact matching among all the graphs. Along the way, we derive independent results about the k-core of Erdos-Renyi graphs.",
        "subjects": [
            "cs.DS",
            "cs.DM",
            "math.ST"
        ],
        "comment": "20 pages, 3 figures"
    },
    {
        "paper id": "2405.12304",
        "abstract url": "https://arxiv.org/abs/2405.12304",
        "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A Non-Linear Programming Approach",
        "rating": "-1",
        "keywords": [
            [
                "Synthesis"
            ]
        ],
        "abstract": "High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations. To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12318",
        "abstract url": "https://arxiv.org/abs/2405.12318",
        "title": "Hierarchical SegNet with Channel and Context Attention for Accurate Lung Segmentation in Chest X-ray Images",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosis",
                "X-ray",
                "disease"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Lung segmentation in chest X-ray images is a critical task in medical image analysis, enabling accurate diagnosis and treatment of various lung diseases. In this paper, we propose a novel approach for lung segmentation by integrating Hierarchical SegNet with a proposed multi-modal attention mechanism. The channel attention mechanism highlights specific feature maps or channels crucial for lung region segmentation, while the context attention mechanism adaptively weighs the importance of different spatial regions. By combining both mechanisms, the proposed mechanism enables the model to better capture complex patterns and relationships between various features, leading to improved segmentation accuracy and better feature representation. Furthermore, an attention gating mechanism is employed to integrate attention information with encoder features, allowing the model to adaptively weigh the importance of different attention features and ignore irrelevant ones. Experimental results demonstrate that our proposed approach achieves state-of-the-art performance in lung segmentation tasks, outperforming existing methods. The proposed approach has the potential to improve the accuracy and efficiency of lung disease diagnosis and treatment, and can be extended to other medical image analysis tasks.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12328",
        "abstract url": "https://arxiv.org/abs/2405.12328",
        "title": "Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "diagnosing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The accurate segmentation of medical images is crucial for diagnosing and treating diseases. Recent studies demonstrate that vision transformer-based methods have significantly improved performance in medical image segmentation, primarily due to their superior ability to establish global relationships among features and adaptability to various inputs. However, these methods struggle with the low signal-to-noise ratio inherent to medical images. Additionally, the effective utilization of channel and spatial information, which are essential for medical image segmentation, is limited by the representation capacity of self-attention. To address these challenges, we propose a multi-dimension transformer with attention-based filtering (MDT-AF), which redesigns the patch embedding and self-attention mechanism for medical image segmentation. MDT-AF incorporates an attention-based feature filtering mechanism into the patch embedding blocks and employs a coarse-to-fine process to mitigate the impact of low signal-to-noise ratio. To better capture complex structures in medical images, MDT-AF extends the self-attention mechanism to incorporate spatial and channel dimensions, enriching feature representation. Moreover, we introduce an interaction mechanism to improve the feature aggregation between spatial and channel dimensions. Experimental results on three public medical image segmentation benchmarks show that MDT-AF achieves state-of-the-art (SOTA) performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12358",
        "abstract url": "https://arxiv.org/abs/2405.12358",
        "title": "Using Color Refinement to Boost Enumeration and Counting for Acyclic CQs of Binary Schemas",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We present an index structure, called the color-index, to boost the evaluation of acyclic conjunctive queries (ACQs) over binary schemas. The color-index is based on the color refinement algorithm, a widely used subroutine for graph isomorphism testing algorithms. Given a database $D$, we use a suitable version of the color refinement algorithm to produce a stable coloring of $D$, an assignment from the active domain of $D$ to a set of colors $C_D$. The main ingredient of the color-index is a particular database $D_c$ whose active domain is $C_D$ and whose size is at most $|D|$. Using the color-index, we can evaluate any free-connex ACQ $Q$ over $D$ with preprocessing time $O(|Q| \\cdot |D_c|)$ and constant delay enumeration. Furthermore, we can also count the number of results of $Q$ over $D$ in time $O(|Q| \\cdot |D_c|)$. Given that $|D_c|$ could be much smaller than $|D|$ (even constant-size for some families of databases), the color-index is the first index structure for evaluating free-connex ACQs that allows efficient enumeration and counting with performance that may be strictly smaller than the database size.",
        "subjects": [
            "cs.DB",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12367",
        "abstract url": "https://arxiv.org/abs/2405.12367",
        "title": "Large-Scale Multi-Center CT and MRI Segmentation of Pancreas with Deep Learning",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "MRI",
                "CT"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Automated volumetric segmentation of the pancreas on cross-sectional imaging is needed for diagnosis and follow-up of pancreatic diseases. While CT-based pancreatic segmentation is more established, MRI-based segmentation methods are understudied, largely due to a lack of publicly available datasets, benchmarking research efforts, and domain-specific deep learning methods. In this retrospective study, we collected a large dataset (767 scans from 499 participants) of T1-weighted (T1W) and T2-weighted (T2W) abdominal MRI series from five centers between March 2004 and November 2022. We also collected CT scans of 1,350 patients from publicly available sources for benchmarking purposes. We developed a new pancreas segmentation method, called PanSegNet, combining the strengths of nnUNet and a Transformer network with a new linear attention module enabling volumetric computation. We tested PanSegNet's accuracy in cross-modality (a total of 2,117 scans) and cross-center settings with Dice and Hausdorff distance (HD95) evaluation metrics. We used Cohen's kappa statistics for intra and inter-rater agreement evaluation and paired t-tests for volume and Dice comparisons, respectively. For segmentation accuracy, we achieved Dice coefficients of 88.3% (std: 7.2%, at case level) with CT, 85.0% (std: 7.9%) with T1W MRI, and 86.3% (std: 6.4%) with T2W MRI. There was a high correlation for pancreas volume prediction with R^2 of 0.91, 0.84, and 0.85 for CT, T1W, and T2W, respectively. We found moderate inter-observer (0.624 and 0.638 for T1W and T2W MRI, respectively) and high intra-observer agreement scores. All MRI data is made available at https://osf.io/kysnj/. Our source code is available at https://github.com/NUBagciLab/PaNSegNet.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "under review version"
    },
    {
        "paper id": "2405.12368",
        "abstract url": "https://arxiv.org/abs/2405.12368",
        "title": "Layout Agnostic Human Activity Recognition in Smart Homes through Textual Descriptions Of Sensor Triggers (TDOST)",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Human activity recognition (HAR) using ambient sensors in smart homes has numerous applications for human healthcare and wellness. However, building general-purpose HAR models that can be deployed to new smart home environments requires a significant amount of annotated sensor data and training overhead. Most smart homes vary significantly in their layouts, i.e., floor plans and the specifics of sensors embedded, resulting in low generalizability of HAR models trained for specific homes. We address this limitation by introducing a novel, layout-agnostic modeling approach for HAR systems in smart homes that utilizes the transferrable representational capacity of natural language descriptions of raw sensor data. To this end, we generate Textual Descriptions Of Sensor Triggers (TDOST) that encapsulate the surrounding trigger conditions and provide cues for underlying activities to the activity recognition models. Leveraging textual embeddings, rather than raw sensor data, we create activity recognition systems that predict standard activities across homes without either (re-)training or adaptation on target homes. Through an extensive evaluation, we demonstrate the effectiveness of TDOST-based models in unseen smart homes through experiments on benchmarked CASAS datasets. Furthermore, we conduct a detailed analysis of how the individual components of our approach affect downstream activity recognition performance.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12369",
        "abstract url": "https://arxiv.org/abs/2405.12369",
        "title": "AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "SDF"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting (3DGS) has recently advanced radiance field reconstruction by offering superior capabilities for novel view synthesis and real-time rendering speed. However, its strategy of blending optimization and adaptive density control might lead to sub-optimal results; it can sometimes yield noisy geometry and blurry artifacts due to prioritizing optimizing large Gaussians at the cost of adequately densifying smaller ones. To address this, we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of various sizes into more uniform-sized Atom Gaussians. The strategy enhances the representation of areas with fine features by placing greater emphasis on densification in accordance with scene details. In addition, we proposed a Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal Loss. This optimization method effectively smooths flat surfaces while preserving intricate details. Our evaluation shows that AtomGS outperforms existing state-of-the-art methods in rendering quality. Additionally, it achieves competitive accuracy in geometry reconstruction and offers a significant improvement in training speed over other SDF-based methods. More interactive demos can be found in our website (\\href{https://rongliu-leo.github.io/AtomGS/}{https://rongliu-leo.github.io/AtomGS/}).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12408",
        "abstract url": "https://arxiv.org/abs/2405.12408",
        "title": "Flexible Active Safety Motion Control for Robotic Obstacle Avoidance: A CBF-Guided MPC Approach",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "A flexible active safety motion (FASM) control approach is proposed for the avoidance of dynamic obstacles and the reference tracking in robot manipulators. The distinctive feature of the proposed method lies in its utilization of control barrier functions (CBF) to design flexible CBF-guided safety criteria (CBFSC) with dynamically optimized decay rates, thereby offering flexibility and active safety for robot manipulators in dynamic environments. First, discrete-time CBFs are employed to formulate the novel flexible CBFSC with dynamic decay rates for robot manipulators. Following that, the model predictive control (MPC) philosophy is applied, integrating flexible CBFSC as safety constraints into the receding-horizon optimization problem. Significantly, the decay rates of the designed CBFSC are incorporated as decision variables in the optimization problem, facilitating the dynamic enhancement of flexibility during the obstacle avoidance process. In particular, a novel cost function that integrates a penalty term is designed to dynamically adjust the safety margins of the CBFSC. Finally, experiments are conducted in various scenarios using a Universal Robots 5 (UR5) manipulator to validate the effectiveness of the proposed approach.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "11 pages, 11 figures"
    },
    {
        "paper id": "2405.12410",
        "abstract url": "https://arxiv.org/abs/2405.12410",
        "title": "Local search for valued constraint satisfaction parameterized by treedepth",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Sometimes local search algorithms cannot efficiently find even local peaks. To understand why, I look at the structure of ascents in fitness landscapes from valued constraint satisfaction problems (VCSPs). Given a VCSP with a constraint graph of treedepth $d$, I prove that from any initial assignment there always exists an ascent of length $2^{d + 1} \\cdot n$ to a local peak. This means that short ascents always exist in fitness landscapes from constraint graphs of logarithmic treedepth, and thus also for all VCSPs of bounded treewidth. But this does not mean that local search algorithms will always find and follow such short ascents in sparse VCSPs. I show that with loglog treedepth, superpolynomial ascents exist; and for polylog treedepth, there are initial assignments from which all ascents are superpolynomial. Together, these results suggest that the study of sparse VCSPs can help us better understand the barriers to efficient local search.",
        "subjects": [
            "cs.DM",
            "cs.DS",
            "cs.NE",
            "q-bio.PE"
        ],
        "comment": "7 pgs"
    },
    {
        "paper id": "2405.12420",
        "abstract url": "https://arxiv.org/abs/2405.12420",
        "title": "GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "diffusion",
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives. Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12447",
        "abstract url": "https://arxiv.org/abs/2405.12447",
        "title": "EPL: Empirical Prototype Learning for Deep Face Recognition",
        "rating": "-1",
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Prototype learning is widely used in face recognition, which takes the row vectors of coefficient matrix in the last linear layer of the feature extraction model as the prototypes for each class. When the prototypes are updated using the facial sample feature gradients in the model training, they are prone to being pulled away from the class center by the hard samples, resulting in decreased overall model performance. In this paper, we explicitly define prototypes as the expectations of sample features in each class and design the empirical prototypes using the existing samples in the dataset. We then devise a strategy to adaptively update these empirical prototypes during the model training based on the similarity between the sample features and the empirical prototypes. Furthermore, we propose an empirical prototype learning (EPL) method, which utilizes an adaptive margin parameter with respect to sample features. EPL assigns larger margins to the normal samples and smaller margins to the hard samples, allowing the learned empirical prototypes to better reflect the class center dominated by the normal samples and finally pull the hard samples towards the empirical prototypes through the learning. The extensive experiments on MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace demonstrate the effectiveness of EPL. Our code is available at $\\href{https://github.com/WakingHours-GitHub/EPL}{https://github.com/WakingHours-GitHub/EPL}$.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16pages, 2 figures, 6 tables"
    },
    {
        "paper id": "2405.12464",
        "abstract url": "https://arxiv.org/abs/2405.12464",
        "title": "Evaluation of Connected Vehicle Identification-Aware Mixed Traffic Freeway Cooperative Merging",
        "rating": "-1",
        "keywords": [
            [
                "trajectory",
                "Vehicle"
            ]
        ],
        "abstract": "Cooperative on-ramp merging control for connected automated vehicles (CAVs) has been extensively investigated. However, they did neglect the connected vehicle identification process, which is a must for CAV cooperations. In this paper, we introduced a connected vehicle identification system (VIS) into the on-ramp merging control process for the first time and proposed an evaluation framework to assess the impacts of VIS on on-ramp merging performance. First, the mixed-traffic cooperative merging problem was formulated. Then, a real-world merging trajectory dataset was processed to generate dangerous merging scenarios. Aiming at resolving the potential collision risks in mixed traffic where CAVs and traditional human-driven vehicles (THVs) coexist, we proposed on-ramp merging strategies for CAVs in different mixed traffic situations considering the connected vehicle identification process. The performances were evaluated via simulations. Results indicated that while safety was assured for all cases with CAVs, the cases with VIS had delayed initiation of cooperation, limiting the range of cooperative merging and leading to increased fuel consumption and acceleration variations.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "@2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works"
    },
    {
        "paper id": "2405.12469",
        "abstract url": "https://arxiv.org/abs/2405.12469",
        "title": "Last-Level Cache Side-Channel Attacks Are Feasible in the Modern Public Cloud (Extended Version)",
        "rating": "-1",
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "Last-level cache side-channel attacks have been mostly demonstrated in highly-controlled, quiescent local environments. Hence, it is unclear whether such attacks are feasible in a production cloud environment. In the cloud, side channels are flooded with noise from activities of other tenants and, in Function-as-a-Service (FaaS) workloads, the attacker has a very limited time window to mount the attack. In this paper, we show that such attacks are feasible in practice, although they require new techniques. We present an end-to-end, cross-tenant attack on a vulnerable ECDSA implementation in the public FaaS Google Cloud Run environment. We introduce several new techniques to improve every step of the attack. First, to speed-up the generation of eviction sets, we introduce L2-driven candidate address filtering and a Binary Search-based algorithm for address pruning. Second, to monitor victim memory accesses with high time resolution, we introduce Parallel Probing. Finally, we leverage power spectral density from signal processing to easily identify the victim's target cache set in the frequency domain. Overall, using these mechanisms, we extract a median value of 81% of the secret ECDSA nonce bits from a victim container in 19 seconds on average.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12472",
        "abstract url": "https://arxiv.org/abs/2405.12472",
        "title": "Optimizing Generative AI Networking: A Dual Perspective with Multi-Agent Systems and Mixture of Experts",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "In the continued development of next-generation networking and artificial intelligence content generation (AIGC) services, the integration of multi-agent systems (MAS) and the mixture of experts (MoE) frameworks is becoming increasingly important. Motivated by this, this article studies the contrasting and converging of MAS and MoE in AIGC-enabled networking. First, we discuss the architectural designs, operational procedures, and inherent advantages of using MAS and MoE in generative AI to explore its functionality and applications fully. Next, we review the applications of MAS and MoE frameworks in content generation and resource allocation, emphasizing their impact on networking operations. Subsequently, we propose a novel multi-agent-enabled MoE-proximal policy optimization (MoE-PPO) framework for 3D object generation and data transfer scenarios. The framework uses MAS for dynamic task coordination of each network service provider agent and MoE for expert-driven execution of respective tasks, thereby improving overall system efficiency and adaptability. The simulation results demonstrate the effectiveness of our proposed framework and significantly improve the performance indicators under different network conditions. Finally, we outline potential future research directions.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2405.12476",
        "abstract url": "https://arxiv.org/abs/2405.12476",
        "title": "Benchmarking Fish Dataset and Evaluation Metric in Keypoint Detection - Towards Precise Fish Morphological Assessment in Aquaculture Breeding",
        "rating": "-1",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate phenotypic analysis in aquaculture breeding necessitates the quantification of subtle morphological phenotypes. Existing datasets suffer from limitations such as small scale, limited species coverage, and inadequate annotation of keypoints for measuring refined and complex morphological phenotypes of fish body parts. To address this gap, we introduce FishPhenoKey, a comprehensive dataset comprising 23,331 high-resolution images spanning six fish species. Notably, FishPhenoKey includes 22 phenotype-oriented annotations, enabling the capture of intricate morphological phenotypes. Motivated by the nuanced evaluation of these subtle morphologies, we also propose a new evaluation metric, Percentage of Measured Phenotype (PMP). It is designed to assess the accuracy of individual keypoint positions and is highly sensitive to the phenotypes measured using the corresponding keypoints. To enhance keypoint detection accuracy, we further propose a novel loss, Anatomically-Calibrated Regularization (ACR), that can be integrated into keypoint detection models, leveraging biological insights to refine keypoint localization. Our contributions set a new benchmark in fish phenotype analysis, addressing the challenges of precise morphological quantification and opening new avenues for research in sustainable aquaculture and genetic studies. Our dataset and code are available at https://github.com/WeizhenLiuBioinform/Fish-Phenotype-Detect.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IJCAI2024, Code: https://github.com/WeizhenLiuBioinform/Fish-Phenotype-Detect"
    },
    {
        "paper id": "2405.12477",
        "abstract url": "https://arxiv.org/abs/2405.12477",
        "title": "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "Graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts. To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction. Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts. Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts. Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12494",
        "abstract url": "https://arxiv.org/abs/2405.12494",
        "title": "Phishing Email Detection Using Inputs From Artificial Intelligence",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Enterprise security is increasingly being threatened by social engineering attacks, such as phishing, which deceive employees into giving access to enterprise data. To protect both the users themselves and enterprise data, more and more organizations provide cyber security training that seeks to teach employees/customers to identify and report suspicious content. By its very nature, such training seeks to focus on signals that are likely to persist across a wide range of attacks. Further, it expects the user to apply the learnings from these training on e-mail messages that were not filtered by existing, automatic enterprise security (e.g., spam filters and commercial phishing detection software). However, relying on such training now shifts the detection of phishing from an automatic process to a human driven one which is fallible especially when a user errs due to distraction, forgetfulness, etc. In this work we explore treating this type of detection as a natural language processing task and modifying training pipelines accordingly. We present a dataset with annotated labels where these labels are created from the classes of signals that users are typically asked to identify in such training. We also present baseline classifier models trained on these classes of labels. With a comparative analysis of performance between human annotators and the models on these labels, we provide insights which can contribute to the improvement of the respective curricula for both machine and human training.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "10 pages, 2 Tables, 1 figure"
    },
    {
        "paper id": "2405.11795",
        "abstract url": "https://arxiv.org/abs/2405.11795",
        "title": "Application of time-series quantum generative model to financial data",
        "rating": "-1.5",
        "keywords": [
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Despite proposing a quantum generative model for time series that successfully learns correlated series with multiple Brownian motions, the model has not been adapted and evaluated for financial problems. In this study, a time-series generative model was applied as a quantum generative model to actual financial data. Future data for two correlated time series were generated and compared with classical methods such as long short-term memory and vector autoregression. Furthermore, numerical experiments were performed to complete missing values. Based on the results, we evaluated the practical applications of the time-series quantum generation model. It was observed that fewer parameter values were required compared with the classical method. In addition, the quantum time-series generation model was feasible for both stationary and nonstationary data. These results suggest that several parameters can be applied to various types of time-series data.",
        "subjects": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2405.11821",
        "abstract url": "https://arxiv.org/abs/2405.11821",
        "title": "A Three-Phase Analysis of Synergistic Effects During Co-pyrolysis of Algae and Wood for Biochar Yield Using Machine Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Biochar"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Pyrolysis techniques have served to be a groundbreaking technique for effectively utilising natural and man-made biomass products like plastics, wood, crop residue, fruit peels etc. Recent advancements have shown a greater yield of essential products like biochar, bio-oil and other non-condensable gases by blending different biomasses in a certain ratio. This synergy effect of combining two pyrolytic raw materials i.e co-pyrolysis of algae and wood biomass has been systematically studied and grouped into 3 phases in this research paper-kinetic analysis of co-pyrolysis, correlation among proximate and ultimate analysis with bio-char yield and lastly grouping of different weight ratios based on biochar yield up to a certain percentage. Different ML and DL algorithms have been utilized for regression and classification techniques to give a comprehensive overview of the effect of the synergy of two different biomass materials on biochar yield. For the first phase, the best prediction of biochar yield was obtained by using a decision tree regressor with a perfect MSE score of 0.00, followed by a gradient-boosting regressor. The second phase was analyzed using both ML and DL techniques. Within ML, SVR proved to be the most convenient model with an accuracy score of 0.972 with DNN employed for deep learning technique. Finally, for the third phase, binary classification was applied to biochar yield with and without heating rate for biochar yield percentage above and below 40%. The best technique for ML was Support Vector followed by Random forest while ANN was the most suitable Deep Learning Technique.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2405.11928",
        "abstract url": "https://arxiv.org/abs/2405.11928",
        "title": "\"Set It Up!\": Functional Object Arrangement with Compositional Generative Models",
        "rating": "-1.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as \"set up a dining table for two\"; previous arrangement approaches have focused on much more explicit instructions, such as \"put object A on the table.\" We introduce a framework, SetItUp, for learning to interpret under-specified instructions. SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types. By leveraging an intermediate graph-like representation of abstract spatial relationships among objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses. SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints. We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "10 pages main paper, 21 pages appendix, RSS 2024"
    },
    {
        "paper id": "2405.11955",
        "abstract url": "https://arxiv.org/abs/2405.11955",
        "title": "Shallow Recurrent Decoder for Reduced Order Modeling of Plasma Dynamics",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reduced order models are becoming increasingly important for rendering complex and multiscale spatio-temporal dynamics computationally tractable. The computational efficiency of such surrogate models is especially important for design, exhaustive exploration and physical understanding. Plasma simulations, in particular those applied to the study of ${\\bf E}\\times {\\bf B}$ plasma discharges and technologies, such as Hall thrusters, require substantial computational resources in order to resolve the multidimentional dynamics that span across wide spatial and temporal scales. Although high-fidelity computational tools are available to simulate such systems over limited conditions and in highly simplified geometries, simulations of full-size systems and/or extensive parametric studies over many geometric configurations and under different physical conditions are computationally intractable with conventional numerical tools. Thus, scientific studies and industrially oriented modeling of plasma systems, including the important ${\\bf E}\\times {\\bf B}$ technologies, stand to significantly benefit from reduced order modeling algorithms. We develop a model reduction scheme based upon a {\\em Shallow REcurrent Decoder} (SHRED) architecture. The scheme uses a neural network for encoding limited sensor measurements in time (sequence-to-sequence encoding) to full state-space reconstructions via a decoder network. Based upon the theory of separation of variables, the SHRED architecture is capable of (i) reconstructing full spatio-temporal fields with as little as three point sensors, even the fields that are not measured with sensor feeds but that are in dynamic coupling with the measured field, and (ii) forecasting the future state of the system using neural network roll-outs from the trained time encoding model.",
        "subjects": [
            "physics.plasm-ph",
            "cs.LG",
            "nlin.PS",
            "physics.comp-ph"
        ],
        "comment": "12 pages, 7 figures"
    },
    {
        "paper id": "2405.11958",
        "abstract url": "https://arxiv.org/abs/2405.11958",
        "title": "Exploring Commonalities in Explanation Frameworks: A Multi-Domain Survey Analysis",
        "rating": "-1.5",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study presents insights gathered from surveys and discussions with specialists in three domains, aiming to find essential elements for a universal explanation framework that could be applied to these and other similar use cases. The insights are incorporated into a software tool that utilizes GP algorithms, known for their interpretability. The applications analyzed include a medical scenario (involving predictive ML), a retail use case (involving prescriptive ML), and an energy use case (also involving predictive ML). We interviewed professionals from each sector, transcribing their conversations for further analysis. Additionally, experts and non-experts in these fields filled out questionnaires designed to probe various dimensions of explanatory methods. The findings indicate a universal preference for sacrificing a degree of accuracy in favor of greater explainability. Additionally, we highlight the significance of feature importance and counterfactual explanations as critical components of such a framework. Our questionnaires are publicly available to facilitate the dissemination of knowledge in the field of XAI.",
        "subjects": [
            "cs.LG",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12038",
        "abstract url": "https://arxiv.org/abs/2405.12038",
        "title": "Adaptive Extraction Network for Multivariate Long Sequence Time-Series Forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Models employing CNN architecture have made significant progress in multivariate long sequence time-series forecasting (MLSTF), particularly in modeling local time series characteristics. However, during the MLSTF process, extracting the global time series patterns and understanding the correlations among different variables are highly significant. To address this challenge, we introduce multi-resolution convolution and deformable convolution operations. By enlarging the receptive field using convolution kernels with different dilation factors to capture temporal correlation information across different resolutions, and adaptively adjusting the sampling positions through additional offset vectors, we enhance the network's ability to capture correlated features between variables. Building upon this, we propose ATVCNet, an adaptive temporal-variable convolutional network designed to effectively model the local/global temporal dependencies and inter-variable dependencies of multivariate time series. Specifically, extracting and fusing time series features at different resolutions, captures both local contextual information and global patterns in the time series. The designed inter-variable feature adaptive extraction module captures the correlation among different variables in the time series. We evaluated the performance of ATVCNet across eight real-world datasets. The results indicate that ATVCNet achieved a performance improvement of approximately 63.4% over state-of-the-art MLSTF models.",
        "subjects": [
            "cs.LG",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12258",
        "abstract url": "https://arxiv.org/abs/2405.12258",
        "title": "Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment",
        "rating": "-1.5",
        "keywords": [
            [
                "Cancer"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large language models (LLMs) have transformed AI and achieved breakthrough performance on a wide range of tasks that require human intelligence. In science, perhaps the most interesting application of LLMs is for hypothesis formation. A feature of LLMs, which results from their probabilistic structure, is that the output text is not necessarily a valid inference from the training text. These are 'hallucinations', and are a serious problem in many applications. However, in science, hallucinations may be useful: they are novel hypotheses whose validity may be tested by laboratory experiments. Here we experimentally test the use of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment. We applied the LLM GPT4 to hypothesize novel pairs of FDA-approved non-cancer drugs that target the MCF7 breast cancer cell line relative to the non-tumorigenic breast cell line MCF10A. In the first round of laboratory experiments GPT4 succeeded in discovering three drug combinations (out of 12 tested) with synergy scores above the positive controls. These combinations were itraconazole + atenolol, disulfiram + simvastatin and dipyridamole + mebendazole. GPT4 was then asked to generate new combinations after considering its initial results. It then discovered three more combinations with positive synergy scores (out of four tested), these were disulfiram + fulvestrant, mebendazole + quinacrine and disulfiram + quinacrine. A limitation of GPT4 as a generator of hypotheses was that its explanations for them were formulaic and unconvincing. We conclude that LLMs are an exciting novel source of scientific hypotheses.",
        "subjects": [
            "q-bio.QM",
            "cs.LG",
            "q-bio.CB"
        ],
        "comment": "20 pages, 7 tables. Supplementary information available"
    },
    {
        "paper id": "2405.12266",
        "abstract url": "https://arxiv.org/abs/2405.12266",
        "title": "EGAN: Evolutional GAN for Ransomware Evasion",
        "rating": "-1.5",
        "keywords": [
            [
                "GAN"
            ],
            [
                "attack"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Adversarial Training is a proven defense strategy against adversarial malware. However, generating adversarial malware samples for this type of training presents a challenge because the resulting adversarial malware needs to remain evasive and functional. This work proposes an attack framework, EGAN, to address this limitation. EGAN leverages an Evolution Strategy and Generative Adversarial Network to select a sequence of attack actions that can mutate a Ransomware file while preserving its original functionality. We tested this framework on popular AI-powered commercial antivirus systems listed on VirusTotal and demonstrated that our framework is capable of bypassing the majority of these systems. Moreover, we evaluated whether the EGAN attack framework can evade other commercial non-AI antivirus solutions. Our results indicate that the adversarial ransomware generated can increase the probability of evading some of them.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12295",
        "abstract url": "https://arxiv.org/abs/2405.12295",
        "title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks",
        "rating": "-1.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which enable the processing of graph-structured data without relying on predefined graph structures, are gaining importance in an increasingly wide variety of applications. As these networks demonstrate proficiency across a range of tasks, they become lucrative targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. A large effort has been made to develop model-stealing attacks that focus on models trained with images and texts. However, little attention has been paid to GNNs trained on graph data. This paper introduces a novel method for unsupervised model-stealing attacks against inductive GNNs, based on graph contrasting learning and spectral graph augmentations to efficiently extract information from the target model. The proposed attack is thoroughly evaluated on six datasets. The results show that this approach demonstrates a higher level of efficiency compared to existing stealing attacks. More concretely, our attack outperforms the baseline on all benchmarks achieving higher fidelity and downstream accuracy of the stolen model while requiring fewer queries sent to the target model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2112.08331 by other authors"
    },
    {
        "paper id": "2405.12309",
        "abstract url": "https://arxiv.org/abs/2405.12309",
        "title": "Accurate Learning of Equivariant Quantum Systems from a Single Ground State",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum",
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting properties across system parameters is an important task in quantum physics, with applications ranging from molecular dynamics to variational quantum algorithms. Recently, provably efficient algorithms to solve this task for ground states within a gapped phase were developed. Here we dramatically improve the efficiency of these algorithms by showing how to learn properties of all ground states for systems with periodic boundary conditions from a single ground state sample. We prove that the prediction error tends to zero in the thermodynamic limit and numerically verify the results.",
        "subjects": [
            "quant-ph",
            "cs.LG"
        ],
        "comment": "5 pages, 3 figures"
    },
    {
        "paper id": "2405.12312",
        "abstract url": "https://arxiv.org/abs/2405.12312",
        "title": "A Principled Approach for a New Bias Measure",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "The widespread use of machine learning and data-driven algorithms for decision making has been steadily increasing over many years. The areas in which this is happening are diverse: healthcare, employment, finance, education, the legal system to name a few; and the associated negative side effects are being increasingly harmful for society. Negative data \\emph{bias} is one of those, which tends to result in harmful consequences for specific groups of people. Any mitigation strategy or effective policy that addresses the negative consequences of bias must start with awareness that bias exists, together with a way to understand and quantify it. However, there is a lack of consensus on how to measure data bias and oftentimes the intended meaning is context dependent and not uniform within the research community. The main contributions of our work are: (1) a general algorithmic framework for defining and efficiently quantifying the bias level of a dataset with respect to a protected group; and (2) the definition of a new bias measure. Our results are experimentally validated using nine publicly available datasets and theoretically analyzed, which provide novel insights about the problem. Based on our approach, we also derive a bias mitigation algorithm that might be useful to policymakers.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12317",
        "abstract url": "https://arxiv.org/abs/2405.12317",
        "title": "Kernel spectral joint embeddings for high-dimensional noisy datasets using duo-landmark integral operators",
        "rating": "-1.5",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Integrative analysis of multiple heterogeneous datasets has become standard practice in many research fields, especially in single-cell genomics and medical informatics. Existing approaches oftentimes suffer from limited power in capturing nonlinear structures, insufficient account of noisiness and effects of high-dimensionality, lack of adaptivity to signals and sample sizes imbalance, and their results are sometimes difficult to interpret. To address these limitations, we propose a novel kernel spectral method that achieves joint embeddings of two independently observed high-dimensional noisy datasets. The proposed method automatically captures and leverages possibly shared low-dimensional structures across datasets to enhance embedding quality. The obtained low-dimensional embeddings can be utilized for many downstream tasks such as simultaneous clustering, data visualization, and denoising. The proposed method is justified by rigorous theoretical analysis. Specifically, we show the consistency of our method in recovering the low-dimensional noiseless signals, and characterize the effects of the signal-to-noise ratios on the rates of convergence. Under a joint manifolds model framework, we establish the convergence of ultimate embeddings to the eigenfunctions of some newly introduced integral operators. These operators, referred to as duo-landmark integral operators, are defined by the convolutional kernel maps of some reproducing kernel Hilbert spaces (RKHSs). These RKHSs capture the either partially or entirely shared underlying low-dimensional nonlinear signal structures of the two datasets. Our numerical experiments and analyses of two single-cell omics datasets demonstrate the empirical advantages of the proposed method over existing methods in both embeddings and several downstream tasks.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "32 pages, 5 figures; comments are welcome"
    },
    {
        "paper id": "2405.12319",
        "abstract url": "https://arxiv.org/abs/2405.12319",
        "title": "Dynamic Line Rating using Hyper-local Weather Predictions: A Machine Learning Approach",
        "rating": "-1.5",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Dynamic Line Rating (DLR) systems are crucial for renewable energy integration in transmission networks. However, traditional methods relying on sensor data face challenges due to the impracticality of installing sensors on every pole or span. Additionally, sensor-based approaches may struggle predicting DLR in rapidly changing weather conditions. This paper proposes a novel approach, leveraging machine learning (ML) techniques alongside hyper-local weather forecast data. Unlike conventional methods, which solely rely on sensor data, this approach utilizes ML models trained to predict hyper-local weather parameters on a full network scale. Integrating topographical data enhances prediction accuracy by accounting for landscape features and obstacles around overhead lines. The paper introduces confidence intervals for DLR assessments to mitigate risks associated with uncertainties. A case study from Estonia demonstrates the practical implementation of the proposed methodology, highlighting its effectiveness in real-world scenarios. By addressing limitations of sensor-based approaches, this research contributes to the discourse of renewable energy integration in transmission systems, advancing efficiency and reliability in the power grid.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": "15 pages and planning to submit it to IEEE Access"
    },
    {
        "paper id": "2405.12327",
        "abstract url": "https://arxiv.org/abs/2405.12327",
        "title": "Diversifying by Intent in Recommender Systems",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "It has become increasingly clear that recommender systems overly focusing on short-term engagement can inadvertently hurt long-term user experience. However, it is challenging to optimize long-term user experience directly as the desired signal is sparse, noisy and manifests over a long horizon. In this work, we show the benefits of incorporating higher-level user understanding, specifically user intents that can persist across multiple interactions or recommendation sessions, for whole-page recommendation toward optimizing long-term user experience. User intent has primarily been investigated within the context of search, but remains largely under-explored for recommender systems. To bridge this gap, we develop a probabilistic intent-based whole-page diversification framework in the final stage of a recommender system. Starting with a prior belief of user intents, the proposed diversification framework sequentially selects items at each position based on these beliefs, and subsequently updates posterior beliefs about the intents. It ensures that different user intents are represented in a page towards optimizing long-term user experience. We experiment with the intent diversification framework on one of the world's largest content recommendation platforms, serving billions of users daily. Our framework incorporates the user's exploration intent, capturing their propensity to explore new interests and content. Live experiments show that the proposed framework leads to an increase in user retention and overall user enjoyment, validating its effectiveness in facilitating long-term planning. In particular, it enables users to consistently discover and engage with diverse contents that align with their underlying intents over time, thereby leading to an improved long-term user experience.",
        "subjects": [
            "cs.IR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12340",
        "abstract url": "https://arxiv.org/abs/2405.12340",
        "title": "Cascade-based Randomization for Inferring Causal Effects under Diffusion Interference",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "The presence of interference, where the outcome of an individual may depend on the treatment assignment and behavior of neighboring nodes, can lead to biased causal effect estimation. Current approaches to network experiment design focus on limiting interference through cluster-based randomization, in which clusters are identified using graph clustering, and cluster randomization dictates the node assignment to treatment and control. However, cluster-based randomization approaches perform poorly when interference propagates in cascades, whereby the response of individuals to treatment propagates to their multi-hop neighbors. When we have knowledge of the cascade seed nodes, we can leverage this interference structure to mitigate the resulting causal effect estimation bias. With this goal, we propose a cascade-based network experiment design that initiates treatment assignment from the cascade seed node and propagates the assignment to their multi-hop neighbors to limit interference during cascade growth and thereby reduce the overall causal effect estimation error. Our extensive experiments on real-world and synthetic datasets demonstrate that our proposed framework outperforms the existing state-of-the-art approaches in estimating causal effects in network data.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12372",
        "abstract url": "https://arxiv.org/abs/2405.12372",
        "title": "DispaRisk: Assessing and Interpreting Disparity Risks in Datasets",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine Learning algorithms (ML) impact virtually every aspect of human lives and have found use across diverse sectors, including healthcare, finance, and education. Often, ML algorithms have been found to exacerbate societal biases presented in datasets, leading to adversarial impacts on subsets/groups of individuals, in many cases minority groups. To effectively mitigate these untoward effects, it is crucial that disparities/biases are identified and assessed early in a ML pipeline. This proactive approach facilitates timely interventions to prevent bias amplification and reduce complexity at later stages of model development. In this paper, we introduce DispaRisk, a novel framework designed to proactively assess the potential risks of disparities in datasets during the initial stages of the ML pipeline. We evaluate DispaRisk's effectiveness by benchmarking it with commonly used datasets in fairness research. Our findings demonstrate the capabilities of DispaRisk to identify datasets with a high-risk of discrimination, model families prone to biases, and characteristics that heighten discrimination susceptibility in a ML pipeline. The code for our experiments is available in the following repository: https://github.com/jovasque156/disparisk",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12382",
        "abstract url": "https://arxiv.org/abs/2405.12382",
        "title": "Stochastic Reservoir Computers",
        "rating": "-1.5",
        "keywords": [
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reservoir computing is a form of machine learning that utilizes nonlinear dynamical systems to perform complex tasks in a cost-effective manner when compared to typical neural networks. Many recent advancements in reservoir computing, in particular quantum reservoir computing, make use of reservoirs that are inherently stochastic. However, the theoretical justification for using these systems has not yet been well established. In this paper, we investigate the universality of stochastic reservoir computers, in which we use a stochastic system for reservoir computing using the probabilities of each reservoir state as the readout instead of the states themselves. In stochastic reservoir computing, the number of distinct states of the entire reservoir computer can potentially scale exponentially with the size of the reservoir hardware, offering the advantage of compact device size. We prove that classes of stochastic echo state networks, and therefore the class of all stochastic reservoir computers, are universal approximating classes. We also investigate the performance of two practical examples of stochastic reservoir computers in classification and chaotic time series prediction. While shot noise is a limiting factor in the performance of stochastic reservoir computing, we show significantly improved performance compared to a deterministic reservoir computer with similar hardware in cases where the effects of noise are small.",
        "subjects": [
            "cs.LG",
            "cs.NE",
            "eess.SY",
            "nlin.AO",
            "stat.ML"
        ],
        "comment": "30 pages, 6 figures"
    },
    {
        "paper id": "2405.12387",
        "abstract url": "https://arxiv.org/abs/2405.12387",
        "title": "Conformal Counterfactual Inference under Hidden Confounding",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios. Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference. Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions. To overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding. With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution. Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data. After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs). We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Published in SIGKDD'24"
    },
    {
        "paper id": "2405.12424",
        "abstract url": "https://arxiv.org/abs/2405.12424",
        "title": "Rethinking Robustness Assessment: Adversarial Attacks on Learning-based Quadrupedal Locomotion Controllers",
        "rating": "-1.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Legged locomotion has recently achieved remarkable success with the progress of machine learning techniques, especially deep reinforcement learning (RL). Controllers employing neural networks have demonstrated empirical and qualitative robustness against real-world uncertainties, including sensor noise and external perturbations. However, formally investigating the vulnerabilities of these locomotion controllers remains a challenge. This difficulty arises from the requirement to pinpoint vulnerabilities across a long-tailed distribution within a high-dimensional, temporally sequential space. As a first step towards quantitative verification, we propose a computational method that leverages sequential adversarial attacks to identify weaknesses in learned locomotion controllers. Our research demonstrates that, even state-of-the-art robust controllers can fail significantly under well-designed, low-magnitude adversarial sequence. Through experiments in simulation and on the real robot, we validate our approach's effectiveness, and we illustrate how the results it generates can be used to robustify the original policy and offer valuable insights into the safety of these black-box policies.",
        "subjects": [
            "cs.RO",
            "cs.LG"
        ],
        "comment": "RSS 2024"
    },
    {
        "paper id": "2405.12427",
        "abstract url": "https://arxiv.org/abs/2405.12427",
        "title": "Deep learning approaches to indoor wireless channel estimation for low-power communication",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the rapidly growing development of the Internet of Things (IoT) infrastructure, achieving reliable wireless communication is a challenge. IoT devices operate in diverse environments with common signal interference and fluctuating channel conditions. Accurate channel estimation helps adapt the transmission strategies to current conditions, ensuring reliable communication. Traditional methods, such as Least Squares (LS) and Minimum Mean Squared Error (MMSE) estimation techniques, often struggle to adapt to the diverse and complex environments typical of IoT networks. This research article delves into the potential of Deep Learning (DL) to enhance channel estimation, focusing on the Received Signal Strength Indicator (RSSI) metric - a critical yet challenging aspect due to its susceptibility to noise and environmental factors. This paper presents two Fully Connected Neural Networks (FCNNs)-based Low Power (LP-IoT) channel estimation models, leveraging RSSI for accurate channel estimation in LP-IoT communication. Our Model A exhibits a remarkable 99.02% reduction in Mean Squared Error (MSE), and Model B demonstrates a notable 90.03% MSE reduction compared to the benchmarks set by current studies. Additionally, the comparative studies of our model A with other DL-based techniques show significant efficiency in our estimation models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12462",
        "abstract url": "https://arxiv.org/abs/2405.12462",
        "title": "Boosting X-formers with Structured Matrix for Long Sequence Time Series Forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Transformer-based models for long sequence time series forecasting (LSTF) problems have gained significant attention due to their exceptional forecasting precision. As the cornerstone of these models, the self-attention mechanism poses a challenge to efficient training and inference due to its quadratic time complexity. In this article, we propose a novel architectural design for Transformer-based models in LSTF, leveraging a substitution framework that incorporates Surrogate Attention Blocks and Surrogate FFN Blocks. The framework aims to boost any well-designed model's efficiency without sacrificing its accuracy. We further establish the equivalence of the Surrogate Attention Block to the self-attention mechanism in terms of both expressiveness and trainability. Through extensive experiments encompassing nine Transformer-based models across five time series tasks, we observe an average performance improvement of 9.45% while achieving a significant reduction in model size by 46%",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12473",
        "abstract url": "https://arxiv.org/abs/2405.12473",
        "title": "Learning Partially Aligned Item Representation for Cross-Domain Sequential Recommendation",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Cross-domain sequential recommendation (CDSR) aims to uncover and transfer users' sequential preferences across multiple recommendation domains. While significant endeavors have been made, they primarily concentrated on developing advanced transfer modules and aligning user representations using self-supervised learning techniques. However, the problem of aligning item representations has received limited attention, and misaligned item representations can potentially lead to sub-optimal sequential modeling and user representation alignment. To this end, we propose a model-agnostic framework called \\textbf{C}ross-domain item representation \\textbf{A}lignment for \\textbf{C}ross-\\textbf{D}omain \\textbf{S}equential \\textbf{R}ecommendation (\\textbf{CA-CDSR}), which achieves sequence-aware generation and adaptively partial alignment for item representations. Specifically, we first develop a sequence-aware feature augmentation strategy, which captures both collaborative and sequential item correlations, thus facilitating holistic item representation generation. Next, we conduct an empirical study to investigate the partial representation alignment problem from a spectrum perspective. It motivates us to devise an adaptive spectrum filter, achieving partial alignment adaptively. Furthermore, the aligned item representations can be fed into different sequential encoders to obtain user representations. The entire framework is optimized in a multi-task learning paradigm with an annealing strategy. Extensive experiments have demonstrated that CA-CDSR can surpass state-of-the-art baselines by a significant margin and can effectively align items in representation spaces to enhance performance.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12475",
        "abstract url": "https://arxiv.org/abs/2405.12475",
        "title": "GASE: Graph Attention Sampling with Edges Fusion for Solving Vehicle Routing Problems",
        "rating": "-1.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Learning-based methods have become increasingly popular for solving vehicle routing problems due to their near-optimal performance and fast inference speed. Among them, the combination of deep reinforcement learning and graph representation allows for the abstraction of node topology structures and features in an encoder-decoder style. Such an approach makes it possible to solve routing problems end-to-end without needing complicated heuristic operators designed by domain experts. Existing research studies have been focusing on novel encoding and decoding structures via various neural network models to enhance the node embedding representation. Despite the sophisticated approaches applied, there is a noticeable lack of consideration for the graph-theoretic properties inherent to routing problems. Moreover, the potential ramifications of inter-nodal interactions on the decision-making efficacy of the models have not been adequately explored. To bridge this gap, we propose an adaptive Graph Attention Sampling with the Edges Fusion framework (GASE),where nodes' embedding is determined through attention calculation from certain highly correlated neighbourhoods and edges, utilizing a filtered adjacency matrix. In detail, the selections of particular neighbours and adjacency edges are led by a multi-head attention mechanism, contributing directly to the message passing and node embedding in graph attention sampling networks. Furthermore, we incorporate an adaptive actor-critic algorithm with policy improvements to expedite the training convergence. We then conduct comprehensive experiments against baseline methods on learning-based VRP tasks from different perspectives. Our proposed model outperforms the existing methods by 2.08\\%-6.23\\% and shows stronger generalization ability, achieving state-of-the-art performance on randomly generated instances and real-world datasets.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "24 pages, 5figures, 4 tables"
    },
    {
        "paper id": "2405.12486",
        "abstract url": "https://arxiv.org/abs/2405.12486",
        "title": "Time Matters: Enhancing Pre-trained News Recommendation Models with Robust User Dwell Time Injection",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have revolutionized text comprehension, leading to State-of-the-Art (SOTA) news recommendation models that utilize LLMs for in-depth news understanding. Despite this, accurately modeling user preferences remains challenging due to the inherent uncertainty of click behaviors. Techniques like multi-head attention in Transformers seek to alleviate this by capturing interactions among clicks, yet they fall short in integrating explicit feedback signals. User Dwell Time emerges as a powerful indicator, offering the potential to enhance the weak signals emanating from clicks. Nonetheless, its real-world applicability is questionable, especially when dwell time data collection is subject to delays. To bridge this gap, this paper proposes two novel and robust dwell time injection strategies, namely Dwell time Weight (DweW) and Dwell time Aware (DweA). Dwe} concentrates on refining Effective User Clicks through detailed analysis of dwell time, integrating with initial behavioral inputs to construct a more robust user preference. DweA empowers the model with awareness of dwell time information, thereby facilitating autonomous adjustment of attention values in user modeling. This enhancement sharpens the model's ability to accurately identify user preferences. In our experiment using the real-world news dataset from MSN website, we validated that our two strategies significantly improve recommendation performance, favoring high-quality news. Crucially, our approaches exhibit robustness to user dwell time information, maintaining their ability to recommend high-quality content even in extreme cases where dwell time data is entirely missing.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "10 pages,5 figures"
    },
    {
        "paper id": "2405.11807",
        "abstract url": "https://arxiv.org/abs/2405.11807",
        "title": "Dual-sided Peltier Elements for Rapid Thermal Feedback in Wearables",
        "rating": "-2",
        "keywords": [
            [
                "Thermal"
            ]
        ],
        "abstract": "This paper introduces a motor-driven Peltier device designed to deliver immediate thermal sensations within extended reality (XR) environments. The system incorporates eight motor-driven Peltier elements, facilitating swift transitions between warm and cool sensations by rotating preheated or cooled elements to opposite sides. A multi-layer structure, comprising aluminum and silicone layers, ensures user comfort and safety while maintaining optimal temperatures for thermal stimuli. Time-temperature characteristic analysis demonstrates the system's ability to provide warm and cool sensations efficiently, with a dual-sided lifetime of up to 206 seconds at a 2V input. Our system design is adaptable to various body parts and can be synchronized with corresponding visual stimuli to enhance the immersive sensation of virtual object interaction and information delivery.",
        "subjects": [
            "cs.HC",
            "cs.RO",
            "eess.SY"
        ],
        "comment": "3 pages, 4 figures, ICRA Wearable Workshop 2024 - 1st Workshop on Advancing Wearable Devices and Applications through Novel Design, Sensing, Actuation, and AI"
    },
    {
        "paper id": "2405.11814",
        "abstract url": "https://arxiv.org/abs/2405.11814",
        "title": "Climatic & Anthropogenic Hazards to the Nasca World Heritage: Application of Remote Sensing, AI, and Flood Modelling",
        "rating": "-2",
        "keywords": [
            [
                "LiDAR"
            ],
            [
                "Remote Sensing"
            ],
            [
                "cs.AI",
                "cs.CY",
                "cs.CV"
            ]
        ],
        "abstract": "Preservation of the Nasca geoglyphs at the UNESCO World Heritage Site in Peru is urgent as natural and human impact accelerates. More frequent weather extremes such as flashfloods threaten Nasca artifacts. We demonstrate that runoff models based on (sub-)meter scale, LiDAR-derived digital elevation data can highlight AI-detected geoglyphs that are in danger of erosion. We recommend measures of mitigation to protect the famous \"lizard\", \"tree\", and \"hand\" geoglyphs located close by, or even cut by the Pan-American Highway.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "accepted at IGARSS 2024"
    },
    {
        "paper id": "2405.11836",
        "abstract url": "https://arxiv.org/abs/2405.11836",
        "title": "PLASMA -- Platform for Service Management in Digital Remote Maintenance Applications",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "To support maintenance and servicing of industrial machines, service processes are even today often performed manually and analogously, although supportive technologies such as augmented reality, virtual reality and digital platforms already exist. In many cases, neither technicians on-site nor remote experts have all the essential information and options for suitable actions available. Existing service products and platforms do not cover all the required functions in practice in order to map end-to-end processes. PLASMA is a concept for a Cloud-based remote maintenance platform designed to meet these demands. But for a real-life implementation of PLASMA, security measures are essential as we show in this paper.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "4 pages"
    },
    {
        "paper id": "2405.11844",
        "abstract url": "https://arxiv.org/abs/2405.11844",
        "title": "NeRTCAM: CAM-Based CMOS Implementation of Reference Frames for Neuromorphic Processors",
        "rating": "-2",
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "Neuromorphic architectures mimicking biological neural networks have been proposed as a much more efficient alternative to conventional von Neumann architectures for the exploding compute demands of AI workloads. Recent neuroscience theory on intelligence suggests that Cortical Columns (CCs) are the fundamental compute units in the neocortex and intelligence arises from CC's ability to store, predict and infer information via structured Reference Frames (RFs). Based on this theory, recent works have demonstrated brain-like visual object recognition using software simulation. Our work is the first attempt towards direct CMOS implementation of Reference Frames for building CC-based neuromorphic processors. We propose NeRTCAM (Neuromorphic Reverse Ternary Content Addressable Memory), a CAM-based building block that supports the key operations (store, predict, infer) required to perform inference using RFs. NeRTCAM architecture is presented in detail including its key components. All designs are implemented in SystemVerilog and synthesized in 7nm CMOS, and hardware complexity scaling is evaluated for varying storage sizes. NeRTCAM system for biologically motivated MNIST inference with a storage size of 1024 entries incurs just 0.15 mm^2 area, 400 mW power and 9.18 us critical path latency, demonstrating the feasibility of direct CMOS implementation of CAM-based Reference Frames.",
        "subjects": [
            "cs.AR",
            "cs.ET"
        ],
        "comment": "Accepted and Presented at Neuro-Inspired Computational Elements (NICE) Conference, La Jolla, CA. 2024"
    },
    {
        "paper id": "2405.11936",
        "abstract url": "https://arxiv.org/abs/2405.11936",
        "title": "UAV-VisLoc: A Large-scale Dataset for UAV Visual Localization",
        "rating": "-2",
        "keywords": [
            [
                "navigation"
            ],
            [
                "UAV",
                "satellite",
                "drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The application of unmanned aerial vehicles (UAV) has been widely extended recently. It is crucial to ensure accurate latitude and longitude coordinates for UAVs, especially when the global navigation satellite systems (GNSS) are disrupted and unreliable. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching the ground-down view image of UAV with the ortho satellite maps. However, collecting UAV ground-down view images across diverse locations is costly, leading to a scarcity of large-scale datasets for real-world scenarios. Existing datasets for UAV visual localization are often limited to small geographic areas or are focused only on urban regions with distinct textures. To address this, we define the UAV visual localization task by determining the UAV's real position coordinates on a large-scale satellite map based on the captured ground-down view. In this paper, we present a large-scale dataset, UAV-VisLoc, to facilitate the UAV visual localization task. This dataset comprises images from diverse drones across 11 locations in China, capturing a range of topographical features. The dataset features images from fixed-wing drones and multi-terrain drones, captured at different altitudes and orientations. Our dataset includes 6,742 drone images and 11 satellite maps, with metadata such as latitude, longitude, altitude, and capture date. Our dataset is tailored to support both the training and testing of models by providing a diverse and extensive data.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11960",
        "abstract url": "https://arxiv.org/abs/2405.11960",
        "title": "Dynamic classifier auditing by unsupervised anomaly detection methods: an application in packaging industry predictive maintenance",
        "rating": "-2",
        "keywords": [
            [
                "Support Vector Machine"
            ],
            [
                "anomaly detection"
            ]
        ],
        "abstract": "Predictive maintenance in manufacturing industry applications is a challenging research field. Packaging machines are widely used in a large number of logistic companies' warehouses and must be working uninterruptedly. Traditionally, preventive maintenance strategies have been carried out to improve the performance of these machines. However, this kind of policies does not take into account the information provided by the sensors implemented in the machines. This paper presents an expert system for the automatic estimation of work orders to implement predictive maintenance policies for packaging machines. The key idea is that, from a set of alarms related to sensors implemented in the machine, the expert system should take a maintenance action while optimizing the response time. The work order estimator will act as a classifier, yielding a binary decision of whether a machine must undergo a maintenance action by a technician or not, followed by an unsupervised anomaly detection-based filtering stage to audit the classifier's output. The methods used for anomaly detection were: One-Class Support Vector Machine (OCSVM), Minimum Covariance Determinant (MCD) and a majority (hard) voting ensemble of them. All anomaly detection methods improve the performance of the baseline classifer but the best performance in terms of F1 score was obtained by the majority voting ensemble.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11977",
        "abstract url": "https://arxiv.org/abs/2405.11977",
        "title": "GuidedRec: Guiding Ill-Posed Unsupervised Volumetric Recovery",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "medical",
                "surgery"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce a novel unsupervised approach to reconstructing a 3D volume from only two planar projections that exploits a previous\\-ly-captured 3D volume of the patient. Such volume is readily available in many important medical procedures and previous methods already used such a volume. Earlier methods that work by deforming this volume to match the projections typically fail when the number of projections is very low as the alignment becomes underconstrained. We show how to use a generative model of the volume structures to constrain the deformation and obtain a correct estimate. Moreover, our method is not bounded to a specific sensor calibration and can be applied to new calibrations without retraining. We evaluate our approach on a challenging dataset and show it outperforms state-of-the-art methods. As a result, our method could be used in treatment scenarios such as surgery and radiotherapy while drastically reducing patient radiation exposure.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12076",
        "abstract url": "https://arxiv.org/abs/2405.12076",
        "title": "GAN-GRID: A Novel Generative Attack on Smart Grid Stability Prediction",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "Attack"
            ]
        ],
        "abstract": "The smart grid represents a pivotal innovation in modernizing the electricity sector, offering an intelligent, digitalized energy network capable of optimizing energy delivery from source to consumer. It hence represents the backbone of the energy sector of a nation. Due to its central role, the availability of the smart grid is paramount and is hence necessary to have in-depth control of its operations and safety. To this aim, researchers developed multiple solutions to assess the smart grid's stability and guarantee that it operates in a safe state. Artificial intelligence and Machine learning algorithms have proven to be effective measures to accurately predict the smart grid's stability. Despite the presence of known adversarial attacks and potential solutions, currently, there exists no standardized measure to protect smart grids against this threat, leaving them open to new adversarial attacks. In this paper, we propose GAN-GRID a novel adversarial attack targeting the stability prediction system of a smart grid tailored to real-world constraints. Our findings reveal that an adversary armed solely with the stability model's output, devoid of data or model knowledge, can craft data classified as stable with an Attack Success Rate (ASR) of 0.99. Also by manipulating authentic data and sensor values, the attacker can amplify grid issues, potentially undetected due to a compromised stability prediction system. These results underscore the imperative of fortifying smart grid security mechanisms against adversarial manipulation to uphold system stability and reliability.",
        "subjects": [
            "cs.CR",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12120",
        "abstract url": "https://arxiv.org/abs/2405.12120",
        "title": "EdgeLoc: A Communication-Adaptive Parallel System for Real-Time Localization in Infrastructure-Assisted Autonomous Driving",
        "rating": "-2",
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "This paper presents EdgeLoc, an infrastructure-assisted, real-time localization system for autonomous driving that addresses the incompatibility between traditional localization methods and deep learning approaches. The system is built on top of the Robot Operating System (ROS) and combines the real-time performance of traditional methods with the high accuracy of deep learning approaches. The system leverages edge computing capabilities of roadside units (RSUs) for precise localization to enhance on-vehicle localization that is based on the real-time visual odometry. EdgeLoc is a parallel processing system, utilizing a proposed uncertainty-aware pose fusion solution. It achieves communication adaptivity through online learning and addresses fluctuations via window-based detection. Moreover, it achieves optimal latency and maximum improvement by utilizing auto-splitting vehicle-infrastructure collaborative inference, as well as online distribution learning for decision-making. Even with the most basic end-to-end deep neural network for localization estimation, EdgeLoc realizes a 67.75\\% reduction in the localization error for real-time local visual odometry, a 29.95\\% reduction for non-real-time collaborative inference, and a 30.26\\% reduction compared to Kalman filtering. Finally, accuracy-to-latency conversion was experimentally validated, and an overall experiment was conducted on a practical cellular network. The system is open sourced at https://github.com/LoganCome/EdgeAssistedLocalization.",
        "subjects": [
            "cs.DC",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12171",
        "abstract url": "https://arxiv.org/abs/2405.12171",
        "title": "State of the Practice for Medical Imaging Software",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We selected 29 medical imaging projects from 48 candidates, assessed 10 software qualities by answering 108 questions for each software project, and interviewed 8 of the 29 development teams. Based on the quantitative data, we ranked the MI software with the Analytic Hierarchy Process (AHP). The four top-ranked software products are 3D Slicer, ImageJ, Fiji, and OHIF Viewer. Generally, MI software is in a healthy state as shown by the following: we observed 88% of the documentation artifacts recommended by research software development guidelines, 100% of MI projects use version control tools, and developers appear to use the common quasi-agile research software development process. However, the current state of the practice deviates from the existing guidelines because of the rarity of some recommended artifacts, low usage of continuous integration (17% of the projects), low use of unit testing (about 50% of projects), and room for improvement with documentation (six of nine developers felt their documentation was not clear enough). From interviewing the developers, we identified five pain points and two qualities of potential concern: lack of development time, lack of funding, technology hurdles, ensuring correctness, usability, maintainability, and reproducibility. The interviewees proposed strategies to improve the state of the practice, to address the identified pain points, and to improve software quality. Combining their ideas with ours, we have the following list of recommendations: increase documentation, increase testing by enriching datasets, increase continuous integration usage, move to web applications, employ linters, use peer reviews, design for change, add assurance cases, and incorporate a \"Generate All Things\" approach.",
        "subjects": [
            "cs.SE",
            "cs.CV"
        ],
        "comment": "73 pages, 14 figures, 12 tables"
    },
    {
        "paper id": "2405.12184",
        "abstract url": "https://arxiv.org/abs/2405.12184",
        "title": "Robust VAR Capability Curve of DER with Uncertain Renewable Generation",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Active distribution system with high penetration of inverter based distributed energy resources (DER) can be utilized for VAR-related ancillary services. To utilize the DER flexibility, transmission system operator (TSO) must be presented with the aggregated DER flexibility of distribution system. However, the uncertainty in renewable generation questions the credibility of aggregated capability curve in practice. In this paper, we incorporate the uncertainty into aggregation process to develop a robust capability curve while preserving the real physics (unbalance and lossy nature) of distribution system. Statistical inference method is employed to quantify uncertainty in solar generation and quantified uncertainty is integrated into a chance constrained optimal power flow (OPF). It provides the grid operator with the dispatchable aggregated reactive power capability. The resulting capability curve with the associated probability can be harnessed by the TSO for decision making for both planning and operation.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This paper is accepted for publication in IEEE PESGM 2024, Seattle. The complete copyright version will be available in IEEE Xplore with published conference proceedings"
    },
    {
        "paper id": "2405.12202",
        "abstract url": "https://arxiv.org/abs/2405.12202",
        "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution",
        "rating": "-2",
        "keywords": [
            [
                "Super-resolution"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we present an arbitrary-scale super-resolution (SR) method to enhance the resolution of scientific data, which often involves complex challenges such as continuity, multi-scale physics, and the intricacies of high-frequency signals. Grounded in operator learning, the proposed method is resolution-invariant. The core of our model is a hierarchical neural operator that leverages a Galerkin-type self-attention mechanism, enabling efficient learning of mappings between function spaces. Sinc filters are used to facilitate the information transfer across different levels in the hierarchy, thereby ensuring representation equivalence in the proposed neural operator. Additionally, we introduce a learnable prior structure that is derived from the spectral resizing of the input data. This loss prior is model-agnostic and is designed to dynamically adjust the weighting of pixel contributions, thereby balancing gradients effectively across the model. We conduct extensive experiments on diverse datasets from different domains and demonstrate consistent improvements compared to strong baselines, which consist of various state-of-the-art SR methods.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "20 pages, 14 figures"
    },
    {
        "paper id": "2405.12333",
        "abstract url": "https://arxiv.org/abs/2405.12333",
        "title": "Efficacy of static analysis tools for software defect detection on open-source projects",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "In software practice, static analysis tools remain an integral part of detecting defects in software and there have been various tools designed to run the analysis in different programming languages like Java, C++, and Python. This paper presents an empirical comparison of popular static analysis tools for identifying software defects using several datasets using Java, C++, and Python code. The study used popular analysis tools such as SonarQube, PMD, Checkstyle, and FindBugs to perform the comparison based on using the datasets. The study also used various evaluation metrics such as Precision, Recall, and F1-score to determine the performance of each analysis tool. The study results show that SonarQube performs considerably well than all other tools in terms of its defect detection across the various three programming languages. These findings remain consistent with other existing studies that also agree on SonarQube being an effective tool for defect detection in software. The study contributes to much insight on static analysis tools with different programming languages and additional information to understand the strengths and weaknesses of each analysis tool. The study also discusses the implications for software development researchers and practitioners, and future directions in this area. Our research approach aim is to provide a recommendation guideline to enable software developers, practitioners, and researchers to make the right choice on static analysis tools to detect errors in their software codes. Also, for researchers to embark on investigating and improving software analysis tools to enhance the quality and reliability of the software systems and its software development processes practice.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "6 pages, 1 figure, 1 table"
    },
    {
        "paper id": "2405.12336",
        "abstract url": "https://arxiv.org/abs/2405.12336",
        "title": "Interoperable Provenance Authentication of Broadcast Media using Open Standards-based Metadata, Watermarking and Cryptography",
        "rating": "-2",
        "keywords": [
            [
                "Watermarking"
            ]
        ],
        "abstract": "The spread of false and misleading information is receiving significant attention from legislative and regulatory bodies. Consumers place trust in specific sources of information, so a scalable, interoperable method for determining the provenance and authenticity of information is needed. In this paper we analyze the posting of broadcast news content to a social media platform, the role of open standards, the interplay of cryptographic metadata and watermarks when validating provenance, and likely success and failure scenarios. We conclude that the open standards for cryptographically authenticated metadata developed by the Coalition for Provenance and Authenticity (C2PA) and for audio and video watermarking developed by the Advanced Television Systems Committee (ATSC) are well suited to address broadcast provenance. We suggest methods for using these standards for optimal success.",
        "subjects": [
            "cs.CR",
            "cs.MM"
        ],
        "comment": "17 pages, 9 figures. Submitted to IBC2024 Technical Papers Programme"
    },
    {
        "paper id": "2405.12353",
        "abstract url": "https://arxiv.org/abs/2405.12353",
        "title": "TinyM$^2$Net-V3: Memory-Aware Compressed Multimodal Deep Neural Networks for Sustainable Edge Deployment",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "thermal"
            ],
            [
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "The advancement of sophisticated artificial intelligence (AI) algorithms has led to a notable increase in energy usage and carbon dioxide emissions, intensifying concerns about climate change. This growing problem has brought the environmental sustainability of AI technologies to the forefront, especially as they expand across various sectors. In response to these challenges, there is an urgent need for the development of sustainable AI solutions. These solutions must focus on energy-efficient embedded systems that are capable of handling diverse data types even in environments with limited resources, thereby ensuring both technological progress and environmental responsibility. Integrating complementary multimodal data into tiny machine learning models for edge devices is challenging due to increased complexity, latency, and power consumption. This work introduces TinyM$^2$Net-V3, a system that processes different modalities of complementary data, designs deep neural network (DNN) models, and employs model compression techniques including knowledge distillation and low bit-width quantization with memory-aware considerations to fit models within lower memory hierarchy levels, reducing latency and enhancing energy efficiency on resource-constrained devices. We evaluated TinyM$^2$Net-V3 in two multimodal case studies: COVID-19 detection using cough, speech, and breathing audios, and pose classification from depth and thermal images. With tiny inference models (6 KB and 58 KB), we achieved 92.95% and 90.7% accuracies, respectively. Our tiny machine learning models, deployed on resource limited hardware, demonstrated low latencies within milliseconds and very high power efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted at AAAI 2024 Workshop SAI"
    },
    {
        "paper id": "2405.12359",
        "abstract url": "https://arxiv.org/abs/2405.12359",
        "title": "Design and Analysis of a Detuned Series-Series IPT System with Solenoid Coil Structure for Drone Charging Applications",
        "rating": "-2",
        "keywords": [
            [
                "Drone"
            ]
        ],
        "abstract": "This paper proposes a new coil configuration that uses solenoid ferrites on the receiver side instead of planar ferrite coils that are employed in existing wireless charging systems.The solenoid ferrites are used in the drone legs to help mount the receiver on a moving truck while the two parts of the transmitter are placed on the truck.To validate this idea a detuned transmitter series-series (SS) compensated IPT prototype system has been investigated to charge the onboard battery of a drone.The proposed detuned design helps to limit the system output power and transmitter side current at even zero coupling condition and tolerates misalignment positions. Experimental results demonstrate that the system switching frequency is 245 kHz and the primary side resonates at 223.5 kHz, which is a detuned design The transmitting current is limited to 4.5 A at zero coupling condition with a low power loss of only 4.2 W at zero coupling condition. The target achievable power of the system is 50 W with a 10 mm air gap and horizontal and vertical misalignment in the range of [0, 50 mm].This system has been validated by experiments to charge an 11.1V battery at 48.2W with 88.2% efficiency.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "ITEC 2024 (IEEE Transportation Electrification Conference & Expo)"
    },
    {
        "paper id": "2405.12385",
        "abstract url": "https://arxiv.org/abs/2405.12385",
        "title": "SciJava Ops: An Improved Algorithms Framework for Fiji and Beyond",
        "rating": "-2",
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "Many scientific software platforms provide plugin mechanisms that simplify the integration, deployment, and execution of externally developed functionality. One of the most widely used platforms in the imaging space is Fiji, a popular open-source application for scientific image analysis. Fiji incorporates and builds on the ImageJ and ImageJ2 platforms, which provide a powerful plugin architecture used by thousands of plugins to solve a wide variety of problems. This capability is a major part of Fiji's success, and it has become a widely used biological image analysis tool and a target for new functionality. However, a plugin-based software architecture cannot unify disparate platforms operating on incompatible data structures; interoperability necessitates the creation of adaptation or \"bridge\" layers to translate data and invoke functionality. As a result, while platforms like Fiji enable a high degree of interconnectivity and extensibility, they were not fundamentally designed to integrate across the many data types, programming languages, and architectural differences of various software platforms.To help address this challenge, we present SciJava Ops, a foundational software library for expressing algorithms as plugins in a unified and extensible way. Continuing the evolution of Fiji's SciJava plugin mechanism, SciJava Ops enables users to harness algorithms from various software platforms within a central execution environment. In addition, SciJava Ops automatically adapts data into the most appropriate structure for each algorithm, allowing users to freely and transparently combine algorithms from otherwise incompatible tools. While SciJava Ops is initially distributed as a Fiji update site, the framework does not require Fiji, ImageJ, or ImageJ2, and would be suitable for integration with additional image analysis platforms.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "33 pages, 11 figures"
    },
    {
        "paper id": "2405.12460",
        "abstract url": "https://arxiv.org/abs/2405.12460",
        "title": "Physics-based Scene Layout Generation from Human Motion",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Physics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games. As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one. This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects. Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints. Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions. In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator. To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints. To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction. We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator. To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels. We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "SIGGRAPH conference"
    },
    {
        "paper id": "2405.12487",
        "abstract url": "https://arxiv.org/abs/2405.12487",
        "title": "3DSS-Mamba: 3D-Spectral-Spatial Mamba for Hyperspectral Image Classification",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Hyperspectral image (HSI) classification constitutes the fundamental research in remote sensing fields. Convolutional Neural Networks (CNNs) and Transformers have demonstrated impressive capability in capturing spectral-spatial contextual dependencies. However, these architectures suffer from limited receptive fields and quadratic computational complexity, respectively. Fortunately, recent Mamba architectures built upon the State Space Model integrate the advantages of long-range sequence modeling and linear computational efficiency, exhibiting substantial potential in low-dimensional scenarios. Motivated by this, we propose a novel 3D-Spectral-Spatial Mamba (3DSS-Mamba) framework for HSI classification, allowing for global spectral-spatial relationship modeling with greater computational efficiency. Technically, a spectral-spatial token generation (SSTG) module is designed to convert the HSI cube into a set of 3D spectral-spatial tokens. To overcome the limitations of traditional Mamba, which is confined to modeling causal sequences and inadaptable to high-dimensional scenarios, a 3D-Spectral-Spatial Selective Scanning (3DSS) mechanism is introduced, which performs pixel-wise selective scanning on 3D hyperspectral tokens along the spectral and spatial dimensions. Five scanning routes are constructed to investigate the impact of dimension prioritization. The 3DSS scanning mechanism combined with conventional mapping operations forms the 3D-spectral-spatial mamba block (3DMB), enabling the extraction of global spectral-spatial semantic representations. Experimental results and analysis demonstrate that the proposed method outperforms the state-of-the-art methods on HSI classification benchmarks.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12491",
        "abstract url": "https://arxiv.org/abs/2405.12491",
        "title": "Bridging the Gap Between Domain-specific Frameworks and Multiple Hardware Devices",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "The rapid development of domain-specific frameworks has presented us with a significant challenge: The current approach of implementing solutions on a case-by-case basis incurs a theoretical complexity of O(M*N), thereby increasing the cost of porting applications to different hardware platforms. To address these challenges, we propose a systematic methodology that effectively bridges the gap between domain-specific frameworks and multiple hardware devices, reducing porting complexity to O(M+N). The approach utilizes multi-layer abstractions. Different domain-specific abstractions are employed to represent applications from various domains. These abstractions are then transformed into a unified abstraction, which is subsequently translated into combinations of primitive operators. Finally, these operators are mapped to multiple hardware platforms. The implemented unified framework supports deep learning, classical machine learning, and data analysis across X86, ARM, RISC-V, IoT devices, and GPU. It outperforms existing solutions like scikit-learn, hummingbird, Spark, and pandas, achieving impressive speedups: 1.1x to 3.83x on X86 servers, 1.06x to 4.33x on ARM IoT devices, 1.25x to 3.72x on RISC-V IoT devices, and 1.93x on GPU. The source code is available at https://github.com/BenchCouncil/bridger.git.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "15pages, 8 figures"
    },
    {
        "paper id": "2405.11785",
        "abstract url": "https://arxiv.org/abs/2405.11785",
        "title": "Guided Multi-objective Generative AI to Enhance Structure-based Drug Design",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "chemistry",
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative AI has the potential to revolutionize drug discovery. Yet, despite recent advances in machine learning, existing models cannot generate molecules that satisfy all desired physicochemical properties. Herein, we describe IDOLpro, a novel generative chemistry AI combining deep diffusion with multi-objective optimization for structure-based drug design. The latent variables of the diffusion model are guided by differentiable scoring functions to explore uncharted chemical space and generate novel ligands in silico, optimizing a plurality of target physicochemical properties. We demonstrate its effectiveness by generating ligands with optimized binding affinity and synthetic accessibility on two benchmark sets. IDOLpro produces ligands with binding affinities over 10% higher than the next best state-of-the-art on each test set. On a test set of experimental complexes, IDOLpro is the first to surpass the performance of experimentally observed ligands. IDOLpro can accommodate other scoring functions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead optimization for drug discovery.",
        "subjects": [
            "physics.chem-ph",
            "cs.LG",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11828",
        "abstract url": "https://arxiv.org/abs/2405.11828",
        "title": "Federated Learning with Incomplete Sensing Modalities",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "physiological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many mobile sensing applications utilize data from various modalities, including motion and physiological sensors in mobile and wearable devices. Federated Learning (FL) is particularly suitable for these applications thanks to its privacy-preserving feature. However, challenges such as limited battery life, poor network conditions, and sensor malfunctions can restrict the use of all available modalities for local model training. Additionally, existing multimodal FL systems also struggle with scalability and efficiency as the number of modality sources increases. To address these issues, we introduce FLISM, a framework designed to enable multimodal FL with incomplete modalities. FLISM leverages simulation technique to learn robust representations that can handle missing modalities and transfers model knowledge across clients with varying set of modalities. The evaluation results using three real-world datasets and simulations demonstrate FLISM's effective balance between model performance and system efficiency. It shows an average improvement of .067 in F1-score, while also reducing communication (2.69x faster) and computational (2.28x more efficient) overheads compared to existing methods addressing incomplete modalities. Moreover, in simulated scenarios involving tasks with a larger number of modalities, FLISM achieves a significant speedup of 3.23x~85.10x in communication and 3.73x~32.29x in computational efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12085",
        "abstract url": "https://arxiv.org/abs/2405.12085",
        "title": "Noise-tolerant learnability of shallow quantum circuits from statistics and the cost of quantum pseudorandomness",
        "rating": "-2.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work studies the learnability of unknown quantum circuits in the near term. We prove the natural robustness of quantum statistical queries for learning quantum processes and provide an efficient way to benchmark various classes of noise from statistics, which gives us a powerful framework for developing noise-tolerant algorithms. We adapt a learning algorithm for constant-depth quantum circuits to the quantum statistical query setting with a small overhead in the query complexity. We prove average-case lower bounds for learning random quantum circuits of logarithmic and higher depths within diamond distance with statistical queries. Additionally, we show the hardness of the quantum threshold search problem from quantum statistical queries and discuss its implications for the learnability of shallow quantum circuits. Finally, we prove that pseudorandom unitaries (PRUs) cannot be constructed using circuits of constant depth by constructing an efficient distinguisher and proving a new variation of the quantum no-free lunch theorem.",
        "subjects": [
            "quant-ph",
            "cs.CC",
            "cs.CR",
            "cs.LG"
        ],
        "comment": "23+7 pages, 2 figures"
    },
    {
        "paper id": "2405.12122",
        "abstract url": "https://arxiv.org/abs/2405.12122",
        "title": "An Active Learning Framework with a Class Balancing Strategy for Time Series Classification",
        "rating": "-2.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Training machine learning models for classification tasks often requires labeling numerous samples, which is costly and time-consuming, especially in time series analysis. This research investigates Active Learning (AL) strategies to reduce the amount of labeled data needed for effective time series classification. Traditional AL techniques cannot control the selection of instances per class for labeling, leading to potential bias in classification performance and instance selection, particularly in imbalanced time series datasets. To address this, we propose a novel class-balancing instance selection algorithm integrated with standard AL strategies. Our approach aims to select more instances from classes with fewer labeled examples, thereby addressing imbalance in time series datasets. We demonstrate the effectiveness of our AL framework in selecting informative data samples for two distinct domains of tactile texture recognition and industrial fault detection. In robotics, our method achieves high-performance texture categorization while significantly reducing labeled training data requirements to 70%. We also evaluate the impact of different sliding window time intervals on robotic texture classification using AL strategies. In synthetic fiber manufacturing, we adapt AL techniques to address the challenge of fault classification, aiming to minimize data annotation cost and time for industries. We also address real-life class imbalances in the multiclass industrial anomalous dataset using our class-balancing instance algorithm integrated with AL strategies. Overall, this thesis highlights the potential of our AL framework across these two distinct domains.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Master's thesis accepted by Memorial University of Newfoundland. Chapter 3 published in the Journal of Frontiers in Robotics and AI. Chapter 4 published in the IEEE Systems Conference 2024"
    },
    {
        "paper id": "2405.12193",
        "abstract url": "https://arxiv.org/abs/2405.12193",
        "title": "The Narrow Depth and Breadth of Corporate Responsible AI Research",
        "rating": "-2.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "patent"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "The transformative potential of AI presents remarkable opportunities, but also significant risks, underscoring the importance of responsible AI development and deployment. Despite a growing emphasis on this area, there is limited understanding of industry's engagement in responsible AI research, i.e., the critical examination of AI's ethical, social, and legal dimensions. To address this gap, we analyzed over 6 million peer-reviewed articles and 32 million patent citations using multiple methods across five distinct datasets to quantify industry's engagement. Our findings reveal that the majority of AI firms show limited or no engagement in this critical subfield of AI. We show a stark disparity between industry's dominant presence in conventional AI research and its limited engagement in responsible AI. Leading AI firms exhibit significantly lower output in responsible AI research compared to their conventional AI research and the contributions of leading academic institutions. Our linguistic analysis documents a narrower scope of responsible AI research within industry, with a lack of diversity in key topics addressed. Our large-scale patent citation analysis uncovers a pronounced disconnect between responsible AI research and the commercialization of AI technologies, suggesting that industry patents rarely build upon insights generated by the responsible AI literature. This gap highlights the potential for AI development to diverge from a socially optimal path, risking unintended consequences due to insufficient consideration of ethical and societal implications. Our results highlight the urgent need for industry to publicly engage in responsible AI research to absorb academic knowledge, cultivate public trust, and proactively mitigate AI-induced societal harms.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12308",
        "abstract url": "https://arxiv.org/abs/2405.12308",
        "title": "Continual Deep Reinforcement Learning for Decentralized Satellite Routing",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a full solution for decentralized routing in Low Earth Orbit satellite constellations based on continual Deep Reinforcement Learning (DRL). This requires addressing multiple challenges, including the partial knowledge at the satellites and their continuous movement, and the time-varying sources of uncertainty in the system, such as traffic, communication links, or communication buffers. We follow a multi-agent approach, where each satellite acts as an independent decision-making agent, while acquiring a limited knowledge of the environment based on the feedback received from the nearby agents. The solution is divided into two phases. First, an offline learning phase relies on decentralized decisions and a global Deep Neural Network (DNN) trained with global experiences. Then, the online phase with local, on-board, and pre-trained DNNs requires continual learning to evolve with the environment, which can be done in two different ways: (1) Model anticipation, where the predictable conditions of the constellation are exploited by each satellite sharing local model with the next satellite; and (2) Federated Learning (FL), where each agent's model is merged first at the cluster level and then aggregated in a global Parameter Server. The results show that, without high congestion, the proposed Multi-Agent DRL framework achieves the same E2E performance as a shortest-path solution, but the latter assumes intensive communication overhead for real-time network-wise knowledge of the system at a centralized node, whereas ours only requires limited feedback exchange among first neighbour satellites. Importantly, our solution adapts well to congestion conditions and exploits less loaded paths. Moreover, the divergence of models over time is easily tackled by the synergy between anticipation, applied in short-term alignment, and FL, utilized for long-term alignment.",
        "subjects": [
            "cs.LG",
            "cs.IT"
        ],
        "comment": "30 pages, 11 figures"
    },
    {
        "paper id": "2405.12442",
        "abstract url": "https://arxiv.org/abs/2405.12442",
        "title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Concept recommendation aims to suggest the next concept for learners to study based on their knowledge states and the human knowledge system. While knowledge states can be predicted using knowledge tracing models, previous approaches have not effectively integrated the human knowledge system into the process of designing these educational models. In the era of rapidly evolving Large Language Models (LLMs), many fields have begun using LLMs to generate and encode text, introducing external knowledge. However, integrating LLMs into concept recommendation presents two urgent challenges: 1) How to construct text for concepts that effectively incorporate the human knowledge system? 2) How to adapt non-smooth, anisotropic text encodings effectively for concept recommendation? In this paper, we propose a novel Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC). We leverage factual knowledge from LLMs as well as the precedence and succession relationships between concepts obtained from the knowledge graph to construct textual representations of concepts. Furthermore, we propose a graph-based adapter to adapt anisotropic text embeddings to the concept recommendation task. This adapter is pre-trained through contrastive learning on the knowledge graph to get a smooth and structure-aware concept representation. Then, it's fine-tuned through the recommendation task, forming a text-to-knowledge-to-recommendation adaptation pipeline, which effectively constructs a structure and knowledge-aware concept representation. Our method does a better job than previous adapters in transforming text encodings for application in concept recommendation. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "11 pages, 8 figures"
    },
    {
        "paper id": "2405.12452",
        "abstract url": "https://arxiv.org/abs/2405.12452",
        "title": "Prompt-Enhanced Spatio-Temporal Graph Transfer Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Spatio-temporal graph neural networks have demonstrated efficacy in capturing complex dependencies for urban computing tasks such as forecasting and kriging. However, their performance is constrained by the reliance on extensive data for training on specific tasks, which limits their adaptability to new urban domains with varied demands. Although transfer learning has been proposed to address this problem by leveraging knowledge across domains, cross-task generalization remains underexplored in spatio-temporal graph transfer learning methods due to the absence of a unified framework. To bridge this gap, we propose Spatio-Temporal Graph Prompting (STGP), a prompt-enhanced transfer learning framework capable of adapting to diverse tasks in data-scarce domains. Specifically, we first unify different tasks into a single template and introduce a task-agnostic network architecture that aligns with this template. This approach enables the capture of spatio-temporal dependencies shared across tasks. Furthermore, we employ learnable prompts to achieve domain and task transfer in a two-stage prompting pipeline, enabling the prompts to effectively capture domain knowledge and task-specific properties at each stage. Extensive experiments demonstrate that STGP outperforms state-of-the-art baselines in three downstream tasks forecasting, kriging, and extrapolation by a notable margin.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12465",
        "abstract url": "https://arxiv.org/abs/2405.12465",
        "title": "A finite element-based physics-informed operator learning framework for spatiotemporal partial differential equations on arbitrary domains",
        "rating": "-2.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "thermal"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a novel finite element-based physics-informed operator learning framework that allows for predicting spatiotemporal dynamics governed by partial differential equations (PDEs). The proposed framework employs a loss function inspired by the finite element method (FEM) with the implicit Euler time integration scheme. A transient thermal conduction problem is considered to benchmark the performance. The proposed operator learning framework takes a temperature field at the current time step as input and predicts a temperature field at the next time step. The Galerkin discretized weak formulation of the heat equation is employed to incorporate physics into the loss function, which is coined finite operator learning (FOL). Upon training, the networks successfully predict the temperature evolution over time for any initial temperature field at high accuracy compared to the FEM solution. The framework is also confirmed to be applicable to a heterogeneous thermal conductivity and arbitrary geometry. The advantages of FOL can be summarized as follows: First, the training is performed in an unsupervised manner, avoiding the need for a large data set prepared from costly simulations or experiments. Instead, random temperature patterns generated by the Gaussian random process and the Fourier series, combined with constant temperature fields, are used as training data to cover possible temperature cases. Second, shape functions and backward difference approximation are exploited for the domain discretization, resulting in a purely algebraic equation. This enhances training efficiency, as one avoids time-consuming automatic differentiation when optimizing weights and biases while accepting possible discretization errors. Finally, thanks to the interpolation power of FEM, any arbitrary geometry can be handled with FOL, which is crucial to addressing various engineering application scenarios.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11808",
        "abstract url": "https://arxiv.org/abs/2405.11808",
        "title": "Online Learning Feedback Control Considering Hysteresis for Musculoskeletal Structures",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "biomimetic"
            ]
        ],
        "abstract": "While the musculoskeletal humanoid has various biomimetic benefits, its complex modeling is difficult, and many learning control methods have been developed. However, for the actual robot, the hysteresis of its joint angle tracking is still an obstacle, and realizing target posture quickly and accurately has been difficult. Therefore, we develop a feedback control method considering the hysteresis. To solve the problem in feedback controls caused by the closed-link structure of the musculoskeletal body, we update a neural network representing the relationship between the error of joint angles and the change in target muscle lengths online, and realize target joint angles accurately in a few trials. We compare the performance of several configurations with various network structures and loss definitions, and verify the effectiveness of this study on an actual musculoskeletal humanoid, Musashi.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at IROS2022"
    },
    {
        "paper id": "2405.11885",
        "abstract url": "https://arxiv.org/abs/2405.11885",
        "title": "Post-Quantum Security: Origin, Fundamentals, and Adoption",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Nowadays, predominant asymmetric cryptographic schemes are considered to be secure because discrete logarithms are believed to be hard to be computed. The algorithm of Shor can effectively compute discrete logarithms, i.e. it can brake such asymmetric schemes. But the algorithm of Shor is a quantum algorithm and at the time this algorithm has been invented, quantum computers that may successfully execute this algorithm seemed to be far out in the future. The latter has changed: quantum computers that are powerful enough are likely to be available in a couple of years. In this article, we first describe the relation between discrete logarithms and two well-known asymmetric security schemes, RSA and Elliptic Curve Cryptography. Next, we present the foundations of lattice-based cryptography which is the bases of schemes that are considered to be safe against attacks by quantum algorithms (as well as by classical algorithms). Then we describe two such quantum-safe algorithms (Kyber and Dilithium) in more detail. Finally, we give a very brief and selective overview of a few actions currently taken by governments and industry as well as standardization in this area. The article especially strives towards being self-contained: the required mathematical foundations to understand post-quantum cryptography are provided and examples are given.",
        "subjects": [
            "cs.CR",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11909",
        "abstract url": "https://arxiv.org/abs/2405.11909",
        "title": "3D Reconfigurable Intelligent Surfaces for Satellite-Terrestrial Networks",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "This paper proposes a three-dimensional (3D) satellite-terrestrial communication network assisted with reconfigurable intelligent surfaces (RISs). Using stochastic geometry models, we present an original framework to derive tractable yet accurate closed-form expressions for coverage probability and ergodic capacity in the presence of fading. A homogeneous Poisson point process models the satellites on a sphere, while RISs are randomly deployed in a 3D cylindrical region. We consider nonidentical channels that correspond to different RISs and follow the \u03ba-\u03bc fading distribution. We verify the high accuracy of the adopted approach through Monte Carlo simulations and demonstrate the significant improvement in system performance due to using RISs. Furthermore, we comprehensively study the effect of the different system parameters on its performance using the derived analytical expressions, which enable system engineers to predict and optimize the expected downlink coverage and capacity performance analytically.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11963",
        "abstract url": "https://arxiv.org/abs/2405.11963",
        "title": "A Simulation Tool for V2G Enabled Demand Response Based on Model Predictive Control",
        "rating": "-3",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "health"
            ]
        ],
        "abstract": "Integrating electric vehicles (EVs) into the power grid can revolutionize energy management strategies, offering both challenges and opportunities for creating a more sustainable and resilient grid. In this context, model predictive control (MPC) emerges as a powerful tool for addressing the complexities of Grid-to-vehicle (G2V) and vehicle-to-grid (V2G) enabled demand response management. By leveraging advanced optimization techniques, MPC algorithms can anticipate future grid conditions and dynamically adjust EV charging and discharging schedules to balance supply and demand while minimizing operational costs and maximizing flexibility. However, no standard tools exist to evaluate novel energy management strategies based on MPC approaches. Our research focuses on harnessing the potential of MPC in G2V and V2G applications, by providing a simulation tool that allows to maximize EV flexibility and support demand response initiatives while mitigating the impact on EV battery health. In this paper, we propose an open-source MPC controller for G2V and V2G-enabled demand response management. The proposed approach is capable of tackling the uncertainties inherent in demand response operations. Through extensive simulation and analysis, we demonstrate the efficacy of our approach in maximizing the benefits of G2V and V2G while assessing the impact on the longevity and reliability of EV batteries. Specifically, our controller enables Charge Point Operators (CPOs) to optimize EV charging and discharging schedules in real-time, taking into account fluctuating energy prices, grid constraints, and EV user preferences.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11976",
        "abstract url": "https://arxiv.org/abs/2405.11976",
        "title": "Position-Guided Prompt Learning for Anomaly Detection in Chest X-Rays",
        "rating": "-3",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Anomaly detection in chest X-rays is a critical task. Most methods mainly model the distribution of normal images, and then regard significant deviation from normal distribution as anomaly. Recently, CLIP-based methods, pre-trained on a large number of medical images, have shown impressive performance on zero/few-shot downstream tasks. In this paper, we aim to explore the potential of CLIP-based methods for anomaly detection in chest X-rays. Considering the discrepancy between the CLIP pre-training data and the task-specific data, we propose a position-guided prompt learning method. Specifically, inspired by the fact that experts diagnose chest X-rays by carefully examining distinct lung regions, we propose learnable position-guided text and image prompts to adapt the task data to the frozen pre-trained CLIP-based model. To enhance the model's discriminative capability, we propose a novel structure-preserving anomaly synthesis method within chest x-rays during the training process. Extensive experiments on three datasets demonstrate that our proposed method outperforms some state-of-the-art methods. The code of our implementation is available at https://github.com/sunzc-sunny/PPAD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "MICCAI 2024 Early Accept"
    },
    {
        "paper id": "2405.12121",
        "abstract url": "https://arxiv.org/abs/2405.12121",
        "title": "Insecurity of Quantum Two-Party Computation with Applications to Cheat-Sensitive Protocols and Oblivious Transfer Reductions",
        "rating": "-3",
        "keywords": [
            [
                "attack"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Oblivious transfer (OT) is a fundamental primitive for secure two-party computation. It is well known that OT cannot be implemented with information-theoretic security if the two players only have access to noiseless communication channels, even in the quantum case. As a result, weaker variants of OT have been studied. In this work, we rigorously establish the impossibility of cheat-sensitive OT, where a dishonest party can cheat, but risks being detected. We construct a general attack on any quantum protocol that allows the receiver to compute all inputs of the sender and provide an explicit upper bound on the success probability of this attack. This implies that cheat-sensitive quantum Symmetric Private Information Retrieval cannot be implemented with statistical information-theoretic security. Leveraging the techniques devised for our proofs, we provide entropic bounds on primitives needed for secure function evaluation. They imply impossibility results for protocols where the players have access to OT as a resource. This result significantly improves upon existing bounds and yields tight bounds for reductions of 1-out-of-n OT to a resource primitive. Our results hold in particular for transformations between a finite number of primitives and for any error.",
        "subjects": [
            "quant-ph",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12177",
        "abstract url": "https://arxiv.org/abs/2405.12177",
        "title": "Establishing Trust in the Beyond-5G Core Network using Trusted Execution Environments",
        "rating": "-3",
        "keywords": [
            [
                "attack"
            ],
            [
                "5G"
            ]
        ],
        "abstract": "The fifth generation (5G) of cellular networks starts a paradigm shift from the traditional monolithic system design to a Service Based Architecture, that fits modern performance requirements and scales efficiently to new services. This paradigm will be the foundation of future cellular core networks beyond 5G. The new architecture splits network functionalities into smaller logical entities that can be disaggregated logically, physically, and geographically. This affords interoperability between the mobile network operators and commercial software and hardware vendors or cloud providers. By making use of commodity services and products, this system construct inherits the vulnerabilities in those underlying technologies, thereby increasing its attack surface and requiring a rigorous security analysis. In this work, we review the security implications introduced in B5G networks, and the security mechanisms that are supported by the 5G standard. We emphasize on the support of Zero Trust Architecture in 5G and its relevance in decentralized deployments. We revisit the definition of trust in modern enterprise network operations and identify important Zero Trust properties that are weakened by the nature of cloud deployments. To that end, we propose a vertical extension of Zero Trust, namely, Zero Trust Execution, to model untrusted execution environments, and we provide an analysis on how to establish trust in Beyond-5G network architectures using Trusted Execution Environments. Our analysis shows how our model architecture handles the increased attack surface and reinforces the Zero Trust Architecture principles in the 5G Core, without any changes to the 5G standard. Finally, we provide experimental results over a 5G testbed using Open5GS and UERANSIM that demonstrate minimal performance overhead, and a monetary cost evaluation.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "18 pages, 8 figures"
    },
    {
        "paper id": "2405.12265",
        "abstract url": "https://arxiv.org/abs/2405.12265",
        "title": "SEL-CIE: Knowledge-Guided Self-Supervised Learning Framework for CIE-XYZ Reconstruction from Non-Linear sRGB Images",
        "rating": "-3",
        "keywords": [
            [
                "medical"
            ],
            [
                "dehazing"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Modern cameras typically offer two types of image states: a minimally processed linear raw RGB image representing the raw sensor data, and a highly-processed non-linear image state, such as the sRGB state. The CIE-XYZ color space is a device-independent linear space used as part of the camera pipeline and can be helpful for computer vision tasks, such as image deblurring, dehazing, and color recognition tasks in medical applications, where color accuracy is important. However, images are usually saved in non-linear states, and achieving CIE-XYZ color images using conventional methods is not always possible. To tackle this issue, classical methodologies have been developed that focus on reversing the acquisition pipeline. More recently, supervised learning has been employed, using paired CIE-XYZ and sRGB representations of identical images. However, obtaining a large-scale dataset of CIE-XYZ and sRGB pairs can be challenging. To overcome this limitation and mitigate the reliance on large amounts of paired data, self-supervised learning (SSL) can be utilized as a substitute for relying solely on paired data. This paper proposes a framework for using SSL methods alongside paired data to reconstruct CIE-XYZ images and re-render sRGB images, outperforming existing approaches. The proposed framework is applied to the sRGB2XYZ dataset.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "10 pages, 4 figures, WSCG 2024 conference"
    },
    {
        "paper id": "2405.12313",
        "abstract url": "https://arxiv.org/abs/2405.12313",
        "title": "Deep learning-based hyperspectral image reconstruction for quality assessment of agro-product",
        "rating": "-3",
        "keywords": [
            [
                "Hyperspectral imaging",
                "agricultural"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Hyperspectral imaging (HSI) has recently emerged as a promising tool for many agricultural applications; however, the technology cannot be directly used in a real-time system due to the extensive time needed to process large volumes of data. Consequently, the development of a simple, compact, and cost-effective imaging system is not possible with the current HSI systems. Therefore, the overall goal of this study was to reconstruct hyperspectral images from RGB images through deep learning for agricultural applications. Specifically, this study used Hyperspectral Convolutional Neural Network - Dense (HSCNN-D) to reconstruct hyperspectral images from RGB images for predicting soluble solid content (SSC) in sweet potatoes. The algorithm accurately reconstructed the hyperspectral images from RGB images, with the resulting spectra closely matching the ground-truth. The partial least squares regression (PLSR) model based on reconstructed spectra outperformed the model using the full spectral range, demonstrating its potential for SSC prediction in sweet potatoes. These findings highlight the potential of deep learning-based hyperspectral image reconstruction as a low-cost, efficient tool for various agricultural uses.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2405.12322",
        "abstract url": "https://arxiv.org/abs/2405.12322",
        "title": "Securing Blockchain-based IoT Systems with Physical Unclonable Functions and Zero-Knowledge Proofs",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "This paper presents a framework for securing blockchain-based IoT systems by integrating Physical Unclonable Functions (PUFs) and Zero-Knowledge Proofs (ZKPs) within a Hyperledger Fabric environment. The proposed framework leverages PUFs for unique device identification and ZKPs for privacy-preserving authentication and transaction processing. Experimental results demonstrate the framework's feasibility, performance, and security against various attacks. This framework provides a comprehensive solution for addressing the security challenges in blockchain-based IoT systems.",
        "subjects": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12357",
        "abstract url": "https://arxiv.org/abs/2405.12357",
        "title": "Paired Conditional Generative Adversarial Network for Highly Accelerated Liver 4D MRI",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "GAN"
            ],
            [
                "MRI",
                "cancer"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Purpose: 4D MRI with high spatiotemporal resolution is desired for image-guided liver radiotherapy. Acquiring densely sampling k-space data is time-consuming. Accelerated acquisition with sparse samples is desirable but often causes degraded image quality or long reconstruction time. We propose the Reconstruct Paired Conditional Generative Adversarial Network (Re-Con-GAN) to shorten the 4D MRI reconstruction time while maintaining the reconstruction quality. Methods: Patients who underwent free-breathing liver 4D MRI were included in the study. Fully- and retrospectively under-sampled data at 3, 6 and 10 times (3x, 6x and 10x) were first reconstructed using the nuFFT algorithm. Re-Con-GAN then trained input and output in pairs. Three types of networks, ResNet9, UNet and reconstruction swin transformer, were explored as generators. PatchGAN was selected as the discriminator. Re-Con-GAN processed the data (3D+t) as temporal slices (2D+t). A total of 48 patients with 12332 temporal slices were split into training (37 patients with 10721 slices) and test (11 patients with 1611 slices). Results: Re-Con-GAN consistently achieved comparable/better PSNR, SSIM, and RMSE scores compared to CS/UNet models. The inference time of Re-Con-GAN, UNet and CS are 0.15s, 0.16s, and 120s. The GTV detection task showed that Re-Con-GAN and CS, compared to UNet, better improved the dice score (3x Re-Con-GAN 80.98%; 3x CS 80.74%; 3x UNet 79.88%) of unprocessed under-sampled images (3x 69.61%). Conclusion: A generative network with adversarial training is proposed with promising and efficient reconstruction results demonstrated on an in-house dataset. The rapid and qualitative reconstruction of 4D liver MR has the potential to facilitate online adaptive MR-guided radiotherapy for liver cancer.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12436",
        "abstract url": "https://arxiv.org/abs/2405.12436",
        "title": "Computational Fabrication and Assembly for In Situ Manufacturing",
        "rating": "-3",
        "keywords": [
            [
                "robotics"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "Fabrication today relies on disparate, large machines spread across industrial facilities. These are operated by domain experts to construct and assemble artefacts in sequential steps from large numbers of parts. This traditional, centralized mass manufacturing paradigm is characterized by large capital costs and inflexibility to changing needs, complex global supply chains hinged on economic and political stability, and waste and over-manufacturing of uniform artefacts that fail to meet the technical and personal needs of today's diverse individuals and use cases. Today, these challenges are particularly severe at points of need, such as the space environment. The space environment is remote and unpredictable, and the ability to manufacture in situ offers unique opportunities to address new challenges as they arise. However, the challenges faced in space are often mirrored on Earth. In hospitals, disaster zones, low resource environments and laboratories, the ability to manufacture customized artefacts at points of need can significantly enhance our ability to respond rapidly to unforeseen events. In this thesis, I introduce digital fabrication platforms with co-developed hardware and software that draw on tools from robotics and human-computer interaction to automate manufacturing of customized artefacts at the point of need. Highlighting three research themes across fabrication machines, modular assembly, and programmable materials, the thesis will cover a digital fabrication platform for producing functional robots, a modular robotic platform for in-space assembly deployed in microgravity, and a method for programming magnetic material to selectively assemble.",
        "subjects": [
            "cs.RO",
            "cs.ET"
        ],
        "comment": "MIT PhD Thesis 2024"
    },
    {
        "paper id": "2405.11848",
        "abstract url": "https://arxiv.org/abs/2405.11848",
        "title": "Alternators For Sequence Modeling",
        "rating": "-3.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "forecast"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations. When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features. In both cases, the OTN learns to produce sequences that match the data. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience, to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "physics.ao-ph",
            "q-bio.NC"
        ],
        "comment": "A new versatile family of sequence models that can be used for both generative modeling and supervised learning. The codebase will be made available upon publication. This paper is dedicated to Thomas Sankara"
    },
    {
        "paper id": "2405.11967",
        "abstract url": "https://arxiv.org/abs/2405.11967",
        "title": "Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home",
        "rating": "-3.5",
        "keywords": [
            [
                "health"
            ],
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "One of the new trends in the development of recommendation algorithms is the dissemination of their capabilities to support the population in managing their health. This article focuses on the problem of improving the effectiveness of cardiovascular diseases (CVD) prevention, since CVD is the leading cause of death worldwide. To address this issue, a knowledge-based recommendation algorithm was proposed to support self-management of CVD risk factors in adults at home. The proposed algorithm is based on the original multidimensional recommendation model and on a new user profile model, which includes predictive assessments of CVD health in addition to its current ones as outlined in official guidelines. The main feature of the proposed algorithm is the combination of rule-based logic with the capabilities of a large language model in generating human-like text for explanatory component of multidimensional recommendation. The verification and evaluation of the proposed algorithm showed the usefulness of the proposed recommendation algorithm for supporting adults in self-management of their CVD risk factors at home. As follows from the comparison with similar knowledge-based recommendation algorithms, the proposed algorithm evaluates a larger number of CVD risk factors and has a greater information and semantic capacity of the generated recommendations.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "26 pages, 5 figures"
    },
    {
        "paper id": "2405.12026",
        "abstract url": "https://arxiv.org/abs/2405.12026",
        "title": "Enzymatic cycle-based receivers with high input impedance for approximate maximum a posteriori demodulation of concentration modulated signals",
        "rating": "-4",
        "keywords": [
            [
                "bio-inspired"
            ],
            [
                "chemical"
            ]
        ],
        "abstract": "Molecular communication is a bio-inspired communication paradigm where molecules are used as the information carrier. This paper considers a molecular communication network where the transmitter uses concentration modulated signals for communication. Our focus is to design receivers that can demodulate these signals. We impose three features on our receivers. We want the receivers to use enzymatic cycles as their building blocks, have high input impedance and can work approximately as a maximum a posteriori (MAP) demodulator. No receivers with all these three features exist in the current molecular communication literature. We consider enzymatic cycles because they are a very common class of chemical reactions that are found in living cells. Since a receiver is to be placed in the communication environment, it should ideally have a high input impedance so that it has minimal impact on the environment and on other receivers. Lastly, a MAP receiver has good statistical performance. In this paper, we show how we can use time-scale separation to make an enzymatic cycle to have high input impedance and how the parameters of the enzymatic cycles can be chosen so that the receiver can approximately implement a MAP demodulator. We use simulation to study the performance of this receiver. In particular, we consider an environment with multiple receivers and show that a receiver has little impact on the bit error ratio of a nearby receiver because they have high input impedance.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12028",
        "abstract url": "https://arxiv.org/abs/2405.12028",
        "title": "The Case for DeepSOH: Addressing Path Dependency for Remaining Useful Life",
        "rating": "-4",
        "keywords": [
            [
                "health"
            ],
            [
                "forecast"
            ]
        ],
        "abstract": "The battery state of health (SOH) based on capacity fade and resistance increase is not sufficient for predicting Remaining Useful life (RUL). The electrochemical community blames the path-dependency of the battery degradation mechanisms for our inability to forecast the degradation. The control community knows that the path-dependency is addressed by full state estimation. We show that even the electrode-specific SOH (eSOH) estimation is not enough to fully define the degradation states by simulating infinite possible degradation trajectories and remaining useful lives (RUL) from a unique eSOH. We finally define the deepSOH states that capture the individual contributions of all the common degradation mechanisms, namely, SEI, plating, and mechanical fracture to the loss of lithium inventory. We show that the addition of cell expansion measurement may allow us to estimate the deepSOH and predict the remaining useful life.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 3 figures"
    },
    {
        "paper id": "2405.12155",
        "abstract url": "https://arxiv.org/abs/2405.12155",
        "title": "Embracing Radiance Field Rendering in 6G: Over-the-Air Training and Inference with 3D Contents",
        "rating": "-4",
        "keywords": [
            [
                "3D",
                "Gaussian splatting",
                "NeRF"
            ],
            [
                "federated learning"
            ],
            [
                "6G"
            ]
        ],
        "abstract": "The efficient representation, transmission, and reconstruction of three-dimensional (3D) contents are becoming increasingly important for sixth-generation (6G) networks that aim to merge virtual and physical worlds for offering immersive communication experiences. Neural radiance field (NeRF) and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D representation techniques based on radiance field rendering, which are able to provide photorealistic rendering results for complex scenes. Therefore, embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent solution to support emerging 3D applications with enhanced quality of experience. This paper provides a comprehensive overview on the integration of NeRF and 3D-GS in 6G. First, we review the basics of the radiance field rendering techniques, and highlight their applications and implementation challenges over wireless networks. Next, we consider the over-the-air training of NeRF and 3D-GS models over wireless networks by presenting various learning techniques. We particularly focus on the federated learning design over a hierarchical device-edge-cloud architecture. Then, we discuss three practical rendering architectures of NeRF and 3D-GS models at wireless network edge. We provide model compression approaches to facilitate the transmission of radiance field models, and present rendering acceleration approaches and joint computation and communication designs to enhance the rendering efficiency. In particular, we propose a new semantic communication enabled 3D content transmission design, in which the radiance field models are exploited as the semantic knowledge base to reduce the communication overhead for distributed inference. Furthermore, we present the utilization of radiance field rendering in wireless applications like radio mapping and radio imaging.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "15 pages,7 figures"
    },
    {
        "paper id": "2405.11922",
        "abstract url": "https://arxiv.org/abs/2405.11922",
        "title": "Effective Clustering on Large Attributed Bipartite Graphs",
        "rating": "-4.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "bioinformatics"
            ],
            [
                "recommendation"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs. Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics. However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality. The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging. In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets. TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence. Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels. Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs.",
        "subjects": [
            "cs.SI",
            "cs.LG"
        ],
        "comment": "The technical report for the paper was accepted to KDD 2024. 14 pages"
    },
    {
        "paper id": "2405.11932",
        "abstract url": "https://arxiv.org/abs/2405.11932",
        "title": "Nonequilbrium physics of generative diffusion models",
        "rating": "-4.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "industrial"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interest from industrial application, but a complete picture about inherent mechanisms is still lacking. In this paper, we provide a transparent physics analysis of the diffusion models, deriving the fluctuation theorem, entropy production, Franz-Parisi potential to understand the intrinsic phase transitions discovered recently. Our analysis is rooted in non-equlibrium physics and concepts from equilibrium physics, i.e., treating both forward and backward dynamics as a Langevin dynamics, and treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder studied in spin glass theory. This unified principle is expected to guide machine learning practitioners to design better algorithms and theoretical physicists to link the machine learning to non-equilibrium thermodynamics.",
        "subjects": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.LG"
        ],
        "comment": "33 pages, 9 figures, 30 refs"
    },
    {
        "paper id": "2405.12377",
        "abstract url": "https://arxiv.org/abs/2405.12377",
        "title": "Spatio-temporal Attention-based Hidden Physics-informed Neural Network for Remaining Useful Life Prediction",
        "rating": "-5.5",
        "keywords": [
            [
                "Health"
            ],
            [
                "industrial"
            ],
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting the Remaining Useful Life (RUL) is essential in Prognostic Health Management (PHM) for industrial systems. Although deep learning approaches have achieved considerable success in predicting RUL, challenges such as low prediction accuracy and interpretability pose significant challenges, hindering their practical implementation. In this work, we introduce a Spatio-temporal Attention-based Hidden Physics-informed Neural Network (STA-HPINN) for RUL prediction, which can utilize the associated physics of the system degradation. The spatio-temporal attention mechanism can extract important features from the input data. With the self-attention mechanism on both the sensor dimension and time step dimension, the proposed model can effectively extract degradation information. The hidden physics-informed neural network is utilized to capture the physics mechanisms that govern the evolution of RUL. With the constraint of physics, the model can achieve higher accuracy and reasonable predictions. The approach is validated on a benchmark dataset, demonstrating exceptional performance when compared to cutting-edge methods, especially in the case of complex conditions.",
        "subjects": [
            "eess.SY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11803",
        "abstract url": "https://arxiv.org/abs/2405.11803",
        "title": "Learning of Balance Controller Considering Changes in Body State for Musculoskeletal Humanoids",
        "rating": "-10",
        "keywords": [],
        "abstract": "The musculoskeletal humanoid is difficult to modelize due to the flexibility and redundancy of its body, whose state can change over time, and so balance control of its legs is challenging. There are some cases where ordinary PID controls may cause instability. In this study, to solve these problems, we propose a method of learning a correlation model among the joint angle, muscle tension, and muscle length of the ankle and the zero moment point to perform balance control. In addition, information on the changing body state is embedded in the model using parametric bias, and the model estimates and adapts to the current body state by learning this information online. This makes it possible to adapt to changes in upper body posture that are not directly taken into account in the model, since it is difficult to learn the complete dynamics of the whole body considering the amount of data and computation. The model can also adapt to changes in body state, such as the change in footwear and change in the joint origin due to recalibration. The effectiveness of this method is verified by a simulation and by using an actual musculoskeletal humanoid, Musashi.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at IROS2022"
    },
    {
        "paper id": "2405.11818",
        "abstract url": "https://arxiv.org/abs/2405.11818",
        "title": "A Rate-Distortion Analysis for Composite Sources Under Subsource-Dependent Fidelity Criteria",
        "rating": "-10",
        "keywords": [],
        "abstract": "A composite source, consisting of multiple subsources and a memoryless switch, outputs one symbol at a time from the subsource selected by the switch. If some data should be encoded more accurately than other data from an information source, the composite source model is suitable because in this model different distortion constraints can be put on the subsources. In this context, we propose subsource-dependent fidelity criteria for composite sources and use them to formulate a rate-distortion problem. We solve the problem and obtain a single-letter expression for the rate-distortion function. Further rate-distortion analysis characterizes the performance of classify-then-compress (CTC) coding, which is frequently used in practice when subsource-dependent fidelity criteria are considered. Our analysis shows that CTC coding generally has performance loss relative to optimal coding, even if the classification is perfect. We also identify the cause of the performance loss, that is, class labels have to be reproduced in CTC coding. Last but not least, we show that the performance loss is negligible for asymptotically small distortion if CTC coding is appropriately designed and some mild conditions are satisfied.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "16 pages, 8 figures, submitted to IEEE Journal on Selected Areas in Communications"
    },
    {
        "paper id": "2405.11825",
        "abstract url": "https://arxiv.org/abs/2405.11825",
        "title": "Measuring Technical Debt in AI-Based Competition Platforms",
        "rating": "-10",
        "keywords": [],
        "abstract": "Advances in AI have led to new types of technical debt in software engineering projects. AI-based competition platforms face challenges due to rapid prototyping and a lack of adherence to software engineering principles by participants, resulting in technical debt. Additionally, organizers often lack methods to evaluate platform quality, impacting sustainability and maintainability. In this research, we identify and categorize types of technical debt in AI systems through a scoping review. We develop a questionnaire for assessing technical debt in AI competition platforms, categorizing debt into various types, such as algorithm, architectural, code, configuration, data etc. We introduce Accessibility Debt, specific to AI competition platforms, highlighting challenges participants face due to inadequate platform usability. Our framework for managing technical debt aims to improve the sustainability and effectiveness of these platforms, providing tools for researchers, organizers, and participants.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11835",
        "abstract url": "https://arxiv.org/abs/2405.11835",
        "title": "Demo Paper: A Game Agents Battle Driven by Free-Form Text Commands Using Code-Generation LLM",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a demonstration of our monster battle game, in which the game agents fight in accordance with their player's language commands. The commands were translated into the knowledge expression called behavior branches by a code-generation large language model. This work facilitated the design of the commanding system more easily, enabling the game agent to comprehend more various and continuous commands than rule-based methods. The results of the commanding and translation process were stored in a database on an Amazon Web Services server for more comprehensive validation. This implementation would provide a sufficient evaluation of this ongoing work, and give insights to the industry that they could use this to develop their interactive game agents.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "submitted to IEEE CoG 2024"
    },
    {
        "paper id": "2405.11849",
        "abstract url": "https://arxiv.org/abs/2405.11849",
        "title": "Jumping Automata Must Pay",
        "rating": "-10",
        "keywords": [],
        "abstract": "Jumping automata are finite automata that read their input in a non-sequential manner, by allowing a reading head to ``jump'' between positions on the input, consuming a permutation of the input word. We argue that allowing the head to jump should incur some cost. To this end, we propose three quantitative semantics for jumping automata, whereby the jumps of the head in an accepting run define the cost of the run. The three semantics correspond to different interpretations of jumps: the \\emph{absolute distance} semantics counts the distance the head jumps, the \\emph{reversal} semantics counts the number of times the head changes direction, and the \\emph{Hamming distance} measures the number of letter-swaps the run makes. We study these measures, with the main focus being the \\emph{boundedness problem}: given a jumping automaton, decide whether its (quantitative) languages is bounded by some given number $k$. We establish the decidability and complexity for this problem under several variants.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11863",
        "abstract url": "https://arxiv.org/abs/2405.11863",
        "title": "Permittivity Characterization of Human Skin Based on a Quasi-optical System at Sub-THz",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces a novel approach to experimentally characterize effective human skin permittivity at sub-Terahertz (sub-THz) frequencies, specifically from $140$~to $210$~GHz, utilizing a quasi-optical measurement system. To ensure accurate measurement of the reflection coefficients of human skin, a planar, rigid, and thick reference plate with a low-loss dielectric is utilized to flatten the human skin surface. A permittivity characterization method is proposed to reduce permittivity estimation deviations resulting from the pressure effects on the phase displacements of skins under the measurements but also to ensure repeatability of the measurement. In practical permittivity characterizations, the complex permittivities of the finger, palm, and arm of seven volunteers show small standard deviations for the repeated measurements, respectively, while those show significant variations across different regions of the skins and for different persons. The proposed measurement system holds significant potential for future skin permittivity estimation in sub-THz bands, facilitating further studies on human-electromagnetic-wave interactions based on the measured permittivity values.",
        "subjects": [
            "physics.med-ph",
            "eess.SP"
        ],
        "comment": "13 pages for Journal paper"
    },
    {
        "paper id": "2405.11873",
        "abstract url": "https://arxiv.org/abs/2405.11873",
        "title": "Equilibria in multiagent online problems with predictions",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the power of (competitive) algorithms with predictions in a multiagent setting. For this we introduce a multiagent version of the ski-rental problem. In this problem agents can collaborate by pooling resources to get a group license for some asset. If the license price is not met agents have to rent the asset individually for the day at a unit price. Otherwise the license becomes available forever to everyone at no extra cost. Our main contribution is a best-response analysis of a single-agent competitive algorithm that assumes perfect knowledge of other agents' actions (but no knowledge of its own renting time). We then analyze the setting when agents have a predictor for their own active time, yielding a tradeoff between robustness and consistency. We investigate the effect of using such a predictor in an equilibrium, as well as the new equilibria formed in this way.",
        "subjects": [
            "cs.GT",
            "cs.DS",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11876",
        "abstract url": "https://arxiv.org/abs/2405.11876",
        "title": "Understanding crypter-as-a-service in a popular underground marketplace",
        "rating": "-10",
        "keywords": [],
        "abstract": "Crypters are pieces of software whose main goal is to transform a target binary so it can avoid detection from Anti Viruses (AVs from now on) applications. They work similar to packers, by taking a malware binary and applying a series of modifications, obfuscations and encryptions to output a binary that evades one or more AVs. The goal is to remain fully undetected, or FUD in the hacking jargon, while maintaining its (often malicious) functionality. In line to the growth of commoditization in cybercrime, the crypter-as-a-service model has gained popularity, in response to the increased sophistication of detection mechanisms. In this business model, customers receive an initial crypter which is soon updated once becomes detected by anti-viruses. This paper provides the first study on an online underground market dedicated to crypter-as-a-service. We compare the most relevant products in sale, analyzing the existent social network on the platform and comparing the different features that they provide. We also conduct an experiment as a case study, to validate the usage of one of the most popular crypters sold in the market, and compare the results before and after crypting binaries (both benign and malware), to show its effectiveness when evading antivirus engines.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "A short version of this paper was accepted at the 6th Workshop on Attackers and Cyber-Crime Operations (WACCO)"
    },
    {
        "paper id": "2405.11889",
        "abstract url": "https://arxiv.org/abs/2405.11889",
        "title": "Lipschitz Continuous Allocations for Optimization Games",
        "rating": "-10",
        "keywords": [],
        "abstract": "In cooperative game theory, the primary focus is the equitable allocation of payoffs or costs among agents. However, in the practical applications of cooperative games, accurately representing games is challenging. In such cases, using an allocation method sensitive to small perturbations in the game can lead to various problems, including dissatisfaction among agents and the potential for manipulation by agents seeking to maximize their own benefits. Therefore, the allocation method must be robust against game perturbations. In this study, we explore optimization games, in which the value of the characteristic function is provided as the optimal value of an optimization problem. To assess the robustness of the allocation methods, we use the Lipschitz constant, which quantifies the extent of change in the allocation vector in response to a unit perturbation in the weight vector of the underlying problem. Thereafter, we provide an algorithm for the matching game that returns an allocation belonging to the $\\left(\\frac{1}{2}-\u03b5\\right)$-approximate core with Lipschitz constant $O(\u03b5^{-1})$. Additionally, we provide an algorithm for a minimum spanning tree game that returns an allocation belonging to the $4$-approximate core with a constant Lipschitz constant. The Shapley value is a popular allocation that satisfies several desirable properties. Therefore, we investigate the robustness of the Shapley value. We demonstrate that the Lipschitz constant of the Shapley value for the minimum spanning tree is constant, whereas that for the matching game is $\u03a9(\\log n)$, where $n$ denotes the number of vertices.",
        "subjects": [
            "cs.GT",
            "cs.DS"
        ],
        "comment": "23 pages, ICALP 2024"
    },
    {
        "paper id": "2405.11927",
        "abstract url": "https://arxiv.org/abs/2405.11927",
        "title": "Response time in a pair of processor sharing queues with Join-the-Shortest-Queue scheduling",
        "rating": "-10",
        "keywords": [],
        "abstract": "Join-the-Shortest-Queue (JSQ) is the scheduling policy of choice for many network providers, cloud servers and traffic management systems, where individual queues are served under processor sharing (PS) queueing discipline. A numerical solution for the response time distribution in two parallel PS queues with JSQ scheduling is derived for the first time. Using the generating function method, two partial differential equations (PDEs) are obtained corresponding to conditional response times, where the conditioning is on a particular traced task joining the first or the second queue. These PDEs are functional equations that contain partial generating functions and their partial derivatives, and therefore cannot be solved by commonly used techniques. We are able to solve these PDEs numerically with good accuracy and perform the deconditioning with respect to the queue-length probabilities by evaluating a certain complex integral. Numerical results for the density and the first four moments compare well against regenerative simulation with 500,000 regeneration cycles.",
        "subjects": [
            "cs.PF",
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11956",
        "abstract url": "https://arxiv.org/abs/2405.11956",
        "title": "PET: Multi-agent Independent PPO-based Automatic ECN Tuning for High-Speed Data Center Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Explicit Congestion Notification (ECN)-based congestion control schemes have been widely adopted in high-speed data center networks (DCNs), where the ECN marking threshold plays a determinant role in guaranteeing a packet lossless DCN. However, existing approaches either employ static settings with immutable thresholds that cannot be dynamically self-adjusted to adapt to network dynamics, or fail to take into account many-to-one traffic patterns and different requirements of different types of traffic, resulting in relatively poor performance. To address these problems, this paper proposes a novel learning-based automatic ECN tuning scheme, named PET, based on the multi-agent Independent Proximal Policy Optimization (IPPO) algorithm. PET dynamically adjusts ECN thresholds by fully considering pivotal congestion-contributing factors, including queue length, output data rate, output rate of ECN-marked packets, current ECN threshold, the extent of incast, and the ratio of mice and elephant flows. PET adopts the Decentralized Training and Decentralized Execution (DTDE) paradigm and combines offline and online training to accommodate network dynamics. PET is also fair and readily deployable with commodity hardware. Comprehensive experimental results demonstrate that, compared with state-of-the-art static schemes and the learning-based automatic scheme, our PET achieves better performance in terms of flow completion time, convergence rate, queue length variance, and system robustness.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11964",
        "abstract url": "https://arxiv.org/abs/2405.11964",
        "title": "Quantifying Individual and Joint Module Impact in Modular Optimization Frameworks",
        "rating": "-10",
        "keywords": [],
        "abstract": "This study explores the influence of modules on the performance of modular optimization frameworks for continuous single-objective black-box optimization. There is an extensive variety of modules to choose from when designing algorithm variants, however, there is a rather limited understanding of how each module individually influences the algorithm performance and how the modules interact with each other when combined. We use the functional ANOVA (f-ANOVA) framework to quantify the influence of individual modules and module combinations for two algorithms, the modular Covariance Matrix Adaptation (modCMA) and the modular Differential Evolution (modDE). We analyze the performance data from 324 modCMA and 576 modDE variants on the BBOB benchmark collection, for two problem dimensions, and three computational budgets. Noteworthy findings include the identification of important modules that strongly influence the performance of modCMA, such as the~\\textit{weights\\ option} and~\\textit{mirrored} modules for low dimensional problems, and the~\\textit{base\\ sampler} for high dimensional problems. The large individual influence of the~\\textit{lpsr} module makes it very important for the performance of modDE, regardless of the problem dimensionality and the computational budget. When comparing modCMA and modDE, modDE undergoes a shift from individual modules being more influential, to module combinations being more influential, while modCMA follows the opposite pattern, with an increase in problem dimensionality and computational budget.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "To appear in the Proc. of IEEE Congress on Evolutionary Computation (WCCI CEC 2024)"
    },
    {
        "paper id": "2405.11965",
        "abstract url": "https://arxiv.org/abs/2405.11965",
        "title": "No Free Lunch: Research Software Testing in Teaching",
        "rating": "-10",
        "keywords": [],
        "abstract": "Software is at the core of most scientific discoveries today. Therefore, the quality of research results highly depends on the quality of the research software. Rigorous testing, as we know it from software engineering in the industry, could ensure the quality of the research software but it also requires a substantial effort that is often not rewarded in academia. Therefore, this research explores the effects of research software testing integrated into teaching on research software. In an in-vivo experiment, we integrated the engineering of a test suite for a large-scale network simulation as group projects into a course on software testing at the Blekinge Institute of Technology, Sweden, and qualitatively measured the effects of this integration on the research software. We found that the research software benefited from the integration through substantially improved documentation and fewer hardware and software dependencies. However, this integration was effortful and although the student teams developed elegant and thoughtful test suites, no code by students went directly into the research software since we were not able to make the integration back into the research software obligatory or even remunerative. Although we strongly believe that integrating research software engineering such as testing into teaching is not only valuable for the research software itself but also for students, the research of the next generation, as they get in touch with research software engineering and bleeding-edge research in their field as part of their education, the uncertainty about the intellectual properties of students' code substantially limits the potential of integrating research software testing into teaching.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11975",
        "abstract url": "https://arxiv.org/abs/2405.11975",
        "title": "A Stochastic Sampling Approach to Privacy",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper proposes an optimal stochastic sampling approach to privacy, in which a sensor observes a process which is correlated to private information. In out set-up, a sampler decides to keep or discard the sensor's observations. The kept samples are shared with an adversary who might attempt to infer the private process based on the sampler's output. The privacy leakages are captured with the mutual information between the private process and sampler's output. We cast the optimal sampling design as an optimization problem with two objectives: (i) minimizing the reconstruction error of the observed process using the sampler's output, (ii) reducing the privacy leakages. We first show the optimal reconstruction policy is deterministic and can be obtained by solving a one-step optimization problem at each time step. We also derive the optimality equations of the privacy-sampler for a general class of processes via the dynamic decomposition method, and show the sampler controls the adversary's belief about the private input. Also, we propose a simplified design for linear Gaussian processes by restricting the sampling policy to a special collection. We show that the optimal reconstruction of the system state and the private process is similar to Kalman filter in the linear Gaussian case, and the objective of the sampler design problem can be analytically expressed based on a conditional mean and covariance matrix. Furthermore, we develop an numerical algorithm to optimize the sampling and reconstruction policies, wherein the policy gradient theorem for the optimal sampling design is derived based on the implicit function theorem. Finally, we verify our design and show it capabilities in state reconstruction, privacy protection and data size reduction via simulations.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11986",
        "abstract url": "https://arxiv.org/abs/2405.11986",
        "title": "Scheduling Jobs with Work-Inefficient Parallel Solutions",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces the \\emph{serial-parallel decision problem}. Consider an online scheduler that receives a series of tasks, where each task has both a parallel and a serial implementation. The parallel implementation has the advantage that it can make progress concurrently on multiple processors, but the disadvantage that it is (potentially) work-inefficient. As tasks arrive, the scheduler must decide for each task which implementation to use. We begin by studying \\emph{total awake time}. We give a simple \\emph{decide-on-arrival} scheduler that achieves a competitive ratio of $3$ for total awake time -- this scheduler makes serial/parallel decisions immediately when jobs arrive. Our second result is an \\emph{parallel-work-oblivious} scheduler that achieves a competitive ratio of $6$ for total awake time -- this scheduler makes all of its decisions based only on the size of each serial job and without needing to know anything about the parallel implementations. Finally, we prove a lower bound showing that, if a scheduler wishes to achieve a competitive ratio of $O(1)$, it can have at most one of the two aforementioned properties (decide-on-arrival or parallel-work-oblivious). We also prove lower bounds of the form $1 + \u03a9(1)$ on the optimal competitive ratio for any scheduler. Next, we turn our attention to optimizing \\emph{mean response time}. Here, we show that it is possible to achieve an $O(1)$ competitive ratio with $O(1)$ speed augmentation. This is the most technically involved of our results. We also prove that, in this setting, it is not possible for a parallel-work-oblivious scheduler to do well. In addition to these results, we present tight bounds on the optimal competitive ratio if we allow for arrival dependencies between tasks (e.g., tasks are components of a single parallel program), and we give an in-depth discussion of the remaining open questions.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures"
    },
    {
        "paper id": "2405.11987",
        "abstract url": "https://arxiv.org/abs/2405.11987",
        "title": "On Separation Logic, Computational Independence, and Pseudorandomness (Extended Version)",
        "rating": "-10",
        "keywords": [],
        "abstract": "Separation logic is a substructural logic which has proved to have numerous and fruitful applications to the verification of programs working on dynamic data structures. Recently, Barthe, Hsu and Liao have proposed a new way of giving semantics to separation logic formulas in which separating conjunction is interpreted in terms of probabilistic independence. The latter is taken in its exact form, i.e., two events are independent if and only if the joint probability is the product of the probabilities of the two events. There is indeed a literature on weaker notions of independence which are computational in nature, i.e. independence holds only against efficient adversaries and modulo a negligible probability of success. The aim of this work is to explore the nature of computational independence in a cryptographic scenario, in view of the aforementioned advances in separation logic. We show on the one hand that the semantics of separation logic can be adapted so as to account for complexity bounded adversaries, and on the other hand that the obtained logical system is useful for writing simple and compact proofs of standard cryptographic results in which the adversary remains hidden. Remarkably, this allows for a fruitful interplay between independence and pseudorandomness, itself a crucial notion in cryptography.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "to be published in CSF'24"
    },
    {
        "paper id": "2405.11996",
        "abstract url": "https://arxiv.org/abs/2405.11996",
        "title": "Max-Min Fairness and PHY-Layer Design of Uplink MIMO Rate-Splitting Multiple Access with Finite Blocklength",
        "rating": "-10",
        "keywords": [],
        "abstract": "Rate-Splitting Multiple Access (RSMA) has emerged as a potent and reliable multiple access and interference management technique in wireless communications. While downlink Multiple-Input Multiple-Ouput (MIMO) RSMA has been widely investigated, uplink MIMO RSMA has not been fully explored. In this paper, we investigate the performance of uplink RSMA in short-packet communications with perfect Channel State Information at Transmitter (CSIT) and Channel State Information at Receiver (CSIR). We propose an uplink MIMO RSMA framework and optimize both precoders and combiners with Max-Min Fairness (MMF) metric and Finite Blocklength (FBL) constraints. Due to the high coupling between precoders and combiners, we apply the Alternating Optimization (AO) to decompose the optimization problem into two subproblems. To tackle these subproblems, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Subsequently, the Physical (PHY)-layer of the uplink MIMO RSMA architecture is designed and evaluated using multi-user Link-Level Simulations (LLS), accounting for finite constellation modulation, finite length polar codes, message splitting, adaptive modulation and coding, and Successive Interference Cancellation (SIC) at the receiver. Numerical results demonstrate that applying RSMA in uplink MIMO with FBL constraints not only achieves MMF gains over conventional transmission schemes such as Space Division Multiple Access (SDMA) and Non-orthogonal Multiple Access (NOMA) but also exhibits robustness to network loads. The benefits of splitting messages from multiple users are also illustrated. LLS results confirm the improved max-min throughput benefits of RSMA over SDMA and NOMA.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11998",
        "abstract url": "https://arxiv.org/abs/2405.11998",
        "title": "Learning to connect in action: Measuring and understanding the emergence of boundary spanners in volatile times",
        "rating": "-10",
        "keywords": [],
        "abstract": "Collective intelligence of diverse groups is key for tackling many of today's grand challenges such as fostering resilience and climate adaptation. Information exchange across such diverse groups is crucial for collective intelligence, especially in volatile environments. To facilitate inter-group information exchange, Informational Boundary Spanners (IBSs) as pivotal information exchange 'hubs' are promising. However, the mechanisms that drive the emergence of IBSs remain poorly understood. To address this gap there is first a need for a method to identify and measure the emergence of IBSs. Second, an Agent-Based Modelling (ABM) framework is not available to systematically study mechanisms for the emergence of IBSs in volatile environments. Third, even though the ability to learn who provides high-quality information is thought to be essential to explain the emergence of IBSs, a rigorous test of this mechanism is missing. The learning mechanism is formalized using an ABM framework, with the model's outputs analyzed using the proposed IBS emergence measurement method. To illustrate both the method and the learning mechanism, we present a case study focused on information sharing in the volatile environment of a disaster. The study shows that learning constitutes a mechanism for the emergence of effective IBSs in (a) low-volatility environments characterised by low uncertainty and (b) in high-volatility environments characterised by rapid change if the number of inter-group connections is sufficient. With the method and model, this paper aims to lay the foundations for exploring mechanisms for the emergence of IBSs that facilitate inter-group information exchange. This article advances collective intelligence by providing the essential elements for measuring and understanding the emergence of IBSs and exploring the effect of learning on their emergence in volatile environments.",
        "subjects": [
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.11999",
        "abstract url": "https://arxiv.org/abs/2405.11999",
        "title": "Multi-Agent Optimization and Learning: A Non-Expansive Operators Perspective",
        "rating": "-10",
        "keywords": [],
        "abstract": "Multi-agent systems are increasingly widespread in a range of application domains, with optimization and learning underpinning many of the tasks that arise in this context. Different approaches have been proposed to enable the cooperative solution of these optimization and learning problems, including first- and second-order methods, and dual (or Lagrangian) methods, all of which rely on consensus and message-passing. In this article we discuss these algorithms through the lens of non-expansive operator theory, providing a unifying perspective. We highlight the insights that this viewpoint delivers, and discuss how it can spark future original research.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12011",
        "abstract url": "https://arxiv.org/abs/2405.12011",
        "title": "Higher weight spectra of ternary codes associated to the quadratic Veronese $3$-fold",
        "rating": "-10",
        "keywords": [],
        "abstract": "The problem studied in this work is to determine the higher weight spectra of the Projective Reed-Muller codes associated to the Veronese $3$-fold $\\mathcal V$ in $PG(9,q)$, which is the image of the quadratic Veronese embedding of $PG(3,q)$ in $PG(9,q)$. We reduce the problem to the following combinatorial problem in finite geometry: For each subset $S$ of $\\mathcal V$, determine the dimension of the linear subspace of $PG(9,q)$ generated by $S$. We develop a systematic method to solve the latter problem. We implement the method for $q=3$, and use it to obtain the higher weight spectra of the associated code. The case of a general finite field $\\mathbb F_q$ will be treated in a future work.",
        "subjects": [
            "math.CO",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12013",
        "abstract url": "https://arxiv.org/abs/2405.12013",
        "title": "Fully graphic degree sequences and P-stable degree sequences",
        "rating": "-10",
        "keywords": [],
        "abstract": "The notion of P-stability played an influential role in approximating the permanents, sampling rapidly the realizations of graphic degree sequences, or even studying and improving network privacy. However, we did not have a good insight of the structure of P-stable degree sequence families. In this paper we develop a remedy to overstep this deficiency. We will show, that if an infinite set of graphic degree sequences, characterized by some simple inequalities of their fundamental parameters, is $P$-stable, then it is ``fully graphic'' -- meaning that every degree sequence with an even sum, meeting the specified inequalities, is graphic. The reverse statement also holds: an infinite, fully graphic set of degree sequences characterized by some simple inequalities of their fundamental parameters is P-stable. Along the way, we will significantly strengthen some well-known, older results, and we construct new P-stable families of degree sequences.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "23 pages, 1 figure"
    },
    {
        "paper id": "2405.12034",
        "abstract url": "https://arxiv.org/abs/2405.12034",
        "title": "Count-Min Sketch with Conservative Updates: Worst-Case Analysis",
        "rating": "-10",
        "keywords": [],
        "abstract": "Count-Min Sketch with Conservative Updates (CMS-CU) is a memory-efficient hash-based data structure used to estimate the occurrences of items within a data stream. CMS-CU stores $m$ counters and employs $d$ hash functions to map items to these counters. We first argue that the estimation error in CMS-CU is maximal when each item appears at most once in the stream. Next, we study CMS-CU in this setting. In the case where $d=m-1$, we prove that the average estimation error and the average counter rate converge almost surely to $\\frac{1}{2}$, contrasting with the vanilla Count-Min Sketch, where the average counter rate is equal to $\\frac{m-1}{m}$. For any given $m$ and $d$, we prove novel lower and upper bounds on the average estimation error, incorporating a positive integer parameter $g$. Larger values of this parameter improve the accuracy of the bounds. Moreover, the computation of each bound involves examining an ergodic Markov process with a state space of size $\\binom{m+g-d}{g}$ and a sparse transition probabilities matrix containing $\\mathcal{O}(m\\binom{m+g-d}{g})$ non-zero entries. For $d=m-1$, $g=1$, and as $m\\to \\infty$, we show that the lower and upper bounds coincide. In general, our bounds exhibit high accuracy for small values of $g$, as shown by numerical computation. For example, for $m=50$, $d=4$, and $g=5$, the difference between the lower and upper bounds is smaller than $10^{-4}$.",
        "subjects": [
            "cs.DS",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12042",
        "abstract url": "https://arxiv.org/abs/2405.12042",
        "title": "Attribute-Based Authentication in Secure Group Messaging for Distributed Environments",
        "rating": "-10",
        "keywords": [],
        "abstract": "Messaging Layer security (MLS) and its underlying Continuous Group Key Agreement (CGKA) protocol allows a group of users to share a cryptographic secret in a dynamic manner, such that the secret is modified in member insertions and deletions. Although this flexibility makes MLS ideal for implementations in distributed environments, a number of issues need to be overcome. Particularly, the use of digital certificates for authentication in a group goes against the group members' privacy. In this work we provide an alternative method of authentication in which the solicitors, instead of revealing their identity, only need to prove possession of certain attributes, dynamically defined by the group, to become a member. Instead of digital certificates, we employ Attribute-Based Credentials accompanied with Selective Disclosure in order to reveal the minimum required amount of information and to prevent attackers from linking the activity of a user through multiple groups. We formally define a CGKA variant named Attribute-Authenticated Continuous Group Key Agreement (AA-CGKA) and provide security proofs for its properties of Requirement Integrity, Unforgeability and Unlinkability. We also provide guidelines for an integration of our construction in MLS.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Submitted to IEEE Transactions on Dependable and Secure Computing This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2405.12053",
        "abstract url": "https://arxiv.org/abs/2405.12053",
        "title": "Complex Principle Kurtosis Analysis",
        "rating": "-10",
        "keywords": [],
        "abstract": "Independent component analysis (ICA) is a fundamental problem in the field of signal processing, and numerous algorithms have been developed to address this issue. The core principle of these algorithms is to find a transformation matrix that maximizes the non-Gaussianity of the separated signals. Most algorithms typically assume that the source signals are mutually independent (orthogonal to each other), thereby imposing an orthogonal constraint on the transformation matrix. However, this assumption is not always valid in practical scenarios, where the orthogonal constraint can lead to inaccurate results. Recently, tensor-based algorithms have attracted much attention due to their ability to reduce computational complexity and enhance separation performance. In these algorithms, ICA is reformulated as an eigenpair problem of a statistical tensor. Importantly, the eigenpairs of a tensor are not inherently orthogonal, making tensor-based algorithms more suitable for nonorthogonal cases. Despite this advantage, finding exact solutions to the tensor's eigenpair problem remains a challenging task. In this paper, we introduce a non-zero volume constraint and a Riemannian gradient-based algorithm to solve the tensor's eigenpair problem. The proposed algorithm can find exact solutions under nonorthogonal conditions, making it more effective for separating nonorthogonal sources. Additionally, existing tensor-based algorithms typically rely on third-order statistics and are limited to real-valued data. To overcome this limitation, we extend tensor-based algorithms to the complex domain by constructing a fourth-order statistical tensor. Experiments conducted on both synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "23 pages, 6 figures"
    },
    {
        "paper id": "2405.12064",
        "abstract url": "https://arxiv.org/abs/2405.12064",
        "title": "Approximating Multi-Dimensional and Multiband Signals",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the problem of representing a discrete tensor that comes from finite uniform samplings of a multi-dimensional and multiband analog signal. Particularly, we consider two typical cases in which the shape of the subbands is cubic or parallelepipedic. For the cubic case, by examining the spectrum of its corresponding time- and band-limited operators, we obtain a low-dimensional optimal dictionary to represent the original tensor. We further prove that the optimal dictionary can be approximated by the famous \\ac{dpss} with certain modulation, leading to an efficient constructing method. For the parallelepipedic case, we show that there also exists a low-dimensional dictionary to represent the original tensor. We present rigorous proof that the numbers of atoms in both dictionaries are approximately equal to the dot of the total number of samplings and the total volume of the subbands. Our derivations are mainly focused on the \\ac{2d} scenarios but can be naturally extended to high dimensions.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12073",
        "abstract url": "https://arxiv.org/abs/2405.12073",
        "title": "Goal-Oriented Communication for Networked Control Assisted by Reconfigurable Meta-Surfaces",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we develop a theoretical framework for goal-oriented communication assisted by reconfigurable meta-surfaces in the context of networked control systems. The relation to goal-oriented communication stems from the fact that optimization of the phase shifts of the meta-surfaces is guided by the performance of networked control systems tasks. To that end, we consider a networked control system in which a set of sensors observe the states of a set of physical processes, and communicate this information over an unreliable wireless channel assisted by a reconfigurable intelligent surface with multiple reflecting elements to a set of controllers that correct the behaviors of the physical processes based on the received information. Our objective is to find the optimal control policy for the controllers and the optimal phase policy for the reconfigurable intelligent surface that jointly minimize a regulation cost function associated with the networked control system. We characterize these policies, and also propose an approximate solution based on a semi-definite relaxation technique.",
        "subjects": [
            "cs.IT",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12079",
        "abstract url": "https://arxiv.org/abs/2405.12079",
        "title": "PARALLELGPUOS: A Concurrent OS-level GPU Checkpoint and Restore System using Validated Speculation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Checkpointing (C) and restoring (R) are key components for GPU tasks. POS is an OS-level GPU C/R system: It can transparently checkpoint or restore processes that use the GPU, without requiring any cooperation from the application, a key feature required by modern systems like the cloud. Moreover, POS is the first OS-level C/R system that can concurrently execute C/R with the application execution: a critical feature that can be trivially achieved when the processes only running on the CPU, but becomes challenging when the processes use GPU. The problem is how to ensure consistency during concurrent execution with the lack of application semantics due to transparency. CPU processes can leverage OS and hardware paging to fix inconsistency without application semantics. Unfortunately, GPU bypasses OS and paging for high performance. POS fills the semantic gap by speculatively extracting buffer access information of GPU kernels during runtime. Thanks to the simple and well-structured nature of GPU kernels, our speculative extraction (with runtime validation) achieves 100% accuracy on applications from training to inference whose domains span from vision, large language models, and reinforcement learning. Based on the extracted semantics, we systematically overlap C/R with application execution, and achieves orders of magnitude higher performance under various tasks compared with the state-of-the-art OS-level GPU C/R, including training fault tolerance, live GPU process migration, and cold starts acceleration in GPU-based serverless computing.",
        "subjects": [
            "cs.DC",
            "cs.OS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12089",
        "abstract url": "https://arxiv.org/abs/2405.12089",
        "title": "Using Formal Verification to Evaluate Single Event Upsets in a RISC-V Core",
        "rating": "-10",
        "keywords": [],
        "abstract": "Reliability has been a major concern in embedded systems. Higher transistor density and lower voltage supply increase the vulnerability of embedded systems to soft errors. A Single Event Upset (SEU), which is also called a soft error, can reverse a bit in a sequential element, resulting in a system failure. Simulation-based fault injection has been widely used to evaluate reliability, as suggested by ISO26262. However, it is practically impossible to test all faults for a complex design. Random fault injection is a compromise that reduces accuracy and fault coverage. Formal verification is an alternative approach. In this paper, we use formal verification, in the form of model checking, to evaluate the hardware reliability of a RISC-V Ibex Core in the presence of soft errors. Backward tracing is performed to identify and categorize faults according to their effects (no effect, Silent Data Corruption, crashes, and hangs). By using formal verification, the entire state space and fault list can be exhaustively explored. It is found that misaligned instructions can amplify fault effects. It is also found that some bits are more vulnerable to SEUs than others. In general, most of the bits in the Ibex Core are vulnerable to Silent Data Corruption, and the second pipeline stage is more vulnerable to Silent Data Corruption than the first.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12101",
        "abstract url": "https://arxiv.org/abs/2405.12101",
        "title": "Sustainable business decision modelling with blockchain and digital twins: A survey",
        "rating": "-10",
        "keywords": [],
        "abstract": "Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions. BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios. Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability. Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure. These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts. To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research. Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads. Several research questions are put forward to motivate further research that significantly impacts BDM. Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM.",
        "subjects": [
            "cs.NI",
            "cs.ET"
        ],
        "comment": "34 pages, 19 figures, 7 tables"
    },
    {
        "paper id": "2405.12115",
        "abstract url": "https://arxiv.org/abs/2405.12115",
        "title": "Clap: a Rust eDSL for PlonKish Proof Systems with a Semantics-preserving Optimizing Compiler",
        "rating": "-10",
        "keywords": [],
        "abstract": "Writing Plonkish constraint systems by hand is tedious and error-prone; as a result, several libraries and DSL's have emerged over the years to facilitate this task as well as techniques to directly analyze constraint systems. However, standalone languages require developers to use a foreign toolchain and leave gaps between the application and its circuits. On the other hand, Rust-embedded DSL like Halo2 or Boojum lack in modularity; furthermore, it is usually impossible to tease apart the circuit from the proof system, making it hard to reuse circuits and even to compare performance of different proof systems on the same circuits. In this paper we introduce Clap, the first Rust eDSL to propose a prover-agnostic circuit format that enables extensibility, automatic optimizations, and formal guarantees for the resulting constraint system. Clap generates Plonkish constraint systems and witness generators that are sound and complete with respect to each other, leaving no room for subtle bugs due to under- or over-constraining. A model of this equivalence is proved in the Agda proof assistant for a subset of Clap's Rust implementation that is expressive enough to capture the compositional properties of our format. In order to increase the reuse of circuits, a number of optimizations are carried out automatically, sparing the developer from over-specifying low-level constraint system details in their circuit descriptions. We test the expressivity and efficiency of Clap on an implementation of the Poseidon2 hash function that produces a constraint system that is competitive in terms of size with hand-optimized Boojum circuits.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12117",
        "abstract url": "https://arxiv.org/abs/2405.12117",
        "title": "Strongly-Consistent Distributed Discrete-event Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Discrete-event (DE) systems are concurrent programs where components communicate via tagged events, where tags are drawn from a totally ordered set. Reactors are an emerging model of computation based on DE and realized in the open-source coordination language Lingua Franca. Distributed DE (DDE) systems are DE systems where the components (reactors) communicate over networks. The prior art has required that for DDE systems with cycles, each cycle must contain at least one logical delay, where the tag of events is incremented. Such delays, however, are not required by the elegant fixed-point semantics of DE. The only requirement is that the program be constructive, meaning it is free of causality cycles. This paper gives a way to coordinate the execution of DDE systems that can execute any constructive program, even one with zero-delay cycles. It provides a formal model that exposes exactly the information that must be shared across networks for such execution to be possible. Furthermore, it describes a concrete implementation that is an extension of the coordination mechanisms in Lingua Franca.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12132",
        "abstract url": "https://arxiv.org/abs/2405.12132",
        "title": "A Vision on Open Science for the Evolution of Software Engineering Research and Practice",
        "rating": "-10",
        "keywords": [],
        "abstract": "Open Science aims to foster openness and collaboration in research, leading to more significant scientific and social impact. However, practicing Open Science comes with several challenges and is currently not properly rewarded. In this paper, we share our vision for addressing those challenges through a conceptual framework that connects essential building blocks for a change in the Software Engineering community, both culturally and technically. The idea behind this framework is that Open Science is treated as a first-class requirement for better Software Engineering research, practice, recognition, and relevant social impact. There is a long road for us, as a community, to truly embrace and gain from the benefits of Open Science. Nevertheless, we shed light on the directions for promoting the necessary culture shift and empowering the Software Engineering community.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Proceedings of the FSE 2024 - Ideas, Visions and Reflections Track"
    },
    {
        "paper id": "2405.12152",
        "abstract url": "https://arxiv.org/abs/2405.12152",
        "title": "A Nearly Quadratic Improvement for Memory Reallocation",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the Memory Reallocation Problem a set of items of various sizes must be dynamically assigned to non-overlapping contiguous chunks of memory. It is guaranteed that the sum of the sizes of all items present at any time is at most a $(1-\\varepsilon)$-fraction of the total size of memory (i.e., the load-factor is at most $1-\\varepsilon$). The allocator receives insert and delete requests online, and can re-arrange existing items to handle the requests, but at a reallocation cost defined to be the sum of the sizes of items moved divided by the size of the item being inserted/deleted. The folklore algorithm for Memory Reallocation achieves a cost of $O(\\varepsilon^{-1})$ per update. In recent work at FOCS'23, Kuszmaul showed that, in the special case where each item is promised to be smaller than an $\\varepsilon^4$-fraction of memory, it is possible to achieve expected update cost $O(\\log\\varepsilon^{-1})$. Kuszmaul conjectures, however, that for larger items the folklore algorithm is optimal. In this work we disprove Kuszmaul's conjecture, giving an allocator that achieves expected update cost $O(\\varepsilon^{-1/2} \\operatorname*{polylog} \\varepsilon^{-1})$ on any input sequence. We also give the first non-trivial lower bound for the Memory Reallocation Problem: we demonstrate an input sequence on which any resizable allocator (even offline) must incur amortized update cost at least $\u03a9(\\log\\varepsilon^{-1})$. Finally, we analyze the Memory Reallocation Problem on a stochastic sequence of inserts and deletes, with random sizes in $[\u03b4, 2 \u03b4]$ for some $\u03b4$. We show that, in this simplified setting, it is possible to achieve $O(\\log\\varepsilon^{-1})$ expected update cost, even in the ``large item'' parameter regime ($\u03b4> \\varepsilon^4$).",
        "subjects": [
            "cs.DS"
        ],
        "comment": "In Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures (SPAA'24)"
    },
    {
        "paper id": "2405.12168",
        "abstract url": "https://arxiv.org/abs/2405.12168",
        "title": "WiDRa -- Enabling Millimeter-Level Differential Ranging Accuracy in Wi-Fi Using Carrier Phase",
        "rating": "-10",
        "keywords": [],
        "abstract": "Although Wi-Fi is an ideal technology for many ranging applications, the performance of current methods is limited by the system bandwidth, leading to low accuracy of $\\sim 1$ m. For many applications, measuring differential range, viz., the change in the range between adjacent measurements, is sufficient. Correspondingly, this work proposes WiDRa - a Wi-Fi based Differential Ranging solution that provides differential range estimates by using the sum-carrier-phase information. The proposed method is not limited by system bandwidth and can track range changes even smaller than the carrier wavelength. The proposed method is first theoretically justified, while taking into consideration the various hardware impairments affecting Wi-Fi chips. In the process, methods to isolate the sum-carrier phase from the hardware impairments are proposed. Extensive simulation results show that WiDRa can achieve a differential range estimation root-mean-square-error (RMSE) of $\\approx 1$ mm in channels with a Rician-factor $\\geq 7$ (a $100 \\times$ improvement to existing methods). The proposed methods are also validated on off-the-shelf Wi-Fi hardware to demonstrate feasibility, where they achieve an RMSE of $< 1$ mm in the differential range. Finally, limitations of current investigation and future directions of exploration are suggested, to further tap into the potential of WiDRa.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Accepted to IEEE JSAC special issue on Positioning and Sensing Over Wireless Networks, 2024"
    },
    {
        "paper id": "2405.12182",
        "abstract url": "https://arxiv.org/abs/2405.12182",
        "title": "Nearest Neighbors GParareal: Improving Scalability of Gaussian Processes for Parallel-in-Time Solvers",
        "rating": "-10",
        "keywords": [],
        "abstract": "With the advent of supercomputers, multi-processor environments and parallel-in-time (PinT) algorithms offer ways to solve initial value problems for ordinary and partial differential equations (ODEs and PDEs) over long time intervals, a task often unfeasible with sequential solvers within realistic time frames. A recent approach, GParareal, combines Gaussian Processes with traditional PinT methodology (Parareal) to achieve faster parallel speed-ups. The method is known to outperform Parareal for low-dimensional ODEs and a limited number of computer cores. Here, we present Nearest Neighbors GParareal (nnGParareal), a novel data-enriched PinT integration algorithm. nnGParareal builds upon GParareal by improving its scalability properties for higher-dimensional systems and increased processor count. Through data reduction, the model complexity is reduced from cubic to log-linear in the sample size, yielding a fast and automated procedure to integrate initial value problems over long time intervals. First, we provide both an upper bound for the error and theoretical details on the speed-up benefits. Then, we empirically illustrate the superior performance of nnGParareal, compared to GParareal and Parareal, on nine different systems with unique features (e.g., stiff, chaotic, high-dimensional, or challenging-to-learn systems).",
        "subjects": [
            "stat.CO",
            "cs.DC",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12187",
        "abstract url": "https://arxiv.org/abs/2405.12187",
        "title": "Brewer-Nash Scrutinised: Mechanised Checking of Policies featuring Write Revocation",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper revisits the Brewer-Nash security policy model inspired by ethical Chinese Wall policies. We draw attention to the fact that write access can be revoked in the Brewer-Nash model. The semantics of write access were underspecified originally, leading to multiple interpretations for which we provide a modern operational semantics. We go on to modernise the analysis of information flow in the Brewer-Nash model, by adopting a more precise definition adapted from Kessler. For our modernised reformulation, we provide full mechanised coverage for all theorems proposed by Brewer & Nash. Most theorems are established automatically using the tool {log} with the exception of a theorem regarding information flow, which combines a lemma in {log} with a theorem mechanised in Coq. Having covered all theorems originally posed by Brewer-Nash, achieving modern precision and mechanisation, we propose this work as a step towards a methodology for automated checking of more complex security policy models.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12195",
        "abstract url": "https://arxiv.org/abs/2405.12195",
        "title": "Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey",
        "rating": "-10",
        "keywords": [],
        "abstract": "As Large Language Models (LLMs), including ChatGPT and analogous systems, continue to advance, their robust natural language processing capabilities and diverse applications have garnered considerable attention. Nonetheless, despite the increasing acknowledgment of the convergence of Artificial Intelligence (AI) and Software Engineering (SE), there is a lack of studies involving the impact of this convergence on the practices and perceptions of software developers. Understanding how software developers perceive and engage with AI tools, such as ChatGPT, is essential for elucidating the impact and potential challenges of incorporating AI-driven tools in the software development process. In this paper, we conducted a survey with 207 software developers to understand the impact of ChatGPT on software quality, productivity, and job satisfaction. Furthermore, the study delves into developers' expectations regarding future adaptations of ChatGPT, concerns about potential job displacement, and perspectives on regulatory interventions.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "31 pages, 9 figures"
    },
    {
        "paper id": "2405.12219",
        "abstract url": "https://arxiv.org/abs/2405.12219",
        "title": "Locational marginal burden: Quantifying the equity of optimal power flow solutions",
        "rating": "-10",
        "keywords": [],
        "abstract": "Fair distribution of benefits in electric power systems is a pertinent energy policymaking problem; however, these efforts cannot be easily quantified in power system engineering studies. Therefore, we propose locational marginal burden (LMB) to provide an interface between a well-studied measure of energy pricing equity, energy burden, with an optimal power flow problem (OPF). This is achieved by investigating the intrinsic link between the dual optimal solution of an OPF problem and the electricity prices, which are used to calculate the energy burden. By applying results from the field of differentiable optimization, locational marginal prices (LMPs) associated with an OPF solution can be differentiated with respect to demand. This enables electricity retail prices, and thereby, energy burden itself, to be differentiated, resulting in the proposed LMB. Simulation of a synthetic Hawaii network interfaced with real-world socioeconomic data shows how the LMB provides new insights into how the operation of the electricity network affects the equity of energy prices.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted to the 15th ACM International Conference on Future Energy Systems"
    },
    {
        "paper id": "2405.12311",
        "abstract url": "https://arxiv.org/abs/2405.12311",
        "title": "Cost-Optimal Microservices Deployment with Cluster Autoscaling and Spot Pricing",
        "rating": "-10",
        "keywords": [],
        "abstract": "Microservices architecture has been established as an ideal software architecture for cloud-based software development and deployment, offering many benefits such as agility and efficiency. Microservices are often associated with containers and container orchestration systems for deployment, as containerization provides convenient tools and techniques for resource management, including the automation of orchestration processes. Among the factors that make the cloud suitable for commercial software deployment, transient pricing options like AWS Spot Pricing are particularly attractive as they allow consumers to significantly reduce cloud costs. However, the dynamic nature of resource demand and the abrupt termination of spot VMs make transient pricing challenging. Nonetheless, containerization and container orchestration systems open new avenues to optimize the cost of microservices deployments by leveraging spot pricing on the public cloud while achieving application and business goals. We propose SpotKube, an open-source, Kubernetes-based, application-aware, genetic algorithm-based solution for cost optimization, which autoscales clusters for microservices-based applications hosted on public clouds with spot pricing options. SpotKube analyzes application characteristics and recommends the optimal configuration for resource allocation to the cluster. It consists of an elastic cluster autoscaler powered by an optimization algorithm that ensures cost-effective microservices deployment while meeting application performance requirements and handling abrupt termination of nodes, thereby minimizing the impact on system availability. We implement and evaluate SpotKube with representative microservices-based applications in a real public cloud setup, demonstrating the effectiveness of our approach against alternative optimization strategies.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "11 pages including references, 11 figures, Keywords: Microservice, Cost optimization, Cluster Autoscaling, Transient Pricing"
    },
    {
        "paper id": "2405.12347",
        "abstract url": "https://arxiv.org/abs/2405.12347",
        "title": "Self-HWDebug: Automation of LLM Self-Instructing for Hardware Security Verification",
        "rating": "-10",
        "keywords": [],
        "abstract": "The rise of instruction-tuned Large Language Models (LLMs) marks a significant advancement in artificial intelligence (AI) (tailored to respond to specific prompts). Despite their popularity, applying such models to debug security vulnerabilities in hardware designs, i.e., register transfer language (RTL) modules, particularly at system-on-chip (SoC) level, presents considerable challenges. One of the main issues lies in the need for precisely designed instructions for pinpointing and mitigating the vulnerabilities, which requires substantial time and expertise from human experts. In response to this challenge, this paper proposes Self-HWDebug, an innovative framework that leverages LLMs to automatically create required debugging instructions. In Self-HWDebug, a set of already identified bugs from the most critical hardware common weakness enumeration (CWE) listings, along with mitigation resolutions, is provided to the framework, followed by prompting the LLMs to generate targeted instructions for such mitigation. The LLM-generated instructions are subsequently used as references to address vulnerabilities within the same CWE category but in totally different designs, effectively demonstrating the framework's ability to extend solutions across related security issues. Self-HWDebug significantly reduces human intervention by using the model's own output to guide debugging. Through comprehensive testing, Self-HWDebug proves not only to reduce experts' effort/time but also to even improve the quality of the debugging process.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12350",
        "abstract url": "https://arxiv.org/abs/2405.12350",
        "title": "A framework for extraction and transformation of documents",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a theoretical framework for the extraction and transformation of text documents. We propose to use a two-phase process where the first phase extracts span-tuples from a document, and the second phase maps the content of the span-tuples into new documents. We base the extraction phase on the framework of document spanners and the transformation phase on the theory of polyregular functions, the class of regular string-to-string functions with polynomial growth. For supporting practical extract-transform scenarios, we propose an extension of document spanners described by regex formulas from span-tuples to so-called multispan-tuples, where variables are mapped to sets of spans. We prove that this extension, called regex multispanners, has the same desirable properties as standard spanners described by regex formulas. In our framework, an Extract-Transform (ET) program is given by a regex multispanner followed by a polyregular function. In this paper, we study the expressibility and evaluation problem of ET programs when the transformation function is linear, called linear ET programs. We show that linear ET programs are equally expressive as non-deterministic streaming string transducers under bag semantics. Moreover, we show that linear ET programs are closed under composition. Finally, we present an enumeration algorithm for evaluating every linear ET program over a document with linear time preprocessing and constant delay.",
        "subjects": [
            "cs.DB",
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12371",
        "abstract url": "https://arxiv.org/abs/2405.12371",
        "title": "Algorithms for Generating Small Random Samples",
        "rating": "-10",
        "keywords": [],
        "abstract": "This report presents algorithms for generating small random samples without replacement. It considers two cases. It presents an algorithm for sampling a pair of distinct integers, and an algorithm for sampling a triple of distinct integers. The worst case runtime of both algorithms is constant, while the worst case runtime of common algorithms for the general case of sampling $k$ elements from a set of $n$ are linear in $n$. Java implementations of both algorithms are included in the open source library $\u03c1\u03bc$.",
        "subjects": [
            "cs.DS",
            "cs.DM",
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12414",
        "abstract url": "https://arxiv.org/abs/2405.12414",
        "title": "The Power of Two in Token Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "In economies without monetary transfers, token systems serve as an alternative to sustain cooperation, alleviate free riding, and increase efficiency. This paper studies whether a token-based economy can be effective in marketplaces with thin exogenous supply. We consider a marketplace in which at each time period one agent requests a service, one agent provides the service, and one token (artificial currency) is used to pay for service provision. The number of tokens each agent has represents the difference between the amount of service provisions and service requests by the agent. We are interested in the behavior of this economy when very few agents are available to provide the requested service. Since balancing the number of tokens across agents is key to sustain cooperation, the agent with the minimum amount of tokens is selected to provide service among the available agents. When exactly one random agent is available to provide service, we show that the token distribution is unstable. However, already when just two random agents are available to provide service, the token distribution is stable, in the sense that agents' token balance is unlikely to deviate much from their initial endowment, and agents return to their initial endowment in finite expected time. Our results mirror the power of two choices paradigm in load balancing problems. Supported by numerical simulations using kidney exchange data, our findings suggest that token systems may generate efficient outcomes in kidney exchange marketplaces by sustaining cooperation between hospitals.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "32 pages"
    },
    {
        "paper id": "2405.12415",
        "abstract url": "https://arxiv.org/abs/2405.12415",
        "title": "Distribution Steering for Discrete-Time Uncertain Ensemble Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Ensemble systems appear frequently in many engineering applications and, as a result, they have become an important research topic in control theory. These systems are best characterized by the evolution of their underlying state distribution. Despite the work to date, few results exist dealing with the problem of directly modifying (i.e., \"steering\") the distribution of an ensemble system. In addition, in most of the existing results, the distribution of the states of an ensemble of discrete-time systems is assumed to be Gaussian. However, in case the system parameters are uncertain, it is not always realistic to assume that the distribution of the system follows a Gaussian distribution, thus complicating the solution of the overall problem. In this paper, we address the general distribution steering problem for first-order discrete-time ensemble systems, where the distributions of the system parameters and the states are arbitrary with finite first few moments. Both linear and nonlinear system dynamics are considered using the method of power moments to transform the original infinite-dimensional problem into a finite-dimensional one. We also propose a control law for the ensuing moment system, which allows us to obtain the power moments of the desired control inputs. Finally, we solve the inverse problem to obtain the feasible control inputs from their corresponding power moments. We provide numerical results to validate our theoretical developments. These include cases where the parameter distribution is uniform, Gaussian, non-Gaussian, and multi-modal, respectively.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "16 pages, 18 figures"
    },
    {
        "paper id": "2405.12426",
        "abstract url": "https://arxiv.org/abs/2405.12426",
        "title": "Inferring Message Flows From System Communication Traces",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper proposes a novel method for automatically inferring message flow specifications from the communication traces of a system-on-chip (SoC) design that captures messages exchanged among the components during a system execution. The inferred message flows characterize the communication and coordination of components in a system design for realizing various system functions, and they are essential for SoC validation and debugging. The proposed method relieves the burden of manual development and maintenance of such specifications on human designers. Our method also uses a new accuracy metric, \\emph{acceptance ratio}, to evaluate the quality of the mined specifications instead of the specification size often used in the previous work, enabling more accurate specifications to be mined. Furthermore, this paper introduces the concept of essential causalities to enhance the accuracy of the message flow mining and accelerate the mining process. The effectiveness of the proposed method is evaluated on both synthetic traces and traces generated from executing several system models in GEM5. In both cases, the proposed method achieves superior accuracies compared to a previous approach. Additionally, this paper includes some practical use cases.",
        "subjects": [
            "cs.LO",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12431",
        "abstract url": "https://arxiv.org/abs/2405.12431",
        "title": "Data Sharing at the Edge of the Network: A Disturbance Resilient Multi-modal ITS",
        "rating": "-10",
        "keywords": [],
        "abstract": "Mobility-as-a-Service (MaaS) is a paradigm that encourages the shift from private cars to more sustainable alternative mobility services. MaaS provides services that enhances and enables multiple modes of transport to operate seamlessly and bringing Multimodal Intelligent Transport Systems (M-ITS) closer to reality. This requires sharing and integration of data collected from multiple sources including modes of transports, sensors, and end-users' devices to allow a seamless and integrated services especially during unprecedented disturbances. This paper discusses the interactions among transportation modes, networks, potential disturbance scenarios, and adaptation strategies to mitigate their impact on MaaS. We particularly discuss the need to share data between the modes of transport and relevant entities that are at the vicinity of each other, taking advantage of edge computing technology to avoid any latency due to communication to the cloud and privacy concerns. However, when sharing at the edge, bandwidth, storage, and computational limitations must be considered.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12432",
        "abstract url": "https://arxiv.org/abs/2405.12432",
        "title": "Power Measurement Based Channel Estimation for IRS-Enhanced Wireless Coverage",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we study an IRS-assisted coverage enhancement problem for a given region, aiming to optimize the passive reflection of the IRS for improving the average communication performance in the region by accounting for both deterministic and random channels in the environment. To this end, we first derive the closed-form expression of the average received signal power in terms of the deterministic base station (BS)-IRS-user cascaded channels over all user locations, and propose an IRS-aided coverage enhancement framework to facilitate the estimation of such deterministic channels for IRS passive reflection design. Specifically, to avoid the exorbitant overhead of estimating the cascaded channels at all possible user locations, a location selection method is first proposed to select only a set of typical user locations for channel estimation by exploiting the channel spatial correlation in the region. To estimate the deterministic cascaded channels at the selected user locations, conventional IRS channel estimation methods require additional pilot signals, which not only results in high system training overhead but also may not be compatible with the existing communication protocols. To overcome this issue, we further propose a single-layer neural network (NN)-enabled IRS channel estimation method in this paper, based on only the average received signal power measurements at each selected location corresponding to different IRS random training reflections, which can be offline implemented in current wireless systems. Numerical results demonstrate that our proposed scheme can significantly improve the coverage performance of the target region and outperform the existing power-measurement-based IRS reflection designs.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2309.08275"
    },
    {
        "paper id": "2405.12444",
        "abstract url": "https://arxiv.org/abs/2405.12444",
        "title": "Reach and hold flexibility characterization and trade-off analysis for aggregations of thermostatically controlled loads",
        "rating": "-10",
        "keywords": [],
        "abstract": "Thermostatically controlled loads (TCLs) have the potential to be flexible and responsive loads to be used in demand response (DR) schemes. With increasing renewable penetration, DR is playing an increasingly important role in enhancing power grid reliability. The aggregate demand of a population of TCLs can be modulated by changing their temperature setpoint. When and/or what proportion of the population sees the setpoint change determines the change in aggregate demand. However, since the TCL population is finite, not all changes in aggregate demand can be maintained for arbitrarily long periods of time. In this paper, the dynamic behavior of a TCL fleet is modeled and used to characterize the set possible changes in aggregate demand that can be reached and the corresponding time for which the demand change can be held, for a given change in setpoint. This set is referred to, in this paper, as the reach and hold set of a TCL fleet. Furthermore, the effect of the setpoint change and ambient temperature on the reach and hold are analyzed. The characterized set is then validated through simulation using both the population TCL models and individual TCL micro-models.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12478",
        "abstract url": "https://arxiv.org/abs/2405.12478",
        "title": "Efficient Economic Model Predictive Control of Water Treatment Process with Learning-based Koopman Operator",
        "rating": "-10",
        "keywords": [],
        "abstract": "Used water treatment plays a pivotal role in advancing environmental sustainability. Economic model predictive control holds the promise of enhancing the overall operational performance of the water treatment facilities. In this study, we propose a data-driven economic predictive control approach within the Koopman modeling framework. First, we propose a deep learning-enabled input-output Koopman modeling approach, which predicts the overall economic operational cost of the wastewater treatment process based on input data and available output measurements that are directly linked to the operational costs. Subsequently, by leveraging this learned input-output Koopman model, a convex economic predictive control scheme is developed. The resulting predictive control problem can be efficiently solved by leveraging quadratic programming solvers, and complex non-convex optimization problems are bypassed. The proposed method is applied to a benchmark wastewater treatment process. The proposed method significantly improves the overall economic operational performance of the water treatment process. Additionally, the computational efficiency of the proposed method is significantly enhanced as compared to benchmark control solutions.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12480",
        "abstract url": "https://arxiv.org/abs/2405.12480",
        "title": "Towards Detecting and Mitigating Cognitive Bias in Spoken Conversational Search",
        "rating": "-10",
        "keywords": [],
        "abstract": "Instruments such as eye-tracking devices have contributed to understanding how users interact with screen-based search engines. However, user-system interactions in audio-only channels -- as is the case for Spoken Conversational Search (SCS) -- are harder to characterize, given the lack of instruments to effectively and precisely capture interactions. Furthermore, in this era of information overload, cognitive bias can significantly impact how we seek and consume information -- especially in the context of controversial topics or multiple viewpoints. This paper draws upon insights from multiple disciplines (including information seeking, psychology, cognitive science, and wearable sensors) to provoke novel conversations in the community. To this end, we discuss future opportunities and propose a framework including multimodal instruments and methods for experimental designs and settings. We demonstrate preliminary results as an example. We also outline the challenges and offer suggestions for adopting this multimodal approach, including ethical considerations, to assist future researchers and practitioners in exploring cognitive biases in SCS.",
        "subjects": [
            "cs.HC",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12484",
        "abstract url": "https://arxiv.org/abs/2405.12484",
        "title": "Meta-Homogenization for Knitwear Simulation",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents meta-homogenization, a spatially varying homogenization scheme for knitwear simulation. We are motivated by the observation that macro-scale fabric dynamics is strongly correlated with its underlying knitting patterns. Therefore, homogenization towards a single material is less effective when the knitting is complex and non-repetitive. Our method tackles this challenge by homogenizing the yarn-level material locally at volumetric elements. Assigning a virtual volume of a knitting structure enables us to model bending and twisting effects via a simple volume-preserving penalty and thus effectively alleviates the material nonlinearity. We employ an adjoint Gauss-Newton formulation to battle the dimensionality challenge of such per-element material optimization. This intuitive material model makes the forward simulation GPU-friendly. To this end, our pipeline also equips a novel domain-decomposed subspace solver crafted for GPU projective dynamics, which makes our simulator hundreds of times faster than the yarn-level simulator. Experiments validate the capability and effectiveness of meta-homogenization. Our method produces realistic animations of knitwear matching the quality of full-scale yarn-level simulations. It is also orders of magnitude faster than existing homogenization techniques in both the training and simulation stages.",
        "subjects": [
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.12497",
        "abstract url": "https://arxiv.org/abs/2405.12497",
        "title": "RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search",
        "rating": "-10",
        "keywords": [],
        "abstract": "Searching for approximate nearest neighbors (ANN) in the high-dimensional Euclidean space is a pivotal problem. Recently, with the help of fast SIMD-based implementations, Product Quantization (PQ) and its variants can often efficiently and accurately estimate the distances between the vectors and have achieved great success in the in-memory ANN search. Despite their empirical success, we note that these methods do not have a theoretical error bound and are observed to fail disastrously on some real-world datasets. Motivated by this, we propose a new randomized quantization method named RaBitQ, which quantizes $D$-dimensional vectors into $D$-bit strings. RaBitQ guarantees a sharp theoretical error bound and provides good empirical accuracy at the same time. In addition, we introduce efficient implementations of RaBitQ, supporting to estimate the distances with bitwise operations or SIMD-based operations. Extensive experiments on real-world datasets confirm that (1) our method outperforms PQ and its variants in terms of accuracy-efficiency trade-off by a clear margin and (2) its empirical performance is well-aligned with our theoretical analysis.",
        "subjects": [
            "cs.DB",
            "cs.DS",
            "cs.IR"
        ],
        "comment": "The paper has been accepted by SIGMOD 2024"
    }
]