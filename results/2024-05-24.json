[
    {
        "paper id": "2405.15228",
        "abstract url": "https://arxiv.org/abs/2405.15228",
        "title": "Learning from True-False Labels via Multi-modal Prompt Retrieving",
        "rating": "2",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Weakly supervised learning has recently achieved considerable success in reducing annotation costs and label noise. Unfortunately, existing weakly supervised learning methods are short of ability in generating reliable labels via pre-trained vision-language models (VLMs). In this paper, we propose a novel weakly supervised labeling setting, namely True-False Labels (TFLs) which can achieve high accuracy when generated by VLMs. The TFL indicates whether an instance belongs to the label, which is randomly and uniformly sampled from the candidate label set. Specifically, we theoretically derive a risk-consistent estimator to explore and utilize the conditional probability distribution information of TFLs. Besides, we propose a convolutional-based Multi-modal Prompt Retrieving (MRP) method to bridge the gap between the knowledge of VLMs and target learning tasks. Experimental results demonstrate the effectiveness of the proposed TFL setting and MRP learning method. The code to reproduce the experiments is at https://github.com/Tranquilxu/TMP.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "15 pages, 4 figures"
    },
    {
        "paper id": "2405.15289",
        "abstract url": "https://arxiv.org/abs/2405.15289",
        "title": "Learning Invariant Causal Mechanism from Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Pre-trained large-scale models have become a major research focus, but their effectiveness is limited in real-world applications due to diverse data distributions. In contrast, humans excel at decision-making across various domains by learning reusable knowledge that remains invariant despite environmental changes in a complex world. Although CLIP, as a successful vision-language pre-trained model, demonstrates remarkable performance in various visual downstream tasks, our experiments reveal unsatisfactory results in specific domains. Our further analysis with causal inference exposes the current CLIP model's inability to capture the invariant causal mechanisms across domains, attributed to its deficiency in identifying latent factors generating the data. To address this, we propose the Invariant Causal Mechanism of CLIP (CLIP-ICM), an algorithm designed to provably identify invariant latent factors with the aid of interventional data, and perform accurate prediction on various domains. Theoretical analysis demonstrates that our method has a lower generalization bound in out-of-distribution (OOD) scenarios. Experimental results showcase the outstanding performance of CLIP-ICM.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15356",
        "abstract url": "https://arxiv.org/abs/2405.15356",
        "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnect between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages. arXiv admin note: text overlap with arXiv:2311.16922 by other authors"
    },
    {
        "paper id": "2405.15525",
        "abstract url": "https://arxiv.org/abs/2405.15525",
        "title": "Sparse Matrix in Large Language Model Fine-tuning",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient fine-tuning",
                "GPU memory"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "LoRA and its variants have become popular parameter-efficient fine-tuning (PEFT) methods due to their ability to avoid excessive computational costs. However, an accuracy gap often exists between PEFT methods and full fine-tuning (FT), and this gap has yet to be systematically studied. In this work, we introduce a method for selecting sparse sub-matrices that aim to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method begins by identifying the most significant sub-matrices in the gradient update, updating only these blocks during the fine-tuning process. In our experiments, we demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issue.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2405.15549",
        "abstract url": "https://arxiv.org/abs/2405.15549",
        "title": "SEP: Self-Enhanced Prompt Tuning for Visual-Language Model",
        "rating": "2",
        "keywords": [
            [
                "Visual-Language",
                "VLMs"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Prompt tuning based on Context Optimization (CoOp) effectively adapts visual-language models (VLMs) to downstream tasks by inferring additional learnable prompt tokens. However, these tokens are less discriminative as they are independent of the pre-trained tokens and fail to capture input-specific knowledge, such as class-aware textual or instance-aware visual knowledge. Leveraging the discriminative and generalization capabilities inherent in pre-trained tokens, we introduce a novel approach named Self-Enhanced Prompt Tuning (SEP). The core principle of SEP involves adapting the learnable prompt tokens at each encoder layer from the corresponding self-pretrained tokens, thereby explicitly incorporating discriminative prior knowledge to enhance both textual-level and visual-level embeddings. Furthermore, SEP's self-enhanced tokens not only boost discrimination but also mitigate domain shifts in unseen domains, enhancing generalization. In practice, SEP selects several representative tokens from all pre-trained tokens for each input data at every layer of the text/visual encoders. Subsequently, a Token Fusion Module (TFM) is introduced to generate a self-enhanced token by merging these representative tokens with the learnable tokens using a cross-attention mechanism. This self-enhanced token is then concatenated with all pre-trained tokens, serving as input for subsequent encoder layers to produce the relevant embeddings. Comprehensive evaluations across various benchmarks and tasks confirm SEP's efficacy in prompt tuning. Code: \\href{Code}{https://github.com/htyao89/SEP}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15574",
        "abstract url": "https://arxiv.org/abs/2405.15574",
        "title": "Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models",
        "rating": "2",
        "keywords": [
            [
                "vision language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning. Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs. These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (e.g., charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions. Drawing from the multifaceted information, we present a new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities. To embed lengthy rationales containing abundant information, we employ the Mamba architecture, capable of processing sequential data with linear time complexity. We introduce a new concept of traversal of rationale that facilitates efficient embedding of rationale. Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale. Through these steps, Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code is available in https://github.com/ByungKwanLee/Meteor"
    },
    {
        "paper id": "2405.15683",
        "abstract url": "https://arxiv.org/abs/2405.15683",
        "title": "VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging the Visual Perception Gap",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Recent interest in Large Vision-Language Models (LVLMs) for practical applications is moderated by the significant challenge of hallucination or the inconsistency between the factual information and the generated text. In this paper, we first perform an in-depth analysis of hallucinations and discover several novel insights about how and when LVLMs hallucinate. From our analysis, we show that: (1) The community's efforts have been primarily targeted towards reducing hallucinations related to visual recognition (VR) prompts (e.g., prompts that only require describing the image), thereby ignoring hallucinations for cognitive prompts (e.g., prompts that require additional skills like reasoning on contents of the image). (2) LVLMs lack visual perception, i.e., they can see but not necessarily understand or perceive the input image. We analyze responses to cognitive prompts and show that LVLMs hallucinate due to a perception gap: although LVLMs accurately recognize visual elements in the input image and possess sufficient cognitive skills, they struggle to respond accurately and hallucinate. To overcome this shortcoming, we propose Visual Description Grounded Decoding (VDGD), a simple, robust, and training-free method for alleviating hallucinations. Specifically, we first describe the image and add it as a prefix to the instruction. Next, during auto-regressive decoding, we sample from the plausible candidates according to their KL-Divergence (KLD) to the description, where lower KLD is given higher preference. Experimental results on several benchmarks and LVLMs show that VDGD improves significantly over other baselines in reducing hallucinations. We also propose VaLLu, a benchmark for the comprehensive evaluation of the cognitive capabilities of LVLMs.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Preprint. Under review. Code will be released on paper acceptance"
    },
    {
        "paper id": "2405.15760",
        "abstract url": "https://arxiv.org/abs/2405.15760",
        "title": "GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction",
        "rating": "2",
        "keywords": [
            [
                "Social biases"
            ],
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.",
        "subjects": [
            "cs.CL",
            "cs.CY"
        ],
        "comment": "Accepted to ACL 2024 (main conference)"
    },
    {
        "paper id": "2405.15973",
        "abstract url": "https://arxiv.org/abs/2405.15973",
        "title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement",
        "rating": "2",
        "keywords": [
            [
                "Visual-Language"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Large vision-language models (LVLMs) have achieved impressive results in various visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there is still significant room for improvement in the alignment between visual and language modalities. Previous methods to enhance this alignment typically require external models or data, heavily depending on their capabilities and quality, which inevitably sets an upper bound on performance. In this paper, we propose SIMA, a framework that enhances visual and language modality alignment through self-improvement, eliminating the needs for external models or data. SIMA leverages prompts from existing vision instruction tuning datasets to self-generate responses and employs an in-context self-critic mechanism to select response pairs for preference tuning. The key innovation is the introduction of three vision metrics during the in-context self-critic process, which can guide the LVLM in selecting responses that enhance image comprehension. Through experiments across 14 hallucination and comprehensive benchmarks, we demonstrate that SIMA not only improves model performance across all benchmarks but also achieves superior modality alignment, outperforming previous approaches.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "15 pages, 8 figures"
    },
    {
        "paper id": "2405.16009",
        "abstract url": "https://arxiv.org/abs/2405.16009",
        "title": "Streaming Long Video Understanding with Large Language Models",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents VideoStreaming, an advanced vision-language large model (VLLM) for video understanding, that capably understands arbitrary-length video with a constant number of video tokens streamingly encoded and adaptively selected. The challenge of video understanding in the vision language area mainly lies in the significant computational burden caused by the great number of tokens extracted from long videos. Previous works rely on sparse sampling or frame compression to reduce tokens. However, such approaches either disregard temporal information in a long time span or sacrifice spatial details, resulting in flawed compression. To address these limitations, our VideoStreaming has two core designs: Memory-Propagated Streaming Encoding and Adaptive Memory Selection. The Memory-Propagated Streaming Encoding architecture segments long videos into short clips and sequentially encodes each clip with a propagated memory. In each iteration, we utilize the encoded results of the preceding clip as historical memory, which is integrated with the current clip to distill a condensed representation that encapsulates the video content up to the current timestamp. After the encoding process, the Adaptive Memory Selection strategy selects a constant number of question-related memories from all the historical memories and feeds them into the LLM to generate informative responses. The question-related selection reduces redundancy within the memories, enabling efficient and precise video understanding. Meanwhile, the disentangled video extraction and reasoning design allows the LLM to answer different questions about a video by directly selecting corresponding memories, without the need to encode the whole video for each question. Our model achieves superior performance and higher efficiency on long video benchmarks, showcasing precise temporal comprehension for detailed question answering.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16057",
        "abstract url": "https://arxiv.org/abs/2405.16057",
        "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a Sparsity-Preserved Parameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, keeping the structure and sparsity of pruned pre-trained models intact. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15225",
        "abstract url": "https://arxiv.org/abs/2405.15225",
        "title": "Unbiased Faster R-CNN for Single-source Domain Generalized Object Detection",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Single-source domain generalization (SDG) for object detection is a challenging yet essential task as the distribution bias of the unseen domain degrades the algorithm performance significantly. However, existing methods attempt to extract domain-invariant features, neglecting that the biased data leads the network to learn biased features that are non-causal and poorly generalizable. To this end, we propose an Unbiased Faster R-CNN (UFR) for generalizable feature learning. Specifically, we formulate SDG in object detection from a causal perspective and construct a Structural Causal Model (SCM) to analyze the data bias and feature bias in the task, which are caused by scene confounders and object attribute confounders. Based on the SCM, we design a Global-Local Transformation module for data augmentation, which effectively simulates domain diversity and mitigates the data bias. Additionally, we introduce a Causal Attention Learning module that incorporates a designed attention invariance loss to learn image-level features that are robust to scene confounders. Moreover, we develop a Causal Prototype Learning module with an explicit instance constraint and an implicit prototype constraint, which further alleviates the negative impact of object attribute confounders. Experimental results on five scenes demonstrate the prominent generalization ability of our method, with an improvement of 3.9% mAP on the Night-Clear scene.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024"
    },
    {
        "paper id": "2405.15243",
        "abstract url": "https://arxiv.org/abs/2405.15243",
        "title": "Less is More: Discovering Concise Network Explanations",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "We introduce Discovering Conceptual Network Explanations (DCNE), a new approach for generating human-comprehensible visual explanations to enhance the interpretability of deep neural image classifiers. Our method automatically finds visual explanations that are critical for discriminating between classes. This is achieved by simultaneously optimizing three criteria: the explanations should be few, diverse, and human-interpretable. Our approach builds on the recently introduced Concept Relevance Propagation (CRP) explainability method. While CRP is effective at describing individual neuronal activations, it generates too many concepts, which impacts human comprehension. Instead, DCNE selects the few most important explanations. We introduce a new evaluation dataset centered on the challenging task of classifying birds, enabling us to compare the alignment of DCNE's explanations to those of human expert-defined ones. Compared to existing eXplainable Artificial Intelligence (XAI) methods, DCNE has a desirable trade-off between conciseness and completeness when summarizing network explanations. It produces 1/30 of CRP's explanations while only resulting in a slight reduction in explanation quality. DCNE represents a step forward in making neural network decisions accessible and interpretable to humans, providing a valuable tool for both researchers and practitioners in XAI and model alignment.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 5 figures; ICLR Re-Align Workshop 2024; Project Page: https://www.vision.caltech.edu/dcne/; Github: https://github.com/nkondapa/DiscoveringConciseNetworkExplanations"
    },
    {
        "paper id": "2405.15282",
        "abstract url": "https://arxiv.org/abs/2405.15282",
        "title": "Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation",
        "rating": "1.5",
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has become the standard for customising Foundation Models (FMs) to user-specific downstream tasks. However, typical PEFT methods require storing multiple task-specific adapters, creating scalability issues as these adapters must be housed and run at the FM server. Traditional prompt tuning offers a potential solution by customising them through task-specific input prefixes, but it under-performs compared to other PEFT methods like LoRA. To address this gap, we propose Low-Rank Prompt Adaptation (LOPA), a prompt-tuning-based approach that performs on par with state-of-the-art PEFT methods and full fine-tuning while being more parameter-efficient and not requiring a server-based adapter. LOPA generates soft prompts by balancing between sharing task-specific information across instances and customization for each instance. It uses a low-rank decomposition of the soft-prompt component encoded for each instance to achieve parameter efficiency. We provide a comprehensive evaluation on multiple natural language understanding and code generation and understanding tasks across a wide range of foundation models with varying sizes.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "11 pages, 4 figures, 3 tables"
    },
    {
        "paper id": "2405.15384",
        "abstract url": "https://arxiv.org/abs/2405.15384",
        "title": "Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate",
        "rating": "1.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Real-world decision-making tasks are usually partially observable Markov decision processes (POMDPs), where the state is not fully observable. Recent progress has demonstrated that recurrent reinforcement learning (RL), which consists of a context encoder based on recurrent neural networks (RNNs) for unobservable state prediction and a multilayer perceptron (MLP) policy for decision making, can mitigate partial observability and serve as a robust baseline for POMDP tasks. However, previous recurrent RL methods face training stability issues due to the gradient instability of RNNs. In this paper, we propose Recurrent Off-policy RL with Context-Encoder-Specific Learning Rate (RESeL) to tackle this issue. Specifically, RESeL uses a lower learning rate for context encoder than other MLP layers to ensure the stability of the former while maintaining the training efficiency of the latter. We integrate this technique into existing off-policy RL methods, resulting in the RESeL algorithm. We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks. The experiments demonstrate significant improvements in training stability with RESeL. Comparative results show that RESeL achieves notable performance improvements over previous recurrent RL baselines in POMDP tasks, and is competitive with or even surpasses state-of-the-art methods in MDP tasks. Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15451",
        "abstract url": "https://arxiv.org/abs/2405.15451",
        "title": "Self-distilled Dynamic Fusion Network for Language-based Fashion Retrieval",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "In the domain of language-based fashion image retrieval, pinpointing the desired fashion item using both a reference image and its accompanying textual description is an intriguing challenge. Existing approaches lean heavily on static fusion techniques, intertwining image and text. Despite their commendable advancements, these approaches are still limited by a deficiency in flexibility. In response, we propose a Self-distilled Dynamic Fusion Network to compose the multi-granularity features dynamically by considering the consistency of routing path and modality-specific information simultaneously. Two new modules are included in our proposed method: (1) Dynamic Fusion Network with Modality Specific Routers. The dynamic network enables a flexible determination of the routing for each reference image and modification text, taking into account their distinct semantics and distributions. (2) Self Path Distillation Loss. A stable path decision for queries benefits the optimization of feature extraction as well as routing, and we approach this by progressively refine the path decision with previous path information. Extensive experiments demonstrate the effectiveness of our proposed model compared to existing methods.",
        "subjects": [
            "cs.CV",
            "cs.IR",
            "cs.MM"
        ],
        "comment": "ICASSP 2024"
    },
    {
        "paper id": "2405.15908",
        "abstract url": "https://arxiv.org/abs/2405.15908",
        "title": "Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine",
        "rating": "1.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Automated penetration testing (AutoPT) based on reinforcement learning (RL) has proven its ability to improve the efficiency of vulnerability identification in information systems. However, RL-based PT encounters several challenges, including poor sampling efficiency, intricate reward specification, and limited interpretability. To address these issues, we propose a knowledge-informed AutoPT framework called DRLRM-PT, which leverages reward machines (RMs) to encode domain knowledge as guidelines for training a PT policy. In our study, we specifically focus on lateral movement as a PT case study and formulate it as a partially observable Markov decision process (POMDP) guided by RMs. We design two RMs based on the MITRE ATT\\&CK knowledge base for lateral movement. To solve the POMDP and optimize the PT policy, we employ the deep Q-learning algorithm with RM (DQRM). The experimental results demonstrate that the DQRM agent exhibits higher training efficiency in PT compared to agents without knowledge embedding. Moreover, RMs encoding more detailed domain knowledge demonstrated better PT performance compared to RMs with simpler knowledge.",
        "subjects": [
            "cs.AI",
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15928",
        "abstract url": "https://arxiv.org/abs/2405.15928",
        "title": "PatchProt: Hydrophobic patch prediction using protein foundation models",
        "rating": "1.5",
        "keywords": [
            [
                "parameter-efficient",
                "Efficient fine-tuning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Hydrophobic patches on protein surfaces play important functional roles in protein-protein and protein-ligand interactions. Large hydrophobic surfaces are also involved in the progression of aggregation diseases. Predicting exposed hydrophobic patches from a protein sequence has been shown to be a difficult task. Fine-tuning foundation models allows for adapting a model to the specific nuances of a new task using a much smaller dataset. Additionally, multi-task deep learning offers a promising solution for addressing data gaps, simultaneously outperforming single-task methods. In this study, we harnessed a recently released leading large language model ESM-2. Efficient fine-tuning of ESM-2 was achieved by leveraging a recently developed parameter-efficient fine-tuning method. This approach enabled comprehensive training of model layers without excessive parameters and without the need to include a computationally expensive multiple sequence analysis. We explored several related tasks, at local (residue) and global (protein) levels, to improve the representation of the model. As a result, our fine-tuned ESM-2 model, PatchProt, cannot only predict hydrophobic patch areas but also outperforms existing methods at predicting primary tasks, including secondary structure and surface accessibility predictions. Importantly, our analysis shows that including related local tasks can improve predictions on more difficult global tasks. This research sets a new standard for sequence-based protein property prediction and highlights the remarkable potential of fine-tuning foundation models enriching the model representation by training over related tasks.",
        "subjects": [
            "q-bio.QM",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15961",
        "abstract url": "https://arxiv.org/abs/2405.15961",
        "title": "Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measures and Synthetic Scene Images",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Domain Generalization (DG) is a challenging task in machine learning that requires a coherent ability to comprehend shifts across various domains through extraction of domain-invariant features. DG performance is typically evaluated by performing image classification in domains of various image styles. However, current methodology lacks quantitative understanding about shifts in stylistic domain, and relies on a vast amount of pre-training data, such as ImageNet1K, which are predominantly in photo-realistic style with weakly supervised class labels. Such a data-driven practice could potentially result in spurious correlation and inflated performance on DG benchmarks. In this paper, we introduce a new DG paradigm to address these risks. We first introduce two new quantitative measures ICV and IDD to describe domain shifts in terms of consistency of classes within one domain and similarity between two stylistic domains. We then present SuperMarioDomains (SMD), a novel synthetic multi-domain dataset sampled from video game scenes with more consistent classes and sufficient dissimilarity compared to ImageNet1K. We demonstrate our DG method SMOS. SMOS first uses SMD to train a precursor model, which is then used to ground the training on a DG benchmark. We observe that SMOS contributes to state-of-the-art performance across five DG benchmarks, gaining large improvements to performances on abstract domains along with on-par or slight improvements to those on photo-realistic domains. Our qualitative analysis suggests that these improvements can be attributed to reduced distributional divergence between originally distant domains. Our data are available at https://github.com/fpsluozi/SMD-SMOS .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at the 3rd CVPR Workshop on Vision Datasets Understanding"
    },
    {
        "paper id": "2405.15223",
        "abstract url": "https://arxiv.org/abs/2405.15223",
        "title": "iVideoGPT: Interactive VideoGPTs are Scalable World Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making. However, the high demand for interactivity poses challenges in harnessing recent advancements in video generative models for developing world models at scale. This work introduces Interactive VideoGPT (iVideoGPT), a scalable autoregressive transformer framework that integrates multimodal signals--visual observations, actions, and rewards--into a sequence of tokens, facilitating an interactive experience of agents via next-token prediction. iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations. Leveraging its scalable architecture, we are able to pre-train iVideoGPT on millions of human and robotic manipulation trajectories, establishing a versatile foundation that is adaptable to serve as interactive world models for a wide range of downstream tasks. These include action-conditioned video prediction, visual planning, and model-based reinforcement learning, where iVideoGPT achieves competitive performance compared with state-of-the-art methods. Our work advances the development of interactive general world models, bridging the gap between generative video models and practical model-based reinforcement learning applications.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15240",
        "abstract url": "https://arxiv.org/abs/2405.15240",
        "title": "Towards Real World Debiasing: A Fine-grained Analysis On Spurious Correlation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Spurious correlations in training data significantly hinder the generalization capability of machine learning models when faced with distribution shifts in real-world scenarios. To tackle the problem, numerous debias approaches have been proposed and benchmarked on datasets intentionally designed with severe biases. However, it remains to be asked: \\textit{1. Do existing benchmarks really capture biases in the real world? 2. Can existing debias methods handle biases in the real world?} To answer the questions, we revisit biased distributions in existing benchmarks and real-world datasets, and propose a fine-grained framework for analyzing dataset bias by disentangling it into the magnitude and prevalence of bias. We observe and theoretically demonstrate that existing benchmarks poorly represent real-world biases. We further introduce two novel biased distributions to bridge this gap, forming a nuanced evaluation framework for real-world debiasing. Building upon these results, we evaluate existing debias methods with our evaluation framework. Results show that existing methods are incapable of handling real-world biases. Through in-depth analysis, we propose a simple yet effective approach that can be easily applied to existing debias methods, named Debias in Destruction (DiD). Empirical results demonstrate the superiority of DiD, improving the performance of existing methods on all types of biases within the proposed evaluation framework.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "9 pages of main paper, 10 pages of appendix"
    },
    {
        "paper id": "2405.15265",
        "abstract url": "https://arxiv.org/abs/2405.15265",
        "title": "Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Cross-Domain Few-shot Semantic Segmentation (CD-FSS) aims to train generalized models that can segment classes from different domains with a few labeled images. Previous works have proven the effectiveness of feature transformation in addressing CD-FSS. However, they completely rely on support images for feature transformation, and repeatedly utilizing a few support images for each class may easily lead to overfitting and overlooking intra-class appearance differences. In this paper, we propose a Doubly Matching Transformation-based Network (DMTNet) to solve the above issue. Instead of completely relying on support images, we propose Self-Matching Transformation (SMT) to construct query-specific transformation matrices based on query images themselves to transform domain-specific query features into domain-agnostic ones. Calculating query-specific transformation matrices can prevent overfitting, especially for the meta-testing stage where only one or several images are used as support images to segment hundreds or thousands of images. After obtaining domain-agnostic features, we exploit a Dual Hypercorrelation Construction (DHC) module to explore the hypercorrelations between the query image with the foreground and background of the support image, based on which foreground and background prediction maps are generated and supervised, respectively, to enhance the segmentation result. In addition, we propose a Test-time Self-Finetuning (TSF) strategy to more accurately self-tune the query prediction in unseen domains. Extensive experiments on four popular datasets show that DMTNet achieves superior performance over state-of-the-art approaches. Code is available at https://github.com/ChenJiayi68/DMTNet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15267",
        "abstract url": "https://arxiv.org/abs/2405.15267",
        "title": "Off-the-shelf ChatGPT is a Good Few-shot Human Motion Predictor",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "To facilitate the application of motion prediction in practice, recently, the few-shot motion prediction task has attracted increasing research attention. Yet, in existing few-shot motion prediction works, a specific model that is dedicatedly trained over human motions is generally required. In this work, rather than tackling this task through training a specific human motion prediction model, we instead propose a novel FMP-OC framework. In FMP-OC, in a totally training-free manner, we enable Few-shot Motion Prediction, which is a non-language task, to be performed directly via utilizing the Off-the-shelf language model ChatGPT. Specifically, to lead ChatGPT as a language model to become an accurate motion predictor, in FMP-OC, we first introduce several novel designs to facilitate extracting implicit knowledge from ChatGPT. Moreover, we also incorporate our framework with a motion-in-context learning mechanism. Extensive experiments demonstrate the efficacy of our proposed framework.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15279",
        "abstract url": "https://arxiv.org/abs/2405.15279",
        "title": "Towards Global Optimal Visual In-Context Learning Prompt Selection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visual In-Context Learning (VICL) is a prevailing way to transfer visual foundation models to new tasks by leveraging contextual information contained in in-context examples to enhance learning and prediction of query sample. The fundamental problem in VICL is how to select the best prompt to activate its power as much as possible, which is equivalent to the ranking problem to test the in-context behavior of each candidate in the alternative set and select the best one. To utilize more appropriate ranking metric and leverage more comprehensive information among the alternative set, we propose a novel in-context example selection framework to approximately identify the global optimal prompt, i.e. choosing the best performing in-context examples from all alternatives for each query sample. Our method, dubbed Partial2Global, adopts a transformer-based list-wise ranker to provide a more comprehensive comparison within several alternatives, and a consistency-aware ranking aggregator to generate globally consistent ranking. The effectiveness of Partial2Global is validated through experiments on foreground segmentation, single object detection and image colorization, demonstrating that Partial2Global selects consistently better in-context examples compared with other methods, and thus establish the new state-of-the-arts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15302",
        "abstract url": "https://arxiv.org/abs/2405.15302",
        "title": "Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models have consistently struggled with complex reasoning tasks, such as mathematical problem-solving. Investigating the internal reasoning mechanisms of these models can help us design better model architectures and training strategies, ultimately enhancing their reasoning capabilities. In this study, we examine the matching mechanism employed by Transformer for multi-step reasoning on a constructed dataset. We investigate factors that influence the model's matching mechanism and discover that small initialization and post-LayerNorm can facilitate the formation of the matching mechanism, thereby enhancing the model's reasoning ability. Moreover, we propose a method to improve the model's reasoning capability by adding orthogonal noise. Finally, we investigate the parallel reasoning mechanism of Transformers and propose a conjecture on the upper bound of the model's reasoning ability based on this phenomenon. These insights contribute to a deeper understanding of the reasoning processes in large language models and guide designing more effective reasoning architectures and training strategies.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15311",
        "abstract url": "https://arxiv.org/abs/2405.15311",
        "title": "Retro: Reusing teacher projection head for efficient embedding distillation on Lightweight Models via Self-supervised Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Self-supervised learning (SSL) is gaining attention for its ability to learn effective representations with large amounts of unlabeled data. Lightweight models can be distilled from larger self-supervised pre-trained models using contrastive and consistency constraints. Still, the different sizes of the projection heads make it challenging for students to mimic the teacher's embedding accurately. We propose \\textsc{Retro}, which reuses the teacher's projection head for students, and our experimental results demonstrate significant improvements over the state-of-the-art on all lightweight models. For instance, when training EfficientNet-B0 using ResNet-50/101/152 as teachers, our approach improves the linear result on ImageNet to $66.9\\%$, $69.3\\%$, and $69.8\\%$, respectively, with significantly fewer parameters.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15318",
        "abstract url": "https://arxiv.org/abs/2405.15318",
        "title": "Are Long-LLMs A Necessity For Long-Context Tasks?",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks' inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2405.15319",
        "abstract url": "https://arxiv.org/abs/2405.15319",
        "title": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "LLMs are computationally expensive to pre-train due to their large scale. Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored. This work identifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1) lack of comprehensive evaluation, ($\\textit{O}$2) untested viability for scaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle $\\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting. Our findings reveal that a depthwise stacking operator, called $G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to address $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested scalability), our study shows that $G_{\\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens. For example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to the same loss with 194B tokens, resulting in a 54.6\\% speedup. We further address $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\\text{stack}}$, making it practical in general LLM pre-training. We also provide in-depth discussions and comprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained model are available at $\\href{https://llm-stacking.github.io/}{https://llm-stacking.github.io/}$.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Preprint; The project link: $\\href{https://llm-stacking.github.io/}{https://llm-stacking.github.io/}$"
    },
    {
        "paper id": "2405.15329",
        "abstract url": "https://arxiv.org/abs/2405.15329",
        "title": "Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The acceleration of Large Language Models (LLMs) research has opened up new possibilities for evaluating generated texts. They serve as scalable and economical evaluators, but the question of how reliable these evaluators are has emerged as a crucial research question. Prior research efforts in the meta-evaluation of LLMs as judges limit the prompting of an LLM to a single use to obtain a final evaluation decision. They then compute the agreement between LLMs' outputs and human labels. This lacks interpretability in understanding the evaluation capability of LLMs. In light of this challenge, we propose Decompose and Aggregate, which breaks down the evaluation process into different stages based on pedagogical practices. Our experiments illustrate that it not only provides a more interpretable window for how well LLMs evaluate, but also leads to improvements up to 39.6% for different LLMs on a variety of meta-evaluation benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15334",
        "abstract url": "https://arxiv.org/abs/2405.15334",
        "title": "Detection and Positive Reconstruction of Cognitive Distortion sentences: Mandarin Dataset and Evaluation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This research introduces a Positive Reconstruction Framework based on positive psychology theory. Overcoming negative thoughts can be challenging, our objective is to address and reframe them through a positive reinterpretation. To tackle this challenge, a two-fold approach is necessary: identifying cognitive distortions and suggesting a positively reframed alternative while preserving the original thought's meaning. Recent studies have investigated the application of Natural Language Processing (NLP) models in English for each stage of this process. In this study, we emphasize the theoretical foundation for the Positive Reconstruction Framework, grounded in broaden-and-build theory. We provide a shared corpus containing 4001 instances for detecting cognitive distortions and 1900 instances for positive reconstruction in Mandarin. Leveraging recent NLP techniques, including transfer learning, fine-tuning pretrained networks, and prompt engineering, we demonstrate the effectiveness of automated tools for both tasks. In summary, our study contributes to multilingual positive reconstruction, highlighting the effectiveness of NLP in cognitive distortion detection and positive reconstruction.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15341",
        "abstract url": "https://arxiv.org/abs/2405.15341",
        "title": "V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the rapidly evolving landscape of AI research and application, Multimodal Large Language Models (MLLMs) have emerged as a transformative force, adept at interpreting and integrating information from diverse modalities such as text, images, and Graphical User Interfaces (GUIs). Despite these advancements, the nuanced interaction and understanding of GUIs pose a significant challenge, limiting the potential of existing models to enhance automation levels. To bridge this gap, this paper presents V-Zen, an innovative Multimodal Large Language Model (MLLM) meticulously crafted to revolutionise the domain of GUI understanding and grounding. Equipped with dual-resolution image encoders, V-Zen establishes new benchmarks in efficient grounding and next-action prediction, thereby laying the groundwork for self-operating computer systems. Complementing V-Zen is the GUIDE dataset, an extensive collection of real-world GUI elements and task-based sequences, serving as a catalyst for specialised fine-tuning. The successful integration of V-Zen and GUIDE marks the dawn of a new era in multimodal AI research, opening the door to intelligent, autonomous computing experiences. This paper extends an invitation to the research community to join this exciting journey, shaping the future of GUI automation. In the spirit of open science, our code, data, and model will be made publicly available, paving the way for multimodal dialogue scenarios with intricate and precise interactions.",
        "subjects": [
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15346",
        "abstract url": "https://arxiv.org/abs/2405.15346",
        "title": "BiSup: Bidirectional Quantization Error Suppression for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "diffusion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "As the size and context length of Large Language Models (LLMs) grow, weight-activation quantization has emerged as a crucial technique for efficient deployment of LLMs. Compared to weight-only quantization, weight-activation quantization presents greater challenges due to the presence of outliers in activations. Existing methods have made significant progress by exploring mixed-precision quantization and outlier suppression. However, these methods primarily focus on optimizing the results of single matrix multiplication, neglecting the bidirectional propagation of quantization errors in LLMs. Specifically, errors accumulate vertically within the same token through layers, and diffuse horizontally across different tokens due to self-attention mechanisms. To address this issue, we introduce BiSup, a Bidirectional quantization error Suppression method. By constructing appropriate optimizable parameter spaces, BiSup utilizes a small amount of data for quantization-aware parameter-efficient fine-tuning to suppress the error vertical accumulation. Besides, BiSup employs prompt mixed-precision quantization strategy, which preserves high precision for the key-value cache of system prompts, to mitigate the error horizontal diffusion. Extensive experiments on Llama and Qwen families demonstrate that BiSup can improve performance over two state-of-the-art methods (the average WikiText2 perplexity decreases from 13.26 to 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128 configuration), further facilitating the practical applications of low-bit weight-activation quantization.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15362",
        "abstract url": "https://arxiv.org/abs/2405.15362",
        "title": "Pipeline Parallelism with Controllable Memory",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology. In this paper, we propose a framework to decompose pipeline schedules as repeating a building block and we show that the lifespan of the building block decides the peak activation memory of the pipeline schedule. Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient. To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput. We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in terms of throughput. When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our proposed methods demonstrate a 16% throughput improvement over the 1F1B baseline for large language models.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15365",
        "abstract url": "https://arxiv.org/abs/2405.15365",
        "title": "U3M: Unbiased Multiscale Modal Fusion Model for Multimodal Semantic Segmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal semantic segmentation is a pivotal component of computer vision and typically surpasses unimodal methods by utilizing rich information set from various sources.Current models frequently adopt modality-specific frameworks that inherently biases toward certain modalities. Although these biases might be advantageous in specific situations, they generally limit the adaptability of the models across different multimodal contexts, thereby potentially impairing performance. To address this issue, we leverage the inherent capabilities of the model itself to discover the optimal equilibrium in multimodal fusion and introduce U3M: An Unbiased Multiscale Modal Fusion Model for Multimodal Semantic Segmentation. Specifically, this method involves an unbiased integration of multimodal visual data. Additionally, we employ feature fusion at multiple scales to ensure the effective extraction and integration of both global and local features. Experimental results demonstrate that our approach achieves superior performance across multiple datasets, verifing its efficacy in enhancing the robustness and versatility of semantic segmentation in diverse settings. Our code is available at U3M-multimodal-semantic-segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15369",
        "abstract url": "https://arxiv.org/abs/2405.15369",
        "title": "Cross-Domain Policy Adaptation by Capturing Representation Mismatch",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "It is vital to learn effective policies that can be transferred to different domains with dynamics discrepancies in reinforcement learning (RL). In this paper, we consider dynamics adaptation settings where there exists dynamics mismatch between the source domain and the target domain, and one can get access to sufficient source domain data, while can only have limited interactions with the target domain. Existing methods address this problem by learning domain classifiers, performing data filtering from a value discrepancy perspective, etc. Instead, we tackle this challenge from a decoupled representation learning perspective. We perform representation learning only in the target domain and measure the representation deviations on the transitions from the source domain, which we show can be a signal of dynamics mismatch. We also show that representation deviation upper bounds performance difference of a given policy in the source domain and target domain, which motivates us to adopt representation deviation as a reward penalty. The produced representations are not involved in either policy or value function, but only serve as a reward penalizer. We conduct extensive experiments on environments with kinematic and morphology mismatch, and the results show that our method exhibits strong performance on many tasks. Our code is publicly available at https://github.com/dmksjfl/PAR.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "ICML 2024"
    },
    {
        "paper id": "2405.15413",
        "abstract url": "https://arxiv.org/abs/2405.15413",
        "title": "MambaVC: Learned Visual Compression with Selective State Spaces",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Learned visual compression is an important and active task in multimedia. Existing approaches have explored various CNN- and Transformer-based designs to model content distribution and eliminate redundancy, where balancing efficacy (i.e., rate-distortion trade-off) and efficiency remains a challenge. Recently, state-space models (SSMs) have shown promise due to their long-range modeling capacity and efficiency. Inspired by this, we take the first step to explore SSMs for visual compression. We introduce MambaVC, a simple, strong and efficient compression network based on SSM. MambaVC develops a visual state space (VSS) block with a 2D selective scanning (2DSS) module as the nonlinear activation function after each downsampling, which helps to capture informative global contexts and enhances compression. On compression benchmark datasets, MambaVC achieves superior rate-distortion performance with lower computational and memory overheads. Specifically, it outperforms CNN and Transformer variants by 9.3% and 15.6% on Kodak, respectively, while reducing computation by 42% and 24%, and saving 12% and 71% of memory. MambaVC shows even greater improvements with high-resolution images, highlighting its potential and scalability in real-world applications. We also provide a comprehensive comparison of different network designs, underscoring MambaVC's advantages. Code is available at https://github.com/QinSY123/2024-MambaVC.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.IT"
        ],
        "comment": "17pages,15 figures"
    },
    {
        "paper id": "2405.15471",
        "abstract url": "https://arxiv.org/abs/2405.15471",
        "title": "Emergence of a High-Dimensional Abstraction Phase in Language Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15476",
        "abstract url": "https://arxiv.org/abs/2405.15476",
        "title": "Editable Concept Bottleneck Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on cases where the data, including concepts, are clean. In many scenarios, we always need to remove/insert some training data or new concepts from trained CBMs due to different reasons, such as privacy concerns, data mislabelling, spurious concepts, and concept annotation errors. Thus, the challenge of deriving efficient editable CBMs without retraining from scratch persists, particularly in large-scale applications. To address these challenges, we propose Editable Concept Bottleneck Models (ECBMs). Specifically, ECBMs support three different levels of data removal: concept-label-level, concept-level, and data-level. ECBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for re-training. Experimental results demonstrate the efficiency and effectiveness of our ECBMs, affirming their adaptability within the realm of CBMs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "33 pages"
    },
    {
        "paper id": "2405.15485",
        "abstract url": "https://arxiv.org/abs/2405.15485",
        "title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We are beginning to see progress in language model assisted scientific discovery. Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical skills required to solve problems. In particular, we look at not just what the pre-trained model already knows, but how it learned to learn from information during in-context learning or instruction-tuning through exploiting the complex knowledge structure within mathematics. Motivated by the Neural Tangent Kernel (NTK), we propose \\textit{NTKEval} to assess changes in LLM's probability distribution via training on different kinds of math data. Our systematic analysis finds evidence of domain understanding during in-context learning. By contrast, certain instruction-tuning leads to similar performance changes irrespective of training on different data, suggesting a lack of domain understanding across different skills.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15500",
        "abstract url": "https://arxiv.org/abs/2405.15500",
        "title": "Hierarchical Loss And Geometric Mask Refinement For Multilabel Ribs Segmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Automatic ribs segmentation and numeration can increase computed tomography assessment speed and reduce radiologists mistakes. We introduce a model for multilabel ribs segmentation with hierarchical loss function, which enable to improve multilabel segmentation quality. Also we propose postprocessing technique to further increase labeling quality. Our model achieved new state-of-the-art 98.2% label accuracy on public RibSeg v2 dataset, surpassing previous result by 6.7%.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Accepted to IEEE ISBI 2024"
    },
    {
        "paper id": "2405.15524",
        "abstract url": "https://arxiv.org/abs/2405.15524",
        "title": "Polyp Segmentation Generalisability of Pretrained Backbones",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "It has recently been demonstrated that pretraining backbones in a self-supervised manner generally provides better fine-tuned polyp segmentation performance, and that models with ViT-B backbones typically perform better than models with ResNet50 backbones. In this paper, we extend this recent work to consider generalisability. I.e., we assess the performance of models on a different dataset to that used for fine-tuning, accounting for variation in network architecture and pretraining pipeline (algorithm and dataset). This reveals how well models with different pretrained backbones generalise to data of a somewhat different distribution to the training data, which will likely arise in deployment due to different cameras and demographics of patients, amongst other factors. We observe that the previous findings, regarding pretraining pipelines for polyp segmentation, hold true when considering generalisability. However, our results imply that models with ResNet50 backbones typically generalise better, despite being outperformed by models with ViT-B backbones in evaluation on the test set from the same dataset used for fine-tuning.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15541",
        "abstract url": "https://arxiv.org/abs/2405.15541",
        "title": "Learning Generalizable Human Motion Generator with Reinforcement Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-driven human motion generation, as one of the vital tasks in computer-aided content creation, has recently attracted increasing attention. While pioneering research has largely focused on improving numerical performance metrics on given datasets, practical applications reveal a common challenge: existing methods often overfit specific motion expressions in the training data, hindering their ability to generalize to novel descriptions like unseen combinations of motions. This limitation restricts their broader applicability. We argue that the aforementioned problem primarily arises from the scarcity of available motion-text pairs, given the many-to-many nature of text-driven motion generation. To tackle this problem, we formulate text-to-motion generation as a Markov decision process and present \\textbf{InstructMotion}, which incorporate the trail and error paradigm in reinforcement learning for generalizable human motion generation. Leveraging contrastive pre-trained text and motion encoders, we delve into optimizing reward design to enable InstructMotion to operate effectively on both paired data, enhancing global semantic level text-motion alignment, and synthetic text-only data, facilitating better generalization to novel prompts without the need for ground-truth motion supervision. Extensive experiments on prevalent benchmarks and also our synthesized unpaired dataset demonstrate that the proposed InstructMotion achieves outstanding performance both quantitatively and qualitatively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15580",
        "abstract url": "https://arxiv.org/abs/2405.15580",
        "title": "Open-Vocabulary SAM3D: Understand Any 3D Scene",
        "rating": "1",
        "keywords": [
            [
                "vision language"
            ],
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Open-vocabulary 3D scene understanding presents a significant challenge in the field. Recent advancements have sought to transfer knowledge embedded in vision language models from the 2D domain to 3D domain. However, these approaches often require learning prior knowledge from specific 3D scene datasets, which limits their applicability in open-world scenarios. The Segment Anything Model (SAM) has demonstrated remarkable zero-shot segmentation capabilities, prompting us to investigate its potential for comprehending 3D scenes without the need for training. In this paper, we introduce OV-SAM3D, a universal framework for open-vocabulary 3D scene understanding. This framework is designed to perform understanding tasks for any 3D scene without requiring prior knowledge of the scene. Specifically, our method is composed of two key sub-modules: First, we initiate the process by generating superpoints as the initial 3D prompts and refine these prompts using segment masks derived from SAM. Moreover, we then integrate a specially designed overlapping score table with open tags from the Recognize Anything Model (RAM) to produce final 3D instances with open-world label. Empirical evaluations conducted on the ScanNet200 and nuScenes datasets demonstrate that our approach surpasses existing open-vocabulary methods in unknown open-world environments.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://hithqd.github.io/projects/OV-SAM3D"
    },
    {
        "paper id": "2405.15585",
        "abstract url": "https://arxiv.org/abs/2405.15585",
        "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLM) based end-to-end task-oriented dialog (TOD) systems built using few-shot (in-context) learning perform better than supervised models only when the train data is limited. This is due to the inherent ability of LLMs to learn any task with just a few demonstrations. As the number of train dialogs increases, supervised SoTA models surpass in-context learning LLMs as they learn to better align with the style of the system responses in the training data, which LLMs struggle to mimic. In response, we propose SyncTOD, which synergizes LLMs with useful hints about the task for improved alignment. At a high level, SyncTOD trains auxiliary models to provide these hints and select exemplars for the in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15590",
        "abstract url": "https://arxiv.org/abs/2405.15590",
        "title": "Profiling checkpointing schedules in adjoint ST-AD",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Checkpointing is a cornerstone of data-flow reversal in adjoint algorithmic differentiation. Checkpointing is a storage/recomputation trade-off that can be applied at different levels, one of which being the call tree. We are looking for good placements of checkpoints onto the call tree of a given application, to reduce run time and memory footprint of its adjoint. There is no known optimal solution to this problem other than a combinatorial search on all placements. We propose a heuristics based on run-time profiling of the adjoint code. We describe implementation of this profiling tool in an existing source-transformation AD tool. We demonstrate the interest of this approach on test cases taken from the MITgcm ocean and atmospheric global circulation model. We discuss the limitations of our approach and propose directions to lift them.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15604",
        "abstract url": "https://arxiv.org/abs/2405.15604",
        "title": "Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Text generation has become more accessible than ever, and the increasing interest in these systems, especially those using large language models, has spurred an increasing number of related publications. We provide a systematic literature review comprising 244 selected papers between 2017 and 2024. This review categorizes works in text generation into five main tasks: open-ended text generation, summarization, translation, paraphrasing, and question answering. For each task, we review their relevant characteristics, sub-tasks, and specific challenges (e.g., missing datasets for multi-document summarization, coherence in story generation, and complex reasoning for question answering). Additionally, we assess current approaches for evaluating text generation systems and ascertain problems with current metrics. Our investigation shows nine prominent challenges common to all tasks and sub-tasks in recent text generation publications: bias, reasoning, hallucinations, misuse, privacy, interpretability, transparency, datasets, and computing. We provide a detailed analysis of these challenges, their potential solutions, and which gaps still require further engagement from the community. This systematic literature review targets two main audiences: early career researchers in natural language processing looking for an overview of the field and promising research directions, as well as experienced researchers seeking a detailed view of tasks, evaluation methodologies, open challenges, and recent mitigation strategies.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "35 pages, 2 figures, 2 tables, Under review"
    },
    {
        "paper id": "2405.15628",
        "abstract url": "https://arxiv.org/abs/2405.15628",
        "title": "A Comparative Analysis of Distributed Training Strategies for GPT-2",
        "rating": "1",
        "keywords": [
            [
                "training efficiency"
            ]
        ],
        "abstract": "The rapid advancement in Large Language Models has been met with significant challenges in their training processes, primarily due to their considerable computational and memory demands. This research examines parallelization techniques developed to address these challenges, enabling the efficient and scalable training of Large Language Models. A comprehensive analysis of both data and model parallelism strategies, including Fully Sharded Data Parallelism and Distributed Data-Parallel frameworks, is provided to assess methods that facilitate efficient model training. Furthermore, the architectural complexities and training methodologies of the Generative Pre-Trained Transformer-2 model are explored. The application of these strategies is further investigated, which is crucial in managing the substantial computational and memory demands of training sophisticated models. This analysis not only highlights the effectiveness of these parallel training strategies in enhancing training efficiency but also their role in enabling the scalable training of large language models. Drawing on recent research findings, through a comprehensive literature review, this research underscores the critical role of parallelization techniques in addressing the computational challenges of training state-of-the-art Large Language Models, thereby contributing to the advancement of training more sophisticated and capable artificial intelligence systems.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Submitted to the International Journal of Parallel Programming and is currently under review"
    },
    {
        "paper id": "2405.15633",
        "abstract url": "https://arxiv.org/abs/2405.15633",
        "title": "Less is more: Summarizing Patch Tokens for efficient Multi-Label Class-Incremental Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Prompt tuning has emerged as an effective rehearsal-free technique for class-incremental learning (CIL) that learns a tiny set of task-specific parameters (or prompts) to instruct a pre-trained transformer to learn on a sequence of tasks. Albeit effective, prompt tuning methods do not lend well in the multi-label class incremental learning (MLCIL) scenario (where an image contains multiple foreground classes) due to the ambiguity in selecting the correct prompt(s) corresponding to different foreground objects belonging to multiple tasks. To circumvent this issue we propose to eliminate the prompt selection mechanism by maintaining task-specific pathways, which allow us to learn representations that do not interact with the ones from the other tasks. Since independent pathways in truly incremental scenarios will result in an explosion of computation due to the quadratically complex multi-head self-attention (MSA) operation in prompt tuning, we propose to reduce the original patch token embeddings into summarized tokens. Prompt tuning is then applied to these fewer summarized tokens to compute the final representation. Our proposed method Multi-Label class incremental learning via summarising pAtch tokeN Embeddings (MULTI-LANE) enables learning disentangled task-specific representations in MLCIL while ensuring fast inference. We conduct experiments in common benchmarks and demonstrate that our MULTI-LANE achieves a new state-of-the-art in MLCIL. Additionally, we show that MULTI-LANE is also competitive in the CIL setting. Source code available at https://github.com/tdemin16/multi-lane",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Published at 3rd Conference on Lifelong Learning Agents (CoLLAs), 2024"
    },
    {
        "paper id": "2405.15640",
        "abstract url": "https://arxiv.org/abs/2405.15640",
        "title": "GECKO: Generative Language Model for English, Code and Korean",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture. In this report, we share the experiences of several efforts to build a better data pipeline for the corpus and to train our model. GECKO shows great efficiency in token generations for both Korean and English, despite its small size of vocabulary. We measure the performance on the representative benchmarks in terms of Korean, English and Code, and it exhibits great performance on KMMLU (Korean MMLU) and modest performance in English and Code, even with its smaller number of trained tokens compared to English-focused LLMs. GECKO is available to the open-source community under a permissive license. We hope our work offers a research baseline and practical insights for Korean LLM research. The model can be found at: https://huggingface.co/kifai/GECKO-7B",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15655",
        "abstract url": "https://arxiv.org/abs/2405.15655",
        "title": "HiddenSpeaker: Generate Imperceptible Unlearnable Audios for Speaker Verification System",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "In recent years, the remarkable advancements in deep neural networks have brought tremendous convenience. However, the training process of a highly effective model necessitates a substantial quantity of samples, which brings huge potential threats, like unauthorized exploitation with privacy leakage. In response, we propose a framework named HiddenSpeaker, embedding imperceptible perturbations within the training speech samples and rendering them unlearnable for deep-learning-based speaker verification systems that employ large-scale speakers for efficient training. The HiddenSpeaker utilizes a simplified error-minimizing method named Single-Level Error-Minimizing (SLEM) to generate specific and effective perturbations. Additionally, a hybrid objective function is employed for human perceptual optimization, ensuring the perturbation is indistinguishable from human listeners. We conduct extensive experiments on multiple state-of-the-art (SOTA) models in the speaker verification domain to evaluate HiddenSpeaker. Our results demonstrate that HiddenSpeaker not only deceives the model with unlearnable samples but also enhances the imperceptibility of the perturbations, showcasing strong transferability across different models.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": "Accepted by IJCNN 2024"
    },
    {
        "paper id": "2405.15658",
        "abstract url": "https://arxiv.org/abs/2405.15658",
        "title": "HDC: Hierarchical Semantic Decoding with Counting Assistance for Generalized Referring Expression Segmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The newly proposed Generalized Referring Expression Segmentation (GRES) amplifies the formulation of classic RES by involving multiple/non-target scenarios. Recent approaches focus on optimizing the last modality-fused feature which is directly utilized for segmentation and object-existence identification. However, the attempt to integrate all-grained information into a single joint representation is impractical in GRES due to the increased complexity of the spatial relationships among instances and deceptive text descriptions. Furthermore, the subsequent binary target justification across all referent scenarios fails to specify their inherent differences, leading to ambiguity in object understanding. To address the weakness, we propose a $\\textbf{H}$ierarchical Semantic $\\textbf{D}$ecoding with $\\textbf{C}$ounting Assistance framework (HDC). It hierarchically transfers complementary modality information across granularities, and then aggregates each well-aligned semantic correspondence for multi-level decoding. Moreover, with complete semantic context modeling, we endow HDC with explicit counting capability to facilitate comprehensive object perception in multiple/single/non-target settings. Experimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO benchmarks demonstrate the effectiveness and rationality of HDC which outperforms the state-of-the-art GRES methods by a remarkable margin. Code will be available $\\href{https://github.com/RobertLuo1/HDC}{here}$.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15668",
        "abstract url": "https://arxiv.org/abs/2405.15668",
        "title": "What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large language models (LLMs) has been effectively used for many computer vision tasks, including image classification. In this paper, we present a simple yet effective approach for zero-shot image classification using multimodal LLMs. By employing multimodal LLMs, we generate comprehensive textual representations from input images. These textual representations are then utilized to generate fixed-dimensional features in a cross-modal embedding space. Subsequently, these features are fused together to perform zero-shot classification using a linear classifier. Our method does not require prompt engineering for each dataset; instead, we use a single, straightforward, set of prompts across all datasets. We evaluated our method on several datasets, and our results demonstrate its remarkable effectiveness, surpassing benchmark accuracy on multiple datasets. On average over ten benchmarks, our method achieved an accuracy gain of 4.1 percentage points, with an increase of 6.8 percentage points on the ImageNet dataset, compared to prior methods. Our findings highlight the potential of multimodal LLMs to enhance computer vision tasks such as zero-shot image classification, offering a significant improvement over traditional methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15684",
        "abstract url": "https://arxiv.org/abs/2405.15684",
        "title": "Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "To bridge the gap between vision and language modalities, Multimodal Large Language Models (MLLMs) usually learn an adapter that converts visual inputs to understandable tokens for Large Language Models (LLMs). However, most adapters generate consistent visual tokens, regardless of the specific objects of interest mentioned in the prompt. Since these adapters distribute equal attention to every detail in the image and focus on the entire scene, they may increase the cognitive load for LLMs, particularly when processing complex scenes. To alleviate this problem, we propose prompt-aware adapters. These adapters are designed with the capability to dynamically embed visual inputs based on the specific focus of the prompt. Specifically, prompt-aware adapters utilize both global and local textual features to capture the most relevant visual clues from the prompt at both coarse and fine granularity levels. This approach significantly enhances the ability of LLMs to understand and interpret visual content. Experiments on various visual question answering tasks, such as counting and position reasoning, demonstrate the effectiveness of prompt-aware adapters.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15687",
        "abstract url": "https://arxiv.org/abs/2405.15687",
        "title": "Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Conventional demographic inference methods have predominantly operated under the supervision of accurately labeled data, yet struggle to adapt to shifting social landscapes and diverse cultural contexts, leading to narrow specialization and limited accuracy in applications. Recently, the emergence of large multimodal models (LMMs) has shown transformative potential across various research tasks, such as visual comprehension and description. In this study, we explore the application of LMMs to demographic inference and introduce a benchmark for both quantitative and qualitative evaluation. Our findings indicate that LMMs possess advantages in zero-shot learning, interpretability, and handling uncurated 'in-the-wild' inputs, albeit with a propensity for off-target predictions. To enhance LMM performance and achieve comparability with supervised learning baselines, we propose a Chain-of-Thought augmented prompting approach, which effectively mitigates the off-target prediction issue.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Accepted to ICME 2024"
    },
    {
        "paper id": "2405.15701",
        "abstract url": "https://arxiv.org/abs/2405.15701",
        "title": "realSEUDO for real-time calcium imaging analysis",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Closed-loop neuroscience experimentation, where recorded neural activity is used to modify the experiment on-the-fly, is critical for deducing causal connections and optimizing experimental time. A critical step in creating a closed-loop experiment is real-time inference of neural activity from streaming recordings. One challenging modality for real-time processing is multi-photon calcium imaging (CI). CI enables the recording of activity in large populations of neurons however, often requires batch processing of the video data to extract single-neuron activity from the fluorescence videos. We use the recently proposed robust time-trace estimator-Sparse Emulation of Unused Dictionary Objects (SEUDO) algorithm-as a basis for a new on-line processing algorithm that simultaneously identifies neurons in the fluorescence video and infers their time traces in a way that is robust to as-yet unidentified neurons. To achieve real-time SEUDO (realSEUDO), we optimize the core estimator via both algorithmic improvements and an fast C-based implementation, and create a new cell finding loop to enable realSEUDO to also identify new cells. We demonstrate comparable performance to offline algorithms (e.g., CNMF), and improved performance over the current on-line approach (OnACID) at speeds of 120 Hz on average.",
        "subjects": [
            "eess.IV",
            "q-bio.NC",
            "q-bio.QM",
            "stat.CO"
        ],
        "comment": "20 pages, 8 figures"
    },
    {
        "paper id": "2405.15708",
        "abstract url": "https://arxiv.org/abs/2405.15708",
        "title": "EmpathicStories++: A Multimodal Dataset for Empathy towards Personal Experiences",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Modeling empathy is a complex endeavor that is rooted in interpersonal and experiential dimensions of human interaction, and remains an open problem within AI. Existing empathy datasets fall short in capturing the richness of empathy responses, often being confined to in-lab or acted scenarios, lacking longitudinal data, and missing self-reported labels. We introduce a new multimodal dataset for empathy during personal experience sharing: the EmpathicStories++ dataset (https://mitmedialab.github.io/empathic-stories-multimodal/) containing 53 hours of video, audio, and text data of 41 participants sharing vulnerable experiences and reading empathically resonant stories with an AI agent. EmpathicStories++ is the first longitudinal dataset on empathy, collected over a month-long deployment of social robots in participants' homes, as participants engage in natural, empathic storytelling interactions with AI agents. We then introduce a novel task of predicting individuals' empathy toward others' stories based on their personal experiences, evaluated in two contexts: participants' own personal shared story context and their reflections on stories they read. We benchmark this task using state-of-the-art models to pave the way for future improvements in contextualized and longitudinal empathy modeling. Our work provides a valuable resource for further research in developing empathetic AI systems and understanding the intricacies of human empathy within genuine, real-world settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to ACL 2024 Findings"
    },
    {
        "paper id": "2405.15729",
        "abstract url": "https://arxiv.org/abs/2405.15729",
        "title": "Optimizing Large Language Models for OpenAPI Code Completion",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) and their utilization in code generation tasks have significantly reshaped the field of software development. Despite the remarkable efficacy of code completion solutions in mainstream programming languages, their performance lags when applied to less ubiquitous formats such as OpenAPI definitions. This study evaluates the OpenAPI completion performance of GitHub Copilot, a prevalent commercial code completion tool, and proposes a set of task-specific optimizations leveraging Meta's open-source model Code Llama. A semantics-aware OpenAPI completion benchmark proposed in this research is used to perform a series of experiments through which the impact of various prompt-engineering and fine-tuning techniques on the Code Llama model's performance is analyzed. The fine-tuned Code Llama model reaches a peak correctness improvement of 55.2% over GitHub Copilot despite utilizing 25 times fewer parameters than the commercial solution's underlying Codex model. Additionally, this research proposes an enhancement to a widely used code infilling training technique, addressing the issue of underperformance when the model is prompted with context sizes smaller than those used during training.",
        "subjects": [
            "cs.SE",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15734",
        "abstract url": "https://arxiv.org/abs/2405.15734",
        "title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks",
        "rating": "1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The success of large language models (LLMs) has fostered a new research trend of multi-modality large language models (MLLMs), which changes the paradigm of various fields in computer vision. Though MLLMs have shown promising results in numerous high-level vision and vision-language tasks such as VQA and text-to-image, no works have demonstrated how low-level vision tasks can benefit from MLLMs. We find that most current MLLMs are blind to low-level features due to their design of vision modules, thus are inherently incapable for solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$, a framework that enables a FROZEN LLM to solve a range of low-level vision tasks without any multi-modal data or prior. This showcases the LLM's strong potential in low-level vision and bridges the gap between MLLMs and low-level vision tasks. We hope this work can inspire new perspectives on LLMs and deeper understanding of their mechanisms.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15738",
        "abstract url": "https://arxiv.org/abs/2405.15738",
        "title": "ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "High-resolution Large Multimodal Models (LMMs) encounter the challenges of excessive visual tokens and quadratic visual complexity. Current high-resolution LMMs address the quadratic complexity while still generating excessive visual tokens. However, the redundancy in visual tokens is the key problem as it leads to more substantial compute. To mitigate this issue, we propose ConvLLaVA, which employs ConvNeXt, a hierarchical backbone, as the visual encoder of LMM to replace Vision Transformer (ViT). ConvLLaVA compresses high-resolution images into information-rich visual features, effectively preventing the generation of excessive visual tokens. To enhance the capabilities of ConvLLaVA, we propose two critical optimizations. Since the low-resolution pretrained ConvNeXt underperforms when directly applied on high resolution, we update it to bridge the gap. Moreover, since ConvNeXt's original compression ratio is inadequate for much higher resolution inputs, we train a successive stage to further compress the visual tokens, thereby reducing redundancy. These optimizations enable ConvLLaVA to support inputs of 1536x1536 resolution generating only 576 visual tokens, capable of handling images of arbitrary aspect ratios. Experimental results demonstrate that our method achieves competitive performance with state-of-the-art models on mainstream benchmarks. The ConvLLaVA model series are publicly available at https://github.com/alibaba/conv-llava.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2405.15750",
        "abstract url": "https://arxiv.org/abs/2405.15750",
        "title": "Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "10 pages + 7 pages of references/appendices. For code and trained models, see http://github.com/CLMBRs/corpus-filtering"
    },
    {
        "paper id": "2405.15755",
        "abstract url": "https://arxiv.org/abs/2405.15755",
        "title": "ETTrack: Enhanced Temporal Motion Predictor for Multi-Object Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Many Multi-Object Tracking (MOT) approaches exploit motion information to associate all the detected objects across frames. However, many methods that rely on filtering-based algorithms, such as the Kalman Filter, often work well in linear motion scenarios but struggle to accurately predict the locations of objects undergoing complex and non-linear movements. To tackle these scenarios, we propose a motion-based MOT approach with an enhanced temporal motion predictor, ETTrack. Specifically, the motion predictor integrates a transformer model and a Temporal Convolutional Network (TCN) to capture short-term and long-term motion patterns, and it predicts the future motion of individual objects based on the historical motion information. Additionally, we propose a novel Momentum Correction Loss function that provides additional information regarding the motion direction of objects during training. This allows the motion predictor rapidly adapt to motion variations and more accurately predict future motion. Our experimental results demonstrate that ETTrack achieves a competitive performance compared with state-of-the-art trackers on DanceTrack and SportsMOT, scoring 56.4% and 74.4% in HOTA metrics, respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 7 figures"
    },
    {
        "paper id": "2405.15756",
        "abstract url": "https://arxiv.org/abs/2405.15756",
        "title": "Sparse Expansion and Neuronal Disentanglement",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We show how to improve the inference efficiency of an LLM by expanding it into a mixture of sparse experts, where each expert is a copy of the original weights, one-shot pruned for a specific cluster of input values. We call this approach $\\textit{Sparse Expansion}$. We show that, for models such as Llama 2 70B, as we increase the number of sparse experts, Sparse Expansion outperforms all other one-shot sparsification approaches for the same inference FLOP budget per token, and that this gap grows as sparsity increases, leading to inference speedups. But why? To answer this, we provide strong evidence that the mixture of sparse experts is effectively $\\textit{disentangling}$ the input-output relationship of every individual neuron across clusters of inputs. Specifically, sparse experts approximate the dense neuron output distribution with fewer weights by decomposing the distribution into a collection of simpler ones, each with a separate sparse dot product covering it. Interestingly, we show that the Wasserstein distance between a neuron's output distribution and a Gaussian distribution is an indicator of its entanglement level and contribution to the accuracy of the model. Every layer of an LLM has a fraction of highly entangled Wasserstein neurons, and model performance suffers more when these are sparsified as opposed to others.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "9 pages, 8 figures, Submitted to NeurIPS 2024 main track"
    },
    {
        "paper id": "2405.15765",
        "abstract url": "https://arxiv.org/abs/2405.15765",
        "title": "Scaling Laws for Discriminative Classification in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Modern large language models (LLMs) represent a paradigm shift in what can plausibly be expected of machine learning models. The fact that LLMs can effectively generate sensible answers to a diverse range of queries suggests that they would be useful in customer support applications. While powerful, LLMs have been observed to be prone to hallucination which unfortunately makes their near term use in customer support applications challenging. To address this issue we present a system that allows us to use an LLM to augment our customer support advocates by re-framing the language modeling task as a discriminative classification task. In this framing, we seek to present the top-K best template responses for a customer support advocate to use when responding to a customer. We present the result of both offline and online experiments where we observed offline gains and statistically significant online lifts for our experimental system. Along the way, we present observed scaling curves for validation loss and top-K accuracy, resulted from model parameter ablation studies. We close by discussing the space of trade-offs with respect to model size, latency, and accuracy as well as and suggesting future applications to explore.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15860",
        "abstract url": "https://arxiv.org/abs/2405.15860",
        "title": "Free Performance Gain from Mixing Multiple Partially Labeled Samples in Multi-label Image Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Multi-label image classification datasets are often partially labeled where many labels are missing, posing a significant challenge to training accurate deep classifiers. However, the powerful Mixup sample-mixing data augmentation cannot be well utilized to address this challenge, as it cannot perform linear interpolation on the unknown labels to construct augmented samples. In this paper, we propose LogicMix, a Mixup variant designed for such partially labeled datasets. LogicMix mixes the sample labels by logical OR so that the unknown labels can be correctly mixed by utilizing OR's logical equivalences, including the domination and identity laws. Unlike Mixup, which mixes exactly two samples, LogicMix can mix multiple ($\\geq2$) partially labeled samples, constructing visually more confused augmented samples to regularize training. LogicMix is more general and effective than other compared Mixup variants in the experiments on various partially labeled dataset scenarios. Moreover, it is plug-and-play and only requires minimal computation, hence it can be easily inserted into existing frameworks to collaborate with other methods to improve model performance with a negligible impact on training time, as demonstrated through extensive experiments. In particular, through the collaboration of LogicMix, RandAugment, Curriculum Labeling, and Category-wise Fine-Tuning, we attain state-of-the-art performance on MS-COCO, VG-200, and Pascal VOC 2007 benchmarking datasets. The remarkable generality, effectiveness, collaboration, and simplicity suggest that LogicMix promises to be a popular and vital data augmentation method.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15877",
        "abstract url": "https://arxiv.org/abs/2405.15877",
        "title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.",
        "subjects": [
            "cs.LG",
            "cs.AR",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15886",
        "abstract url": "https://arxiv.org/abs/2405.15886",
        "title": "A Neurosymbolic Framework for Bias Correction in CNNs",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent efforts in interpreting Convolutional Neural Networks (CNNs) focus on translating the activation of CNN filters into stratified Answer Set Programming (ASP) rule-sets. The CNN filters are known to capture high-level image concepts, thus the predicates in the rule-set are mapped to the concept that their corresponding filter represents. Hence, the rule-set effectively exemplifies the decision-making process of the CNN in terms of the concepts that it learns for any image classification task. These rule-sets help expose and understand the biases in CNNs, although correcting the biases effectively remains a challenge. We introduce a neurosymbolic framework called NeSyBiCor for bias correction in a trained CNN. Given symbolic concepts that the CNN is biased towards, expressed as ASP constraints, we convert the undesirable and desirable concepts to their corresponding vector representations. Then, the CNN is retrained using our novel semantic similarity loss that pushes the filters away from the representations of concepts that are undesirable while pushing them closer to the concepts that are desirable. The final ASP rule-set obtained after retraining, satisfies the constraints to a high degree, thus showing the revision in the knowledge of the CNN for the image classification task. We demonstrate that our NeSyBiCor framework successfully corrects the biases of CNNs trained with subsets of classes from the Places dataset while sacrificing minimal accuracy and improving interpretability, by greatly decreasing the size of the final bias-corrected rule-set w.r.t. the initial rule-set.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15896",
        "abstract url": "https://arxiv.org/abs/2405.15896",
        "title": "Enhancing Augmentative and Alternative Communication with Card Prediction and Colourful Semantics",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents an approach to enhancing Augmentative and Alternative Communication (AAC) systems by integrating Colourful Semantics (CS) with transformer-based language models specifically tailored for Brazilian Portuguese. We introduce an adapted BERT model, BERTptCS, which incorporates the CS framework for improved prediction of communication cards. The primary aim is to enhance the accuracy and contextual relevance of communication card predictions, which are essential in AAC systems for individuals with complex communication needs (CCN). We compared BERTptCS with a baseline model, BERTptAAC, which lacks CS integration. Our results demonstrate that BERTptCS significantly outperforms BERTptAAC in various metrics, including top-k accuracy, Mean Reciprocal Rank (MRR), and Entropy@K. Integrating CS into the language model improves prediction accuracy and offers a more intuitive and contextual understanding of user inputs, facilitating more effective communication.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15902",
        "abstract url": "https://arxiv.org/abs/2405.15902",
        "title": "Hacc-Man: An Arcade Game for Jailbreaking LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The recent leaps in complexity and fluency of Large Language Models (LLMs) mean that, for the first time in human history, people can interact with computers using natural language alone. This creates monumental possibilities of automation and accessibility of computing, but also raises severe security and safety threats: When everyone can interact with LLMs, everyone can potentially break into the systems running LLMs. All it takes is creative use of language. This paper presents Hacc-Man, a game which challenges its players to \"jailbreak\" an LLM: subvert the LLM to output something that it is not intended to. Jailbreaking is at the intersection between creative problem solving and LLM security. The purpose of the game is threefold: 1. To heighten awareness of the risks of deploying fragile LLMs in everyday systems, 2. To heighten people's self-efficacy in interacting with LLMs, and 3. To discover the creative problem solving strategies, people deploy in this novel context.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15924",
        "abstract url": "https://arxiv.org/abs/2405.15924",
        "title": "SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The long-standing one-to-many problem of gold standard responses in open-domain dialogue systems presents challenges for automatic evaluation metrics. Though prior works have demonstrated some success by applying powerful Large Language Models (LLMs), existing approaches still struggle with the one-to-many problem, and exhibit subpar performance in domain-specific scenarios. We assume the commonsense reasoning biases within LLMs may hinder their performance in domainspecific evaluations. To address both issues, we propose a novel framework SLIDE (Small and Large Integrated for Dialogue Evaluation), that leverages both a small, specialised model (SLM), and LLMs for the evaluation of open domain dialogues. Our approach introduces several techniques: (1) Contrastive learning to differentiate between robust and non-robust response embeddings; (2) A novel metric for semantic sensitivity that combines embedding cosine distances with similarity learned through neural networks, and (3) a strategy for incorporating the evaluation results from both the SLM and LLMs. Our empirical results demonstrate that our approach achieves state-of-the-art performance in both the classification and evaluation tasks, and additionally the SLIDE evaluator exhibits better correlation with human judgements. Our code is available at https:// github.com/hegehongcha/SLIDE-ACL2024.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by ACL2024 Findings"
    },
    {
        "paper id": "2405.15930",
        "abstract url": "https://arxiv.org/abs/2405.15930",
        "title": "ArguSense: Argument-Centric Analysis of Online Discourse",
        "rating": "1",
        "keywords": [
            [
                "cs.SI"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "How can we model arguments and their dynamics in online forum discussions? The meteoric rise of online forums presents researchers across different disciplines with an unprecedented opportunity: we have access to texts containing discourse between groups of users generated in a voluntary and organic fashion. Most prior work so far has focused on classifying individual monological comments as either argumentative or not argumentative. However, few efforts quantify and describe the dialogical processes between users found in online forum discourse: the structure and content of interpersonal argumentation. Modeling dialogical discourse requires the ability to identify the presence of arguments, group them into clusters, and summarize the content and nature of clusters of arguments within a discussion thread in the forum. In this work, we develop ArguSense, a comprehensive and systematic framework for understanding arguments and debate in online forums. Our framework consists of methods for, among other things: (a) detecting argument topics in an unsupervised manner; (b) describing the structure of arguments within threads with powerful visualizations; and (c) quantifying the content and diversity of threads using argument similarity and clustering algorithms. We showcase our approach by analyzing the discussions of four communities on the Reddit platform over a span of 21 months. Specifically, we analyze the structure and content of threads related to GMOs in forums related to agriculture or farming to demonstrate the value of our framework.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "Accepted for publication at the 18th International AAAI Conference on Web and Social Media (ICWSM 2024). Please cite accordingly"
    },
    {
        "paper id": "2405.15932",
        "abstract url": "https://arxiv.org/abs/2405.15932",
        "title": "Steerable Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work we introduce Steerable Transformers, an extension of the Vision Transformer mechanism that maintains equivariance to the special Euclidean group $\\mathrm{SE}(d)$. We propose an equivariant attention mechanism that operates on features extracted by steerable convolutions. Operating in Fourier space, our network utilizes Fourier space non-linearities. Our experiments in both two and three dimensions show that adding a steerable transformer encoder layer to a steerable convolution network enhances performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15936",
        "abstract url": "https://arxiv.org/abs/2405.15936",
        "title": "Zero-Shot Spam Email Classification Using Pre-trained Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This paper investigates the application of pre-trained large language models (LLMs) for spam email classification using zero-shot prompting. We evaluate the performance of both open-source (Flan-T5) and proprietary LLMs (ChatGPT, GPT-4) on the well-known SpamAssassin dataset. Two classification approaches are explored: (1) truncated raw content from email subject and body, and (2) classification based on summaries generated by ChatGPT. Our empirical analysis, leveraging the entire dataset for evaluation without further training, reveals promising results. Flan-T5 achieves a 90% F1-score on the truncated content approach, while GPT-4 reaches a 95% F1-score using summaries. While these initial findings on a single dataset suggest the potential for classification pipelines of LLM-based subtasks (e.g., summarisation and classification), further validation on diverse datasets is necessary. The high operational costs of proprietary models, coupled with the general inference costs of LLMs, could significantly hinder real-world deployment for spam filtering.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15939",
        "abstract url": "https://arxiv.org/abs/2405.15939",
        "title": "Diversifying Human Pose in Synthetic Data for Aerial-view Human Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a framework for diversifying human poses in a synthetic dataset for aerial-view human detection. Our method firstly constructs a set of novel poses using a pose generator and then alters images in the existing synthetic dataset to assume the novel poses while maintaining the original style using an image translator. Since images corresponding to the novel poses are not available in training, the image translator is trained to be applicable only when the input and target poses are similar, thus training does not require the novel poses and their corresponding images. Next, we select a sequence of target novel poses from the novel pose set, using Dijkstra's algorithm to ensure that poses closer to each other are located adjacently in the sequence. Finally, we repeatedly apply the image translator to each target pose in sequence to produce a group of novel pose images representing a variety of different limited body movements from the source pose. Experiments demonstrate that, regardless of how the synthetic data is used for training or the data size, leveraging the pose-diversified synthetic dataset in training generally presents remarkably better accuracy than using the original synthetic dataset on three aerial-view human detection benchmarks (VisDrone, Okutama-Action, and ICG) in the few-shot regime.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15943",
        "abstract url": "https://arxiv.org/abs/2405.15943",
        "title": "Transformers represent belief state geometry in their residual stream",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on. Our work provides a framework connecting the structure of training data to the computational structure and representations that transformers use to carry out their behavior.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15953",
        "abstract url": "https://arxiv.org/abs/2405.15953",
        "title": "Activator: GLU Activations as The Core Functions of a Vision Transformer",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transformer architecture currently represents the main driver behind many successes in a variety of tasks addressed by deep learning, especially the recent advances in natural language processing (NLP) culminating with large language models (LLM). In addition, transformer architecture has found a wide spread of interest from computer vision (CV) researchers and practitioners, allowing for many advancements in vision-related tasks and opening the door for multi-task and multi-modal deep learning architectures that share the same principle of operation. One drawback to these architectures is their reliance on the scaled dot product attention mechanism with the softmax activation function, which is computationally expensive and requires large compute capabilities both for training and inference. This paper investigates substituting the attention mechanism usually adopted for transformer architecture with an architecture incorporating gated linear unit (GLU) activation within a multi-layer perceptron (MLP) structure in conjunction with the default MLP incorporated in the traditional transformer design. Another step forward taken by this paper is to eliminate the second non-gated MLP to further reduce the computational cost. Experimental assessments conducted by this research show that both proposed modifications and reductions offer competitive performance in relation to baseline architectures, in support of the aims of this work in establishing a more efficient yet capable alternative to the traditional attention mechanism as the core component in designing transformer architectures.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2403.02411"
    },
    {
        "paper id": "2405.15962",
        "abstract url": "https://arxiv.org/abs/2405.15962",
        "title": "Wearable-based behaviour interpolation for semi-supervised human activity recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "While traditional feature engineering for Human Activity Recognition (HAR) involves a trial-anderror process, deep learning has emerged as a preferred method for high-level representations of sensor-based human activities. However, most deep learning-based HAR requires a large amount of labelled data and extracting HAR features from unlabelled data for effective deep learning training remains challenging. We, therefore, introduce a deep semi-supervised HAR approach, MixHAR, which concurrently uses labelled and unlabelled activities. Our MixHAR employs a linear interpolation mechanism to blend labelled and unlabelled activities while addressing both inter- and intra-activity variability. A unique challenge identified is the activityintrusion problem during mixing, for which we propose a mixing calibration mechanism to mitigate it in the feature embedding space. Additionally, we rigorously explored and evaluated the five conventional/popular deep semi-supervised technologies on HAR, acting as the benchmark of deep semi-supervised HAR. Our results demonstrate that MixHAR significantly improves performance, underscoring the potential of deep semi-supervised techniques in HAR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15964",
        "abstract url": "https://arxiv.org/abs/2405.15964",
        "title": "A hierarchical Bayesian model for syntactic priming",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The effect of syntactic priming exhibits three well-documented empirical properties: the lexical boost, the inverse frequency effect, and the asymmetrical decay. We aim to show how these three empirical phenomena can be reconciled in a general learning framework, the hierarchical Bayesian model (HBM). The model represents syntactic knowledge in a hierarchical structure of syntactic statistics, where a lower level represents the verb-specific biases of syntactic decisions, and a higher level represents the abstract bias as an aggregation of verb-specific biases. This knowledge is updated in response to experience by Bayesian inference. In simulations, we show that the HBM captures the above-mentioned properties of syntactic priming. The results indicate that some properties of priming which are usually explained by a residual activation account can also be explained by an implicit learning account. We also discuss the model's implications for the lexical basis of syntactic priming.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "6 pages; accepted to CogSci 2024"
    },
    {
        "paper id": "2405.15971",
        "abstract url": "https://arxiv.org/abs/2405.15971",
        "title": "Robust width: A lightweight and certifiable adversarial defense",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Deep neural networks are vulnerable to so-called adversarial examples: inputs which are intentionally constructed to cause the model to make incorrect predictions or classifications. Adversarial examples are often visually indistinguishable from natural data samples, making them hard to detect. As such, they pose significant threats to the reliability of deep learning systems. In this work, we study an adversarial defense based on the robust width property (RWP), which was recently introduced for compressed sensing. We show that a specific input purification scheme based on the RWP gives theoretical robustness guarantees for images that are approximately sparse. The defense is easy to implement and can be applied to any existing model without additional training or finetuning. We empirically validate the defense on ImageNet against $L^\\infty$ perturbations at perturbation budgets ranging from $4/255$ to $32/255$. In the black-box setting, our method significantly outperforms the state-of-the-art, especially for large perturbations. In the white-box setting, depending on the choice of base classifier, we closely match the state of the art in robust ImageNet classification while avoiding the need for additional data, larger models or expensive adversarial training routines. Our code is available at https://github.com/peck94/robust-width-defense.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15976",
        "abstract url": "https://arxiv.org/abs/2405.15976",
        "title": "Understanding the Impact of Training Set Size on Animal Re-identification",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in the automatic re-identification of animal individuals from images have opened up new possibilities for studying wildlife through camera traps and citizen science projects. Existing methods leverage distinct and permanent visual body markings, such as fur patterns or scars, and typically employ one of two strategies: local features or end-to-end learning. In this study, we delve into the impact of training set size by conducting comprehensive experiments across six different methods and five animal species. While it is well known that end-to-end learning-based methods surpass local feature-based methods given a sufficient amount of good-quality training data, the challenge of gathering such datasets for wildlife animals means that local feature-based methods remain a more practical approach for many species. We demonstrate the benefits of both local feature and end-to-end learning-based approaches and show that species-specific characteristics, particularly intra-individual variance, have a notable effect on training data requirements.",
        "subjects": [
            "cs.CV",
            "q-bio.PE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15995",
        "abstract url": "https://arxiv.org/abs/2405.15995",
        "title": "Efficient Temporal Action Segmentation via Boundary-aware Query Voting",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although the performance of Temporal Action Segmentation (TAS) has improved in recent years, achieving promising results often comes with a high computational cost due to dense inputs, complex model structures, and resource-intensive post-processing requirements. To improve the efficiency while keeping the performance, we present a novel perspective centered on per-segment classification. By harnessing the capabilities of Transformers, we tokenize each video segment as an instance token, endowed with intrinsic instance segmentation. To realize efficient action segmentation, we introduce BaFormer, a boundary-aware Transformer network. It employs instance queries for instance segmentation and a global query for class-agnostic boundary prediction, yielding continuous segment proposals. During inference, BaFormer employs a simple yet effective voting strategy to classify boundary-wise segments based on instance segmentation. Remarkably, as a single-stage approach, BaFormer significantly reduces the computational costs, utilizing only 6% of the running time compared to state-of-the-art method DiffAct, while producing better or comparable accuracy over several popular benchmarks. The code for this project is publicly available at https://github.com/peiyao-w/BaFormer.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "17 pages, 8 figures, 11 tables"
    },
    {
        "paper id": "2405.16000",
        "abstract url": "https://arxiv.org/abs/2405.16000",
        "title": "Carnatic Raga Identification System using Rigorous Time-Delay Neural Network",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Large scale machine learning-based Raga identification continues to be a nontrivial issue in the computational aspects behind Carnatic music. Each raga consists of many unique and intrinsic melodic patterns that can be used to easily identify them from others. These ragas can also then be used to cluster songs within the same raga, as well as identify songs in other closely related ragas. In this case, the input sound is analyzed using a combination of steps including using a Discrete Fourier transformation and using Triangular Filtering to create custom bins of possible notes, extracting features from the presence of particular notes or lack thereof. Using a combination of Neural Networks including 1D Convolutional Neural Networks conventionally known as Time-Delay Neural Networks) and Long Short-Term Memory (LSTM), which are a form of Recurrent Neural Networks, the backbone of the classification strategy to build the model can be created. In addition, to help with variations in shruti, a long-time attention-based mechanism will be implemented to determine the relative changes in frequency rather than the absolute differences. This will provide a much more meaningful data point when training audio clips in different shrutis. To evaluate the accuracy of the classifier, a dataset of 676 recordings is used. The songs are distributed across the list of ragas. The goal of this program is to be able to effectively and efficiently label a much wider range of audio clips in more shrutis, ragas, and with more background noise.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "7 pages, 2 tables, 3 figures"
    },
    {
        "paper id": "2405.16030",
        "abstract url": "https://arxiv.org/abs/2405.16030",
        "title": "Constrained Ensemble Exploration for Unsupervised Skill Discovery",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Unsupervised Reinforcement Learning (RL) provides a promising paradigm for learning useful behaviors via reward-free per-training. Existing methods for unsupervised RL mainly conduct empowerment-driven skill discovery or entropy-based exploration. However, empowerment often leads to static skills, and pure exploration only maximizes the state coverage rather than learning useful behaviors. In this paper, we propose a novel unsupervised RL framework via an ensemble of skills, where each skill performs partition exploration based on the state prototypes. Thus, each skill can explore the clustered area locally, and the ensemble skills maximize the overall state coverage. We adopt state-distribution constraints for the skill occupancy and the desired cluster for learning distinguishable skills. Theoretical analysis is provided for the state entropy and the resulting skill distributions. Based on extensive experiments on several challenging tasks, we find our method learns well-explored ensemble skills and achieves superior performance in various downstream tasks compared to previous methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by ICML 2024"
    },
    {
        "paper id": "2405.16036",
        "abstract url": "https://arxiv.org/abs/2405.16036",
        "title": "Certifying Adapters: Enabling and Enhancing the Certification of Classifier Adversarial Robustness",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Randomized smoothing has become a leading method for achieving certified robustness in deep classifiers against l_{p}-norm adversarial perturbations. Current approaches for achieving certified robustness, such as data augmentation with Gaussian noise and adversarial training, require expensive training procedures that tune large models for different Gaussian noise levels and thus cannot leverage high-performance pre-trained neural networks. In this work, we introduce a novel certifying adapters framework (CAF) that enables and enhances the certification of classifier adversarial robustness. Our approach makes few assumptions about the underlying training algorithm or feature extractor and is thus broadly applicable to different feature extractor architectures (e.g., convolutional neural networks or vision transformers) and smoothing algorithms. We show that CAF (a) enables certification in uncertified models pre-trained on clean datasets and (b) substantially improves the performance of certified classifiers via randomized smoothing and SmoothAdv at multiple radii in CIFAR-10 and ImageNet. We demonstrate that CAF achieves improved certified accuracies when compared to methods based on random or denoised smoothing, and that CAF is insensitive to certifying adapter hyperparameters. Finally, we show that an ensemble of adapters enables a single pre-trained feature extractor to defend against a range of noise perturbation scales.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16042",
        "abstract url": "https://arxiv.org/abs/2405.16042",
        "title": "Incremental Comprehension of Garden-Path Sentences by Large Language Models: Semantic Interpretation, Syntactic Re-Analysis, and Attention",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "When reading temporarily ambiguous garden-path sentences, misinterpretations sometimes linger past the point of disambiguation. This phenomenon has traditionally been studied in psycholinguistic experiments using online measures such as reading times and offline measures such as comprehension questions. Here, we investigate the processing of garden-path sentences and the fate of lingering misinterpretations using four large language models (LLMs): GPT-2, LLaMA-2, Flan-T5, and RoBERTa. The overall goal is to evaluate whether humans and LLMs are aligned in their processing of garden-path sentences and in the lingering misinterpretations past the point of disambiguation, especially when extra-syntactic information (e.g., a comma delimiting a clause boundary) is present to guide processing. We address this goal using 24 garden-path sentences that have optional transitive and reflexive verbs leading to temporary ambiguities. For each sentence, there are a pair of comprehension questions corresponding to the misinterpretation and the correct interpretation. In three experiments, we (1) measure the dynamic semantic interpretations of LLMs using the question-answering task; (2) track whether these models shift their implicit parse tree at the point of disambiguation (or by the end of the sentence); and (3) visualize the model components that attend to disambiguating information when processing the question probes. These experiments show promising alignment between humans and LLMs in the processing of garden-path sentences, especially when extra-syntactic information is available to guide processing.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by CogSci-24"
    },
    {
        "paper id": "2405.16043",
        "abstract url": "https://arxiv.org/abs/2405.16043",
        "title": "Theoretical Analysis of Weak-to-Strong Generalization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model's errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call pseudolabel correction and coverage expansion, respectively. We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "comment": "36 pages, 3 figures"
    },
    {
        "paper id": "2405.17476",
        "abstract url": "https://arxiv.org/abs/2405.17476",
        "title": "How to Leverage Diverse Demonstrations in Offline Imitation Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Offline Imitation Learning (IL) with imperfect demonstrations has garnered increasing attention owing to the scarcity of expert data in many real-world domains. A fundamental problem in this scenario is how to extract positive behaviors from noisy data. In general, current approaches to the problem select data building on state-action similarity to given expert demonstrations, neglecting precious information in (potentially abundant) $\\textit{diverse}$ state-actions that deviate from expert ones. In this paper, we introduce a simple yet effective data selection method that identifies positive behaviors based on their resultant states -- a more informative criterion enabling explicit utilization of dynamics information and effective extraction of both expert and beneficial diverse behaviors. Further, we devise a lightweight behavior cloning algorithm capable of leveraging the expert and selected data correctly. In the experiments, we evaluate our method on a suite of complex and high-dimensional offline IL benchmarks, including continuous-control and vision-based tasks. The results demonstrate that our method achieves state-of-the-art performance, outperforming existing methods on $\\textbf{20/21}$ benchmarks, typically by $\\textbf{2-5x}$, while maintaining a comparable runtime to Behavior Cloning ($\\texttt{BC}$).",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "International Conference on Machine Learning (ICML)"
    },
    {
        "paper id": "2405.17484",
        "abstract url": "https://arxiv.org/abs/2405.17484",
        "title": "Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "While following different technical routes, both low-rank and orthogonal adaptation techniques can efficiently adapt large-scale pre-training models in specific tasks or domains based on a small piece of trainable parameters. In this study, we bridge the gap between these two techniques, proposing a simple but effective adaptation method based on Householder reflections. Given a pre-trained model, our method fine-tunes its layers by multiplying each frozen weight matrix with an orthogonal matrix constructed by a chain of learnable Householder reflections (HRs). This HR-based orthogonal fine-tuning is equivalent to an adaptive low-rank adaptation. Moreover, we show that the orthogonality of the reflection planes corresponding to the HRs impacts the model capacity and regularity. The analysis motivates us to regularize the orthogonality of the HRs, leading to different implementations of the proposed Householder reflection adaptation (HRA) method. Compared with state-of-the-art methods, HRA achieves superior performance with fewer learnable parameters when adapting large language models and conditional image generators. The code is available at https://github.com/DaShenZi721/HRA",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15230",
        "abstract url": "https://arxiv.org/abs/2405.15230",
        "title": "$i$REPO: $i$mplicit Reward Pairwise Difference based Empirical Preference Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "While astonishingly capable, large Language Models (LLM) can sometimes produce outputs that deviate from human expectations. Such deviations necessitate an alignment phase to prevent disseminating untruthful, toxic, or biased information. Traditional alignment methods based on reinforcement learning often struggle with the identified instability, whereas preference optimization methods are limited by their overfitting to pre-collected hard-label datasets. In this paper, we propose a novel LLM alignment framework named $i$REPO, which utilizes implicit Reward pairwise difference regression for Empirical Preference Optimization. Particularly, $i$REPO employs self-generated datasets labelled by empirical human (or AI annotator) preference to iteratively refine the aligned policy through a novel regression-based loss function. Furthermore, we introduce an innovative algorithm backed by theoretical guarantees for achieving optimal results under ideal assumptions and providing a practical performance-gap result without such assumptions. Experimental results with Phi-2 and Mistral-7B demonstrate that $i$REPO effectively achieves self-alignment using soft-label, self-generated responses and the logit of empirical AI annotators. Furthermore, our approach surpasses preference optimization baselines in evaluations using the Language Model Evaluation Harness and Multi-turn benchmarks.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2405.15251",
        "abstract url": "https://arxiv.org/abs/2405.15251",
        "title": "Learning to optimize: A tutorial for continuous and mixed-integer optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning to Optimize (L2O) stands at the intersection of traditional optimization and machine learning, utilizing the capabilities of machine learning to enhance conventional optimization techniques. As real-world optimization problems frequently share common structures, L2O provides a tool to exploit these structures for better or faster solutions. This tutorial dives deep into L2O techniques, introducing how to accelerate optimization algorithms, promptly estimate the solutions, or even reshape the optimization problem itself, making it more adaptive to real-world applications. By considering the prerequisites for successful applications of L2O and the structure of the optimization problems at hand, this tutorial provides a comprehensive guide for practitioners and researchers alike.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15268",
        "abstract url": "https://arxiv.org/abs/2405.15268",
        "title": "ParamReL: Learning Parameter Space Representation via Progressively Encoding Bayesian Flow Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The recently proposed Bayesian Flow Networks~(BFNs) show great potential in modeling parameter spaces, offering a unified strategy for handling continuous, discretized, and discrete data. However, BFNs cannot learn high-level semantic representation from the parameter space since {common encoders, which encode data into one static representation, cannot capture semantic changes in parameters.} This motivates a new direction: learning semantic representations hidden in the parameter spaces to characterize mixed-typed noisy data. {Accordingly, we propose a representation learning framework named ParamReL, which operates in the parameter space to obtain parameter-wise latent semantics that exhibit progressive structures. Specifically, ParamReL proposes a \\emph{self-}encoder to learn latent semantics directly from parameters, rather than from observations. The encoder is then integrated into BFNs, enabling representation learning with various formats of observations. Mutual information terms further promote the disentanglement of latent semantics and capture meaningful semantics simultaneously.} We illustrate {conditional generation and reconstruction} in ParamReL via expanding BFNs, and extensive {quantitative} experimental results demonstrate the {superior effectiveness} of ParamReL in learning parameter representation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15285",
        "abstract url": "https://arxiv.org/abs/2405.15285",
        "title": "Minimizing UCB: a Better Local Search Strategy in Local Bayesian Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Local Bayesian optimization is a promising practical approach to solve the high dimensional black-box function optimization problem. Among them is the approximated gradient class of methods, which implements a strategy similar to gradient descent. These methods have achieved good experimental results and theoretical guarantees. However, given the distributional properties of the Gaussian processes applied on these methods, there may be potential to further exploit the information of the Gaussian processes to facilitate the BO search. In this work, we develop the relationship between the steps of the gradient descent method and one that minimizes the Upper Confidence Bound (UCB), and show that the latter can be a better strategy than direct gradient descent when a Gaussian process is applied as a surrogate. Through this insight, we propose a new local Bayesian optimization algorithm, MinUCB, which replaces the gradient descent step with minimizing UCB in GIBO. We further show that MinUCB maintains a similar convergence rate with GIBO. We then improve the acquisition function of MinUCB further through a look ahead strategy, and obtain a more efficient algorithm LA-MinUCB. We apply our algorithms on different synthetic and real-world functions, and the results show the effectiveness of our method. Our algorithms also illustrate improvements on local search strategies from an upper bound perspective in Bayesian optimization, and provides a new direction for future algorithm design.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15294",
        "abstract url": "https://arxiv.org/abs/2405.15294",
        "title": "Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We provide a theoretical and computational investigation of the Gamma-Maximin method with soft revision, which was recently proposed as a robust criterion for pseudo-label selection (PLS) in semi-supervised learning. Opposed to traditional methods for PLS we use credal sets of priors (\"generalized Bayes\") to represent the epistemic modeling uncertainty. These latter are then updated by the Gamma-Maximin method with soft revision. We eventually select pseudo-labeled data that are most likely in light of the least favorable distribution from the so updated credal set. We formalize the task of finding optimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revision as an optimization problem. A concrete implementation for the class of logistic models then allows us to compare the predictive power of the method with competing approaches. It is observed that the Gamma-Maximin method with soft revision can achieve very promising results, especially when the proportion of labeled data is low.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG",
            "math.ST",
            "stat.ME"
        ],
        "comment": "Accepted at the 11th International Conference on Soft Methods in Probability and Statistics (SMPS) 2024"
    },
    {
        "paper id": "2405.15312",
        "abstract url": "https://arxiv.org/abs/2405.15312",
        "title": "Resource-Efficient Heartbeat Classification Using Multi-Feature Fusion and Bidirectional LSTM",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this article, we present a resource-efficient approach for electrocardiogram (ECG) based heartbeat classification using multi-feature fusion and bidirectional long short-term memory (Bi-LSTM). The dataset comprises five original classes from the MIT-BIH Arrhythmia Database: Normal (N), Left Bundle Branch Block (LBBB), Right Bundle Branch Block (RBBB), Premature Ventricular Contraction (PVC), and Paced Beat (PB). Preprocessing methods including the discrete wavelet transform and dual moving average windows are used to reduce noise and artifacts in the raw ECG signal, and extract the main points (PQRST) of the ECG waveform. Multi-feature fusion is achieved by utilizing time intervals and the proposed under-the-curve areas, which are inherently robust against noise, as input features. Simulations demonstrated that incorporating under-the-curve area features improved the classification accuracy for the challenging RBBB and LBBB classes from 31.4\\% to 84.3\\% for RBBB, and from 69.6\\% to 87.0\\% for LBBB. Using a Bi-LSTM network, rather than a conventional LSTM network, resulted in higher accuracy (33.8\\% vs 21.8\\%) with a 28\\% reduction in required network parameters for the RBBB class. Multiple neural network models with varying parameter sizes, including tiny (84k), small (150k), medium (478k), and large (1.25M) models, are developed to achieve high accuracy \\textit{across all classes}, a more crucial and challenging goal than overall classification accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15314",
        "abstract url": "https://arxiv.org/abs/2405.15314",
        "title": "Output-Constrained Decision Trees",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "When there is a correlation between any pair of targets, one needs a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in various applications. This paper introduces new variants of decision trees that can handle not only multi-target output but also the constraints among the targets. We focus on the customization of conventional decision trees by adjusting the splitting criteria to handle the constraints and obtain feasible predictions. We present both an optimization-based exact approach and several heuristics, complete with a discussion on their respective advantages and disadvantages. To support our findings, we conduct a computational study to demonstrate and compare the results of the proposed approaches.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 6 figures"
    },
    {
        "paper id": "2405.15332",
        "abstract url": "https://arxiv.org/abs/2405.15332",
        "title": "Cross-Validated Off-Policy Evaluation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we study the problem of estimator selection and hyper-parameter tuning in off-policy evaluation. Although cross-validation is the most popular method for model selection in supervised learning, off-policy evaluation relies mostly on theory-based approaches, which provide only limited guidance to practitioners. We show how to use cross-validation for off-policy evaluation. This challenges a popular belief that cross-validation in off-policy evaluation is not feasible. We evaluate our method empirically and show that it addresses a variety of use cases.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15337",
        "abstract url": "https://arxiv.org/abs/2405.15337",
        "title": "Discriminative Estimation of Total Variation Distance: A Fidelity Auditor for Generative Data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the proliferation of generative AI and the increasing volume of generative data (also called as synthetic data), assessing the fidelity of generative data has become a critical concern. In this paper, we propose a discriminative approach to estimate the total variation (TV) distance between two distributions as an effective measure of generative data fidelity. Our method quantitatively characterizes the relation between the Bayes risk in classifying two distributions and their TV distance. Therefore, the estimation of total variation distance reduces to that of the Bayes risk. In particular, this paper establishes theoretical results regarding the convergence rate of the estimation error of TV distance between two Gaussian distributions. We demonstrate that, with a specific choice of hypothesis class in classification, a fast convergence rate in estimating the TV distance can be achieved. Specifically, the estimation accuracy of the TV distance is proven to inherently depend on the separation of two Gaussian distributions: smaller estimation errors are achieved when the two Gaussian distributions are farther apart. This phenomenon is also validated empirically through extensive simulations. In the end, we apply this discriminative estimation method to rank fidelity of synthetic image data using the MNIST dataset.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15357",
        "abstract url": "https://arxiv.org/abs/2405.15357",
        "title": "Strong screening rules for group-based SLOPE models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters. Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting. We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE. The developed rules are applicable for the wider family of group-based OWL models, including OSCAR. Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process. The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.ME"
        ],
        "comment": "31 pages, 18 figures, 10 tables"
    },
    {
        "paper id": "2405.15379",
        "abstract url": "https://arxiv.org/abs/2405.15379",
        "title": "Log-Concave Sampling on Compact Supports: A Versatile Proximal Framework",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we explore sampling from strongly log-concave distributions defined on convex and compact supports. We propose a general proximal framework that involves projecting onto the constrained set, which is highly flexible and supports various projection options. Specifically, we consider the cases of Euclidean and Gauge projections, with the latter having the advantage of being performed efficiently using a membership oracle. This framework can be seamlessly integrated with multiple sampling methods. Our analysis focuses on Langevin-type sampling algorithms within the context of constrained sampling. We provide nonasymptotic upper bounds on the W1 and W2 errors, offering a detailed comparison of the performance of these methods in constrained sampling.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.PR",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15380",
        "abstract url": "https://arxiv.org/abs/2405.15380",
        "title": "Full-stack evaluation of Machine Learning inference workloads for RISC-V systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Architectural simulators hold a vital role in RISC-V research, providing a crucial platform for workload evaluation without the need for costly physical prototypes. They serve as a dynamic environment for exploring innovative architectural concepts, enabling swift iteration and thorough analysis of performance metrics. As deep learning algorithms become increasingly pervasive, it is essential to benchmark new architectures with machine learning workloads. The diverse computational kernels used in deep learning algorithms highlight the necessity for a comprehensive compilation toolchain to map to target hardware platforms. This study evaluates the performance of a wide array of machine learning workloads on RISC-V architectures using gem5, an open-source architectural simulator. Leveraging an open-source compilation toolchain based on Multi-Level Intermediate Representation (MLIR), the research presents benchmarking results specifically focused on deep learning inference workloads. Additionally, the study sheds light on current limitations of gem5 when simulating RISC-V architectures, offering insights for future development and refinement.",
        "subjects": [
            "cs.AR",
            "cs.AI"
        ],
        "comment": "RISC-V Summit Europe 2024"
    },
    {
        "paper id": "2405.15393",
        "abstract url": "https://arxiv.org/abs/2405.15393",
        "title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hyperparameter optimization is crucial for obtaining peak performance of machine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide optimization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross-validation scheme, are often recommended. We show that, surprisingly, reshuffling the splits for every configuration often improves the final model's generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simulation study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "39 pages, 4 tables, 29 figures"
    },
    {
        "paper id": "2405.15421",
        "abstract url": "https://arxiv.org/abs/2405.15421",
        "title": "Model-free reinforcement learning with noisy actions for automated experimental control in optics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Experimental control involves a lot of manual effort with non-trivial decisions for precise adjustments. Here, we study the automatic experimental alignment for coupling laser light into an optical fiber using reinforcement learning (RL). We face several real-world challenges, such as time-consuming training, partial observability, and noisy actions due to imprecision in the mirror steering motors. We show that we can overcome these challenges: To save time, we use a virtual testbed to tune our environment for dealing with partial observability and use relatively sample-efficient model-free RL algorithms like Soft Actor-Critic (SAC) or Truncated Quantile Critics (TQC). Furthermore, by fully training on the experiment, the agent learns directly to handle the noise present. In our extensive experimentation, we show that we are able to achieve 90% coupling, showcasing the effectiveness of our proposed approaches. We reach this efficiency, which is comparable to that of a human expert, without additional feedback loops despite the motors' inaccuracies. Our result is an example of the readiness of RL for real-world tasks. We consider RL a promising tool for reducing the workload in labs.",
        "subjects": [
            "cs.LG",
            "physics.optics"
        ],
        "comment": "10 pages + 10 pages appendices, 3 + 11 figures"
    },
    {
        "paper id": "2405.15424",
        "abstract url": "https://arxiv.org/abs/2405.15424",
        "title": "Smoothed Online Classification can be Harder than Batch Classification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study online classification under smoothed adversaries. In this setting, at each time point, the adversary draws an example from a distribution that has a bounded density with respect to a fixed base measure, which is known apriori to the learner. For binary classification and scalar-valued regression, previous works \\citep{haghtalab2020smoothed, block2022smoothed} have shown that smoothed online learning is as easy as learning in the iid batch setting under PAC model. However, we show that smoothed online classification can be harder than the iid batch classification when the label space is unbounded. In particular, we construct a hypothesis class that is learnable in the iid batch setting under the PAC model but is not learnable under the smoothed online model. Finally, we identify a condition that ensures that the PAC learnability of a hypothesis class is sufficient for its smoothed online learnability.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2405.15430",
        "abstract url": "https://arxiv.org/abs/2405.15430",
        "title": "Counterexample-Guided Repair of Reinforcement Learning Systems Using Safety Critics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation.",
        "subjects": [
            "cs.LG",
            "cs.LO"
        ],
        "comment": "7 pages + references"
    },
    {
        "paper id": "2405.15431",
        "abstract url": "https://arxiv.org/abs/2405.15431",
        "title": "Counterfactual Explanations for Linear Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The concept of counterfactual explanations (CE) has emerged as one of the important concepts to understand the inner workings of complex AI systems. In this paper, we translate the idea of CEs to linear optimization and propose, motivate, and analyze three different types of CEs: strong, weak, and relative. While deriving strong and weak CEs appears to be computationally intractable, we show that calculating relative CEs can be done efficiently. By detecting and exploiting the hidden convex structure of the optimization problem that arises in the latter case, we show that obtaining relative CEs can be done in the same magnitude of time as solving the original linear optimization problem. This is confirmed by an extensive numerical experiment study on the NETLIB library.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15437",
        "abstract url": "https://arxiv.org/abs/2405.15437",
        "title": "Learning about Data, Algorithms, and Algorithmic Justice on TikTok in Personally Meaningful Ways",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "TikTok, a popular short video sharing application, emerged as the dominant social media platform for young people, with a pronounced influence on how young women and people of color interact online. The application has become a global space for youth to connect with each other, offering not only entertainment but also opportunities to engage with artificial intelligence/machine learning (AI/ML)-driven recommendations and create content using AI/M-powered tools, such as generative AI filters. This provides opportunities for youth to explore and question the inner workings of these systems, their implications, and even use them to advocate for causes they are passionate about. We present different perspectives on how youth may learn in personally meaningful ways when engaging with TikTok. We discuss how youth investigate how TikTok works (considering data and algorithms), take into account issues of ethics and algorithmic justice and use their understanding of the platform to advocate for change.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15441",
        "abstract url": "https://arxiv.org/abs/2405.15441",
        "title": "Statistical and Computational Guarantees of Kernel Max-Sliced Wasserstein Distances",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Optimal transport has been very successful for various machine learning tasks; however, it is known to suffer from the curse of dimensionality. Hence, dimensionality reduction is desirable when applied to high-dimensional data with low-dimensional structures. The kernel max-sliced (KMS) Wasserstein distance is developed for this purpose by finding an optimal nonlinear mapping that reduces data into $1$ dimensions before computing the Wasserstein distance. However, its theoretical properties have not yet been fully developed. In this paper, we provide sharp finite-sample guarantees under milder technical assumptions compared with state-of-the-art for the KMS $p$-Wasserstein distance between two empirical distributions with $n$ samples for general $p\\in[1,\\infty)$. Algorithm-wise, we show that computing the KMS $2$-Wasserstein distance is NP-hard, and then we further propose a semidefinite relaxation (SDR) formulation (which can be solved efficiently in polynomial time) and provide a relaxation gap for the SDP solution. We provide numerical examples to demonstrate the good performance of our scheme for high-dimensional two-sample testing.",
        "subjects": [
            "stat.ML",
            "cs.CC",
            "cs.LG"
        ],
        "comment": "34 pages, 7 figures, 4 tables"
    },
    {
        "paper id": "2405.15443",
        "abstract url": "https://arxiv.org/abs/2405.15443",
        "title": "Fairness-Accuracy Trade-Offs: A Causal Perspective",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Systems based on machine learning may exhibit discriminatory behavior based on sensitive characteristics such as gender, sex, religion, or race. In light of this, various notions of fairness and methods to quantify discrimination were proposed, leading to the development of numerous approaches for constructing fair predictors. At the same time, imposing fairness constraints may decrease the utility of the decision-maker, highlighting a tension between fairness and utility. This tension is also recognized in legal frameworks, for instance in the disparate impact doctrine of Title VII of the Civil Rights Act of 1964 -- in which specific attention is given to considerations of business necessity -- possibly allowing the usage of proxy variables associated with the sensitive attribute in case a high-enough utility cannot be achieved without them. In this work, we analyze the tension between fairness and accuracy from a causal lens for the first time. We introduce the notion of a path-specific excess loss (PSEL) that captures how much the predictor's loss increases when a causal fairness constraint is enforced. We then show that the total excess loss (TEL), defined as the difference between the loss of predictor fair along all causal pathways vs. an unconstrained predictor, can be decomposed into a sum of more local PSELs. At the same time, enforcing a causal constraint often reduces the disparity between demographic groups. Thus, we introduce a quantity that summarizes the fairness-utility trade-off, called the causal fairness/utility ratio, defined as the ratio of the reduction in discrimination vs. the excess loss from constraining a causal pathway. This quantity is suitable for comparing the fairness-utility trade-off across causal pathways. Finally, as our approach requires causally-constrained fair predictors, we introduce a new neural approach for causally-constrained fair learning.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15444",
        "abstract url": "https://arxiv.org/abs/2405.15444",
        "title": "HyperInterval: Hypernetwork approach to training weight interval regions in continual learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recently, a new Continual Learning (CL) paradigm was presented to control catastrophic forgetting, called Interval Continual Learning (InterContiNet), which relies on enforcing interval constraints on the neural network parameter space. Unfortunately, InterContiNet training is challenging due to the high dimensionality of the weight space, making intervals difficult to manage. To address this issue, we introduce HyperInterval, a technique that employs interval arithmetic within the embedding space and utilizes a hypernetwork to map these intervals to the target network parameter space. We train interval embeddings for consecutive tasks and train a hypernetwork to transform these embeddings into weights of the target network. An embedding for a given task is trained along with the hypernetwork, preserving the response of the target network for the previous task embeddings. Interval arithmetic works with a more manageable, lower-dimensional embedding space rather than directly preparing intervals in a high-dimensional weight space. Our model allows faster and more efficient training. Furthermore, HyperInterval maintains the guarantee of not forgetting. At the end of training, we can choose one universal embedding to produce a single network dedicated to all tasks. In such a framework, hypernetwork is used only for training and can be seen as a meta-trainer. HyperInterval obtains significantly better results than InterContiNet and gives SOTA results on several benchmarks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15446",
        "abstract url": "https://arxiv.org/abs/2405.15446",
        "title": "Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Investigating fairness and equity of automated systems has become a critical field of inquiry. Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline. For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score $S$ used for binary classification, they need not be satisfied after an application of a simple thresholding operation on $S$ (as commonly used in practice). In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness. We introduce the notion of a margin complement, which measures how much a prediction score $S$ changes due to a thresholding operation. We then demonstrate that the marginal difference in the optimal 0/1 predictor $\\widehat Y$ between groups, written $P(\\hat y \\mid x_1) - P(\\hat y \\mid x_0)$, can be causally decomposed into the influences of $X$ on the $L_2$-optimal prediction score $S$ and the influences of $X$ on the margin complement $M$, along different causal pathways (direct, indirect, spurious). We then show that under suitable causal assumptions, the influences of $X$ on the prediction score $S$ are equal to the influences of $X$ on the true outcome $Y$. This yields a new decomposition of the disparity in the predictor $\\widehat Y$ that allows us to disentangle causal differences inherited from the true outcome $Y$ that exists in the real world vs. those coming from the optimization procedure itself. This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of weak and strong business necessity, together with an algorithm for assessing whether these notions are satisfied.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15459",
        "abstract url": "https://arxiv.org/abs/2405.15459",
        "title": "Repetita Iuvant: Data Repetition Allows SGD to Learn High-Dimensional Multi-Index Functions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural networks can identify low-dimensional relevant structures within high-dimensional noisy data, yet our mathematical understanding of how they do so remains scarce. Here, we investigate the training dynamics of two-layer shallow neural networks trained with gradient-based algorithms, and discuss how they learn pertinent features in multi-index models, that is target functions with low-dimensional relevant directions. In the high-dimensional regime, where the input dimension $d$ diverges, we show that a simple modification of the idealized single-pass gradient descent training scenario, where data can now be repeated or iterated upon twice, drastically improves its computational efficiency. In particular, it surpasses the limitations previously believed to be dictated by the Information and Leap exponents associated with the target function to be learned. Our results highlight the ability of networks to learn relevant structures from data alone without any pre-processing. More precisely, we show that (almost) all directions are learned with at most $O(d \\log d)$ steps. Among the exceptions is a set of hard functions that includes sparse parities. In the presence of coupling between directions, however, these can be learned sequentially through a hierarchical mechanism that generalizes the notion of staircase functions. Our results are proven by a rigorous study of the evolution of the relevant statistics for high-dimensional dynamics.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15480",
        "abstract url": "https://arxiv.org/abs/2405.15480",
        "title": "Fundamental limits of weak learnability in high-dimensional multi-index models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multi-index models -- functions which only depend on the covariates through a non-linear transformation of their projection on a subspace -- are a useful benchmark for investigating feature learning with neural networks. This paper examines the theoretical boundaries of learnability in this hypothesis class, focusing particularly on the minimum sample complexity required for weakly recovering their low-dimensional structure with first-order iterative algorithms, in the high-dimensional regime where the number of samples is $n=\u03b1d$ is proportional to the covariate dimension $d$. Our findings unfold in three parts: (i) first, we identify under which conditions a \\textit{trivial subspace} can be learned with a single step of a first-order algorithm for any $\u03b1\\!>\\!0$; (ii) second, in the case where the trivial subspace is empty, we provide necessary and sufficient conditions for the existence of an {\\it easy subspace} consisting of directions that can be learned only above a certain sample complexity $\u03b1\\!>\\!\u03b1_c$. The critical threshold $\u03b1_{c}$ marks the presence of a computational phase transition, in the sense that no efficient iterative algorithm can succeed for $\u03b1\\!<\\!\u03b1_c$. In a limited but interesting set of really hard directions -- akin to the parity problem -- $\u03b1_c$ is found to diverge. Finally, (iii) we demonstrate that interactions between different directions can result in an intricate hierarchical learning phenomenon, where some directions can be learned sequentially when coupled to easier ones. Our analytical approach is built on the optimality of approximate message-passing algorithms among first-order iterative methods, delineating the fundamental learnability limit across a broad spectrum of algorithms, including neural networks trained with gradient descent.",
        "subjects": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15481",
        "abstract url": "https://arxiv.org/abs/2405.15481",
        "title": "Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The growing computational demands posed by increasingly number of neural network's parameters necessitate low-memory-consumption training approaches. Previous memory reduction techniques, such as Low-Rank Adaptation (LoRA) and ReLoRA, suffer from the limitation of low rank and saddle point issues, particularly during intensive tasks like pre-training. In this paper, we propose Sparse Spectral Training (SST), an advanced training methodology that updates all singular values and selectively updates singular vectors of network weights, thereby optimizing resource usage while closely approximating full-rank training. SST refines the training process by employing a targeted updating strategy for singular vectors, which is determined by a multinomial sampling method weighted by the significance of the singular values, ensuring both high performance and memory reduction. Through comprehensive testing on both Euclidean and hyperbolic neural networks across various tasks, including natural language generation, machine translation, node classification and link prediction, SST demonstrates its capability to outperform existing memory reduction training methods and is comparable with full-rank training in some cases. On OPT-125M, with rank equating to 8.3% of embedding dimension, SST reduces the perplexity gap to full-rank training by 67.6%, demonstrating a significant reduction of the performance loss with prevalent low-rank methods. This approach offers a strong alternative to traditional training techniques, paving the way for more efficient and scalable neural network training solutions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15498",
        "abstract url": "https://arxiv.org/abs/2405.15498",
        "title": "Node Accessibility Characterization of Radially-Grown Structures",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Complex systems have motivated continuing interest from the scientific community, leading to new concepts and methods. Growing systems represent a case of particular interest, as their topological, geometrical, and also dynamical properties change along time, as new elements are incorporated into the existing structure. In the present work, an approach is the case in which systems grown radially around some straight axis of reference, such as particle deposition on electrodes, or urban expansion along avenues, roads, coastline, or rivers, among several other possibilities. More specifically, we aim at characterizing the topological properties of simulated growing structures, which are represented as graphs, in terms of a measurement corresponding to the accessibility of each involved node. The incorporation of new elements (nodes and links) is performed preferentially to the angular orientation respectively to the reference axis. Several interesting results are reported, including the tendency of structures grown preferentially to the orientation normal to the axis to have smaller accessibility.",
        "subjects": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": "14 pages, 8 figures"
    },
    {
        "paper id": "2405.15505",
        "abstract url": "https://arxiv.org/abs/2405.15505",
        "title": "Revisiting Counterfactual Regression through the Lens of Gromov-Wasserstein Information Bottleneck",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "As a promising individualized treatment effect (ITE) estimation method, counterfactual regression (CFR) maps individuals' covariates to a latent space and predicts their counterfactual outcomes. However, the selection bias between control and treatment groups often imbalances the two groups' latent distributions and negatively impacts this method's performance. In this study, we revisit counterfactual regression through the lens of information bottleneck and propose a novel learning paradigm called Gromov-Wasserstein information bottleneck (GWIB). In this paradigm, we learn CFR by maximizing the mutual information between covariates' latent representations and outcomes while penalizing the kernelized mutual information between the latent representations and the covariates. We demonstrate that the upper bound of the penalty term can be implemented as a new regularizer consisting of $i)$ the fused Gromov-Wasserstein distance between the latent representations of different groups and $ii)$ the gap between the transport cost generated by the model and the cross-group Gromov-Wasserstein distance between the latent representations and the covariates. GWIB effectively learns the CFR model through alternating optimization, suppressing selection bias while avoiding trivial latent distributions. Experiments on ITE estimation tasks show that GWIB consistently outperforms state-of-the-art CFR methods. To promote the research community, we release our project at https://github.com/peteryang1031/Causal-GWIB.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2405.15509",
        "abstract url": "https://arxiv.org/abs/2405.15509",
        "title": "Randomized algorithms and PAC bounds for inverse reinforcement learning in continuous spaces",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work studies discrete-time discounted Markov decision processes with continuous state and action spaces and addresses the inverse problem of inferring a cost function from observed optimal behavior. We first consider the case in which we have access to the entire expert policy and characterize the set of solutions to the inverse problem by using occupation measures, linear duality, and complementary slackness conditions. To avoid trivial solutions and ill-posedness, we introduce a natural linear normalization constraint. This results in an infinite-dimensional linear feasibility problem, prompting a thorough analysis of its properties. Next, we use linear function approximators and adopt a randomized approach, namely the scenario approach and related probabilistic feasibility guarantees, to derive epsilon-optimal solutions for the inverse problem. We further discuss the sample complexity for a desired approximation accuracy. Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": "29 pages, 4 figures"
    },
    {
        "paper id": "2405.15512",
        "abstract url": "https://arxiv.org/abs/2405.15512",
        "title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas like higher education. This paper explores this issue by using advanced classification techniques to differentiate between code written by humans and that generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms - including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting - to achieve this differentiation with an impressive accuracy of 98%. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well but provide at most 85-88% accuracy. We also show that untrained humans solve the same task not better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Submitted to AI journal of MDPI"
    },
    {
        "paper id": "2405.15514",
        "abstract url": "https://arxiv.org/abs/2405.15514",
        "title": "On the Convexity and Reliability of the Bethe Free Energy Approximation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The Bethe free energy approximation provides an effective way for relaxing NP-hard problems of probabilistic inference. However, its accuracy depends on the model parameters and particularly degrades if a phase transition in the model occurs. In this work, we analyze when the Bethe approximation is reliable and how this can be verified. We argue and show by experiment that it is mostly accurate if it is convex on a submanifold of its domain, the 'Bethe box'. For verifying its convexity, we derive two sufficient conditions that are based on the definiteness properties of the Bethe Hessian matrix: the first uses the concept of diagonal dominance, and the second decomposes the Bethe Hessian matrix into a sum of sparse matrices and characterizes the definiteness properties of the individual matrices in that sum. These theoretical results provide a simple way to estimate the critical phase transition temperature of a model. As a practical contribution we propose $\\texttt{BETHE-MIN}$, a projected quasi-Newton method to efficiently find a minimum of the Bethe free energy.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "This work has been submitted to the Journal of Machine Learning Research (JMLR) for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2405.15521",
        "abstract url": "https://arxiv.org/abs/2405.15521",
        "title": "A Preference-oriented Diversity Model Based on Mutual-information in Re-ranking for E-commerce Search",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Re-ranking is a process of rearranging ranking list to more effectively meet user demands by accounting for the interrelationships between items. Existing methods predominantly enhance the precision of search results, often at the expense of diversity, leading to outcomes that may not fulfill the varied needs of users. Conversely, methods designed to promote diversity might compromise the precision of the results, failing to satisfy the users' requirements for accuracy. To alleviate the above problems, this paper proposes a Preference-oriented Diversity Model Based on Mutual-information (PODM-MI), which consider both accuracy and diversity in the re-ranking process. Specifically, PODM-MI adopts Multidimensional Gaussian distributions based on variational inference to capture users' diversity preferences with uncertainty. Then we maximize the mutual information between the diversity preferences of the users and the candidate items using the maximum variational inference lower bound to enhance their correlations. Subsequently, we derive a utility matrix based on the correlations, enabling the adaptive ranking of items in line with user preferences and establishing a balance between the aforementioned objectives. Experimental results on real-world online e-commerce systems demonstrate the significant improvements of PODM-MI, and we have successfully deployed PODM-MI on an e-commerce search platform.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15539",
        "abstract url": "https://arxiv.org/abs/2405.15539",
        "title": "A generalized neural tangent kernel for surrogate gradient learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.",
        "subjects": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cs.LG",
            "math.PR",
            "q-bio.NC"
        ],
        "comment": "52 pages, 3 figures + 2 supplementary figures"
    },
    {
        "paper id": "2405.15545",
        "abstract url": "https://arxiv.org/abs/2405.15545",
        "title": "Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex Finite-Sum Optimization with Heterogeneous Asynchronous Computations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times. We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations. By being robust to \"stragglers\" and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods. Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit. This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when $\\sqrt{m} \\geq n$, where $n$ is # of workers, and $m$ is # of data samples.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "43 pages, 2 figures"
    },
    {
        "paper id": "2405.15561",
        "abstract url": "https://arxiv.org/abs/2405.15561",
        "title": "When Generative AI Meets Workplace Learning: Creating A Realistic & Motivating Learning Experience With A Generative PCA",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Workplace learning is used to train employees systematically, e.g., via e-learning or in 1:1 training. However, this is often deemed ineffective and costly. Whereas pure e-learning lacks the possibility of conversational exercise and personal contact, 1:1 training with human instructors involves a high level of personnel and organizational costs. Hence, pedagogical conversational agents (PCAs), based on generative AI, seem to compensate for the disadvantages of both forms. Following Action Design Research, this paper describes an organizational communication training with a Generative PCA (GenPCA). The evaluation shows promising results: the agent was perceived positively among employees and contributed to an improvement in self-determined learning. However, the integration of such agent comes not without limitations. We conclude with suggestions concerning the didactical methods, which are supported by a GenPCA, and possible improvements of such an agent for workplace learning.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15568",
        "abstract url": "https://arxiv.org/abs/2405.15568",
        "title": "OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Open-ended and AI-generating algorithms aim to continuously generate and solve increasingly complex tasks indefinitely, offering a promising path toward more general intelligence. To accomplish this grand vision, learning must occur within a vast array of potential tasks. Existing approaches to automatically generating environments are constrained within manually predefined, often narrow distributions of environment, limiting their ability to create any learning environment. To address this limitation, we introduce a novel framework, OMNI-EPIC, that augments previous work in Open-endedness via Models of human Notions of Interestingness (OMNI) with Environments Programmed in Code (EPIC). OMNI-EPIC leverages foundation models to autonomously generate code specifying the next learnable (i.e., not too easy or difficult for the agent's current skill set) and interesting (e.g., worthwhile and novel) tasks. OMNI-EPIC generates both environments (e.g., an obstacle course) and reward functions (e.g., progress through the obstacle course quickly without touching red objects), enabling it, in principle, to create any simulatable learning task. We showcase the explosive creativity of OMNI-EPIC, which continuously innovates to suggest new, interesting learning challenges. We also highlight how OMNI-EPIC can adapt to reinforcement learning agents' learning progress, generating tasks that are of suitable difficulty. Overall, OMNI-EPIC can endlessly create learnable and interesting environments, further propelling the development of self-improving AI systems and AI-Generating Algorithms. Project website with videos: https://dub.sh/omniepic",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15569",
        "abstract url": "https://arxiv.org/abs/2405.15569",
        "title": "Randomized heuristic repair for large-scale multidimensional knapsack problem",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The multidimensional knapsack problem (MKP) is an NP-hard combinatorial optimization problem whose solution is determining a subset of maximum total profit items that do not violate capacity constraints. Due to its hardness, large-scale MKP instances are usually a target for metaheuristics, a context in which effective feasibility maintenance strategies are crucial. In 1998, Chu and Beasley proposed an effective heuristic repair that is still relevant for recent metaheuristics. However, due to its deterministic nature, the diversity of solutions such heuristic provides is insufficient for long runs. As a result, the search for new solutions ceases after a while. This paper proposes an efficiency-based randomization strategy for the heuristic repair that increases the variability of the repaired solutions without deteriorating quality and improves the overall results.",
        "subjects": [
            "cs.AI",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15579",
        "abstract url": "https://arxiv.org/abs/2405.15579",
        "title": "Generating density nowcasts for U.S. GDP growth with deep learning: Bayes by Backprop and Monte Carlo dropout",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent results in the literature indicate that artificial neural networks (ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy of GDP nowcasts. Compared to the DFM, the performance advantage of these highly flexible, nonlinear estimators is particularly evident in periods of recessions and structural breaks. From the perspective of policy-makers, however, nowcasts are the most useful when they are conveyed with uncertainty attached to them. While the DFM and other classical time series approaches analytically derive the predictive (conditional) distribution for GDP growth, ANNs can only produce point nowcasts based on their default training procedure (backpropagation). To fill this gap, first in the literature, we adapt two different deep learning algorithms that enable ANNs to generate density nowcasts for U.S. GDP growth: Bayes by Backprop and Monte Carlo dropout. The accuracy of point nowcasts, defined as the mean of the empirical predictive distribution, is evaluated relative to a naive constant growth model for GDP and a benchmark DFM specification. Using a 1D CNN as the underlying ANN architecture, both algorithms outperform those benchmarks during the evaluation period (2012:Q1 -- 2022:Q4). Furthermore, both algorithms are able to dynamically adjust the location (mean), scale (variance), and shape (skew) of the empirical predictive distribution. The results indicate that both Bayes by Backprop and Monte Carlo dropout can effectively augment the scope and functionality of ANNs, rendering them a fully compatible and competitive alternative for classical time series approaches.",
        "subjects": [
            "econ.EM",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15583",
        "abstract url": "https://arxiv.org/abs/2405.15583",
        "title": "Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We pursue transfer learning to improve classifier accuracy on a target task with few labeled examples available for training. Recent work suggests that using a source task to learn a prior distribution over neural net weights, not just an initialization, can boost target task performance. In this study, we carefully compare transfer learning with and without source task informed priors across 5 datasets. We find that standard transfer learning informed by an initialization only performs far better than reported in previous comparisons. The relative gains of methods using informative priors over standard transfer learning vary in magnitude across datasets. For the scenario of 5-300 examples per class, we find negative or negligible gains on 2 datasets, modest gains (between 1.5-3 points of accuracy) on 2 other datasets, and substantial gains (>8 points) on one dataset. Among methods using informative priors, we find that an isotropic covariance appears competitive with learned low-rank covariance matrix while being substantially simpler to understand and tune. Further analysis suggests that the mechanistic justification for informed priors -- hypothesized improved alignment between train and test loss landscapes -- is not consistently supported due to high variability in empirical landscapes. We release code to allow independent reproduction of all experiments.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15593",
        "abstract url": "https://arxiv.org/abs/2405.15593",
        "title": "MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a new variant of the Adam optimizer [Kingma and Ba, 2014] called MICROADAM that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees. We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly. We control the resulting compression error via a novel instance of the classical error feedback mechanism from distributed optimization [Seide et al., 2014, Alistarh et al., 2018, Karimireddy et al., 2019] in which the error correction information is itself compressed to allow for practical memory gains. We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance. Specifically, we show that MICROADAM can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with lower memory usage and similar running time. Our code is available at https://github.com/IST-DASLab/MicroAdam.",
        "subjects": [
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15599",
        "abstract url": "https://arxiv.org/abs/2405.15599",
        "title": "On the Computational Landscape of Replicable Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study computational aspects of algorithmic replicability, a notion of stability introduced by Impagliazzo, Lei, Pitassi, and Sorrell [2022]. Motivated by a recent line of work that established strong statistical connections between replicability and other notions of learnability such as online learning, private learning, and SQ learning, we aim to understand better the computational connections between replicability and these learning paradigms. Our first result shows that there is a concept class that is efficiently replicably PAC learnable, but, under standard cryptographic assumptions, no efficient online learner exists for this class. Subsequently, we design an efficient replicable learner for PAC learning parities when the marginal distribution is far from uniform, making progress on a question posed by Impagliazzo et al. [2022]. To obtain this result, we design a replicable lifting framework inspired by Blanc, Lange, Malik, and Tan [2023] that transforms in a black-box manner efficient replicable PAC learners under the uniform marginal distribution over the Boolean hypercube to replicable PAC learners under any marginal distribution, with sample and time complexity that depends on a certain measure of the complexity of the distribution. Finally, we show that any pure DP learner can be transformed to a replicable one in time polynomial in the accuracy, confidence parameters and exponential in the representation dimension of the underlying hypothesis class.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15605",
        "abstract url": "https://arxiv.org/abs/2405.15605",
        "title": "Fast-PGM: Fast Probabilistic Graphical Model Learning and Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Probabilistic graphical models (PGMs) serve as a powerful framework for modeling complex systems with uncertainty and extracting valuable insights from data. However, users face challenges when applying PGMs to their problems in terms of efficiency and usability. This paper presents Fast-PGM, an efficient and open-source library for PGM learning and inference. Fast-PGM supports comprehensive tasks on PGMs, including structure and parameter learning, as well as exact and approximate inference, and enhances efficiency of the tasks through computational and memory optimizations and parallelization techniques. Concurrently, Fast-PGM furnishes developers with flexible building blocks, furnishes learners with detailed documentation, and affords non-experts user-friendly interfaces, thereby ameliorating the usability of PGMs to users across a spectrum of expertise levels. The source code of Fast-PGM is available at https://github.com/jjiantong/FastPGM.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15614",
        "abstract url": "https://arxiv.org/abs/2405.15614",
        "title": "Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Despite various approaches being employed to detect vulnerabilities, the number of reported vulnerabilities shows an upward trend over the years. This suggests the problems are not caught before the code is released, which could be caused by many factors, like lack of awareness, limited efficacy of the existing vulnerability detection tools or the tools not being user-friendly. To help combat some issues with traditional vulnerability detection tools, we propose using large language models (LLMs) to assist in finding vulnerabilities in source code. LLMs have shown a remarkable ability to understand and generate code, underlining their potential in code-related tasks. The aim is to test multiple state-of-the-art LLMs and identify the best prompting strategies, allowing extraction of the best value from the LLMs. We provide an overview of the strengths and weaknesses of the LLM-based approach and compare the results to those of traditional static analysis tools. We find that LLMs can pinpoint many more issues than traditional static analysis tools, outperforming traditional tools in terms of recall and F1 scores. The results should benefit software developers and security analysts responsible for ensuring that the code is free of vulnerabilities.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15618",
        "abstract url": "https://arxiv.org/abs/2405.15618",
        "title": "MLPs Learn In-Context",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In-context learning (ICL), the remarkable ability to solve a task from only input exemplars, has commonly been assumed to be a unique hallmark of Transformer models. In this study, we demonstrate that multi-layer perceptrons (MLPs) can also learn in-context. Moreover, we find that MLPs, and the closely related MLP-Mixer models, learn in-context competitively with Transformers given the same compute budget. We further show that MLPs outperform Transformers on a subset of ICL tasks designed to test relational reasoning. These results suggest that in-context learning is not exclusive to Transformers and highlight the potential of exploring this phenomenon beyond attention-based architectures. In addition, MLPs' surprising success on relational tasks challenges prior assumptions about simple connectionist models. Altogether, our results endorse the broad trend that ``less inductive bias is better\" and contribute to the growing interest in all-MLP alternatives to task-specific architectures.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": "29 pages, 9 figures, code available at https://github.com/wtong98/mlp-icl"
    },
    {
        "paper id": "2405.15624",
        "abstract url": "https://arxiv.org/abs/2405.15624",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Aligning Large Language Models (LLMs) is crucial for enhancing their safety and utility. However, existing methods, primarily based on preference datasets, face challenges such as noisy labels, high annotation costs, and privacy concerns. In this work, we introduce Alignment from Demonstrations (AfD), a novel approach leveraging high-quality demonstration data to overcome these challenges. We formalize AfD within a sequential decision-making framework, highlighting its unique challenge of missing reward signals. Drawing insights from forward and inverse reinforcement learning, we introduce divergence minimization objectives for AfD. Analytically, we elucidate the mass-covering and mode-seeking behaviors of various approaches, explaining when and why certain methods are superior. Practically, we propose a computationally efficient algorithm that extrapolates over a tailored reward model for AfD. We validate our key insights through experiments on the Harmless and Helpful tasks, demonstrating their strong empirical performance while maintaining simplicity.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15625",
        "abstract url": "https://arxiv.org/abs/2405.15625",
        "title": "Nonlinear denoising score matching for enhanced learning of structured distributions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a novel method for training score-based generative models which uses nonlinear noising dynamics to improve learning of structured distributions. Generalizing to a nonlinear drift allows for additional structure to be incorporated into the dynamics, thus making the training better adapted to the data, e.g., in the case of multimodality or (approximate) symmetries. Such structure can be obtained from the data by an inexpensive preprocessing step. The nonlinear dynamics introduces new challenges into training which we address in two ways: 1) we develop a new nonlinear denoising score matching (NDSM) method, 2) we introduce neural control variates in order to reduce the variance of the NDSM training objective. We demonstrate the effectiveness of this method on several examples: a) a collection of low-dimensional examples, motivated by clustering in latent space, b) high-dimensional images, addressing issues with mode collapse, small training sets, and approximate symmetries, the latter being a challenge for methods based on equivariant neural networks, which require exact symmetries.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "14 pages, 8 figures"
    },
    {
        "paper id": "2405.15682",
        "abstract url": "https://arxiv.org/abs/2405.15682",
        "title": "The Road Less Scheduled",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available (https://github.com/facebookresearch/schedule_free).",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.OC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15699",
        "abstract url": "https://arxiv.org/abs/2405.15699",
        "title": "Dimension-free deterministic equivalents for random feature regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work we investigate the generalization performance of random feature ridge regression (RFRR). Our main contribution is a general deterministic equivalent for the test error of RFRR. Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues. Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension -- allowing for infinite-dimensional features. We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets. As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay. In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate.",
        "subjects": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15706",
        "abstract url": "https://arxiv.org/abs/2405.15706",
        "title": "The Impact of Geometric Complexity on Neural Collapse in Transfer Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many of the recent remarkable advances in computer vision and language models can be attributed to the success of transfer learning via the pre-training of large foundation models. However, a theoretical framework which explains this empirical success is incomplete and remains an active area of research. Flatness of the loss surface and neural collapse have recently emerged as useful pre-training metrics which shed light on the implicit biases underlying pre-training. In this paper, we explore the geometric complexity of a model's learned representations as a fundamental mechanism that relates these two concepts. We show through experiments and theory that mechanisms which affect the geometric complexity of the pre-trained network also influence the neural collapse. Furthermore, we show how this effect of the geometric complexity generalizes to the neural collapse of new classes as well, thus encouraging better performance on downstream tasks, particularly in the few-shot setting.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15709",
        "abstract url": "https://arxiv.org/abs/2405.15709",
        "title": "Information-theoretic Generalization Analysis for Expected Calibration Error",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "While the expected calibration error (ECE), which employs binning, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited. In this paper, we present the first comprehensive analysis of the estimation bias in the two common binning strategies, uniform mass and uniform width binning. Our analysis establishes upper bounds on the bias, achieving an improved convergence rate. Moreover, our bounds reveal, for the first time, the optimal number of bins to minimize the estimation bias. We further extend our bias analysis to generalization error analysis based on the information-theoretic approach, deriving upper bounds that enable the numerical evaluation of how small the ECE is for unknown data. Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.",
        "subjects": [
            "cs.LG",
            "math.ST",
            "stat.ML"
        ],
        "comment": "34 pages, 3 figures"
    },
    {
        "paper id": "2405.15722",
        "abstract url": "https://arxiv.org/abs/2405.15722",
        "title": "Models That Prove Their Own Correctness",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measured \\emph{on average} over a distribution of inputs, giving no guarantee for any fixed input. This paper proposes a theoretically-founded solution to this problem: to train *Self-Proving models* that prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof. Self-Proving models satisfy that, with high probability over a random input, the model generates a correct output \\emph{and} successfully proves its correctness to $V\\!$. The *soundness* property of $V$ guarantees that, for *every* input, no model can convince $V$ of the correctness of an incorrect output. Thus, a Self-Proving model proves correctness of most of its outputs, while *all* incorrect outputs (of any model) are detected by $V$. We devise a generic method for learning Self-Proving models, and we prove convergence bounds under certain assumptions. The theoretical framework and results are complemented by experiments on an arithmetic capability: computing the greatest common divisor (GCD) of two integers. Our learning method is used to train a Self-Proving transformer that computes the GCD *and* proves the correctness of its answer.",
        "subjects": [
            "cs.LG",
            "cs.CC",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15731",
        "abstract url": "https://arxiv.org/abs/2405.15731",
        "title": "Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15739",
        "abstract url": "https://arxiv.org/abs/2405.15739",
        "title": "Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge cut-off date, encompassing 3,066 references in total. In our experiment, GPT-4 was tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias in GPT-4, which persists even after controlling for publication year, title length, number of authors, and venue. Additionally, we observe a large consistency between the characteristics of GPT-4's existing and non-existent generated references, indicating the model's internalization of citation patterns. By analyzing citation graphs, we show that the references recommended by GPT-4 are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases and introduce new ones, potentially skewing scientific knowledge dissemination. Our results underscore the need for identifying the model's biases and for developing balanced methods to interact with LLMs in general.",
        "subjects": [
            "cs.DL",
            "cs.AI",
            "cs.LG",
            "cs.SI"
        ],
        "comment": "28 pages, 11 figures"
    },
    {
        "paper id": "2405.15743",
        "abstract url": "https://arxiv.org/abs/2405.15743",
        "title": "Sparse maximal update parameterization: A holistic approach to sparse training dynamics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Several challenges make it difficult for sparse neural networks to compete with dense models. First, setting a large fraction of weights to zero impairs forward and gradient signal propagation. Second, sparse studies often need to test multiple sparsity levels, while also introducing new hyperparameters (HPs), leading to prohibitive tuning costs. Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models. Unfortunately, we show sparse and dense networks do not share the same optimal HPs. Without stable dynamics and effective training recipes, it is costly to test sparsity at scale, which is key to surpassing dense networks and making the business case for sparsity acceleration in hardware. A holistic approach is needed to tackle these challenges and we propose S$\u03bc$Par as one such approach. S$\u03bc$Par ensures activations, gradients, and weight updates all scale independently of sparsity level. Further, by reparameterizing the HPs, S$\u03bc$Par enables the same HP values to be optimal as we vary both sparsity level and model width. HPs can be tuned on small dense networks and transferred to large sparse models, greatly reducing tuning costs. On large-scale language modeling, S$\u03bc$Par training improves loss by up to 8.2% over the common approach of using the dense model standard parameterization.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages main text, 11 pages reference and appendix, 11 figures"
    },
    {
        "paper id": "2405.15767",
        "abstract url": "https://arxiv.org/abs/2405.15767",
        "title": "Improved Particle Approximation Error for Mean Field Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional defined over the space of probability distributions. MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks. Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finite-particle setting. Recent works (Chen et al., 2022; Suzuki et al., 2023b) have demonstrated the uniform-in-time propagation of chaos for MFLD, showing that the gap between the particle system and its mean-field limit uniformly shrinks over time as the number of particles increases. In this work, we improve the dependence on logarithmic Sobolev inequality (LSI) constants in their particle approximation errors, which can exponentially deteriorate with the regularization coefficient. Specifically, we establish an LSI-constant-free particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization. As the application, we demonstrate improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution, and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2405.15768",
        "abstract url": "https://arxiv.org/abs/2405.15768",
        "title": "Canonical Variates in Wasserstein Metric Space",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we address the classification of instances each characterized not by a singular point, but by a distribution on a vector space. We employ the Wasserstein metric to measure distances between distributions, which are then used by distance-based classification algorithms such as k-nearest neighbors, k-means, and pseudo-mixture modeling. Central to our investigation is dimension reduction within the Wasserstein metric space to enhance classification accuracy. We introduce a novel approach grounded in the principle of maximizing Fisher's ratio, defined as the quotient of between-class variation to within-class variation. The directions in which this ratio is maximized are termed discriminant coordinates or canonical variates axes. In practice, we define both between-class and within-class variations as the average squared distances between pairs of instances, with the pairs either belonging to the same class or to different classes. This ratio optimization is achieved through an iterative algorithm, which alternates between optimal transport and maximization steps within the vector space. We conduct empirical studies to assess the algorithm's convergence and, through experimental validation, demonstrate that our dimension reduction technique substantially enhances classification performance. Moreover, our method outperforms well-established algorithms that operate on vector representations derived from distributional data. It also exhibits robustness against variations in the distributional representations of data clouds.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "double space 37 pages, 6 figures"
    },
    {
        "paper id": "2405.15831",
        "abstract url": "https://arxiv.org/abs/2405.15831",
        "title": "Transmission Interface Power Flow Adjustment: A Deep Reinforcement Learning Approach based on Multi-task Attribution Map",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Transmission interface power flow adjustment is a critical measure to ensure the security and economy operation of power systems. However, conventional model-based adjustment schemes are limited by the increasing variations and uncertainties occur in power systems, where the adjustment problems of different transmission interfaces are often treated as several independent tasks, ignoring their coupling relationship and even leading to conflict decisions. In this paper, we introduce a novel data-driven deep reinforcement learning (DRL) approach, to handle multiple power flow adjustment tasks jointly instead of learning each task from scratch. At the heart of the proposed method is a multi-task attribution map (MAM), which enables the DRL agent to explicitly attribute each transmission interface task to different power system nodes with task-adaptive attention weights. Based on this MAM, the agent can further provide effective strategies to solve the multi-task adjustment problem with a near-optimal operation cost. Simulation results on the IEEE 118-bus system, a realistic 300-bus system in China, and a very large European system with 9241 buses demonstrate that the proposed method significantly improves the performance compared with several baseline methods, and exhibits high interpretability with the learnable MAM.",
        "subjects": [
            "eess.SY",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted by IEEE Transactions on Power Systems"
    },
    {
        "paper id": "2405.15834",
        "abstract url": "https://arxiv.org/abs/2405.15834",
        "title": "A Fisher-Rao gradient flow for entropic mean-field min-max games",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Gradient flows play a substantial role in addressing many machine learning problems. We examine the convergence in continuous-time of a \\textit{Fisher-Rao} (Mean-Field Birth-Death) gradient flow in the context of solving convex-concave min-max games with entropy regularization. We propose appropriate Lyapunov functions to demonstrate convergence with explicit rates to the unique mixed Nash equilibrium.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "math.PR"
        ],
        "comment": "18 pages. arXiv admin note: substantial text overlap with arXiv:2306.03033"
    },
    {
        "paper id": "2405.15842",
        "abstract url": "https://arxiv.org/abs/2405.15842",
        "title": "Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapid development of large language models (LLMs) has led to significant advancements in code completion tasks. While larger models have higher accuracy, they also cost much more to run. Meanwhile, model cascading has been proven effective to conserve computational resources while enhancing accuracy in LLMs on natural language generation tasks. It generates output with the smallest model in a set, and only queries the larger models when it fails to meet predefined quality criteria. However, this strategy has not been used in code completion tasks, primarily because assessing the quality of code completions differs substantially from assessing natural language, where the former relies heavily on the functional correctness. To address this, we propose letting each model generate and execute a set of test cases for their solutions, and use the test results as the cascading threshold. We show that our model cascading strategy reduces computational costs while increases accuracy compared to generating the output with a single model. We also introduce a heuristics to determine the optimal combination of the number of solutions, test cases, and test lines each model should generate, based on the budget. Compared to speculative decoding, our method works on black-box models, having the same level of cost-accuracy trade-off, yet providing much more choices based on the server's budget. Ours is the first work to optimize cost-accuracy trade-off for LLM code generation with model cascading.",
        "subjects": [
            "cs.SE",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15868",
        "abstract url": "https://arxiv.org/abs/2405.15868",
        "title": "LLS: Local Learning Rule for Deep Neural Networks Inspired by Neural Activity Synchronization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Training deep neural networks (DNNs) using traditional backpropagation (BP) presents challenges in terms of computational complexity and energy consumption, particularly for on-device learning where computational resources are limited. Various alternatives to BP, including random feedback alignment, forward-forward, and local classifiers, have been explored to address these challenges. These methods have their advantages, but they can encounter difficulties when dealing with intricate visual tasks or demand considerable computational resources. In this paper, we propose a novel Local Learning rule inspired by neural activity Synchronization phenomena (LLS) observed in the brain. LLS utilizes fixed periodic basis vectors to synchronize neuron activity within each layer, enabling efficient training without the need for additional trainable parameters. We demonstrate the effectiveness of LLS and its variations, LLS-M and LLS-MxM, on multiple image classification datasets, achieving accuracy comparable to BP with reduced computational complexity and minimal additional parameters. Furthermore, the performance of LLS on the Visual Wake Word (VWW) dataset highlights its suitability for on-device learning tasks, making it a promising candidate for edge hardware implementations.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "14 pages, 4 figures"
    },
    {
        "paper id": "2405.15893",
        "abstract url": "https://arxiv.org/abs/2405.15893",
        "title": "Quantifying Influencer Effects on Affective Polarization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "In an era where digital platforms increasingly mediate public discourse, grasping the complexities and nuances in affective polarization--especially as influenced by key figures on social media--has never been more vital. This study delves into the intricate web of interactions on Twitter, now rebranded as 'X', to unravel how influencer-led conversations catalyze shifts in public sentiment, laying bare the complex dynamics that underpin online polarization. Employing a novel methodological framework that includes counterfactual analysis, we analyze scenarios with and without specific influencer-led conversations. Our findings illuminate the significant role influencers play in shaping public discourse, offering insights into the mechanisms of online polarization and suggesting pathways for future research to mitigate divisiveness and explore new methods for quantifying affective polarization. This research contributes to the broader understanding of digital communication's impact on societal polarization, underscoring the importance of detailed analysis in developing strategies to foster a more cohesive digital public sphere.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2405.15894",
        "abstract url": "https://arxiv.org/abs/2405.15894",
        "title": "Derivatives of Stochastic Gradient Descent",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider stochastic optimization problems where the objective depends on some parameter, as commonly found in hyperparameter optimization for instance. We investigate the behavior of the derivatives of the iterates of Stochastic Gradient Descent (SGD) with respect to that parameter and show that they are driven by an inexact SGD recursion on a different objective function, perturbed by the convergence of the original SGD. This enables us to establish that the derivatives of SGD converge to the derivative of the solution mapping in terms of mean squared error whenever the objective is strongly convex. Specifically, we demonstrate that with constant step-sizes, these derivatives stabilize within a noise ball centered at the solution derivative, and that with vanishing step-sizes they exhibit $O(\\log(k)^2 / k)$ convergence rates. Additionally, we prove exponential convergence in the interpolation regime. Our theoretical findings are illustrated by numerical experiments on synthetic tasks.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15895",
        "abstract url": "https://arxiv.org/abs/2405.15895",
        "title": "Predicting the Impact of Model Expansion through the Minima Manifold: A Loss Landscape Perspective",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The optimal model for a given task is often challenging to determine, requiring training multiple models from scratch which becomes prohibitive as dataset and model sizes grow. A more efficient alternative is to reuse smaller pre-trained models by expanding them, however, this is not widely adopted as how this impacts training dynamics remains poorly understood. While prior works have introduced statistics to measure these effects, they remain flawed. To rectify this, we offer a new approach for understanding and quantifying the impact of expansion through the lens of the loss landscape, which has been shown to contain a manifold of linearly connected minima. Building on this new perspective, we propose a metric to study the impact of expansion by estimating the size of the manifold. Experimental results show a clear relationship between gains in performance and manifold size, enabling the comparison of candidate models and presenting a first step towards expanding models more reliably based on geometric properties of the loss landscape.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15907",
        "abstract url": "https://arxiv.org/abs/2405.15907",
        "title": "Belief-State Query Policies for Planning With Preferences Under Partial Observability",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Planning in real-world settings often entails addressing partial observability while aligning with users' preferences. We present a novel framework for expressing users' preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) preferences in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs). We present the first formal analysis of such preferences and prove that while the expected value of a BSQ preference is not a convex function w.r.t its parameters, it is piecewise constant and yields an implicit discrete parameter search space that is finite for finite horizons. This theoretical result leads to novel algorithms that optimize gPOMDP agent behavior while guaranteeing user preference compliance. Theoretical analysis proves that our algorithms converge to the optimal preference-compliant behavior in the limit. Empirical results show that BSQ preferences provide a computationally feasible approach for planning with preferences in partially observable settings.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15911",
        "abstract url": "https://arxiv.org/abs/2405.15911",
        "title": "Learning accurate and interpretable decision trees",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Decision trees are a popular tool in machine learning and yield easy-to-understand models. Several techniques have been proposed in the literature for learning a decision tree classifier, with different techniques working well for data from different domains. In this work, we develop approaches to design decision tree learning algorithms given repeated access to data from the same domain. We propose novel parameterized classes of node splitting criteria in top-down algorithms, which interpolate between popularly used entropy and Gini impurity based criteria, and provide theoretical bounds on the number of samples needed to learn the splitting function appropriate for the data at hand. We also study the sample complexity of tuning prior parameters in Bayesian decision tree learning, and extend our results to decision tree regression. We further consider the problem of tuning hyperparameters in pruning the decision tree for classical pruning algorithms including min-cost complexity pruning. We also study the interpretability of the learned decision trees and introduce a data-driven approach for optimizing the explainability versus accuracy trade-off using decision trees. Finally, we demonstrate the significance of our approach on real world datasets by learning data-specific decision trees which are simultaneously more accurate and interpretable.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "26 pages, UAI 2024"
    },
    {
        "paper id": "2405.15912",
        "abstract url": "https://arxiv.org/abs/2405.15912",
        "title": "Uncertainty Quantification for Neurosymbolic Programs via Compositional Conformal Prediction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning has become an effective tool for automatically annotating unstructured data (e.g., images) with structured labels (e.g., object detections). As a result, a new programming paradigm called neurosymbolic programming has emerged where users write queries against these predicted annotations. However, due to the intrinsic fallibility of machine learning models, these programs currently lack any notion of correctness. In many domains, users may want some kind of conservative guarantee that the results of their queries contain all possibly relevant instances. Conformal prediction has emerged as a promising strategy for quantifying uncertainty in machine learning by modifying models to predict sets of labels instead of individual labels; it provides a probabilistic guarantee that the prediction set contains the true label with high probability. We propose a novel framework for adapting conformal prediction to neurosymbolic programs; our strategy is to represent prediction sets as abstract values in some abstract domain, and then to use abstract interpretation to propagate prediction sets through the program. Our strategy satisfies three key desiderata: (i) correctness (i.e., the program outputs a prediction set that contains the true output with high probability), (ii) compositionality (i.e., we can quantify uncertainty separately for different modules and then compose them together), and (iii) structured values (i.e., we can provide uncertainty quantification for structured values such as lists). When the full program is available ahead-of-time, we propose an optimization that incorporates conformal prediction at intermediate program points to reduce imprecision in abstract interpretation. We evaluate our approach on programs that take MNIST and MS-COCO images as input, demonstrating that it produces reasonably sized prediction sets while satisfying a coverage guarantee.",
        "subjects": [
            "cs.PL",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "arXiv abstract edited"
    },
    {
        "paper id": "2405.15913",
        "abstract url": "https://arxiv.org/abs/2405.15913",
        "title": "Scaling up the Banded Matrix Factorization Mechanism for Differentially Private ML",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "DP-BandMF offers a powerful approach to differentially private machine learning, balancing privacy amplification with noise correlation for optimal noise reduction. However, its scalability has been limited to settings where the number of training iterations is less than $10^4$. In this work, we present techniques that significantly extend DP-BandMF's reach, enabling use in settings with and over $10^6$ training iterations. Our enhanced implementation, coupled with extensive experiments, provides clear guidelines on selecting the optimal number of bands. These insights offer practitioners a deeper understanding of DP-BandMF's performance and how to maximize its utility for privacy-preserving machine learning.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15920",
        "abstract url": "https://arxiv.org/abs/2405.15920",
        "title": "SF-DQN: Provable Knowledge Transfer using Successor Feature for Deep Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper studies the transfer reinforcement learning (RL) problem where multiple RL problems have different reward functions but share the same underlying transition dynamics. In this setting, the Q-function of each RL problem (task) can be decomposed into a successor feature (SF) and a reward mapping: the former characterizes the transition dynamics, and the latter characterizes the task-specific reward function. This Q-function decomposition, coupled with a policy improvement operator known as generalized policy improvement (GPI), reduces the sample complexity of finding the optimal Q-function, and thus the SF \\& GPI framework exhibits promising empirical performance compared to traditional RL methods like Q-learning. However, its theoretical foundations remain largely unestablished, especially when learning the successor features using deep neural networks (SF-DQN). This paper studies the provable knowledge transfer using SFs-DQN in transfer RL problems. We establish the first convergence analysis with provable generalization guarantees for SF-DQN with GPI. The theory reveals that SF-DQN with GPI outperforms conventional RL approaches, such as deep Q-network, in terms of both faster convergence rate and better generalization. Numerical experiments on real and synthetic RL tasks support the superior performance of SF-DQN \\& GPI, aligning with our theoretical findings.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2310.16173"
    },
    {
        "paper id": "2405.15926",
        "abstract url": "https://arxiv.org/abs/2405.15926",
        "title": "Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Despite the remarkable empirical performance of Transformers, their theoretical understanding remains elusive. Here, we consider a deep multi-head self-attention network, that is closely related to Transformers yet analytically tractable. We develop a statistical mechanics theory of Bayesian learning in this model, deriving exact equations for the network's predictor statistics under the finite-width thermodynamic limit, i.e., $N,P\\rightarrow\\infty$, $P/N=\\mathcal{O}(1)$, where $N$ is the network width and $P$ is the number of training examples. Our theory shows that the predictor statistics are expressed as a sum of independent kernels, each one pairing different 'attention paths', defined as information pathways through different attention heads across layers. The kernels are weighted according to a 'task-relevant kernel combination' mechanism that aligns the total kernel with the task labels. As a consequence, this interplay between attention paths enhances generalization performance. Experiments confirm our findings on both synthetic and real-world sequence classification tasks. Finally, our theory explicitly relates the kernel combination mechanism to properties of the learned weights, allowing for a qualitative transfer of its insights to models trained via gradient descent. As an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads that are deemed less relevant by our theory.",
        "subjects": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15941",
        "abstract url": "https://arxiv.org/abs/2405.15941",
        "title": "A Unified Theory of Stochastic Proximal Point Methods without Smoothness",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a comprehensive analysis of a broad range of variations of the stochastic proximal point method (SPPM). Proximal point methods have attracted considerable interest owing to their numerical stability and robustness against imperfect tuning, a trait not shared by the dominant stochastic gradient descent (SGD) algorithm. A framework of assumptions that we introduce encompasses methods employing techniques such as variance reduction and arbitrary sampling. A cornerstone of our general theoretical approach is a parametric assumption on the iterates, correction and control vectors. We establish a single theorem that ensures linear convergence under this assumption and the $\u03bc$-strong convexity of the loss function, and without the need to invoke smoothness. This integral theorem reinstates best known complexity and convergence guarantees for several existing methods which demonstrates the robustness of our approach. We expand our study by developing three new variants of SPPM, and through numerical experiments we elucidate various properties inherent to them.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15950",
        "abstract url": "https://arxiv.org/abs/2405.15950",
        "title": "A Systematic Bias of Machine Learning Regression Models and Its Correction: an Application to Imaging-based Brain Age Prediction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning models for continuous outcomes often yield systematically biased predictions, particularly for values that largely deviate from the mean. Specifically, predictions for large-valued outcomes tend to be negatively biased, while those for small-valued outcomes are positively biased. We refer to this linear central tendency warped bias as the \"systematic bias of machine learning regression\". In this paper, we first demonstrate that this issue persists across various machine learning models, and then delve into its theoretical underpinnings. We propose a general constrained optimization approach designed to correct this bias and develop a computationally efficient algorithm to implement our method. Our simulation results indicate that our correction method effectively eliminates the bias from the predicted outcomes. We apply the proposed approach to the prediction of brain age using neuroimaging data. In comparison to competing machine learning models, our method effectively addresses the longstanding issue of \"systematic bias of machine learning regression\" in neuroimaging-based brain age calculation, yielding unbiased predictions of brain age.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15956",
        "abstract url": "https://arxiv.org/abs/2405.15956",
        "title": "CFGs: Causality Constrained Counterfactual Explanations using goal-directed ASP",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning models that automate decision-making are increasingly used in consequential areas such as loan approvals, pretrial bail approval, and hiring. Unfortunately, most of these models are black boxes, i.e., they are unable to reveal how they reach these prediction decisions. A need for transparency demands justification for such predictions. An affected individual might also desire explanations to understand why a decision was made. Ethical and legal considerations require informing the individual of changes in the input attribute (s) that could be made to produce a desirable outcome. Our work focuses on the latter problem of generating counterfactual explanations by considering the causal dependencies between features. In this paper, we present the framework CFGs, CounterFactual Generation with s(CASP), which utilizes the goal-directed Answer Set Programming (ASP) system s(CASP) to automatically generate counterfactual explanations from models generated by rule-based machine learning algorithms in particular. We benchmark CFGs with the FOLD-SE model. Reaching the counterfactual state from the initial state is planned and achieved using a series of interventions. To validate our proposal, we show how counterfactual explanations are computed and justified by imagining worlds where some or all factual assumptions are altered/changed. More importantly, we show how CFGs navigates between these worlds, namely, go from our initial state where we obtain an undesired outcome to the imagined goal state where we obtain the desired decision, taking into account the causal relationships among features.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.LO"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2402.04382"
    },
    {
        "paper id": "2405.15960",
        "abstract url": "https://arxiv.org/abs/2405.15960",
        "title": "Human-Centered Automation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid advancement of Generative Artificial Intelligence (AI), such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLM), has the potential to revolutionize the way we work and interact with digital systems across various industries. However, the current state of software automation, such as Robotic Process Automation (RPA) frameworks, often requires domain expertise and lacks visibility and intuitive interfaces, making it challenging for users to fully leverage these technologies. This position paper argues for the emerging area of Human-Centered Automation (HCA), which prioritizes user needs and preferences in the design and development of automation systems. Drawing on empirical evidence from human-computer interaction research and case studies, we highlight the importance of considering user perspectives in automation and propose a framework for designing human-centric automation solutions. The paper discusses the limitations of existing automation approaches, the challenges in integrating AI and RPA, and the benefits of human-centered automation for productivity, innovation, and democratizing access to these technologies. We emphasize the importance of open-source solutions and provide examples of how HCA can empower individuals and organizations in the era of rapidly progressing AI, helping them remain competitive. The paper also explores pathways to achieve more advanced and context-aware automation solutions. We conclude with a call to action for researchers and practitioners to focus on developing automation technologies that adapt to user needs, provide intuitive interfaces, and leverage the capabilities of high-end AI to create a more accessible and user-friendly future of automation.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": "12 pages, 0 figures"
    },
    {
        "paper id": "2405.15983",
        "abstract url": "https://arxiv.org/abs/2405.15983",
        "title": "Hierarchical Clustering via Local Search",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we introduce a local search algorithm for hierarchical clustering. For the local step, we consider a tree re-arrangement operation, known as the {\\em interchange}, which involves swapping two closely positioned sub-trees within a tree hierarchy. The interchange operation has been previously used in the context of phylogenetic trees. As the objective function for evaluating the resulting hierarchies, we utilize the revenue function proposed by Moseley and Wang (NIPS 2017.) In our main result, we show that any locally optimal tree guarantees a revenue of at least $\\frac{n-2}{3}\\sum_{i < j}w(i,j)$ where is $n$ the number of objects and $w: [n] \\times [n] \\rightarrow \\mathbb{R}^+$ is the associated similarity function. This finding echoes the previously established bound for the average link algorithm as analyzed by Moseley and Wang. We demonstrate that this alignment is not coincidental, as the average link trees enjoy the property of being locally optimal with respect to the interchange operation. Consequently, our study provides an alternative insight into the average link algorithm and reveals the existence of a broader range of hierarchies with relatively high revenue achievable through a straightforward local search algorithm. Furthermore, we present an implementation of the local search framework, where each local step requires $O(n)$ computation time. Our empirical results indicate that the proposed method, used as post-processing step, can effectively generate a hierarchical clustering with substantial revenue.",
        "subjects": [
            "cs.DS",
            "cs.LG"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2405.15987",
        "abstract url": "https://arxiv.org/abs/2405.15987",
        "title": "Modes of Analyzing Disinformation Narratives With AI/ML/Text Mining to Assist in Mitigating the Weaponization of Social Media",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "This paper highlights the developing need for quantitative modes for capturing and monitoring malicious communication in social media. There has been a deliberate \"weaponization\" of messaging through the use of social networks including by politically oriented entities both state sponsored and privately run. The article identifies a use of AI/ML characterization of generalized \"mal-info,\" a broad term which includes deliberate malicious narratives similar with hate speech, which adversely impact society. A key point of the discussion is that this mal-info will dramatically increase in volume, and it will become essential for sharable quantifying tools to provide support for human expert intervention. Despite attempts to introduce moderation on major platforms like Facebook and X/Twitter, there are now established alternative social networks that offer completely unmoderated spaces. The paper presents an introduction to these platforms and the initial results of a qualitative and semi-quantitative analysis of characteristic mal-info posts. The authors perform a rudimentary text mining function for a preliminary characterization in order to evaluate the modes for better-automated monitoring. The action examines several inflammatory terms using text analysis and, importantly, discusses the use of generative algorithms by one political agent in particular, providing some examples of the potential risks to society. This latter is of grave concern, and monitoring tools must be established. This paper presents a preliminary step to selecting relevant sources and to setting a foundation for characterizing the mal-info, which must be monitored. The AI/ML methods provide a means for semi-quantitative signature capture. The impending use of \"mal-GenAI\" is presented.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Accepted at ICWSM-2024 Workshop on Digital State Sponsored Disinformation and Propaganda: Challenges and Opportunities (DSSDP24)"
    },
    {
        "paper id": "2405.15992",
        "abstract url": "https://arxiv.org/abs/2405.15992",
        "title": "Data Complexity Estimates for Operator Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Operator learning has emerged as a new paradigm for the data-driven approximation of nonlinear operators. Despite its empirical success, the theoretical underpinnings governing the conditions for efficient operator learning remain incomplete. The present work develops theory to study the data complexity of operator learning, complementing existing research on the parametric complexity. We investigate the fundamental question: How many input/output samples are needed in operator learning to achieve a desired accuracy $\u03b5$? This question is addressed from the point of view of $n$-widths, and this work makes two key contributions. The first contribution is to derive lower bounds on $n$-widths for general classes of Lipschitz and Fr\u00e9chet differentiable operators. These bounds rigorously demonstrate a ``curse of data-complexity'', revealing that learning on such general classes requires a sample size exponential in the inverse of the desired accuracy $\u03b5$. The second contribution of this work is to show that ``parametric efficiency'' implies ``data efficiency''; using the Fourier neural operator (FNO) as a case study, we show rigorously that on a narrower class of operators, efficiently approximated by FNO in terms of the number of tunable parameters, efficient operator learning is attainable in data complexity as well. Specifically, we show that if only an algebraically increasing number of tunable parameters is needed to reach a desired approximation accuracy, then an algebraically bounded number of data samples is also sufficient to achieve the same accuracy.",
        "subjects": [
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15994",
        "abstract url": "https://arxiv.org/abs/2405.15994",
        "title": "Verified Safe Reinforcement Learning for Neural Network Dynamic Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Learning reliably safe autonomous control is one of the core problems in trustworthy autonomy. However, training a controller that can be formally verified to be safe remains a major challenge. We introduce a novel approach for learning verified safe control policies in nonlinear neural dynamical systems while maximizing overall performance. Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts. The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon. The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs. Finally, we learn multiple verified initial-state-dependent controllers, an idea that is especially valuable for more complex domains where learning a single universal verified safe controller is extremely challenging. Our experiments on five safe control problems demonstrate that our trained controllers can achieve verified safety over horizons that are as much as an order of magnitude longer than state-of-the-art baselines, while maintaining high reward, as well as a perfect safety record over entire episodes.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16013",
        "abstract url": "https://arxiv.org/abs/2405.16013",
        "title": "Convergence Behavior of an Adversarial Weak Supervision Method",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Labeling data via rules-of-thumb and minimal label supervision is central to Weak Supervision, a paradigm subsuming subareas of machine learning such as crowdsourced learning and semi-supervised ensemble learning. By using this labeled data to train modern machine learning methods, the cost of acquiring large amounts of hand labeled data can be ameliorated. Approaches to combining the rules-of-thumb falls into two camps, reflecting different ideologies of statistical estimation. The most common approach, exemplified by the Dawid-Skene model, is based on probabilistic modeling. The other, developed in the work of Balsubramani-Freund and others, is adversarial and game-theoretic. We provide a variety of statistical results for the adversarial approach under log-loss: we characterize the form of the solution, relate it to logistic regression, demonstrate consistency, and give rates of convergence. On the other hand, we find that probabilistic approaches for the same model class can fail to be consistent. Experimental results are provided to corroborate the theoretical results.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "49 pages, 16 figures, to be published in UAI 2024"
    },
    {
        "paper id": "2405.16027",
        "abstract url": "https://arxiv.org/abs/2405.16027",
        "title": "Feature Protection For Out-of-distribution Generalization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the availability of large pre-trained models, a modern workflow for building real-world machine learning solutions is to fine-tune such models on a downstream task with a relatively small domain-specific dataset. In such applications, one major challenge is that the small fine-tuning dataset does not have sufficient coverage of the distribution encountered when the model is deployed. It is thus important to design fine-tuning methods that are robust to out-of-distribution (OOD) data that are under-represented by the training data. This paper compares common fine-tuning methods to investigate their OOD performance and demonstrates that standard methods will result in a significant change to the pre-trained model so that the fine-tuned features overfit the fine-tuning dataset. However, this causes deteriorated OOD performance. To overcome this issue, we show that protecting pre-trained features leads to a fine-tuned model more robust to OOD generalization. We validate the feature protection methods with extensive experiments of fine-tuning CLIP on ImageNet and DomainNet.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2309.06256"
    },
    {
        "paper id": "2405.16029",
        "abstract url": "https://arxiv.org/abs/2405.16029",
        "title": "Online Resource Allocation for Edge Intelligence with Colocated Model Retraining and Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "With edge intelligence, AI models are increasingly pushed to the edge to serve ubiquitous users. However, due to the drift of model, data, and task, AI model deployed at the edge suffers from degraded accuracy in the inference serving phase. Model retraining handles such drifts by periodically retraining the model with newly arrived data. When colocating model retraining and model inference serving for the same model on resource-limited edge servers, a fundamental challenge arises in balancing the resource allocation for model retraining and inference, aiming to maximize long-term inference accuracy. This problem is particularly difficult due to the underlying mathematical formulation being time-coupled, non-convex, and NP-hard. To address these challenges, we introduce a lightweight and explainable online approximation algorithm, named ORRIC, designed to optimize resource allocation for adaptively balancing the accuracy of model training and inference. The competitive ratio of ORRIC outperforms that of the traditional Inference-Only paradigm, especially when data drift persists for a sufficiently lengthy time. This highlights the advantages and applicable scenarios of colocating model retraining and inference. Notably, ORRIC can be translated into several heuristic algorithms for different resource environments. Experiments conducted in real scenarios validate the effectiveness of ORRIC.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This paper has been accepted by the IEEE INFOCOM 2024 Main Conference"
    },
    {
        "paper id": "2405.16039",
        "abstract url": "https://arxiv.org/abs/2405.16039",
        "title": "MoEUT: Mixture-of-Experts Universal Transformers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced \"moot\"), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17479",
        "abstract url": "https://arxiv.org/abs/2405.17479",
        "title": "A rationale from frequency perspective for grokking in training neural network",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Grokking is the phenomenon where neural networks NNs initially fit the training data and later generalize to the test data during training. In this paper, we empirically provide a frequency perspective to explain the emergence of this phenomenon in NNs. The core insight is that the networks initially learn the less salient frequency components present in the test data. We observe this phenomenon across both synthetic and real datasets, offering a novel viewpoint for elucidating the grokking phenomenon by characterizing it through the lens of frequency dynamics during the training process. Our empirical frequency-based analysis sheds new light on understanding the grokking phenomenon and its underlying mechanisms.",
        "subjects": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17481",
        "abstract url": "https://arxiv.org/abs/2405.17481",
        "title": "Improving Simulation Regression Efficiency using a Machine Learning-based Method in Design Verification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The verification throughput is becoming a major challenge bottleneck, since the complexity and size of SoC designs are still ever increasing. Simply adding more CPU cores and running more tests in parallel will not scale anymore. This paper discusses various methods of improving verification throughput: ranking and the new machine learning (ML) based technology introduced by Cadence i.e. Xcelium ML. Both methods aim at getting comparable coverage in less CPU time by applying more efficient stimulus. Ranking selects specific seeds that simply turned out to come up with the largest coverage in previous simulations, while Xcelium ML generates optimized patterns as a result of finding correlations between randomization points and achieved coverage of previous regressions. Quantified results as well as pros & cons of each approach are discussed in this paper at the example of three actual industry projects. Both Xcelium ML and Ranking methods gave comparable compression & speedup factors around 3 consistently. But the optimized ML based regressions simulated new random scenarios occasionally producing a coverage regain of more than 100%. Finally, a methodology is proposed to use Xcelium ML efficiently throughout the product development.",
        "subjects": [
            "cs.LG",
            "cs.AR"
        ],
        "comment": "Published in DVCon Europe 2022"
    },
    {
        "paper id": "2405.17485",
        "abstract url": "https://arxiv.org/abs/2405.17485",
        "title": "$\\textit{Comet:}$ A $\\underline{Com}$munication-$\\underline{e}$fficient and Performant Approxima$\\underline{t}$ion for Private Transformer Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The prevalent use of Transformer-like models, exemplified by ChatGPT in modern language processing applications, underscores the critical need for enabling private inference essential for many cloud-based services reliant on such models. However, current privacy-preserving frameworks impose significant communication burden, especially for non-linear computation in Transformer model. In this paper, we introduce a novel plug-in method Comet to effectively reduce the communication cost without compromising the inference performance. We second introduce an efficient approximation method to eliminate the heavy communication in finding good initial approximation. We evaluate our Comet on Bert and RoBERTa models with GLUE benchmark datasets, showing up to 3.9$\\times$ less communication and 3.5$\\times$ speedups while keep competitive model performance compared to the prior art.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17489",
        "abstract url": "https://arxiv.org/abs/2405.17489",
        "title": "On the Inflation of KNN-Shapley Value",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Shapley value-based data valuation methods, originating from cooperative game theory, quantify the usefulness of each individual sample by considering its contribution to all possible training subsets. Despite their extensive applications, these methods encounter the challenge of value inflation - while samples with negative Shapley values are detrimental, some with positive values can also be harmful. This challenge prompts two fundamental questions: the suitability of zero as a threshold for distinguishing detrimental from beneficial samples and the determination of an appropriate threshold. To address these questions, we focus on KNN-Shapley and propose Calibrated KNN-Shapley (CKNN-Shapley), which calibrates zero as the threshold to distinguish detrimental samples from beneficial ones by mitigating the negative effects of small-sized training subsets. Through extensive experiments, we demonstrate the effectiveness of CKNN-Shapley in alleviating data valuation inflation, detecting detrimental samples, and assessing data quality. We also extend our approach beyond conventional classification settings, applying it to diverse and practical scenarios such as learning with mislabeled data, online learning with stream data, and active learning for label annotation.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15216",
        "abstract url": "https://arxiv.org/abs/2405.15216",
        "title": "Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition",
        "rating": "0",
        "keywords": [
            [
                "synthesize"
            ],
            [
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Language models (LMs) have long been used to improve results of automatic speech recognition (ASR) systems, but they are unaware of the errors that ASR systems make. Error correction models are designed to fix ASR errors, however, they showed little improvement over traditional LMs mainly due to the lack of supervised training data. In this paper, we present Denoising LM (DLM), which is a $\\textit{scaled}$ error correction model trained with vast amounts of synthetic data, significantly exceeding prior attempts meanwhile achieving new state-of-the-art ASR performance. We use text-to-speech (TTS) systems to synthesize audio, which is fed into an ASR system to produce noisy hypotheses, which are then paired with the original texts to train the DLM. DLM has several $\\textit{key ingredients}$: (i) up-scaled model and data; (ii) usage of multi-speaker TTS systems; (iii) combination of multiple noise augmentation strategies; and (iv) new decoding techniques. With a Transformer-CTC ASR, DLM achieves 1.5% word error rate (WER) on $\\textit{test-clean}$ and 3.3% WER on $\\textit{test-other}$ on Librispeech, which to our knowledge are the best reported numbers in the setting where no external audio data are used and even match self-supervised methods which use external audio data. Furthermore, a single DLM is applicable to different ASRs, and greatly surpassing the performance of conventional LM based beam-search rescoring. These results indicate that properly investigated error correction models have the potential to replace conventional LMs, holding the key to a new level of accuracy in ASR systems.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "under review"
    },
    {
        "paper id": "2405.15217",
        "abstract url": "https://arxiv.org/abs/2405.15217",
        "title": "NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The success of denoising diffusion models in representing rich data distributions over 2D raster images has prompted research on extending them to other data representations, such as vector graphics. Unfortunately due to their variable structure and scarcity of vector training data, directly applying diffusion models on this domain remains a challenging problem. Using workarounds like optimization via Score Distillation Sampling (SDS) is also fraught with difficulty, as vector representations are non trivial to directly optimize and tend to result in implausible geometries such as redundant or self-intersecting shapes. NIVeL addresses these challenges by reinterpreting the problem on an alternative, intermediate domain which preserves the desirable properties of vector graphics -- mainly sparsity of representation and resolution-independence. This alternative domain is based on neural implicit fields expressed in a set of decomposable, editable layers. Based on our experiments, NIVeL produces text-to-vector graphics results of significantly better quality than the state-of-the-art.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15232",
        "abstract url": "https://arxiv.org/abs/2405.15232",
        "title": "DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple and effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and another well-known benchmark, POPE, for object hallucination. Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2405.15269",
        "abstract url": "https://arxiv.org/abs/2405.15269",
        "title": "BDetCLIP: Multimodal Prompting Contrastive Test-Time Backdoor Detection",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal contrastive learning methods (e.g., CLIP) have shown impressive zero-shot classification performance due to their strong ability to joint representation learning for visual and textual modalities. However, recent research revealed that multimodal contrastive learning on poisoned pre-training data with a small proportion of maliciously backdoored data can induce backdoored CLIP that could be attacked by inserted triggers in downstream tasks with a high success rate. To defend against backdoor attacks on CLIP, existing defense methods focus on either the pre-training stage or the fine-tuning stage, which would unfortunately cause high computational costs due to numerous parameter updates. In this paper, we provide the first attempt at a computationally efficient backdoor detection method to defend against backdoored CLIP in the inference stage. We empirically find that the visual representations of backdoored images are insensitive to both benign and malignant changes in class description texts. Motivated by this observation, we propose BDetCLIP, a novel test-time backdoor detection method based on contrastive prompting. Specifically, we first prompt the language model (e.g., GPT-4) to produce class-related description texts (benign) and class-perturbed random texts (malignant) by specially designed instructions. Then, the distribution difference in cosine similarity between images and the two types of class description texts can be used as the criterion to detect backdoor samples. Extensive experiments validate that our proposed BDetCLIP is superior to state-of-the-art backdoor detection methods, in terms of both effectiveness and efficiency.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15287",
        "abstract url": "https://arxiv.org/abs/2405.15287",
        "title": "StyleMaster: Towards Flexible Stylized Image Generation with Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Stylized Text-to-Image Generation (STIG) aims to generate images based on text prompts and style reference images. We in this paper propose a novel framework dubbed as StyleMaster for this task by leveraging pretrained Stable Diffusion (SD), which tries to solve the previous problems such as insufficient style and inconsistent semantics. The enhancement lies in two novel module, namely multi-source style embedder and dynamic attention adapter. In order to provide SD with better style embeddings, we propose the multi-source style embedder considers both global and local level visual information along with textual one, which provide both complementary style-related and semantic-related knowledge. Additionally, aiming for better balance between the adaptor capacity and semantic control, the proposed dynamic attention adapter is applied to the diffusion UNet in which adaptation weights are dynamically calculated based on the style embeddings. Two objective functions are introduced to optimize the model together with denoising loss, which can further enhance semantic and style consistency. Extensive experiments demonstrate the superiority of StyleMaster over existing methods, rendering images with variable target styles while successfully maintaining the semantic information from the text prompts.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15299",
        "abstract url": "https://arxiv.org/abs/2405.15299",
        "title": "Transparent Object Depth Completion",
        "rating": "0",
        "keywords": [
            [
                "RGB-D",
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The perception of transparent objects for grasp and manipulation remains a major challenge, because existing robotic grasp methods which heavily rely on depth maps are not suitable for transparent objects due to their unique visual properties. These properties lead to gaps and inaccuracies in the depth maps of the transparent objects captured by depth sensors. To address this issue, we propose an end-to-end network for transparent object depth completion that combines the strengths of single-view RGB-D based depth completion and multi-view depth estimation. Moreover, we introduce a depth refinement module based on confidence estimation to fuse predicted depth maps from single-view and multi-view modules, which further refines the restored depth map. The extensive experiments on the ClearPose and TransCG datasets demonstrate that our method achieves superior accuracy and robustness in complex scenarios with significant occlusion compared to the state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15305",
        "abstract url": "https://arxiv.org/abs/2405.15305",
        "title": "Diff3DS: Generating View-Consistent 3D Sketch via Differentiable Curve Rendering",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D sketches are widely used for visually representing the 3D shape and structure of objects or scenes. However, the creation of 3D sketch often requires users to possess professional artistic skills. Existing research efforts primarily focus on enhancing the ability of interactive sketch generation in 3D virtual systems. In this work, we propose Diff3DS, a novel differentiable rendering framework for generating view-consistent 3D sketch by optimizing 3D parametric curves under various supervisions. Specifically, we perform perspective projection to render the 3D rational B\u00e9zier curves into 2D curves, which are subsequently converted to a 2D raster image via our customized differentiable rasterizer. Our framework bridges the domains of 3D sketch and raster image, achieving end-toend optimization of 3D sketch through gradients computed in the 2D image domain. Our Diff3DS can enable a series of novel 3D sketch generation tasks, including textto-3D sketch and image-to-3D sketch, supported by the popular distillation-based supervision, such as Score Distillation Sampling (SDS). Extensive experiments have yielded promising results and demonstrated the potential of our framework.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project: https://yiboz2001.github.io/Diff3DS/"
    },
    {
        "paper id": "2405.15306",
        "abstract url": "https://arxiv.org/abs/2405.15306",
        "title": "DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ",
        "rating": "0",
        "keywords": [
            [
                "Synthesizing"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTikZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and SciCap++, a collection of diverse scientific figures and associated metadata. We train DeTikZify on SciCap++ and DaTikZv2, along with synthetically generated sketches learned from SketchFig. We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": "Project page: https://github.com/potamides/DeTikZify"
    },
    {
        "paper id": "2405.15307",
        "abstract url": "https://arxiv.org/abs/2405.15307",
        "title": "Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation",
        "rating": "0",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) driven by In-Context Learning (ICL) have significantly improved the performance of text-to-SQL. Previous methods generally employ a two-stage reasoning framework, namely 1) schema linking and 2) logical synthesis, making the framework not only effective but also interpretable. Despite these advancements, the inherent bad nature of the generalization of LLMs often results in hallucinations, which limits the full potential of LLMs. In this work, we first identify and categorize the common types of hallucinations at each stage in text-to-SQL. We then introduce a novel strategy, Task Alignment (TA), designed to mitigate hallucinations at each stage. TA encourages LLMs to take advantage of experiences from similar tasks rather than starting the tasks from scratch. This can help LLMs reduce the burden of generalization, thereby mitigating hallucinations effectively. We further propose TA-SQL, a text-to-SQL framework based on this strategy. The experimental results and comprehensive analysis demonstrate the effectiveness and robustness of our framework. Specifically, it enhances the performance of the GPT-4 baseline by 21.23% relatively on BIRD dev and it yields significant improvements across six models and four mainstream, complex text-to-SQL benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to ACL Findings 2024"
    },
    {
        "paper id": "2405.15313",
        "abstract url": "https://arxiv.org/abs/2405.15313",
        "title": "Enhancing Text-to-Image Editing via Hybrid Mask-Informed Fusion",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "Image Editing",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, text-to-image (T2I) editing has been greatly pushed forward by applying diffusion models. Despite the visual promise of the generated images, inconsistencies with the expected textual prompt remain prevalent. This paper aims to systematically improve the text-guided image editing techniques based on diffusion models, by addressing their limitations. Notably, the common idea in diffusion-based editing firstly reconstructs the source image via inversion techniques e.g., DDIM Inversion. Then following a fusion process that carefully integrates the source intermediate (hidden) states (obtained by inversion) with the ones of the target image. Unfortunately, such a standard pipeline fails in many cases due to the interference of texture retention and the new characters creation in some regions. To mitigate this, we incorporate human annotation as an external knowledge to confine editing within a ``Mask-informed'' region. Then we carefully Fuse the edited image with the source image and a constructed intermediate image within the model's Self-Attention module. Extensive empirical results demonstrate the proposed ``MaSaFusion'' significantly improves the existing T2I editing techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15324",
        "abstract url": "https://arxiv.org/abs/2405.15324",
        "title": "Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving",
        "rating": "0",
        "keywords": [
            [
                "Autonomous Driving"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Autonomous driving has advanced significantly due to sensors, machine learning, and artificial intelligence improvements. However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments. To address the above problems, we introduce LeapAD, a novel paradigm for autonomous driving inspired by the human cognitive process. Specifically, LeapAD emulates human attention by selecting critical objects relevant to driving decisions, simplifying environmental interpretation, and mitigating decision-making complexities. Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing. The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning. Through reflection mechanisms and a growing memory bank, LeapAD continuously improves itself from past mistakes in a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD outperforms all methods relying solely on camera input, requiring 1-2 orders of magnitude less labeled data. Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement. Code will be released at https://github.com/PJLab-ADG/LeapAD.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "23 pages, 16 figures"
    },
    {
        "paper id": "2405.15330",
        "abstract url": "https://arxiv.org/abs/2405.15330",
        "title": "Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Recently, the strong latent Diffusion Probabilistic Model (DPM) has been applied to high-quality Text-to-Image (T2I) generation (e.g., Stable Diffusion), by injecting the encoded target text prompt into the gradually denoised diffusion image generator. Despite the success of DPM in practice, the mechanism behind it remains to be explored. To fill this blank, we begin by examining the intermediate statuses during the gradual denoising generation process in DPM. The empirical observations indicate, the shape of image is reconstructed after the first few denoising steps, and then the image is filled with details (e.g., texture). The phenomenon is because the low-frequency signal (shape relevant) of the noisy image is not corrupted until the final stage in the forward process (initial stage of generation) of adding noise in DPM. Inspired by the observations, we proceed to explore the influence of each token in the text prompt during the two stages. After a series of experiments of T2I generations conditioned on a set of text prompts. We conclude that in the earlier generation stage, the image is mostly decided by the special token [\\texttt{EOS}] in the text prompt, and the information in the text prompt is already conveyed in this stage. After that, the diffusion model completes the details of generated images by information from themselves. Finally, we propose to apply this observation to accelerate the process of T2I generation by properly removing text guidance, which finally accelerates the sampling up to 25\\%+.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15338",
        "abstract url": "https://arxiv.org/abs/2405.15338",
        "title": "SoundLoCD: An Efficient Conditional Discrete Contrastive Latent Diffusion Model for Text-to-Sound Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "We present SoundLoCD, a novel text-to-sound generation framework, which incorporates a LoRA-based conditional discrete contrastive latent diffusion model. Unlike recent large-scale sound generation models, our model can be efficiently trained under limited computational resources. The integration of a contrastive learning strategy further enhances the connection between text conditions and the generated outputs, resulting in coherent and high-fidelity performance. Our experiments demonstrate that SoundLoCD outperforms the baseline with greatly reduced computational resources. A comprehensive ablation study further validates the contribution of each component within SoundLoCD. Demo page: \\url{https://XinleiNIU.github.io/demo-SoundLoCD/}.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15343",
        "abstract url": "https://arxiv.org/abs/2405.15343",
        "title": "Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The development of AI-Generated Content (AIGC) has empowered the creation of remarkably realistic AI-generated videos, such as those involving Sora. However, the widespread adoption of these models raises concerns regarding potential misuse, including face video scams and copyright disputes. Addressing these concerns requires the development of robust tools capable of accurately determining video authenticity. The main challenges lie in the dataset and neural classifier for training. Current datasets lack a varied and comprehensive repository of real and generated content for effective discrimination. In this paper, we first introduce an extensive video dataset designed specifically for AI-Generated Video Detection (GenVidDet). It includes over 2.66 M instances of both real and generated videos, varying in categories, frames per second, resolutions, and lengths. The comprehensiveness of GenVidDet enables the training of a generalizable video detector. We also present the Dual-Branch 3D Transformer (DuB3D), an innovative and effective method for distinguishing between real and generated videos, enhanced by incorporating motion information alongside visual appearance. DuB3D utilizes a dual-branch architecture that adaptively leverages and fuses raw spatio-temporal data and optical flow. We systematically explore the critical factors affecting detection performance, achieving the optimal configuration for DuB3D. Trained on GenVidDet, DuB3D can distinguish between real and generated video content with 96.77% accuracy, and strong generalization capability even for unseen types.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15349",
        "abstract url": "https://arxiv.org/abs/2405.15349",
        "title": "UnKE: Unstructured Knowledge Editing in Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent knowledge editing methods have primarily focused on modifying structured knowledge in large language models, heavily relying on the assumption that structured knowledge is stored as key-value pairs locally in MLP layers or specific neurons. However, this task setting overlooks the fact that a significant portion of real-world knowledge is stored in an unstructured format, characterized by long-form content, noise, and a complex yet comprehensive nature. The \"knowledge locating\" and \"term-driven optimization\" techniques conducted from the assumption used in previous methods (e.g., MEMIT) are ill-suited for unstructured knowledge. To address these challenges, we propose a novel unstructured knowledge editing method, namely UnKE, which extends previous assumptions in the layer dimension and token dimension. Firstly, in the layer dimension, we discard the \"knowledge locating\" step and treat first few layers as the key, which expand knowledge storage through layers to break the \"knowledge stored locally\" assumption. Next, we replace \"term-driven optimization\" with \"cause-driven optimization\" across all inputted tokens in the token dimension, directly optimizing the last layer of the key generator to perform editing to generate the required key vectors. By utilizing key-value pairs at the layer level, UnKE effectively represents and edits complex and comprehensive unstructured knowledge, leveraging the potential of both the MLP and attention layers. Results on newly proposed unstructure knowledge editing dataset (UnKEBench) and traditional structured datasets demonstrate that UnKE achieves remarkable performance, surpassing strong baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15364",
        "abstract url": "https://arxiv.org/abs/2405.15364",
        "title": "NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Synthesizer"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "By harnessing the potent generative capabilities of pre-trained large video diffusion models, we propose NVS-Solver, a new novel view synthesis (NVS) paradigm that operates \\textit{without} the need for training. NVS-Solver adaptively modulates the diffusion sampling process with the given views to enable the creation of remarkable visual experiences from single or multiple views of static scenes or monocular videos of dynamic scenes. Specifically, built upon our theoretical modeling, we iteratively modulate the score function with the given scene priors represented with warped input views to control the video diffusion process. Moreover, by theoretically exploring the boundary of the estimation error, we achieve the modulation in an adaptive fashion according to the view pose and the number of diffusion steps. Extensive evaluations on both static and dynamic scenes substantiate the significant superiority of our NVS-Solver over state-of-the-art methods both quantitatively and qualitatively. \\textit{ Source code in } \\href{https://github.com/ZHU-Zhiyu/NVS_Solver}{https://github.com/ZHU-Zhiyu/NVS$\\_$Solver}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Technical Report"
    },
    {
        "paper id": "2405.15370",
        "abstract url": "https://arxiv.org/abs/2405.15370",
        "title": "Large Language Models can Deliver Accurate and Interpretable Time Series Anomaly Detection",
        "rating": "0",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Time series anomaly detection (TSAD) plays a crucial role in various industries by identifying atypical patterns that deviate from standard trends, thereby maintaining system integrity and enabling prompt response measures. Traditional TSAD models, which often rely on deep learning, require extensive training data and operate as black boxes, lacking interpretability for detected anomalies. To address these challenges, we propose LLMAD, a novel TSAD method that employs Large Language Models (LLMs) to deliver accurate and interpretable TSAD results. LLMAD innovatively applies LLMs for in-context anomaly detection by retrieving both positive and negative similar time series segments, significantly enhancing LLMs' effectiveness. Furthermore, LLMAD employs the Anomaly Detection Chain-of-Thought (AnoCoT) approach to mimic expert logic for its decision-making process. This method further enhances its performance and enables LLMAD to provide explanations for their detections through versatile perspectives, which are particularly important for user decision-making. Experiments on three datasets indicate that our LLMAD achieves detection performance comparable to state-of-the-art deep learning methods while offering remarkable interpretability for detections. To the best of our knowledge, this is the first work that directly employs LLMs for TSAD.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15374",
        "abstract url": "https://arxiv.org/abs/2405.15374",
        "title": "Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The proposed research aims to develop an innovative semantic query processing system that enables users to obtain comprehensive information about research works produced by Computer Science (CS) researchers at the Australian National University (ANU). The system integrates Large Language Models (LLMs) with the ANU Scholarly Knowledge Graph (ASKG), a structured repository of all research-related artifacts produced at ANU in the CS field. Each artifact and its parts are represented as textual nodes stored in a Knowledge Graph (KG). To address the limitations of traditional scholarly KG construction and utilization methods, which often fail to capture fine-grained details, we propose a novel framework that integrates the Deep Document Model (DDM) for comprehensive document representation and the KG-enhanced Query Processing (KGQP) for optimized complex query handling. DDM enables a fine-grained representation of the hierarchical structure and semantic relationships within academic papers, while KGQP leverages the KG structure to improve query accuracy and efficiency with LLMs. By combining the ASKG with LLMs, our approach enhances knowledge utilization and natural language understanding capabilities. The proposed system employs an automatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from the ASKG. Initial experiments demonstrate that our framework is superior to baseline methods in terms of accuracy retrieval and query efficiency. We showcase the practical application of our framework in academic research scenarios, highlighting its potential to revolutionize scholarly knowledge management and discovery. This work empowers researchers to acquire and utilize knowledge from documents more effectively and provides a foundation for developing precise and reliable interactions with LLMs.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "for the associated repository, see http://w3id.org/kgcp/KGQP"
    },
    {
        "paper id": "2405.15399",
        "abstract url": "https://arxiv.org/abs/2405.15399",
        "title": "Stochastic super-resolution for Gaussian microtextures",
        "rating": "0",
        "keywords": [
            [
                "super-resolution"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Super-Resolution (SR) is the problem that consists in reconstructing images that have been degraded by a zoom-out operator. This is an ill-posed problem that does not have a unique solution, and numerical approaches rely on a prior on high-resolution images. While optimization-based methods are generally deterministic, with the rise of image generative models more and more interest has been given to stochastic SR, that is, sampling among all possible SR images associated with a given low-resolution input. In this paper, we construct an efficient, stable and provably exact sampler for the stochastic SR of Gaussian microtextures. Even though our approach is limited regarding the scope of images it encompasses, our algorithm is competitive with deep learning state-of-the-art methods both in terms of perceptual metric and execution time when applied to microtextures. The framework of Gaussian microtextures also allows us to rigorously discuss the limitations of various reconstruction metrics to evaluate the efficiency of SR routines.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15452",
        "abstract url": "https://arxiv.org/abs/2405.15452",
        "title": "Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Multi-hop Question Answering (MQA) under knowledge editing (KE) is a key challenge in Large Language Models (LLMs). While best-performing solutions in this domain use a plan and solve paradigm to split a question into sub-questions followed by response generation, we claim that this approach is sub-optimal as it fails for hard to decompose questions, and it does not explicitly cater to correlated knowledge updates resulting as a consequence of knowledge edits. This has a detrimental impact on the overall consistency of the updated knowledge. To address these issues, in this paper, we propose a novel framework named RULE-KE, i.e., RULE based Knowledge Editing, which is a cherry on the top for augmenting the performance of all existing MQA methods under KE. Specifically, RULE-KE leverages rule discovery to discover a set of logical rules. Then, it uses these discovered rules to update knowledge about facts highly correlated with the edit. Experimental evaluation using existing and newly curated datasets (i.e., RKE-EVAL) shows that RULE-KE helps augment both performances of parameter-based and memory-based solutions up to 92% and 112.9%, respectively.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2405.15454",
        "abstract url": "https://arxiv.org/abs/2405.15454",
        "title": "Linearly Controlled Language Generation with Performative Guarantees",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The increasing prevalence of Large Language Models (LMs) in critical applications highlights the need for controlled language generation strategies that are not only computationally efficient but that also enjoy performance guarantees. To achieve this, we use a common model of concept semantics as linearly represented in an LM's latent space. In particular, we take the view that natural language generation traces a trajectory in this continuous semantic space, realized by the language model's hidden activations. This view permits a control-theoretic treatment of text generation in latent space, in which we propose a lightweight, gradient-free intervention that dynamically steers trajectories away from regions corresponding to undesired meanings. Crucially, we show that this intervention, which we compute in closed form, is guaranteed (in probability) to steer the output into the allowed region. Finally, we demonstrate on a toxicity avoidance objective that the intervention steers language away from undesired content while maintaining text quality.",
        "subjects": [
            "cs.CL",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15458",
        "abstract url": "https://arxiv.org/abs/2405.15458",
        "title": "FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler",
        "rating": "0",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Federated learning (FL) enables collaborative machine learning across distributed data owners, but data heterogeneity poses a challenge for model calibration. While prior work focused on improving accuracy for non-iid data, calibration remains under-explored. This study reveals existing FL aggregation approaches lead to sub-optimal calibration, and theoretical analysis shows despite constraining variance in clients' label distributions, global calibration error is still asymptotically lower bounded. To address this, we propose a novel Federated Calibration (FedCal) approach, emphasizing both local and global calibration. It leverages client-specific scalers for local calibration to effectively correct output misalignment without sacrificing prediction accuracy. These scalers are then aggregated via weight averaging to generate a global scaler, minimizing the global calibration error. Extensive experiments demonstrate FedCal significantly outperforms the best-performing baseline, reducing global calibration error by 47.66% on average.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "This paper has been accepted by ICML'24"
    },
    {
        "paper id": "2405.15465",
        "abstract url": "https://arxiv.org/abs/2405.15465",
        "title": "Scale-Invariant Feature Disentanglement via Adversarial Learning for UAV-based Object Detection",
        "rating": "0",
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "UAV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting objects from Unmanned Aerial Vehicles (UAV) is often hindered by a large number of small objects, resulting in low detection accuracy. To address this issue, mainstream approaches typically utilize multi-stage inferences. Despite their remarkable detecting accuracies, real-time efficiency is sacrificed, making them less practical to handle real applications. To this end, we propose to improve the single-stage inference accuracy through learning scale-invariant features. Specifically, a Scale-Invariant Feature Disentangling module is designed to disentangle scale-related and scale-invariant features. Then an Adversarial Feature Learning scheme is employed to enhance disentanglement. Finally, scale-invariant features are leveraged for robust UAV-based object detection. Furthermore, we construct a multi-modal UAV object detection dataset, State-Air, which incorporates annotated UAV state parameters. We apply our approach to three state-of-the-art lightweight detection frameworks on three benchmark datasets, including State-Air. Extensive experiments demonstrate that our approach can effectively improve model accuracy. Our code and dataset are provided in Supplementary Materials and will be publicly available once the paper is accepted.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15491",
        "abstract url": "https://arxiv.org/abs/2405.15491",
        "title": "GSDeformer: Direct Cage-based Deformation for 3D Gaussian Splatting",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present GSDeformer, a method that achieves free-form deformation on 3D Gaussian Splatting(3DGS) without requiring any architectural changes. Our method extends cage-based deformation, a traditional mesh deformation method, to 3DGS. This is done by converting 3DGS into a novel proxy point cloud representation, where its deformation can be used to infer the transformations to apply on the 3D gaussians making up 3DGS. We also propose an automatic cage construction algorithm for 3DGS to minimize manual work. Our method does not modify the underlying architecture of 3DGS. Therefore, any existing trained vanilla 3DGS can be easily edited by our method. We compare the deformation capability of our method against other existing methods, demonstrating the ease of use and comparable quality of our method, despite being more direct and thus easier to integrate with other concurrent developments on 3DGS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "For project page, see https://jhuangbu.github.io/gsdeformer"
    },
    {
        "paper id": "2405.15519",
        "abstract url": "https://arxiv.org/abs/2405.15519",
        "title": "Confocal structured illumination microscopy",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Confocal microscopy, a critical advancement in optical imaging, is widely applied because of its excellent anti-noise ability. However, it has low imaging efficiency and can cause phototoxicity. Optical-sectioning structured illumination microscopy (OS-SIM) can overcome the limitations of confocal microscopy but still face challenges in imaging depth and signal-to-noise ratio (SNR). We introduce the concept of confocal imaging into OS-SIM and propose confocal structured illumination microscopy (CSIM) to enhance the imaging performance of OS-SIM. CSIM exploits the principle of dual photography to reconstruct a dual image from each pixel of the camera. The reconstructed dual image is equivalent to the image obtained by using the spatial light modulator (SLM) as a virtual camera, enabling the separation of the conjugate and non-conjugate signals recorded by the camera pixel. We can reject the non-conjugate signals by extracting the conjugate signal from each dual image to reconstruct a confocal image when establishing the conjugate relationship between the camera and the SLM. We have constructed the theoretical framework of CSIM. Optical-sectioning experimental results demonstrate that CSIM can reconstruct images with superior SNR and greater imaging depth compared with existing OS-SIM. CSIM is expected to expand the application scope of OS-SIM.",
        "subjects": [
            "physics.optics",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15523",
        "abstract url": "https://arxiv.org/abs/2405.15523",
        "title": "Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The immense datasets used to develop Large Language Models (LLMs) often include copyright-protected content, typically without the content creator's consent. Copyright traps have been proposed to be injected into the original content, improving content detectability in newly released LLMs. Traps, however, rely on the exact duplication of a unique text sequence, leaving them vulnerable to commonly deployed data deduplication techniques. We here propose the generation of fuzzy copyright traps, featuring slight modifications across duplication. When injected in the fine-tuning data of a 1.3B LLM, we show fuzzy trap sequences to be memorized nearly as well as exact duplicates. Specifically, the Membership Inference Attack (MIA) ROC AUC only drops from 0.90 to 0.87 when 4 tokens are replaced across the fuzzy duplicates. We also find that selecting replacement positions to minimize the exact overlap between fuzzy duplicates leads to similar memorization, while making fuzzy duplicates highly unlikely to be removed by any deduplication process. Lastly, we argue that the fact that LLMs memorize across fuzzy duplicates challenges the study of LLM memorization relying on naturally occurring duplicates. Indeed, we find that the commonly used training dataset, The Pile, contains significant amounts of fuzzy duplicates. This introduces a previously unexplored confounding factor in post-hoc studies of LLM memorization, and questions the effectiveness of (exact) data deduplication as a privacy protection technique.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15556",
        "abstract url": "https://arxiv.org/abs/2405.15556",
        "title": "Certifiably Robust RAG against Retrieval Corruption",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. In this paper, we propose RobustRAG as the first defense framework against retrieval corruption attacks. The key insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM responses from each passage in isolation and then securely aggregate these isolated responses. To instantiate RobustRAG, we design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. Notably, RobustRAG can achieve certifiable robustness: we can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when the attacker has full knowledge of our defense and can arbitrarily inject a small number of malicious passages. We evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability across various tasks and datasets.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15622",
        "abstract url": "https://arxiv.org/abs/2405.15622",
        "title": "LAM3D: Large Image-Point-Cloud Alignment Model for 3D Reconstruction from Single Image",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images. Despite their success, these models often produce 3D meshes with geometric inaccuracies, stemming from the inherent challenges of deducing 3D shapes solely from image data. In this work, we introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes. Our methodology begins with the development of a point-cloud-based network that effectively generates precise and meaningful latent tri-planes, laying the groundwork for accurate 3D mesh reconstruction. Building upon this, our Image-Point-Cloud Feature Alignment technique processes a single input image, aligning to the latent tri-planes to imbue image features with robust 3D information. This process not only enriches the image features but also facilitates the production of high-fidelity 3D meshes without the need for multi-view input, significantly reducing geometric distortions. Our approach achieves state-of-the-art high-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "19 pages, 10 figures"
    },
    {
        "paper id": "2405.15636",
        "abstract url": "https://arxiv.org/abs/2405.15636",
        "title": "Visualize and Paint GAN Activations",
        "rating": "0",
        "keywords": [
            [
                "GAN"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We investigate how generated structures of GANs correlate with their activations in hidden layers, with the purpose of better understanding the inner workings of those models and being able to paint structures with unconditionally trained GANs. This gives us more control over the generated images, allowing to generate them from a semantic segmentation map while not requiring such a segmentation in the training data. To this end we introduce the concept of tileable features, allowing us to identify activations that work well for painting.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15677",
        "abstract url": "https://arxiv.org/abs/2405.15677",
        "title": "SMART: Scalable Multi-agent Real-time Simulation via Next-token Prediction",
        "rating": "0",
        "keywords": [
            [
                "autonomous driving",
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Data-driven autonomous driving motion generation tasks are frequently impacted by the limitations of dataset size and the domain gap between datasets, which precludes their extensive application in real-world scenarios. To address this issue, we introduce SMART, a novel autonomous driving motion generation paradigm that models vectorized map and agent trajectory data into discrete sequence tokens. These tokens are then processed through a decoder-only transformer architecture to train for the next token prediction task across spatial-temporal series. This GPT-style method allows the model to learn the motion distribution in real driving scenarios. SMART achieves state-of-the-art performance across most of the metrics on the generative Sim Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset (WOMD), demonstrating remarkable inference speed. Moreover, SMART represents the generative model in the autonomous driving motion domain, exhibiting zero-shot generalization capabilities: Using only the NuPlan dataset for training and WOMD for validation, SMART achieved a competitive score of 0.71 on the Sim Agents challenge. Lastly, we have collected over 1 billion motion tokens from multiple datasets, validating the model's scalability. These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications. We have released all the code to promote the exploration of models for motion generation in the autonomous driving field.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15728",
        "abstract url": "https://arxiv.org/abs/2405.15728",
        "title": "Disease-informed Adaptation of Vision-Language Models",
        "rating": "0",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "medical",
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In medical image analysis, the expertise scarcity and the high cost of data annotation limits the development of large artificial intelligence models. This paper investigates the potential of transfer learning with pre-trained vision-language models (VLMs) in this domain. Currently, VLMs still struggle to transfer to the underrepresented diseases with minimal presence and new diseases entirely absent from the pretraining dataset. We argue that effective adaptation of VLMs hinges on the nuanced representation learning of disease concepts. By capitalizing on the joint visual-linguistic capabilities of VLMs, we introduce disease-informed contextual prompting in a novel disease prototype learning framework. This approach enables VLMs to grasp the concepts of new disease effectively and efficiently, even with limited data. Extensive experiments across multiple image modalities showcase notable enhancements in performance compared to existing techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Early Accepted by MICCAI 2024"
    },
    {
        "paper id": "2405.15757",
        "abstract url": "https://arxiv.org/abs/2405.15757",
        "title": "Looking Backward: Streaming Video-to-Video Translation with Feature Banks",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames. At the heart of StreamV2V lies a backward-looking principle that relates the present to the past. This is realized by maintaining a feature bank, which archives information from past frames. For incoming frames, StreamV2V extends self-attention to include banked keys and values and directly fuses similar past features into the output. The feature bank is continually updated by merging stored and new features, making it compact but informative. StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative metrics and user studies confirm StreamV2V's exceptional ability to maintain temporal consistency.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": "Project page: https://jeff-liangf.github.io/projects/streamv2v"
    },
    {
        "paper id": "2405.15763",
        "abstract url": "https://arxiv.org/abs/2405.15763",
        "title": "FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis",
        "rating": "0",
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-to-motion synthesis is a crucial task in computer vision. Existing methods are limited in their universality, as they are tailored for single-person or two-person scenarios and can not be applied to generate motions for more individuals. To achieve the number-free motion synthesis, this paper reconsiders motion generation and proposes to unify the single and multi-person motion by the conditional motion distribution. Furthermore, a generation module and an interaction module are designed for our FreeMotion framework to decouple the process of conditional motion generation and finally support the number-free motion synthesis. Besides, based on our framework, the current single-person motion spatial control method could be seamlessly integrated, achieving precise control of multi-person motion. Extensive experiments demonstrate the superior performance of our method and our capability to infer single and multi-human motions simultaneously.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15766",
        "abstract url": "https://arxiv.org/abs/2405.15766",
        "title": "Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development",
        "rating": "0",
        "keywords": [
            [
                "VLMs"
            ],
            [
                "biomedical",
                "Medical",
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance, enhancing patient safety by identifying potential risks associated with medications, facilitating early detection of adverse events, and guiding regulatory decision-making. Traditional ADE detection methods are reliable but slow, not easily adaptable to large-scale operations, and offer limited information. With the exponential increase in data sources like social media content, biomedical literature, and Electronic Medical Records (EMR), extracting relevant ADE-related information from these unstructured texts is imperative. Previous ADE mining studies have focused on text-based methodologies, overlooking visual cues, limiting contextual comprehension, and hindering accurate interpretation. To address this gap, we present a MultiModal Adverse Drug Event (MMADE) detection dataset, merging ADE-related textual information with visual aids. Additionally, we introduce a framework that leverages the capabilities of LLMs and VLMs for ADE detection by generating detailed descriptions of medical images depicting ADEs, aiding healthcare professionals in visually identifying adverse events. Using our MMADE dataset, we showcase the significance of integrating visual cues from images to enhance overall performance. This approach holds promise for patient safety, ADE awareness, and healthcare accessibility, paving the way for further exploration in personalized healthcare.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "comment": "ACL Findings 2024"
    },
    {
        "paper id": "2405.15769",
        "abstract url": "https://arxiv.org/abs/2405.15769",
        "title": "FastDrag: Manipulate Anything in One Step",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "image editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Drag-based image editing using generative models provides precise control over image contents, enabling users to manipulate anything in an image with a few clicks. However, prevailing methods typically adopt $n$-step iterations for latent semantic optimization to achieve drag-based image editing, which is time-consuming and limits practical applications. In this paper, we introduce a novel one-step drag-based image editing method, i.e., FastDrag, to accelerate the editing process. Central to our approach is a latent warpage function (LWF), which simulates the behavior of a stretched material to adjust the location of individual pixels within the latent space. This innovation achieves one-step latent semantic optimization and hence significantly promotes editing speeds. Meanwhile, null regions emerging after applying LWF are addressed by our proposed bilateral nearest neighbor interpolation (BNNI) strategy. This strategy interpolates these regions using similar features from neighboring areas, thus enhancing semantic integrity. Additionally, a consistency-preserving strategy is introduced to maintain the consistency between the edited and original images by adopting semantic information from the original image, saved as key and value pairs in self-attention module during diffusion inversion, to guide the diffusion sampling. Our FastDrag is validated on the DragBench dataset, demonstrating substantial improvements in processing time over existing methods, while achieving enhanced editing performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2405.15863",
        "abstract url": "https://arxiv.org/abs/2405.15863",
        "title": "Quality-aware Masked Diffusion Transformer for Enhanced Music Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "synthesizing"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "In recent years, diffusion-based text-to-music (TTM) generation has gained prominence, offering a novel approach to synthesizing musical content from textual descriptions. Achieving high accuracy and diversity in this generation process requires extensive, high-quality data, which often constitutes only a fraction of available datasets. Within open-source datasets, the prevalence of issues like mislabeling, weak labeling, unlabeled data, and low-quality music waveform significantly hampers the development of music generation models. To overcome these challenges, we introduce a novel quality-aware masked diffusion transformer (QA-MDT) approach that enables generative models to discern the quality of input music waveform during training. Building on the unique properties of musical signals, we have adapted and implemented a MDT model for TTM task, while further unveiling its distinct capacity for quality control. Moreover, we address the issue of low-quality captions with a caption refinement data processing approach. Our demo page is shown in https://qa-mdt.github.io/. Code on https://github.com/ivcylc/qa-mdt",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15881",
        "abstract url": "https://arxiv.org/abs/2405.15881",
        "title": "Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In recent developments, the Mamba architecture, known for its selective state space approach, has shown potential in the efficient modeling of long sequences. However, its application in image generation remains underexplored. Traditional diffusion transformers (DiT), which utilize self-attention blocks, are effective but their computational complexity scales quadratically with the input length, limiting their use for high-resolution images. To address this challenge, we introduce a novel diffusion architecture, Diffusion Mamba (DiM), which foregoes traditional attention mechanisms in favor of a scalable alternative. By harnessing the inherent efficiency of the Mamba architecture, DiM achieves rapid inference times and reduced computational load, maintaining linear complexity with respect to sequence length. Our architecture not only scales effectively but also outperforms existing diffusion transformers in both image and video generation tasks. The results affirm the scalability and efficiency of DiM, establishing a new benchmark for image and video generation techniques. This work advances the field of generative models and paves the way for further applications of scalable architectures.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15916",
        "abstract url": "https://arxiv.org/abs/2405.15916",
        "title": "Recasting Generic Pretrained Vision Transformers As Object-Centric Scene Encoders For Manipulation Policies",
        "rating": "0",
        "keywords": [
            [
                "robotics",
                "robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generic re-usable pre-trained image representation encoders have become a standard component of methods for many computer vision tasks. As visual representations for robots however, their utility has been limited, leading to a recent wave of efforts to pre-train robotics-specific image encoders that are better suited to robotic tasks than their generic counterparts. We propose Scene Objects From Transformers, abbreviated as SOFT, a wrapper around pre-trained vision transformer (PVT) models that bridges this gap without any further training. Rather than construct representations out of only the final layer activations, SOFT individuates and locates object-like entities from PVT attentions, and describes them with PVT activations, producing an object-centric embedding. Across standard choices of generic pre-trained vision transformers PVT, we demonstrate in each case that policies trained on SOFT(PVT) far outstrip standard PVT representations for manipulation tasks in simulated and real settings, approaching the state-of-the-art robotics-aware representations. Code, appendix and videos: https://sites.google.com/view/robot-soft/",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Accepted to International Conference on Robotics and Automation(ICRA) 2024"
    },
    {
        "paper id": "2405.15984",
        "abstract url": "https://arxiv.org/abs/2405.15984",
        "title": "Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "With the emergence of large language models, such as LLaMA and OpenAI GPT-3, In-Context Learning (ICL) gained significant attention due to its effectiveness and efficiency. However, ICL is very sensitive to the choice, order, and verbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented ICL methods try to address this problem by leveraging retrievers to extract semantically related examples as demonstrations. While this approach yields more accurate results, its robustness against various types of adversarial attacks, including perturbations on test samples, demonstrations, and retrieved data, remains under-explored. Our study reveals that retrieval-augmented models can enhance robustness against test sample attacks, outperforming vanilla ICL with a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit overconfidence in the demonstrations, leading to a 2% increase in ASR for demonstration attacks. Adversarial training can help improve the robustness of ICL methods to adversarial attacks; however, such a training scheme can be too costly in the context of LLMs. As an alternative, we introduce an effective training-free adversarial defence method, DARD, which enriches the example pool with those attacked samples. We show that DARD yields improvements in performance and robustness, achieving a 15% reduction in ASR over the baselines. Code and data are released to encourage further research: https://github.com/simonucl/adv-retreival-icl",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "29 pages, 6 figures"
    },
    {
        "paper id": "2405.16005",
        "abstract url": "https://arxiv.org/abs/2405.16005",
        "title": "PTQ4DiT: Post-training Quantization for Diffusion Transformers",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The recent introduction of Diffusion Transformers (DiTs) has demonstrated exceptional capabilities in image generation by using a different backbone architecture, departing from traditional U-Nets and embracing the scalable nature of transformers. Despite their advanced capabilities, the wide deployment of DiTs, particularly for real-time applications, is currently hampered by considerable computational demands at the inference stage. Post-training Quantization (PTQ) has emerged as a fast and data-efficient solution that can significantly reduce computation and memory footprint by using low-bit weights and activations. However, its applicability to DiTs has not yet been explored and faces non-trivial difficulties due to the unique design of DiTs. In this paper, we propose PTQ4DiT, a specifically designed PTQ method for DiTs. We discover two primary quantization challenges inherent in DiTs, notably the presence of salient channels with extreme magnitudes and the temporal variability in distributions of salient activation over multiple timesteps. To tackle these challenges, we propose Channel-wise Salience Balancing (CSB) and Spearmen's $\u03c1$-guided Salience Calibration (SSC). CSB leverages the complementarity property of channel magnitudes to redistribute the extremes, alleviating quantization errors for both activations and weights. SSC extends this approach by dynamically adjusting the balanced salience to capture the temporal variations in activation. Additionally, to eliminate extra computational costs caused by PTQ4DiT during inference, we design an offline re-parameterization strategy for DiTs. Experiments demonstrate that our PTQ4DiT successfully quantizes DiTs to 8-bit precision (W8A8) while preserving comparable generation ability and further enables effective quantization to 4-bit weight precision (W4A8) for the first time.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "12 pages, 6 figures"
    },
    {
        "paper id": "2405.16008",
        "abstract url": "https://arxiv.org/abs/2405.16008",
        "title": "Intensity and Texture Correction of Omnidirectional Image Using Camera Images for Indirect Augmented Reality",
        "rating": "0",
        "keywords": [
            [
                "inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Augmented reality (AR) using camera images in mobile devices is becoming popular for tourism promotion. However, obstructions such as tourists appearing in the camera images may cause the camera pose estimation error, resulting in CG misalignment and reduced visibility of the contents. To avoid this problem, Indirect AR (IAR), which does not use real-time camera images, has been proposed. In this method, an omnidirectional image is captured and virtual objects are synthesized on the image in advance. Users can experience AR by viewing a scene extracted from the synthesized omnidirectional image according to the device's sensor. This enables robustness and high visibility. However, if the weather conditions and season in the pre-captured 360 images differs from the current weather conditions and season when AR is experienced, the realism of the AR experience is reduced. To overcome the problem, we propose a method for correcting the intensity and texture of a past omnidirectional image using camera images from mobile devices. We first perform semantic segmentation. We then reproduce the current sky pattern by panoramic image composition and inpainting. For the other areas, we correct the intensity by histogram matching. In experiments, we show the effectiveness of the proposed method using various scenes.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "International Workshop on Frontiers of Computer Vision (IW-FCV2024)"
    },
    {
        "paper id": "2405.17477",
        "abstract url": "https://arxiv.org/abs/2405.17477",
        "title": "OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning",
        "rating": "0",
        "keywords": [
            [
                "unlearning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "In this paper, we study offline-to-online Imitation Learning (IL) that pretrains an imitation policy from static demonstration data, followed by fast finetuning with minimal environmental interaction. We find the na\u00efve combination of existing offline IL and online IL methods tends to behave poorly in this context, because the initial discriminator (often used in online IL) operates randomly and discordantly against the policy initialization, leading to misguided policy optimization and $\\textit{unlearning}$ of pretraining knowledge. To overcome this challenge, we propose a principled offline-to-online IL method, named $\\texttt{OLLIE}$, that simultaneously learns a near-expert policy initialization along with an $\\textit{aligned discriminator initialization}$, which can be seamlessly integrated into online IL, achieving smooth and fast finetuning. Empirically, $\\texttt{OLLIE}$ consistently and significantly outperforms the baseline methods in $\\textbf{20}$ challenging tasks, from continuous control to vision-based domains, in terms of performance, demonstration efficiency, and convergence speed. This work may serve as a foundation for further exploration of pretraining and finetuning in the context of IL.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "International Conference on Machine Learning (ICML)"
    },
    {
        "paper id": "2405.20770",
        "abstract url": "https://arxiv.org/abs/2405.20770",
        "title": "Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for defense, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15218",
        "abstract url": "https://arxiv.org/abs/2405.15218",
        "title": "AGS-GNN: Attribute-guided Sampling for Graph Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose AGS-GNN, a novel attribute-guided sampling algorithm for Graph Neural Networks (GNNs) that exploits node features and connectivity structure of a graph while simultaneously adapting for both homophily and heterophily in graphs. (In homophilic graphs vertices of the same class are more likely to be connected, and vertices of different classes tend to be linked in heterophilic graphs.) While GNNs have been successfully applied to homophilic graphs, their application to heterophilic graphs remains challenging. The best-performing GNNs for heterophilic graphs do not fit the sampling paradigm, suffer high computational costs, and are not inductive. We employ samplers based on feature-similarity and feature-diversity to select subsets of neighbors for a node, and adaptively capture information from homophilic and heterophilic neighborhoods using dual channels. Currently, AGS-GNN is the only algorithm that we know of that explicitly controls homophily in the sampled subgraph through similar and diverse neighborhood samples. For diverse neighborhood sampling, we employ submodularity, which was not used in this context prior to our work. The sampling distribution is pre-computed and highly parallel, achieving the desired scalability. Using an extensive dataset consisting of 35 small ($\\le$ 100K nodes) and large (>100K nodes) homophilic and heterophilic graphs, we demonstrate the superiority of AGS-GNN compare to the current approaches in the literature. AGS-GNN achieves comparable test accuracy to the best-performing heterophilic GNNs, even outperforming methods using the entire graph for node classification. AGS-GNN also converges faster compared to methods that sample neighborhoods randomly, and can be incorporated into existing GNN models that employ node or graph sampling.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "The paper has been accepted to KDD'24 in the research track"
    },
    {
        "paper id": "2405.15231",
        "abstract url": "https://arxiv.org/abs/2405.15231",
        "title": "Cardinality Estimation on Hyper-relational Knowledge Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Cardinality Estimation (CE) for query is to estimate the number of results without execution, which is an effective index in query optimization. Recently, CE over has achieved great success in knowledge graphs (KGs) that consist of triple facts. To more precisely represent facts, current researchers propose hyper-relational KGs (HKGs) to represent a triple fact with qualifiers, where qualifiers provide additional context to the fact. However, existing CE methods over KGs achieve unsatisfying performance on HKGs due to the complexity of qualifiers in HKGs. Also, there is only one dataset for HKG query cardinality estimation, i.e., WD50K-QE, which is not comprehensive and only covers limited patterns. The lack of querysets over HKG also becomes a bottleneck to comprehensively investigate CE problems on HKGs. In this work, we first construct diverse and unbiased hyper-relational querysets over three popular HKGs for investigating CE. Besides, we also propose a novel qualifier-attached graph neural network (GNN) model that effectively incorporates qualifier information and adaptively combines outputs from multiple GNN layers, to accurately predict the cardinality. Our experiments illustrate that the proposed hyper-relational query encoder outperforms all state-of-the-art CE methods over three popular HKGs on the diverse and unbiased benchmark.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15244",
        "abstract url": "https://arxiv.org/abs/2405.15244",
        "title": "Adversarial Attacks on Hidden Tasks in Multi-Task Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning models are susceptible to adversarial attacks, where slight perturbations to input data lead to misclassification. Adversarial attacks become increasingly effective with access to information about the targeted classifier. In the context of multi-task learning, where a single model learns multiple tasks simultaneously, attackers may aim to exploit vulnerabilities in specific tasks with limited information. This paper investigates the feasibility of attacking hidden tasks within multi-task classifiers, where model access regarding the hidden target task and labeled data for the hidden target task are not available, but model access regarding the non-target tasks is available. We propose a novel adversarial attack method that leverages knowledge from non-target tasks and the shared backbone network of the multi-task model to force the model to forget knowledge related to the target task. Experimental results on CelebA and DeepFashion datasets demonstrate the effectiveness of our method in degrading the accuracy of hidden tasks while preserving the performance of visible tasks, contributing to the understanding of adversarial vulnerabilities in multi-task classifiers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 6 figures"
    },
    {
        "paper id": "2405.15245",
        "abstract url": "https://arxiv.org/abs/2405.15245",
        "title": "Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee",
        "rating": "-0.5",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The safety of decentralized reinforcement learning (RL) is a challenging problem since malicious agents can share their poisoned policies with benign agents. The paper investigates a cooperative backdoor attack in a decentralized reinforcement learning scenario. Differing from the existing methods that hide a whole backdoor attack behind their shared policies, our method decomposes the backdoor behavior into multiple components according to the state space of RL. Each malicious agent hides one component in its policy and shares its policy with the benign agents. When a benign agent learns all the poisoned policies, the backdoor attack is assembled in its policy. The theoretical proof is given to show that our cooperative method can successfully inject the backdoor into the RL policies of benign agents. Compared with the existing backdoor attacks, our cooperative method is more covert since the policy from each attacker only contains a component of the backdoor attack and is harder to detect. Extensive simulations are conducted based on Atari environments to demonstrate the efficiency and covertness of our method. To the best of our knowledge, this is the first paper presenting a provable cooperative backdoor attack in decentralized reinforcement learning.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15252",
        "abstract url": "https://arxiv.org/abs/2405.15252",
        "title": "Fast 3D Molecule Generation via Unified Geometric Optimal Transport",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper proposes a new 3D molecule generation framework, called GOAT, for fast and effective 3D molecule generation based on the flow-matching optimal transport objective. Specifically, we formulate a geometric transport formula for measuring the cost of mapping multi-modal features (e.g., continuous atom coordinates and categorical atom types) between a base distribution and a target data distribution. Our formula is solved within a unified, equivalent, and smooth representation space. This is achieved by transforming the multi-modal features into a continuous latent space with equivalent networks. In addition, we find that identifying optimal distributional coupling is necessary for fast and effective transport between any two distributions. We further propose a flow refinement and purification mechanism for optimal coupling identification. By doing so, GOAT can turn arbitrary distribution couplings into new deterministic couplings, leading to a unified optimal transport path for fast 3D molecule generation. The purification filters the subpar molecules to ensure the ultimate generation performance. We theoretically prove the proposed method indeed reduced the transport cost. Finally, extensive experiments show that GOAT enjoys the efficiency of solving geometric optimal transport, leading to a double speedup compared to the sub-optimal method while achieving the best generation quality regarding validity, uniqueness, and novelty.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15254",
        "abstract url": "https://arxiv.org/abs/2405.15254",
        "title": "Novel Kernel Models and Exact Representor Theory for Neural Networks Beyond the Over-Parameterized Regime",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents two models of neural-networks and their training applicable to neural networks of arbitrary width, depth and topology, assuming only finite-energy neural activations; and a novel representor theory for neural networks in terms of a matrix-valued kernel. The first model is exact (un-approximated) and global, casting the neural network as an elements in a reproducing kernel Banach space (RKBS); we use this model to provide tight bounds on Rademacher complexity. The second model is exact and local, casting the change in neural network function resulting from a bounded change in weights and biases (ie. a training step) in reproducing kernel Hilbert space (RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model provides insight into model adaptation through tight bounds on Rademacher complexity of network adaptation. We also prove that the neural tangent kernel (NTK) is a first-order approximation of the LiNK kernel. Finally, and noting that the LiNK does not provide a representor theory for technical reasons, we present an exact novel representor theory for layer-wise neural network training with unregularized gradient descent in terms of a local-extrinsic neural kernel (LeNK). This representor theory gives insight into the role of higher-order statistics in neural network training and the effect of kernel evolution in neural-network kernel models. Throughout the paper (a) feedforward ReLU networks and (b) residual networks (ResNet) are used as illustrative examples.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15273",
        "abstract url": "https://arxiv.org/abs/2405.15273",
        "title": "Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders",
        "rating": "-0.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Time series anomaly detection plays a vital role in a wide range of applications. Existing methods require training one specific model for each dataset, which exhibits limited generalization capability across different target datasets, hindering anomaly detection performance in various scenarios with scarce training data. Aiming at this problem, we propose constructing a general time series anomaly detection model, which is pre-trained on extensive multi-domain datasets and can subsequently apply to a multitude of downstream scenarios. The significant divergence of time series data across different domains presents two primary challenges in building such a general model: (1) meeting the diverse requirements of appropriate information bottlenecks tailored to different datasets in one unified model, and (2) enabling distinguishment between multiple normal and abnormal patterns, both are crucial for effective anomaly detection in various target scenarios. To tackle these two challenges, we propose a General time series anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders (DADA), which enables flexible selection of bottlenecks based on different data and explicitly enhances clear differentiation between normal and abnormal series. We conduct extensive experiments on nine target datasets from different domains. After pre-training on multi-domain data, DADA, serving as a zero-shot anomaly detector for these datasets, still achieves competitive or even superior results compared to those models tailored to each specific dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15303",
        "abstract url": "https://arxiv.org/abs/2405.15303",
        "title": "Trajectory-Based Multi-Objective Hyperparameter Optimization for Model Retraining",
        "rating": "-0.5",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Training machine learning models inherently involves a resource-intensive and noisy iterative learning procedure that allows epoch-wise monitoring of the model performance. However, in multi-objective hyperparameter optimization scenarios, the insights gained from the iterative learning procedure typically remain underutilized. We notice that tracking the model performance across multiple epochs under a hyperparameter setting creates a trajectory in the objective space and that trade-offs along the trajectories are often overlooked despite their potential to offer valuable insights to decision-making for model retraining. Therefore, in this study, we propose to enhance the multi-objective hyperparameter optimization problem by having training epochs as an additional decision variable to incorporate trajectory information. Correspondingly, we present a novel trajectory-based multi-objective Bayesian optimization algorithm characterized by two features: 1) an acquisition function that captures the improvement made by the predictive trajectory of any hyperparameter setting and 2) a multi-objective early stopping mechanism that determines when to terminate the trajectory to maximize epoch efficiency. Numerical experiments on diverse synthetic simulations and hyperparameter tuning benchmarks indicate that our algorithm outperforms the state-of-the-art multi-objective optimizers in both locating better trade-offs and tuning efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15310",
        "abstract url": "https://arxiv.org/abs/2405.15310",
        "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
        "rating": "-0.5",
        "keywords": [
            [
                "kernel learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods use a subset of combinations of component functions and weight matrices within the random features paradigm. We identify the need for a systematic comparison of different combinations of weight matrix and component functions for attention learning in Transformer. In this work, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in linearized attention of the Transformer. We experiment with broad classes of component functions and weight matrices for three textual tasks in the LRA benchmark. Our experimentation with multiple combinations of component functions and weight matrices leads us to a novel combination with 23.4% faster training time and 25.2% lower memory consumption over the previous SOTA random feature Transformer, while maintaining the performance, as compared to the Original Transformer. Our code is available at: https://github.com/dukeraphaelng/spectraformer .",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15358",
        "abstract url": "https://arxiv.org/abs/2405.15358",
        "title": "Coordinated Multi-Neighborhood Learning on a Directed Acyclic Graph",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning the structure of causal directed acyclic graphs (DAGs) is useful in many areas of machine learning and artificial intelligence, with wide applications. However, in the high-dimensional setting, it is challenging to obtain good empirical and theoretical results without strong and often restrictive assumptions. Additionally, it is questionable whether all of the variables purported to be included in the network are observable. It is of interest then to restrict consideration to a subset of the variables for relevant and reliable inferences. In fact, researchers in various disciplines can usually select a set of target nodes in the network for causal discovery. This paper develops a new constraint-based method for estimating the local structure around multiple user-specified target nodes, enabling coordination in structure learning between neighborhoods. Our method facilitates causal discovery without learning the entire DAG structure. We establish consistency results for our algorithm with respect to the local neighborhood structure of the target nodes in the true graph. Experimental results on synthetic and real-world data show that our algorithm is more accurate in learning the neighborhood structures with much less computational cost than standard methods that estimate the entire DAG. An R package implementing our methods may be accessed at https://github.com/stephenvsmith/CML.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "13 pages, 6 figures"
    },
    {
        "paper id": "2405.15375",
        "abstract url": "https://arxiv.org/abs/2405.15375",
        "title": "A Planet Scale Spatial-Temporal Knowledge Graph Based On OpenStreetMap And H3 Grid",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Geospatial data plays a central role in modeling our world, for which OpenStreetMap (OSM) provides a rich source of such data. While often spatial data is represented in a tabular format, a graph based representation provides the possibility to interconnect entities which would have been separated in a tabular representation. We propose in our paper a framework which supports a planet scale transformation of OpenStreetMap data into a Spatial Temporal Knowledge Graph. In addition to OpenStreetMap data, we align the different OpenStreetMap geometries on individual h3 grid cells. We compare our constructed spatial knowledge graph to other spatial knowledge graphs and outline our contribution in this paper. As a basis for our computation, we use Apache Sedona as a computational framework for our Spatial Temporal Knowledge Graph construction",
        "subjects": [
            "cs.AI",
            "cs.DB",
            "cs.DC"
        ],
        "comment": "12 pages, 2 figures, GeoLD2024: 6th Geospatial Linked Data Workshop, May 26, 2024, Hersonissos, Greece"
    },
    {
        "paper id": "2405.15376",
        "abstract url": "https://arxiv.org/abs/2405.15376",
        "title": "Fast, accurate training and sampling of Restricted Boltzmann Machines",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Thanks to their simple architecture, Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting interpretable insights from data. However, training RBMs, as other energy-based models, on highly structured data poses a major challenge, as effective training relies on mixing the Markov chain Monte Carlo simulations used to estimate the gradient. This process is often hindered by multiple second-order phase transitions and the associated critical slowdown. In this paper, we present an innovative method in which the principal directions of the dataset are integrated into a low-rank RBM through a convex optimization procedure. This approach enables efficient sampling of the equilibrium measure via a static Monte Carlo process. By starting the standard training process with a model that already accurately represents the main modes of the data, we bypass the initial phase transitions. Our results show that this strategy successfully trains RBMs to capture the full diversity of data in datasets where previous methods fail. Furthermore, we use the training trajectories to propose a new sampling method, {\\em parallel trajectory tempering}, which allows us to sample the equilibrium measure of the trained model much faster than previous optimized MCMC approaches and a better estimation of the log-likelihood. We illustrate the success of the training method on several highly structured datasets.",
        "subjects": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech"
        ],
        "comment": "18 pages, 8 figures"
    },
    {
        "paper id": "2405.15383",
        "abstract url": "https://arxiv.org/abs/2405.15383",
        "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search",
        "rating": "-0.5",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has the advantages of being precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "10 pages in main text, 24 pages including references and supplementary materials. 2 figures and 3 tables in the main text, 9 figures and 12 tables when including the supplementary materials"
    },
    {
        "paper id": "2405.15388",
        "abstract url": "https://arxiv.org/abs/2405.15388",
        "title": "Language-Driven Interactive Traffic Trajectory Generation",
        "rating": "-0.5",
        "keywords": [
            [
                "Trajectory",
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Realistic trajectory generation with natural language control is pivotal for advancing autonomous vehicle technology. However, previous methods focus on individual traffic participant trajectory generation, thus failing to account for the complexity of interactive traffic dynamics. In this work, we propose InteractTraj, the first language-driven traffic trajectory generator that can generate interactive traffic trajectories. InteractTraj interprets abstract trajectory descriptions into concrete formatted interaction-aware numerical codes and learns a mapping between these formatted codes and the final interactive trajectories. To interpret language descriptions, we propose a language-to-code encoder with a novel interaction-aware encoding strategy. To produce interactive traffic trajectories, we propose a code-to-trajectory decoder with interaction-aware feature aggregation that synergizes vehicle interactions with the environmental map and the vehicle moves. Extensive experiments show our method demonstrates superior performance over previous SoTA methods, offering a more realistic generation of interactive traffic trajectories with high controllability via diverse natural language commands. Our code is available at https://github.com/X1a-jk/InteractTraj.git",
        "subjects": [
            "cs.AI",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15389",
        "abstract url": "https://arxiv.org/abs/2405.15389",
        "title": "Tensor Frames -- How To Make Any Message Passing Network Equivariant",
        "rating": "-0.5",
        "keywords": [
            [
                "point cloud"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In many applications of geometric deep learning, the choice of global coordinate frame is arbitrary, and predictions should be independent of the reference frame. In other words, the network should be equivariant with respect to rotations and reflections of the input, i.e., the transformations of O(d). We present a novel framework for building equivariant message passing architectures and modifying existing non-equivariant architectures to be equivariant. Our approach is based on local coordinate frames, between which geometric information is communicated consistently by including tensorial objects in the messages. Our framework can be applied to message passing on geometric data in arbitrary dimensional Euclidean space. While many other approaches for equivariant message passing require specialized building blocks, such as non-standard normalization layers or non-linearities, our approach can be adapted straightforwardly to any existing architecture without such modifications. We explicitly demonstrate the benefit of O(3)-equivariance for a popular point cloud architecture and produce state-of-the-art results on normal vector regression on point clouds.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15406",
        "abstract url": "https://arxiv.org/abs/2405.15406",
        "title": "A Misleading Gallery of Fluid Motion by Generative Artificial Intelligence",
        "rating": "-0.5",
        "keywords": [
            [
                "text-to-image"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this technical report, we extensively investigate the accuracy of outputs from well-known generative artificial intelligence (AI) applications in response to prompts describing common fluid motion phenomena familiar to the fluid mechanics community. We examine a range of applications, including Midjourney, Dall-E, Runway ML, Microsoft Designer, Gemini, Meta AI, and Leonardo AI, introduced by prominent companies such as Google, OpenAI, Meta, and Microsoft. Our text prompts for generating images or videos include examples such as \"Von Karman vortex street\", \"flow past an airfoil\", \"Kelvin-Helmholtz instability\", \"shock waves on a sharp-nosed supersonic body\", etc. We compare the images generated by these applications with real images from laboratory experiments and numerical software. Our findings indicate that these generative AI models are not adequately trained in fluid dynamics imagery, leading to potentially misleading outputs. Beyond text-to-image/video generation, we further explore the transition from image/video to text generation using these AI tools, aiming to investigate the accuracy of their descriptions of fluid motion phenomena. This report serves as a cautionary note for educators in academic institutions, highlighting the potential for these tools to mislead students. It also aims to inform researchers at these renowned companies, encouraging them to address this issue. We conjecture that a primary reason for this shortcoming is the limited access to copyright-protected fluid motion images from scientific journals.",
        "subjects": [
            "physics.flu-dyn",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15407",
        "abstract url": "https://arxiv.org/abs/2405.15407",
        "title": "Towards Client Driven Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Conventional federated learning (FL) frameworks follow a server-driven model where the server determines session initiation and client participation, which faces challenges in accommodating clients' asynchronous needs for model updates. We introduce Client-Driven Federated Learning (CDFL), a novel FL framework that puts clients at the driving role. In CDFL, each client independently and asynchronously updates its model by uploading the locally trained model to the server and receiving a customized model tailored to its local task. The server maintains a repository of cluster models, iteratively refining them using received client models. Our framework accommodates complex dynamics in clients' data distributions, characterized by time-varying mixtures of cluster distributions, enabling rapid adaptation to new tasks with superior performance. In contrast to traditional clustered FL protocols that send multiple cluster models to a client to perform distribution estimation, we propose a paradigm that offloads the estimation task to the server and only sends a single model to a client, and novel strategies to improve estimation accuracy. We provide a theoretical analysis of CDFL's convergence. Extensive experiments across various datasets and system settings highlight CDFL's substantial advantages in model performance and computation efficiency over baselines.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15414",
        "abstract url": "https://arxiv.org/abs/2405.15414",
        "title": "Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Building open agents has always been the ultimate goal in AI research, and creative agents are the more enticing. Existing LLM agents excel at long-horizon tasks with well-defined goals (e.g., `mine diamonds' in Minecraft). However, they encounter difficulties on creative tasks with open goals and abstract criteria due to the inability to bridge the gap between them, thus lacking feedback for self-improvement in solving the task. In this work, we introduce autonomous embodied verification techniques for agents to fill the gap, laying the groundwork for creative tasks. Specifically, we propose the Luban agent target creative building tasks in Minecraft, which equips with two-level autonomous embodied verification inspired by human design practices: (1) visual verification of 3D structural speculates, which comes from agent synthesized CAD modeling programs; (2) pragmatic verification of the creation by generating and verifying environment-relevant functionality programs based on the abstract criteria. Extensive multi-dimensional human studies and Elo ratings show that the Luban completes diverse creative building tasks in our proposed benchmark and outperforms other baselines ($33\\%$ to $100\\%$) in both visualization and pragmatism. Additional demos on the real-world robotic arm show the creation potential of the Luban in the physical world.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15423",
        "abstract url": "https://arxiv.org/abs/2405.15423",
        "title": "Lost in the Averages: A New Specific Setup to Evaluate Membership Inference Attacks Against Machine Learning Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Membership Inference Attacks (MIAs) are widely used to evaluate the propensity of a machine learning (ML) model to memorize an individual record and the privacy risk releasing the model poses. MIAs are commonly evaluated similarly to ML models: the MIA is performed on a test set of models trained on datasets unseen during training, which are sampled from a larger pool, $D_{eval}$. The MIA is evaluated across all datasets in this test set, and is thus evaluated across the distribution of samples from $D_{eval}$. While this was a natural extension of ML evaluation to MIAs, recent work has shown that a record's risk heavily depends on its specific dataset. For example, outliers are particularly vulnerable, yet an outlier in one dataset may not be one in another. The sources of randomness currently used to evaluate MIAs may thus lead to inaccurate individual privacy risk estimates. We propose a new, specific evaluation setup for MIAs against ML models, using weight initialization as the sole source of randomness. This allows us to accurately evaluate the risk associated with the release of a model trained on a specific dataset. Using SOTA MIAs, we empirically show that the risk estimates given by the current setup lead to many records being misclassified as low risk. We derive theoretical results which, combined with empirical evidence, suggest that the risk calculated in the current setup is an average of the risks specific to each sampled dataset, validating our use of weight initialization as the only source of randomness. Finally, we consider an MIA with a stronger adversary leveraging information about the target dataset to infer membership. Taken together, our results show that current MIA evaluation is averaging the risk across datasets leading to inaccurate risk estimates, and the risk posed by attacks leveraging information about the target dataset to be potentially underestimated.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15429",
        "abstract url": "https://arxiv.org/abs/2405.15429",
        "title": "E(n) Equivariant Topological Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features. Topological deep learning (TDL) has emerged recently as a promising tool for addressing this issue. TDL enables the principled modeling of arbitrary multi-way, hierarchical higher-order interactions by operating on combinatorial topological spaces, such as simplicial or cell complexes, instead of graphs. However, little is known about how to leverage geometric features such as positions and velocities for TDL. This paper introduces E(n)-Equivariant Topological Neural Networks (ETNNs), which are E(n)-equivariant message-passing networks operating on combinatorial complexes, formal objects unifying graphs, hypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric node features while respecting rotation and translation equivariance. Moreover, ETNNs are natively ready for settings with heterogeneous interactions. We provide a theoretical analysis to show the improved expressiveness of ETNNs over architectures for geometric graphs. We also show how several E(n) equivariant variants of TDL models can be directly derived from our framework. The broad applicability of ETNNs is demonstrated through two tasks of vastly different nature: i) molecular property prediction on the QM9 benchmark and ii) land-use regression for hyper-local estimation of air pollution with multi-resolution irregular geospatial data. The experiment results indicate that ETNNs are an effective tool for learning from diverse types of richly structured data, highlighting the benefits of principled geometric inductive bias.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": "32 pages, 11 figures, 8 tables"
    },
    {
        "paper id": "2405.15436",
        "abstract url": "https://arxiv.org/abs/2405.15436",
        "title": "Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented Knowledge Graphs and Vector Database for Accreditation Reporting Assistance",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In higher education, accreditation is a quality assurance process, where an institution demonstrates a commitment to delivering high quality programs and services to their students. For business schools nationally and internationally the Association to Advance Collegiate Schools of Business (AACSB) accreditation is the gold standard. For a business school to receive and subsequently maintain accreditation, the school must undertake a rigorous, time consuming reporting and peer review process, to demonstrate alignment with the AACSB Standards. For this project we create a hybrid context retrieval augmented generation pipeline that can assist in the documentation alignment and reporting process necessary for accreditation. We implement both a vector database and knowledge graph, as knowledge stores containing both institutional data and AACSB Standard data. The output of the pipeline can be used by institution stakeholders to build their accreditation report, dually grounded by the context from the knowledge stores. To develop our knowledge graphs we utilized both a manual construction process as well as an LLM Augmented Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework and observed optimal performance on answer relevancy and answer correctness metrics.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "17 pages, 9 figures"
    },
    {
        "paper id": "2405.15473",
        "abstract url": "https://arxiv.org/abs/2405.15473",
        "title": "Encoder Embedding for General Graph and Node Classification",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph encoder embedding, a recent technique for graph data, offers speed and scalability in producing vertex-level representations from binary graphs. In this paper, we extend the applicability of this method to a general graph model, which includes weighted graphs, distance matrices, and kernel matrices. We prove that the encoder embedding satisfies the law of large numbers and the central limit theorem on a per-observation basis. Under certain condition, it achieves asymptotic normality on a per-class basis, enabling optimal classification through discriminant analysis. These theoretical findings are validated through a series of experiments involving weighted graphs, as well as text and image data transformed into general graph representations using appropriate distance metrics.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "cs.SI"
        ],
        "comment": "26 pages"
    },
    {
        "paper id": "2405.15489",
        "abstract url": "https://arxiv.org/abs/2405.15489",
        "title": "Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Protein diffusion models have emerged as a promising approach for protein design. One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter. In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation. Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations. This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions. On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty. Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions. Taken together, these advances set a new standard for structure-based protein design. Genie 2 inference and training code, as well as model weights, are freely available at: https://github.com/aqlaboratory/genie2.",
        "subjects": [
            "q-bio.BM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15495",
        "abstract url": "https://arxiv.org/abs/2405.15495",
        "title": "Towards Natural Machine Unlearning",
        "rating": "-0.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine unlearning (MU) aims to eliminate information that has been learned from specific training data, namely forgetting data, from a pre-trained model. Currently, the mainstream of existing MU methods involves modifying the forgetting data with incorrect labels and subsequently fine-tuning the model. While learning such incorrect information can indeed remove knowledge, the process is quite unnatural as the unlearning process undesirably reinforces the incorrect information and leads to over-forgetting. Towards more \\textit{natural} machine unlearning, we inject correct information from the remaining data to the forgetting samples when changing their labels. Through pairing these adjusted samples with their labels, the model will tend to use the injected correct information and naturally suppress the information meant to be forgotten. Albeit straightforward, such a first step towards natural machine unlearning can significantly outperform current state-of-the-art approaches. In particular, our method substantially reduces the over-forgetting and leads to strong robustness to hyperparameters, making it a promising candidate for practical machine unlearning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15544",
        "abstract url": "https://arxiv.org/abs/2405.15544",
        "title": "Knowledge-enhanced Relation Graph and Task Sampling for Few-shot Molecular Property Prediction",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recently, few-shot molecular property prediction (FSMPP) has garnered increasing attention. Despite impressive breakthroughs achieved by existing methods, they often overlook the inherent many-to-many relationships between molecules and properties, which limits their performance. For instance, similar substructures of molecules can inspire the exploration of new compounds. Additionally, the relationships between properties can be quantified, with high-related properties providing more information in exploring the target property than those low-related. To this end, this paper proposes a novel meta-learning FSMPP framework (KRGTS), which comprises the Knowledge-enhanced Relation Graph module and the Task Sampling module. The knowledge-enhanced relation graph module constructs the molecule-property multi-relation graph (MPMRG) to capture the many-to-many relationships between molecules and properties. The task sampling module includes a meta-training task sampler and an auxiliary task sampler, responsible for scheduling the meta-training process and sampling high-related auxiliary tasks, respectively, thereby achieving efficient meta-knowledge learning and reducing noise introduction. Empirically, extensive experiments on five datasets demonstrate the superiority of KRGTS over a variety of state-of-the-art methods. The code is available in https://github.com/Vencent-Won/KRGTS-public.",
        "subjects": [
            "q-bio.QM",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15551",
        "abstract url": "https://arxiv.org/abs/2405.15551",
        "title": "Thinking Forward: Memory-Efficient Federated Finetuning of Language Models",
        "rating": "-0.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Finetuning large language models (LLMs) in federated learning (FL) settings has become important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. This work introduces Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using Forward-mode AD that are closer estimates of the true gradients. Spry achieves a low memory footprint, high accuracy, and fast convergence. We theoretically show that the global gradients in Spry are unbiased estimates of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, Spry reduces the memory footprint during training by 1.4-7.1$\\times$ in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. Spry reduces the convergence time by 1.2-20.3$\\times$ and achieves 5.2-13.5\\% higher accuracy against state-of-the-art zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory usage of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices. Source code is available at https://github.com/Astuary/Spry.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15557",
        "abstract url": "https://arxiv.org/abs/2405.15557",
        "title": "Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large linear systems are ubiquitous in modern computational science. The main recipe for solving them is iterative solvers with well-designed preconditioners. Deep learning models may be used to precondition residuals during iteration of such linear solvers as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners of the predefined sparsity pattern. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN. Numerical experiments demonstrate that our approach outperforms both classical methods and neural network-based preconditioning. We also provide a heuristic justification for the loss function used and validate our approach on complex datasets.",
        "subjects": [
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15589",
        "abstract url": "https://arxiv.org/abs/2405.15589",
        "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "19 pages, 4 figures"
    },
    {
        "paper id": "2405.15632",
        "abstract url": "https://arxiv.org/abs/2405.15632",
        "title": "Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks. However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature. To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space). Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours. Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named Federated Behavioural Shields to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "[v1] Preprint (24 pages)"
    },
    {
        "paper id": "2405.15644",
        "abstract url": "https://arxiv.org/abs/2405.15644",
        "title": "Harnessing Increased Client Participation with Cohort-Parallel Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) is a machine learning approach where nodes collaboratively train a global model. As more nodes participate in a round of FL, the effectiveness of individual model updates by nodes also diminishes. In this study, we increase the effectiveness of client updates by dividing the network into smaller partitions, or cohorts. We introduce Cohort-Parallel Federated Learning (CPFL): a novel learning approach where each cohort independently trains a global model using FL, until convergence, and the produced models by each cohort are then unified using one-shot Knowledge Distillation (KD) and a cross-domain, unlabeled dataset. The insight behind CPFL is that smaller, isolated networks converge quicker than in a one-network setting where all nodes participate. Through exhaustive experiments involving realistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST image classification tasks, we investigate the balance between the number of cohorts, model accuracy, training time, and compute and communication resources. Compared to traditional FL, CPFL with four cohorts, non-IID data distribution, and CIFAR-10 yields a 1.9$\\times$ reduction in train time and a 1.3$\\times$ reduction in resource usage, with a minimal drop in test accuracy.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15662",
        "abstract url": "https://arxiv.org/abs/2405.15662",
        "title": "Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning",
        "rating": "-0.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In current AI era, users may request AI companies to delete their data from the training dataset due to the privacy concerns. As a model owner, retraining a model will consume significant computational resources. Therefore, machine unlearning is a new emerged technology to allow model owner to delete requested training data or a class with little affecting on the model performance. However, for large-scaling complex data, such as image or text data, unlearning a class from a model leads to a inferior performance due to the difficulty to identify the link between classes and model. An inaccurate class deleting may lead to over or under unlearning. In this paper, to accurately defining the unlearning class of complex data, we apply the definition of Concept, rather than an image feature or a token of text data, to represent the semantic information of unlearning class. This new representation can cut the link between the model and the class, leading to a complete erasing of the impact of a class. To analyze the impact of the concept of complex data, we adopt a Post-hoc Concept Bottleneck Model, and Integrated Gradients to precisely identify concepts across different classes. Next, we take advantage of data poisoning with random and targeted labels to propose unlearning methods. We test our methods on both image classification models and large language models (LLMs). The results consistently show that the proposed methods can accurately erase targeted information from models and can largely maintain the performance of the models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15676",
        "abstract url": "https://arxiv.org/abs/2405.15676",
        "title": "Taming Score-Based Diffusion Priors for Infinite-Dimensional Nonlinear Inverse Problems",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work introduces a sampling method capable of solving Bayesian inverse problems in function space. It does not assume the log-concavity of the likelihood, meaning that it is compatible with nonlinear inverse problems. The method leverages the recently defined infinite-dimensional score-based diffusion models as a learning-based prior, while enabling provable posterior sampling through a Langevin-type MCMC algorithm defined on function spaces. A novel convergence analysis is conducted, inspired by the fixed-point methods established for traditional regularization-by-denoising algorithms and compatible with weighted annealing. The obtained convergence bound explicitly depends on the approximation error of the score; a well-approximated score is essential to obtain a well-approximated posterior. Stylized and PDE-based examples are provided, demonstrating the validity of our convergence analysis. We conclude by presenting a discussion of the method's challenges related to learning the score and computational complexity.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15712",
        "abstract url": "https://arxiv.org/abs/2405.15712",
        "title": "Infinite Limits of Multi-head Transformer Dynamics",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime. We identify the set of parameterizations that admit well-defined infinite width and depth limits, allowing the attention layers to update throughout training--a relevant notion of feature learning in these models. We then use tools from dynamical mean field theory (DMFT) to analyze various infinite limits (infinite key/query dimension, infinite heads, and infinite depth) which have different statistical descriptions depending on which infinite limit is taken and how attention layers are scaled. We provide numerical evidence of convergence to the limits and discuss how the parameterization qualitatively influences learned features.",
        "subjects": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15723",
        "abstract url": "https://arxiv.org/abs/2405.15723",
        "title": "Bisimulation Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce a data-driven approach to computing finite bisimulations for state transition systems with very large, possibly infinite state space. Our novel technique computes stutter-insensitive bisimulations of deterministic systems, which we characterize as the problem of learning a state classifier together with a ranking function for each class. Our procedure learns a candidate state classifier and candidate ranking functions from a finite dataset of sample states; then, it checks whether these generalise to the entire state space using satisfiability modulo theory solving. Upon the affirmative answer, the procedure concludes that the classifier constitutes a valid stutter-insensitive bisimulation of the system. Upon a negative answer, the solver produces a counterexample state for which the classifier violates the claim, adds it to the dataset, and repeats learning and checking in a counterexample-guided inductive synthesis loop until a valid bisimulation is found. We demonstrate on a range of benchmarks from reactive verification and software model checking that our method yields faster verification results than alternative state-of-the-art tools in practice. Our method produces succinct abstractions that enable an effective verification of linear temporal logic without next operator, and are interpretable for system diagnostics.",
        "subjects": [
            "cs.LO",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15727",
        "abstract url": "https://arxiv.org/abs/2405.15727",
        "title": "Anomalous Change Point Detection Using Probabilistic Predictive Coding",
        "rating": "-0.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Change point detection (CPD) and anomaly detection (AD) are essential techniques in various fields to identify abrupt changes or abnormal data instances. However, existing methods are often constrained to univariate data, face scalability challenges with large datasets due to computational demands, and experience reduced performance with high-dimensional or intricate data, as well as hidden anomalies. Furthermore, they often lack interpretability and adaptability to domain-specific knowledge, which limits their versatility across different fields. In this work, we propose a deep learning-based CPD/AD method called Probabilistic Predictive Coding (PPC) that jointly learns to encode sequential data to low dimensional latent space representations and to predict the subsequent data representations as well as the corresponding prediction uncertainties. The model parameters are optimized with maximum likelihood estimation by comparing these predictions with the true encodings. At the time of application, the true and predicted encodings are used to determine the probability of conformity, an interpretable and meaningful anomaly score. Furthermore, our approach has linear time complexity, scalability issues are prevented, and the method can easily be adjusted to a wide range of data types and intricate applications. We demonstrate the effectiveness and adaptability of our proposed method across synthetic time series experiments, image data, and real-world magnetic resonance spectroscopic imaging data.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "Submitted to Machine Learning"
    },
    {
        "paper id": "2405.15744",
        "abstract url": "https://arxiv.org/abs/2405.15744",
        "title": "CAFe: Cost and Age aware Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In many federated learning (FL) models, a common strategy employed to ensure the progress in the training process, is to wait for at least $M$ clients out of the total $N$ clients to send back their local gradients based on a reporting deadline $T$, once the parameter server (PS) has broadcasted the global model. If enough clients do not report back within the deadline, the particular round is considered to be a failed round and the training round is restarted from scratch. If enough clients have responded back, the round is deemed successful and the local gradients of all the clients that responded back are used to update the global model. In either case, the clients that failed to report back an update within the deadline would have wasted their computational resources. Having a tighter deadline (small $T$) and waiting for a larger number of participating clients (large $M$) leads to a large number of failed rounds and therefore greater communication cost and computation resource wastage. However, having a larger $T$ leads to longer round durations whereas smaller $M$ may lead to noisy gradients. Therefore, there is a need to optimize the parameters $M$ and $T$ such that communication cost and the resource wastage is minimized while having an acceptable convergence rate. In this regard, we show that the average age of a client at the PS appears explicitly in the theoretical convergence bound, and therefore, can be used as a metric to quantify the convergence of the global model. We provide an analytical scheme to select the parameters $M$ and $T$ in this setting.",
        "subjects": [
            "cs.LG",
            "cs.DC",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15754",
        "abstract url": "https://arxiv.org/abs/2405.15754",
        "title": "Score-based generative models are provably robust: an uncertainty quantification perspective",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation. Our primary tool is the Wasserstein uncertainty propagation (WUP) theorem, a model-form UQ bound that describes how the $L^2$ error from learning the score function propagates to a Wasserstein-1 ($\\mathbf{d}_1$) ball around the true data distribution under the evolution of the Fokker-Planck equation. We show how errors due to (a) finite sample approximation, (b) early stopping, (c) score-matching objective choice, (d) score function parametrization expressiveness, and (e) reference distribution choice, impact the quality of the generative model in terms of a $\\mathbf{d}_1$ bound of computable quantities. The WUP theorem relies on Bernstein estimates for Hamilton-Jacobi-Bellman partial differential equations (PDE) and the regularizing properties of diffusion processes. Specifically, PDE regularity theory shows that stochasticity is the key mechanism ensuring SGM algorithms are provably robust. The WUP theorem applies to integral probability metrics beyond $\\mathbf{d}_1$, such as the total variation distance and the maximum mean discrepancy. Sample complexity and generalization bounds in $\\mathbf{d}_1$ follow directly from the WUP theorem. Our approach requires minimal assumptions, is agnostic to the manifold hypothesis and avoids absolute continuity assumptions for the target distribution. Additionally, our results clarify the trade-offs among multiple error sources in SGMs.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15861",
        "abstract url": "https://arxiv.org/abs/2405.15861",
        "title": "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL pose a significant challenge to its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. In this paper, we introduce a novel dimension-free communication strategy for FL, leveraging zero-order optimization techniques. We propose a new algorithm, FedDisco, which facilitates the transmission of only a constant number of scalar values between clients and the server in each communication round, thereby reducing the communication cost from $\\mathscr{O}(d)$ to $\\mathscr{O}(1)$, where $d$ is the dimension of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions and dimension-free rate for low effective rank scenarios. Empirical evaluations through classic deep learning training and large language model fine-tuning substantiate significant reductions in communication overhead compared to traditional FL approaches.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15871",
        "abstract url": "https://arxiv.org/abs/2405.15871",
        "title": "CausalConceptTS: Causal Attributions for Time Series Classification using High Fidelity Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Despite the excelling performance of machine learning models, understanding the decisions of machine learning models remains a long-standing goal. While commonly used attribution methods in explainable AI attempt to address this issue, they typically rely on associational rather than causal relationships. In this study, within the context of time series classification, we introduce a novel framework to assess the causal effect of concepts, i.e., predefined segments within a time series, on specific classification outcomes. To achieve this, we leverage state-of-the-art diffusion-based generative models to estimate counterfactual outcomes. Our approach compares these causal attributions with closely related associational attributions, both theoretically and empirically. We demonstrate the insights gained by our approach for a diverse set of qualitatively different time series classification tasks. Although causal and associational attributions might often share some similarities, in all cases they differ in important details, underscoring the risks associated with drawing causal conclusions from associational data alone. We believe that the proposed approach is widely applicable also in other domains, particularly where predefined segmentations are available, to shed some light on the limits of associational attributions.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "17 pages, 8 figures. Source code under https://github.com/AI4HealthUOL/CausalConceptTS"
    },
    {
        "paper id": "2405.15880",
        "abstract url": "https://arxiv.org/abs/2405.15880",
        "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
        "rating": "-0.5",
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a domain-specific language (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers.",
        "subjects": [
            "cs.PL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15885",
        "abstract url": "https://arxiv.org/abs/2405.15885",
        "title": "Diffusion Bridge Implicit Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we present diffusion bridge implicit models (DBIMs) for accelerated sampling of diffusion bridges without extra training. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same training objective as DDBMs. These generalized diffusion bridges give rise to generative processes ranging from stochastic to deterministic (i.e., an implicit probabilistic model) while being up to 25$\\times$ faster than the vanilla sampler of DDBMs. Moreover, the deterministic sampling procedure yielded by DBIMs enables faithful encoding and reconstruction by a booting noise used in the initial sampling step, and allows us to perform semantically meaningful interpolation in image translation tasks by regarding the booting noise as the latent variable.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15942",
        "abstract url": "https://arxiv.org/abs/2405.15942",
        "title": "Can Implicit Bias Imply Adversarial Robustness?",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The implicit bias of gradient-based training algorithms has been considered mostly beneficial as it leads to trained networks that often generalize well. However, Frei et al. (2023) show that such implicit bias can harm adversarial robustness. Specifically, when the data consists of clusters with small inter-cluster correlation, a shallow (two-layer) ReLU network trained by gradient flow generalizes well, but it is not robust to adversarial attacks of small radius, despite the existence of a much more robust classifier that can be explicitly constructed from a shallow network. In this paper, we extend recent analyses of neuron alignment to show that a shallow network with a polynomial ReLU activation (pReLU) trained by gradient flow not only generalizes well but is also robust to adversarial attacks. Our results highlight the importance of the interplay between data structure and architecture design in the implicit bias and robustness of trained networks.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "icml 2024"
    },
    {
        "paper id": "2405.15979",
        "abstract url": "https://arxiv.org/abs/2405.15979",
        "title": "BadGD: A unified data-centric framework to identify gradient descent vulnerabilities",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present BadGD, a unified theoretical framework that exposes the vulnerabilities of gradient descent algorithms through strategic backdoor attacks. Backdoor attacks involve embedding malicious triggers into a training dataset to disrupt the model's learning process. Our framework introduces three novel constructs: Max RiskWarp Trigger, Max GradWarp Trigger, and Max GradDistWarp Trigger, each designed to exploit specific aspects of gradient descent by distorting empirical risk, deterministic gradients, and stochastic gradients respectively. We rigorously define clean and backdoored datasets and provide mathematical formulations for assessing the distortions caused by these malicious backdoor triggers. By measuring the impact of these triggers on the model training procedure, our framework bridges existing empirical findings with theoretical insights, demonstrating how a malicious party can exploit gradient descent hyperparameters to maximize attack effectiveness. In particular, we show that these exploitations can significantly alter the loss landscape and gradient calculations, leading to compromised model integrity and performance. This research underscores the severe threats posed by such data-centric attacks and highlights the urgent need for robust defenses in machine learning. BadGD sets a new standard for understanding and mitigating adversarial manipulations, ensuring the reliability and security of AI systems.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "stat.ML"
        ],
        "comment": "25 pages, 1 figure"
    },
    {
        "paper id": "2405.15986",
        "abstract url": "https://arxiv.org/abs/2405.15986",
        "title": "Accelerating Diffusion Models with Parallel Sampling: Inference at Sub-Linear Time Complexity",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models have become a leading method for generative modeling of both image and scientific data. As these models are costly to train and evaluate, reducing the inference cost for diffusion models remains a major goal. Inspired by the recent empirical success in accelerating diffusion models via the parallel sampling technique~\\cite{shih2024parallel}, we propose to divide the sampling process into $\\mathcal{O}(1)$ blocks with parallelizable Picard iterations within each block. Rigorous theoretical analysis reveals that our algorithm achieves $\\widetilde{\\mathcal{O}}(\\mathrm{poly} \\log d)$ overall time complexity, marking the first implementation with provable sub-linear complexity w.r.t. the data dimension $d$. Our analysis is based on a generalized version of Girsanov's theorem and is compatible with both the SDE and probability flow ODE implementations. Our results shed light on the potential of fast and efficient sampling of high-dimensional data on fast-evolving modern large-memory GPU clusters.",
        "subjects": [
            "cs.LG",
            "cs.DC",
            "math.NA",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15991",
        "abstract url": "https://arxiv.org/abs/2405.15991",
        "title": "R\u00e9nyi Neural Processes",
        "rating": "-0.5",
        "keywords": [
            [
                "inpainting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Neural Processes (NPs) are variational frameworks that aim to represent stochastic processes with deep neural networks. Despite their obvious benefits in uncertainty estimation for complex distributions via data-driven priors, NPs enforce network parameter sharing between the conditional prior and posterior distributions, thereby risking introducing a misspecified prior. We hereby propose R\u00e9nyi Neural Processes (RNP) to relax the influence of the misspecified prior and optimize a tighter bound of the marginal likelihood. More specifically, by replacing the standard KL divergence with the R\u00e9nyi divergence between the posterior and the approximated prior, we ameliorate the impact of the misspecified prior via a parameter \u03b1 so that the resulting posterior focuses more on tail samples and reduce density on overconfident regions. Our experiments showed log-likelihood improvements on several existing NP families. We demonstrated the superior performance of our approach on various benchmarks including regression and image inpainting tasks. We also validate the effectiveness of RNPs on real-world tabular regression problems.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16002",
        "abstract url": "https://arxiv.org/abs/2405.16002",
        "title": "Does SGD really happen in tiny subspaces?",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss Hessian, referred to as the dominant subspace. Given this alignment, this paper explores whether neural networks can be trained within the dominant subspace, which, if feasible, could lead to more efficient training methods. Our primary observation is that when the SGD update is projected onto the dominant subspace, the training loss does not decrease further. This suggests that the observed alignment between the gradient and the dominant subspace is spurious. Surprisingly, projecting out the dominant subspace proves to be just as effective as the original update, despite removing the majority of the original update component. Similar observations are made for the large learning rate regime (also known as Edge of Stability) and Sharpness-Aware Minimization. We discuss the main causes and implications of this spurious alignment, shedding light on the intricate dynamics of neural network training.",
        "subjects": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2405.16012",
        "abstract url": "https://arxiv.org/abs/2405.16012",
        "title": "Pessimistic Backward Policy for GFlowNets",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper studies Generative Flow Networks (GFlowNets), which learn to sample objects proportionally to a given reward function through the trajectory of state transitions. In this work, we observe that GFlowNets tend to under-exploit the high-reward objects due to training on insufficient number of trajectories, which may lead to a large gap between the estimated flow and the (known) reward value. In response to this challenge, we propose a pessimistic backward policy for GFlowNets (PBP-GFN), which maximizes the observed flow to align closely with the true reward for the object. We extensively evaluate PBP-GFN across eight benchmarks, including hyper-grid environment, bag generation, structured set generation, molecular generation, and four RNA sequence generation tasks. In particular, PBP-GFN enhances the discovery of high-reward objects, maintains the diversity of the objects, and consistently outperforms existing methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16051",
        "abstract url": "https://arxiv.org/abs/2405.16051",
        "title": "A Bi-Objective Approach to Last-Mile Delivery Routing Considering Driver Preferences",
        "rating": "-0.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The Multi-Objective Vehicle Routing Problem (MOVRP) is a complex optimization problem in the transportation and logistics industry. This paper proposes a novel approach to the MOVRP that aims to create routes that consider drivers' and operators' decisions and preferences. We evaluate two approaches to address this objective: visually attractive route planning and data mining of historical driver behavior to plan similar routes. Using a real-world dataset provided by Amazon, we demonstrate that data mining of historical patterns is more effective than visual attractiveness metrics found in the literature. Furthermore, we propose a bi-objective problem to balance the similarity of routes to historical routes and minimize routing costs. We propose a two-stage GRASP algorithm with heuristic box splitting to solve this problem. The proposed algorithm aims to approximate the Pareto front and to present routes that cover a wide range of the objective function space. The results demonstrate that our approach can generate a small number of non-dominated solutions per instance, which can help decision-makers to identify trade-offs between routing costs and drivers' preferences. Our approach has the potential to enhance the last-mile delivery operations of logistics companies by balancing these conflicting objectives.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16055",
        "abstract url": "https://arxiv.org/abs/2405.16055",
        "title": "Federated Learning for Non-factorizable Models using Deep Generative Prior Approximations",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) allows for collaborative model training across decentralized clients while preserving privacy by avoiding data sharing. However, current FL methods assume conditional independence between client models, limiting the use of priors that capture dependence, such as Gaussian processes (GPs). We introduce the Structured Independence via deep Generative Model Approximation (SIGMA) prior which enables FL for non-factorizable models across clients, expanding the applicability of FL to fields such as spatial statistics, epidemiology, environmental science, and other domains where modeling dependencies is crucial. The SIGMA prior is a pre-trained deep generative model that approximates the desired prior and induces a specified conditional independence structure in the latent variables, creating an approximate model suitable for FL settings. We demonstrate the SIGMA prior's effectiveness on synthetic data and showcase its utility in a real-world example of FL for spatial data, using a conditional autoregressive prior to model spatial dependence across Australia. Our work enables new FL applications in domains where modeling dependent data is essential for accurate predictions and decision-making.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.CO",
            "stat.ME"
        ],
        "comment": "25 pages, 7 figures, 2 tables"
    },
    {
        "paper id": "2405.17490",
        "abstract url": "https://arxiv.org/abs/2405.17490",
        "title": "Revisit, Extend, and Enhance Hessian-Free Influence Functions",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Influence functions serve as crucial tools for assessing sample influence in model interpretation, subset training set selection, noisy label detection, and more. By employing the first-order Taylor extension, influence functions can estimate sample influence without the need for expensive model retraining. However, applying influence functions directly to deep models presents challenges, primarily due to the non-convex nature of the loss function and the large size of model parameters. This difficulty not only makes computing the inverse of the Hessian matrix costly but also renders it non-existent in some cases. Various approaches, including matrix decomposition, have been explored to expedite and approximate the inversion of the Hessian matrix, with the aim of making influence functions applicable to deep models. In this paper, we revisit a specific, albeit naive, yet effective approximation method known as TracIn. This method substitutes the inverse of the Hessian matrix with an identity matrix. We provide deeper insights into why this simple approximation method performs well. Furthermore, we extend its applications beyond measuring model utility to include considerations of fairness and robustness. Finally, we enhance TracIn through an ensemble strategy. To validate its effectiveness, we conduct experiments on synthetic data and extensive evaluations on noisy label detection, sample selection for large language model fine-tuning, and defense against adversarial attacks.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.19360",
        "abstract url": "https://arxiv.org/abs/2405.19360",
        "title": "ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users",
        "rating": "-0.5",
        "keywords": [
            [
                "vision language"
            ],
            [
                "Text-to-Image"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15214",
        "abstract url": "https://arxiv.org/abs/2405.15214",
        "title": "PointRWKV: Efficient RWKV-Like Model for Hierarchical Point Cloud Learning",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transformers have revolutionized the point cloud learning task, but the quadratic complexity hinders its extension to long sequence and makes a burden on limited computational resources. The recent advent of RWKV, a fresh breed of deep sequence models, has shown immense potential for sequence modeling in NLP tasks. In this paper, we present PointRWKV, a model of linear complexity derived from the RWKV model in the NLP field with necessary modifications for point cloud learning tasks. Specifically, taking the embedded point patches as input, we first propose to explore the global processing capabilities within PointRWKV blocks using modified multi-headed matrix-valued states and a dynamic attention recurrence mechanism. To extract local geometric features simultaneously, we design a parallel branch to encode the point cloud efficiently in a fixed radius near-neighbors graph with a graph stabilizer. Furthermore, we design PointRWKV as a multi-scale framework for hierarchical feature learning of 3D point clouds, facilitating various downstream tasks. Extensive experiments on different point cloud learning tasks show our proposed PointRWKV outperforms the transformer- and mamba-based counterparts, while significantly saving about 46\\% FLOPs, demonstrating the potential option for constructing foundational 3D models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15222",
        "abstract url": "https://arxiv.org/abs/2405.15222",
        "title": "Leveraging Unknown Objects to Construct Labeled-Unlabeled Meta-Relationships for Zero-Shot Object Navigation",
        "rating": "-1",
        "keywords": [
            [
                "Navigation"
            ],
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Zero-shot object navigation (ZSON) addresses situation where an agent navigates to an unseen object that does not present in the training set. Previous works mainly train agent using seen objects with known labels, and ignore the seen objects without labels. In this paper, we introduce seen objects without labels, herein termed as ``unknown objects'', into training procedure to enrich the agent's knowledge base with distinguishable but previously overlooked information. Furthermore, we propose the label-wise meta-correlation module (LWMCM) to harness relationships among objects with and without labels, and obtain enhanced objects information. Specially, we propose target feature generator (TFG) to generate the features representation of the unlabeled target objects. Subsequently, the unlabeled object identifier (UOI) module assesses whether the unlabeled target object appears in the current observation frame captured by the camera and produces an adapted target features representation specific to the observed context. In meta contrastive feature modifier (MCFM), the target features is modified via approaching the features of objects within the observation frame while distancing itself from features of unobserved objects. Finally, the meta object-graph learner (MOGL) module is utilized to calculate the relationships among objects based on the features. Experiments conducted on AI2THOR and RoboTHOR platforms demonstrate the effectiveness of our proposed method.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15227",
        "abstract url": "https://arxiv.org/abs/2405.15227",
        "title": "Neural Elevation Models for Terrain Mapping and Path Planning",
        "rating": "-1",
        "keywords": [
            [
                "NeRF",
                "Radiance Fields"
            ]
        ],
        "abstract": "This work introduces Neural Elevations Models (NEMos), which adapt Neural Radiance Fields to a 2.5D continuous and differentiable terrain model. In contrast to traditional terrain representations such as digital elevation models, NEMos can be readily generated from imagery, a low-cost data source, and provide a lightweight representation of terrain through an implicit continuous and differentiable height field. We propose a novel method for jointly training a height field and radiance field within a NeRF framework, leveraging quantile regression. Additionally, we introduce a path planning algorithm that performs gradient-based optimization of a continuous cost function for minimizing distance, slope changes, and control effort, enabled by differentiability of the height field. We perform experiments on simulated and real-world terrain imagery, demonstrating NEMos ability to generate high-quality reconstructions and produce smoother paths compared to discrete path planning methods. Future work will explore the incorporation of features and semantics into the height field, creating a generalized terrain model.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15264",
        "abstract url": "https://arxiv.org/abs/2405.15264",
        "title": "Self-Contrastive Weakly Supervised Learning Framework for Prognostic Prediction Using Whole Slide Images",
        "rating": "-1",
        "keywords": [
            [
                "Whole Slide",
                "cancer"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "We present a pioneering investigation into the application of deep learning techniques to analyze histopathological images for addressing the substantial challenge of automated prognostic prediction. Prognostic prediction poses a unique challenge as the ground truth labels are inherently weak, and the model must anticipate future events that are not directly observable in the image. To address this challenge, we propose a novel three-part framework comprising of a convolutional network based tissue segmentation algorithm for region of interest delineation, a contrastive learning module for feature extraction, and a nested multiple instance learning classification module. Our study explores the significance of various regions of interest within the histopathological slides and exploits diverse learning scenarios. The pipeline is initially validated on artificially generated data and a simpler diagnostic task. Transitioning to prognostic prediction, tasks become more challenging. Employing bladder cancer as use case, our best models yield an AUC of 0.721 and 0.678 for recurrence and treatment outcome prediction respectively.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "https://github.com/Biomedical-Data-Analysis-Laboratory/HistoPrognostics"
    },
    {
        "paper id": "2405.15266",
        "abstract url": "https://arxiv.org/abs/2405.15266",
        "title": "Conditional Variational Auto Encoder Based Dynamic Motion for Multi-task Imitation Learning",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "The dynamic motion primitive-based (DMP) method is an effective method of learning from demonstrations. However, most of the current DMP-based methods focus on learning one task with one module. Although, some deep learning-based frameworks can learn to multi-task at the same time. However, those methods require a large number of training data and have limited generalization of the learned behavior to the untrained state. In this paper, we propose a framework that combines the advantages of the traditional DMP-based method and conditional variational auto-encoder (CVAE). The encoder and decoder are made of a dynamic system and deep neural network. Deep neural networks are used to generate torque conditioned on the task ID. Then, this torque is used to create the desired trajectory in the dynamic system based on the final state. In this way, the generated tractory can adjust to the new goal position. We also propose a finetune method to guarantee the via-point constraint. Our model is trained on the handwriting number dataset and can be used to solve robotic tasks -- reaching and pushing directly. The proposed model is validated in the simulation environment. The results show that after training on the handwriting number dataset, it achieves a 100\\% success rate on pushing and reaching tasks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15274",
        "abstract url": "https://arxiv.org/abs/2405.15274",
        "title": "Talk to Parallel LiDARs: A Human-LiDAR Interaction Method Based on 3D Visual Grounding",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "LiDAR sensors play a crucial role in various applications, especially in autonomous driving. Current research primarily focuses on optimizing perceptual models with point cloud data as input, while the exploration of deeper cognitive intelligence remains relatively limited. To address this challenge, parallel LiDARs have emerged as a novel theoretical framework for the next-generation intelligent LiDAR systems, which tightly integrate physical, digital, and social systems. To endow LiDAR systems with cognitive capabilities, we introduce the 3D visual grounding task into parallel LiDARs and present a novel human-computer interaction paradigm for LiDAR systems. We propose Talk2LiDAR, a large-scale benchmark dataset tailored for 3D visual grounding in autonomous driving. Additionally, we present a two-stage baseline approach and an efficient one-stage method named BEVGrounding, which significantly improves grounding accuracy by fusing coarse-grained sentence and fine-grained word embeddings with visual features. Our experiments on Talk2Car-3D and Talk2LiDAR datasets demonstrate the superior performance of BEVGrounding, laying a foundation for further research in this domain.",
        "subjects": [
            "cs.CV",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15275",
        "abstract url": "https://arxiv.org/abs/2405.15275",
        "title": "NMGrad: Advancing Histopathological Bladder Cancer Grading with Weakly Supervised Deep Learning",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "Cancer",
                "Clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The most prevalent form of bladder cancer is urothelial carcinoma, characterized by a high recurrence rate and substantial lifetime treatment costs for patients. Grading is a prime factor for patient risk stratification, although it suffers from inconsistencies and variations among pathologists. Moreover, absence of annotations in medical imaging difficults training deep learning models. To address these challenges, we introduce a pipeline designed for bladder cancer grading using histological slides. First, it extracts urothelium tissue tiles at different magnification levels, employing a convolutional neural network for processing for feature extraction. Then, it engages in the slide-level prediction process. It employs a nested multiple instance learning approach with attention to predict the grade. To distinguish different levels of malignancy within specific regions of the slide, we include the origins of the tiles in our analysis. The attention scores at region level is shown to correlate with verified high-grade regions, giving some explainability to the model. Clinical evaluations demonstrate that our model consistently outperforms previous state-of-the-art methods.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "q-bio.QM"
        ],
        "comment": "https://github.com/Biomedical-Data-Analysis-Laboratory/GradeMIL"
    },
    {
        "paper id": "2405.15278",
        "abstract url": "https://arxiv.org/abs/2405.15278",
        "title": "MindShot: Brain Decoding Framework Using Only One Image",
        "rating": "-1",
        "keywords": [
            [
                "biological",
                "fMRI"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Brain decoding, which aims at reconstructing visual stimuli from brain signals, primarily utilizing functional magnetic resonance imaging (fMRI), has recently made positive progress. However, it is impeded by significant challenges such as the difficulty of acquiring fMRI-image pairs and the variability of individuals, etc. Most methods have to adopt the per-subject-per-model paradigm, greatly limiting their applications. To alleviate this problem, we introduce a new and meaningful task, few-shot brain decoding, while it will face two inherent difficulties: 1) the scarcity of fMRI-image pairs and the noisy signals can easily lead to overfitting; 2) the inadequate guidance complicates the training of a robust encoder. Therefore, a novel framework named MindShot, is proposed to achieve effective few-shot brain decoding by leveraging cross-subject prior knowledge. Firstly, inspired by the hemodynamic response function (HRF), the HRF adapter is applied to eliminate unexplainable cognitive differences between subjects with small trainable parameters. Secondly, a Fourier-based cross-subject supervision method is presented to extract additional high-level and low-level biological guidance information from signals of other subjects. Under the MindShot, new subjects and pretrained individuals only need to view images of the same semantic class, significantly expanding the model's applicability. Experimental results demonstrate MindShot's ability of reconstructing semantically faithful images in few-shot scenarios and outperforms methods based on the per-subject-per-model paradigm. The promising results of the proposed method not only validate the feasibility of few-shot brain decoding but also provide the possibility for the learning of large models under the condition of reducing data dependence.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15286",
        "abstract url": "https://arxiv.org/abs/2405.15286",
        "title": "3D Unsupervised Learning by Distilling 2D Open-Vocabulary Segmentation Models for Autonomous Driving",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Point cloud"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Point cloud data labeling is considered a time-consuming and expensive task in autonomous driving, whereas unsupervised learning can avoid it by learning point cloud representations from unannotated data. In this paper, we propose UOV, a novel 3D Unsupervised framework assisted by 2D Open-Vocabulary segmentation models. It consists of two stages: In the first stage, we innovatively integrate high-quality textual and image features of 2D open-vocabulary models and propose the Tri-Modal contrastive Pre-training (TMP). In the second stage, spatial mapping between point clouds and images is utilized to generate pseudo-labels, enabling cross-modal knowledge distillation. Besides, we introduce the Approximate Flat Interaction (AFI) to address the noise during alignment and label confusion. To validate the superiority of UOV, extensive experiments are conducted on multiple related datasets. We achieved a record-breaking 47.73% mIoU on the annotation-free point cloud segmentation task in nuScenes, surpassing the previous best model by 10.70% mIoU. Meanwhile, the performance of fine-tuning with 1% data on nuScenes and SemanticKITTI reached a remarkable 51.75% mIoU and 48.14% mIoU, outperforming all previous pre-trained models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "25 pages, 6 figures, codes are available at https://github.com/sbysbysbys/UOV"
    },
    {
        "paper id": "2405.15320",
        "abstract url": "https://arxiv.org/abs/2405.15320",
        "title": "Organic Data-Driven Approach for Turkish Grammatical Error Correction and LLMs",
        "rating": "-1",
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Grammatical Error Correction has seen significant progress with the recent advancements in deep learning. As those methods require huge amounts of data, synthetic datasets are being built to fill this gap. Unfortunately, synthetic datasets are not organic enough in some cases and even require clean data to start with. Furthermore, most of the work that has been done is focused mostly on English. In this work, we introduce a new organic data-driven approach, clean insertions, to build parallel Turkish Grammatical Error Correction datasets from any organic data, and to clean the data used for training Large Language Models. We achieve state-of-the-art results on two Turkish Grammatical Error Correction test sets out of the three publicly available ones. We also show the effectiveness of our method on the training losses of training language models.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15321",
        "abstract url": "https://arxiv.org/abs/2405.15321",
        "title": "SG-Adapter: Enhancing Text-to-Image Generation with Scene Graph Guidance",
        "rating": "-1",
        "keywords": [
            [
                "diffusion",
                "Text-to-Image"
            ],
            [
                "Graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in text-to-image generation have been propelled by the development of diffusion models and multi-modality learning. However, since text is typically represented sequentially in these models, it often falls short in providing accurate contextualization and structural control. So the generated images do not consistently align with human expectations, especially in complex scenarios involving multiple objects and relationships. In this paper, we introduce the Scene Graph Adapter(SG-Adapter), leveraging the structured representation of scene graphs to rectify inaccuracies in the original text embeddings. The SG-Adapter's explicit and non-fully connected graph representation greatly improves the fully connected, transformer-based text representations. This enhancement is particularly notable in maintaining precise correspondence in scenarios involving multiple relationships. To address the challenges posed by low-quality annotated datasets like Visual Genome, we have manually curated a highly clean, multi-relational scene graph-image paired dataset MultiRels. Furthermore, we design three metrics derived from GPT-4V to effectively and thoroughly measure the correspondence between images and scene graphs. Both qualitative and quantitative results validate the efficacy of our approach in controlling the correspondence in multiple relationships.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15322",
        "abstract url": "https://arxiv.org/abs/2405.15322",
        "title": "Dishonest Approximate Computing: A Coming Crisis for Cloud Clients",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Approximate Computing (AC) has emerged as a promising technique for achieving energy-efficient architectures and is expected to become an effective technique for reducing the electricity cost for cloud service providers (CSP). However, the potential misuse of AC has not received adequate attention, which is a coming crisis behind the blueprint of AC. Driven by the pursuit of illegal financial profits, untrusted CSPs may deploy low-cost AC devices and deceive clients by presenting AC services as promised accurate computing products, while falsely claiming AC outputs as accurate results. This misuse of AC will cause both financial loss and computing degradation to cloud clients. In this paper, we define this malicious attack as DisHonest Approximate Computing (DHAC) and analyze the technical challenges faced by clients in detecting such attacks. To address this issue, we propose two golden model free detection methods: Residual Class Check (RCC) and Forward-Backward Check (FBC). RCC provides clients a low-cost approach to infer the residual class to which a legitimate accurate output should belong. By comparing the residual class of the returned result, clients can determine whether a computing service contains any AC elements. FBC detects potential DHAC by computing an invertible check branch using the intermediate values of the program. It compares the values before entering and after returning from the check branch to identify any discrepancies. Both RCC and FBC can be executed concurrently with real computing tasks, enabling real-time DHAC detection with current inputs. Our experimental results show that both RCC and FBC can detect over 96%-99% of DHAC cases without misjudging any legitimate accurate results.",
        "subjects": [
            "cs.CR",
            "cs.AR"
        ],
        "comment": "12 pages, 9 figures"
    },
    {
        "paper id": "2405.15342",
        "abstract url": "https://arxiv.org/abs/2405.15342",
        "title": "Implementation of New Security Features in CMSWEB Kubernetes Cluster at CERN",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "The CMSWEB cluster is pivotal to the activities of the Compact Muon Solenoid (CMS) experiment, as it hosts critical services required for the operational needs of the CMS experiment. The security of these services and the corresponding data is crucial to CMS. Any malicious attack can compromise the availability of our services. Therefore, it is important to construct a robust security infrastructure. In this work, we discuss new security features introduced to the CMSWEB Kubernetes (\"k8s\") cluster. The new features include the implementation of network policies, deployment of Open Policy Agent (OPA), enforcement of OPA policies, and the integration of Vault. The network policies act as an inside-the-cluster firewall to limit the network communication between the pods to the minimum necessary, and its dynamic nature allows us to work with microservices. The OPA validates the objects against some custom-defined policies during create, update, and delete operations to further enhance security. Without recompiling or changing the configuration of the Kubernetes API server, it can apply customized policies on Kubernetes objects and their audit functionality enabling us to detect pre-existing conflicts and issues. Although Kubernetes incorporates the concepts of secrets, they are only base64 encoded and are not dynamically configured. This is where Vault comes into play: Vault dynamically secures, stores, and tightly controls access to sensitive data. This way, the secret information is encrypted, secured, and centralized, making it more scalable and easier to manage. Thus, the implementation of these three security features corroborate the enhanced security and reliability of the CMSWEB Kubernetes infrastructure.",
        "subjects": [
            "cs.CR",
            "cs.DC"
        ],
        "comment": "26TH INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY & NUCLEAR PHYSICS - 2023"
    },
    {
        "paper id": "2405.15372",
        "abstract url": "https://arxiv.org/abs/2405.15372",
        "title": "When far is better: The Chamberlin-Courant approach to obnoxious committee selection",
        "rating": "-1",
        "keywords": [
            [
                "avatar"
            ]
        ],
        "abstract": "Classical work on metric space based committee selection problem interprets distance as ``near is better''. In this work, motivated by real-life situations, we interpret distance as ``far is better''. Formally stated, we initiate the study of ``obnoxious'' committee scoring rules when the voters' preferences are expressed via a metric space. To this end, we propose a model where large distances imply high satisfaction and study the egalitarian avatar of the well-known Chamberlin-Courant voting rule and some of its generalizations. For a given integer value $1 \\le \u03bb\\le k$, the committee size k, a voter derives satisfaction from only the $\u03bb$-th favorite committee member; the goal is to maximize the satisfaction of the least satisfied voter. For the special case of $\u03bb= 1$, this yields the egalitarian Chamberlin-Courant rule. In this paper, we consider general metric space and the special case of a $d$-dimensional Euclidean space. We show that when $\u03bb$ is $1$ and $k$, the problem is polynomial-time solvable in $\\mathbb{R}^2$ and general metric space, respectively. However, for $\u03bb= k-1$, it is NP-hard even in $\\mathbb{R}^2$. Thus, we have ``double-dichotomy'' in $\\mathbb{R}^2$ with respect to the value of \u03bb, where the extreme cases are solvable in polynomial time but an intermediate case is NP-hard. Furthermore, this phenomenon appears to be ``tight'' for $\\mathbb{R}^2$ because the problem is NP-hard for general metric space, even for $\u03bb=1$. Consequently, we are motivated to explore the problem in the realm of (parameterized) approximation algorithms and obtain positive results. Interestingly, we note that this generalization of Chamberlin-Courant rules encodes practical constraints that are relevant to solutions for certain facility locations.",
        "subjects": [
            "cs.DS",
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15394",
        "abstract url": "https://arxiv.org/abs/2405.15394",
        "title": "Leveraging knowledge distillation for partial multi-task learning from multiple remote sensing datasets",
        "rating": "-1",
        "keywords": [
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Partial multi-task learning where training examples are annotated for one of the target tasks is a promising idea in remote sensing as it allows combining datasets annotated for different tasks and predicting more tasks with fewer network parameters. The na\u00efve approach to partial multi-task learning is sub-optimal due to the lack of all-task annotations for learning joint representations. This paper proposes using knowledge distillation to replace the need of ground truths for the alternate task and enhance the performance of such approach. Experiments conducted on the public ISPRS 2D Semantic Labeling Contest dataset show the effectiveness of the proposed idea on partial multi-task learning for semantic tasks including object detection and semantic segmentation in aerial images.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted for oral presentation at IGARSS 2024"
    },
    {
        "paper id": "2405.15416",
        "abstract url": "https://arxiv.org/abs/2405.15416",
        "title": "Planar Cycle-Extendable Graphs",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "For most problems pertaining to perfect matchings, one may restrict attention to matching covered graphs -- that is, connected nontrivial graphs with the property that each edge belongs to some perfect matching. There is extensive literature on these graphs that are also known as $1$-extendable graphs (since each edge extends to a perfect matching) including an ear decomposition theorem due to Lovasz and Plummer. A cycle $C$ of a graph $G$ is conformal if $G-V(C)$ has a perfect matching; such cycles play an important role in the study of perfect matchings, especially when investigating the Pfaffian orientation problem. A matching covered graph $G$ is cycle-extendable if -- for each even cycle $C$ -- the cycle $C$ is conformal, or equivalently, each perfect matching of $C$ extends to a perfect matching of $G$, or equivalently, $C$ is the symmetric difference of two perfect matchings of $G$, or equivalently, $C$ extends to an ear decomposition of $G$. In the literature, these are also known as cycle-nice or as $1$-cycle resonant graphs. Zhang, Wang, Yuan, Ng and Cheng [Discrete Mathematics, 345:7 (2022), 112876] provided a characterization of claw-free cycle-extendable graphs. Guo and Zhang [Discrete Mathematics, 275:1-3 (2004), 151-164] and independently Zhang and Li [Discrete Applied Mathematics, 160:13-14 (2012), 2069-2074], provided characterizations of bipartite planar cycle-extendable graphs. In this paper, we establish a characterization of all planar cycle-extendable graphs -- in terms of $K_2$ and four infinite families.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "Submitted to Electronic Journal of Combinatorics"
    },
    {
        "paper id": "2405.15425",
        "abstract url": "https://arxiv.org/abs/2405.15425",
        "title": "Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "voxel",
                "radiance fields"
            ],
            [
                "flight"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a volumetric representation based on primitives to model scattering and emissive media. Accurate scene representations enabling efficient rendering are essential for many computer graphics applications. General and unified representations that can handle surface and volume-based representations simultaneously, allowing for physically accurate modeling, remain a research challenge. Inspired by recent methods for scene reconstruction that leverage mixtures of 3D Gaussians to model radiance fields, we formalize and generalize the modeling of scattering and emissive media using mixtures of simple kernel-based volumetric primitives. We introduce closed-form solutions for transmittance and free-flight distance sampling for 3D Gaussian kernels, and propose several optimizations to use our method efficiently within any off-the-shelf volumetric path tracer by leveraging ray tracing for efficiently querying the medium. We demonstrate our method as an alternative to other forms of volume modeling (e.g. voxel grid-based representations) for forward and inverse rendering of scattering media. Furthermore, we adapt our method to the problem of radiance field optimization and rendering, and demonstrate comparable performance to the state of the art, while providing additional flexibility in terms of performance and usability.",
        "subjects": [
            "cs.GR",
            "cs.CV"
        ],
        "comment": "17 pages, 10 figures"
    },
    {
        "paper id": "2405.15428",
        "abstract url": "https://arxiv.org/abs/2405.15428",
        "title": "Enhancing Pollinator Conservation towards Agriculture 4.0: Monitoring of Bees through Object Recognition",
        "rating": "-1",
        "keywords": [
            [
                "survival"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In an era of rapid climate change and its adverse effects on food production, technological intervention to monitor pollinator conservation is of paramount importance for environmental monitoring and conservation for global food security. The survival of the human species depends on the conservation of pollinators. This article explores the use of Computer Vision and Object Recognition to autonomously track and report bee behaviour from images. A novel dataset of 9664 images containing bees is extracted from video streams and annotated with bounding boxes. With training, validation and testing sets (6722, 1915, and 997 images, respectively), the results of the COCO-based YOLO model fine-tuning approaches show that YOLOv5m is the most effective approach in terms of recognition accuracy. However, YOLOv5s was shown to be the most optimal for real-time bee detection with an average processing and inference time of 5.1ms per video frame at the cost of slightly lower ability. The trained model is then packaged within an explainable AI interface, which converts detection events into timestamped reports and charts, with the aim of facilitating use by non-technical users such as expert stakeholders from the apiculture industry towards informing responsible consumption and production.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15432",
        "abstract url": "https://arxiv.org/abs/2405.15432",
        "title": "Throughput Requirements for RAN Functional Splits in 3D-Networks",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "The rapid growth of non-terrestrial communication necessitates its integration with existing terrestrial networks, as highlighted in 3GPP Releases 16 and 17. This paper analyses the concept of functional splits in 3D-Networks. To manage this complex structure effectively, the adoption of a Radio Access Network (RAN) architecture with Functional Split (FS) offers advantages in flexibility, scalability, and cost-efficiency. RAN achieves this by disaggregating functionalities into three separate units. Analogous to the terrestrial network approach, 3GPP is extending this concept to non-terrestrial platforms as well. This work presents a general analysis of the requested Fronthaul (FH) data rate on feeder link between a non-terrestrial platform and the ground-station. Each split option is a trade-of between FH data rate and the respected complexity. Since flying nodes face more limitations regarding power consumption and complexity on board in comparison to terrestrial ones, we are investigating the split options between lower and higher physical layer.",
        "subjects": [
            "eess.SP",
            "cs.NI"
        ],
        "comment": "submitted to Globecom2024 SELECTED AREAS IN COMMUNICATIONS SATELLITE AND SPACE COMMUNICATIONS"
    },
    {
        "paper id": "2405.15434",
        "abstract url": "https://arxiv.org/abs/2405.15434",
        "title": "Biometrics and Behavioral Modelling for Detecting Distractions in Online Learning",
        "rating": "-1",
        "keywords": [
            [
                "Biometrics",
                "physiological"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In this article, we explore computer vision approaches to detect abnormal head pose during e-learning sessions and we introduce a study on the effects of mobile phone usage during these sessions. We utilize behavioral data collected from 120 learners monitored while participating in a MOOC learning sessions. Our study focuses on the influence of phone-usage events on behavior and physiological responses, specifically attention, heart rate, and meditation, before, during, and after phone usage. Additionally, we propose an approach for estimating head pose events using images taken by the webcam during the MOOC learning sessions to detect phone-usage events. Our hypothesis suggests that head posture undergoes significant changes when learners interact with a mobile phone, contrasting with the typical behavior seen when learners face a computer during e-learning sessions. We propose an approach designed to detect deviations in head posture from the average observed during a learner's session, operating as a semi-supervised method. This system flags events indicating alterations in head posture for subsequent human review and selection of mobile phone usage occurrences with a sensitivity over 90%.",
        "subjects": [
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "Accepted in CEDI 2024 (VII Congreso Espa\u00f1ol de Inform\u00e1tica), A Coru\u00f1a, Spain"
    },
    {
        "paper id": "2405.15442",
        "abstract url": "https://arxiv.org/abs/2405.15442",
        "title": "Towards Precision Healthcare: Robust Fusion of Time Series and Image Data",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "Healthcare",
                "clinical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "With the increasing availability of diverse data types, particularly images and time series data from medical experiments, there is a growing demand for techniques designed to combine various modalities of data effectively. Our motivation comes from the important areas of predicting mortality and phenotyping where using different modalities of data could significantly improve our ability to predict. To tackle this challenge, we introduce a new method that uses two separate encoders, one for each type of data, allowing the model to understand complex patterns in both visual and time-based information. Apart from the technical challenges, our goal is to make the predictive model more robust in noisy conditions and perform better than current methods. We also deal with imbalanced datasets and use an uncertainty loss function, yielding improved results while simultaneously providing a principled means of modeling uncertainty. Additionally, we include attention mechanisms to fuse different modalities, allowing the model to focus on what's important for each task. We tested our approach using the comprehensive multimodal MIMIC dataset, combining MIMIC-IV and MIMIC-CXR datasets. Our experiments show that our method is effective in improving multimodal deep learning for clinical applications. The code will be made available online.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15449",
        "abstract url": "https://arxiv.org/abs/2405.15449",
        "title": "Faster $(\u0394+ 1)$-Edge Coloring: Breaking the $m \\sqrt{n}$ Time Barrier",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Vizing's theorem states that any $n$-vertex $m$-edge graph of maximum degree $\u0394$ can be {\\em edge colored} using at most $\u0394+ 1$ different colors [Diskret.~Analiz, '64]. Vizing's original proof is algorithmic and shows that such an edge coloring can be found in $\\tilde{O}(mn)$ time. This was subsequently improved to $\\tilde O(m\\sqrt{n})$, independently by Arjomandi [1982] and by Gabow et al.~[1985]. In this paper we present an algorithm that computes such an edge coloring in $\\tilde O(mn^{1/3})$ time, giving the first polynomial improvement for this fundamental problem in over 40 years.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "Started to circulate in April 2024"
    },
    {
        "paper id": "2405.15453",
        "abstract url": "https://arxiv.org/abs/2405.15453",
        "title": "Benchmarking Pre-trained Large Language Models' Potential Across Urdu NLP tasks",
        "rating": "-1",
        "keywords": [
            [
                "quality assessment"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) pre-trained on multilingual data have revolutionized natural language processing research, by transitioning from languages and task specific model pipelines to a single model adapted on a variety of tasks. However majority of existing multilingual NLP benchmarks for LLMs provide evaluation data in only few languages with little linguistic diversity. In addition these benchmarks lack quality assessment against the respective state-of the art models. This study presents an in-depth examination of prominent LLMs; GPT-3.5-turbo, Llama2-7B-Chat, Bloomz 7B1 and Bloomz 3B, across 14 tasks using 15 Urdu datasets, in a zero-shot setting, and their performance against state-of-the-art (SOTA) models, has been compared and analysed. Our experiments show that SOTA models surpass all the encoder-decoder pre-trained language models in all Urdu NLP tasks with zero-shot learning. Our results further show that LLMs with fewer parameters, but more language specific data in the base model perform better than larger computational models, but low language data.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15475",
        "abstract url": "https://arxiv.org/abs/2405.15475",
        "title": "Efficient Degradation-aware Any Image Restoration",
        "rating": "-1",
        "keywords": [
            [
                "Image Restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reconstructing missing details from degraded low-quality inputs poses a significant challenge. Recent progress in image restoration has demonstrated the efficacy of learning large models capable of addressing various degradations simultaneously. Nonetheless, these approaches introduce considerable computational overhead and complex learning paradigms, limiting their practical utility. In response, we propose \\textit{DaAIR}, an efficient All-in-One image restorer employing a Degradation-aware Learner (DaLe) in the low-rank regime to collaboratively mine shared aspects and subtle nuances across diverse degradations, generating a degradation-aware embedding. By dynamically allocating model capacity to input degradations, we realize an efficient restorer integrating holistic and specific learning within a unified model. Furthermore, DaAIR introduces a cost-efficient parameter update mechanism that enhances degradation awareness while maintaining computational efficiency. Extensive comparisons across five image degradations demonstrate that our DaAIR outperforms both state-of-the-art All-in-One models and degradation-specific counterparts, affirming our efficacy and practicality. The source will be publicly made available at \\url{https://eduardzamfir.github.io/daair/}",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15477",
        "abstract url": "https://arxiv.org/abs/2405.15477",
        "title": "MagicBathyNet: A Multimodal Remote Sensing Dataset for Bathymetry Prediction and Pixel-based Classification in Shallow Waters",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Accurate, detailed, and high-frequent bathymetry, coupled with complex semantic content, is crucial for the undermapped shallow seabed areas facing intense climatological and anthropogenic pressures. Current methods exploiting remote sensing images to derive bathymetry or seabed classes mainly exploit non-open data. This lack of openly accessible benchmark archives prevents the wider use of deep learning methods in such applications. To address this issue, in this paper we present the MagicBathyNet, which is a benchmark dataset made up of image patches of Sentinel2, SPOT-6 and aerial imagery, bathymetry in raster format and annotations of seabed classes. MagicBathyNet is then exploited to benchmark state-of-the-art methods in learning-based bathymetry and pixel-based classification. Dataset, pre-trained weights, and code are publicly available at www.magicbathy.eu/magicbathynet.html.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "5 pages, 3 figures, 5 tables. Accepted at IEEE International Geoscience and Remote Sensing Symposium (IGARSS) 2024"
    },
    {
        "paper id": "2405.15492",
        "abstract url": "https://arxiv.org/abs/2405.15492",
        "title": "Finding Induced Subgraphs from Graphs with Small Mim-Width",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In the last decade, algorithmic frameworks based on a structural graph parameter called mim-width have been developed to solve generally NP-hard problems. However, it is known that the frameworks cannot be applied to the Clique problem, and the complexity status of many problems of finding dense induced subgraphs remains open when parameterized by mim-width. In this paper, we investigate the complexity of the problem of finding a maximum induced subgraph that satisfies prescribed properties from a given graph with small mim-width. We first give a meta-theorem implying that various induced subgraph problems are NP-hard for bounded mim-width graphs. Moreover, we show that some problems, including Clique and Induced Cluster Subgraph, remain NP-hard even for graphs with (linear) mim-width at most 2. In contrast to the intractability, we provide an algorithm that, given a graph and its branch decomposition with mim-width at most 1, solves Induced Cluster Subgraph in polynomial time. We emphasize that our algorithmic technique is applicable to other problems such as Induced Polar Subgraph and Induced Split Subgraph. Since a branch decomposition with mim-width at most 1 can be constructed in polynomial time for block graphs, interval graphs, permutation graphs, cographs, distance-hereditary graphs, convex graphs, and their complement graphs, our positive results reveal the polynomial-time solvability of various problems for these graph classes.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "To appear in the proceedings of the 19th Scandinavian Symposium on Algorithm Theory (SWAT 2024)"
    },
    {
        "paper id": "2405.15518",
        "abstract url": "https://arxiv.org/abs/2405.15518",
        "title": "Feature Splatting for Better Novel View Synthesis with Low Overlap",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "Synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives. However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views. In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are first \"splatted\" into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values. To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information. Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views. Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels. We will release the code upon acceptance. Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15537",
        "abstract url": "https://arxiv.org/abs/2405.15537",
        "title": "Do Not Trust Power Management: Challenges and Hints for Securing Future Trusted Execution Environments",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Over the past few years, several research groups have introduced innovative hardware designs for Trusted Execution Environments (TEEs), aiming to secure applications against potentially compromised privileged software, including the kernel. Since 2017, Tang et al. introduced a new class of software-enabled hardware attacks, which leverages energy management mechanisms. These attacks aim at bypassing TEE security guarantees and exposing sensitive information like cryptographic keys. They have increased in prevalence over the past few years. Despite that, current RISC-V TEE architectures have yet to incorporate them into their threat models. Proprietary implementations, such as Arm TrustZone and Intel SGX, embed countermeasures. However, these countermeasures are not viable in the long term and hinder the capabilities of energy management mechanisms. This article presents the first comprehensive knowledge survey of these attacks, along with an evaluation of literature countermeasures. Our analysis highlights a substantial security gap between assumed threat models and the actual ones, presenting considerable threats in modern systems-on-chip that can undermine even the security guarantees provided by TEEs. We advocate for the enhancement of the next generation of RISC-V TEEs to address these attacks within their threat models, and we believe this study will spur further community efforts in this direction.",
        "subjects": [
            "cs.CR",
            "cs.AR",
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15543",
        "abstract url": "https://arxiv.org/abs/2405.15543",
        "title": "A tame vs. feral dichotomy for graph classes excluding an induced minor or induced topological minor",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A minimal separator in a graph is an inclusion-minimal set of vertices that separates some fixed pair of nonadjacent vertices. A graph class is said to be tame if there exists a polynomial upper bound for the number of minimal separators of every graph in the class, and feral if it contains arbitrarily large graphs with exponentially many minimal separators. Building on recent works of Gartland and Lokshtanov [SODA 2023] and Gajarsk\u00fd, Jaffke, Lima, Novotn\u00e1, Pilipczuk, Rz\u0105\u017cewski, and Souza [arXiv, 2022], we show that every graph class defined by a single forbidden induced minor or induced topological minor is either tame or feral, and classify the two cases. This leads to new graph classes in which Maximum Weight Independent Set and many other problems are solvable in polynomial time. We complement the classification results with polynomial-time recognition algorithms for the maximal tame graph classes appearing in the obtained classifications.",
        "subjects": [
            "math.CO",
            "cs.DM",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15550",
        "abstract url": "https://arxiv.org/abs/2405.15550",
        "title": "CowScreeningDB: A public benchmark dataset for lameness detection in dairy cows",
        "rating": "-1",
        "keywords": [
            [
                "pathological"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Lameness is one of the costliest pathological problems affecting dairy animals. It is usually assessed by trained veterinary clinicians who observe features such as gait symmetry or gait parameters as step counts in real-time. With the development of artificial intelligence, various modular systems have been proposed to minimize subjectivity in lameness assessment. However, the major limitation in their development is the unavailability of a public dataset which is currently either commercial or privately held. To tackle this limitation, we have introduced CowScreeningDB which was created using sensory data. This dataset was sourced from 43 cows at a dairy located in Gran Canaria, Spain. It consists of a multi-sensor dataset built on data collected using an Apple Watch 6 during the normal daily routine of a dairy cow. Thanks to the collection environment, sampling technique, information regarding the sensors, the applications used for data conversion and storage make the dataset a transparent one. This transparency of data can thus be used for further development of techniques for lameness detection for dairy cows which can be objectively compared. Aside from the public sharing of the dataset, we have also shared a machine-learning technique which classifies the caws in healthy and lame by using the raw sensory data. Hence validating the major objective which is to establish the relationship between sensor data and lameness.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15553",
        "abstract url": "https://arxiv.org/abs/2405.15553",
        "title": "Massive MIMO-ISAC System With 1-Bit ADCs/DACs",
        "rating": "-1",
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "This paper investigates a hardware-efficient massive multiple-input multiple-output integrated sensing and communication (MIMO-ISAC) system with 1-bit analog-to-digital converters (ADCs)/digital-to-analog converters (DACs). The proposed system, referred to as 1BitISAC, employs 1-bit DACs at the ISAC transmitter and 1-bit ADCs at the sensing receiver, achieving significant reductions in power consumption and hardware costs. For such kind of systems, two 1BitISAC joint transceiver designs, i.e., i) quality of service constrained 1BitISAC design and ii) quality of detection constrained design, are considered and the corresponding problems are formulated. In order to address these problems, we thoroughly analyze the radar detection performance after 1-bit ADCs quantization and the communication bit error rate. This analysis yields new design insights and leads to unique radar and communication metrics, which enables us to simplify the original problems and employ majorization-minimization and integer linear programming methods to solve the problems. Numerical results are provided to validate the performance analysis of the proposed 1BitISAC and to compare with other ISAC configurations. The superiority of the proposed 1BitISAC system in terms of balancing ISAC performance and energy efficiency is also demonstrated.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15563",
        "abstract url": "https://arxiv.org/abs/2405.15563",
        "title": "Heterogeneous virus classification using a functional deep learning model based on transmission electron microscopy images (Preprint)",
        "rating": "-1",
        "keywords": [
            [
                "biological",
                "diagnosis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Viruses are submicroscopic agents that can infect all kinds of lifeforms and use their hosts' living cells to replicate themselves. Despite having some of the simplest genetic structures among all living beings, viruses are highly adaptable, resilient, and given the right conditions, are capable of causing unforeseen complications in their hosts' bodies. Due to their multiple transmission pathways, high contagion rate, and lethality, viruses are the biggest biological threat faced by animal and plant species. It is often challenging to promptly detect the presence of a virus in a possible host's body and accurately determine its type using manual examination techniques; however, it can be done using computer-based automatic diagnosis methods. Most notably, the analysis of Transmission Electron Microscopy (TEM) images has been proven to be quite successful in instant virus identification. Using TEM images collected from a recently published dataset, this article proposes a deep learning-based classification model to identify the type of virus within those images correctly. The methodology of this study includes two coherent image processing techniques to reduce the noise present in the raw microscopy images. Experimental results show that it can differentiate among the 14 types of viruses present in the dataset with a maximum of 97.44% classification accuracy and F1-score, which asserts the effectiveness and reliability of the proposed method. Implementing this scheme will impart a fast and dependable way of virus identification subsidiary to the thorough diagnostic procedures.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15567",
        "abstract url": "https://arxiv.org/abs/2405.15567",
        "title": "PyCellMech: A shape-based feature extraction pipeline for use in medical and biological studies",
        "rating": "-1",
        "keywords": [
            [
                "biological",
                "medical",
                "disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Summary: Medical researchers obtain knowledge about the prevention and treatment of disability and disease using physical measurements and image data. To assist in this endeavor, feature extraction packages are available that are designed to collect data from the image structure. In this study, we aim to augment current works by adding to the current mix of shape-based features. The significance of shape-based features has been explored extensively in research for several decades, but there is no single package available in which all shape-related features can be extracted easily by the researcher. PyCellMech has been crafted to address this gap. The PyCellMech package extracts three classes of shape features, which are classified as one-dimensional, geometric, and polygonal. Future iterations will be expanded to include other feature classes, such as scale-space. Availability and implementation: PyCellMech is freely available at https://github.com/icm-dac/pycellmech.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 1 figure, 1 table, in submission"
    },
    {
        "paper id": "2405.15587",
        "abstract url": "https://arxiv.org/abs/2405.15587",
        "title": "Composed Image Retrieval for Remote Sensing",
        "rating": "-1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "text-to-image"
            ],
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This work introduces composed image retrieval to remote sensing. It allows to query a large image archive by image examples alternated by a textual description, enriching the descriptive power over unimodal queries, either visual or textual. Various attributes can be modified by the textual part, such as shape, color, or context. A novel method fusing image-to-image and text-to-image similarity is introduced. We demonstrate that a vision-language model possesses sufficient descriptive power and no further learning step or training data are necessary. We present a new evaluation benchmark focused on color, context, density, existence, quantity, and shape modifications. Our work not only sets the state-of-the-art for this task, but also serves as a foundational step in addressing a gap in the field of remote sensing image retrieval. Code at: https://github.com/billpsomas/rscir",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted for ORAL presentation at the 2024 IEEE International Geoscience and Remote Sensing Symposium"
    },
    {
        "paper id": "2405.15596",
        "abstract url": "https://arxiv.org/abs/2405.15596",
        "title": "Multimodal Object Detection via Probabilistic a priori Information Integration",
        "rating": "-1",
        "keywords": [
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal object detection has shown promise in remote sensing. However, multimodal data frequently encounter the problem of low-quality, wherein the modalities lack strict cell-to-cell alignment, leading to mismatch between different modalities. In this paper, we investigate multimodal object detection where only one modality contains the target object and the others provide crucial contextual information. We propose to resolve the alignment problem by converting the contextual binary information into probability maps. We then propose an early fusion architecture that we validate with extensive experiments on the DOTA dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15613",
        "abstract url": "https://arxiv.org/abs/2405.15613",
        "title": "Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach",
        "rating": "-1",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Self-supervised features are the cornerstone of modern machine learning systems. They are typically pre-trained on data collections whose construction and curation typically require extensive human effort. This manual process has some limitations similar to those encountered in supervised learning, e.g., the crowd-sourced selection of data is costly and time-consuming, preventing scaling the dataset size. In this work, we consider the problem of automatic curation of high-quality datasets for self-supervised pre-training. We posit that such datasets should be large, diverse and balanced, and propose a clustering-based approach for building ones satisfying all these criteria. Our method involves successive and hierarchical applications of $k$-means on a large and diverse data repository to obtain clusters that distribute uniformly among data concepts, followed by a hierarchical, balanced sampling step from these clusters. Extensive experiments on three different data domains including web-based images, satellite images and text show that features trained on our automatically curated datasets outperform those trained on uncurated data while being on par or better than ones trained on manually curated data.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15619",
        "abstract url": "https://arxiv.org/abs/2405.15619",
        "title": "DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Monocular camera calibration is a key precondition for numerous 3D vision applications. Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data. Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images. This success suggests a strong potential of the models to effectively understand varied visual information. In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation. Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task. The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models. The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference. Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation. Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors. Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15638",
        "abstract url": "https://arxiv.org/abs/2405.15638",
        "title": "M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models",
        "rating": "-1",
        "keywords": [
            [
                "Healthcare"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Multilingual multimodal reasoning is a core component in achieving human-level intelligence. However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores. This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored. In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 8,931 samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German. Using M4U, we conduct extensive evaluations of 21 leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results show that the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences. Our in-depth analysis indicates that leading LMMs, including GPT-4o, suffer performance degradation when prompted with cross-lingual multimodal questions, such as images with key textual information in Chinese while the question is in German. We believe that M4U can serve as a crucial tool for systematically evaluating LMMs based on their multilingual multimodal reasoning capabilities and monitoring their development. The homepage, codes and data are public available.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2405.15646",
        "abstract url": "https://arxiv.org/abs/2405.15646",
        "title": "LLM-based Robot Task Planning with Exceptional Handling for General Purpose Service Robots",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "The development of a general purpose service robot for daily life necessitates the robot's ability to deploy a myriad of fundamental behaviors judiciously. Recent advancements in training Large Language Models (LLMs) can be used to generate action sequences directly, given an instruction in natural language with no additional domain information. However, while the outputs of LLMs are semantically correct, the generated task plans may not accurately map to acceptable actions and might encompass various linguistic ambiguities. LLM hallucinations pose another challenge for robot task planning, which results in content that is inconsistent with real-world facts or user inputs. In this paper, we propose a task planning method based on a constrained LLM prompt scheme, which can generate an executable action sequence from a command. An exceptional handling module is further proposed to deal with LLM hallucinations problem. This module can ensure the LLM-generated results are admissible in the current environment. We evaluate our method on the commands generated by the RoboCup@Home Command Generator, observing that the robot demonstrates exceptional performance in both comprehending instructions and executing tasks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15652",
        "abstract url": "https://arxiv.org/abs/2405.15652",
        "title": "$$\\mathbf{L^2\\cdot M = C^2}$$ Large Language Models as Covert Channels... a Systematic Analysis",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have gained significant popularity in the last few years due to their performance in diverse tasks such as translation, prediction, or content generation. At the same time, the research community has shown that LLMs are susceptible to various attacks but can also improve the security of diverse systems. However, besides enabling more secure systems, how well do open source LLMs behave as covertext distributions to, e.g., facilitate censorship resistant communication? In this paper, we explore the capabilities of open-source LLM-based covert channels. We approach this problem from the experimental side by empirically measuring the security vs. capacity of the open-source LLM model (Llama-7B) to assess how well it performs as a covert channel. Although our results indicate that such channels are not likely to achieve high practical bitrates, which depend on message length and model entropy, we also show that the chance for an adversary to detect covert communication is low. To ensure that our results can be used with the least effort as a general reference, we employ a conceptually simple and concise scheme and only assume public models.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15660",
        "abstract url": "https://arxiv.org/abs/2405.15660",
        "title": "Low-Light Video Enhancement via Spatial-Temporal Consistent Illumination and Reflection Decomposition",
        "rating": "-1",
        "keywords": [
            [
                "Video Enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Low-Light Video Enhancement (LLVE) seeks to restore dynamic and static scenes plagued by severe invisibility and noise. One critical aspect is formulating a consistency constraint specifically for temporal-spatial illumination and appearance enhanced versions, a dimension overlooked in existing methods. In this paper, we present an innovative video Retinex-based decomposition strategy that operates without the need for explicit supervision to delineate illumination and reflectance components. We leverage dynamic cross-frame correspondences for intrinsic appearance and enforce a scene-level continuity constraint on the illumination field to yield satisfactory consistent decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a novel cross-frame interaction mechanism. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features, thus achieving the desired temporal propagation. Extensive experiments are conducted on widely recognized LLVE benchmarks, covering diverse scenarios. Our framework consistently outperforms existing methods, establishing a new state-of-the-art (SOTA) performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15663",
        "abstract url": "https://arxiv.org/abs/2405.15663",
        "title": "Soft happy colourings and community structure of networks",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "For $0<\u03c1\\leq 1$, a $\u03c1$-happy vertex $v$ in a coloured graph $G$ has at least $\u03c1\\cdot \\mathrm{deg}(v)$ same-colour neighbours, and a $\u03c1$-happy colouring (aka soft happy colouring) of $G$ is a vertex colouring that makes all the vertices $\u03c1$-happy. A community is a subgraph whose vertices are more adjacent to themselves than the rest of the vertices. Graphs with community structures can be modelled by random graph models such as the stochastic block model (SBM). In this paper, we present several theorems showing that both of these notions are related, with numerous real-world applications. We show that, with high probability, communities of graphs in the stochastic block model induce $\u03c1$-happy colouring on all vertices if certain conditions on the model parameters are satisfied. Moreover, a probabilistic threshold on $\u03c1$ is derived so that communities of a graph in the SBM induce a $\u03c1$-happy colouring. Furthermore, the asymptotic behaviour of $\u03c1$-happy colouring induced by the graph's communities is discussed when $\u03c1$ is less than a threshold. We develop heuristic polynomial-time algorithms for soft happy colouring that often correlate with the graphs' community structure. Finally, we present an experimental evaluation to compare the performance of the proposed algorithms thereby demonstrating the validity of the theoretical results.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "30 pages, 12 figures, 1 table"
    },
    {
        "paper id": "2405.15664",
        "abstract url": "https://arxiv.org/abs/2405.15664",
        "title": "GroundGrid:LiDAR Point Cloud Ground Segmentation and Terrain Estimation",
        "rating": "-1",
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The precise point cloud ground segmentation is a crucial prerequisite of virtually all perception tasks for LiDAR sensors in autonomous vehicles. Especially the clustering and extraction of objects from a point cloud usually relies on an accurate removal of ground points. The correct estimation of the surrounding terrain is important for aspects of the drivability of a surface, path planning, and obstacle prediction. In this article, we propose our system GroundGrid which relies on 2D elevation maps to solve the terrain estimation and point cloud ground segmentation problems. We evaluate the ground segmentation and terrain estimation performance of GroundGrid and compare it to other state-of-the-art methods using the SemanticKITTI dataset and a novel evaluation method relying on airborne LiDAR scanning. The results show that GroundGrid is capable of outperforming other state-of-the-art systems with an average IoU of 94.78% while maintaining a high run-time performance of 171Hz. The source code is available at https://github.com/dcmlr/groundgrid",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "This letter has been accepted for publication in IEEE Robotics and Automation Letters"
    },
    {
        "paper id": "2405.15688",
        "abstract url": "https://arxiv.org/abs/2405.15688",
        "title": "UNION: Unsupervised 3D Object Detection using Object Appearance-based Pseudo-Classes",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Unsupervised 3D object detection methods have emerged to leverage vast amounts of data efficiently without requiring manual labels for training. Recent approaches rely on dynamic objects for learning to detect objects but penalize the detections of static instances during training. Multiple rounds of (self) training are used in which detected static instances are added to the set of training targets; this procedure to improve performance is computationally expensive. To address this, we propose the method UNION. We use spatial clustering and self-supervised scene flow to obtain a set of static and dynamic object proposals from LiDAR. Subsequently, object proposals' visual appearances are encoded to distinguish static objects in the foreground and background by selecting static instances that are visually similar to dynamic objects. As a result, static and dynamic foreground objects are obtained together, and existing detectors can be trained with a single training. In addition, we extend 3D object discovery to detection by using object appearance-based cluster labels as pseudo-class labels for training object classification. We conduct extensive experiments on the nuScenes dataset and increase the state-of-the-art performance for unsupervised object discovery, i.e. UNION more than doubles the average precision to 33.9. The code will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2405.15700",
        "abstract url": "https://arxiv.org/abs/2405.15700",
        "title": "Trackastra: Transformer-based cell tracking for live-cell microscopy",
        "rating": "-1",
        "keywords": [
            [
                "biologically"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Cell tracking is an omnipresent image analysis task in live-cell microscopy. It is similar to multiple object tracking (MOT), however, each frame contains hundreds of similar-looking objects that can divide, making it a challenging problem. Current state-of-the-art approaches follow the tracking-by-detection paradigm, i.e. first all cells are detected per frame and successively linked in a second step to form biologically consistent cell tracks. Linking is commonly solved via discrete optimization methods, which require manual tuning of hyperparameters for each dataset and are therefore cumbersome to use in practice. Here we propose Trackastra, a general purpose cell tracking approach that uses a simple transformer architecture to directly learn pairwise associations of cells within a temporal window from annotated data. Importantly, unlike existing transformer-based MOT pipelines, our learning architecture also accounts for dividing objects such as cells and allows for accurate tracking even with simple greedy linking, thus making strides towards removing the requirement for a complex linking step. The proposed architecture operates on the full spatio-temporal context of detections within a time window by avoiding the computational burden of processing dense images. We show that our tracking approach performs on par with or better than highly tuned state-of-the-art cell tracking algorithms for various biological datasets, such as bacteria, cell cultures and fluorescent particles. We provide code at https://github.com/weigertlab/trackastra.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15724",
        "abstract url": "https://arxiv.org/abs/2405.15724",
        "title": "Reconfiguration Algorithms for Cubic Modular Robots with Realistic Movement Constraints",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "We introduce and analyze a model for self-reconfigurable robots made up of unit-cube modules. Compared to past models, our model aims to newly capture two important practical aspects of real-world robots. First, modules often do not occupy an exact unit cube, but rather have features like bumps extending outside the allotted space so that modules can interlock. Thus, for example, our model forbids modules from squeezing in between two other modules that are one unit distance apart. Second, our model captures the practical scenario of many passive modules assembled by a single robot, instead of requiring all modules to be able to move on their own. We prove two universality results. First, with a supply of auxiliary modules, we show that any connected polycube structure can be constructed by a carefully aligned plane sweep. Second, without additional modules, we show how to construct any structure for which a natural notion of external feature size is at least a constant; this property largely consolidates forbidden-pattern properties used in previous works on reconfigurable modular robots.",
        "subjects": [
            "cs.CG",
            "cs.DM",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15753",
        "abstract url": "https://arxiv.org/abs/2405.15753",
        "title": "Data Reconstruction: When You See It and When You Don't",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "We revisit the fundamental question of formally defining what constitutes a reconstruction attack. While often clear from the context, our exploration reveals that a precise definition is much more nuanced than it appears, to the extent that a single all-encompassing definition may not exist. Thus, we employ a different strategy and aim to \"sandwich\" the concept of reconstruction attacks by addressing two complementing questions: (i) What conditions guarantee that a given system is protected against such attacks? (ii) Under what circumstances does a given attack clearly indicate that a system is not protected? More specifically, * We introduce a new definitional paradigm -- Narcissus Resiliency -- to formulate a security definition for protection against reconstruction attacks. This paradigm has a self-referential nature that enables it to circumvent shortcomings of previously studied notions of security. Furthermore, as a side-effect, we demonstrate that Narcissus resiliency captures as special cases multiple well-studied concepts including differential privacy and other security notions of one-way functions and encryption schemes. * We formulate a link between reconstruction attacks and Kolmogorov complexity. This allows us to put forward a criterion for evaluating when such attacks are convincingly successful.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15837",
        "abstract url": "https://arxiv.org/abs/2405.15837",
        "title": "An Audio-Based Iterative Controller for Soft Landing of Electromechanical Relays",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Electromechanical relays and contactors suffer from strong collisions at the end of the switching operations. This causes several undesirable phenomena, such as clicking, mechanical wear and contact bounce. Thus, there is great interest in mitigating these switching impacts while keeping the advantageous features of these devices. This paper proposes a complete control strategy for soft landing. The control structure includes three main components. The first one is a real-time flux-tracking feedback controller, which presents several advantages over voltage or current control. The second one is a feedforward controller, which computes the flux reference signal based on a proposed dynamical model and the desired position trajectory for the switching operations. Lastly, the third control component is a learning-type run-to-run adaptation law that iteratively adapts the model parameters based on an audio signal. It exploits the repetitive nature of these devices in order to circumvent modeling discrepancies due to unit-to-unit variability or small changes between operations. The effectiveness of the proposed control is demonstrated through various experiments.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "9 pages, 10 figures. Version accepted for publication in the IEEE Transactions on Industrial Electronics"
    },
    {
        "paper id": "2405.15891",
        "abstract url": "https://arxiv.org/abs/2405.15891",
        "title": "Score Distillation via Reparametrized DDIM",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "While 2D diffusion models generate realistic, high-detail images, 3D shape generation methods like Score Distillation Sampling (SDS) built on these 2D diffusion models produce cartoon-like, over-smoothed shapes. To help explain this discrepancy, we show that the image guidance used in Score Distillation can be understood as the velocity field of a 2D denoising generative process, up to the choice of a noise term. In particular, after a change of variables, SDS resembles a high-variance version of Denoising Diffusion Implicit Models (DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d. randomly at each step, while DDIM infers it from the previous noise predictions. This excessive variance can lead to over-smoothing and unrealistic outputs. We show that a better noise approximation can be recovered by inverting DDIM in each SDS update step. This modification makes SDS's generative process for 2D images almost identical to DDIM. In 3D, it removes over-smoothing, preserves higher-frequency detail, and brings the generation quality closer to that of 2D samplers. Experimentally, our method achieves better or similar 3D generation quality compared to other state-of-the-art Score Distillation methods, all without training additional neural networks or multi-view supervision, and providing useful insights into relationship between 2D and 3D asset generation with diffusion models.",
        "subjects": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "comment": "Preprint. Some browsers might incorrectly display images"
    },
    {
        "paper id": "2405.15914",
        "abstract url": "https://arxiv.org/abs/2405.15914",
        "title": "ExactDreamer: High-Fidelity Text-to-3D Content Creation via Exact Score Matching",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-to-3D content creation is a rapidly evolving research area. Given the scarcity of 3D data, current approaches often adapt pre-trained 2D diffusion models for 3D synthesis. Among these approaches, Score Distillation Sampling (SDS) has been widely adopted. However, the issue of over-smoothing poses a significant limitation on the high-fidelity generation of 3D models. To address this challenge, LucidDreamer replaces the Denoising Diffusion Probabilistic Model (DDPM) in SDS with the Denoising Diffusion Implicit Model (DDIM) to construct Interval Score Matching (ISM). However, ISM inevitably inherits inconsistencies from DDIM, causing reconstruction errors during the DDIM inversion process. This results in poor performance in the detailed generation of 3D objects and loss of content. To alleviate these problems, we propose a novel method named Exact Score Matching (ESM). Specifically, ESM leverages auxiliary variables to mathematically guarantee exact recovery in the DDIM reverse process. Furthermore, to effectively capture the dynamic changes of the original and auxiliary variables, the LoRA of a pre-trained diffusion model implements these exact paths. Extensive experiments demonstrate the effectiveness of ESM in text-to-3D generation, particularly highlighting its superiority in detailed generation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15923",
        "abstract url": "https://arxiv.org/abs/2405.15923",
        "title": "Spiketrum: An FPGA-based Implementation of a Neuromorphic Cochlea",
        "rating": "-1",
        "keywords": [
            [
                "biologically"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This paper presents a novel FPGA-based neuromorphic cochlea, leveraging the general-purpose spike-coding algorithm, Spiketrum. The focus of this study is on the development and characterization of this cochlea model, which excels in transforming audio vibrations into biologically realistic auditory spike trains. These spike trains are designed to withstand neural fluctuations and spike losses while accurately encapsulating the spatial and precise temporal characteristics of audio, along with the intensity of incoming vibrations. Noteworthy features include the ability to generate real-time spike trains with minimal information loss and the capacity to reconstruct original signals. This fine-tuning capability allows users to optimize spike rates, achieving an optimal balance between output quality and power consumption. Furthermore, the integration of a feedback system into Spiketrum enables selective amplification of specific features while attenuating others, facilitating adaptive power consumption based on application requirements. The hardware implementation supports both spike-based and non-spike-based processors, making it versatile for various computing systems. The cochlea's ability to encode diverse sensory information, extending beyond sound waveforms, positions it as a promising sensory input for current and future spike-based intelligent computing systems, offering compact and real-time spike train generation.",
        "subjects": [
            "eess.SP",
            "cs.NE",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "To be published at \"IEEE Transactions on Circuits and Systems\""
    },
    {
        "paper id": "2405.15925",
        "abstract url": "https://arxiv.org/abs/2405.15925",
        "title": "MUCM-Net: A Mamba Powered UCM-Net for Skin Lesion Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "diagnosis",
                "cancer"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Skin lesion segmentation is key for early skin cancer detection. Challenges in automatic segmentation from dermoscopic images include variations in color, texture, and artifacts of indistinct lesion boundaries. Deep learning methods like CNNs and U-Net have shown promise in addressing these issues. To further aid early diagnosis, especially on mobile devices with limited computing power, we present MUCM-Net. This efficient model combines Mamba State-Space Models with our UCM-Net architecture for improved feature learning and segmentation. MUCM-Net's Mamba-UCM Layer is optimized for mobile deployment, offering high accuracy with low computational needs. Tested on ISIC datasets, it outperforms other methods in accuracy and computational efficiency, making it a scalable tool for early detection in settings with limited resources. Our MUCM-Net source code is available for research and collaboration, supporting advances in mobile health diagnostics and the fight against skin cancer. In order to facilitate accessibility and further research in the field, the MUCM-Net source code is https://github.com/chunyuyuan/MUCM-Net",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "11 pages, 8 figures, journal paper (under review)"
    },
    {
        "paper id": "2405.15963",
        "abstract url": "https://arxiv.org/abs/2405.15963",
        "title": "Learning Minimum Linear Arrangement of Cliques and Lines",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In the well-known Minimum Linear Arrangement problem (MinLA), the goal is to arrange the nodes of an undirected graph into a permutation so that the total stretch of the edges is minimized. This paper studies an online (learning) variant of MinLA where the graph is not given at the beginning, but rather revealed piece-by-piece. The algorithm starts in a fixed initial permutation, and after a piece of the graph is revealed, the algorithm must update its current permutation to be a MinLA of the subgraph revealed so far. The objective is to minimize the total number of swaps of adjacent nodes as the algorithm updates the permutation. The main result of this paper is an online randomized algorithm that solves this online variant for the restricted cases where the revealed graph is either a collection of cliques or a collection of lines. We show that the algorithm is $8 \\ln n$-competitive, where $n$ is the number of nodes of the graph. We complement this result by constructing an asymptotically matching lower bound of $\u03a9(\\ln n)$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "ICDCS 2024"
    },
    {
        "paper id": "2405.15965",
        "abstract url": "https://arxiv.org/abs/2405.15965",
        "title": "What is a Goldilocks Face Verification Test Set?",
        "rating": "-1",
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Face Recognition models are commonly trained with web-scraped datasets containing millions of images and evaluated on test sets emphasizing pose, age and mixed attributes. With train and test sets both assembled from web-scraped images, it is critical to ensure disjoint sets of identities between train and test sets. However, existing train and test sets have not considered this. Moreover, as accuracy levels become saturated, such as LFW $>99.8\\%$, more challenging test sets are needed. We show that current train and test sets are generally not identity- or even image-disjoint, and that this results in an optimistic bias in the estimated accuracy. In addition, we show that identity-disjoint folds are important in the 10-fold cross-validation estimate of test accuracy. To better support continued advances in face recognition, we introduce two \"Goldilocks\" test sets, Hadrian and Eclipse. The former emphasizes challenging facial hairstyles and latter emphasizes challenging over- and under-exposure conditions. Images in both datasets are from a large, controlled-acquisition (not web-scraped) dataset, so they are identity- and image-disjoint with all popular training sets. Accuracy for these new test sets generally falls below that observed on LFW, CPLFW, CALFW, CFP-FP and AgeDB-30, showing that these datasets represent important dimensions for improvement of face recognition. The datasets are available at: \\url{https://github.com/HaiyuWu/SOTA-Face-Recognition-Train-and-Test}",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15978",
        "abstract url": "https://arxiv.org/abs/2405.15978",
        "title": "Exploring Age-of-Information Weighting in Federated Learning under Data Heterogeneity",
        "rating": "-1",
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "This paper investigates federated learning in a wireless communication system, where random device selection is employed with non-independent and identically distributed (non-IID) data. The analysis indicates that while training deep learning networks using federated stochastic gradient descent (FedSGD) on non-IID datasets, device selection can generate gradient errors that accumulate, leading to potential weight divergence. To mitigate training divergence, we design an age-weighted FedSGD to scale local gradients according to the previous state of devices. To further improve learning performance by increasing device participation under the maximum time consumption constraint, we formulate an energy consumption minimization problem by including resource allocation and sub-channel assignment. By transforming the resource allocation problem into convex and utilizing KKT conditions, we derived the optimal resource allocation solution. Moreover, this paper develops a matching based algorithm to generate the enhanced sub-channel assignment. Simulation results indicate that i) age-weighted FedSGD is able to outperform conventional FedSGD in terms of convergence rate and achievable accuracy, and ii) the proposed resource allocation and sub-channel assignment strategies can significantly reduce energy consumption and improve learning performance by increasing the number of selected devices.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15989",
        "abstract url": "https://arxiv.org/abs/2405.15989",
        "title": "TreeFormers -- An Exploration of Vision Transformers for Deforestation Driver Classification",
        "rating": "-1",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper addresses the critical issue of deforestation by exploring the application of vision transformers (ViTs) for classifying the drivers of deforestation using satellite imagery from Indonesian forests. Motivated by the urgency of this problem, I propose an approach that leverages ViTs and machine learning techniques. The input to my algorithm is a 332x332-pixel satellite image, and I employ a ViT architecture to predict the deforestation driver class; grassland shrubland, other, plantation, or smallholder agriculture. My methodology involves fine-tuning a pre-trained ViT on a dataset from the Stanford ML Group, and I experiment with rotational data augmentation techniques (among others) and embedding of longitudinal data to improve classification accuracy. I also tried training a ViT from scratch. Results indicate a significant improvement over baseline models, achieving a test accuracy of 72.9%. I conduct a comprehensive analysis, including error patterns and metrics, to highlight the strengths and limitations of my approach. This research contributes to the ongoing efforts to address deforestation challenges through advanced computer vision techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15996",
        "abstract url": "https://arxiv.org/abs/2405.15996",
        "title": "Selfie Taking with Facial Expression Recognition Using Omni-directional Camera",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent studies have shown that visually impaired people have desires to take selfies in the same way as sighted people do to record their photos and share them with others. Although support applications using sound and vibration have been developed to help visually impaired people take selfies using smartphone cameras, it is still difficult to capture everyone in the angle of view, and it is also difficult to confirm that they all have good expressions in the photo. To mitigate these issues, we propose a method to take selfies with multiple people using an omni-directional camera. Specifically, a user takes a few seconds of video with an omni-directional camera, followed by face detection on all frames. The proposed method then eliminates false face detections and complements undetected ones considering the consistency across all frames. After performing facial expression recognition on all the frames, the proposed method finally extracts the frame in which the participants are happiest, and generates a perspective projection image in which all the participants are in the angle of view from the omni-directional frame. In experiments, we use several scenes with different number of people taken to demonstrate the effectiveness of the proposed method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "International Workshop on Frontiers of Computer Vision (IW-FCV2024)"
    },
    {
        "paper id": "2405.16016",
        "abstract url": "https://arxiv.org/abs/2405.16016",
        "title": "ComFace: Facial Representation Learning with Synthetic Data for Comparing Faces",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "health",
                "healthcare",
                "Facial"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Daily monitoring of intra-personal facial changes associated with health and emotional conditions has great potential to be useful for medical, healthcare, and emotion recognition fields. However, the approach for capturing intra-personal facial changes is relatively unexplored due to the difficulty of collecting temporally changing face images. In this paper, we propose a facial representation learning method using synthetic images for comparing faces, called ComFace, which is designed to capture intra-personal facial changes. For effective representation learning, ComFace aims to acquire two feature representations, i.e., inter-personal facial differences and intra-personal facial changes. The key point of our method is the use of synthetic face images to overcome the limitations of collecting real intra-personal face images. Facial representations learned by ComFace are transferred to three extensive downstream tasks for comparing faces: estimating facial expression changes, weight changes, and age changes from two face images of the same individual. Our ComFace, trained using only synthetic data, achieves comparable to or better transfer performance than general pre-training and state-of-the-art representation learning methods trained using real images.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16021",
        "abstract url": "https://arxiv.org/abs/2405.16021",
        "title": "VADER: Visual Affordance Detection and Error Recovery for Multi Robot Human Collaboration",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Robots today can exploit the rich world knowledge of large language models to chain simple behavioral skills into long-horizon tasks. However, robots often get interrupted during long-horizon tasks due to primitive skill failures and dynamic environments. We propose VADER, a plan, execute, detect framework with seeking help as a new skill that enables robots to recover and complete long-horizon tasks with the help of humans or other robots. VADER leverages visual question answering (VQA) modules to detect visual affordances and recognize execution errors. It then generates prompts for a language model planner (LMP) which decides when to seek help from another robot or human to recover from errors in long-horizon task execution. We show the effectiveness of VADER with two long-horizon robotic tasks. Our pilot study showed that VADER is capable of performing complex long-horizon tasks by asking for help from another robot to clear a table. Our user study showed that VADER is capable of performing complex long-horizon tasks by asking for help from a human to clear a path. We gathered feedback from people (N=19) about the performance of the VADER performance vs. a robot that did not ask for help. https://google-vader.github.io/",
        "subjects": [
            "cs.RO"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2405.16038",
        "abstract url": "https://arxiv.org/abs/2405.16038",
        "title": "Rethinking Early-Fusion Strategies for Improved Multispectral Object Detection",
        "rating": "-1",
        "keywords": [
            [
                "thermal"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Most recent multispectral object detectors employ a two-branch structure to extract features from RGB and thermal images. While the two-branch structure achieves better performance than a single-branch structure, it overlooks inference efficiency. This conflict is increasingly aggressive, as recent works solely pursue higher performance rather than both performance and efficiency. In this paper, we address this issue by improving the performance of efficient single-branch structures. We revisit the reasons causing the performance gap between these structures. For the first time, we reveal the information interference problem in the naive early-fusion strategy adopted by previous single-branch structures. Besides, we find that the domain gap between multispectral images, and weak feature representation of the single-branch structure are also key obstacles for performance. Focusing on these three problems, we propose corresponding solutions, including a novel shape-priority early-fusion strategy, a weakly supervised learning method, and a core knowledge distillation technique. Experiments demonstrate that single-branch networks equipped with these three contributions achieve significant performance enhancements while retaining high efficiency. Our code will be available at \\url{https://github.com/XueZ-phd/Efficient-RGB-T-Early-Fusion-Detection}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17483",
        "abstract url": "https://arxiv.org/abs/2405.17483",
        "title": "Concept-based Explainable Malignancy Scoring on Pulmonary Nodules in CT Images",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "CT",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.LG",
                "eess.IV"
            ]
        ],
        "abstract": "To increase the transparency of modern computer-aided diagnosis (CAD) systems for assessing the malignancy of lung nodules, an interpretable model based on applying the generalized additive models and the concept-based learning is proposed. The model detects a set of clinically significant attributes in addition to the final malignancy regression score and learns the association between the lung nodule attributes and a final diagnosis decision as well as their contributions into the decision. The proposed concept-based learning framework provides human-readable explanations in terms of different concepts (numerical and categorical), their values, and their contribution to the final prediction. Numerical experiments with the LIDC-IDRI dataset demonstrate that the diagnosis results obtained using the proposed model, which explicitly explores internal relationships, are in line with similar patterns observed in clinical practice. Additionally, the proposed model shows the competitive classification and the nodule attribute scoring performance, highlighting its potential for effective decision-making in the lung nodule diagnosis.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.19359",
        "abstract url": "https://arxiv.org/abs/2405.19359",
        "title": "Modally Reduced Representation Learning of Multi-Lead ECG Signals through Simultaneous Alignment and Reconstruction",
        "rating": "-1",
        "keywords": [
            [
                "cardiac"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Electrocardiogram (ECG) signals, profiling the electrical activities of the heart, are used for a plethora of diagnostic applications. However, ECG systems require multiple leads or channels of signals to capture the complete view of the cardiac system, which limits their application in smartwatches and wearables. In this work, we propose a modally reduced representation learning method for ECG signals that is capable of generating channel-agnostic, unified representations for ECG signals. Through joint optimization of reconstruction and alignment, we ensure that the embeddings of the different channels contain an amalgamation of the overall information across channels while also retaining their specific information. On an independent test dataset, we generated highly correlated channel embeddings from different ECG channels, leading to a moderate approximation of the 12-lead signals from a single-channel embedding. Our generated embeddings can work as competent features for ECG signals for downstream tasks.",
        "subjects": [
            "eess.SP",
            "cs.LG"
        ],
        "comment": "Accepted as a Workshop Paper at TS4H@ICLR2024"
    },
    {
        "paper id": "2405.15247",
        "abstract url": "https://arxiv.org/abs/2405.15247",
        "title": "Learning Antenna Pointing Correction in Operations: Efficient Calibration of a Black Box",
        "rating": "-1.5",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose an efficient offline pointing calibration method for operational antenna systems which does not require any downtime. Our approach minimizes the calibration effort and exploits technical signal information which is typically used for monitoring and control purposes in ground station operations. Using a standard antenna interface and data from an operational satellite contact, we come up with a robust strategy for training data set generation. On top of this, we learn the parameters of a suitable coordinate transform by means of linear regression. In our experiments, we show the usefulness of the method in a real-world setup.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "5 pages, to be published in the conference proceedings of the European Signal Processing Conference (EUSIPCO) 2024"
    },
    {
        "paper id": "2405.15250",
        "abstract url": "https://arxiv.org/abs/2405.15250",
        "title": "Coaching Copilot: Blended Form of an LLM-Powered Chatbot and a Human Coach to Effectively Support Self-Reflection for Leadership Growth",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Chatbots' role in fostering self-reflection is now widely recognized, especially in inducing users' behavior change. While the benefits of 24/7 availability, scalability, and consistent responses have been demonstrated in contexts such as healthcare and tutoring to help one form a new habit, their utilization in coaching necessitating deeper introspective dialogue to induce leadership growth remains unexplored. This paper explores the potential of such a chatbot powered by recent Large Language Models (LLMs) in collaboration with professional coaches in the field of executive coaching. Through a design workshop with them and two weeks of user study involving ten coach-client pairs, we explored the feasibility and nuances of integrating chatbots to complement human coaches. Our findings highlight the benefits of chatbots' ubiquity and reasoning capabilities enabled by LLMs while identifying their limitations and design necessities for effective collaboration between human coaches and chatbots. By doing so, this work contributes to the foundation for augmenting one's self-reflective process with prevalent conversational agents through the human-in-the-loop approach.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": "Accepted by the International ACM Conversational User Interfaces Conference (CUI '24)"
    },
    {
        "paper id": "2405.15256",
        "abstract url": "https://arxiv.org/abs/2405.15256",
        "title": "FTMixer: Frequency and Time Domain Representations Fusion for Time Series Modeling",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Time series data can be represented in both the time and frequency domains, with the time domain emphasizing local dependencies and the frequency domain highlighting global dependencies. To harness the strengths of both domains in capturing local and global dependencies, we propose the Frequency and Time Domain Mixer (FTMixer). To exploit the global characteristics of the frequency domain, we introduce the Frequency Channel Convolution (FCC) module, designed to capture global inter-series dependencies. Inspired by the windowing concept in frequency domain transformations, we present the Windowing Frequency Convolution (WFC) module to capture local dependencies. The WFC module first applies frequency transformation within each window, followed by convolution across windows. Furthermore, to better capture these local dependencies, we employ channel-independent scheme to mix the time domain and frequency domain patches. Notably, FTMixer employs the Discrete Cosine Transformation (DCT) with real numbers instead of the complex-number-based Discrete Fourier Transformation (DFT), enabling direct utilization of modern deep learning operators in the frequency domain. Extensive experimental results across seven real-world long-term time series datasets demonstrate the superiority of FTMixer, in terms of both forecasting performance and computational efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15272",
        "abstract url": "https://arxiv.org/abs/2405.15272",
        "title": "Physiological Data: Challenges for Privacy and Ethics",
        "rating": "-1.5",
        "keywords": [
            [
                "Physiological"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Wearable devices that measure and record physiological signals are now becoming widely available to the general public with ever-increasing affordability and signal quality. The data from these devices introduce serious ethical challenges that remain largely unaddressed. Users do not always understand how these data can be leveraged to reveal private information about them and developers of these devices may not fully grasp how physiological data collected today could be used in the future for completely different purposes. We discuss the potential for wearable devices, initially designed to help users improve their well-being or enhance the experience of some digital application, to be appropriated in ways that extend far beyond their original intended purpose. We identify how the currently available technology can be misused, discuss how pairing physiological data with non-physiological data can radically expand the predictive capacity of physiological wearables, and explore the implications of these expanded capacities for a variety of stakeholders.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15292",
        "abstract url": "https://arxiv.org/abs/2405.15292",
        "title": "Towards a Probabilistic Fusion Approach for Robust Battery Prognostics",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Batteries are a key enabling technology for the decarbonization of transport and energy sectors. The safe and reliable operation of batteries is crucial for battery-powered systems. In this direction, the development of accurate and robust battery state-of-health prognostics models can unlock the potential of autonomous systems for complex, remote and reliable operations. The combination of Neural Networks, Bayesian modelling concepts and ensemble learning strategies, form a valuable prognostics framework to combine uncertainty in a robust and accurate manner. Accordingly, this paper introduces a Bayesian ensemble learning approach to predict the capacity depletion of lithium-ion batteries. The approach accurately predicts the capacity fade and quantifies the uncertainty associated with battery design and degradation processes. The proposed Bayesian ensemble methodology employs a stacking technique, integrating multiple Bayesian neural networks (BNNs) as base learners, which have been trained on data diversity. The proposed method has been validated using a battery aging dataset collected by the NASA Ames Prognostics Center of Excellence. Obtained results demonstrate the improved accuracy and robustness of the proposed probabilistic fusion approach with respect to (i) a single BNN model and (ii) a classical stacking strategy based on different BNNs.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15301",
        "abstract url": "https://arxiv.org/abs/2405.15301",
        "title": "Rankability-enhanced Revenue Uplift Modeling Framework for Online Marketing",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Uplift modeling has been widely employed in online marketing by predicting the response difference between the treatment and control groups, so as to identify the sensitive individuals toward interventions like coupons or discounts. Compared with traditional \\textit{conversion uplift modeling}, \\textit{revenue uplift modeling} exhibits higher potential due to its direct connection with the corporate income. However, previous works can hardly handle the continuous long-tail response distribution in revenue uplift modeling. Moreover, they have neglected to optimize the uplift ranking among different individuals, which is actually the core of uplift modeling. To address such issues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss to regress the responses and customize the corresponding modeling network, which can be adapted to different existing uplift models. Then, we study the ranking-related uplift modeling error from the theoretical perspective and propose two tighter error bounds as the additional loss terms to the conventional response regression loss. Finally, we directly model the uplift ranking error for the entire population with a listwise uplift ranking loss. The experiment results on offline public and industrial datasets validate the effectiveness of our method for revenue uplift modeling. Furthermore, we conduct large-scale experiments on a prominent online fintech marketing platform, Tencent FiT, which further demonstrates the superiority of our method in practical applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by KDD 2024"
    },
    {
        "paper id": "2405.15316",
        "abstract url": "https://arxiv.org/abs/2405.15316",
        "title": "Decaf: Data Distribution Decompose Attack against Federated Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In contrast to prevalent Federated Learning (FL) privacy inference techniques such as generative adversarial networks attacks, membership inference attacks, property inference attacks, and model inversion attacks, we devise an innovative privacy threat: the Data Distribution Decompose Attack on FL, termed Decaf. This attack enables an honest-but-curious FL server to meticulously profile the proportion of each class owned by the victim FL user, divulging sensitive information like local market item distribution and business competitiveness. The crux of Decaf lies in the profound observation that the magnitude of local model gradient changes closely mirrors the underlying data distribution, including the proportion of each class. Decaf addresses two crucial challenges: accurately identify the missing/null class(es) given by any victim user as a premise and then quantify the precise relationship between gradient changes and each remaining non-null class. Notably, Decaf operates stealthily, rendering it entirely passive and undetectable to victim users regarding the infringement of their data distribution privacy. Experimental validation on five benchmark datasets (MNIST, FASHION-MNIST, CIFAR-10, FER-2013, and SkinCancer) employing diverse model architectures, including customized convolutional networks, standardized VGG16, and ResNet18, demonstrates Decaf's efficacy. Results indicate its ability to accurately decompose local user data distribution, regardless of whether it is IID or non-IID distributed. Specifically, the dissimilarity measured using $L_{\\infty}$ distance between the distribution decomposed by Decaf and ground truth is consistently below 5\\% when no null classes exist. Moreover, Decaf achieves 100\\% accuracy in determining any victim user's null classes, validated through formal proof.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15317",
        "abstract url": "https://arxiv.org/abs/2405.15317",
        "title": "NuwaTS: a Foundation Model Mending Every Incomplete Time Series",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Time series imputation plays a crucial role in various real-world systems and has been extensively explored. Models for time series imputation often require specialization, necessitating distinct designs for different domains and missing patterns. In this study, we introduce NuwaTS, a framework to repurpose Pre-trained Language Model (PLM) for general time series imputation. Once trained, this model can be applied to imputation tasks on incomplete time series from any domain with any missing patterns. We begin by devising specific embeddings for each sub-series patch of the incomplete time series. These embeddings encapsulate information about the patch itself, the missing data patterns within the patch, and the patch's statistical characteristics. To enhance the model's adaptability to different missing patterns, we propose a contrastive learning approach to make representations of the same patch more similar across different missing patterns. By combining this contrastive loss with the missing data imputation task, we train PLMs to obtain a one-for-all imputation model. Furthermore, we utilize a plug-and-play layer-wise fine-tuning approach to train domain-specific models. Experimental results demonstrate that leveraging a dataset of over seventeen million time series from diverse domains, we obtain a one-for-all imputation model which outperforms existing domain-specific models across various datasets and missing patterns. Additionally, we find that NuwaTS can be generalized to other time series tasks such as forecasting. Our codes are available at https://github.com/Chengyui/NuwaTS.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "22 pages, 13 figures"
    },
    {
        "paper id": "2405.15325",
        "abstract url": "https://arxiv.org/abs/2405.15325",
        "title": "On the Identification of Temporally Causal Representation with Instantaneous Dependence",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \\textbf{ID}entification framework for instantane\\textbf{O}us \\textbf{L}atent dynamics (\\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15373",
        "abstract url": "https://arxiv.org/abs/2405.15373",
        "title": "Autonomous Quilt Spreading for Caregiving Robots",
        "rating": "-1.5",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this work, we propose a novel strategy to ensure infants, who inadvertently displace their quilts during sleep, are promptly and accurately re-covered. Our approach is formulated into two subsequent steps: interference resolution and quilt spreading. By leveraging the DWPose human skeletal detection and the Segment Anything instance segmentation models, the proposed method can accurately recognize the states of the infant and the quilt over her, which involves addressing the interferences resulted from an infant's limbs laid on part of the quilt. Building upon prior research, the EM*D deep learning model is employed to forecast quilt state transitions before and after quilt spreading actions. To improve the sensitivity of the network in distinguishing state variation of the handled quilt, we introduce an enhanced loss function that translates the voxelized quilt state into a more representative one. Both simulation and real-world experiments validate the efficacy of our method, in spreading and recover a quilt over an infant.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15403",
        "abstract url": "https://arxiv.org/abs/2405.15403",
        "title": "Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In most practical applications such as recommendation systems, display advertising, and so forth, the collected data often contains missing values and those missing values are generally missing-not-at-random, which deteriorates the prediction performance of models. Some existing estimators and regularizers attempt to achieve unbiased estimation to improve the predictive performance. However, variances and generalization bound of these methods are generally unbounded when the propensity scores tend to zero, compromising their stability and robustness. In this paper, we first theoretically reveal that limitations of regularization techniques. Besides, we further illustrate that, for more general estimators, unbiasedness will inevitably lead to unbounded variance. These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off. Instead, it involves a quantitative joint optimization of bias and variance. Then, we develop a systematic fine-grained dynamic learning framework to jointly optimize bias and variance, which adaptively selects an appropriate estimator for each user-item pair according to the predefined objective function. With this operation, the generalization bounds and variances of models are reduced and bounded with theoretical guarantees. Extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed dynamic learning framework.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15412",
        "abstract url": "https://arxiv.org/abs/2405.15412",
        "title": "ORCA: A Global Ocean Emulator for Multi-year to Decadal Predictions",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Ocean dynamics plays a crucial role in driving global weather and climate patterns. Accurate and efficient modeling of ocean dynamics is essential for improved understanding of complex ocean circulation and processes, for predicting climate variations and their associated teleconnections, and for addressing the challenges of climate change. While great efforts have been made to improve numerical Ocean General Circulation Models (OGCMs), accurate forecasting of global oceanic variations for multi-year remains to be a long-standing challenge. Here, we introduce ORCA (Oceanic Reliable foreCAst), the first data-driven model predicting global ocean circulation from multi-year to decadal time scales. ORCA accurately simulates the three-dimensional circulations and dynamics of the global ocean with high physical consistency. Hindcasts of key oceanic variables demonstrate ORCA's remarkable prediction skills in predicting ocean variations compared with state-of-the-art numerical OGCMs and abilities in capturing occurrences of extreme events at the subsurface ocean and ENSO vertical patterns. These results demonstrate the potential of data-driven ocean models for providing cheap, efficient, and accurate global ocean modeling and prediction. Moreover, ORCA stably and faithfully emulates ocean dynamics at decadal timescales, demonstrating its potential even for climate projections. The model will be available at https://github.com/OpenEarthLab/ORCA.",
        "subjects": [
            "physics.ao-ph",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15463",
        "abstract url": "https://arxiv.org/abs/2405.15463",
        "title": "PoinTramba: A Hybrid Transformer-Mamba Framework for Point Cloud Analysis",
        "rating": "-1.5",
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "BIO"
            ],
            [
                "cs.CV"
            ],
            [
                "NIPS"
            ]
        ],
        "abstract": "Point cloud analysis has seen substantial advancements due to deep learning, although previous Transformer-based methods excel at modeling long-range dependencies on this task, their computational demands are substantial. Conversely, the Mamba offers greater efficiency but shows limited potential compared with Transformer-based methods. In this study, we introduce PoinTramba, a pioneering hybrid framework that synergies the analytical power of Transformer with the remarkable computational efficiency of Mamba for enhanced point cloud analysis. Specifically, our approach first segments point clouds into groups, where the Transformer meticulously captures intricate intra-group dependencies and produces group embeddings, whose inter-group relationships will be simultaneously and adeptly captured by efficient Mamba architecture, ensuring comprehensive analysis. Unlike previous Mamba approaches, we introduce a bi-directional importance-aware ordering (BIO) strategy to tackle the challenges of random ordering effects. This innovative strategy intelligently reorders group embeddings based on their calculated importance scores, significantly enhancing Mamba's performance and optimizing the overall analytical process. Our framework achieves a superior balance between computational efficiency and analytical performance by seamlessly integrating these advanced techniques, marking a substantial leap forward in point cloud analysis. Extensive experiments on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart demonstrate the effectiveness of our approach, establishing a new state-of-the-art analysis benchmark on point cloud recognition. For the first time, this paradigm leverages the combined strengths of both Transformer and Mamba architectures, facilitating a new standard in the field. The code is available at https://github.com/xiaoyao3302/PoinTramba.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 4 figures, 6 tables, NIPS submission"
    },
    {
        "paper id": "2405.15474",
        "abstract url": "https://arxiv.org/abs/2405.15474",
        "title": "Unlearning during Learning: An Efficient Federated Machine Unlearning Method",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Unlearning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, Federated Learning (FL) has garnered significant attention as a distributed machine learning paradigm. To facilitate the implementation of the right to be forgotten, the concept of federated machine unlearning (FMU) has also emerged. However, current FMU approaches often involve additional time-consuming steps and may not offer comprehensive unlearning capabilities, which renders them less practical in real FL scenarios. In this paper, we introduce FedAU, an innovative and efficient FMU framework aimed at overcoming these limitations. Specifically, FedAU incorporates a lightweight auxiliary unlearning module into the learning process and employs a straightforward linear operation to facilitate unlearning. This approach eliminates the requirement for extra time-consuming steps, rendering it well-suited for FL. Furthermore, FedAU exhibits remarkable versatility. It not only enables multiple clients to carry out unlearning tasks concurrently but also supports unlearning at various levels of granularity, including individual data samples, specific classes, and even at the client level. We conducted extensive experiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the performance of FedAU. The results demonstrate that FedAU effectively achieves the desired unlearning effect while maintaining model accuracy.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "Accepted by IJCAI 2024"
    },
    {
        "paper id": "2405.15506",
        "abstract url": "https://arxiv.org/abs/2405.15506",
        "title": "Learning to Discretize Denoising Diffusion ODEs",
        "rating": "-1.5",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Diffusion",
                "synthesis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion Probabilistic Models (DPMs) are powerful generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. However, sampling from pre-trained DPMs involves multiple neural function evaluations (NFE) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, a crucial problem is to reduce NFE while preserving generation quality. To this end, we propose LD3, a lightweight framework for learning time discretization while sampling from the diffusion ODE encapsulated by DPMs. LD3 can be combined with various diffusion ODE solvers and consistently improves performance without retraining resource-intensive neural networks. We demonstrate analytically and empirically that LD3 enhances sampling efficiency compared to distillation-based methods, without the extensive computational overhead. We evaluate our method with extensive experiments on 5 datasets, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. For example, in about 5 minutes of training on a single GPU, our method reduces the FID score from 6.63 to 2.68 on CIFAR10 (7 NFE), and in around 20 minutes, decreases the FID from 8.51 to 5.03 on class-conditional ImageNet-256 (5 NFE). LD3 complements distillation methods, offering a more efficient approach to sampling from pre-trained diffusion models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15508",
        "abstract url": "https://arxiv.org/abs/2405.15508",
        "title": "Human-in-the-loop Reinforcement Learning for Data Quality Monitoring in Particle Physics Experiments",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Data Quality Monitoring (DQM) is a crucial task in large particle physics experiments, since detector malfunctioning can compromise the data. DQM is currently performed by human shifters, which is costly and results in limited accuracy. In this work, we provide a proof-of-concept for applying human-in-the-loop Reinforcement Learning (RL) to automate the DQM process while adapting to operating conditions that change over time. We implement a prototype based on the Proximal Policy Optimization (PPO) algorithm and validate it on a simplified synthetic dataset. We demonstrate how a multi-agent system can be trained for continuous automated monitoring during data collection, with human intervention actively requested only when relevant. We show that random, unbiased noise in human classification can be reduced, leading to an improved accuracy over the baseline. Additionally, we propose data augmentation techniques to deal with scarce data and to accelerate the learning process. Finally, we discuss further steps needed to implement the approach in the real world, including protocols for periodic control of the algorithm's outputs.",
        "subjects": [
            "hep-ex",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15564",
        "abstract url": "https://arxiv.org/abs/2405.15564",
        "title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
        "rating": "-1.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) have exhibited prominent performance in learning graph-structured data. Considering node classification task, based on the i.i.d assumption among node labels, the traditional supervised learning simply sums up cross-entropy losses of the independent training nodes and applies the average loss to optimize GNNs' weights. But different from other data formats, the nodes are naturally connected. It is found that the independent distribution modeling of node labels restricts GNNs' capability to generalize over the entire graph and defend adversarial attacks. In this work, we propose a new framework, termed joint-cluster supervised learning, to model the joint distribution of each node with its corresponding cluster. We learn the joint distribution of node and cluster labels conditioned on their representations, and train GNNs with the obtained joint loss. In this way, the data-label reference signals extracted from the local cluster explicitly strengthen the discrimination ability on the target node. The extensive experiments demonstrate that our joint-cluster supervised learning can effectively bolster GNNs' node classification accuracy. Furthermore, being benefited from the reference signals which may be free from spiteful interference, our learning paradigm significantly protects the node classification from being affected by the adversarial attack.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "20 pages, 4 figures"
    },
    {
        "paper id": "2405.15586",
        "abstract url": "https://arxiv.org/abs/2405.15586",
        "title": "DAGER: Exact Gradient Inversion for Large Language Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder- and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15598",
        "abstract url": "https://arxiv.org/abs/2405.15598",
        "title": "MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel Data Fusion Network Model Integrating CNN, LSTM, and GRU",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Accurate demand forecasting is crucial for optimizing supply chain management. Traditional methods often fail to capture complex patterns from seasonal variability and special events. Despite advancements in deep learning, interpretable forecasting models remain a challenge. To address this, we introduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture that integrates Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive performance by extracting spatial and temporal features from time series data. Our rigorous benchmarking demonstrates that MCDFN outperforms seven other deep-learning models, achieving superior metrics: MSE (23.5738%), RMSE (4.8553%), MAE (3.9991%), and MAPE (20.1575%). Additionally, MCDFN's predictions were statistically indistinguishable from actual values, confirmed by a paired t-test with a 5% p-value and a 10-fold cross-validated statistical paired t-test. We apply explainable AI techniques like ShapTime and Permutation Feature Importance to enhance interpretability. This research advances demand forecasting methodologies and offers practical guidelines for integrating MCDFN into supply chain systems, highlighting future research directions for scalability and user-friendly deployment.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15616",
        "abstract url": "https://arxiv.org/abs/2405.15616",
        "title": "Neuromorphic dreaming: A pathway to efficient learning in artificial agents",
        "rating": "-1.5",
        "keywords": [
            [
                "Biological"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Achieving energy efficiency in learning is a key challenge for artificial intelligence (AI) computing platforms. Biological systems demonstrate remarkable abilities to learn complex skills quickly and efficiently. Inspired by this, we present a hardware implementation of model-based reinforcement learning (MBRL) using spiking neural networks (SNNs) on mixed-signal analog/digital neuromorphic hardware. This approach leverages the energy efficiency of mixed-signal neuromorphic chips while achieving high sample efficiency through an alternation of online learning, referred to as the \"awake\" phase, and offline learning, known as the \"dreaming\" phase. The model proposed includes two symbiotic networks: an agent network that learns by combining real and simulated experiences, and a learned world model network that generates the simulated experiences. We validate the model by training the hardware implementation to play the Atari game Pong. We start from a baseline consisting of an agent network learning without a world model and dreaming, which successfully learns to play the game. By incorporating dreaming, the number of required real game experiences are reduced significantly compared to the baseline. The networks are implemented using a mixed-signal neuromorphic processor, with the readout layers trained using a computer in-the-loop, while the other layers remain fixed. These results pave the way toward energy-efficient neuromorphic learning systems capable of rapid learning in real world applications and use-cases.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15642",
        "abstract url": "https://arxiv.org/abs/2405.15642",
        "title": "Effective Confidence Region Prediction Using Probability Forecasters",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Confidence region prediction is a practically useful extension to the commonly studied pattern recognition problem. Instead of predicting a single label, the constraint is relaxed to allow prediction of a subset of labels given a desired confidence level 1-delta. Ideally, effective region predictions should be (1) well calibrated - predictive regions at confidence level 1-delta should err with relative frequency at most delta and (2) be as narrow (or certain) as possible. We present a simple technique to generate confidence region predictions from conditional probability estimates (probability forecasts). We use this 'conversion' technique to generate confidence region predictions from probability forecasts output by standard machine learning algorithms when tested on 15 multi-class datasets. Our results show that approximately 44% of experiments demonstrate well-calibrated confidence region predictions, with the K-Nearest Neighbour algorithm tending to perform consistently well across all data. Our results illustrate the practical benefits of effective confidence region prediction with respect to medical diagnostics, where guarantees of capturing the true disease label can be given.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "10 pages, originally posted in 2005"
    },
    {
        "paper id": "2405.15673",
        "abstract url": "https://arxiv.org/abs/2405.15673",
        "title": "Consistency of Neural Causal Partial Identification",
        "rating": "-1.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models. In this work, we prove consistency of partial identification via NCMs in a general setting with both continuous and categorical variables. Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase. In particular, we provide a counterexample showing that without Lipschitz regularization the NCM may not be asymptotically consistent. Our results are enabled by new results on the approximability of structural causal models via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "37 pages, 8 figures"
    },
    {
        "paper id": "2405.15732",
        "abstract url": "https://arxiv.org/abs/2405.15732",
        "title": "Neural Persistence Dynamics",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of learning the dynamics in the topology of time-evolving point clouds, the prevalent spatiotemporal model for systems exhibiting collective behavior, such as swarms of insects and birds or particles in physics. In such systems, patterns emerge from (local) interactions among self-propelled entities. While several well-understood governing equations for motion and interaction exist, they are difficult to fit to data due to the often large number of entities and missing correspondences between the observation times, which may also not be equidistant. To evade such confounding factors, we investigate collective behavior from a \\textit{topological perspective}, but instead of summarizing entire observation sequences (as in prior work), we propose learning a latent dynamical model from topological features \\textit{per time point}. The latter is then used to formulate a downstream regression task to predict the parametrization of some a priori specified governing equation. We implement this idea based on a latent ODE learned from vectorized (static) persistence diagrams and show that this modeling choice is justified by a combination of recent stability results for persistent homology. Various (ablation) experiments not only demonstrate the relevance of each individual model component, but provide compelling empirical evidence that our proposed model -- \\textit{neural persistence dynamics} -- substantially outperforms the state-of-the-art across a diverse set of parameter regression tasks.",
        "subjects": [
            "cs.LG",
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15840",
        "abstract url": "https://arxiv.org/abs/2405.15840",
        "title": "Learning the Language of Protein Structure",
        "rating": "-1.5",
        "keywords": [
            [
                "biology"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Representation learning and \\emph{de novo} generation of proteins are pivotal computational biology tasks. Whilst natural language processing (NLP) techniques have proven highly effective for protein sequence modelling, structure modelling presents a complex challenge, primarily due to its continuous and three-dimensional nature. Motivated by this discrepancy, we introduce an approach using a vector-quantized autoencoder that effectively tokenizes protein structures into discrete representations. This method transforms the continuous, complex space of protein structures into a manageable, discrete format with a codebook ranging from 4096 to 64000 tokens, achieving high-fidelity reconstructions with backbone root mean square deviations (RMSD) of approximately 1-5 \u00c5. To demonstrate the efficacy of our learned representations, we show that a simple GPT model trained on our codebooks can generate novel, diverse, and designable protein structures. Our approach not only provides representations of protein structure, but also mitigates the challenges of disparate modal representations and sets a foundation for seamless, multi-modal integration, enhancing the capabilities of computational methods in protein design.",
        "subjects": [
            "q-bio.QM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15882",
        "abstract url": "https://arxiv.org/abs/2405.15882",
        "title": "Risk Factor Identification In Osteoporosis Using Unsupervised Machine Learning Techniques",
        "rating": "-1.5",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this study, the reliability of identified risk factors associated with osteoporosis is investigated using a new clustering-based method on electronic medical records. This study proposes utilizing a new CLustering Iterations Framework (CLIF) that includes an iterative clustering framework that can adapt any of the following three components: clustering, feature selection, and principal feature identification. The study proposes using Wasserstein distance to identify principal features, borrowing concepts from the optimal transport theory. The study also suggests using a combination of ANOVA and ablation tests to select influential features from a data set. Some risk factors presented in existing works are endorsed by our identified significant clusters, while the reliability of some other risk factors is weakened.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "24 pages, 10 figures, 4 algorithms"
    },
    {
        "paper id": "2405.15934",
        "abstract url": "https://arxiv.org/abs/2405.15934",
        "title": "Clustering Survival Data using a Mixture of Non-parametric Experts",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "Survival"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Survival analysis aims to predict the timing of future events across various fields, from medical outcomes to customer churn. However, the integration of clustering into survival analysis, particularly for precision medicine, remains underexplored. This study introduces SurvMixClust, a novel algorithm for survival analysis that integrates clustering with survival function prediction within a unified framework. SurvMixClust learns latent representations for clustering while also predicting individual survival functions using a mixture of non-parametric experts. Our evaluations on five public datasets show that SurvMixClust creates balanced clusters with distinct survival curves, outperforms clustering baselines, and competes with non-clustering survival models in predictive accuracy, as measured by the time-dependent c-index and log-rank metrics.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15975",
        "abstract url": "https://arxiv.org/abs/2405.15975",
        "title": "Inference of Utilities and Time Preference in Sequential Decision-Making",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a novel stochastic control framework to enhance the capabilities of automated investment managers, or robo-advisors, by accurately inferring clients' investment preferences from past activities. Our approach leverages a continuous-time model that incorporates utility functions and a generic discounting scheme of a time-varying rate, tailored to each client's risk tolerance, valuation of daily consumption, and significant life goals. We address the resulting time inconsistency issue through state augmentation and the establishment of the dynamic programming principle and the verification theorem. Additionally, we provide sufficient conditions for the identifiability of client investment preferences. To complement our theoretical developments, we propose a learning algorithm based on maximum likelihood estimation within a discrete-time Markov Decision Process framework, augmented with entropy regularization. We prove that the log-likelihood function is locally concave, facilitating the fast convergence of our proposed algorithm. Practical effectiveness and efficiency are showcased through two numerical examples, including Merton's problem and an investment problem with unhedgeable risks. Our proposed framework not only advances financial technology by improving personalized investment advice but also contributes broadly to other fields such as healthcare, economics, and artificial intelligence, where understanding individual preferences is crucial.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "q-fin.CP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15985",
        "abstract url": "https://arxiv.org/abs/2405.15985",
        "title": "The Impact and Opportunities of Generative AI in Fact-Checking",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Generative AI appears poised to transform white collar professions, with more than 90% of Fortune 500 companies using OpenAI's flagship GPT models, which have been characterized as \"general purpose technologies\" capable of effecting epochal changes in the economy. But how will such technologies impact organizations whose job is to verify and report factual information, and to ensure the health of the information ecosystem? To investigate this question, we conducted 30 interviews with N=38 participants working at 29 fact-checking organizations across six continents, asking about how they use generative AI and the opportunities and challenges they see in the technology. We found that uses of generative AI envisioned by fact-checkers differ based on organizational infrastructure, with applications for quality assurance in Editing, for trend analysis in Investigation, and for information literacy in Advocacy. We used the TOE framework to describe participant concerns ranging from the Technological (lack of transparency), to the Organizational (resource constraints), to the Environmental (uncertain and evolving policy). Building on the insights of our participants, we describe value tensions between fact-checking and generative AI, and propose a novel Verification dimension to the design space of generative models for information verification work. Finally, we outline an agenda for fairness, accountability, and transparency research to support the responsible use of generative AI in fact-checking. Throughout, we highlight the importance of human infrastructure and labor in producing verified information in collaboration with AI. We expect that this work will inform not only the scientific literature on fact-checking, but also contribute to understanding of organizational adaptation to a powerful but unreliable new technology.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "To be published at the ACM Conference on Fairness, Accountability, and Transparency (FAccT) 2024"
    },
    {
        "paper id": "2405.16041",
        "abstract url": "https://arxiv.org/abs/2405.16041",
        "title": "Explainable Molecular Property Prediction: Aligning Chemical Concepts with Predictions via Language Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Chemical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Providing explainable molecule property predictions is critical for many scientific domains, such as drug discovery and material science. Though transformer-based language models have shown great potential in accurate molecular property prediction, they neither provide chemically meaningful explanations nor faithfully reveal the molecular structure-property relationships. In this work, we develop a new framework for explainable molecular property prediction based on language models, dubbed as Lamole, which can provide chemical concepts-aligned explanations. We first leverage a designated molecular representation -- the Group SELFIES -- as it can provide chemically meaningful semantics. Because attention mechanisms in Transformers can inherently capture relationships within the input, we further incorporate the attention weights and gradients together to generate explanations for capturing the functional group interactions. We then carefully craft a marginal loss to explicitly optimize the explanations to be able to align with the chemists' annotations. We bridge the manifold hypothesis with the elaborated marginal loss to prove that the loss can align the explanations with the tangent space of the data manifold, leading to concept-aligned explanations. Experimental results over six mutagenicity datasets and one hepatotoxicity dataset demonstrate Lamole can achieve comparable classification accuracy and boost the explanation accuracy by up to 14.8%, being the state-of-the-art in explainable molecular property prediction.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16053",
        "abstract url": "https://arxiv.org/abs/2405.16053",
        "title": "Pausing Policy Learning in Non-stationary Reinforcement Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Real-time inference is a challenge of real-world reinforcement learning due to temporal differences in time-varying environments: the system collects data from the past, updates the decision model in the present, and deploys it in the future. We tackle a common belief that continually updating the decision is optimal to minimize the temporal gap. We propose forecasting an online reinforcement learning framework and show that strategically pausing decision updates yields better overall performance by effectively managing aleatoric uncertainty. Theoretically, we compute an optimal ratio between policy update and hold duration, and show that a non-zero policy hold duration provides a sharper upper bound on the dynamic regret. Our experimental evaluations on three different environments also reveal that a non-zero policy hold duration yields higher rewards compared to continuous decision updates.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "conference"
    },
    {
        "paper id": "2405.17478",
        "abstract url": "https://arxiv.org/abs/2405.17478",
        "title": "ROSE: Register Assisted General Time Series Forecasting with Decomposed Frequency Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the increasing collection of time series data from various domains, there arises a strong demand for general time series forecasting models pre-trained on a large number of time-series datasets to support a variety of downstream prediction tasks. Enabling general time series forecasting faces two challenges: how to obtain unified representations from multi-domian time series data, and how to capture domain-specific features from time series data across various domains for adaptive transfer in downstream tasks. To address these challenges, we propose a Register Assisted General Time Series Forecasting Model with Decomposed Frequency Learning (ROSE), a novel pre-trained model for time series forecasting. ROSE employs Decomposed Frequency Learning for the pre-training task, which decomposes coupled semantic and periodic information in time series with frequency-based masking and reconstruction to obtain unified representations across domains. We also equip ROSE with a Time Series Register, which learns to generate a register codebook to capture domain-specific representations during pre-training and enhances domain-adaptive transfer by selecting related register tokens on downstream tasks. After pre-training on large-scale time series data, ROSE achieves state-of-the-art forecasting performance on 8 real-world benchmarks. Remarkably, even in few-shot scenarios, it demonstrates competitive or superior performance compared to existing methods trained with full data.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17486",
        "abstract url": "https://arxiv.org/abs/2405.17486",
        "title": "eQMARL: Entangled Quantum Multi-Agent Reinforcement Learning for Distributed Cooperation over Quantum Channels",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Collaboration is a key challenge in distributed multi-agent reinforcement learning (MARL) environments. Learning frameworks for these decentralized systems must weigh the benefits of explicit player coordination against the communication overhead and computational cost of sharing local observations and environmental data. Quantum computing has sparked a potential synergy between quantum entanglement and cooperation in multi-agent environments, which could enable more efficient distributed collaboration with minimal information sharing. This relationship is largely unexplored, however, as current state-of-the-art quantum MARL (QMARL) implementations rely on classical information sharing rather than entanglement over a quantum channel as a coordination medium. In contrast, in this paper, a novel framework dubbed entangled QMARL (eQMARL) is proposed. The proposed eQMARL is a distributed actor-critic framework that facilitates cooperation over a quantum channel and eliminates local observation sharing via a quantum entangled split critic. Introducing a quantum critic uniquely spread across the agents allows coupling of local observation encoders through entangled input qubits over a quantum channel, which requires no explicit sharing of local observations and reduces classical communication overhead. Further, agent policies are tuned through joint observation-value function estimation via joint quantum measurements, thereby reducing the centralized computational burden. Experimental results show that eQMARL with $\u03a8^{+}$ entanglement converges to a cooperative strategy up to $17.8\\%$ faster and with a higher overall score compared to split classical and fully centralized classical and quantum baselines. The results also show that eQMARL achieves this performance with a constant factor of $25$-times fewer centralized parameters compared to the split classical baseline.",
        "subjects": [
            "quant-ph",
            "cs.ET",
            "cs.LG",
            "cs.MA"
        ],
        "comment": "19 pages, 8 figures"
    },
    {
        "paper id": "2405.19363",
        "abstract url": "https://arxiv.org/abs/2405.19363",
        "title": "Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification",
        "rating": "-1.5",
        "keywords": [
            [
                "biomarkers",
                "Medical",
                "healthcare",
                "diagnosing",
                "EEG",
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Medical time series data, such as Electroencephalography (EEG) and Electrocardiography (ECG), play a crucial role in healthcare, such as diagnosing brain and heart diseases. Existing methods for medical time series classification primarily rely on handcrafted biomarkers extraction and CNN-based models, with limited exploration of transformers tailored for medical time series. In this paper, we introduce Medformer, a multi-granularity patching transformer tailored specifically for medical time series classification. Our method incorporates three novel mechanisms to leverage the unique characteristics of medical time series: cross-channel patching to leverage inter-channel correlations, multi-granularity embedding for capturing features at different scales, and two-stage (intra- and inter-granularity) multi-granularity self-attention for learning features and correlations within and among granularities. We conduct extensive experiments on five public datasets under both subject-dependent and challenging subject-independent setups. Results demonstrate Medformer's superiority over 10 baselines, achieving top averaged ranking across five datasets on all six evaluation metrics. These findings underscore the significant impact of our method on healthcare applications, such as diagnosing Myocardial Infarction, Alzheimer's, and Parkinson's disease. We release the source code at \\url{https://github.com/DL4mHealth/Medformer}.",
        "subjects": [
            "eess.SP",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "20pages (14 pages main paper + 6 pages supplementary materials)"
    },
    {
        "paper id": "2405.20348",
        "abstract url": "https://arxiv.org/abs/2405.20348",
        "title": "Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper demonstrates that pre-trained language models (PLMs) are strong foundation models for on-device meteorological variables modeling. We present LM-Weather, a generic approach to taming PLMs, that have learned massive sequential knowledge from the universe of natural language databases, to acquire an immediate capability to obtain highly customized models for heterogeneous meteorological data on devices while keeping high efficiency. Concretely, we introduce a lightweight personalized adapter into PLMs and endows it with weather pattern awareness. During communication between clients and the server, low-rank-based transmission is performed to effectively fuse the global knowledge among devices while maintaining high communication efficiency and ensuring privacy. Experiments on real-wold dataset show that LM-Weather outperforms the state-of-the-art results by a large margin across various tasks (e.g., forecasting and imputation at different scales). We provide extensive and in-depth analyses experiments, which verify that LM-Weather can (1) indeed leverage sequential knowledge from natural language to accurately handle meteorological sequence, (2) allows each devices obtain highly customized models under significant heterogeneity, and (3) generalize under data-limited and out-of-distribution (OOD) scenarios.",
        "subjects": [
            "physics.ao-ph",
            "cs.LG"
        ],
        "comment": "42 pages, under review"
    },
    {
        "paper id": "2405.15234",
        "abstract url": "https://arxiv.org/abs/2405.15234",
        "title": "Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "Unlearning"
            ],
            [
                "attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs' image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at: https://github.com/OPTML-Group/AdvUnlearn",
        "subjects": [
            "cs.CV",
            "cs.CR"
        ],
        "comment": "Codes are available at https://github.com/OPTML-Group/AdvUnlearn"
    },
    {
        "paper id": "2405.15258",
        "abstract url": "https://arxiv.org/abs/2405.15258",
        "title": "Leakage-Resilient and Carbon-Neutral Aggregation Featuring the Federated AI-enabled Critical Infrastructure",
        "rating": "-2",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "AI-enabled critical infrastructures (ACIs) integrate artificial intelligence (AI) technologies into various essential systems and services that are vital to the functioning of society, offering significant implications for efficiency, security and resilience. While adopting decentralized AI approaches (such as federated learning technology) in ACIs is plausible, private and sensitive data are still susceptible to data reconstruction attacks through gradient optimization. In this work, we propose Compressed Differentially Private Aggregation (CDPA), a leakage-resilient, communication-efficient, and carbon-neutral approach for ACI networks. Specifically, CDPA has introduced a novel random bit-flipping mechanism as its primary innovation. This mechanism first converts gradients into a specific binary representation and then selectively flips masked bits with a certain probability. The proposed bit-flipping introduces a larger variance to the noise while providing differentially private protection and commendable efforts in energy savings while applying vector quantization techniques within the context of federated learning. The experimental evaluation indicates that CDPA can reduce communication cost by half while preserving model utility. Moreover, we demonstrate that CDPA can effectively defend against state-of-the-art data reconstruction attacks in both computer vision and natural language processing tasks. We highlight existing benchmarks that generate 2.6x to over 100x more carbon emissions than CDPA. We hope that the CDPA developed in this paper can inform the federated AI-enabled critical infrastructure of a more balanced trade-off between utility and privacy, resilience protection, as well as a better carbon offset with less communication overhead.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15335",
        "abstract url": "https://arxiv.org/abs/2405.15335",
        "title": "Challenges and Opportunities in 3D Content Generation",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion",
                "synthesizing"
            ]
        ],
        "abstract": "This paper explores the burgeoning field of 3D content generation within the landscape of Artificial Intelligence Generated Content (AIGC) and large-scale models. It investigates innovative methods like Text-to-3D and Image-to-3D, which translate text or images into 3D objects, reshaping our understanding of virtual and real-world simulations. Despite significant advancements in text and image generation, automatic 3D content generation remains nascent. This paper emphasizes the urgency for further research in this area. By leveraging pre-trained diffusion models, which have demonstrated prowess in high-fidelity image generation, this paper aims to summary 3D content creation, addressing challenges such as data scarcity and computational resource limitations. Additionally, this paper discusses the challenges and proposes solutions for using pre-trained diffusion models in 3D content generation. By synthesizing relevant research and outlining future directions, this study contributes to advancing the field of 3D content generation amidst the proliferation of large-scale AIGC models.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "Brief report"
    },
    {
        "paper id": "2405.15345",
        "abstract url": "https://arxiv.org/abs/2405.15345",
        "title": "Hybrid-Field Channel Estimation for XL-MIMO Systems with Stochastic Gradient Pursuit Algorithm",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Extremely large-scale multiple-input multiple-output (XL-MIMO) is crucial for satisfying the high data rate requirements of the sixth-generation (6G) wireless networks. In this context, ensuring accurate acquisition of channel state information (CSI) with low complexity becomes imperative. Moreover, deploying an extremely large antenna array at the base station (BS) might result in some scatterers being located in near-field, while others are situated in far-field, leading to a hybrid-field communication scenario. To address these challenges, this paper introduces two stochastic gradient pursuit (SGP)-based schemes for the hybrid-field channel estimation in two scenarios. For the first scenario in which the prior knowledge of the specific proportion of the number of near-field and far-field channel paths is known, the scheme can effectively leverage the angular-domain sparsity of the far-field channels and the polar-domain sparsity of the near-field channels such that the channel estimation in these two fields can be performed separately. For the second scenario which the proportion is not available, we propose an off-grid SGP-based channel estimation scheme, which iterates through the values of the proportion parameter based on a criterion before performing the hybrid-field channel estimation. We demonstrate numerically that both of the proposed channel estimation schemes achieve superior performance in terms of both estimation accuracy and achievable rates while enjoying lower computational complexity compared with existing schemes. Additionally, we reveal that as the number of antennas at the UE increases, the normalized mean square error (NMSE) performances of the proposed schemes remain basically unchanged, while the NMSE performances of existing ones improve. Remarkably, even in this scenario, the proposed schemes continue to outperform the existing ones.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "30 pages, 6 figures, been ACCEPTED for publication as a REGULAR paper in the IEEE Transactions on Signal Processing"
    },
    {
        "paper id": "2405.15368",
        "abstract url": "https://arxiv.org/abs/2405.15368",
        "title": "Complexity of Robust Orbit Problems for Torus Actions and the abc-conjecture",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "When a group acts on a set, it naturally partitions it into orbits, giving rise to orbit problems. These are natural algorithmic problems, as symmetries are central in numerous questions and structures in physics, mathematics, computer science, optimization, and more. Accordingly, it is of high interest to understand their computational complexity. Recently, B\u00fcrgisser et al. gave the first polynomial-time algorithms for orbit problems of torus actions, that is, actions of commutative continuous groups on Euclidean space. In this work, motivated by theoretical and practical applications, we study the computational complexity of robust generalizations of these orbit problems, which amount to approximating the distance of orbits in $\\mathbb{C}^n$ up to a factor $\u03b3>1$. In particular, this allows deciding whether two inputs are approximately in the same orbit or far from being so. On the one hand, we prove the NP-hardness of this problem for $\u03b3= n^{\u03a9(1/\\log\\log n)}$ by reducing the closest vector problem for lattices to it. On the other hand, we describe algorithms for solving this problem for an approximation factor $\u03b3= \\exp(\\mathrm{poly}(n))$. Our algorithms combine tools from invariant theory and algorithmic lattice theory, and they also provide group elements witnessing the proximity of the given orbits (in contrast to the algebraic algorithms of prior work). We prove that they run in polynomial time if and only if a version of the famous number-theoretic $abc$-conjecture holds -- establishing a new and surprising connection between computational complexity and number theory.",
        "subjects": [
            "cs.CC",
            "cs.DS",
            "math.AG",
            "math.RT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15385",
        "abstract url": "https://arxiv.org/abs/2405.15385",
        "title": "CPT-Interp: Continuous sPatial and Temporal Motion Modeling for 4D Medical Image Interpolation",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Motion information from 4D medical imaging offers critical insights into dynamic changes in patient anatomy for clinical assessments and radiotherapy planning and, thereby, enhances the capabilities of 3D image analysis. However, inherent physical and technical constraints of imaging hardware often necessitate a compromise between temporal resolution and image quality. Frame interpolation emerges as a pivotal solution to this challenge. Previous methods often suffer from discretion when they estimate the intermediate motion and execute the forward warping. In this study, we draw inspiration from fluid mechanics to propose a novel approach for continuously modeling patient anatomic motion using implicit neural representation. It ensures both spatial and temporal continuity, effectively bridging Eulerian and Lagrangian specifications together to naturally facilitate continuous frame interpolation. Our experiments across multiple datasets underscore the method's superior accuracy and speed. Furthermore, as a case-specific optimization (training-free) approach, it circumvents the need for extensive datasets and addresses model generalization issues.",
        "subjects": [
            "cs.CV",
            "physics.med-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15398",
        "abstract url": "https://arxiv.org/abs/2405.15398",
        "title": "PriCE: Privacy-Preserving and Cost-Effective Scheduling for Parallelizing the Large Medical Image Processing Workflow over Hybrid Clouds",
        "rating": "-2",
        "keywords": [
            [
                "graph"
            ],
            [
                "Medical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Running deep neural networks for large medical images is a resource-hungry and time-consuming task with centralized computing. Outsourcing such medical image processing tasks to hybrid clouds has benefits, such as a significant reduction of execution time and monetary cost. However, due to privacy concerns, it is still challenging to process sensitive medical images over clouds, which would hinder their deployment in many real-world applications. To overcome this, we first formulate the overall optimization objectives of the privacy-preserving distributed system model, i.e., minimizing the amount of information about the private data learned by the adversaries throughout the process, reducing the maximum execution time and cost under the user budget constraint. We propose a novel privacy-preserving and cost-effective method called PriCE to solve this multi-objective optimization problem. We performed extensive simulation experiments for artifact detection tasks on medical images using an ensemble of five deep convolutional neural network inferences as the workflow task. Experimental results show that PriCE successfully splits a wide range of input gigapixel medical images with graph-coloring-based strategies, yielding desired output utility and lowering the privacy risk, makespan, and monetary cost under user's budget.",
        "subjects": [
            "cs.CE",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.ET"
        ],
        "comment": "Acccepted at Europar 2024"
    },
    {
        "paper id": "2405.15405",
        "abstract url": "https://arxiv.org/abs/2405.15405",
        "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
        "rating": "-2",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at IEEE International Geoscience and Remote Sensing Symposium (IGARSS) 2024. Our code is available at https://git.tu-berlin.de/rsim/FL-Transformer"
    },
    {
        "paper id": "2405.15439",
        "abstract url": "https://arxiv.org/abs/2405.15439",
        "title": "Text-guided 3D Human Motion Generation with Keyframe-based Parallel Skip Transformer",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "robot"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Text-driven human motion generation is an emerging task in animation and humanoid robot design. Existing algorithms directly generate the full sequence which is computationally expensive and prone to errors as it does not pay special attention to key poses, a process that has been the cornerstone of animation for decades. We propose KeyMotion, that generates plausible human motion sequences corresponding to input text by first generating keyframes followed by in-filling. We use a Variational Autoencoder (VAE) with Kullback-Leibler regularization to project the keyframes into a latent space to reduce dimensionality and further accelerate the subsequent diffusion process. For the reverse diffusion, we propose a novel Parallel Skip Transformer that performs cross-modal attention between the keyframe latents and text condition. To complete the motion sequence, we propose a text-guided Transformer designed to perform motion-in-filling, ensuring the preservation of both fidelity and adherence to the physical constraints of human motion. Experiments show that our method achieves state-of-theart results on the HumanML3D dataset outperforming others on all R-precision metrics and MultiModal Distance. KeyMotion also achieves competitive performance on the KIT dataset, achieving the best results on Top3 R-precision, FID, and Diversity metrics.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15450",
        "abstract url": "https://arxiv.org/abs/2405.15450",
        "title": "Faster and Better Quantum Software Testing through Specification Reduction and Projective Measurements",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum computing promises polynomial and exponential speedups in many domains, such as unstructured search and prime number factoring. However, quantum programs yield probabilistic outputs from exponentially growing distributions and are vulnerable to quantum-specific faults. Existing quantum software testing (QST) approaches treat quantum superpositions as classical distributions. This leads to two major limitations when applied to quantum programs: (1) an exponentially growing sample space distribution and (2) failing to detect quantum-specific faults such as phase flips. To overcome these limitations, we introduce a QST approach, which applies a reduction algorithm to a quantum program specification. The reduced specification alleviates the limitations (1) by enabling faster sampling through quantum parallelism and (2) by performing projective measurements in the mixed Hadamard basis. Our evaluation of 143 quantum programs across four categories demonstrates significant improvements in test runtimes and fault detection with our reduction approach. Average test runtimes improved from 169.9s to 11.8s, with notable enhancements in programs with large circuit depths (383.1s to 33.4s) and large program specifications (464.8s to 7.7s). Furthermore, our approach increases mutation scores from 54.5% to 74.7%, effectively detecting phase flip faults that non-reduced specifications miss. These results underline our approach's importance to improve QST efficiency and effectiveness.",
        "subjects": [
            "cs.SE",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15460",
        "abstract url": "https://arxiv.org/abs/2405.15460",
        "title": "TD3 Based Collision Free Motion Planning for Robot Navigation",
        "rating": "-2",
        "keywords": [
            [
                "LiDAR"
            ],
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "This paper addresses the challenge of collision-free motion planning in automated navigation within complex environments. Utilizing advancements in Deep Reinforcement Learning (DRL) and sensor technologies like LiDAR, we propose the TD3-DWA algorithm, an innovative fusion of the traditional Dynamic Window Approach (DWA) with the Twin Delayed Deep Deterministic Policy Gradient (TD3). This hybrid algorithm enhances the efficiency of robotic path planning by optimizing the sampling interval parameters of DWA to effectively navigate around both static and dynamic obstacles. The performance of the TD3-DWA algorithm is validated through various simulation experiments, demonstrating its potential to significantly improve the reliability and safety of autonomous navigation systems.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15607",
        "abstract url": "https://arxiv.org/abs/2405.15607",
        "title": "Channel Estimation and Reconstruction in Fluid Antenna System: Oversampling is Essential",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Fluid antenna system (FAS) has recently surfaced as a promising technology for the upcoming sixth generation (6G) wireless networks. Unlike traditional antenna system (TAS) with fixed antenna location, FAS introduces a flexible component where the radiating element can switch its position within a predefined space. This capability allows FAS to achieve additional diversity and multiplexing gains. Nevertheless, to fully reap the benefits of FAS, obtaining channel state information (CSI) over the predefined space is crucial. In this paper, we explore the interaction between a transmitter equipped with a traditional antenna and a receiver with a fluid antenna over an electromagnetic-compliant channel model. We address the challenges of channel estimation and reconstruction using Nyquist sampling and maximum likelihood estimation (MLE) methods. Our analysis reveals a fundamental tradeoff between the accuracy of the reconstructed channel and the number of estimated channels, indicating that half-wavelength sampling is insufficient for perfect reconstruction and that oversampling is essential to enhance accuracy. Despite its advantages, oversampling can introduce practical challenges. Consequently, we propose a suboptimal sampling distance that facilitates efficient channel reconstruction. In addition, we employ the MLE method to bound the channel estimation error by $\u03b5$, with a specific confidence interval (CI). Our findings enable us to determine the minimum number of estimated channels and the total number of pilot symbols required for efficient channel reconstruction in a given space. Lastly, we investigate the rate performance of FAS and TAS and demonstrate that FAS with imperfect CSI can outperform TAS with perfect CSI.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "12 pages, 14 figures - including subfigures. Submitted for potential publication"
    },
    {
        "paper id": "2405.15608",
        "abstract url": "https://arxiv.org/abs/2405.15608",
        "title": "Design and fabrication of autonomous electronic lablets for chemical control",
        "rating": "-2",
        "keywords": [
            [
                "chemical"
            ]
        ],
        "abstract": "Lablets are autonomous microscopic particles with programmable CMOS electronics that canvcontrol electrokinetic phenomena and electrochemical reactions in solution via actuator and sensor microelectrodes. The lablets are designed to be rechargeable using an integrated supercapacitor, and to allow docking to one another or to a smart surface for interchange of energy, electronic information and chemicals. In this paper, we describe the design and fabrication of singulated lablets (CMOS2) at the scale of 100 by 200 \u03bcm, with the supercap adjacent to the functional lablet and occupying half the space. In other works, we have characterized the supercap and described the electronic design and proven functionality using arrays of these lablets. Here we present fabrication details for integrating functional coatings and the supercap and demonstrate electronic functionality of the lablets following singulation.",
        "subjects": [
            "cs.RO",
            "cond-mat.mtrl-sci"
        ],
        "comment": "This article was originally submitted (2016) for review as supporting information for the final review of the EU MICREAgents Project # 318671 (2012-2016). Here it is presented in slightly revised form"
    },
    {
        "paper id": "2405.15645",
        "abstract url": "https://arxiv.org/abs/2405.15645",
        "title": "An Online Probabilistic Distributed Tracing System",
        "rating": "-2",
        "keywords": [
            [
                "diagnosing"
            ]
        ],
        "abstract": "Distributed tracing has become a fundamental tool for diagnosing performance issues in the cloud by recording causally ordered, end-to-end workflows of request executions. However, tracing in production workloads can introduce significant overheads due to the extensive instrumentation needed for identifying performance variations. This paper addresses the trade-off between the cost of tracing and the utility of the \"spans\" within that trace through Astraea, an online probabilistic distributed tracing system. Astraea is based on our technique that combines online Bayesian learning and multi-armed bandit frameworks. This formulation enables Astraea to effectively steer tracing towards the useful instrumentation needed for accurate performance diagnosis. Astraea localizes performance variations using only 10-28% of available instrumentation, markedly reducing tracing overhead, storage, compute costs, and trace analysis time.",
        "subjects": [
            "cs.PF",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15711",
        "abstract url": "https://arxiv.org/abs/2405.15711",
        "title": "An Adaptive Framework for Manipulator Skill Reproduction in Dynamic Environments",
        "rating": "-2",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "Robot skill learning and execution in uncertain and dynamic environments is a challenging task. This paper proposes an adaptive framework that combines Learning from Demonstration (LfD), environment state prediction, and high-level decision making. Proactive adaptation prevents the need for reactive adaptation, which lags behind changes in the environment rather than anticipating them. We propose a novel LfD representation, Elastic-Laplacian Trajectory Editing (ELTE), which continuously adapts the trajectory shape to predictions of future states. Then, a high-level reactive system using an Unscented Kalman Filter (UKF) and Hidden Markov Model (HMM) prevents unsafe execution in the current state of the dynamic environment based on a discrete set of decisions. We first validate our LfD representation in simulation, then experimentally assess the entire framework using a legged mobile manipulator in 36 real-world scenarios. We show the effectiveness of the proposed framework under different dynamic changes in the environment. Our results show that the proposed framework produces robust and stable adaptive behaviors.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Paper accepted at Ubiquitous Robots 2024 held at New York University on June 24 to June 27, 2024"
    },
    {
        "paper id": "2405.15719",
        "abstract url": "https://arxiv.org/abs/2405.15719",
        "title": "Hierarchical Uncertainty Exploration via Feedforward Posterior Trees",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "image restoration"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "When solving ill-posed inverse problems, one often desires to explore the space of potential solutions rather than be presented with a single plausible reconstruction. Valuable insights into these feasible solutions and their associated probabilities are embedded in the posterior distribution. However, when confronted with data of high dimensionality (such as images), visualizing this distribution becomes a formidable challenge, necessitating the application of effective summarization techniques before user examination. In this work, we introduce a new approach for visualizing posteriors across multiple levels of granularity using tree-valued predictions. Our method predicts a tree-valued hierarchical summarization of the posterior distribution for any input measurement, in a single forward pass of a neural network. We showcase the efficacy of our approach across diverse datasets and image restoration challenges, highlighting its prowess in uncertainty quantification and visualization. Our findings reveal that our method performs comparably to a baseline that hierarchically clusters samples from a diffusion-based posterior sampler, yet achieves this with orders of magnitude greater speed.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "stat.ML"
        ],
        "comment": "32 pages, 21 figures"
    },
    {
        "paper id": "2405.15736",
        "abstract url": "https://arxiv.org/abs/2405.15736",
        "title": "Single-Round Proofs of Quantumness from Knowledge Assumptions",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "A proof of quantumness is an efficiently verifiable interactive test that an efficient quantum computer can pass, but all efficient classical computers cannot (under some cryptographic assumption). Such protocols play a crucial role in the certification of quantum devices. Existing single-round protocols (like asking the quantum computer to factor a large number) require large quantum circuits, whereas multi-round ones use smaller circuits but require experimentally challenging mid-circuit measurements. As such, current proofs of quantumness are out of reach for near-term devices. In this work, we construct efficient single-round proofs of quantumness based on existing knowledge assumptions. While knowledge assumptions have not been previously considered in this context, we show that they provide a natural basis for separating classical and quantum computation. Specifically, we show that multi-round protocols based on Decisional Diffie-Hellman (DDH) or Learning With Errors (LWE) can be \"compiled\" into single-round protocols using a knowledge-of-exponent assumption or knowledge-of-lattice-point assumption, respectively. We also prove an adaptive hardcore-bit statement for a family of claw-free functions based on DDH, which might be of independent interest. Previous approaches to constructing single-round protocols relied on the random oracle model and thus incurred the overhead associated with instantiating the oracle with a cryptographic hash function. In contrast, our protocols have the same resource requirements as their multi-round counterparts without necessitating mid-circuit measurements, making them, arguably, the most efficient single-round proofs of quantumness to date. Our work also helps in understanding the interplay between black-box/white-box reductions and cryptographic assumptions in the design of proofs of quantumness.",
        "subjects": [
            "quant-ph",
            "cs.CR"
        ],
        "comment": "51 pages"
    },
    {
        "paper id": "2405.15749",
        "abstract url": "https://arxiv.org/abs/2405.15749",
        "title": "Collaborative Access Control for IoT -- A Blockchain Approach",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "The Internet of Things (IoT) necessitates robust access control mechanisms to secure a vast array of interconnected devices. Most of the existing IoT systems in practice use centralized solutions. We identify the problems in such solutions and adopt the blockchain based decentralized access control approach. Though there are works in the literature that use blockchain for access control, there are some gaps in these works. We develop a blockchain embedded access control (BEAC) framework to bridge the gaps. First, blockchain based solutions for access control require an enabling P2P network while existing P2P overlays do not support some required features. We develop a novel P2P infrastructure to seamlessly support our BEAC framework. Second, most of the works consider blockchain based access control for a single access control model, and we develop a generic blockchain mechanism and show that it can support the embedding of various access control models. Finally, existing works adopt existing blockchain mechanisms which may incur a high communication overhead. We develop a shortcut approach to improve the number of message rounds in the access protocol. Our experiments demonstrate the efficacy of our system, showing that the shortcut mechanism can reduces access time by approximately 43%.",
        "subjects": [
            "cs.DC",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15830",
        "abstract url": "https://arxiv.org/abs/2405.15830",
        "title": "Diff-DTI: Fast Diffusion Tensor Imaging Using A Feature-Enhanced Joint Diffusion Model",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "diagnosis",
                "disease",
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Magnetic resonance diffusion tensor imaging (DTI) is a critical tool for neural disease diagnosis. However, long scan time greatly hinders the widespread clinical use of DTI. To accelerate image acquisition, a feature-enhanced joint diffusion model (Diff-DTI) is proposed to obtain accurate DTI parameter maps from a limited number of diffusion-weighted images (DWIs). Diff-DTI introduces a joint diffusion model that directly learns the joint probability distribution of DWIs with DTI parametric maps for conditional generation. Additionally, a feature enhancement fusion mechanism (FEFM) is designed and incorporated into the generative process of Diff-DTI to preserve fine structures in the generated DTI maps. A comprehensive evaluation of the performance of Diff-DTI was conducted on the Human Connectome Project dataset. The results demonstrate that Diff-DTI outperforms existing state-of-the-art fast DTI imaging methods in terms of visual quality and quantitative metrics. Furthermore, Diff-DTI has shown the ability to produce high-fidelity DTI maps with only three DWIs, thus overcoming the requirement of a minimum of six DWIs for DTI.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "11 pages, 7 figures"
    },
    {
        "paper id": "2405.15866",
        "abstract url": "https://arxiv.org/abs/2405.15866",
        "title": "Governing the Commons: Code Ownership and Code-Clones in Large-Scale Software Development",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Context: In software development organizations employing weak or collective ownership, different teams are allowed and expected to autonomously perform changes in various components. This creates diversity both in the knowledge of, and in the responsibility for, individual components. Objective: Our objective is to understand how and why different teams introduce technical debt in the form of code clones as they change different components. Method: We collected data about change size and clone introductions made by ten teams in eight components which was part of a large industrial software system. We then designed a Multi-Level Generalized Linear Model (MLGLM), to illustrate the teams' differing behavior. Finally, the results were reported to the studied organization, responses were recorded and thematically coded. Results: The results show that teams do behave differently in different components, and the feedback from the teams indicates that this method of illustrating team behavior can be useful as a complement to traditional summary statistics of ownership. Conclusions: We find that our model-based approach produces useful visualizations of team introductions of code clones as they change different components. Practitioners stated that the visualizations gave them insights that were useful, and by comparing with an average team, inter-team comparisons can be avoided. Thus, this has the potential to be a useful feedback tool for teams in software development organizations that employ weak or collective ownership.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "29 pages, 13 figures"
    },
    {
        "paper id": "2405.15872",
        "abstract url": "https://arxiv.org/abs/2405.15872",
        "title": "Extended Reality (XR) Codec Adaptation in 5G using Multi-Agent Reinforcement Learning with Attention Action Selection",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Extended Reality (XR) services will revolutionize applications over 5th and 6th generation wireless networks by providing seamless virtual and augmented reality experiences. These applications impose significant challenges on network infrastructure, which can be addressed by machine learning algorithms due to their adaptability. This paper presents a Multi- Agent Reinforcement Learning (MARL) solution for optimizing codec parameters of XR traffic, comparing it to the Adjust Packet Size (APS) algorithm. Our cooperative multi-agent system uses an Optimistic Mixture of Q-Values (oQMIX) approach for handling Cloud Gaming (CG), Augmented Reality (AR), and Virtual Reality (VR) traffic. Enhancements include an attention mechanism and slate-Markov Decision Process (MDP) for improved action selection. Simulations show our solution outperforms APS with average gains of 30.1%, 15.6%, 16.5% 50.3% in XR index, jitter, delay, and Packet Loss Ratio (PLR), respectively. APS tends to increase throughput but also packet losses, whereas oQMIX reduces PLR, delay, and jitter while maintaining goodput.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "6 pages, 5 figures, 2 tables"
    },
    {
        "paper id": "2405.15901",
        "abstract url": "https://arxiv.org/abs/2405.15901",
        "title": "A Survey on Application Layer Protocols for IoT Networks",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Nowadays, all sectors utilize devices that are part of the Internet of Things (IoT) for the purpose of connecting and exchanging information with other devices and systems over the Internet. This increases the diversity of devices and their working environments, which, in turn, creates new challenges, such as real-time interaction, security, interoperability, performance, and robustness of IoT systems. To address these, many applications protocols were adopted and developed for devices with constrained resources. This paper surveys communication protocols divided according to their goals along with their merits, demerits, and suitability towards IoT applications. We summarize the challenges of communication protocols as well as some relevant solutions.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2405.15927",
        "abstract url": "https://arxiv.org/abs/2405.15927",
        "title": "Application based Evaluation of an Efficient Spike-Encoder, \"Spiketrum\"",
        "rating": "-2",
        "keywords": [
            [
                "biologically-plausible"
            ]
        ],
        "abstract": "Spike-based encoders represent information as sequences of spikes or pulses, which are transmitted between neurons. A prevailing consensus suggests that spike-based approaches demonstrate exceptional capabilities in capturing the temporal dynamics of neural activity and have the potential to provide energy-efficient solutions for low-power applications. The Spiketrum encoder efficiently compresses input data using spike trains or code sets (for non-spiking applications) and is adaptable to both hardware and software implementations, with lossless signal reconstruction capability. The paper proposes and assesses Spiketrum's hardware, evaluating its output under varying spike rates and its classification performance with popular spiking and non-spiking classifiers, and also assessing the quality of information compression and hardware resource utilization. The paper extensively benchmarks both Spiketrum hardware and its software counterpart against state-of-the-art, biologically-plausible encoders. The evaluations encompass benchmarking criteria, including classification accuracy, training speed, and sparsity when using encoder outputs in pattern recognition and classification with both spiking and non-spiking classifiers. Additionally, they consider encoded output entropy and hardware resource utilization and power consumption of the hardware version of the encoders. Results demonstrate Spiketrum's superiority in most benchmarking criteria, making it a promising choice for various applications. It efficiently utilizes hardware resources with low power consumption, achieving high classification accuracy. This work also emphasizes the potential of encoders in spike-based processing to improve the efficiency and performance of neural computing systems.",
        "subjects": [
            "eess.SP",
            "cs.NE",
            "eess.SY"
        ],
        "comment": "To be published at \"IEEE/ACM Transactions on Audio, Speech, and Language Processing\""
    },
    {
        "paper id": "2405.16056",
        "abstract url": "https://arxiv.org/abs/2405.16056",
        "title": "FedSheafHN: Personalized Federated Learning on Graph-structured Data",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Federated Learning"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Personalized subgraph Federated Learning (FL) is a task that customizes Graph Neural Networks (GNNs) to individual client needs, accommodating diverse data distributions. However, applying hypernetworks in FL, while aiming to facilitate model personalization, often encounters challenges due to inadequate representation of client-specific characteristics. To overcome these limitations, we propose a model called FedSheafHN, using enhanced collaboration graph embedding and efficient personalized model parameter generation. Specifically, our model embeds each client's local subgraph into a server-constructed collaboration graph. We utilize sheaf diffusion in the collaboration graph to learn client representations. Our model improves the integration and interpretation of complex client characteristics. Furthermore, our model ensures the generation of personalized models through advanced hypernetworks optimized for parallel operations across clients. Empirical evaluations demonstrate that FedSheafHN outperforms existing methods in most scenarios, in terms of client model performance on various graph-structured datasets. It also has fast model convergence and effective new clients generalization.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This paper was submitted to ICML 2024 in Feb 2024. You can find a record here:https://github.com/CarrieWFF/ICML-2024-submission-recording/blob/main/Screenshot%20of%20FedSheafHN%20submission%20to%20ICML%202024.png"
    },
    {
        "paper id": "2405.15280",
        "abstract url": "https://arxiv.org/abs/2405.15280",
        "title": "DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The graph-based recommendation has achieved great success in recent years. However, most existing graph-based recommendations focus on capturing user preference based on positive edges/feedback, while ignoring negative edges/feedback (e.g., dislike, low rating) that widely exist in real-world recommender systems. How to utilize negative feedback in graph-based recommendations still remains underexplored. In this study, we first conducted a comprehensive experimental analysis and found that (1) existing graph neural networks are not well-suited for modeling negative feedback, which acts as a high-frequency signal in a user-item graph. (2) The graph-based recommendation suffers from the representation degeneration problem. Based on the two observations, we propose a novel model that models positive and negative feedback from a frequency filter perspective called Dual-frequency Graph Neural Network for Sign-aware Recommendation (DFGNN). Specifically, in DFGNN, the designed dual-frequency graph filter (DGF) captures both low-frequency and high-frequency signals that contain positive and negative feedback. Furthermore, the proposed signed graph regularization is applied to maintain the user/item embedding uniform in the embedding space to alleviate the representation degeneration problem. Additionally, we conduct extensive experiments on real-world datasets and demonstrate the effectiveness of the proposed model. Codes of our model will be released upon acceptance.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted by KDD 2024 Research Track"
    },
    {
        "paper id": "2405.15542",
        "abstract url": "https://arxiv.org/abs/2405.15542",
        "title": "SATSense: Multi-Satellite Collaborative Framework for Spectrum Sensing",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "Satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Low Earth Orbit satellite Internet has recently been deployed, providing worldwide service with non-terrestrial networks. With the large-scale deployment of both non-terrestrial and terrestrial networks, limited spectrum resources will not be allocated enough. Consequently, dynamic spectrum sharing is crucial for their coexistence in the same spectrum, where accurate spectrum sensing is essential. However, spectrum sensing in space is more challenging than in terrestrial networks due to variable channel conditions, making single-satellite sensing unstable. Therefore, we first attempt to design a collaborative sensing scheme utilizing diverse data from multiple satellites. However, it is non-trivial to achieve this collaboration due to heterogeneous channel quality, considerable raw sampling data, and packet loss. To address the above challenges, we first establish connections between the satellites by modeling their sensing data as a graph and devising a graph neural network-based algorithm to achieve effective spectrum sensing. Meanwhile, we establish a joint sub-Nyquist sampling and autoencoder data compression framework to reduce the amount of transmitted sensing data. Finally, we propose a contrastive learning-based mechanism compensates for missing packets. Extensive experiments demonstrate that our proposed strategy can achieve efficient spectrum sensing performance and outperform the conventional deep learning algorithm in spectrum sensing accuracy.",
        "subjects": [
            "cs.NI",
            "cs.DC",
            "cs.LG",
            "eess.SP"
        ],
        "comment": "13 pages, 16 figures"
    },
    {
        "paper id": "2405.15603",
        "abstract url": "https://arxiv.org/abs/2405.15603",
        "title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Physics-informed neural networks (PINNs) are infamous for being hard to train. Recently, second-order methods based on natural gradient and Gauss-Newton methods have shown promising performance, improving the accuracy achieved by first-order methods by several orders of magnitude. While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix. We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks. Our approach goes beyond the established KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization. To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights. This allows us to apply KFAC thanks to a recently-developed general formulation for networks with weight sharing. Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS.",
        "subjects": [
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15643",
        "abstract url": "https://arxiv.org/abs/2405.15643",
        "title": "Reducing the cost of posterior sampling in linear inverse problems via task-dependent score learning",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Score-based diffusion models (SDMs) offer a flexible approach to sample from the posterior distribution in a variety of Bayesian inverse problems. In the literature, the prior score is utilized to sample from the posterior by different methods that require multiple evaluations of the forward mapping in order to generate a single posterior sample. These methods are often designed with the objective of enabling the direct use of the unconditional prior score and, therefore, task-independent training. In this paper, we focus on linear inverse problems, when evaluation of the forward mapping is computationally expensive and frequent posterior sampling is required for new measurement data, such as in medical imaging. We demonstrate that the evaluation of the forward mapping can be entirely bypassed during posterior sample generation. Instead, without introducing any error, the computational effort can be shifted to an offline task of training the score of a specific diffusion-like random process. In particular, the training is task-dependent requiring information about the forward mapping but not about the measurement data. It is shown that the conditional score corresponding to the posterior can be obtained from the auxiliary score by suitable affine transformations. We prove that this observation generalizes to the framework of infinite-dimensional diffusion models introduced recently and provide numerical analysis of the method. Moreover, we validate our findings with numerical experiments.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.AP",
            "math.NA",
            "math.PR"
        ],
        "comment": "23 pages, 2 figues"
    },
    {
        "paper id": "2405.15903",
        "abstract url": "https://arxiv.org/abs/2405.15903",
        "title": "UnitNorm: Rethinking Normalization for Transformers in Time Series",
        "rating": "-2.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Normalization techniques are crucial for enhancing Transformer models' performance and stability in time series analysis tasks, yet traditional methods like batch and layer normalization often lead to issues such as token shift, attention shift, and sparse attention. We propose UnitNorm, a novel approach that scales input vectors by their norms and modulates attention patterns, effectively circumventing these challenges. Grounded in existing normalization frameworks, UnitNorm's effectiveness is demonstrated across diverse time series analysis tasks, including forecasting, classification, and anomaly detection, via a rigorous evaluation on 6 state-of-the-art models and 10 datasets. Notably, UnitNorm shows superior performance, especially in scenarios requiring robust attention mechanisms and contextual comprehension, evidenced by significant improvements by up to a 1.46 decrease in MSE for forecasting, and a 4.89% increase in accuracy for classification. This work not only calls for a reevaluation of normalization strategies in time series Transformers but also sets a new direction for enhancing model performance and stability. The source code is available at https://anonymous.4open.science/r/UnitNorm-5B84.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15988",
        "abstract url": "https://arxiv.org/abs/2405.15988",
        "title": "Transductive Confidence Machine and its application to Medical Data Sets",
        "rating": "-2.5",
        "keywords": [
            [
                "SVM"
            ],
            [
                "Medical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The Transductive Confidence Machine Nearest Neighbours (TCMNN) algorithm and a supporting, simple user interface was developed. Different settings of the TCMNN algorithms' parameters were tested on medical data sets, in addition to the use of different Minkowski metrics and polynomial kernels. The effect of increasing the number of nearest neighbours and marking results with significance was also investigated. SVM implementation of the Transductive Confidence Machine was compared with Nearest Neighbours implementation. The application of neural networks was investigated as a useful comparison to the transductive algorithms.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "160 pages"
    },
    {
        "paper id": "2405.16003",
        "abstract url": "https://arxiv.org/abs/2405.16003",
        "title": "Disentangling Heterogeneous Knowledge Concept Embedding for Cognitive Diagnosis on Untested Knowledge",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "Diagnosis"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Cognitive diagnosis is a fundamental and critical task in learning assessment, which aims to infer students' proficiency on knowledge concepts from their response logs. Current works assume each knowledge concept will certainly be tested and covered by multiple exercises. However, whether online or offline courses, it's hardly feasible to completely cover all knowledge concepts in several exercises. Restricted tests lead to undiscovered knowledge deficits, especially untested knowledge concepts(UKCs). In this paper, we propose a novel \\underline{Dis}entangling Heterogeneous \\underline{K}nowledge \\underline{C}ognitive \\underline{D}iagnosis framework on untested knowledge(DisKCD). Specifically, we leverage course grades, exercise questions, and resources to learn the potential representations of students, exercises, and knowledge concepts. In particular, knowledge concepts are disentangled into tested and untested based on the limiting actual exercises. We construct a heterogeneous relation graph network via students, exercises, tested knowledge concepts(TKCs), and UKCs. Then, through a hierarchical heterogeneous message-passing mechanism, the fine-grained relations are incorporated into the embeddings of the entities. Finally, the embeddings will be applied to multiple existing cognitive diagnosis models to infer students' proficiency on UKCs. Experimental results on real-world datasets show that the proposed model can effectively improve the performance of the task of diagnosing students' proficiency on UKCs. Our anonymous code is available at https://anonymous.4open.science/r/DisKCD.",
        "subjects": [
            "cs.AI",
            "cs.CY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17488",
        "abstract url": "https://arxiv.org/abs/2405.17488",
        "title": "Pattern-Based Time-Series Risk Scoring for Anomaly Detection and Alert Filtering -- A Predictive Maintenance Case Study",
        "rating": "-2.5",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Fault detection is a key challenge in the management of complex systems. In the context of SparkCognition's efforts towards predictive maintenance in large scale industrial systems, this problem is often framed in terms of anomaly detection - identifying patterns of behavior in the data which deviate from normal. Patterns of normal behavior aren't captured simply in the coarse statistics of measured signals. Rather, the multivariate sequential pattern itself can be indicative of normal vs. abnormal behavior. For this reason, normal behavior modeling that relies on snapshots of the data without taking into account temporal relationships as they evolve would be lacking. However, common strategies for dealing with temporal dependence, such as Recurrent Neural Networks or attention mechanisms are oftentimes computationally expensive and difficult to train. In this paper, we propose a fast and efficient approach to anomaly detection and alert filtering based on sequential pattern similarities. In our empirical analysis section, we show how this approach can be leveraged for a variety of purposes involving anomaly detection on a large scale real-world industrial system. Subsequently, we test our approach on a publicly-available dataset in order to establish its general applicability and robustness compared to a state-of-the-art baseline. We also demonstrate an efficient way of optimizing the framework based on an alert recall objective function.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.18444",
        "abstract url": "https://arxiv.org/abs/2405.18444",
        "title": "Discovering deposition process regimes: leveraging unsupervised learning for process insights, surrogate modeling, and sensitivity analysis",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work introduces a comprehensive approach utilizing data-driven methods to elucidate the deposition process regimes in Chemical Vapor Deposition (CVD) reactors and the interplay of physical mechanism that dominate in each one of them. Through this work, we address three key objectives. Firstly, our methodology relies on process outcomes, derived by a detailed CFD model, to identify clusters of \"outcomes\" corresponding to distinct process regimes, wherein the relative influence of input variables undergoes notable shifts. This phenomenon is experimentally validated through Arrhenius plot analysis, affirming the efficacy of our approach. Secondly, we demonstrate the development of an efficient surrogate model, based on Polynomial Chaos Expansion (PCE), that maintains accuracy, facilitating streamlined computational analyses. Finally, as a result of PCE, sensitivity analysis is made possible by means of Sobol' indices, that quantify the impact of process inputs across identified regimes. The insights gained from our analysis contribute to the formulation of hypotheses regarding phenomena occurring beyond the transition regime. Notably, the significance of temperature even in the diffusion-limited regime, as evidenced by the Arrhenius plot, suggests activation of gas phase reactions at elevated temperatures. Importantly, our proposed methods yield insights that align with experimental observations and theoretical principles, aiding decision-making in process design and optimization. By circumventing the need for costly and time-consuming experiments, our approach offers a pragmatic pathway towards enhanced process efficiency. Moreover, this study underscores the potential of data-driven computational methods for innovating reactor design paradigms.",
        "subjects": [
            "physics.chem-ph",
            "cs.LG",
            "eess.SY",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15239",
        "abstract url": "https://arxiv.org/abs/2405.15239",
        "title": "Automating the Diagnosis of Human Vision Disorders by Cross-modal 3D Generation",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "vehicle"
            ],
            [
                "healthcare",
                "Diagnosis",
                "fMRI",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding the hidden mechanisms behind human's visual perception is a fundamental quest in neuroscience, underpins a wide variety of critical applications, e.g. clinical diagnosis. To that end, investigating into the neural responses of human mind activities, such as functional Magnetic Resonance Imaging (fMRI), has been a significant research vehicle. However, analyzing fMRI signals is challenging, costly, daunting, and demanding for professional training. Despite remarkable progress in artificial intelligence (AI) based fMRI analysis, existing solutions are limited and far away from being clinically meaningful. In this context, we leap forward to demonstrate how AI can go beyond the current state of the art by decoding fMRI into visually plausible 3D visuals, enabling automatic clinical analysis of fMRI data, even without healthcare professionals. Innovationally, we reformulate the task of analyzing fMRI data as a conditional 3D scene reconstruction problem. We design a novel cross-modal 3D scene representation learning method, Brain3D, that takes as input the fMRI data of a subject who was presented with a 2D object image, and yields as output the corresponding 3D object visuals. Importantly, we show that in simulated scenarios our AI agent captures the distinct functionalities of each region of human vision system as well as their intricate interplay relationships, aligning remarkably with the established discoveries of neuroscience. Non-expert diagnosis indicate that Brain3D can successfully identify the disordered brain regions, such as V1, V2, V3, V4, and the medial temporal lobe (MTL) within the human visual system. We also present results in cross-modal 3D visual construction setting, showcasing the perception quality of our 3D scene generation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "25 pages, 16 figures, project page: https://brain-3d.github.io/"
    },
    {
        "paper id": "2405.15271",
        "abstract url": "https://arxiv.org/abs/2405.15271",
        "title": "Seamless Integration and Implementation of Distributed Contact and Contactless Vital Sign Monitoring",
        "rating": "-3",
        "keywords": [
            [
                "radar"
            ],
            [
                "medical",
                "health"
            ]
        ],
        "abstract": "Real-time vital sign monitoring is gaining immense significance not only in the medical field but also in personal health management. Facing the needs of different application scenarios of the smart and healthy city in the future, the low-cost, large-scale, scalable, and distributed vital sign monitoring system is of great significance. In this work, a seamlessly integrated contact and contactless vital sign monitoring system, which can simultaneously implement respiration and heartbeat monitoring, is proposed. In contact vital sign monitoring, the chest wall movement due to respiration and heartbeat is translated into changes in the optical output intensity of a fiber Bragg grating (FBG). The FBG is also an important part of radar signal generation for contactless vital sign monitoring, in which the chest wall movement is translated into phase changes of the radar de-chirped signal. By analyzing the intensity of the FBG output and phase of the radar de-chirped signal, real-time respiration and heartbeat monitoring are realized. In addition, due to the distributed structure of the system and its good integration with the wavelength-division multiplexing optical network, it can be massively scaled by employing more wavelengths. A proof-of-concept experiment is carried out. Contact and contactless respiration and heartbeat monitoring of three people are simultaneously realized. During a monitoring time of 60 s, the maximum absolute measurement errors of respiration and heartbeat rates are 1.6 respirations per minute and 2.3 beats per minute, respectively. The measurement error does not have an obvious change even when the monitoring time is decreased to 5 s.",
        "subjects": [
            "eess.SY",
            "physics.ins-det",
            "physics.optics"
        ],
        "comment": "14 pages,9 figures"
    },
    {
        "paper id": "2405.15304",
        "abstract url": "https://arxiv.org/abs/2405.15304",
        "title": "Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient",
        "rating": "-3",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "Unlearning"
            ],
            [
                "surgery"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Current text-to-image diffusion models have achieved groundbreaking results in image generation tasks. However, the unavoidable inclusion of sensitive information during pre-training introduces significant risks such as copyright infringement and privacy violations in the generated images. Machine Unlearning (MU) provides a effective way to the sensitive concepts captured by the model, has been shown to be a promising approach to addressing these issues. Nonetheless, existing MU methods for concept erasure encounter two primary bottlenecks: 1) generalization issues, where concept erasure is effective only for the data within the unlearn set, and prompts outside the unlearn set often still result in the generation of sensitive concepts; and 2) utility drop, where erasing target concepts significantly degrades the model's performance. To this end, this paper first proposes a concept domain correction framework for unlearning concepts in diffusion models. By aligning the output domains of sensitive concepts and anchor concepts through adversarial training, we enhance the generalizability of the unlearning results. Secondly, we devise a concept-preserving scheme based on gradient surgery. This approach alleviates the parts of the unlearning gradient that contradict the relearning gradient, ensuring that the process of unlearning minimally disrupts the model's performance. Finally, extensive experiments validate the effectiveness of our model, demonstrating our method's capability to address the challenges of concept unlearning in diffusion models while preserving model utility.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15426",
        "abstract url": "https://arxiv.org/abs/2405.15426",
        "title": "AuthNet: Neural Network with Integrated Authentication Logic",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "watermarking"
            ]
        ],
        "abstract": "Model stealing, i.e., unauthorized access and exfiltration of deep learning models, has become one of the major threats. Proprietary models may be protected by access controls and encryption. However, in reality, these measures can be compromised due to system breaches, query-based model extraction or a disgruntled insider. Security hardening of neural networks is also suffering from limits, for example, model watermarking is passive, cannot prevent the occurrence of piracy and not robust against transformations. To this end, we propose a native authentication mechanism, called AuthNet, which integrates authentication logic as part of the model without any additional structures. Our key insight is to reuse redundant neurons with low activation and embed authentication bits in an intermediate layer, called a gate layer. Then, AuthNet fine-tunes the layers after the gate layer to embed authentication logic so that only inputs with special secret key can trigger the correct logic of AuthNet. It exhibits two intuitive advantages. It provides the last line of defense, i.e., even being exfiltrated, the model is not usable as the adversary cannot generate valid inputs without the key. Moreover, the authentication logic is difficult to inspect and identify given millions or billions of neurons in the model. We theoretically demonstrate the high sensitivity of AuthNet to the secret key and its high confusion for unauthorized samples. AuthNet is compatible with any convolutional neural network, where our extensive evaluations show that AuthNet successfully achieves the goal in rejecting unauthenticated users (whose average accuracy drops to 22.03%) with a trivial accuracy decrease (1.18% on average) for legitimate users, and is robust against model transformation and adaptive attacks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15468",
        "abstract url": "https://arxiv.org/abs/2405.15468",
        "title": "Semantic Aware Diffusion Inverse Tone Mapping",
        "rating": "-3",
        "keywords": [
            [
                "Diffusion",
                "inpainting"
            ],
            [
                "graph"
            ],
            [
                "HDR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The range of real-world scene luminance is larger than the capture capability of many digital camera sensors which leads to details being lost in captured images, most typically in bright regions. Inverse tone mapping attempts to boost these captured Standard Dynamic Range (SDR) images back to High Dynamic Range (HDR) by creating a mapping that linearizes the well exposed values from the SDR image, and provides a luminance boost to the clipped content. However, in most cases, the details in the clipped regions cannot be recovered or estimated. In this paper, we present a novel inverse tone mapping approach for mapping SDR images to HDR that generates lost details in clipped regions through a semantic-aware diffusion based inpainting approach. Our method proposes two major contributions - first, we propose to use a semantic graph to guide SDR diffusion based inpainting in masked regions in a saturated image. Second, drawing inspiration from traditional HDR imaging and bracketing methods, we propose a principled formulation to lift the SDR inpainted regions to HDR that is compatible with generative inpainting methods. Results show that our method demonstrates superior performance across different datasets on objective metrics, and subjective experiments show that the proposed method matches (and in most cases outperforms) state-of-art inverse tone mapping operators in terms of objective metrics and outperforms them for visual fidelity.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15562",
        "abstract url": "https://arxiv.org/abs/2405.15562",
        "title": "Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration",
        "rating": "-3",
        "keywords": [
            [
                "RGB-D"
            ],
            [
                "LiDAR"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "11 pages,4 figures"
    },
    {
        "paper id": "2405.15661",
        "abstract url": "https://arxiv.org/abs/2405.15661",
        "title": "Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF) Tables",
        "rating": "-3",
        "keywords": [
            [
                "skin lesions"
            ],
            [
                "watermark"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The rise of deep learning in image classification has brought unprecedented accuracy but also highlighted a key issue: the use of 'shortcuts' by models. Such shortcuts are easy-to-learn patterns from the training data that fail to generalise to new data. Examples include the use of a copyright watermark to recognise horses, snowy background to recognise huskies, or ink markings to detect malignant skin lesions. The explainable AI (XAI) community has suggested using instance-level explanations to detect shortcuts without external data, but this requires the examination of many explanations to confirm the presence of such shortcuts, making it a labour-intensive process. To address these challenges, we introduce Counterfactual Frequency (CoF) tables, a novel approach that aggregates instance-based explanations into global insights, and exposes shortcuts. The aggregation implies the need for some semantic concepts to be used in the explanations, which we solve by labelling the segments of an image. We demonstrate the utility of CoF tables across several datasets, revealing the shortcuts learned from them.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "10 pages, 18 figures"
    },
    {
        "paper id": "2405.15758",
        "abstract url": "https://arxiv.org/abs/2405.15758",
        "title": "InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation",
        "rating": "-3",
        "keywords": [
            [
                "Avatar"
            ],
            [
                "diffusion"
            ],
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is https://wangyuchi369.github.io/InstructAvatar/.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Project page: https://wangyuchi369.github.io/InstructAvatar/"
    },
    {
        "paper id": "2405.15843",
        "abstract url": "https://arxiv.org/abs/2405.15843",
        "title": "SpotNet: An Image Centric, Lidar Anchored Approach To Long Range Perception",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Lidar"
            ],
            [
                "BEV"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we propose SpotNet: a fast, single stage, image-centric but LiDAR anchored approach for long range 3D object detection. We demonstrate that our approach to LiDAR/image sensor fusion, combined with the joint learning of 2D and 3D detection tasks, can lead to accurate 3D object detection with very sparse LiDAR support. Unlike more recent bird's-eye-view (BEV) sensor-fusion methods which scale with range $r$ as $O(r^2)$, SpotNet scales as $O(1)$ with range. We argue that such an architecture is ideally suited to leverage each sensor's strength, i.e. semantic understanding from images and accurate range finding from LiDAR data. Finally we show that anchoring detections on LiDAR points removes the need to regress distances, and so the architecture is able to transfer from 2MP to 8MP resolution images without re-training.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16034",
        "abstract url": "https://arxiv.org/abs/2405.16034",
        "title": "DiffuBox: Refining 3D Object Detection with Point Diffusion",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Ensuring robust 3D object detection and localization is crucial for many applications in robotics and autonomous driving. Recent models, however, face difficulties in maintaining high performance when applied to domains with differing sensor setups or geographic locations, often resulting in poor localization accuracy due to domain shift. To overcome this challenge, we introduce a novel diffusion-based box refinement approach. This method employs a domain-agnostic diffusion model, conditioned on the LiDAR points surrounding a coarse bounding box, to simultaneously refine the box's location, size, and orientation. We evaluate this approach under various domain adaptation settings, and our results reveal significant improvements across different datasets, object classes and detectors.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15328",
        "abstract url": "https://arxiv.org/abs/2405.15328",
        "title": "Multi-Modal Recommendation Unlearning",
        "rating": "-3.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "graph"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Unlearning methods for recommender systems (RS) have emerged to address privacy issues and concerns about legal compliance. However, evolving user preferences and content licensing issues still remain unaddressed. This is particularly true in case of multi-modal recommender systems (MMRS), which aim to accommodate the growing influence of multi-modal information on user preferences. Previous unlearning methods for RS are inapplicable to MMRS due to incompatibility of multi-modal user-item behavior data graph with the matrix based representation of RS. Partitioning based methods degrade recommendation performance and incur significant overhead costs during aggregation. This paper introduces MMRecUN, a new framework for multi-modal recommendation unlearning, which, to the best of our knowledge, is the first attempt in this direction. Given the trained recommendation model and marked forget data, we devise Reverse Bayesian Personalized Ranking (BPR) objective to force the model to forget it. MMRecUN employs both reverse and forward BPR loss mechanisms to selectively attenuate the impact of interactions within the forget set while concurrently reinforcing the significance of interactions within the retain set. Our experiments demonstrate that MMRecUN outperforms baseline methods across various unlearning requests when evaluated on benchmark multi-modal recommender datasets. MMRecUN achieves recall performance improvements of up to $\\mathbf{49.85%}$ compared to the baseline methods. It is up to $\\mathbf{1.3}\\times$ faster than the \\textsc{Gold} model, which is trained on retain data from scratch. MMRecUN offers advantages such as superior performance in removing target elements, preservation of performance for retained elements, and zero overhead costs in comparison to previous methods.",
        "subjects": [
            "cs.LG",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15540",
        "abstract url": "https://arxiv.org/abs/2405.15540",
        "title": "Bundle Neural Networks for message diffusion on graphs",
        "rating": "-3.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "GNN",
                "graph"
            ],
            [
                "pathological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The dominant paradigm for learning on graph-structured data is message passing. Despite being a strong inductive bias, the local message passing mechanism suffers from pathological issues such as over-smoothing, over-squashing, and limited node-level expressivity. To address these limitations we propose Bundle Neural Networks (BuNN), a new type of GNN that operates via message diffusion over flat vector bundles - structures analogous to connections on Riemannian manifolds that augment the graph by assigning to each node a vector space and an orthogonal map. A BuNN layer evolves the features according to a diffusion-type partial differential equation. When discretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a recently proposed MPNN capable of mitigating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate on larger scales of the graph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN can approximate any feature transformation over nodes on any (potentially infinite) family of graphs given injective positional encodings, resulting in universal node-level expressivity. We support our theory via synthetic experiments and showcase the strong empirical performance of BuNNs over a range of real-world tasks, achieving state-of-the-art results on several standard benchmarks in transductive and inductive settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15835",
        "abstract url": "https://arxiv.org/abs/2405.15835",
        "title": "Analyzing the Impact of Climate Change With Major Emphasis on Pollution: A Comparative Study of ML and Statistical Models in Time Series Data",
        "rating": "-3.5",
        "keywords": [
            [
                "Industrial"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Industrial operations have grown exponentially over the last century, driving advancements in energy utilization through vehicles and machinery.This growth has significant environmental implications, necessitating the use of sophisticated technology to monitor and analyze climate data.The surge in industrial activities presents a complex challenge in forecasting its diverse environmental impacts, which vary greatly across different regions.Aim to understand these dynamics more deeply to predict and mitigate the environmental impacts of industrial activities.",
        "subjects": [
            "stat.AP",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15253",
        "abstract url": "https://arxiv.org/abs/2405.15253",
        "title": "Seeing the World through an Antenna's Eye: Reception Quality Visualization Using Incomplete Technical Signal Information",
        "rating": "-4",
        "keywords": [
            [
                "inpainting"
            ],
            [
                "satellite"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We come up with a novel application for image analysis methods in the context of direction dependent signal characteristics. For this purpose, we describe an inpainting approach adding benefit to technical signal information which are typically only used for monitoring and control purposes in ground station operations. Recalling the theoretical properties of the employed inpainting technique and appropriate modeling allow us to demonstrate the usefulness of our approach for satellite data reception quality assessment. In our application, we show the advantages of inpainting products over raw data as well as the rich potential of the visualization of technical signal information.",
        "subjects": [
            "cs.CV",
            "math.NA",
            "physics.data-an"
        ],
        "comment": "5 pages, to be published in the conference proceedings of the European Signal Processing Conference (EUSIPCO) 2024"
    },
    {
        "paper id": "2405.15438",
        "abstract url": "https://arxiv.org/abs/2405.15438",
        "title": "Comparing remote sensing-based forest biomass mapping approaches using new forest inventory plots in contrasting forests in northeastern and southwestern China",
        "rating": "-4",
        "keywords": [
            [
                "LiDAR",
                "Radar"
            ],
            [
                "biomass"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Large-scale high spatial resolution aboveground biomass (AGB) maps play a crucial role in determining forest carbon stocks and how they are changing, which is instrumental in understanding the global carbon cycle, and implementing policy to mitigate climate change. The advent of the new space-borne LiDAR sensor, NASA's GEDI instrument, provides unparalleled possibilities for the accurate and unbiased estimation of forest AGB at high resolution, particularly in dense and tall forests, where Synthetic Aperture Radar (SAR) and passive optical data exhibit saturation. However, GEDI is a sampling instrument, collecting dispersed footprints, and its data must be combined with that from other continuous cover satellites to create high-resolution maps, using local machine learning methods. In this study, we developed local models to estimate forest AGB from GEDI L2A data, as the models used to create GEDI L4 AGB data incorporated minimal field data from China. We then applied LightGBM and random forest regression to generate wall-to-wall AGB maps at 25 m resolution, using extensive GEDI footprints as well as Sentinel-1 data, ALOS-2 PALSAR-2 and Sentinel-2 optical data. Through a 5-fold cross-validation, LightGBM demonstrated a slightly better performance than Random Forest across two contrasting regions. However, in both regions, the computation speed of LightGBM is substantially faster than that of the random forest model, requiring roughly one-third of the time to compute on the same hardware. Through the validation against field data, the 25 m resolution AGB maps generated using the local models developed in this study exhibited higher accuracy compared to the GEDI L4B AGB data. We found in both regions an increase in error as slope increased. The trained models were tested on nearby but different regions and exhibited good performance.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15517",
        "abstract url": "https://arxiv.org/abs/2405.15517",
        "title": "Erase to Enhance: Data-Efficient Machine Unlearning in MRI Reconstruction",
        "rating": "-4",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "medical",
                "MRI"
            ],
            [
                "recommendation"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Machine unlearning is a promising paradigm for removing unwanted data samples from a trained model, towards ensuring compliance with privacy regulations and limiting harmful biases. Although unlearning has been shown in, e.g., classification and recommendation systems, its potential in medical image-to-image translation, specifically in image recon-struction, has not been thoroughly investigated. This paper shows that machine unlearning is possible in MRI tasks and has the potential to benefit for bias removal. We set up a protocol to study how much shared knowledge exists between datasets of different organs, allowing us to effectively quantify the effect of unlearning. Our study reveals that combining training data can lead to hallucinations and reduced image quality in the reconstructed data. We use unlearning to remove hallucinations as a proxy exemplar of undesired data removal. Indeed, we show that machine unlearning is possible without full retraining. Furthermore, our observations indicate that maintaining high performance is feasible even when using only a subset of retain data. We have made our code publicly accessible.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "The paper is accpeted by MIDL 2024"
    },
    {
        "paper id": "2405.15548",
        "abstract url": "https://arxiv.org/abs/2405.15548",
        "title": "UAV-assisted C-RAN for On-demand Cellular Coverage: Opportunities and Challenges",
        "rating": "-4",
        "keywords": [
            [
                "5G"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "The deployment of beyond fifth-generation (5G) infrastructure over disaster-affected regions, temporary hotspot situations (e.g., massive gatherings, etc.), complex terrains (e.g., sea, hills, marshes, etc.) poses numerous challenges for cellular service providers. Recently, unmanned aerial vehicles (UAVs) have emerged as potential candidates to overcome the aforementioned technical issues based on their multi-role capabilities to serve as aerial base stations, mobile relays, and flying wireless access points. As such, the UAVs can act as portable platforms that can be deployed immediately on demand without requiring massive ground infrastructure to support wireless services. This article introduces the integration of UAVs to cloud radio access networks (C-RAN) for beyond 5G applications. The article mainly focuses on the underlying opportunities and challenges to realize the UAV-assisted C-RAN (UC-RAN) architecture in view of three generic application scenarios, i.e., disaster management, hotspots, and complex terrains. A preliminary performance analysis via simulation is further provided for the proposed UC-RAN under hotspot application scenario based on the relevant metrics.",
        "subjects": [
            "cs.NI",
            "cs.ET"
        ],
        "comment": "15 pages, 4 figures, 2 Tables, Submitted for possible publication as a magazine article"
    },
    {
        "paper id": "2405.15705",
        "abstract url": "https://arxiv.org/abs/2405.15705",
        "title": "Sums: Sniffing Unknown Multiband Signals under Low Sampling Rates",
        "rating": "-4",
        "keywords": [
            [
                "5G"
            ],
            [
                "satellite"
            ]
        ],
        "abstract": "Due to sophisticated deployments of all kinds of wireless networks (e.g., 5G, Wi-Fi, Bluetooth, LEO satellite, etc.), multiband signals distribute in a large bandwidth (e.g., from 70 MHz to 8 GHz). Consequently, for network monitoring and spectrum sharing applications, a sniffer for extracting physical layer information, such as structure of packet, with low sampling rate (especially, sub-Nyquist sampling) can significantly improve their cost- and energy-efficiency. However, to achieve a multiband signals sniffer is really a challenge. To this end, we propose Sums, a system that can sniff and analyze multiband signals in a blind manner. Our Sums takes advantage of hardware and algorithm co-design, multi-coset sub-Nyquist sampling hardware, and a multi-task deep learning framework. The hardware component breaks the Nyquist rule to sample GHz bandwidth, but only pays for a 50 MSPS sampling rate. Our multi-task learning framework directly tackles the sampling data to perform spectrum sensing, physical layer protocol recognition, and demodulation for deep inspection from multiband signals. Extensive experiments demonstrate that Sums achieves higher accuracy than the state-of-theart baselines in spectrum sensing, modulation classification, and demodulation. As a result, our Sums can help researchers and end-users to diagnose or troubleshoot their problems of wireless infrastructures deployments in practice.",
        "subjects": [
            "cs.AR",
            "eess.SY"
        ],
        "comment": "12 pages, 9 figures"
    },
    {
        "paper id": "2405.17480",
        "abstract url": "https://arxiv.org/abs/2405.17480",
        "title": "Resilience-by-Design Concepts for 6G Communication Networks",
        "rating": "-4",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "6G"
            ]
        ],
        "abstract": "The sixth generation (6G) mobile communication networks are expected to intelligently integrate into various aspects of modern digital society, including smart cities, homes, healthcare, transportation, and factories. While offering a multitude of services, it is likely that societies become increasingly reliant on 6G infrastructure. Any disruption to these digital services, whether due to human or technical failures, natural disasters, or terrorism, would significantly impact citizens' daily lives. Hence, 6G networks need not only to provide high-performance services but also to be resilient in maintaining essential services in the face of potentially unknown challenges. This paper introduces a comprehensive concept for designing resilient 6G communication networks, summarizing our initial studies within the German Open6GHub project. Adopting an interdisciplinary approach, we propose to embed physical and cyber resilience across all communication system layers, addressing electronics, physical channel, network components and functions, networks, services, and cross-layer and cross-infrastructure considerations. After reviewing the background on resilience concepts, definitions, and approaches, we introduce the proposed resilience-by-design (RBD) concept for 6G communication networks. We further elaborate on the proposed RBD concept along with selected 6G use-cases and present various open problems for future research on 6G resilience.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15832",
        "abstract url": "https://arxiv.org/abs/2405.15832",
        "title": "DETECTA 2.0: Research into non-intrusive methodologies supported by Industry 4.0 enabling technologies for predictive and cyber-secure maintenance in SMEs",
        "rating": "-4.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "industrial"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The integration of predictive maintenance and cybersecurity represents a transformative advancement for small and medium-sized enterprises (SMEs) operating within the Industry 4.0 paradigm. Despite their economic importance, SMEs often face significant challenges in adopting advanced technologies due to resource constraints and knowledge gaps. The DETECTA 2.0 project addresses these hurdles by developing an innovative system that harmonizes real-time anomaly detection, sophisticated analytics, and predictive forecasting capabilities. The system employs a semi-supervised methodology, combining unsupervised anomaly detection with supervised learning techniques. This approach enables more agile and cost-effective development of AI detection systems, significantly reducing the time required for manual case review. At the core lies a Digital Twin interface, providing intuitive real-time visualizations of machine states and detected anomalies. Leveraging cutting-edge AI engines, the system intelligently categorizes anomalies based on observed patterns, differentiating between technical errors and potential cybersecurity incidents. This discernment is fortified by detailed analytics, including certainty levels that enhance alert reliability and minimize false positives. The predictive engine uses advanced time series algorithms like N-HiTS to forecast future machine utilization trends. This proactive approach optimizes maintenance planning, enhances cybersecurity measures, and minimizes unplanned downtimes despite variable production processes. With its modular architecture enabling seamless integration across industrial setups and low implementation costs, DETECTA 2.0 presents an attractive solution for SMEs to strengthen their predictive maintenance and cybersecurity strategies.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15241",
        "abstract url": "https://arxiv.org/abs/2405.15241",
        "title": "Blaze3DM: Marry Triplane Representation with Diffusion for 3D Medical Inverse Problem Solving",
        "rating": "-5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion",
                "super-resolution"
            ],
            [
                "Medical",
                "MRI",
                "CT"
            ],
            [
                "image restoration"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Solving 3D medical inverse problems such as image restoration and reconstruction is crucial in modern medical field. However, the curse of dimensionality in 3D medical data leads mainstream volume-wise methods to suffer from high resource consumption and challenges models to successfully capture the natural distribution, resulting in inevitable volume inconsistency and artifacts. Some recent works attempt to simplify generation in the latent space but lack the capability to efficiently model intricate image details. To address these limitations, we present Blaze3DM, a novel approach that enables fast and high-fidelity generation by integrating compact triplane neural field and powerful diffusion model. In technique, Blaze3DM begins by optimizing data-dependent triplane embeddings and a shared decoder simultaneously, reconstructing each triplane back to the corresponding 3D volume. To further enhance 3D consistency, we introduce a lightweight 3D aware module to model the correlation of three vertical planes. Then, diffusion model is trained on latent triplane embeddings and achieves both unconditional and conditional triplane generation, which is finally decoded to arbitrary size volume. Extensive experiments on zero-shot 3D medical inverse problem solving, including sparse-view CT, limited-angle CT, compressed-sensing MRI, and MRI isotropic super-resolution, demonstrate that Blaze3DM not only achieves state-of-the-art performance but also markedly improves computational efficiency over existing methods (22~40x faster than previous work).",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15336",
        "abstract url": "https://arxiv.org/abs/2405.15336",
        "title": "An iterative closest point algorithm for marker-free 3D shape registration of continuum robots",
        "rating": "-5",
        "keywords": [
            [
                "3D"
            ],
            [
                "robot"
            ],
            [
                "medical",
                "surgical"
            ],
            [
                "physics"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Continuum robots have emerged as a promising technology in the medical field due to their potential of accessing deep sited locations of the human body with low surgical trauma. When deriving physics-based models for these robots, evaluating the models poses a significant challenge due to the difficulty in accurately measuring their intricate shapes. In this work, we present an optimization based 3D shape registration algorithm for estimation of the backbone shape of slender continuum robots as part of a pho togrammetric measurement. Our approach to estimating the backbones optimally matches a parametric three-dimensional curve to images of the robot. Since we incorporate an iterative closest point algorithm into our method, we do not need prior knowledge of the robots position within the respective images. In our experiments with artificial and real images of a concentric tube continuum robot, we found an average maximum deviation of the reconstruction from simulation data of 0.665 mm and 0.939 mm from manual measurements. These results show that our algorithm is well capable of producing high accuracy positional data from images of continuum robots.",
        "subjects": [
            "cs.RO",
            "eess.IV"
        ],
        "comment": "11 pages, 8 figures, 2 algorithms, journal"
    },
    {
        "paper id": "2405.15395",
        "abstract url": "https://arxiv.org/abs/2405.15395",
        "title": "Fieldscale: Locality-Aware Field-based Adaptive Rescaling for Thermal Infrared Image",
        "rating": "-5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Infrared"
            ],
            [
                "Thermal"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Thermal infrared (TIR) cameras are emerging as promising sensors in safety-related fields due to their robustness against external illumination. However, RAW TIR image has 14 bits of pixel depth and needs to be rescaled into 8 bits for general applications. Previous works utilize a global 1D look-up table to compute pixel-wise gain solely based on its intensity, which degrades image quality by failing to consider the local nature of the heat. We propose Fieldscale, a rescaling based on locality-aware 2D fields where both the intensity value and spatial context of each pixel within an image are embedded. It can adaptively determine the pixel gain for each region and produce spatially consistent 8-bit rescaled images with minimal information loss and high visibility. Consistent performance improvement on image quality assessment and two other downstream tasks support the effectiveness and usability of Fieldscale. All the codes are publicly opened to facilitate research advancements in this field. https://github.com/hyeonjaegil/fieldscale",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 8 figures, accepted to RA-L"
    },
    {
        "paper id": "2405.15248",
        "abstract url": "https://arxiv.org/abs/2405.15248",
        "title": "Logic for conditional strong historical necessity in branching time and analyses of an argument for future determinism",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we present a logic for conditional strong historical necessity in branching time and apply it to analyze a nontheological version of Lavenham's argument for future determinism. Strong historical necessity is motivated from a linguistical perspective, and an example of it is ``If I had not gotten away, I must have been dead''. The approach of the logic is as follows. The agent accepts ontic rules concerning how the world evolves over time. She takes some rules as indefeasible, which determine acceptable timelines. When evaluating a sentence with conditional strong historical necessity, we introduce its antecedent as an indefeasible ontic rule and then check whether its consequent holds for all acceptable timelines. The argument is not sound by the logic.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15259",
        "abstract url": "https://arxiv.org/abs/2405.15259",
        "title": "Robust Economic Dispatch with Flexible Demand and Adjustable Uncertainty Set",
        "rating": "-10",
        "keywords": [],
        "abstract": "With more renewable energy sources (RES) integrated into the power system, the intermittency of RES places a heavy burden on the system. The uncertainty of RES is traditionally handled by controllable generators to balance the real time wind power deviation. As the demand side management develops, the flexibility of aggregate loads can be leveraged to mitigate the negative impact of the wind power. In view of this, we study the problem of how to exploit the multi-dimensional flexibility of elastic loads to balance the trade-off between a low generation cost and a low system risk related to the wind curtailment and the power deficiency. These risks are captured by the conditional value-at-risk. Also, unlike most of the existing studies, the uncertainty set of the wind power output in our model is not fixed. By contrast, it is undetermined and co-optimized based on the available load flexibility. We transform the original optimization problem into a convex one using surrogate affine approximation such that it can be solved efficiently. In case studies, we apply our model on a six-bus transmission network and demonstrate that how flexible load aggregators can help to determine the optimal admissible region for the wind power deviations.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15270",
        "abstract url": "https://arxiv.org/abs/2405.15270",
        "title": "A Note on Solving Problems of Substantially Super-linear Complexity in $N^{o(1)}$ Rounds of the Congested Clique",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the possibility of designing $N^{o(1)}$-round protocols for problems of substantially super-linear polynomial-time complexity on the congested clique with about $N^{1/2}$ nodes, where $N$ is the input size. We show that the exponent of the polynomial (if any) bounding the average time complexity of local computations performed at a node in such protocols has to be larger than that of the polynomial bounding the time complexity of the given problem.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2405.15288",
        "abstract url": "https://arxiv.org/abs/2405.15288",
        "title": "The Writing is on the Wall: Analyzing the Boom of Inscriptions and its Impact on EVM-compatible Blockchains",
        "rating": "-10",
        "keywords": [],
        "abstract": "Despite the level of attention given to rollups there is limited empirical research about their performance. To address this gap, we conduct a comprehensive data-driven analysis of the late 2023 transaction boom that is attributed to inscriptions: a novel approach to record data onto a blockchain with no outside server needed. Inscriptions were first introduced on the Bitcoin blockchain to allow for the representation of NFTs or ERC-20-like tokens without smart contracts, but were later spread to other blockchains. This work examines the applications of inscription transactions in Ethereum and its major EVM-compatible rollups and their impact on blockchain scalability during periods of sudden transaction surges. We found that on certain days, inscription-related transactions comprised over 89% on Arbitrum, over 88% on zkSync Era, and over 53% on Ethereum. Furthermore, 99% of these transactions were related to the minting of meme coins, followed by limited trading activity. Unlike L1 blockchains, during periods of transaction surges, zkSync and Arbitrum experienced lower median gas fees, attributable to the compression of L2 transactions for a single L1 batch. Additionally, zkSync Era, a ZK rollup, demonstrated a stronger reduction in fees than optimistic rollups considered in our study: Arbitrum, Base, and Optimism.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "This research article is a work of scholarship and reflects the authors' own views and opinions. It does not necessarily reflect the views or opinions of any other person or organization, including the authors' employer. Readers should not rely on this article for making strategic or commercial decisions, and the authors are not responsible for any losses that may result from such use. arXiv admin note: substantial text overlap with arXiv:2404.11189"
    },
    {
        "paper id": "2405.15293",
        "abstract url": "https://arxiv.org/abs/2405.15293",
        "title": "Transaction Fee Estimation in the Bitcoin System",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the Bitcoin system, transaction fees serve as an incentive for blockchain confirmations. In general, a transaction with a higher fee is likely to be included in the next block mined, whereas a transaction with a smaller fee or no fee may be delayed or never processed at all. However, the transaction fee needs to be specified when submitting a transaction and almost cannot be altered thereafter. Hence it is indispensable to help a client set a reasonable fee, as a higher fee incurs over-spending and a lower fee could delay the confirmation. In this work, we focus on estimating the transaction fee for a new transaction to help with its confirmation within a given expected time. We identify two major drawbacks in the existing works. First, the current industry products are built on explicit analytical models, ignoring the complex interactions of different factors which could be better captured by machine learning based methods; Second, all of the existing works utilize limited knowledge for the estimation which hinders the potential of further improving the estimation quality. As a result, we propose a framework FENN, which aims to integrate the knowledge from a wide range of sources, including the transaction itself, unconfirmed transactions in the mempool and the blockchain confirmation environment, into a neural network model in order to estimate a proper transaction fee. Finally, we conduct experiments on real blockchain datasets to demonstrate the effectiveness and efficiency of our proposed framework over the state-of-the-art works evaluated by MAPE and RMSE. Each variation model in our framework can finish training within one block interval, which shows the potential of our framework to process the realtime transaction updates in the Bitcoin blockchain.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15308",
        "abstract url": "https://arxiv.org/abs/2405.15308",
        "title": "Nudging Users to Change Breached Passwords Using the Protection Motivation Theory",
        "rating": "-10",
        "keywords": [],
        "abstract": "We draw on the Protection Motivation Theory (PMT) to design nudges that encourage users to change breached passwords. Our online experiment ($n$=$1,386$) compared the effectiveness of a threat appeal (highlighting negative consequences of breached passwords) and a coping appeal (providing instructions on how to change the breached password) in a 2x2 factorial design. Compared to the control condition, participants receiving the threat appeal were more likely to intend to change their passwords, and participants receiving both appeals were more likely to end up changing their passwords; both comparisons have a small effect size. Participants' password change behaviors are further associated with other factors such as their security attitudes (SA-6) and time passed since the breach, suggesting that PMT-based nudges are useful but insufficient to fully motivate users to change their passwords. Our study contributes to PMT's application in security research and provides concrete design implications for improving compromised credential notifications.",
        "subjects": [
            "cs.CR",
            "cs.HC"
        ],
        "comment": "Manuscript under review at ACM Transactions on Computer-Human Interaction"
    },
    {
        "paper id": "2405.15339",
        "abstract url": "https://arxiv.org/abs/2405.15339",
        "title": "Environment Sensing-aided Beam Prediction with Transfer Learning for Smart Factory",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we propose an environment sensing-aided beam prediction model for smart factory that can be transferred from given environments to a new environment. In particular, we first design a pre-training model that predicts the optimal beam by sensing the present environmental information. When encountering a new environment, it generally requires collecting a large amount of new training data to retrain the model, whose cost severely impedes the application of the designed pre-training model. Therefore, we next design a transfer learning strategy that fine-tunes the pre-trained model by limited labeled data of the new environment. Simulation results show that when the pre-trained model is fine-tuned by 30\\% of labeled data from the new environment, the Top-10 beam prediction accuracy reaches 94\\%. Moreover, compared with the way to completely re-training the prediction model, the amount of training data and the time cost of the proposed transfer learning strategy reduce 70\\% and 75\\% respectively.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15371",
        "abstract url": "https://arxiv.org/abs/2405.15371",
        "title": "Engineering Optimal Parallel Task Scheduling",
        "rating": "-10",
        "keywords": [],
        "abstract": "The NP-hard scheduling problem P||C_max encompasses a set of tasks with known execution time which must be mapped to a set of identical machines such that the overall completion time is minimized. In this work, we improve existing techniques for optimal P||C_max scheduling with a combination of new theoretical insights and careful practical engineering. Most importantly, we derive techniques to prune vast portions of the search space of branch-and-bound (BnB) approaches. We also propose improved upper and lower bounding techniques which can be combined with any approach to P||C_max. Moreover, we present new benchmarks for P||C_max, based on diverse application data, which can shed light on aspects which prior synthetic instances fail to capture. In an extensive evaluation, we observe that our pruning techniques reduce the number of explored nodes by 90$\\times$ and running times by 12$\\times$. Compared to a state-of-the-art ILP-based approach, our approach is preferable for short running time limits and for instances with large makespans.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15377",
        "abstract url": "https://arxiv.org/abs/2405.15377",
        "title": "Dynamic Planning for Sequential Whole-body Mobile Manipulation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The dynamic Sequential Mobile Manipulation Planning (SMMP) framework is essential for the safe and robust operation of mobile manipulators in dynamic environments. Previous research has primarily focused on either motion-level or task-level dynamic planning, with limitations in handling state changes that have long-term effects or in generating responsive motions for diverse tasks, respectively. This paper presents a holistic dynamic planning framework that extends the Virtual Kinematic Chain (VKC)-based SMMP method, automating dynamic long-term task planning and reactive whole-body motion generation for SMMP problems. The framework consists of an online task planning module designed to respond to environment changes with long-term effects, a VKC-based whole-body motion planning module for manipulating both rigid and articulated objects, alongside a reactive Model Predictive Control (MPC) module for obstacle avoidance during execution. Simulations and real-world experiments validate the framework, demonstrating its efficacy and validity across sequential mobile manipulation tasks, even in scenarios involving human interference.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15381",
        "abstract url": "https://arxiv.org/abs/2405.15381",
        "title": "Single-Event Upset Analysis of a Systolic Array based Deep Neural Network Accelerator",
        "rating": "-10",
        "keywords": [],
        "abstract": "Deep Neural Network (DNN) accelerators are extensively used to improve the computational efficiency of DNNs, but are prone to faults through Single-Event Upsets (SEUs). In this work, we present an in-depth analysis of the impact of SEUs on a Systolic Array (SA) based DNN accelerator. A fault injection campaign is performed through a Register-Transfer Level (RTL) based simulation environment to improve the observability of each hardware block, including the SA itself as well as the post-processing pipeline. From this analysis, we present the sensitivity, independent of a DNN model architecture, for various flip-flop groups both in terms of fault propagation probability and fault magnitude. This allows us to draw detailed conclusions and determine optimal mitigation strategies.",
        "subjects": [
            "cs.AR",
            "eess.SP"
        ],
        "comment": "This work has been submitted to RADECS 2024 for possible publication"
    },
    {
        "paper id": "2405.15392",
        "abstract url": "https://arxiv.org/abs/2405.15392",
        "title": "Decentralized Virtual Research Environment: Empowering Peer-to-Peer Trustworthy Data Sharing and Collaboration",
        "rating": "-10",
        "keywords": [],
        "abstract": "Scientific research, increasingly reliant on data and computational analysis, confronts the challenge of integrating collaboration and data sharing across disciplines. Collaborative frameworks that support decentralized decision-making and knowledge-sharing are essential, yet integrating them into computational environments presents technical challenges, such as decentralized identity, user-centered policy-making, flexible asset management, automated provenance, and distributed collaborative workflow management. In this work, we propose a novel framework called Decentralized Virtual Research Environment (D-VRE) that addresses barriers to scientific research collaboration and data sharing, offering a scalable and adaptable decentralized model. It enhances seamless, trusted data sharing and collaboration within research lifecycles. It incorporates custom sharing policies, secure asset management, collaborative workflows, and research activity tracking, all without centralized oversight. Demonstrated through a real-world case study in the CLARIFY project, D-VRE proved effective in enabling advanced data sharing and collaborative scenarios, showcasing its adaptability in scientific research. Integrated into JupyterLab, D-VRE supports custom collaboration agreements and smart contract-based automated execution on the Ethereum blockchain. This ensures secure, verifiable transactions and promotes trust and reliability in shared research findings.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "This work has been submitted to the journal named Software: Practice and Experience"
    },
    {
        "paper id": "2405.15397",
        "abstract url": "https://arxiv.org/abs/2405.15397",
        "title": "Comparative Analysis of Four Prominent Ant Colony Optimization Variants: Ant System, Rank-Based Ant System, Max-Min Ant System, and Ant Colony System",
        "rating": "-10",
        "keywords": [],
        "abstract": "This research conducts a comparative analysis of four Ant Colony Optimization (ACO) variants -- Ant System (AS), Rank-Based Ant System (ASRank), Max-Min Ant System (MMAS), and Ant Colony System (ACS) -- for solving the Traveling Salesman Problem (TSP). Our findings demonstrate that algorithm performance is significantly influenced by problem scale and instance type. ACS excels in smaller TSP instances due to its rapid convergence, while PACS proves more adaptable for medium-sized problems. MMAS consistently achieves competitive results across all scales, particularly for larger instances, due to its ability to avoid local optima. ASRank, however, struggles to match the performance of the other algorithms. This research provides insights into the strengths and weaknesses of these ACO variants, guiding the selection of the most suitable algorithm for specific TSP applications.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15415",
        "abstract url": "https://arxiv.org/abs/2405.15415",
        "title": "Semi-Supervised Learning via Cross-Prediction-Powered Inference for Wireless Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "In many wireless application scenarios, acquiring labeled data can be prohibitively costly, requiring complex optimization processes or measurement campaigns. Semi-supervised learning leverages unlabeled samples to augment the available dataset by assigning synthetic labels obtained via machine learning (ML)-based predictions. However, treating the synthetic labels as true labels may yield worse-performing models as compared to models trained using only labeled data. Inspired by the recently developed prediction-powered inference (PPI) framework, this work investigates how to leverage the synthetic labels produced by an ML model, while accounting for the inherent bias with respect to true labels. To this end, we first review PPI and its recent extensions, namely tuned PPI and cross-prediction-powered inference (CPPI). Then, we introduce a novel variant of PPI, referred to as tuned CPPI, that provides CPPI with an additional degree of freedom in adapting to the quality of the ML-based labels. Finally, we showcase two applications of PPI-based techniques in wireless systems, namely beam alignment based on channel knowledge maps in millimeter-wave systems and received signal strength information-based indoor localization. Simulation results show the advantages of PPI-based techniques over conventional approaches that rely solely on labeled data or that apply standard pseudo-labeling strategies from semi-supervised learning. Furthermore, the proposed tuned CPPI method is observed to guarantee the best performance among all benchmark schemes, especially in the regime of limited labeled data.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15417",
        "abstract url": "https://arxiv.org/abs/2405.15417",
        "title": "A Systematic Review on Custom Data Gloves",
        "rating": "-10",
        "keywords": [],
        "abstract": "Hands are a fundamental tool humans use to interact with the environment and objects. Through hand motions, we can obtain information about the shape and materials of the surfaces we touch, modify our surroundings by interacting with objects, manipulate objects and tools, or communicate with other people by leveraging the power of gestures. For these reasons, sensorized gloves, which can collect information about hand motions and interactions, have been of interest since the 1980s in various fields, such as Human-Machine Interaction (HMI) and the analysis and control of human motions. Over the last 40 years, research in this field explored different technological approaches and contributed to the popularity of wearable custom and commercial products targeting hand sensorization. Despite a positive research trend, these instruments are not widespread yet outside research environments and devices aimed at research are often ad hoc solutions with a low chance of being reused. This paper aims to provide a systematic literature review for custom gloves to analyze their main characteristics and critical issues, from the type and number of sensors to the limitations due to device encumbrance. The collection of this information lays the foundation for a standardization process necessary for future breakthroughs in this research field.",
        "subjects": [
            "cs.HC",
            "cs.RO"
        ],
        "comment": "17 pages, 12 images"
    },
    {
        "paper id": "2405.15447",
        "abstract url": "https://arxiv.org/abs/2405.15447",
        "title": "A global approach for the redefinition of higher-order flexibility and rigidity",
        "rating": "-10",
        "keywords": [],
        "abstract": "The famous example of the double-Watt mechanism given by Connelly and Servatius raises some problems concerning the classical definitions of higher-order flexibility and rigidity, respectively, as they attest the cusp configuration of the mechanism a third-order rigidity, which conflicts with its continuous flexion. Some attempts were done to resolve the dilemma but they could not settle the problem. As cusp mechanisms demonstrate the basic shortcoming of any local mobility analysis using higher-order constraints, we present a global approach inspired by Sabitov's finite algorithm for testing the bendability of a polyhedron, which allows us (a) to compute iteratively configurations with a higher-order flexion and (b) to come up with a proper redefinition of higher-order flexibility and rigidity. We also give algorithms for computing the flexion orders as well as the associated flexes. The presented approach is demonstrated on several examples (double-Watt mechanisms and Tarnai's Leonardo structure). Moreover, we determine all configurations of a given 3-RPR manipulator with a third-order flexion and present a corresponding joint-bar framework of flexion order 23.",
        "subjects": [
            "math.AG",
            "cs.RO"
        ],
        "comment": "29 pages, 12 figures, 10 examples"
    },
    {
        "paper id": "2405.15461",
        "abstract url": "https://arxiv.org/abs/2405.15461",
        "title": "Optimal market-neutral currency trading on the cryptocurrency platform",
        "rating": "-10",
        "keywords": [],
        "abstract": "This research proposes a novel arbitrage approach with respect to multivariate pair trading called Optimal Trading Technique (OTT). We introduce the method to selectively form a \"bucket\" of fiat currencies anchored to cryptocurrency for simultaneously monitoring and exploiting trading opportunities. To handle the quantitative conflicts that arise when receiving multiple trading signals, a bi-objective convex optimization process is designed to cater to the investor's preference between profitability and risk tolerance. This process includes tunable parameters such as volatility punishment, action thresholds. During our experiments in the cryptocurrency market from 2020 to 2022 when the market was experiencing a vigorous bull-run immediately followed by a bear-run, the OTT realized an annualized profit of 15.49%. We further carried out the experiments in bull, bear, and full-cycle market conditions separately, and found that OTT is capable of achieving stable profit under various market conditions. Apart from the profitability side of the OTT, the arbitrage operation provides a new perspective of trading, which requires no external shorting and never hold intermediate cryptocurrency during the arbitrage period.",
        "subjects": [
            "cs.CE",
            "q-fin.MF"
        ],
        "comment": "27 pages, 8 figures, 10 tables"
    },
    {
        "paper id": "2405.15482",
        "abstract url": "https://arxiv.org/abs/2405.15482",
        "title": "An input-output continuous-time version of Willems' lemma",
        "rating": "-10",
        "keywords": [],
        "abstract": "We illustrate a novel version of Willems' lemma for data-based representation of continuous-time systems. The main novelties compared to previous works are two. First, the proposed framework relies only on measured input-output trajectories from the system and no internal (state) information is required. Second, our system representation makes use of exact system trajectories, without resorting to orthogonal bases representations and consequent approximations. We first establish sufficient and necessary conditions for data-based generation of system trajectories in terms of suitable latent variables. Subsequently, we reformulate these conditions using measured input-output data and show how to span the full behavior of the system. Furthermore, we show how to use the developed framework to solve the data-based continuous-time simulation problem.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2405.15493",
        "abstract url": "https://arxiv.org/abs/2405.15493",
        "title": "Design and Implementation of DC-DC Buck Converter based on Deep Neural Network Sliding Mode Control",
        "rating": "-10",
        "keywords": [],
        "abstract": "In order to address the challenge of traditional sliding mode controllers struggling to balance between suppressing system jitter and accelerating convergence speed, a deep neural network (DNN)-based sliding mode control strategy is proposed in this paper. The strategy achieves dynamic adjustment of parameters by modelling and learning the system through deep neural networks, which suppresses the system jitter while ensuring the convergence speed of the system. To demonstrate the stability of the system, a Lyapunov function is designed to prove the stability of the mathematical model of the DNN-based sliding mode control strategy for DC-DC buck switching power supply. We adopt a double closed-loop control mode to combine the sliding mode control of the voltage inner loop with the PI control of the current outer loop. Simultaneously, The DNN performance is evaluated through simulation and hardware experiments and compared with conventional control methods. The results demonstrate that the sliding mode controller based on the DNN exhibits faster system convergence speed, enhanced jitter suppression capability, and greater robustness.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15497",
        "abstract url": "https://arxiv.org/abs/2405.15497",
        "title": "Finite-time convergence to an $\u03b5$-efficient Nash equilibrium in potential games",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper investigates the convergence time of log-linear learning to an $\u03b5$-efficient Nash equilibrium (NE) in potential games. In such games, an efficient NE is defined as the maximizer of the potential function. Existing results are limited to potential games with stringent structural assumptions and entail exponential convergence times in $1/\u03b5$. Unaddressed so far, we tackle general potential games and prove the first finite-time convergence to an $\u03b5$-efficient NE. In particular, by using a problem-dependent analysis, our bound depends polynomially on $1/\u03b5$. Furthermore, we provide two extensions of our convergence result: first, we show that a variant of log-linear learning that requires a factor $A$ less feedback on the utility per round enjoys a similar convergence time; second, we demonstrate the robustness of our convergence guarantee if log-linear learning is subject to small perturbations such as alterations in the learning rule or noise-corrupted utilities.",
        "subjects": [
            "cs.MA"
        ],
        "comment": "9 main pages, 25 pages, 1 Table"
    },
    {
        "paper id": "2405.15516",
        "abstract url": "https://arxiv.org/abs/2405.15516",
        "title": "Source Code Archiving to the Rescue of Reproducible Deployment",
        "rating": "-10",
        "keywords": [],
        "abstract": "The ability to verify research results and to experiment with methodologies are core tenets of science. As research results are increasingly the outcome of computational processes, software plays a central role. GNU Guix is a software deployment tool that supports reproducible software deployment, making it a foundation for computational research workflows. To achieve reproducibility, we must first ensure the source code of software packages Guix deploys remains available.We describe our work connecting Guix with Software Heritage, the universal source code archive, making Guix the first free software distribution and tool backed by a stable archive. Our contribution is twofold: we explain the rationale and present the design and implementation we came up with; second, we report on the archival coverage for package source code with data collected over five years and discuss remaining challenges.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "2024 ACM Conference on Reproducibility and Replicability, Jun 2024, Rennes, France"
    },
    {
        "paper id": "2405.15520",
        "abstract url": "https://arxiv.org/abs/2405.15520",
        "title": "From Data Complexity to User Simplicity: A Framework for Linked Open Data Reconciliation and Serendipitous Discovery",
        "rating": "-10",
        "keywords": [],
        "abstract": "This article introduces a novel software solution to create a Web portal to align Linked Open Data sources and provide user-friendly interfaces for serendipitous discovery. We present the Polifonia Web portal as a motivating scenario and case study to address research problems such as data reconciliation and serving generous interfaces in the music heritage domain.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "6 pages, 3 figures, part of \"XIII Convegno Annuale AIUCD 2024 Proceedings\""
    },
    {
        "paper id": "2405.15570",
        "abstract url": "https://arxiv.org/abs/2405.15570",
        "title": "Multi-Gigabit Interactive Extended Reality over Millimeter-Wave: An End-to-End System Approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "Achieving high-quality wireless interactive Extended Reality (XR) will require multi-gigabit throughput at extremely low latency. The Millimeter-Wave (mmWave) frequency bands, between 24 and 300GHz, can achieve such extreme performance. However, maintaining a consistently high Quality of Experience with highly mobile users is challenging, as mmWave communications are inherently directional. In this work, we present and evaluate an end-to-end approach to such a mmWave-based mobile XR system. We perform a highly realistic simulation of the system, incorporating accurate XR data traffic, detailed mmWave propagation models and actual user motion. We evaluate the impact of the beamforming strategy and frequency on the overall performance. In addition, we provide the first system-level evaluation of the CoVRage algorithm, a proactive and spatially aware user-side beamforming approach designed specifically for highly mobile XR environments.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "Accepted at IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) 2024"
    },
    {
        "paper id": "2405.15571",
        "abstract url": "https://arxiv.org/abs/2405.15571",
        "title": "RCInvestigator: Towards Better Investigation of Anomaly Root Causes in Cloud Computing Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Finding the root causes of anomalies in cloud computing systems quickly is crucial to ensure availability and efficiency since accurate root causes can guide engineers to take appropriate actions to address the anomalies and maintain customer satisfaction. However, it is difficult to investigate and identify the root causes based on large-scale and high-dimension monitoring data collected from complex cloud computing environments. Due to the inherently dynamic characteristics of cloud computing systems, the existing approaches in practice largely rely on manual analyses for flexibility and reliability, but massive unpredictable factors and high data complexity make the process time-consuming. Despite recent advances in automated detection and investigation approaches, the speed and quality of root cause analyses remain limited by the lack of expert involvement in these approaches. The limitations found in the current solutions motivate us to propose a visual analytics approach that facilitates the interactive investigation of the anomaly root causes in cloud computing systems. We identified three challenges, namely, a) modeling databases for the root cause investigation, b) inferring root causes from large-scale time series, and c) building comprehensible investigation results. In collaboration with domain experts, we addressed these challenges with RCInvestigator, a novel visual analytics system that establishes a tight collaboration between human and machine and assists experts in investigating the root causes of cloud computing system anomalies. We evaluated the effectiveness of RCInvestigator through two use cases based on real-world data and received positive feedback from experts.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15573",
        "abstract url": "https://arxiv.org/abs/2405.15573",
        "title": "Uniform $\\mathcal{H}$-matrix Compression with Applications to Boundary Integral Equations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Boundary integral equation formulations of elliptic partial differential equations lead to dense system matrices when discretized, yet they are data-sparse. Using the $\\mathcal{H}$-matrix format, this sparsity is exploited to achieve $\\mathcal{O}(N\\log N)$ complexity for storage and multiplication by a vector. This is achieved purely algebraically, based on low-rank approximations of subblocks, and hence the format is also applicable to a wider range of problems. The $\\mathcal{H}^2$-matrix format improves the complexity to $\\mathcal{O}(N)$ by introducing a recursive structure onto subblocks on multiple levels. However, in practice this comes with a large proportionality constant, making the $\\mathcal{H}^2$-matrix format advantageous mostly for large problems. In this paper we investigate the usefulness of a matrix format that lies in between these two: Uniform $\\mathcal{H}$-matrices. An algebraic compression algorithm is introduced to transform a regular $\\mathcal{H}$-matrix into a uniform $\\mathcal{H}$-matrix, which maintains the asymptotic complexity.",
        "subjects": [
            "math.NA",
            "cs.MS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15578",
        "abstract url": "https://arxiv.org/abs/2405.15578",
        "title": "Distributed Locking as a Data Type",
        "rating": "-10",
        "keywords": [],
        "abstract": "Mixed-consistency programming models assist programmers in designing applications that provide high availability while still ensuring application-specific safety invariants. However, existing models often make specific system assumptions, such as building on a particular database system or having baked-in coordination strategies. This makes it difficult to apply these strategies in diverse settings, ranging from client/server to ad-hoc peer-to-peer networks. This work proposes a new strategy for building programmable coordination mechanisms based on the algebraic replicated data types (ARDTs) approach. ARDTs allow for simple and composable implementations of various protocols, while making minimal assumptions about the network environment. As a case study, two different locking protocols are presented, both implemented as ARDTs. In addition, we elaborate on our ongoing efforts to integrate the approach into the LoRe mixed-consistency programming language.",
        "subjects": [
            "cs.PL",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15610",
        "abstract url": "https://arxiv.org/abs/2405.15610",
        "title": "Increasing Efficiency and Result Reliability of Continuous Benchmarking for FaaS Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "In a continuous deployment setting, Function-as-a-Service (FaaS) applications frequently receive updated releases, each of which can cause a performance regression. While continuous benchmarking, i.e., comparing benchmark results of the updated and the previous version, can detect such regressions, performance variability of FaaS platforms necessitates thousands of function calls, thus, making continuous benchmarking time-intensive and expensive. In this paper, we propose DuetFaaS, an approach which adapts duet benchmarking to FaaS applications. With DuetFaaS, we deploy two versions of FaaS function in a single cloud function instance and execute them in parallel to reduce the impact of platform variability. We evaluate our approach against state-of-the-art approaches, running on AWS Lambda. Overall, DuetFaaS requires fewer invocations to accurately detect performance regressions than other state-of-the-art approaches. In 99.65% of evaluated cases, our approach provides smaller confidence interval sizes than the comparing approaches, and can reduce the size by up to 98.23%.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15627",
        "abstract url": "https://arxiv.org/abs/2405.15627",
        "title": "The Scattering Matrix-Based Characteristic Mode for Structure amidst Arbitrary Background: Theory, Benchmark and Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a novel approach for computing substructure characteristic modes. This method leverages electromagnetic scattering matrices and spherical wave expansion to directly decompose electromagnetic fields. Unlike conventional methods that rely on the impedance matrix generated by the method of moments (MoM), our technique simplifies the problem into a small-scale ordinary eigenvalue problem, improving numerical dynamics and computational efficiency. We have developed analytical substructure characteristic mode solutions for a scenario involving two spheres, which can serve as benchmarks for evaluating other numerical solvers. A key advantage of our method is its independence from specific MoM frameworks, allowing for the use of various numerical methods. This flexibility paves the way for substructure characteristic mode decomposition to become a universal frequency technique.",
        "subjects": [
            "physics.class-ph",
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15630",
        "abstract url": "https://arxiv.org/abs/2405.15630",
        "title": "GPTZoo: A Large-scale Dataset of GPTs for the Research Community",
        "rating": "-10",
        "keywords": [],
        "abstract": "The rapid advancements in Large Language Models (LLMs) have revolutionized natural language processing, with GPTs, customized versions of ChatGPT available on the GPT Store, emerging as a prominent technology for specific domains and tasks. To support academic research on GPTs, we introduce GPTZoo, a large-scale dataset comprising 730,420 GPT instances. Each instance includes rich metadata with 21 attributes describing its characteristics, as well as instructions, knowledge files, and third-party services utilized during its development. GPTZoo aims to provide researchers with a comprehensive and readily available resource to study the real-world applications, performance, and potential of GPTs. To facilitate efficient retrieval and analysis of GPTs, we also developed an automated command-line interface (CLI) that supports keyword-based searching of the dataset. To promote open research and innovation, the GPTZoo dataset will undergo continuous updates, and we are granting researchers public access to GPTZoo and its associated tools.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15637",
        "abstract url": "https://arxiv.org/abs/2405.15637",
        "title": "Clearing the Path for Software Sustainability",
        "rating": "-10",
        "keywords": [],
        "abstract": "The advancement of software sustainability encounters notable challenges, underscoring the necessity for understanding these challenges to facilitate significant progress and pave the way for effective solutions to advance software sustainability. This paper outlines key challenges identified in literature based on findings from a tertiary study. Challenges identified include: confusion regarding the definition of software sustainability, uncertainty about when to consider sustainability in software development, lack of assessment metrics and tools, narrow perspectives on sustainability in software systems, insufficient awareness and education, and a lack of serious considerations in practice. The paper aims at clarifying the confusion surrounding software sustainability to motivate effective solutions. The provided recommendations aim to give a more organized approach towards advancing sustainable software development, emphasizing comprehensive strategies, the integration of sustainability as a fundamental aspect of software development, actionable research directions, and the cultivation of a common understanding of sustainable software.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15647",
        "abstract url": "https://arxiv.org/abs/2405.15647",
        "title": "A Logic of Knowledge and Justifications, with an Application to Computational Trust",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a logical framework that enables us to define a formal theory of computational trust in which this notion is analysed in terms of epistemic attitudes towards the possible objects of trust and in relation to existing evidence in favour of the trustworthiness of these objects. The framework is based on a quantified epistemic and justification logic featuring a non-standard handling of identities. Thus, the theory is able to account for the hyperintensional nature of computational trust. We present a proof system and a frame semantics for the logic, we prove soundness and completeness results and we introduce the syntactical machinery required to define a theory of trust.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15665",
        "abstract url": "https://arxiv.org/abs/2405.15665",
        "title": "Examining Ownership Models in Software Teams: A Systematic Literature Review and a Replication Study",
        "rating": "-10",
        "keywords": [],
        "abstract": "Effective ownership of software artifacts, particularly code, is crucial for accountability, knowledge sharing, and code quality enhancement. Researchers have proposed models linking ownership of software artifacts with developer performance and code quality. Our study aims to systematically examine various ownership models and provide a structured literature overview. Conducting a systematic literature review, we identified 79 relevant papers published between 2005 and 2022. We developed a taxonomy of ownership artifacts based on type, owners, and degree of ownership, along with compiling modeling variables and analytics types used in each study. Additionally, we assessed the replication status of each study. As a result, we identified nine distinct software artifacts whose ownership has been discussed in the literature, with \"Code\" being the most frequently analyzed artifact. We found that only three papers (3.79%) provided code and data, whereas nine papers (11.4%) provided only data. Using our systematic literature review results, we replicated experiments on nine priority projects at \\texttt{Brightsquid}. The company aimed to compare its code quality against ownership factors in other teams, so we conducted a replication study using their data. Unlike prior studies, we found no strong correlation between minor contributors and bug numbers. Surprisingly, we found no strong link between the total number of developers modifying a file and bug counts, contrasting previous findings. However, we observed a significant correlation between major contributors and bug counts, diverging from earlier research.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Pre-print an accepted paper for the ESE journal"
    },
    {
        "paper id": "2405.15669",
        "abstract url": "https://arxiv.org/abs/2405.15669",
        "title": "Enhancing Reentry Support Programs Through Digital Literacy Integration",
        "rating": "-10",
        "keywords": [],
        "abstract": "Challenges faced by formerly incarcerated individuals in the United States raise questions about our society's ability to truly provide second chances. This paper presents the outcomes of our ongoing collaboration with a non-profit organization dedicated to reentry support. We highlight the multifaceted challenges individuals face during their reentry journey, including support programs that prioritize supervision over service, unresponsive support systems, limited access to resources, financial struggles exacerbated by restricted employment opportunities, and technological barriers. In the face of such complex social challenges, our work aims to facilitate our partner organization's ongoing efforts to promote digital literacy through a web application that is integrated into their existing processes. We share initial feedback from the stakeholders, draw out four implications: supporting continuity of care, promoting reflection through slow technology, building in flexibility, and reconfiguring toward existing infrastructure, and conclude with a reflection on our role as partners on the side.",
        "subjects": [
            "cs.HC",
            "cs.DL"
        ],
        "comment": "15 pages, 1 table, 3 figures"
    },
    {
        "paper id": "2405.15671",
        "abstract url": "https://arxiv.org/abs/2405.15671",
        "title": "The Undecidability of Quantified Announcements",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper demonstrates the undecidability of a number of logics with quantification over public announcements: arbitrary public announcement logic (APAL), group announcement logic (GAL), and coalition announcement logic (CAL). In APAL we consider the informative consequences of any announcement, in GAL we consider the informative consequences of a group of agents (this group may be a proper subset of the set of all agents) all of which are simultaneously (and publicly) making known announcements. So this is more restrictive than APAL. Finally, CAL is as GAL except that we now quantify over anything the agents not in that group may announce simultaneously as well. The logic CAL therefore has some features of game logic and of ATL. We show that when there are multiple agents in the language, the satisfiability problem is undecidable for APAL, GAL, and CAL. In the single agent case, the satisfiability problem is decidable for all three logics. This paper corrects an error to the submitted version of Undecidability of Quantified Announcements, identified by Yuta Assami . The nature of the error was in the definition of the formula cga (X) (see Subsection 5.2) which is corrected in this version.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "This paper contains a correction to the 2016 article, The Undecidablity of Quantified Announcements, published in Studia Logica"
    },
    {
        "paper id": "2405.15690",
        "abstract url": "https://arxiv.org/abs/2405.15690",
        "title": "A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback",
        "rating": "-10",
        "keywords": [],
        "abstract": "Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored. In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously-generated patches. To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Code, data and artifacts are available: http://tinyurl.com/vrpilot-artifacts"
    },
    {
        "paper id": "2405.15717",
        "abstract url": "https://arxiv.org/abs/2405.15717",
        "title": "Integrated Design for Wave Energy Converter Farms: Assessing Plant, Control, Layout, and Site Selection Coupling in the Presence of Irregular Waves",
        "rating": "-10",
        "keywords": [],
        "abstract": "A promising direction towards reducing the levelized cost of energy for wave energy converter (WEC) farms is to improve their performance. WEC design studies generally focus on a single design domain (e.g., geometry, control, or layout) to improve the farm's performance under simplifying assumptions, such as regular waves. This strategy, however, has resulted in design recommendations that are impractical or limited in scope because WEC farms are complex systems that exhibit strong coupling among geometry, control, and layout domains. In addition, the location of the candidate site, which has a large impact on the performance of the farm, is often overlooked. Motivated by some of the limitations observed in WEC literature, this study uses an integrated design framework, based on simultaneous control co-design (CCD) principles, to discuss the impact of site selection and wave type on WEC farm design. Interactions among plant, control, and layout are also investigated and discussed using a wide range of simulations and optimization studies. All of the studies were conducted using frequency-domain heaving cylinder WEC devices within a farm with a linear reactive controller in the presence of irregular probabilistic waves. The results provide high-level guidelines to help the WEC design community move toward an integrated design perspective.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "12 pages and 7 figures"
    },
    {
        "paper id": "2405.15737",
        "abstract url": "https://arxiv.org/abs/2405.15737",
        "title": "More Insight from Being More Focused: Analysis of Clustered Market Apps",
        "rating": "-10",
        "keywords": [],
        "abstract": "The increasing attraction of mobile apps has inspired researchers to analyze apps from different perspectives. As with any software product, apps have different attributes such as size, content maturity, rating, category, or number of downloads. Current research studies mostly consider sampling across all apps. This often results in comparisons of apps being quite different in nature and category (games compared with weather and calendar apps), also being different in size and complexity. Similar to proprietary software and web-based services, more specific results can be expected from looking at more homogeneous samples as they can be received as a result of applying clustering. In this paper, we target homogeneous samples of apps to increase the degree of insight gained from analytics. As a proof-of-concept, we applied the clustering technique DBSCAN and subsequent correlation analysis between app attributes for a set of 940 open-source mobile apps from F-Droid. We showed that (i) clusters of apps with similar characteristics provided more insight compared to applying the same to the whole data and (ii) defining the similarity of apps based on the similarity of topics as created from the topic modeling technique Latent Dirichlet Allocation does not significantly improve clustering results.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Authors pre-print"
    },
    {
        "paper id": "2405.15747",
        "abstract url": "https://arxiv.org/abs/2405.15747",
        "title": "Over-the-Air Runtime Wi-Fi MAC Address Re-randomization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Medium Access Control (MAC) address randomization is a key component for privacy protection in Wi-Fi networks. Current proposals periodically change the mobile device MAC addresses when it disconnects from the Access Point (AP). This way frames cannot be linked across changes, but the mobile device presence is exposed as long as it remains connected: all its communication is trivially linkable by observing the randomized yet same MAC address throughout the connection. Our runtime MAC re-randomization scheme addresses this issue, reducing or eliminating Wi-Fi frames linkability without awaiting for or requiring a disconnection. Our MAC re-randomization is practically 'over-the-air': MAC addresses are re-randomized just before transmission, while the protocol stacks (at the mobile and the AP) maintain locally the original connection MAC addresses - making our MAC layer scheme transparent to upper layers. With an implementation and a set of small-scale experiments with off-the-shelf devices, we show the feasibility of our scheme and the potential towards future deployment.",
        "subjects": [
            "cs.NI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.15752",
        "abstract url": "https://arxiv.org/abs/2405.15752",
        "title": "Analysis of Marketed versus Not-marketed Mobile App Releases",
        "rating": "-10",
        "keywords": [],
        "abstract": "Market and user characteristics of mobile apps make their release management different from proprietary software products and web services. Despite the wealth of information regarding users' feedback on an app, an in-depth analysis of app releases is difficult due to the inconsistency and uncertainty of the information. To better understand and potentially improve app release processes, we analyze major, minor, and patch releases for releases following semantic versioning. In particular, we were interested in finding out the difference between marketed and not-marketed releases. Our results show that, in general, major, minor, and patch releases have significant differences in the release cycle duration, nature, and change velocity. We also observed that there is a significant difference between marketed and non-marketed mobile app releases in terms of cycle duration, nature and the extent of changes, and the number of opened and closed issues.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Authors version"
    },
    {
        "paper id": "2405.15762",
        "abstract url": "https://arxiv.org/abs/2405.15762",
        "title": "Sliding-Mode Nash Equilibrium Seeking for a Quadratic Duopoly Game",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces a new method to achieve stable convergence to Nash equilibrium in duopoly noncooperative games. Inspired by the recent fixed-time Nash Equilibrium seeking (NES) as well as prescribed-time extremum seeking (ES) and source seeking schemes, our approach employs a distributed sliding mode control (SMC) scheme, integrating extremum seeking with sinusoidal perturbation signals to estimate the pseudogradients of quadratic payoff functions. Notably, this is the first attempt to address noncooperative games without relying on models, combining classical extremum seeking with relay components instead of proportional control laws. We prove finite-time convergence of the closed-loop average system to Nash equilibrium using stability analysis techniques such as time-scaling, Lyapunov's direct method, and averaging theory for discontinuous systems. Additionally, we quantify the size of residual sets around the Nash equilibrium and validate our theoretical results through simulations.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "8 pages and 2 figures. arXiv admin note: substantial text overlap with arXiv:2404.07287"
    },
    {
        "paper id": "2405.15879",
        "abstract url": "https://arxiv.org/abs/2405.15879",
        "title": "Global Output-Feedback Extremum Seeking Control with Source Seeking Experiments",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper discusses the design of an extremum seeking controller that relies on a monitoring function for a class of SISO uncertain nonlinear systems characterized by arbitrary and uncertain relative degree. Our demonstration illustrates the feasibility of achieving an arbitrarily small proximity to the desired optimal point through output feedback. The core concept involves integrating a monitoring function with a norm state observer for the unitary relative degree case and its expansion to arbitrary relative degrees by means of the employment of a time-scaling technique. Significantly, our proposed scheme attains the extremum of an unknown nonlinear mapping across the entire domain of initial conditions, ensuring global convergence and stability for the real-time optimization algorithm. Furthermore, we provide tuning rules to ensure convergence to the global maximum in the presence of local extrema. To validate the effectiveness of the proposed approach, we present a numerical example and apply it to a source-seeking problem involving a cart-track linear positioning servomechanism. Notably, the cart lacks the ability to sense its velocity or the source's position, but can detect the source of a light signal of unknown concentration field.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "27 pages, 14 figures"
    },
    {
        "paper id": "2405.15929",
        "abstract url": "https://arxiv.org/abs/2405.15929",
        "title": "Product Design Using Generative Adversarial Network: Incorporating Consumer Preference and External Data",
        "rating": "-10",
        "keywords": [],
        "abstract": "The development of Generative AI enables large-scale automation of product design. However, this automated process usually does not incorporate consumer preference information from a company's internal dataset. Meanwhile, external sources such as social media and user-generated content (UGC) websites often contain rich product design and consumer preference information, but such information is not utilized by companies in design generation. We propose a semi-supervised deep generative framework that integrates consumer preferences and external data into product design, allowing companies to generate consumer-preferred designs in a cost-effective and scalable way. We train a predictor model to learn consumer preferences and use predicted popularity levels as additional input labels to guide the training of a Continuous Conditional Generative Adversarial Network (CcGAN). The CcGAN can be instructed to generate new designs of a certain popularity level, enabling companies to efficiently create consumer-preferred designs and save resources by avoiding developing and testing unpopular designs. The framework also incorporates existing product designs and consumer preference information from external sources, which is particularly helpful for small or start-up companies who have limited internal data and face the \"cold-start\" problem. We apply the proposed framework to a real business setting by helping a large self-aided photography chain in China design new photo templates. We show that our proposed model performs well in generating appealing template designs for the company.",
        "subjects": [
            "econ.GN",
            "cs.HC"
        ],
        "comment": "46 pages, 26 figures, 5 tables"
    },
    {
        "paper id": "2405.15935",
        "abstract url": "https://arxiv.org/abs/2405.15935",
        "title": "Optimizing Secrecy Codes Using Gradient Descent",
        "rating": "-10",
        "keywords": [],
        "abstract": "Recent theoretical developments in coset coding theory have provided continuous-valued functions which give the equivocation and maximum likelihood (ML) decoding probability of coset secrecy codes. In this work, we develop a method for incorporating these functions, along with a complex set of constraints, into a gradient descent optimization algorithm. This algorithm employs a movement cost function and trigonometric update step to ensure that the continuous-valued code definition vector ultimately reaches a value which yields a realizable coset code. This algorithm is used to produce coset codes with blocklength up to a few thousand. These codes were compared against published codes, including both short-blocklength and capacity-achieving constructions. For most code sizes, codes generated using gradient descent outperformed all others, especially capacity-achieving constructions, which performed significantly worse than randomly-generated codes at short blocklength.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "25 pages, 23 figures. Submitted to Transactions on Information Theory"
    },
    {
        "paper id": "2405.15969",
        "abstract url": "https://arxiv.org/abs/2405.15969",
        "title": "Massive Digital Over-the-Air Computation for Communication-Efficient Federated Edge Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Over-the-air computation (AirComp) is a promising technology converging communication and computation over wireless networks, which can be particularly effective in model training, inference, and more emerging edge intelligence applications. AirComp relies on uncoded transmission of individual signals, which are added naturally over the multiple access channel thanks to the superposition property of the wireless medium. Despite significantly improved communication efficiency, how to accommodate AirComp in the existing and future digital communication networks, that are based on discrete modulation schemes, remains a challenge. This paper proposes a massive digital AirComp (MD-AirComp) scheme, that leverages an unsourced massive access protocol, to enhance compatibility with both current and next-generation wireless networks. MD-AirComp utilizes vector quantization to reduce the uplink communication overhead, and employs shared quantization and modulation codebooks. At the receiver, we propose a near-optimal approximate message passing-based algorithm to compute the model aggregation results from the superposed sequences, which relies on estimating the number of devices transmitting each code sequence, rather than trying to decode the messages of individual transmitters. We apply MD-AirComp to the federated edge learning (FEEL), and show that it significantly accelerates FEEL convergence compared to state-of-the-art while using the same amount of communication resources. To support further research and ensure reproducibility, we have made our code available at https://github.com/liqiao19/MD-AirComp.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "To be published in the IEEE Journal on Selected Areas in Communications"
    },
    {
        "paper id": "2405.15982",
        "abstract url": "https://arxiv.org/abs/2405.15982",
        "title": "Automated Assessment and Adaptive Multimodal Formative Feedback Improves Psychomotor Skills Training Outcomes in Quadrotor Teleoperation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The workforce will need to continually upskill in order to meet the evolving demands of industry, especially working with robotic and autonomous systems. Current training methods are not scalable and do not adapt to the skills that learners already possess. In this work, we develop a system that automatically assesses learner skill in a quadrotor teleoperation task using temporal logic task specifications. This assessment is used to generate multimodal feedback based on the principles of effective formative feedback. Participants perceived the feedback positively. Those receiving formative feedback viewed the feedback as more actionable compared to receiving summary statistics. Participants in the multimodal feedback condition were more likely to achieve a safe landing and increased their safe landings more over the experiment compared to other feedback conditions. Finally, we identify themes to improve adaptive feedback and discuss and how training for complex psychomotor tasks can be integrated with learning theories.",
        "subjects": [
            "cs.RO",
            "cs.HC"
        ],
        "comment": "Under review at Human-Agent Interaction 2024 conference"
    },
    {
        "paper id": "2405.15997",
        "abstract url": "https://arxiv.org/abs/2405.15997",
        "title": "$\\textit{UniSaT}$: Unified-Objective Belief Model and Planner to Search for and Track Multiple Objects",
        "rating": "-10",
        "keywords": [],
        "abstract": "The problem of path planning for autonomously searching and tracking multiple objects is important to reconnaissance, surveillance, and many other data-gathering applications. Due to the inherent competing objectives of searching for new objects while maintaining tracks for found objects, most current approaches rely on multi-objective planning methods, leaving it up to the user to tune parameters to balance between the two objectives, usually based on heuristics or trial and error. In this paper, we introduce $\\textit{UniSaT}$ ($\\textit{Unified Search and Track}$), a unified-objective formulation for the search and track problem based on Random Finite Sets (RFS). This is done by modeling both the unknown and known objects through a combined generalized labeled multi-Bernoulli (GLMB) filter. For the unseen objects, we can leverage both cardinality and spatial prior distributions, which means $\\textit{UniSaT}$ does not rely on knowing the exact count of the expected number of objects in the space. The planner maximizes the mutual information of this unified belief model, creating balanced search and tracking behaviors. We demonstrate our work in a simulated environment and show both qualitative results as well as quantitative improvements over a multi-objective method.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "13 pages, 5 figures, 1 table"
    },
    {
        "paper id": "2405.16004",
        "abstract url": "https://arxiv.org/abs/2405.16004",
        "title": "Transmission des Ondes \u00c9lectromagn\u00e9tiques",
        "rating": "-10",
        "keywords": [],
        "abstract": "This book aims to familiarize Electrical Engineering students with exercises and problems related to electromagnetic wave transmission. It includes a brief theoretical introduction to classical electromagnetism formulas to bridge the gap between theory and practical applications. The book focuses on free space, coaxial cables, and waveguides, emphasizing the role of complex permittivity in transmission. It does not cover antennas or the generation of electromagnetic waves. The content is based on courses taught at \u00c9cole Polytechnique de Montr\u00e9al from 1986 to 2017. -- Ce livre vise \u00e0 familiariser les \u00e9tudiants en G\u00e9nie \u00c9lectrique avec des exercices et des probl\u00e8mes concernant la transmission des ondes \u00e9lectromagn\u00e9tiques. Il inclut une br\u00e8ve introduction th\u00e9orique aux formules classiques de l'\u00e9lectromagn\u00e9tisme pour combler l'\u00e9cart entre la th\u00e9orie et les applications pratiques. Le livre se concentre sur l'espace libre, les c\u00e2bles coaxiaux et les guides d'ondes, en soulignant le r\u00f4le de la permittivit\u00e9 complexe dans la transmission. Il ne couvre pas les antennes ni la g\u00e9n\u00e9ration des ondes \u00e9lectromagn\u00e9tiques. Le contenu est bas\u00e9 sur des cours dispens\u00e9s \u00e0 l'\u00c9cole Polytechnique de Montr\u00e9al de 1986 \u00e0 2017.",
        "subjects": [
            "eess.SP",
            "physics.class-ph"
        ],
        "comment": "in French language"
    },
    {
        "paper id": "2405.16011",
        "abstract url": "https://arxiv.org/abs/2405.16011",
        "title": "Semantic Importance-Aware Communications with Semantic Correction Using Large Language Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "Semantic communications, a promising approach for agent-human and agent-agent interactions, typically operate at a feature level, lacking true semantic understanding. This paper explores understanding-level semantic communications (ULSC), transforming visual data into human-intelligible semantic content. We employ an image caption neural network (ICNN) to derive semantic representations from visual data, expressed as natural language descriptions. These are further refined using a pre-trained large language model (LLM) for importance quantification and semantic error correction. The subsequent semantic importance-aware communications (SIAC) aim to minimize semantic loss while respecting transmission delay constraints, exemplified through adaptive modulation and coding strategies. At the receiving end, LLM-based semantic error correction is utilized. If visual data recreation is desired, a pre-trained generative artificial intelligence (AI) model can regenerate it using the corrected descriptions. We assess semantic similarities between transmitted and recovered content, demonstrating ULSC's superior ability to convey semantic understanding compared to feature-level semantic communications (FLSC). ULSC's conversion of visual data to natural language facilitates various cognitive tasks, leveraging human knowledge bases. Additionally, this method enhances privacy, as neither original data nor features are directly transmitted.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16023",
        "abstract url": "https://arxiv.org/abs/2405.16023",
        "title": "Spiking Neural Network Phase Encoding for Cognitive Computing",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a novel approach for signal reconstruction using Spiking Neural Networks (SNN) based on the principles of Cognitive Informatics and Cognitive Computing. The proposed SNN leverages the Discrete Fourier Transform (DFT) to represent and reconstruct arbitrary time series signals. By employing N spiking neurons, the SNN captures the frequency components of the input signal, with each neuron assigned a unique frequency. The relationship between the magnitude and phase of the spiking neurons and the DFT coefficients is explored, enabling the reconstruction of the original signal. Additionally, the paper discusses the encoding of impulse delays and the phase differences between adjacent frequency components. This research contributes to the field of signal processing and provides insights into the application of SNN for cognitive signal analysis and reconstruction.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "8 pages, 9 figures, IEEE ICCI*CC 2023: 2023 IEEE 22nd International Conference on Cognitive Informatics and Cognitive Computing Stanford Univ. Palo Alto, CA, United States, August 19-21, 2023"
    },
    {
        "paper id": "2405.16033",
        "abstract url": "https://arxiv.org/abs/2405.16033",
        "title": "Wrangling Data Issues to be Wrangled: Literature Review, Taxonomy, and Industry Case Study",
        "rating": "-10",
        "keywords": [],
        "abstract": "Data quality is vital for user experience in products reliant on data. As solutions for data quality problems, researchers have developed various taxonomies for different types of issues. However, although some of the existing taxonomies are near-comprehensive, the over-complexity has limited their actionability in data issue solution development. Hence, recent researchers issued new sets of data issue categories that are more concise for better usability. Although more concise, modern data issue labeling's over-catering to the solution systems may sometimes cause the taxonomy to be not mutually exclusive. Consequently, different categories sometimes overlap in determining the issue types, or the same categories share different definitions across research. This hinders solution development and confounds issue detection. Therefore, based on observations from a literature review and feedback from our industry partner, we propose a comprehensive taxonomy of data quality issues from two distinct dimensions: the attribute dimension represents the intrinsic characteristics and the outcome dimension that indicates the manifestation of the issues. With the categories redefined, we labeled the reported data issues in our industry partner's data warehouse. The labeled issues provide us with a general idea of the distributions of each type of problem and which types of issues require the most effort and care to deal with. Our work aims to address a widely generalizable taxonomy rule in modern data quality issue engineering and helps practitioners and researchers understand their data issues and estimate the efforts required for issue fixing.",
        "subjects": [
            "cs.DB",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16047",
        "abstract url": "https://arxiv.org/abs/2405.16047",
        "title": "Unified Timing Analysis for Closed-Loop Goal-Oriented Wireless Communication",
        "rating": "-10",
        "keywords": [],
        "abstract": "Goal-oriented communication has become one of the focal concepts in sixth-generation communication systems owing to its potential to provide intelligent, immersive, and real-time mobile services. The emerging paradigms of goal-oriented communication constitute closed loops integrating communication, computation, and sensing. However, challenges arise for closed-loop timing analysis due to multiple random factors that affect the communication/computation latency, as well as the heterogeneity of feedback mechanisms across multi-modal sensing data. To tackle these problems, we aim to provide a unified timing analysis framework for closed-loop goal-oriented communication (CGC) systems over fading channels. The proposed framework is unified as it considers computation, compression, and communication latency in the loop with different configurations. To capture the heterogeneity across multi-modal feedback, we categorize the sensory data into the periodic-feedback and event-triggered, respectively. We formulate timing constraints based on average and tail performance, covering timeliness, jitter, and reliability of CGC systems. A method based on saddlepoint approximation is proposed to obtain the distribution of closed-loop latency. The results show that the modified saddlepoint approximation is capable of accurately characterizing the latency distribution of the loop with analytically tractable expressions. This sets the basis for low-complexity co-design of communication and computation.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "submitted to IEEE Trans. Wireless Commun"
    },
    {
        "paper id": "2405.16048",
        "abstract url": "https://arxiv.org/abs/2405.16048",
        "title": "A New Construction of Optimal Symmetrical ZCCS",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose new constructions for a two-dimensional ($2$D) perfect array, complete complementary code (CCC), and multiple CCCs as an optimal symmetrical $Z$-complementary code set (ZCCS). We propose a method to generate a two-dimensional perfect array and CCC. By utilising mutually orthogonal sequences, we developed a method to extend the length of a CCC without affecting the set or code size. Additionally, this concept is extended to include the development of multiple CCCs, and the correlation characteristics of these multiple CCCs are identical with the characteristics of optimal symmetrical ZCCS.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "This paper has been accepted in 'IEEE International Symposium on Information Theory (ISIT 2024)'"
    },
    {
        "paper id": "2405.16050",
        "abstract url": "https://arxiv.org/abs/2405.16050",
        "title": "Rationalizability, Iterated Dominance, and the Theorems of Radon and Carath\u00e9odory",
        "rating": "-10",
        "keywords": [],
        "abstract": "The game theoretic concepts of rationalizability and iterated dominance are closely related and provide characterizations of each other. Indeed, the equivalence between them implies that in a two player finite game, the remaining set of actions available to players after iterated elimination of strictly dominated strategies coincides with the rationalizable actions. I prove a dimensionality result following from these ideas. I show that for two player games, the number of actions available to the opposing player provides a (tight) upper bound on how a player's pure strategies may be strictly dominated by mixed strategies. I provide two different frameworks and interpretations of dominance to prove this result, and in doing so relate it to Radon's Theorem and Carath\u00e9odory's Theorem from convex geometry. These approaches may be seen as following from point-line duality. A new proof of the classical equivalence between these solution concepts is also given.",
        "subjects": [
            "cs.GT",
            "econ.TH"
        ],
        "comment": "Published in Stanford Economic Review"
    }
]