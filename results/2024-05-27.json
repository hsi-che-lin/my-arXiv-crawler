[
    {
        "paper id": "2405.16860",
        "abstract url": "https://arxiv.org/abs/2405.16860",
        "title": "Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes and discrimination. In this paper, we focus on mitigating gender bias towards vision-language tasks. We identify object hallucination as the essence of gender bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances. Moreover, most VLMs rely on the co-occurrence between specific objects and gender attributes to infer the ignored features, ultimately resulting in gender bias. We propose GAMA, a task-agnostic generation framework to mitigate gender bias. GAMA consists of two stages: narrative generation and answer inference. During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes. During answer inference, GAMA integrates the image, generated narrative, and a task-specific question prompt to infer answers for different vision-language tasks. This approach allows the model to rethink gender attributes and answers. We conduct extensive experiments on GAMA, demonstrating its debiasing and generalization ability.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accept to NAACL 2024(main)"
    },
    {
        "paper id": "2405.16915",
        "abstract url": "https://arxiv.org/abs/2405.16915",
        "title": "Multilingual Diversity Improves Vision-Language Representations",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning. These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet). Consequently, existing data curation techniques gravitate towards using predominantly English image-text pairs and discard many potentially useful non-English samples. Our work questions this practice. Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data. We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks. By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set. Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark. On a geographically diverse task like GeoDE, we also observe improvements across all regions, with the biggest gain coming from Africa. In addition, we quantitatively show that English and non-English data are significantly different in both image and (translated) text space. We hope that our findings motivate future work to be more intentional about including multicultural and multilingual data, not just when non-English or geographically diverse tasks are involved, but to enhance model capabilities at large.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16934",
        "abstract url": "https://arxiv.org/abs/2405.16934",
        "title": "Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visual Commonsense Reasoning (VCR) calls for explanatory reasoning behind question answering over visual scenes. To achieve this goal, a model is required to provide an acceptable rationale as the reason for the predicted answers. Progress on the benchmark dataset stems largely from the recent advancement of Vision-Language Transformers (VL Transformers). These models are first pre-trained on some generic large-scale vision-text datasets, and then the learned representations are transferred to the downstream VCR task. Despite their attractive performance, this paper posits that the VL Transformers do not exhibit visual commonsense, which is the key to VCR. In particular, our empirical results pinpoint several shortcomings of existing VL Transformers: small gains from pre-training, unexpected language bias, limited model architecture for the two inseparable sub-tasks, and neglect of the important object-tag correlation. With these findings, we tentatively suggest some future directions from the aspect of dataset, evaluation metric, and training tricks. We believe this work could make researchers revisit the intuition and goals of VCR, and thus help tackle the remaining challenges in visual reasoning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17062",
        "abstract url": "https://arxiv.org/abs/2405.17062",
        "title": "Unifying Demonstration Selection and Compression for In-Context Learning",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In-context learning (ICL) facilitates large language models (LLMs) exhibiting spectacular emergent capabilities in various scenarios. Unfortunately, introducing demonstrations easily makes the prompt length explode, bringing a significant burden to hardware. In addition, random demonstrations usually achieve limited improvements in ICL, necessitating demonstration selection among accessible candidates. Previous studies introduce extra modules to perform demonstration compression or selection independently. In this paper, we propose an ICL framework UniICL, which Unifies demonstration selection and compression, and final response generation via a single frozen LLM. Specifically, UniICL first projects actual demonstrations and inference text inputs into short virtual tokens, respectively. Then, virtual tokens are applied to select suitable demonstrations by measuring semantic similarity within latent space among candidate demonstrations and inference input. Finally, inference text inputs together with selected virtual demonstrations are fed into the same frozen LLM for response generation. Notably, UniICL is a parameter-efficient framework that only contains 17M trainable parameters originating from the projection layer. We conduct experiments and analysis over in- and out-domain datasets of both generative and understanding tasks, encompassing ICL scenarios with plentiful and limited demonstration candidates. Results show that UniICL effectively unifies $12 \\times$ compression, demonstration selection, and response generation, efficiently scaling up the baseline from 4-shot to 64-shot ICL in IMDb with 24 GB CUDA allocation",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17357",
        "abstract url": "https://arxiv.org/abs/2405.17357",
        "title": "DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Fine-tuning large-scale pre-trained models is inherently a resource-intensive task. While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes. To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget. Our code is available at https://github.com/MIkumikumi0116/DoRA",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by the main conference of ACL 2024"
    },
    {
        "paper id": "2405.17423",
        "abstract url": "https://arxiv.org/abs/2405.17423",
        "title": "Privacy-Aware Visual Language Models",
        "rating": "2",
        "keywords": [
            [
                "Visual Language",
                "VLMs"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark PrivBench, which contains images from 8 sensitive categories such as passports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this benchmark and observe a generally limited understanding of privacy, highlighting a significant area for model improvement. Based on this we introduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs with knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa and MiniGPT-v2, on this small dataset, we achieve strong gains in their ability to recognize sensitive content, outperforming even GPT4-V. At the same time, we show that privacy-tuning only minimally affects the VLMs performance on standard benchmarks such as VQA. Overall, this paper lays out a crucial challenge for making VLMs effective in handling real-world data safely and provides a simple recipe that takes the first step towards building privacy-aware VLMs.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "preprint"
    },
    {
        "paper id": "2405.17604",
        "abstract url": "https://arxiv.org/abs/2405.17604",
        "title": "LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The recent trend in scaling language models has led to a growing demand for parameter-efficient tuning (PEFT) methods such as LoRA (Low-Rank Adaptation). LoRA consistently matches or surpasses the full fine-tuning baseline with fewer parameters. However, handling numerous task-specific or user-specific LoRA modules on top of a base model still presents significant storage challenges. To address this, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small number of parameters), a novel approach leveraging Singular Value Decomposition (SVD) for parameter-efficient fine-tuning. LoRA-XS introduces a small r x r weight matrix between frozen LoRA matrices, which are constructed by SVD of the original weight matrix. Training only r x r weight matrices ensures independence from model dimensions, enabling more parameter-efficient fine-tuning, especially for larger models. LoRA-XS achieves a remarkable reduction of trainable parameters by over 100x in 7B models compared to LoRA. Our benchmarking across various scales, including GLUE, GSM8k, and MATH benchmarks, shows that our approach outperforms LoRA and recent state-of-the-art approaches like VeRA in terms of parameter efficiency while maintaining competitive performance.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17815",
        "abstract url": "https://arxiv.org/abs/2405.17815",
        "title": "Visual Anchors Are Strong Information Aggregators For Multimodal Large Language Model",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the realm of Multimodal Large Language Models (MLLMs), vision-language connector plays a crucial role to link the pre-trained vision encoders with Large Language Models (LLMs). Despite its importance, the vision-language connector has been relatively less explored. In this study, we aim to propose a strong vision-language connector that enables MLLMs to achieve high accuracy while maintain low computation cost. We first reveal the existence of the visual anchors in Vision Transformer and propose a cost-effective search algorithm to extract them. Building on these findings, we introduce the Anchor Former (AcFormer), a novel vision-language connector designed to leverage the rich prior knowledge obtained from these visual anchors during pretraining, guiding the aggregation of information. Through extensive experimentation, we demonstrate that the proposed method significantly reduces computational costs by nearly two-thirds compared with baseline, while simultaneously outperforming baseline methods. This highlights the effectiveness and efficiency of AcFormer.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17820",
        "abstract url": "https://arxiv.org/abs/2405.17820",
        "title": "Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This study addresses the issue observed in Large Vision Language Models (LVLMs), where excessive attention on a few image tokens, referred to as blind tokens, leads to hallucinatory responses in tasks requiring fine-grained understanding of visual objects. We found that tokens receiving lower attention weights often hold essential information for identifying nuanced object details -- ranging from merely recognizing object existence to identifying their attributes (color, position, etc.) and understanding their relationships. To counteract the over-emphasis on blind tokens and to accurately respond to user queries, we introduce a technique called Attentional Vision Calibration (AVC). During the decoding phase, AVC identifies blind tokens by analyzing the image-related attention distribution. It then dynamically adjusts the logits for the next token prediction by contrasting the logits conditioned on the original visual tokens with those conditioned on the blind tokens. This effectively lowers the dependency on blind tokens and promotes a more balanced consideration of all tokens. We validate AVC on benchmarks such as POPE, MME, and AMBER, where it consistently outperforms existing decoding techniques in mitigating object hallucinations in LVLMs.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Project page: https://sangminwoo.github.io/AvisC/"
    },
    {
        "paper id": "2405.17821",
        "abstract url": "https://arxiv.org/abs/2405.17821",
        "title": "RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs",
        "rating": "2",
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have revolutionized how machines understand and generate textual responses based on visual inputs. Despite their impressive capabilities, they often produce \"hallucinatory\" outputs that do not accurately reflect the visual information, posing challenges in reliability and trustworthiness. Current methods such as contrastive decoding have made strides in addressing these issues by contrasting the original probability distribution of generated tokens with distorted counterparts; yet, generating visually-faithful outputs remains a challenge. In this work, we shift our focus to the opposite: What could serve as a complementary enhancement to the original probability distribution? We propose a simple, training-free method termed RITUAL to enhance robustness against hallucinations in LVLMs. Our approach employs random image transformations as complements to the original probability distribution, aiming to mitigate the likelihood of hallucinatory visual explanations by enriching the model's exposure to varied visual scenarios. Our empirical results show that while the isolated use of transformed images initially degrades performance, strategic implementation of these transformations can indeed serve as effective complements. Notably, our method is compatible with current contrastive decoding methods and does not require external models or costly self-feedback mechanisms, making it a practical addition. In experiments, RITUAL significantly outperforms existing contrastive decoding methods across several object hallucination benchmarks, including POPE, CHAIR, and MME.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Project page: https://sangminwoo.github.io/RITUAL/"
    },
    {
        "paper id": "2405.16833",
        "abstract url": "https://arxiv.org/abs/2405.16833",
        "title": "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters. Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose Safe LoRA, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that Safe LoRA is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model. Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data, Safe LoRA mitigates the negative effect made by malicious data while preserving performance on downstream tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16964",
        "abstract url": "https://arxiv.org/abs/2405.16964",
        "title": "Exploring the LLM Journey from Cognition to Expression with Linear Representations",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "This paper presents an in-depth examination of the evolution and interplay of cognitive and expressive capabilities in large language models (LLMs), with a specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese and English) LLM series. We define and explore the model's cognitive and expressive capabilities through linear representations across three critical phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Cognitive capability is defined as the quantity and quality of information conveyed by the neuron output vectors within the network, similar to the neural signal processing in human cognition. Expressive capability is defined as the model's capability to produce word-level output. Our findings unveil a sequential development pattern, where cognitive abilities are largely established during Pretraining, whereas expressive abilities predominantly advance during SFT and RLHF. Statistical analyses confirm a significant correlation between the two capabilities, suggesting that cognitive capacity may limit expressive potential. The paper also explores the theoretical underpinnings of these divergent developmental trajectories and their connection to the LLMs' architectural design. Moreover, we evaluate various optimization-independent strategies, such as few-shot learning and repeated sampling, which bridge the gap between cognitive and expressive capabilities. This research reveals the potential connection between the hidden space and the output space, contributing valuable insights into the interpretability and controllability of their training processes.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Published in ICML 2024"
    },
    {
        "paper id": "2405.17247",
        "abstract url": "https://arxiv.org/abs/2405.17247",
        "title": "An Introduction to Vision-Language Modeling",
        "rating": "1.5",
        "keywords": [
            [
                "Vision-Language",
                "VLM"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17258",
        "abstract url": "https://arxiv.org/abs/2405.17258",
        "title": "$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning",
        "rating": "1.5",
        "keywords": [
            [
                "Parameter Efficient",
                "PEFT",
                "Efficient Finetuning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17381",
        "abstract url": "https://arxiv.org/abs/2405.17381",
        "title": "Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by ICML 2024. Yiran Zhong is the corresponding author. Code is released at github.com/OpenNLPLab/TransnormerLLM"
    },
    {
        "paper id": "2405.17730",
        "abstract url": "https://arxiv.org/abs/2405.17730",
        "title": "MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Multimodal learning methods with targeted unimodal learning objectives have exhibited their superior efficacy in alleviating the imbalanced multimodal learning problem. However, in this paper, we identify the previously ignored gradient conflict between multimodal and unimodal learning objectives, potentially misleading the unimodal encoder optimization. To well diminish these conflicts, we observe the discrepancy between multimodal loss and unimodal loss, where both gradient magnitude and covariance of the easier-to-learn multimodal loss are smaller than the unimodal one. With this property, we analyze Pareto integration under our multimodal scenario and propose MMPareto algorithm, which could ensure a final gradient with direction that is common to all learning objectives and enhanced magnitude to improve generalization, providing innocent unimodal assistance. Finally, experiments across multiple types of modalities and frameworks with dense cross-modal interaction indicate our superior and extendable method performance. Our method is also expected to facilitate multi-task cases with a clear discrepancy in task difficulty, demonstrating its ideal scalability. The source code and dataset are available at https://github.com/GeWu-Lab/MMPareto_ICML2024.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM"
        ],
        "comment": "Accepted by ICML2024"
    },
    {
        "paper id": "2405.17784",
        "abstract url": "https://arxiv.org/abs/2405.17784",
        "title": "Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation",
        "rating": "1.5",
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Model-Free Reinforcement Learning~(MFRL), leveraging the policy gradient theorem, has demonstrated considerable success in continuous control tasks. However, these approaches are plagued by high gradient variance due to zeroth-order gradient estimation, resulting in suboptimal policies. Conversely, First-Order Model-Based Reinforcement Learning~(FO-MBRL) methods, employing differentiable simulation, provide gradients with reduced variance but are susceptible to sampling error in scenarios involving stiff dynamics, such as physical contact. This paper investigates the source of this error and introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that reduces gradient error by adapting the model-based horizon to avoid stiff dynamics. Empirical findings reveal that AHAC outperforms MFRL baselines, attaining 40\\% more reward across a set of locomotion tasks, and efficiently scaling to high-dimensional control environments with improved wall-clock-time efficiency.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Website https://adaptive-horizon-actor-critic.github.io/"
    },
    {
        "paper id": "2405.16834",
        "abstract url": "https://arxiv.org/abs/2405.16834",
        "title": "Speech enhancement deep-learning architecture for efficient edge processing",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Deep learning has become a de facto method of choice for speech enhancement tasks with significant improvements in speech quality. However, real-time processing with reduced size and computations for low-power edge devices drastically degrades speech quality. Recently, transformer-based architectures have greatly reduced the memory requirements and provided ways to improve the model performance through local and global contexts. However, the transformer operations remain computationally heavy. In this work, we introduce WaveUNet squeeze-excitation Res2 (WSR)-based metric generative adversarial network (WSR-MGAN) architecture that can be efficiently implemented on low-power edge devices for noise suppression tasks while maintaining speech quality. We utilize multi-scale features using Res2Net blocks that can be related to spectral content used in speech-processing tasks. In the generator, we integrate squeeze-excitation blocks (SEB) with multi-scale features for maintaining local and global contexts along with gated recurrent units (GRUs). The proposed approach is optimized through a combined loss function calculated over raw waveform, multi-resolution magnitude spectrogram, and objective metrics using a metric discriminator. Experimental results in terms of various objective metrics on VoiceBank+DEMAND and DNS-2020 challenge datasets demonstrate that the proposed speech enhancement (SE) approach outperforms the baselines and achieves state-of-the-art (SOTA) performance in the time domain.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16845",
        "abstract url": "https://arxiv.org/abs/2405.16845",
        "title": "On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Autoregressively trained transformers have brought a profound revolution to the world, especially with their in-context learning (ICL) ability to address downstream tasks. Recently, several studies suggest that transformers learn a mesa-optimizer during autoregressive (AR) pretraining to implement ICL. Namely, the forward pass of the trained transformer is equivalent to optimizing an inner objective function in-context. However, whether the practical non-convex training dynamics will converge to the ideal mesa-optimizer is still unclear. Towards filling this gap, we investigate the non-convex dynamics of a one-layer linear causal self-attention model autoregressively trained by gradient flow, where the sequences are generated by an AR process $x_{t+1} = W x_t$. First, under a certain condition of data distribution, we prove that an autoregressively trained transformer learns $W$ by implementing one step of gradient descent to minimize an ordinary least squares (OLS) problem in-context. It then applies the learned $\\widehat{W}$ for next-token prediction, thereby verifying the mesa-optimization hypothesis. Next, under the same data conditions, we explore the capability limitations of the obtained mesa-optimizer. We show that a stronger assumption related to the moments of data is the sufficient and necessary condition that the learned mesa-optimizer recovers the distribution. Besides, we conduct exploratory analyses beyond the first data condition and prove that generally, the trained transformer will not perform vanilla gradient descent for the OLS problem. Finally, our simulation results verify the theoretical results.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "comment": "37pages"
    },
    {
        "paper id": "2405.16847",
        "abstract url": "https://arxiv.org/abs/2405.16847",
        "title": "TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Autoregressive next-token prediction is a standard pretraining method for large-scale language models, but its application to vision tasks is hindered by the non-sequential nature of image data, leading to cumulative errors. Most vision models employ masked autoencoder (MAE) based pretraining, which faces scalability issues. To address these challenges, we introduce \\textbf{TokenUnify}, a novel pretraining method that integrates random token prediction, next-token prediction, and next-all token prediction. We provide theoretical evidence demonstrating that TokenUnify mitigates cumulative errors in visual autoregression. Cooperated with TokenUnify, we have assembled a large-scale electron microscopy (EM) image dataset with ultra-high resolution, ideal for creating spatially correlated long sequences. This dataset includes over 120 million annotated voxels, making it the largest neuron segmentation dataset to date and providing a unified benchmark for experimental validation. Leveraging the Mamba network inherently suited for long-sequence modeling on this dataset, TokenUnify not only reduces the computational complexity but also leads to a significant 45\\% improvement in segmentation performance on downstream EM neuron segmentation tasks compared to existing methods. Furthermore, TokenUnify demonstrates superior scalability over MAE and traditional autoregressive methods, effectively bridging the gap between pretraining strategies for language and vision models. Code is available at \\url{https://github.com/ydchen0806/TokenUnify}.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16856",
        "abstract url": "https://arxiv.org/abs/2405.16856",
        "title": "Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The study explores mitigating overconfidence bias in LLMs to improve their reliability. We introduce a knowledge transfer (KT) method utilizing chain of thoughts, where \"big\" LLMs impart knowledge to \"small\" LLMs via detailed, sequential reasoning paths. This method uses advanced reasoning of larger models to fine-tune smaller models, enabling them to produce more accurate predictions with calibrated confidence. Experimental evaluation using multiple-choice questions and sentiment analysis across diverse datasets demonstrated the KT method's superiority over the vanilla and question-answer pair (QA) fine-tuning methods. The most significant improvement in three key metrics, where the KT method outperformed the vanilla and QA methods by an average of 55.3% and 43.1%, respectively. These findings underscore the KT method's potential in enhancing model trustworthiness and accuracy, offering precise outputs with well-matched confidence levels across various contexts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16884",
        "abstract url": "https://arxiv.org/abs/2405.16884",
        "title": "Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Entity matching (EM) is a critical step in entity resolution. Recently, entity matching based on large language models (LLMs) has shown great promise. However, current LLM-based entity matching approaches typically follow a binary matching paradigm that ignores the global consistency between different records. In this paper, we investigate various methodologies for LLM-based entity matching that incorporate record interactions from different perspectives. Specifically, we comprehensively compare three representative strategies: matching, comparing, and selecting, and analyze their respective advantages and challenges in diverse scenarios. Based on our findings, we further design a compositional entity matching (ComEM) framework that leverages the composition of multiple strategies and LLMs. In this way, ComEM can benefit from the advantages of different sides and achieve improvements in both effectiveness and efficiency. Experimental results show that ComEM not only achieves significant performance gains on various datasets but also reduces the cost of LLM-based entity matching in real-world application.",
        "subjects": [
            "cs.CL",
            "cs.DB"
        ],
        "comment": "Under revision. Code is available at https://github.com/tshu-w/LLM4EM"
    },
    {
        "paper id": "2405.16886",
        "abstract url": "https://arxiv.org/abs/2405.16886",
        "title": "Hawk: Learning to Understand Open-World Video Anomalies",
        "rating": "1",
        "keywords": [
            [
                "Visual Language",
                "VLM"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios. In this paper, we introduce Hawk, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, Hawk explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that Hawk achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16908",
        "abstract url": "https://arxiv.org/abs/2405.16908",
        "title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., \"I'm not sure, but I think...\"). We formalize faithful response uncertainty based on the gap between the model's intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully communicating uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16919",
        "abstract url": "https://arxiv.org/abs/2405.16919",
        "title": "VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "While large multi-modal models (LMMs) have exhibited impressive capabilities across diverse tasks, their effectiveness in handling complex tasks has been limited by the prevailing single-step reasoning paradigm. To this end, this paper proposes VoCoT, a multi-step Visually grounded object-centric Chain-of-Thought reasoning framework tailored for inference with LMMs. VoCoT is characterized by two key features: (1) object-centric reasoning paths that revolve around cross-modal shared object-level information, and (2) visually grounded representation of object concepts in a multi-modal interleaved and aligned manner, which effectively bridges the modality gap within LMMs during long-term generation. Additionally, we construct an instruction dataset to facilitate LMMs in adapting to reasoning with VoCoT. By introducing VoCoT into the prevalent open-source LMM architecture, we introduce VolCano. With only 7B parameters and limited input resolution, VolCano demonstrates excellent performance across various scenarios, surpassing SOTA models, including GPT-4V, in tasks requiring complex reasoning. Our code, data and model will be available at https://github.com/RupertLuo/VoCoT.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16930",
        "abstract url": "https://arxiv.org/abs/2405.16930",
        "title": "From Obstacle to Opportunity: Enhancing Semi-supervised Learning with Synthetic Data",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semi-supervised learning (SSL) can utilize unlabeled data to enhance model performance. In recent years, with increasingly powerful generative models becoming available, a large number of synthetic images have been uploaded to public image sets. Therefore, when collecting unlabeled data from these sources, the inclusion of synthetic images is inevitable. This prompts us to consider the impact of unlabeled data mixed with real and synthetic images on SSL. In this paper, we set up a new task, Real and Synthetic hybrid SSL (RS-SSL), to investigate this problem. We discover that current SSL methods are unable to fully utilize synthetic data and are sometimes negatively affected. Then, by analyzing the issues caused by synthetic images, we propose a new SSL method, RSMatch, to tackle the RS-SSL problem. Extensive experimental results show that RSMatch can better utilize the synthetic data in unlabeled images to improve the SSL performance. The effectiveness is further verified through ablation studies and visualization.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16953",
        "abstract url": "https://arxiv.org/abs/2405.16953",
        "title": "Evaluation of Resource-Efficient Crater Detectors on Embedded Systems",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Real-time analysis of Martian craters is crucial for mission-critical operations, including safe landings and geological exploration. This work leverages the latest breakthroughs for on-the-edge crater detection aboard spacecraft. We rigorously benchmark several YOLO networks using a Mars craters dataset, analyzing their performance on embedded systems with a focus on optimization for low-power devices. We optimize this process for a new wave of cost-effective, commercial-off-the-shelf-based smaller satellites. Implementations on diverse platforms, including Google Coral Edge TPU, AMD Versal SoC VCK190, Nvidia Jetson Nano and Jetson AGX Orin, undergo a detailed trade-off analysis. Our findings identify optimal network-device pairings, enhancing the feasibility of crater detection on resource-constrained hardware and setting a new precedent for efficient and resilient extraterrestrial imaging. Code at: https://github.com/billpsomas/mars_crater_detection.",
        "subjects": [
            "cs.CV",
            "cs.DC",
            "cs.PF"
        ],
        "comment": "Accepted at 2024 IEEE International Geoscience and Remote Sensing Symposium"
    },
    {
        "paper id": "2405.16961",
        "abstract url": "https://arxiv.org/abs/2405.16961",
        "title": "Blind Data Adaptation to tackle Covariate Shift in Operational Steganalysis",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "eess.IV"
            ]
        ],
        "abstract": "The proliferation of image manipulation for unethical purposes poses significant challenges in social networks. One particularly concerning method is Image Steganography, allowing individuals to hide illegal information in digital images without arousing suspicions. Such a technique pose severe security risks, making it crucial to develop effective steganalysis methods enabling to detect manipulated images for clandestine communications. Although significant advancements have been achieved with machine learning models, a critical issue remains: the disparity between the controlled datasets used to train steganalysis models against real-world datasets of forensic practitioners, undermining severely the practical effectiveness of standardized steganalysis models. In this paper, we address this issue focusing on a realistic scenario where practitioners lack crucial information about the limited target set of images under analysis, including details about their development process and even whereas it contains manipulated images or not. By leveraging geometric alignment and distribution matching of source and target residuals, we develop TADA (Target Alignment through Data Adaptation), a novel methodology enabling to emulate sources aligned with specific targets in steganalysis, which is also relevant for highly unbalanced targets. The emulator is represented by a light convolutional network trained to align distributions of image residuals. Experimental validation demonstrates the potential of our strategy over traditional methods fighting covariate shift in steganalysis.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16969",
        "abstract url": "https://arxiv.org/abs/2405.16969",
        "title": "The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology. The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before. This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.",
        "subjects": [
            "cs.CL",
            "stat.AP"
        ],
        "comment": "working paper, 20 pages"
    },
    {
        "paper id": "2405.16980",
        "abstract url": "https://arxiv.org/abs/2405.16980",
        "title": "DSU-Net: Dynamic Snake U-Net for 2-D Seismic First Break Picking",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In seismic exploration, identifying the first break (FB) is a critical component in establishing subsurface velocity models. Various automatic picking techniques based on deep neural networks have been developed to expedite this procedure. The most popular class is using semantic segmentation networks to pick on a shot gather called 2-dimensional (2-D) picking. Generally, 2-D segmentation-based picking methods input an image of a shot gather, and output a binary segmentation map, in which the maximum of each column is the location of FB. However, current designed segmentation networks is difficult to ensure the horizontal continuity of the segmentation. Additionally, FB jumps also exist in some areas, and it is not easy for current networks to detect such jumps. Therefore, it is important to pick as much as possible and ensure horizontal continuity. To alleviate this problem, we propose a novel semantic segmentation network for the 2-D seismic FB picking task, where we introduce the dynamic snake convolution into U-Net and call the new segmentation network dynamic-snake U-Net (DSU-Net). Specifically, we develop original dynamic-snake convolution (DSConv) in CV and propose a novel DSConv module, which can extract the horizontal continuous feature in the shallow feature of the shot gather. Many experiments have shown that DSU-Net demonstrates higher accuracy and robustness than the other 2-D segmentation-based models, achieving state-of-the-art (SOTA) performance in 2-D seismic field surveys. Particularly, it can effectively detect FB jumps and better ensure the horizontal continuity of FB. In addition, the ablation experiment and the anti-noise experiment, respectively, verify the optimal structure of the DSConv module and the robustness of the picking.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16996",
        "abstract url": "https://arxiv.org/abs/2405.16996",
        "title": "Mitigating Noisy Correspondence by Geometrical Structure Consistency Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Noisy correspondence that refers to mismatches in cross-modal data pairs, is prevalent on human-annotated or web-crawled datasets. Prior approaches to leverage such data mainly consider the application of uni-modal noisy label learning without amending the impact on both cross-modal and intra-modal geometrical structures in multimodal learning. Actually, we find that both structures are effective to discriminate noisy correspondence through structural differences when being well-established. Inspired by this observation, we introduce a Geometrical Structure Consistency (GSC) method to infer the true correspondence. Specifically, GSC ensures the preservation of geometrical structures within and between modalities, allowing for the accurate discrimination of noisy samples based on structural differences. Utilizing these inferred true correspondence labels, GSC refines the learning of geometrical structures by filtering out the noisy samples. Experiments across four cross-modal datasets confirm that GSC effectively identifies noisy samples and significantly outperforms the current leading methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 5 figures, received by IEEE/CVF Computer Science and Pattern Recognition"
    },
    {
        "paper id": "2405.17009",
        "abstract url": "https://arxiv.org/abs/2405.17009",
        "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
        "rating": "1",
        "keywords": [
            [
                "cs.AI"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "17 pages, camera-ready version of ICML 2024"
    },
    {
        "paper id": "2405.17022",
        "abstract url": "https://arxiv.org/abs/2405.17022",
        "title": "Compositional Few-Shot Class-Incremental Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Few-shot class-incremental learning (FSCIL) is proposed to continually learn from novel classes with only a few samples after the (pre-)training on base classes with sufficient data. However, this remains a challenge. In contrast, humans can easily recognize novel classes with a few samples. Cognitive science demonstrates that an important component of such human capability is compositional learning. This involves identifying visual primitives from learned knowledge and then composing new concepts using these transferred primitives, making incremental learning both effective and interpretable. To imitate human compositional learning, we propose a cognitive-inspired method for the FSCIL task. We define and build a compositional model based on set similarities, and then equip it with a primitive composition module and a primitive reuse module. In the primitive composition module, we propose to utilize the Centered Kernel Alignment (CKA) similarity to approximate the similarity between primitive sets, allowing the training and evaluation based on primitive compositions. In the primitive reuse module, we enhance primitive reusability by classifying inputs based on primitives replaced with the closest primitives from other classes. Experiments on three datasets validate our method, showing it outperforms current state-of-the-art methods with improved interpretability. Our code is available at https://github.com/Zoilsen/Comp-FSCIL.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17029",
        "abstract url": "https://arxiv.org/abs/2405.17029",
        "title": "Multi-view Disparity Estimation Using a Novel Gradient Consistency Model",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Variational approaches to disparity estimation typically use a linearised brightness constancy constraint, which only applies in smooth regions and over small distances. Accordingly, current variational approaches rely on a schedule to progressively include image data. This paper proposes the use of Gradient Consistency information to assess the validity of the linearisation; this information is used to determine the weights applied to the data term as part of an analytically inspired Gradient Consistency Model. The Gradient Consistency Model penalises the data term for view pairs that have a mismatch between the spatial gradients in the source view and the spatial gradients in the target view. Instead of relying on a tuned or learned schedule, the Gradient Consistency Model is self-scheduling, since the weights evolve as the algorithm progresses. We show that the Gradient Consistency Model outperforms standard coarse-to-fine schemes and the recently proposed progressive inclusion of views approach in both rate of convergence and accuracy.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "11 pages, 11 figures. Submitted to Transactions on Image Processing"
    },
    {
        "paper id": "2405.17039",
        "abstract url": "https://arxiv.org/abs/2405.17039",
        "title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively. In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs. In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task. This model has three components: a language world model, an inverse dynamics model, and a cognitive policy. Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token. The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs. With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters). Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears. This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling. Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity. On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17052",
        "abstract url": "https://arxiv.org/abs/2405.17052",
        "title": "SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Long prompt leads to huge hardware costs when using Large Language Models (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long task-inputs, and the wide application of in-context learning easily makes the prompt length explode. Inspired by the language understanding ability of LLMs, this paper proposes SelfCP, which uses the LLM \\textbf{itself} to \\textbf{C}ompress long \\textbf{P}rompt into compact virtual tokens. SelfCP applies a general frozen LLM twice, first as an encoder to compress the prompt and then as a decoder to generate responses. Specifically, given a long prompt, we place special tokens within the lengthy segment for compression and signal the LLM to generate $k$ virtual tokens. Afterward, the virtual tokens concatenate with the uncompressed prompt and are fed into the same LLM to generate the response. In general, SelfCP facilitates the unconditional and conditional compression of prompts, fitting both standard tasks and those with specific objectives. Since the encoder and decoder are frozen, SelfCP only contains 17M trainable parameters and allows for convenient adaptation across various backbones. We implement SelfCP with two LLM backbones and evaluate it in both in- and out-domain tasks. Results show that the compressed virtual tokens can substitute $12 \\times$ larger original prompts effectively",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17057",
        "abstract url": "https://arxiv.org/abs/2405.17057",
        "title": "ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Code generation plays a crucial role in various tasks, such as code auto-completion and mathematical reasoning. Previous work has proposed numerous methods to enhance code generation performance, including integrating feedback from the compiler. Inspired by this, we present ReflectionCoder, a novel approach that effectively leverages reflection sequences constructed by integrating compiler feedback to improve one-off code generation performance. Furthermore, we propose reflection self-distillation and dynamically masked distillation to effectively utilize these reflection sequences. Extensive experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E, demonstrate that models fine-tuned with our method achieve state-of-the-art performance. Notably, ReflectionCoder-DeepSeek-Coder-33B reaches pass@1 of 82.9 (76.8) on HumanEval (+) and 84.1 (72.0) on MBPP (+), on par with GPT-3.5-Turbo and Claude-3-opus, and surpasses early GPT-4. Beyond the code domain, we believe this approach can benefit other domains that focus on final results and require long reasoning paths. Code and data are available at https://github.com/SenseLLM/ReflectionCoder.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17067",
        "abstract url": "https://arxiv.org/abs/2405.17067",
        "title": "Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in language understanding and generation. Nonetheless, it was also witnessed that LLMs tend to produce inaccurate responses to specific queries. This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs. In fact, incorrect tokenization is the critical point that hinders LLMs in understanding the input precisely, thus leading to unsatisfactory output. To demonstrate this flaw of LLMs, we construct an adversarial dataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs to challenge LLMs' tokenization. ADT consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto. Our empirical results reveal that our ADT is highly effective on challenging the tokenization of leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max and so on, thus degrading these LLMs' capabilities. Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs. To the best of our knowledge, our study is the first to investigating LLMs' vulnerability in terms of challenging their token segmentation, which will shed light on the subsequent research of improving LLMs' capabilities through optimizing their tokenization process and algorithms.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "17 pages, 3 figures, this paper is submitted to neurips 2024"
    },
    {
        "paper id": "2405.17076",
        "abstract url": "https://arxiv.org/abs/2405.17076",
        "title": "Leveraging small language models for Text2SPARQL tasks to improve the resilience of AI assistance",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In this work we will show that language models with less than one billion parameters can be used to translate natural language to SPARQL queries after fine-tuning. Using three different datasets ranging from academic to real world, we identify prerequisites that the training data must fulfill in order for the training to be successful. The goal is to empower users of semantic web technology to use AI assistance with affordable commodity hardware, making them more resilient against external factors.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.IR"
        ],
        "comment": "To appear in Proceedings of the Workshop on Linked Data-driven Resilience Research 2024 (D2R2) co-located with Extended Semantic Web Conference 2024 (ESWC 2024)"
    },
    {
        "paper id": "2405.17103",
        "abstract url": "https://arxiv.org/abs/2405.17103",
        "title": "Empowering Character-level Text Infilling by Eliminating Sub-Tokens",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In infilling tasks, sub-tokens, representing instances where a complete token is segmented into two parts, often emerge at the boundaries of prefixes, middles, and suffixes. Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage. Alternately, some approaches considered character-level infilling, but they relied on predicting sub-tokens in inference, yet this strategy diminished ability in character-level infilling tasks due to the large perplexity of the model on sub-tokens. In this paper, we introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and Ending character constraints. The proposed method addresses character-level infilling tasks by utilizing a line-level format to avoid predicting any sub-token in inference. In addition, we incorporate two special tokens to signify the rest of the incomplete lines, thereby enhancing generation guidance. Extensive experiments demonstrate that our proposed approach surpasses previous methods, offering a significant advantage. Code is available at https://github.com/SenseLLM/FIM-SE.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted to ACL 2024 (main conference)"
    },
    {
        "paper id": "2405.17104",
        "abstract url": "https://arxiv.org/abs/2405.17104",
        "title": "LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Visual grounding is an essential tool that links user-provided text queries with query-specific regions within an image. Despite advancements in visual grounding models, their ability to comprehend complex queries remains limited. To overcome this limitation, we introduce LLM-Optic, an innovative method that utilizes Large Language Models (LLMs) as an optical lens to enhance existing visual grounding models in comprehending complex text queries involving intricate text structures, multiple objects, or object spatial relationships, situations that current models struggle with. LLM-Optic first employs an LLM as a Text Grounder to interpret complex text queries and accurately identify objects the user intends to locate. Then a pre-trained visual grounding model is used to generate candidate bounding boxes given the refined query by the Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes with numerical marks to establish a connection between text and specific image regions, thereby linking two distinct modalities. Finally, it employs a Large Multimodal Model (LMM) as a Visual Grounder to select the marked candidate objects that best correspond to the original text query. Through LLM-Optic, we have achieved universal visual grounding, which allows for the detection of arbitrary objects specified by arbitrary human language input. Importantly, our method achieves this enhancement without requiring additional training or fine-tuning. Extensive experiments across various challenging benchmarks demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding capabilities. Project Page: https://haoyu-zhao.github.io/LLM-Optic.github.io/.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Project Page: https://haoyu-zhao.github.io/LLM-Optic.github.io/"
    },
    {
        "paper id": "2405.17116",
        "abstract url": "https://arxiv.org/abs/2405.17116",
        "title": "Mixtures of Unsupervised Lexicon Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents a mixture version of the method-of-moment unsupervised lexicon classification by an incorporation of a Dirichlet process.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "A draft on lexicon classification unsupervised learning. It shows that aggregating lexicon scores is equivalent to a finite mixture of multinomial Naive Bayes models. A very preliminary work of a few days man-hours, like a weekly report/note, but might be useful"
    },
    {
        "paper id": "2405.17129",
        "abstract url": "https://arxiv.org/abs/2405.17129",
        "title": "TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task. Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems. We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques. Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that LLM-based approaches provided good performance on multilingual emotion detection. Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "(Under review) Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis"
    },
    {
        "paper id": "2405.17130",
        "abstract url": "https://arxiv.org/abs/2405.17130",
        "title": "Exploiting the Layered Intrinsic Dimensionality of Deep Models for Practical Adversarial Training",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Despite being a heavily researched topic, Adversarial Training (AT) is rarely, if ever, deployed in practical AI systems for two primary reasons: (i) the gained robustness is frequently accompanied by a drop in generalization and (ii) generating adversarial examples (AEs) is computationally prohibitively expensive. To address these limitations, we propose SMAAT, a new AT algorithm that leverages the manifold conjecture, stating that off-manifold AEs lead to better robustness while on-manifold AEs result in better generalization. Specifically, SMAAT aims at generating a higher proportion of off-manifold AEs by perturbing the intermediate deepnet layer with the lowest intrinsic dimension. This systematically results in better scalability compared to classical AT as it reduces the PGD chains length required for generating the AEs. Additionally, our study provides, to the best of our knowledge, the first explanation for the difference in the generalization and robustness trends between vision and language models, ie., AT results in a drop in generalization in vision models whereas, in encoder-based language models, generalization either improves or remains unchanged. We show that vision transformers and decoder-based models tend to have low intrinsic dimensionality in the earlier layers of the network (more off-manifold AEs), while encoder-based models have low intrinsic dimensionality in the later layers. We demonstrate the efficacy of SMAAT; on several tasks, including robustifying (i) sentiment classifiers, (ii) safety filters in decoder-based models, and (iii) retrievers in RAG setups. SMAAT requires only 25-33% of the GPU time compared to standard AT, while significantly improving robustness across all applications and maintaining comparable generalization.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17137",
        "abstract url": "https://arxiv.org/abs/2405.17137",
        "title": "Jump-teaching: Ultra Efficient and Robust Learning with Noisy Label",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Sample selection is the most straightforward technique to combat label noise, aiming to distinguish mislabeled samples during training and avoid the degradation of the robustness of the model. In the workflow, $\\textit{selecting possibly clean data}$ and $\\textit{model update}$ are iterative. However, their interplay and intrinsic characteristics hinder the robustness and efficiency of learning with noisy labels: 1) The model chooses clean data with selection bias, leading to the accumulated error in the model update. 2) Most selection strategies leverage partner networks or supplementary information to mitigate label corruption, albeit with increased computation resources and lower throughput speed. Therefore, we employ only one network with the jump manner update to decouple the interplay and mine more semantic information from the loss for a more precise selection. Specifically, the selection of clean data for each model update is based on one of the prior models, excluding the last iteration. The strategy of model update exhibits a jump behavior in the form. Moreover, we map the outputs of the network and labels into the same semantic feature space, respectively. In this space, a detailed and simple loss distribution is generated to distinguish clean samples more effectively. Our proposed approach achieves almost up to $2.53\\times$ speedup, $0.46\\times$ peak memory footprint, and superior robustness over state-of-the-art works with various noise settings.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17139",
        "abstract url": "https://arxiv.org/abs/2405.17139",
        "title": "Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning. Various architectures, from vision transformers (ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks. This paper explores the differences across various CLIP-trained vision backbones. Despite using the same data and training objective, we find that these architectures have notably different representations, different classification performance across datasets, and different robustness properties to certain types of image perturbations. Our findings indicate a remarkable possible synergy across backbones by leveraging their respective strengths. In principle, classification accuracy could be improved by over 40 percentage with an informed selection of the optimal backbone per test example.Using this insight, we develop a straightforward yet powerful approach to adaptively ensemble multiple backbones. The approach uses as few as one labeled example per class to tune the adaptive combination of backbones. On a large collection of datasets, the method achieves a remarkable increase in accuracy of up to 39.1% over the best single backbone, well beyond traditional ensembles",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2312.14400"
    },
    {
        "paper id": "2405.17146",
        "abstract url": "https://arxiv.org/abs/2405.17146",
        "title": "Compressed-Language Models for Understanding Compressed File Formats: a JPEG Exploration",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This study investigates whether Compressed-Language Models (CLMs), i.e. language models operating on raw byte streams from Compressed File Formats~(CFFs), can understand files compressed by CFFs. We focus on the JPEG format as a representative CFF, given its commonality and its representativeness of key concepts in compression, such as entropy coding and run-length encoding. We test if CLMs understand the JPEG format by probing their capabilities to perform along three axes: recognition of inherent file properties, handling of files with anomalies, and generation of new files. Our findings demonstrate that CLMs can effectively perform these tasks. These results suggest that CLMs can understand the semantics of compressed data when directly operating on the byte streams of files produced by CFFs. The possibility to directly operate on raw compressed files offers the promise to leverage some of their remarkable characteristics, such as their ubiquity, compactness, multi-modality and segment-nature.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17156",
        "abstract url": "https://arxiv.org/abs/2405.17156",
        "title": "The Scaling Law in Stellar Light Curves",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Analyzing time series of fluxes from stars, known as stellar light curves, can reveal valuable information about stellar properties. However, most current methods rely on extracting summary statistics, and studies using deep learning have been limited to supervised approaches. In this research, we investigate the scaling law properties that emerge when learning from astronomical time series data using self-supervised techniques. By employing the GPT-2 architecture, we show the learned representation improves as the number of parameters increases from $10^4$ to $10^9$, with no signs of performance plateauing. We demonstrate that a self-supervised Transformer model achieves 3-10 times the sample efficiency compared to the state-of-the-art supervised learning model when inferring the surface gravity of stars as a downstream task. Our research lays the groundwork for analyzing stellar light curves by examining them through large-scale auto-regressive generative models.",
        "subjects": [
            "astro-ph.IM",
            "astro-ph.SR",
            "cs.LG"
        ],
        "comment": "11 pages, 5 figures, submitted to ICML 2024 AI4Science workshop"
    },
    {
        "paper id": "2405.17159",
        "abstract url": "https://arxiv.org/abs/2405.17159",
        "title": "Stop! In the Name of Flaws: Disentangling Personal Names and Sociodemographic Attributes in NLP",
        "rating": "1",
        "keywords": [
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Personal names simultaneously differentiate individuals and categorize them in ways that are important in a given society. While the natural language processing community has thus associated personal names with sociodemographic characteristics in a variety of tasks, researchers have engaged to varying degrees with the established methodological problems in doing so. To guide future work, we present an interdisciplinary background on names and naming. We then survey the issues inherent to associating names with sociodemographic attributes, covering problems of validity (e.g., systematic error, construct validity), as well as ethical concerns (e.g., harms, differential impact, cultural insensitivity). Finally, we provide guiding questions along with normative recommendations to avoid validity and ethical pitfalls when dealing with names and sociodemographic characteristics in natural language processing.",
        "subjects": [
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17188",
        "abstract url": "https://arxiv.org/abs/2405.17188",
        "title": "The SkatingVerse Workshop & Challenge: Methods and Results",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The SkatingVerse Workshop & Challenge aims to encourage research in developing novel and accurate methods for human action understanding. The SkatingVerse dataset used for the SkatingVerse Challenge has been publicly released. There are two subsets in the dataset, i.e., the training subset and testing subset. The training subsets consists of 19,993 RGB video sequences, and the testing subsets consists of 8,586 RGB video sequences. Around 10 participating teams from the globe competed in the SkatingVerse Challenge. In this paper, we provide a brief summary of the SkatingVerse Workshop & Challenge including brief introductions to the top three methods. The submission leaderboard will be reopened for researchers that are interested in the human action understanding challenge. The benchmark dataset and other information can be found at: https://skatingverse.github.io/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17202",
        "abstract url": "https://arxiv.org/abs/2405.17202",
        "title": "Efficient multi-prompt evaluation of LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Our code and data can be found at https://github.com/felipemaiapolo/prompt-eval.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17216",
        "abstract url": "https://arxiv.org/abs/2405.17216",
        "title": "Autoformalizing Euclidean Geometry",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Autoformalization involves automatically translating informal math into formal theorems and proofs that are machine-verifiable. Euclidean geometry provides an interesting and controllable domain for studying autoformalization. In this paper, we introduce a neuro-symbolic framework for autoformalizing Euclidean geometry, which combines domain knowledge, SMT solvers, and large language models (LLMs). One challenge in Euclidean geometry is that informal proofs rely on diagrams, leaving gaps in texts that are hard to formalize. To address this issue, we use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model. We also provide automatic semantic evaluation for autoformalized theorem statements. We construct LeanEuclid, an autoformalization benchmark consisting of problems from Euclid's Elements and the UniGeo dataset formalized in the Lean proof assistant. Experiments with GPT-4 and GPT-4V show the capability and limitations of state-of-the-art LLMs on autoformalizing geometry problems. The data and code are available at https://github.com/loganrjmurphy/LeanEuclid.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.LO",
            "stat.ML"
        ],
        "comment": "Accepted to ICML 2024. The first two authors contributed equally"
    },
    {
        "paper id": "2405.17217",
        "abstract url": "https://arxiv.org/abs/2405.17217",
        "title": "Collage is the New Writing: Exploring the Fragmentation of Text and User Interfaces in AI Tools",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This essay proposes and explores the concept of Collage for the design of AI writing tools, transferred from avant-garde literature with four facets: 1) fragmenting text in writing interfaces, 2) juxtaposing voices (content vs command), 3) integrating material from multiple sources (e.g. text suggestions), and 4) shifting from manual writing to editorial and compositional decision-making, such as selecting and arranging snippets. The essay then employs Collage as an analytical lens to analyse the user interface design of recent AI writing tools, and as a constructive lens to inspire new design directions. Finally, a critical perspective relates the concerns that writers historically expressed through literary collage to AI writing tools. In a broad view, this essay explores how literary concepts can help advance design theory around AI writing tools. It encourages creators of future writing tools to engage not only with new technological possibilities, but also with past writing innovations.",
        "subjects": [
            "cs.HC",
            "cs.CL"
        ],
        "comment": "19 pages, 7 figures, 2 tables, ACM DIS 2024"
    },
    {
        "paper id": "2405.17220",
        "abstract url": "https://arxiv.org/abs/2405.17220",
        "title": "RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Learning from feedback reduces the hallucination of multimodal large language models (MLLMs) by aligning them with human preferences. While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention. However, these methods heavily rely on costly proprietary models like GPT-4V, resulting in scalability issues. Moreover, this paradigm essentially distills the proprietary models to provide a temporary solution to quickly bridge the performance gap. As this gap continues to shrink, the community is soon facing the essential challenge of aligning MLLMs using labeler models of comparable capability. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm for super GPT-4V trustworthiness. RLAIF-V maximally exploits the open-source feedback from two perspectives, including high-quality feedback data and online feedback learning algorithm. Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models without sacrificing performance on other tasks. Using a 34B model as labeler, RLAIF-V 7B model reduces object hallucination by 82.9\\% and overall hallucination by 42.1\\%, outperforming the labeler model. Remarkably, RLAIF-V also reveals the self-alignment potential of open-source MLLMs, where a 12B model can learn from the feedback of itself to achieve less than 29.5\\% overall hallucination rate, surpassing GPT-4V (45.9\\%) by a large margin. The results shed light on a promising route to enhance the efficacy of leading-edge MLLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Project Website: https://github.com/RLHF-V/RLAIF-V"
    },
    {
        "paper id": "2405.17260",
        "abstract url": "https://arxiv.org/abs/2405.17260",
        "title": "Accelerating Simulation of Two-Phase Flows with Neural PDE Surrogates",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Simulation is a powerful tool to better understand physical systems, but generally requires computationally expensive numerical methods. Downstream applications of such simulations can become computationally infeasible if they require many forward solves, for example in the case of inverse design with many degrees of freedom. In this work, we investigate and extend neural PDE solvers as a tool to aid in scaling simulations for two-phase flow problems, and simulations of oil expulsion from a pore specifically. We extend existing numerical methods for this problem to a more complex setting involving varying geometries of the domain to generate a challenging dataset. Further, we investigate three prominent neural PDE solver methods, namely the UNet, DRN and U-FNO, and extend them for characteristics of the oil-expulsion problem: (1) spatial conditioning on the geometry; (2) periodicity in the boundary; (3) approximate mass conservation. We scale all methods and benchmark their speed-accuracy trade-off, evaluate qualitative properties, and perform an ablation study. We find that the investigated methods can accurately model the droplet dynamics with up to three orders of magnitude speed-up, that our extensions improve performance over the baselines, and that the introduced varying geometries constitute a significantly more challenging setting over the previously considered oil expulsion problem.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "physics.flu-dyn"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17264",
        "abstract url": "https://arxiv.org/abs/2405.17264",
        "title": "On the Noise Robustness of In-Context Learning for Text Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the \"noisy\" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17267",
        "abstract url": "https://arxiv.org/abs/2405.17267",
        "title": "FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation",
        "rating": "1",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "Federated Learning"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally. In practice, distinct model architectures, varying data distributions, and limited resources across local clients inevitably cause model performance degradation and a slowdown in convergence speed. However, existing FL methods can only solve some of the above heterogeneous challenges and have obvious performance limitations. Notably, a unified framework has not yet been explored to overcome these challenges. Accordingly, we propose FedHPL, a parameter-efficient unified $\\textbf{Fed}$erated learning framework for $\\textbf{H}$eterogeneous settings based on $\\textbf{P}$rompt tuning and $\\textbf{L}$ogit distillation. Specifically, we employ a local prompt tuning scheme that leverages a few learnable visual prompts to efficiently fine-tune the frozen pre-trained foundation model for downstream tasks, thereby accelerating training and improving model performance under limited local resources and data heterogeneity. Moreover, we design a global logit distillation scheme to handle the model heterogeneity and guide the local training. In detail, we leverage logits to implicitly capture local knowledge and design a weighted knowledge aggregation mechanism to generate global client-specific logits. We provide a theoretical guarantee on the generalization error bound for FedHPL. The experiments on various benchmark datasets under diverse settings of models and data demonstrate that our framework outperforms state-of-the-art FL approaches, with less computation overhead and training rounds.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "35 pages"
    },
    {
        "paper id": "2405.17284",
        "abstract url": "https://arxiv.org/abs/2405.17284",
        "title": "An NLP Crosswalk Between the Common Core State Standards and NAEP Item Specifications",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Natural language processing (NLP) is rapidly developing for applications in educational assessment. In this paper, I describe an NLP-based procedure that can be used to support subject matter experts in establishing a crosswalk between item specifications and content standards. This paper extends recent work by proposing and demonstrating the use of multivariate similarity based on embedding vectors for sentences or texts. In particular, a hybrid regression procedure is demonstrated for establishing the match of each content standard to multiple item specifications. The procedure is used to evaluate the match of the Common Core State Standards (CCSS) for mathematics at grade 4 to the corresponding item specifications for the 2026 National Assessment of Educational Progress (NAEP).",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17299",
        "abstract url": "https://arxiv.org/abs/2405.17299",
        "title": "Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Simplicity bias, the propensity of deep models to over-rely on simple features, has been identified as a potential reason for limited out-of-distribution generalization of neural networks (Shah et al., 2020). Despite the important implications, this phenomenon has been theoretically confirmed and characterized only under strong dataset assumptions, such as linear separability (Lyu et al., 2021). In this work, we characterize simplicity bias for general datasets in the context of two-layer neural networks initialized with small weights and trained with gradient flow. Specifically, we prove that in the early training phases, network features cluster around a few directions that do not depend on the size of the hidden layer. Furthermore, for datasets with an XOR-like pattern, we precisely identify the learned features and demonstrate that simplicity bias intensifies during later training stages. These results indicate that features learned in the middle stages of training may be more useful for OOD transfer. We support this hypothesis with experiments on image data.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.OC"
        ],
        "comment": "ICML 2024, camera-ready version"
    },
    {
        "paper id": "2405.17323",
        "abstract url": "https://arxiv.org/abs/2405.17323",
        "title": "Tracking Small Birds by Detection Candidate Region Filtering and Detection History-aware Association",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper focuses on tracking birds that appear small in a panoramic video. When the size of the tracked object is small in the image (small object tracking) and move quickly, object detection and association suffers. To address these problems, we propose Adaptive Slicing Aided Hyper Inference (Adaptive SAHI), which reduces the candidate regions to apply detection, and Detection History-aware Similarity Criterion (DHSC), which accurately associates objects in consecutive frames based on the detection history. Experiments on the NUBird2022 dataset verifies the effectiveness of the proposed method by showing improvements in both accuracy and speed.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "6 pages, 6 figures"
    },
    {
        "paper id": "2405.17337",
        "abstract url": "https://arxiv.org/abs/2405.17337",
        "title": "Cost-efficient Knowledge-based Question Answering with Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge-based question answering (KBQA) is widely used in many scenarios that necessitate domain knowledge. Large language models (LLMs) bring opportunities to KBQA, while their costs are significantly higher and absence of domain-specific knowledge during pre-training. We are motivated to combine LLMs and prior small models on knowledge graphs (KGMs) for both inferential accuracy and cost saving. However, it remains challenging since accuracy and cost are not readily combined in the optimization as two distinct metrics. It is also laborious for model selection since different models excel in diverse knowledge. To this end, we propose Coke, a novel cost-efficient strategy for KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize calls to LLMs within limited budgets. We first formulate the accuracy expectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A context-aware policy is optimized to further distinguish the expert model subject to the question semantics. The overall decision is bounded by the cost regret according to historical expenditure on failures. Extensive experiments showcase the superior performance of Coke, which moves the Pareto frontier with up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on the benchmark datasets.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17345",
        "abstract url": "https://arxiv.org/abs/2405.17345",
        "title": "Exploring and steering the moral compass of Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have become central to advancing automation and decision-making across various sectors, raising significant ethical questions. This study proposes a comprehensive comparative analysis of the most advanced LLMs to assess their moral profiles. We subjected several state-of-the-art models to a selection of ethical dilemmas and found that all the proprietary ones are mostly utilitarian and all of the open-weights ones align mostly with values-based ethics. Furthermore, when using the Moral Foundations Questionnaire, all models we probed - except for Llama 2- displayed a strong liberal bias. Lastly, in order to causally intervene in one of the studied models, we propose a novel similarity-specific activation steering technique. Using this method, we were able to reliably steer the model's moral compass to different ethical schools. All of these results showcase that there is an ethical dimension in already deployed LLMs, an aspect that is generally overlooked.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17358",
        "abstract url": "https://arxiv.org/abs/2405.17358",
        "title": "Rethinking Transformers in Solving POMDPs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Sequential decision-making algorithms such as reinforcement learning (RL) in real-world scenarios inevitably face environments with partial observability. This paper scrutinizes the effectiveness of a popular architecture, namely Transformers, in Partially Observable Markov Decision Processes (POMDPs) and reveals its theoretical limitations. We establish that regular languages, which Transformers struggle to model, are reducible to POMDPs. This poses a significant challenge for Transformers in learning POMDP-specific inductive biases, due to their lack of inherent recurrence found in other models like RNNs. This paper casts doubt on the prevalent belief in Transformers as sequence models for RL and proposes to introduce a point-wise recurrent structure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited alternative for Partially Observable RL, with empirical results highlighting the sub-optimal performance of the Transformer and considerable strength of LRU.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted by ICML 2024; references added; typos fixed"
    },
    {
        "paper id": "2405.17361",
        "abstract url": "https://arxiv.org/abs/2405.17361",
        "title": "A One-Layer Decoder-Only Transformer is a Two-Layer RNN: With an Application to Certified Robustness",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper reveals a key insight that a one-layer decoder-only Transformer is equivalent to a two-layer Recurrent Neural Network (RNN). Building on this insight, we propose ARC-Tran, a novel approach for verifying the robustness of decoder-only Transformers against arbitrary perturbation spaces. Compared to ARC-Tran, current robustness verification techniques are limited either to specific and length-preserving perturbations like word substitutions or to recursive models like LSTMs. ARC-Tran addresses these limitations by meticulously managing position encoding to prevent mismatches and by utilizing our key insight to achieve precise and scalable verification. Our evaluation shows that ARC-Tran (1) trains models more robust to arbitrary perturbation spaces than those produced by existing techniques and (2) shows high certification accuracy of the resulting models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17364",
        "abstract url": "https://arxiv.org/abs/2405.17364",
        "title": "Speech Loudness in Broadcasting and Streaming",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "The introduction and regulation of loudness in broadcasting and streaming brought clear benefits to the audience, e.g., a level of uniformity across programs and channels. Yet, speech loudness is frequently reported as being too low in certain passages, which can hinder the full understanding and enjoyment of movies and TV programs. This paper proposes expanding the set of loudness-based measures typically used in the industry. We focus on speech loudness, and we show that, when clean speech is not available, Deep Neural Networks (DNNs) can be used to isolate the speech signal and so to accurately estimate speech loudness, providing a more precise estimate compared to speech-gated loudness. Moreover, we define critical passages, i.e., passages in which speech is likely to be hard to understand. Critical passages are defined based on the local Speech Loudness Deviation (SLD) and the local Speech-to-Background Loudness Difference (SBLD), as SLD and SBLD significantly contribute to intelligibility and listening effort. In contrast to other more comprehensive measures of intelligibility and listening effort, SLD and SBLD can be straightforwardly measured, are intuitive, and, most importantly, can be easily controlled by adjusting the speech level in the mix or by enabling personalization at the user's end. Finally, examples are provided that show how the detection of critical passages can support the evaluation and control of the speech signal during and after content production.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted for presentation at the Audio Engineering Society (AES) 156th Convention, June 2024, Madrid, Spain"
    },
    {
        "paper id": "2405.17382",
        "abstract url": "https://arxiv.org/abs/2405.17382",
        "title": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make the reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results. Code is available at https://github.com/hyunseoklee-ai/reward_llm_detect.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "20 pages"
    },
    {
        "paper id": "2405.17383",
        "abstract url": "https://arxiv.org/abs/2405.17383",
        "title": "Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present the Linear Complexity Sequence Model (LCSM), a comprehensive solution that unites various sequence modeling techniques with linear complexity, including linear attention, state space model, long convolution, and linear RNN, within a single framework. The goal is to enhance comprehension of these models by analyzing the impact of each component from a cohesive and streamlined viewpoint. Specifically, we segment the modeling processes of these models into three distinct stages: Expand, Oscillation, and Shrink (EOS), with each model having its own specific settings. The Expand stage involves projecting the input signal onto a high-dimensional memory state. This is followed by recursive operations performed on the memory state in the Oscillation stage. Finally, the memory state is projected back to a low-dimensional space in the Shrink stage. We perform comprehensive experiments to analyze the impact of different stage settings on language modeling and retrieval tasks. Our results show that data-driven methods are crucial for the effectiveness of the three stages in language modeling, whereas hand-crafted methods yield better performance in retrieval tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Technical report. Yiran Zhong is the corresponding author"
    },
    {
        "paper id": "2405.17386",
        "abstract url": "https://arxiv.org/abs/2405.17386",
        "title": "MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17390",
        "abstract url": "https://arxiv.org/abs/2405.17390",
        "title": "KSW: Khmer Stop Word based Dictionary for Keyword Extraction",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces KSW, a Khmer-specific approach to keyword extraction that leverages a specialized stop word dictionary. Due to the limited availability of natural language processing resources for the Khmer language, effective keyword extraction has been a significant challenge. KSW addresses this by developing a tailored stop word dictionary and implementing a preprocessing methodology to remove stop words, thereby enhancing the extraction of meaningful keywords. Our experiments demonstrate that KSW achieves substantial improvements in accuracy and relevance compared to previous methods, highlighting its potential to advance Khmer text processing and information retrieval. The KSW resources, including the stop word dictionary, are available at the following GitHub repository: (https://github.com/back-kh/KSWv2-Khmer-Stop-Word-based-Dictionary-for-Keyword-Extraction.git).",
        "subjects": [
            "cs.IR",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17394",
        "abstract url": "https://arxiv.org/abs/2405.17394",
        "title": "The Expressive Capacity of State Space Models: A Formal Language Perspective",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.",
        "subjects": [
            "cs.CL",
            "cs.FL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17402",
        "abstract url": "https://arxiv.org/abs/2405.17402",
        "title": "THREAD: Thinking Deeper with Recursive Spawning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17413",
        "abstract url": "https://arxiv.org/abs/2405.17413",
        "title": "Enhancing Music Genre Classification through Multi-Algorithm Analysis and User-Friendly Visualization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The aim of this study is to teach an algorithm how to recognize different types of music. Users will submit songs for analysis. Since the algorithm hasn't heard these songs before, it needs to figure out what makes each song unique. It does this by breaking down the songs into different parts and studying things like rhythm, melody, and tone via supervised learning because the program learns from examples that are already labelled. One important thing to consider when classifying music is its genre, which can be quite complex. To ensure accuracy, we use five different algorithms, each working independently, to analyze the songs. This helps us get a more complete understanding of each song's characteristics. Therefore, our goal is to correctly identify the genre of each submitted song. Once the analysis is done, the results are presented using a graphing tool, making it easy for users to understand and provide feedback.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17416",
        "abstract url": "https://arxiv.org/abs/2405.17416",
        "title": "A Recipe for Unbounded Data Augmentation in Visual Reinforcement Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "$Q$-learning algorithms are appealing for real-world applications due to their data-efficiency, but they are very prone to overfitting and training instabilities when trained from visual observations. Prior work, namely SVEA, finds that selective application of data augmentation can improve the visual generalization of RL agents without destabilizing training. We revisit its recipe for data augmentation, and find an assumption that limits its effectiveness to augmentations of a photometric nature. Addressing these limitations, we propose a generalized recipe, SADA, that works with wider varieties of augmentations. We benchmark its effectiveness on DMC-GB2 -- our proposed extension of the popular DMControl Generalization Benchmark -- as well as tasks from Meta-World and the Distracting Control Suite, and find that our method, SADA, greatly improves training stability and generalization of RL agents across a diverse set of augmentations. Visualizations, code, and benchmark: see https://aalmuzairee.github.io/SADA/",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Accepted at RLC 2024"
    },
    {
        "paper id": "2405.17418",
        "abstract url": "https://arxiv.org/abs/2405.17418",
        "title": "Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation",
        "rating": "1",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "Robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Robot manipulation policies have shown unsatisfactory action performance when confronted with novel task or object instances. Hence, the capability to automatically detect and self-correct failure action is essential for a practical robotic system. Recently, Multimodal Large Language Models (MLLMs) have shown promise in visual instruction following and demonstrated strong reasoning abilities in various tasks. To unleash general MLLMs as an end-to-end robotic agent, we introduce a Self-Corrected (SC)-MLLM, equipping our model not only to predict end-effector poses but also to autonomously recognize and correct failure actions. Specifically, we first conduct parameter-efficient fine-tuning to empower MLLM with pose prediction ability, which is reframed as a language modeling problem. When facing execution failures, our model learns to identify low-level action error causes (i.e., position and rotation errors) and adaptively seeks prompt feedback from experts. Based on the feedback, SC-MLLM rethinks the current failure scene and generates the corrected actions. Furthermore, we design a continuous policy learning method for successfully corrected samples, enhancing the model's adaptability to the current scene configuration and reducing the frequency of expert intervention. To evaluate our SC-MLLM, we conduct extensive experiments in both simulation and real-world settings. SC-MLLM agent significantly improve manipulation accuracy compared to previous state-of-the-art robotic MLLM (ManipLLM), increasing from 57\\% to 79\\% on seen object categories and from 47\\% to 69\\% on unseen novel categories.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17424",
        "abstract url": "https://arxiv.org/abs/2405.17424",
        "title": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Due to the need to interact with the real world, embodied agents are required to possess comprehensive prior knowledge, long-horizon planning capability, and a swift response speed. Despite recent large language model (LLM) based agents achieving promising performance, they still exhibit several limitations. For instance, the output of LLMs is a descriptive sentence, which is ambiguous when determining specific actions. To address these limitations, we introduce the large auto-regressive model (LARM). LARM leverages both text and multi-view images as input and predicts subsequent actions in an auto-regressive manner. To train LARM, we develop a novel data format named auto-regressive node transmission structure and assemble a corresponding dataset. Adopting a two-phase training regimen, LARM successfully harvests enchanted equipment in Minecraft, which demands significantly more complex decision-making chains than the highest achievements of prior best methods. Besides, the speed of LARM is 6.8x faster.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17428",
        "abstract url": "https://arxiv.org/abs/2405.17428",
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model with a variety of architectural designs and training procedures to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For model training, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval datasets into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. Combining these techniques, our NV-Embed model, using only publicly available data, has achieved a record-high score of 69.32, ranking No. 1 on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval, reranking, classification, clustering, and semantic textual similarity tasks. Notably, our model also attains the highest score of 59.36 on 15 retrieval tasks in the MTEB benchmark (also known as BEIR). We will open-source the model at: https://huggingface.co/nvidia/NV-Embed-v1.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17430",
        "abstract url": "https://arxiv.org/abs/2405.17430",
        "title": "Matryoshka Multimodal Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Project Page: https://matryoshka-mm.github.io/"
    },
    {
        "paper id": "2405.17517",
        "abstract url": "https://arxiv.org/abs/2405.17517",
        "title": "WASH: Train your Ensemble with Communication-Efficient Weight Shuffling, then Average",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The performance of deep neural networks is enhanced by ensemble methods, which average the output of several models. However, this comes at an increased cost at inference. Weight averaging methods aim at balancing the generalization of ensembling and the inference speed of a single model by averaging the parameters of an ensemble of models. Yet, naive averaging results in poor performance as models converge to different loss basins, and aligning the models to improve the performance of the average is challenging. Alternatively, inspired by distributed training, methods like DART and PAPA have been proposed to train several models in parallel such that they will end up in the same basin, resulting in good averaging accuracy. However, these methods either compromise ensembling accuracy or demand significant communication between models during training. In this paper, we introduce WASH, a novel distributed method for training model ensembles for weight averaging that achieves state-of-the-art image classification accuracy. WASH maintains models within the same basin by randomly shuffling a small percentage of weights during training, resulting in diverse models and lower communication costs compared to standard parameter averaging methods.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "cs.NE",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17531",
        "abstract url": "https://arxiv.org/abs/2405.17531",
        "title": "Evolutive Rendering Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The landscape of computer graphics has undergone significant transformations with the recent advances of differentiable rendering models. These rendering models often rely on heuristic designs that may not fully align with the final rendering objectives. We address this gap by pioneering \\textit{evolutive rendering models}, a methodology where rendering models possess the ability to evolve and adapt dynamically throughout the rendering process. In particular, we present a comprehensive learning framework that enables the optimization of three principal rendering elements, including the gauge transformations, the ray sampling mechanisms, and the primitive organization. Central to this framework is the development of differentiable versions of these rendering elements, allowing for effective gradient backpropagation from the final rendering objectives. A detailed analysis of gradient characteristics is performed to facilitate a stable and goal-oriented elements evolution. Our extensive experiments demonstrate the large potential of evolutive rendering models for enhancing the rendering performance across various domains, including static and dynamic scene representations, generative modeling, and texture mapping.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://fnzhan.com/Evolutive-Rendering-Models/"
    },
    {
        "paper id": "2405.17568",
        "abstract url": "https://arxiv.org/abs/2405.17568",
        "title": "ExtremeMETA: High-speed Lightweight Image Segmentation Model by Remodeling Multi-channel Metamaterial Imagers",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep neural networks (DNNs) have heavily relied on traditional computational units like CPUs and GPUs. However, this conventional approach brings significant computational burdens, latency issues, and high power consumption, limiting their effectiveness. This has sparked the need for lightweight networks like ExtremeC3Net. On the other hand, there have been notable advancements in optical computational units, particularly with metamaterials, offering the exciting prospect of energy-efficient neural networks operating at the speed of light. Yet, the digital design of metamaterial neural networks (MNNs) faces challenges such as precision, noise, and bandwidth, limiting their application to intuitive tasks and low-resolution images. In this paper, we propose a large kernel lightweight segmentation model, ExtremeMETA. Based on the ExtremeC3Net, the ExtremeMETA maximizes the ability of the first convolution layer by exploring a larger convolution kernel and multiple processing paths. With the proposed large kernel convolution model, we extend the optic neural network application boundary to the segmentation task. To further lighten the computation burden of the digital processing part, a set of model compression methods is applied to improve model efficiency in the inference stage. The experimental results on three publicly available datasets demonstrate that the optimized efficient design improved segmentation performance from 92.45 to 95.97 on mIoU while reducing computational FLOPs from 461.07 MMacs to 166.03 MMacs. The proposed the large kernel lightweight model ExtremeMETA showcases the hybrid design's ability on complex tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17587",
        "abstract url": "https://arxiv.org/abs/2405.17587",
        "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.",
        "subjects": [
            "cs.IR",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17596",
        "abstract url": "https://arxiv.org/abs/2405.17596",
        "title": "GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary Semantic-space Hyperplane",
        "rating": "1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D open-vocabulary scene understanding, crucial for advancing augmented reality and robotic applications, involves interpreting and locating specific regions within a 3D space as directed by natural language instructions. To this end, we introduce GOI, a framework that integrates semantic features from 2D vision-language foundation models into 3D Gaussian Splatting (3DGS) and identifies 3D Gaussians of Interest using an Optimizable Semantic-space Hyperplane. Our approach includes an efficient compression method that utilizes scene priors to condense noisy high-dimensional semantic features into compact low-dimensional vectors, which are subsequently embedded in 3DGS. During the open-vocabulary querying process, we adopt a distinct approach compared to existing methods, which depend on a manually set fixed empirical threshold to select regions based on their semantic feature distance to the query text embedding. This traditional approach often lacks universal accuracy, leading to challenges in precisely identifying specific target areas. Instead, our method treats the feature selection process as a hyperplane division within the feature space, retaining only those features that are highly relevant to the query. We leverage off-the-shelf 2D Referring Expression Segmentation (RES) models to fine-tune the semantic-space hyperplane, enabling a more precise distinction between target regions and others. This fine-tuning substantially improves the accuracy of open-vocabulary queries, ensuring the precise localization of pertinent 3D Gaussians. Extensive experiments demonstrate GOI's superiority over previous state-of-the-art methods. Our project page is available at https://goi-hyperplane.github.io/ .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Our project page is available at https://goi-hyperplane.github.io/"
    },
    {
        "paper id": "2405.17610",
        "abstract url": "https://arxiv.org/abs/2405.17610",
        "title": "Explainable machine learning multi-label classification of Spanish legal judgements",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Artificial Intelligence techniques such as Machine Learning (ML) have not been exploited to their maximum potential in the legal domain. This has been partially due to the insufficient explanations they provided about their decisions. Automatic expert systems with explanatory capabilities can be specially useful when legal practitioners search jurisprudence to gather contextual knowledge for their cases. Therefore, we propose a hybrid system that applies ML for multi-label classification of judgements (sentences) and visual and natural language descriptions for explanation purposes, boosted by Natural Language Processing techniques and deep legal reasoning to identify the entities, such as the parties, involved. We are not aware of any prior work on automatic multi-label classification of legal judgements also providing natural language explanations to the end-users with comparable overall quality. Our solution achieves over 85 % micro precision on a labelled data set annotated by legal experts. This endorses its interest to relieve human experts from monotonous labour-intensive legal classification tasks.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17615",
        "abstract url": "https://arxiv.org/abs/2405.17615",
        "title": "Listenable Maps for Zero-Shot Audio Classifiers",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Interpreting the decisions of deep learning models, including audio classifiers, is crucial for ensuring the transparency and trustworthiness of this technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Audio Classifiers in the Zero-Shot context), which, to the best of our knowledge, is the first decoder-based post-hoc interpretation method for explaining the decisions of zero-shot audio classifiers. The proposed method utilizes a novel loss function that maximizes the faithfulness to the original similarity between a given text-and-audio pair. We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our interpreter remains faithful to the decisions in a zero-shot classification context. Moreover, we qualitatively show that our method produces meaningful explanations that correlate well with different text prompts.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17633",
        "abstract url": "https://arxiv.org/abs/2405.17633",
        "title": "HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with N=2,624 participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17653",
        "abstract url": "https://arxiv.org/abs/2405.17653",
        "title": "InversionView: A General-Purpose Method for Reading Information from Neural Activations",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present three case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17658",
        "abstract url": "https://arxiv.org/abs/2405.17658",
        "title": "Generative Query Reformulation Using Ensemble Prompting, Document Fusion, and Relevance Feedback",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Query Reformulation (QR) is a set of techniques used to transform a user's original search query to a text that better aligns with the user's intent and improves their search experience. Recently, zero-shot QR has been a promising approach due to its ability to exploit knowledge inherent in large language models. Inspired by the success of ensemble prompting strategies which have benefited other tasks, we investigate if they can improve query reformulation. In this context, we propose two ensemble-based prompting techniques, GenQREnsemble and GenQRFusion which leverage paraphrases of a zero-shot instruction to generate multiple sets of keywords to improve retrieval performance ultimately. We further introduce their post-retrieval variants to incorporate relevance feedback from a variety of sources, including an oracle simulating a human user and a \"critic\" LLM. We demonstrate that an ensemble of query reformulations can improve retrieval effectiveness by up to 18% on nDCG@10 in pre-retrieval settings and 9% on post-retrieval settings on multiple benchmarks, outperforming all previously reported SOTA results. We perform subsequent analyses to investigate the effects of feedback documents, incorporate domain-specific instructions, filter reformulations, and generate fluent reformulations that might be more beneficial to human searchers. Together, the techniques and the results presented in this paper establish a new state of the art in automated query reformulation for retrieval and suggest promising directions for future research.",
        "subjects": [
            "cs.IR",
            "cs.CL"
        ],
        "comment": "Extended Work of GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation, Dhole and Agichtein, ECIR 2024. arXiv admin note: text overlap with arXiv:2404.03746"
    },
    {
        "paper id": "2405.17660",
        "abstract url": "https://arxiv.org/abs/2405.17660",
        "title": "LoReTrack: Efficient and Accurate Low-Resolution Transformer Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "High-performance Transformer trackers have shown excellent results, yet they often bear a heavy computational load. Observing that a smaller input can immediately and conveniently reduce computations without changing the model, an easy solution is to adopt the low-resolution input for efficient Transformer tracking. Albeit faster, this hurts tracking accuracy much due to information loss in low resolution tracking. In this paper, we aim to mitigate such information loss to boost the performance of the low-resolution Transformer tracking via dual knowledge distillation from a frozen high-resolution (but not a larger) Transformer tracker. The core lies in two simple yet effective distillation modules, comprising query-key-value knowledge distillation (QKV-KD) and discrimination knowledge distillation (Disc-KD), across resolutions. The former, from the global view, allows the low-resolution tracker to inherit the features and interactions from the high-resolution tracker, while the later, from the target-aware view, enhances the target-background distinguishing capacity via imitating discriminative regions from its high-resolution counterpart. With the dual knowledge distillation, our Low-Resolution Transformer Tracker (LoReTrack) enjoys not only high efficiency owing to reduced computation but also enhanced accuracy by distilling knowledge from the high-resolution tracker. In extensive experiments, LoReTrack with a 256x256 resolution consistently improves baseline with the same resolution, and shows competitive or even better results compared to 384x384 high-resolution Transformer tracker, while running 52% faster and saving 56% MACs. Moreover, LoReTrack is resolution-scalable. With a 128x128 resolution, it runs 25 fps on a CPU with 64.9%/46.4% SUC scores on LaSOT/LaSOText, surpassing all other CPU real-time trackers. Code will be released.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17663",
        "abstract url": "https://arxiv.org/abs/2405.17663",
        "title": "What's the Opposite of a Face? Finding Shared Decodable Concepts and their Negations in the Brain",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Prior work has offered evidence for functional localization in the brain; different anatomical regions preferentially activate for certain types of visual input. For example, the fusiform face area preferentially activates for visual stimuli that include a face. However, the spectrum of visual semantics is extensive, and only a few semantically-tuned patches of cortex have so far been identified in the human brain. Using a multimodal (natural language and image) neural network architecture (CLIP) we train a highly accurate contrastive model that maps brain responses during naturalistic image viewing to CLIP embeddings. We then use a novel adaptation of the DBSCAN clustering algorithm to cluster the parameters of these participant-specific contrastive models. This reveals what we call Shared Decodable Concepts (SDCs): clusters in CLIP space that are decodable from common sets of voxels across multiple participants. Examining the images most and least associated with each SDC cluster gives us additional insight into the semantic properties of each SDC. We note SDCs for previously reported visual features (e.g. orientation tuning in early visual cortex) as well as visual semantic concepts such as faces, places and bodies. In cases where our method finds multiple clusters for a visuo-semantic concept, the least associated images allow us to dissociate between confounding factors. For example, we discovered two clusters of food images, one driven by color, the other by shape. We also uncover previously unreported areas such as regions of extrastriate body area (EBA) tuned for legs/hands and sensitivity to numerosity in right intraparietal sulcus, and more. Thus, our contrastive-learning methodology better characterizes new and existing visuo-semantic representations in the brain by leveraging multimodal neural network representations and a novel adaptation of clustering algorithms.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17678",
        "abstract url": "https://arxiv.org/abs/2405.17678",
        "title": "TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial Robustness and Generalization Ability",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This work addresses the challenge of achieving zero-shot adversarial robustness while preserving zero-shot generalization in large-scale foundation models, with a focus on the popular Contrastive Language-Image Pre-training (CLIP). Although foundation models were reported to have exceptional zero-shot generalization, they are highly vulnerable to adversarial perturbations. Existing methods achieve a comparable good tradeoff between zero-shot adversarial robustness and generalization under small adversarial perturbations. However, they fail to achieve a good tradeoff under large adversarial perturbations. To this end, we propose a novel Text-Image Mutual Awareness (TIMA) method that strikes a balance between zero-shot adversarial robustness and generalization. More precisely, we propose an Image-Aware Text (IAT) tuning mechanism that increases the inter-class distance of text embeddings by incorporating the Minimum Hyperspherical Energy (MHE). Simultaneously, fixed pre-trained image embeddings are used as cross-modal auxiliary supervision to maintain the similarity between the MHE-tuned and original text embeddings by the knowledge distillation, preserving semantic information between different classes. Besides, we introduce a Text-Aware Image (TAI) tuning mechanism, which increases inter-class distance between image embeddings during the training stage by Text-distance based Adaptive Margin (TAM). Similarly, a knowledge distillation is utilized to retain the similarity between fine-tuned and pre-trained image embeddings. Extensive experimental results demonstrate the effectiveness of our approach, showing impressive zero-shot performance against a wide range of adversarial perturbations while preserving the zero-shot generalization capabilities of the original CLIP model.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17686",
        "abstract url": "https://arxiv.org/abs/2405.17686",
        "title": "Towards Causal Physical Error Discovery in Video Analytics Systems",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video analytics systems based on deep learning models are often opaque and brittle and require explanation systems to help users debug. Current model explanation system are very good at giving literal explanations of behavior in terms of pixel contributions but cannot integrate information about the physical or systems processes that might influence a prediction. This paper introduces the idea that a simple form of causal reasoning, called a regression discontinuity design, can be used to associate changes in multiple key performance indicators to physical real world phenomena to give users a more actionable set of video analytics explanations. We overview the system architecture and describe a vision of the impact that such a system might have.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17703",
        "abstract url": "https://arxiv.org/abs/2405.17703",
        "title": "Mechanistic Interpretability of Binary and Ternary Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent research (arXiv:2310.11453, arXiv:2402.17764) has proposed binary and ternary transformer networks as a way to significantly reduce memory and improve inference speed in Large Language Models (LLMs) while maintaining accuracy. In this work, we apply techniques from mechanistic interpretability to investigate whether such networks learn distinctly different or similar algorithms when compared to full-precision transformer networks. In particular, we reverse engineer the algorithms learned for the toy problem of modular addition where we find that binary and ternary networks learn similar algorithms as full precision networks. This provides evidence against the possibility of using binary and ternary networks as a more interpretable alternative in the LLM setting.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17706",
        "abstract url": "https://arxiv.org/abs/2405.17706",
        "title": "Video Enriched Retrieval Augmented Generation Using Aligned Video Captions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we propose the use of \"aligned visual captions\" as a mechanism for integrating information contained within videos into retrieval augmented generation (RAG) based chat assistant systems. These captions are able to describe the visual and audio content of videos in a large corpus while having the advantage of being in a textual format that is both easy to reason about & incorporate into large language model (LLM) prompts, but also typically require less multimedia content to be inserted into the multimodal LLM context window, where typical configurations can aggressively fill up the context window by sampling video frames from the source video. Furthermore, visual captions can be adapted to specific use cases by prompting the original foundational model / captioner for particular visual details or fine tuning. In hopes of helping advancing progress in this area, we curate a dataset and describe automatic evaluation procedures on common RAG tasks.",
        "subjects": [
            "cs.AI",
            "cs.CV",
            "cs.IR"
        ],
        "comment": "SIGIR 2024 Workshop on Multimodal Representation and Retrieval (MRR 2024)"
    },
    {
        "paper id": "2405.17712",
        "abstract url": "https://arxiv.org/abs/2405.17712",
        "title": "CLAIM Your Data: Enhancing Imputation Accuracy with Contextual Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces the Contextual Language model for Accurate Imputation Method (CLAIM), a novel strategy that capitalizes on the expansive knowledge and reasoning capabilities of pre-trained large language models (LLMs) to address missing data challenges in tabular datasets. Unlike traditional imputation methods, which predominantly rely on numerical estimations, CLAIM utilizes contextually relevant natural language descriptors to fill missing values. This approach transforms datasets into natural language contextualized formats that are inherently more aligned with LLMs' capabilities, thereby facilitating the dual use of LLMs: first, to generate missing value descriptors, and then, to fine-tune the LLM on the enriched dataset for improved performance in downstream tasks. Our evaluations across diverse datasets and missingness patterns reveal CLAIM's superior performance over existing imputation techniques. Furthermore, our investigation into the effectiveness of context-specific versus generic descriptors for missing data highlights the importance of contextual accuracy in enhancing LLM performance for data imputation. The results underscore CLAIM's potential to markedly improve the reliability and quality of data analysis and machine learning models, offering a more nuanced and effective solution for handling missing data.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17713",
        "abstract url": "https://arxiv.org/abs/2405.17713",
        "title": "AI Alignment with Changing and Influenceable Reward Functions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Existing AI alignment approaches assume that preferences are static, which is unrealistic: our preferences change, and may even be influenced by our interactions with AI systems themselves. To clarify the consequences of incorrectly assuming static preferences, we introduce Dynamic Reward Markov Decision Processes (DR-MDPs), which explicitly model preference changes and the AI's influence on them. We show that despite its convenience, the static-preference assumption may undermine the soundness of existing alignment techniques, leading them to implicitly reward AI systems for influencing user preferences in ways users may not truly want. We then explore potential solutions. First, we offer a unifying perspective on how an agent's optimization horizon may partially help reduce undesirable AI influence. Then, we formalize different notions of AI alignment that account for preference change from the outset. Comparing the strengths and limitations of 8 such notions of alignment, we find that they all either err towards causing undesirable AI influence, or are overly risk-averse, suggesting that a straightforward solution to the problems of changing preferences may not exist. As there is no avoiding grappling with changing preferences in real-world settings, this makes it all the more important to handle these issues with care, balancing risks and capabilities. We hope our work can provide conceptual clarity and constitute a first step towards AI alignment practices which explicitly account for (and contend with) the changing and influenceable nature of human preferences.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted to ICML 2024"
    },
    {
        "paper id": "2405.17718",
        "abstract url": "https://arxiv.org/abs/2405.17718",
        "title": "AdapNet: Adaptive Noise-Based Network for Low-Quality Image Retrieval",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Image retrieval aims to identify visually similar images within a database using a given query image. Traditional methods typically employ both global and local features extracted from images for matching, and may also apply re-ranking techniques to enhance accuracy. However, these methods often fail to account for the noise present in query images, which can stem from natural or human-induced factors, thereby negatively impacting retrieval performance. To mitigate this issue, we introduce a novel setting for low-quality image retrieval, and propose an Adaptive Noise-Based Network (AdapNet) to learn robust abstract representations. Specifically, we devise a quality compensation block trained to compensate for various low-quality factors in input images. Besides, we introduce an innovative adaptive noise-based loss function, which dynamically adjusts its focus on the gradient in accordance with image quality, thereby augmenting the learning of unknown noisy samples during training and enhancing intra-class compactness. To assess the performance, we construct two datasets with low-quality queries, which is built by applying various types of noise on clean query images on the standard Revisited Oxford and Revisited Paris datasets. Comprehensive experimental results illustrate that AdapNet surpasses state-of-the-art methods on the Noise Revisited Oxford and Noise Revisited Paris benchmarks, while maintaining competitive performance on high-quality datasets. The code and constructed datasets will be made available.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17732",
        "abstract url": "https://arxiv.org/abs/2405.17732",
        "title": "C$^{3}$Bench: A Comprehensive Classical Chinese Understanding Benchmark for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Classical Chinese Understanding (CCU) holds significant value in preserving and exploration of the outstanding traditional Chinese culture. Recently, researchers have attempted to leverage the potential of Large Language Models (LLMs) for CCU by capitalizing on their remarkable comprehension and semantic capabilities. However, no comprehensive benchmark is available to assess the CCU capabilities of LLMs. To fill this gap, this paper introduces C$^{3}$bench, a Comprehensive Classical Chinese understanding benchmark, which comprises 50,000 text pairs for five primary CCU tasks, including classification, retrieval, named entity recognition, punctuation, and translation. Furthermore, the data in C$^{3}$bench originates from ten different domains, covering most of the categories in classical Chinese. Leveraging the proposed C$^{3}$bench, we extensively evaluate the quantitative performance of 15 representative LLMs on all five CCU tasks. Our results not only establish a public leaderboard of LLMs' CCU capabilities but also gain some findings. Specifically, existing LLMs are struggle with CCU tasks and still inferior to supervised models. Additionally, the results indicate that CCU is a task that requires special attention. We believe this study could provide a standard benchmark, comprehensive baselines, and valuable insights for the future advancement of LLM-based CCU research. The evaluation pipeline and dataset are available at \\url{https://github.com/SCUT-DLVCLab/C3bench}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17755",
        "abstract url": "https://arxiv.org/abs/2405.17755",
        "title": "XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Length generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs. To address this problem, the existing methods either require substantial costs or introduce precision loss. In this paper, we empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty. Based on this, we propose an efficient training free framework, named XL3M (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning. Under the XL3M framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common ``question'' which is a few tokens from the end of the original context. Then XL3M gives a method to measure the relevance between each segment and the ``question'', and constructs a concise key context by splicing all the relevant segments in chronological order. The key context is further used instead of the original context to complete the inference task. Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "11 pages, 5 figures"
    },
    {
        "paper id": "2405.17764",
        "abstract url": "https://arxiv.org/abs/2405.17764",
        "title": "On the Sequence Evaluation based on Stochastic Processes",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Modeling and analyzing long sequences of text is an essential task for Natural Language Processing. Success in capturing long text dynamics using neural language models will facilitate many downstream tasks such as coherence evaluation, text generation, machine translation and so on. This paper presents a novel approach to model sequences through a stochastic process. We introduce a likelihood-based training objective for the text encoder and design a more thorough measurement (score) for long text evaluation compared to the previous approach. The proposed training objective effectively preserves the sequence coherence, while the new score comprehensively captures both temporal and spatial dependencies. Theoretical properties of our new score show its advantages in sequence evaluation. Experimental results show superior performance in various sequence evaluation tasks, including global and local discrimination within and between documents of different lengths. We also demonstrate the encoder achieves competitive results on discriminating human and AI written text.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17767",
        "abstract url": "https://arxiv.org/abs/2405.17767",
        "title": "Linguistic Collapse: Neural Collapse in (Large) Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as \\textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$ properties that develop with scaling are linked to generalization. Moreover, there is evidence of some relationship between $\\mathcal{NC}$ and generalization independent of scale. Our work therefore underscores the generality of $\\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\\mathcal{NC}$-related properties.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "comment": "29 pages, 27 figures"
    },
    {
        "paper id": "2405.17790",
        "abstract url": "https://arxiv.org/abs/2405.17790",
        "title": "Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a novel instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Instruct-ReID is the first exploration of a general ReID setting, where existing 6 ReID tasks can be viewed as special cases by assigning different instructions. To facilitate research in this new instruct-ReID task, we propose a large-scale OmniReID++ benchmark equipped with diverse data and comprehensive evaluation methods e.g., task specific and task-free evaluation settings. In the task-specific evaluation setting, gallery sets are categorized according to specific ReID tasks. We propose a novel baseline model, IRM, with an adaptive triplet loss to handle various retrieval tasks within a unified framework. For task-free evaluation setting, where target person images are retrieved from task-agnostic gallery sets, we further propose a new method called IRM++ with novel memory bank-assisted learning. Extensive evaluations of IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our proposed methods, achieving state-of-the-art performance on 10 test sets. The datasets, the model, and the code will be available at https://github.com/hwz-zju/Instruct-ReID",
        "subjects": [
            "cs.CV"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2306.07520"
    },
    {
        "paper id": "2405.17799",
        "abstract url": "https://arxiv.org/abs/2405.17799",
        "title": "Exploring Activation Patterns of Parameters in Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Most work treats large language models as black boxes without in-depth understanding of their internal working mechanism. In order to explain the internal representations of LLMs, we propose a gradient-based metric to assess the activation level of model parameters. Based on this metric, we obtain three preliminary findings. (1) When the inputs are in the same domain, parameters in the shallow layers will be activated densely, which means a larger portion of parameters will have great impacts on the outputs. In contrast, parameters in the deep layers are activated sparsely. (2) When the inputs are across different domains, parameters in shallow layers exhibit higher similarity in the activation behavior than deep layers. (3) In deep layers, the similarity of the distributions of activated parameters is positively correlated to the empirical data relevance. Further, we develop three validation experiments to solidify these findings. (1) Firstly, starting from the first finding, we attempt to configure different prune ratios for different layers, and find this method can benefit model pruning. (2) Secondly, we find that a pruned model based on one calibration set can better handle tasks related to the calibration task than those not related, which validate the second finding. (3) Thirdly, Based on the STS-B and SICK benchmark, we find that two sentences with consistent semantics tend to share similar parameter activation patterns in deep layers, which aligns with our third finding. Our work sheds light on the behavior of parameter activation in LLMs, and we hope these findings will have the potential to inspire more practical applications.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17809",
        "abstract url": "https://arxiv.org/abs/2405.17809",
        "title": "TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "There is a rising interest and trend in research towards directly translating speech from one language to another, known as end-to-end speech-to-speech translation. However, most end-to-end models struggle to outperform cascade models, i.e., a pipeline framework by concatenating speech recognition, machine translation and text-to-speech models. The primary challenges stem from the inherent complexities involved in direct translation tasks and the scarcity of data. In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability. Furthermore, we propose two separated encoders to preserve the speaker's voice characteristics and isochrony from the source speech during the translation process, making it highly suitable for scenarios such as video dubbing. Our experiments on the French-English language pair demonstrate that our model outperforms the current state-of-the-art speech-to-speech translation model.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2405.17816",
        "abstract url": "https://arxiv.org/abs/2405.17816",
        "title": "Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In the open world, detecting out-of-distribution (OOD) data, whose labels are disjoint with those of in-distribution (ID) samples, is important for reliable deep neural networks (DNNs). To achieve better detection performance, one type of approach proposes to fine-tune the model with auxiliary OOD datasets to amplify the difference between ID and OOD data through a separation loss defined on model outputs. However, none of these studies consider enlarging the feature disparity, which should be more effective compared to outputs. The main difficulty lies in the diversity of OOD samples, which makes it hard to describe their feature distribution, let alone design losses to separate them from ID features. In this paper, we neatly fence off the problem based on an aggregation property of ID features named Neural Collapse (NC). NC means that the penultimate features of ID samples within a class are nearly identical to the last layer weight of the corresponding class. Based on this property, we propose a simple but effective loss called OrthLoss, which binds the features of OOD data in a subspace orthogonal to the principal subspace of ID features formed by NC. In this way, the features of ID and OOD samples are separated by different dimensions. By optimizing the feature separation loss rather than purely enlarging output differences, our detection achieves SOTA performance on CIFAR benchmarks without any additional data augmentation or sampling, demonstrating the importance of feature separation in OOD detection. The code will be published.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17822",
        "abstract url": "https://arxiv.org/abs/2405.17822",
        "title": "Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16836",
        "abstract url": "https://arxiv.org/abs/2405.16836",
        "title": "Enhancing Fast Feed Forward Networks with Load Balancing and a Master Leaf Node",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Fast feedforward networks (FFFs) are a class of neural networks that exploit the observation that different regions of the input space activate distinct subsets of neurons in wide networks. FFFs partition the input space into separate sections using a differentiable binary tree of neurons and during inference descend the binary tree in order to improve computational efficiency. Inspired by Mixture of Experts (MoE) research, we propose the incorporation of load balancing and Master Leaf techniques into the FFF architecture to improve performance and simplify the training process. We reproduce experiments found in literature and present results on FFF models enhanced using these techniques. The proposed architecture and training recipe achieves up to 16.3% and 3% absolute classification accuracy increase in training and test accuracy, respectively, compared to the original FFF architecture. Additionally, we observe a smaller variance in the results compared to those reported in prior research. These findings demonstrate the potential of integrating MoE-inspired techniques into FFFs for developing more accurate and efficient models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16843",
        "abstract url": "https://arxiv.org/abs/2405.16843",
        "title": "Non-stochastic Bandits With Evolving Observations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce a novel online learning framework that unifies and generalizes pre-established models, such as delayed and corrupted feedback, to encompass adversarial environments where action feedback evolves over time. In this setting, the observed loss is arbitrary and may not correlate with the true loss incurred, with each round updating previous observations adversarially. We propose regret minimization algorithms for both the full-information and bandit settings, with regret bounds quantified by the average feedback accuracy relative to the true loss. Our algorithms match the known regret bounds across many special cases, while also introducing previously unknown bounds.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16865",
        "abstract url": "https://arxiv.org/abs/2405.16865",
        "title": "An Investigation of Conformal Isometry Hypothesis for Grid Cells",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates the conformal isometry hypothesis as a potential explanation for the emergence of hexagonal periodic patterns in the response maps of grid cells. The hypothesis posits that the activities of the population of grid cells form a high-dimensional vector in the neural space, representing the agent's self-position in 2D physical space. As the agent moves in the 2D physical space, the vector rotates in a 2D manifold in the neural space, driven by a recurrent neural network. The conformal isometry hypothesis proposes that this 2D manifold in the neural space is a conformally isometric embedding of the 2D physical space, in the sense that local displacements of the vector in neural space are proportional to local displacements of the agent in the physical space. Thus the 2D manifold forms an internal map of the 2D physical space, equipped with an internal metric. In this paper, we conduct numerical experiments to show that this hypothesis underlies the hexagon periodic patterns of grid cells. We also conduct theoretical analysis to further support this hypothesis. In addition, we propose a conformal modulation of the input velocity of the agent so that the recurrent neural network of grid cells satisfies the conformal isometry hypothesis automatically. To summarize, our work provides numerical and theoretical evidences for the conformal isometry hypothesis for grid cells and may serve as a foundation for further development of normative models of grid cells and beyond.",
        "subjects": [
            "q-bio.NC",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2310.19192"
    },
    {
        "paper id": "2405.16883",
        "abstract url": "https://arxiv.org/abs/2405.16883",
        "title": "Scorch: A Library for Sparse Deep Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms. Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations. To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs. Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures. Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference. Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability. More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations. This flexibility is crucial for exploring novel sparse architectures. We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains. With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks. Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem. We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.MS",
            "cs.PL"
        ],
        "comment": "25 pages, 8 figures"
    },
    {
        "paper id": "2405.16887",
        "abstract url": "https://arxiv.org/abs/2405.16887",
        "title": "A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As productivity advances, the demand of customers for multi-variety and small-batch production is increasing, thereby putting forward higher requirements for manufacturing systems. When production tasks frequent changes due to this demand, traditional manufacturing systems often cannot response promptly. The multi-agent manufacturing system is proposed to address this problem. However, because of technical limitations, the negotiation among agents in this kind of system is realized through predefined heuristic rules, which is not intelligent enough to deal with the multi-variety and small batch production. To this end, a Large Language Model-based (LLM-based) multi-agent manufacturing system for intelligent shopfloor is proposed in the present study. This system delineates the diverse agents and defines their collaborative methods. The roles of the agents encompass Machine Server Agent (MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and Decision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability of analyzing the shopfloor condition and choosing the most suitable machine, as opposed to executing a predefined program artificially. The negotiation between BAs and BIA is the most crucial step in connecting manufacturing resources. With the support of TA and DA, BIA will finalize the distribution of orders, relying on the information of each machine returned by BA. MSAs bears the responsibility for connecting the agents with the physical shopfloor. This system aims to distribute and transmit workpieces through the collaboration of the agents with these distinct roles, distinguishing it from other scheduling approaches. Comparative experiments were also conducted to validate the performance of this system.",
        "subjects": [
            "cs.AI",
            "cs.MA",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16899",
        "abstract url": "https://arxiv.org/abs/2405.16899",
        "title": "Partial Models for Building Adaptive Model-Based Reinforcement Learning Agents",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In neuroscience, one of the key behavioral tests for determining whether a subject of study exhibits model-based behavior is to study its adaptiveness to local changes in the environment. In reinforcement learning, however, recent studies have shown that modern model-based agents display poor adaptivity to such changes. The main reason for this is that modern agents are typically designed to improve sample efficiency in single task settings and thus do not take into account the challenges that can arise in other settings. In local adaptation settings, one particularly important challenge is in quickly building and maintaining a sufficiently accurate model after a local change. This is challenging for deep model-based agents as their models and replay buffers are monolithic structures lacking distribution shift handling capabilities. In this study, we show that the conceptually simple idea of partial models can allow deep model-based agents to overcome this challenge and thus allow for building locally adaptive model-based agents. By modeling the different parts of the state space through different models, the agent can not only maintain a model that is accurate across the state space, but it can also quickly adapt it in the presence of a local change in the environment. We demonstrate this by showing that the use of partial models in agents such as deep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the local changes in their environments.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Published as a conference paper at CoLLAs 2024"
    },
    {
        "paper id": "2405.16906",
        "abstract url": "https://arxiv.org/abs/2405.16906",
        "title": "Harnessing the Power of Vicinity-Informed Analysis for Classification under Covariate Shift",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transfer learning enhances prediction accuracy on a target distribution by leveraging data from a source distribution, demonstrating significant benefits in various applications. This paper introduces a novel dissimilarity measure that utilizes vicinity information, i.e., the local structure of data points, to analyze the excess error in classification under covariate shift, a transfer learning setting where marginal feature distributions differ but conditional label distributions remain the same. We characterize the excess error using the proposed measure and demonstrate faster or competitive convergence rates compared to previous techniques. Notably, our approach is effective in situations where the non-absolute continuousness assumption, which often appears in real-world applications, holds. Our theoretical analysis bridges the gap between current theoretical findings and empirical observations in transfer learning, particularly in scenarios with significant differences between source and target distributions.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16922",
        "abstract url": "https://arxiv.org/abs/2405.16922",
        "title": "Theories of synaptic memory consolidation and intelligent plasticity for continual learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Humans and animals learn throughout life. Such continual learning is crucial for intelligence. In this chapter, we examine the pivotal role plasticity mechanisms with complex internal synaptic dynamics could play in enabling this ability in neural networks. By surveying theoretical research, we highlight two fundamental enablers for continual learning. First, synaptic plasticity mechanisms must maintain and evolve an internal state over several behaviorally relevant timescales. Second, plasticity algorithms must leverage the internal state to intelligently regulate plasticity at individual synapses to facilitate the seamless integration of new memories while avoiding detrimental interference with existing ones. Our chapter covers successful applications of these principles to deep neural networks and underscores the significance of synaptic metaplasticity in sustaining continual learning capabilities. Finally, we outline avenues for further research to understand the brain's superb continual learning abilities and harness similar mechanisms for artificial intelligence systems.",
        "subjects": [
            "q-bio.NC",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "An introductory-level book chapter. 34 pages, 14 figures"
    },
    {
        "paper id": "2405.16924",
        "abstract url": "https://arxiv.org/abs/2405.16924",
        "title": "Demystifying amortized causal discovery with transformers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA (Ke et al., 2023), a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16925",
        "abstract url": "https://arxiv.org/abs/2405.16925",
        "title": "OED: Towards One-stage End-to-End Dynamic Scene Graph Generation",
        "rating": "0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Dynamic Scene Graph Generation (DSGG) focuses on identifying visual relationships within the spatial-temporal domain of videos. Conventional approaches often employ multi-stage pipelines, which typically consist of object detection, temporal association, and multi-relation classification. However, these methods exhibit inherent limitations due to the separation of multiple stages, and independent optimization of these sub-problems may yield sub-optimal solutions. To remedy these limitations, we propose a one-stage end-to-end framework, termed OED, which streamlines the DSGG pipeline. This framework reformulates the task as a set prediction problem and leverages pair-wise features to represent each subject-object pair within the scene graph. Moreover, another challenge of DSGG is capturing temporal dependencies, we introduce a Progressively Refined Module (PRM) for aggregating temporal context without the constraints of additional trackers or handcrafted trajectories, enabling end-to-end optimization of the network. Extensive experiments conducted on the Action Genome benchmark demonstrate the effectiveness of our design. The code and models are available at \\url{https://github.com/guanw-pku/OED}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR'24"
    },
    {
        "paper id": "2405.16928",
        "abstract url": "https://arxiv.org/abs/2405.16928",
        "title": "TopoLa: a novel embedding framework for understanding complex networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Complex networks, which are the abstractions of many real-world systems, present a persistent challenge across disciplines for people to decipher their underlying information. Recently, hyperbolic geometry of latent spaces has gained traction in network analysis, due to its ability to preserve certain local intrinsic properties of the nodes. In this study, we explore the problem from a much broader perspective: understanding the impact of nodes' global topological structures on latent space placements. Our investigations reveal a direct correlation between the topological structure of nodes and their positioning within the latent space. Building on this deep and strong connection between node distance and network topology, we propose a novel embedding framework called Topology-encoded Latent Hyperbolic Geometry (TopoLa) for analyzing complex networks. With the encoded topological information in the latent space, TopoLa is capable of enhancing both conventional and low-rank networks, using the singular value gap to clarify the mathematical principles behind this enhancement. Meanwhile, we show that the equipped TopoLa distance can also help augment pivotal deep learning models encompassing knowledge distillation and contrastive learning.",
        "subjects": [
            "cs.SI",
            "cs.GT"
        ],
        "comment": "85 pages, 17 figures"
    },
    {
        "paper id": "2405.16951",
        "abstract url": "https://arxiv.org/abs/2405.16951",
        "title": "Fast ML-driven Analog Circuit Layout using Reinforcement Learning and Steiner Trees",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents an artificial intelligence driven methodology to reduce the bottleneck often encountered in the analog ICs layout phase. We frame the floorplanning problem as a Markov Decision Process and leverage reinforcement learning for automatic placement generation under established topological constraints. Consequently, we introduce Steiner tree-based methods for the global routing step and generate guiding paths to be used to connect every circuit block. Finally, by integrating these solutions into a procedural generation framework, we present a unified pipeline that bridges the divide between circuit design and verification steps. Experimental results demonstrate the efficacy in generating complete layouts, eventually reducing runtimes to 1.5% compared to manual efforts.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "4 pages, 3 figures, accepted by SMACD 2024 conference"
    },
    {
        "paper id": "2405.16954",
        "abstract url": "https://arxiv.org/abs/2405.16954",
        "title": "Convergence of SGD with momentum in the nonconvex case: A novel time window-based analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a novel time window-based analysis technique to investigate the convergence behavior of the stochastic gradient descent method with momentum (SGDM) in nonconvex settings. Despite its popularity, the convergence behavior of SGDM remains less understood in nonconvex scenarios. This is primarily due to the absence of a sufficient descent property and challenges in controlling stochastic errors in an almost sure sense. To address these challenges, we study the behavior of SGDM over specific time windows, rather than examining the descent of consecutive iterates as in traditional analyses. This time window-based approach simplifies the convergence analysis and enables us to establish the first iterate convergence result for SGDM under the Kurdyka-Lojasiewicz (KL) property. Based on the underlying KL exponent and the utilized step size scheme, we further characterize local convergence rates of SGDM.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": "28 pages"
    },
    {
        "paper id": "2405.16956",
        "abstract url": "https://arxiv.org/abs/2405.16956",
        "title": "Functional Programming Paradigm of Python for Scientific Computation Pipeline Integration",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The advent of modern data processing has led to an increasing tendency towards interdisciplinarity, which frequently involves the importation of different technical approaches. Consequently, there is an urgent need for a unified data control system to facilitate the integration of varying libraries. This integration is of profound significance in accelerating prototype verification, optimising algorithm performance and minimising maintenance costs. This paper presents a novel functional programming (FP) paradigm based on the Python architecture and associated suites in programming practice, designed for the integration of pipelines of different data mapping operations. In particular, the solution is intended for the integration of scientific computation flows, which affords a robust yet flexible solution for the aforementioned challenges.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CE",
            "cs.PL",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16958",
        "abstract url": "https://arxiv.org/abs/2405.16958",
        "title": "Large Deviations of Gaussian Neural Networks with ReLU activation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We prove a large deviation principle for deep neural networks with Gaussian weights and (at most linearly growing) activation functions. This generalises earlier work, in which bounded and continuous activation functions were considered. In practice, linearly growing activation functions such as ReLU are most commonly used. We furthermore simplify previous expressions for the rate function and a give power-series expansions for the ReLU case.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.PR"
        ],
        "comment": "13 pages, 2 figures"
    },
    {
        "paper id": "2405.16966",
        "abstract url": "https://arxiv.org/abs/2405.16966",
        "title": "Dual-Delayed Asynchronous SGD for Arbitrarily Heterogeneous Data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the distributed learning problem with data dispersed across multiple workers under the orchestration of a central server. Asynchronous Stochastic Gradient Descent (SGD) has been widely explored in such a setting to reduce the synchronization overhead associated with parallelization. However, the performance of asynchronous SGD algorithms often depends on a bounded dissimilarity condition among the workers' local data, a condition that can drastically affect their efficiency when the workers' data are highly heterogeneous. To overcome this limitation, we introduce the \\textit{dual-delayed asynchronous SGD (DuDe-ASGD)} algorithm designed to neutralize the adverse effects of data heterogeneity. DuDe-ASGD makes full use of stale stochastic gradients from all workers during asynchronous training, leading to two distinct time lags in the model parameters and data samples utilized in the server's iterations. Furthermore, by adopting an incremental aggregation strategy, DuDe-ASGD maintains a per-iteration computational cost that is on par with traditional asynchronous SGD algorithms. Our analysis demonstrates that DuDe-ASGD achieves a near-minimax-optimal convergence rate for smooth nonconvex problems, even when the data across workers are extremely heterogeneous. Numerical experiments indicate that DuDe-ASGD compares favorably with existing asynchronous and synchronous SGD-based algorithms.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17017",
        "abstract url": "https://arxiv.org/abs/2405.17017",
        "title": "Analysis of Multiscale Reinforcement Q-Learning Algorithms for Mean Field Control Games",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Mean Field Control Games (MFCG), introduced in [Angiuli et al., 2022a], represent competitive games between a large number of large collaborative groups of agents in the infinite limit of number and size of groups. In this paper, we prove the convergence of a three-timescale Reinforcement Q-Learning (RL) algorithm to solve MFCG in a model-free approach from the point of view of representative agents. Our analysis uses a Q-table for finite state and action spaces updated at each discrete time-step over an infinite horizon. In [Angiuli et al., 2023], we proved convergence of two-timescale algorithms for MFG and MFC separately highlighting the need to follow multiple population distributions in the MFC case. Here, we integrate this feature for MFCG as well as three rates of update decreasing to zero in the proper ratios. Our technique of proof uses a generalization to three timescales of the two-timescale analysis in [Borkar, 1997]. We give a simple example satisfying the various hypothesis made in the proof of convergence and illustrating the performance of the algorithm.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "cs.MA"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2312.06659"
    },
    {
        "paper id": "2405.17025",
        "abstract url": "https://arxiv.org/abs/2405.17025",
        "title": "SWAT: Scalable and Efficient Window Attention-based Transformers Acceleration on FPGAs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Efficiently supporting long context length is crucial for Transformer models. The quadratic complexity of the self-attention computation plagues traditional Transformers. Sliding window-based static sparse attention mitigates the problem by limiting the attention scope of the input tokens, reducing the theoretical complexity from quadratic to linear. Although the sparsity induced by window attention is highly structured, it does not align perfectly with the microarchitecture of the conventional accelerators, leading to suboptimal implementation. In response, we propose a dataflow-aware FPGA-based accelerator design, SWAT, that efficiently leverages the sparsity to achieve scalable performance for long input. The proposed microarchitecture is based on a design that maximizes data reuse by using a combination of row-wise dataflow, kernel fusion optimization, and an input-stationary design considering the distributed memory and computation resources of FPGA. Consequently, it achieves up to 22$\\times$ and 5.7$\\times$ improvement in latency and energy efficiency compared to the baseline FPGA-based accelerator and 15$\\times$ energy efficiency compared to GPU-based solution.",
        "subjects": [
            "cs.AR",
            "cs.AI"
        ],
        "comment": "Accepeted paper for DAC'22"
    },
    {
        "paper id": "2405.17027",
        "abstract url": "https://arxiv.org/abs/2405.17027",
        "title": "Supervised Batch Normalization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Batch Normalization (BN), a widely-used technique in neural networks, enhances generalization and expedites training by normalizing each mini-batch to the same mean and variance. However, its effectiveness diminishes when confronted with diverse data distributions. To address this challenge, we propose Supervised Batch Normalization (SBN), a pioneering approach. We expand normalization beyond traditional single mean and variance parameters, enabling the identification of data modes prior to training. This ensures effective normalization for samples sharing common features. We define contexts as modes, categorizing data with similar characteristics. These contexts are explicitly defined, such as domains in domain adaptation or modalities in multimodal systems, or implicitly defined through clustering algorithms based on data similarity. We illustrate the superiority of our approach over BN and other commonly employed normalization techniques through various experiments on both single and multi-task datasets. Integrating SBN with Vision Transformer results in a remarkable \\textit{15.13}\\% accuracy enhancement on CIFAR-100. Additionally, in domain adaptation scenarios, employing AdaMatch demonstrates an impressive \\textit{22.25}\\% accuracy improvement on MNIST and SVHN compared to BN.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17031",
        "abstract url": "https://arxiv.org/abs/2405.17031",
        "title": "Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Model-based methods in reinforcement learning offer a promising approach to enhance data efficiency by facilitating policy exploration within a dynamics model. However, accurately predicting sequential steps in the dynamics model remains a challenge due to the bootstrapping prediction, which attributes the next state to the prediction of the current state. This leads to accumulated errors during model roll-out. In this paper, we propose the Any-step Dynamics Model (ADM) to mitigate the compounding error by reducing bootstrapping prediction to direct prediction. ADM allows for the use of variable-length plans as inputs for predicting future states without frequent bootstrapping. We design two algorithms, ADMPO-ON and ADMPO-OFF, which apply ADM in online and offline model-based frameworks, respectively. In the online setting, ADMPO-ON demonstrates improved sample efficiency compared to previous state-of-the-art methods. In the offline setting, ADMPO-OFF not only demonstrates superior performance compared to recent state-of-the-art offline approaches but also offers better quantification of model uncertainty using only a single ADM.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17038",
        "abstract url": "https://arxiv.org/abs/2405.17038",
        "title": "Advancements in Tactile Hand Gesture Recognition for Enhanced Human-Machine Interaction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Motivated by the growing interest in enhancing intuitive physical Human-Machine Interaction (HRI/HVI), this study aims to propose a robust tactile hand gesture recognition system. We performed a comprehensive evaluation of different hand gesture recognition approaches for a large area tactile sensing interface (touch interface) constructed from conductive textiles. Our evaluation encompassed traditional feature engineering methods, as well as contemporary deep learning techniques capable of real-time interpretation of a range of hand gestures, accommodating variations in hand sizes, movement velocities, applied pressure levels, and interaction points. Our extensive analysis of the various methods makes a significant contribution to tactile-based gesture recognition in the field of human-machine interaction.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17053",
        "abstract url": "https://arxiv.org/abs/2405.17053",
        "title": "WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.",
        "subjects": [
            "cs.NI",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17054",
        "abstract url": "https://arxiv.org/abs/2405.17054",
        "title": "Improving Data-aware and Parameter-aware Robustness for Continual Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The goal of Continual Learning (CL) task is to continuously learn multiple new tasks sequentially while achieving a balance between the plasticity and stability of new and old knowledge. This paper analyzes that this insufficiency arises from the ineffective handling of outliers, leading to abnormal gradients and unexpected model updates. To address this issue, we enhance the data-aware and parameter-aware robustness of CL, proposing a Robust Continual Learning (RCL) method. From the data perspective, we develop a contrastive loss based on the concepts of uniformity and alignment, forming a feature distribution that is more applicable to outliers. From the parameter perspective, we present a forward strategy for worst-case perturbation and apply robust gradient projection to the parameters. The experimental results on three benchmarks show that the proposed method effectively maintains robustness and achieves new state-of-the-art (SOTA) results. The code is available at: https://github.com/HanxiXiao/RCL",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17061",
        "abstract url": "https://arxiv.org/abs/2405.17061",
        "title": "Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study a new class of MDPs that employs multinomial logit (MNL) function approximation to ensure valid probability distributions over the state space. Despite its benefits, introducing non-linear function approximation raises significant challenges in both computational and statistical efficiency. The best-known method of Hwang and Oh [2023] has achieved an $\\widetilde{\\mathcal{O}}(\u03ba^{-1}dH^2\\sqrt{K})$ regret, where $\u03ba$ is a problem-dependent quantity, $d$ is the feature space dimension, $H$ is the episode length, and $K$ is the number of episodes. While this result attains the same rate in $K$ as the linear cases, the method requires storing all historical data and suffers from an $\\mathcal{O}(K)$ computation cost per episode. Moreover, the quantity $\u03ba$ can be exponentially small, leading to a significant gap for the regret compared to the linear cases. In this work, we first address the computational concerns by proposing an online algorithm that achieves the same regret with only $\\mathcal{O}(1)$ computation cost. Then, we design two algorithms that leverage local information to enhance statistical efficiency. They not only maintain an $\\mathcal{O}(1)$ computation cost per episode but achieve improved regrets of $\\widetilde{\\mathcal{O}}(\u03ba^{-1/2}dH^2\\sqrt{K})$ and $\\widetilde{\\mathcal{O}}(dH^2\\sqrt{K} + \u03ba^{-1}d^2H^2)$ respectively. Finally, we establish a lower bound, justifying the optimality of our results in $d$ and $K$. To the best of our knowledge, this is the first work that achieves almost the same computational and statistical efficiency as linear function approximation while employing non-linear function approximation for reinforcement learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17066",
        "abstract url": "https://arxiv.org/abs/2405.17066",
        "title": "Saturn: Sample-efficient Generative Molecular Design using Memory Manipulation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative molecular design for drug discovery has very recently achieved a wave of experimental validation, with language-based backbones being the most common architectures employed. The most important factor for downstream success is whether an in silico oracle is well correlated with the desired end-point. To this end, current methods use cheaper proxy oracles with higher throughput before evaluating the most promising subset with high-fidelity oracles. The ability to directly optimize high-fidelity oracles would greatly enhance generative design and be expected to improve hit rates. However, current models are not efficient enough to consider such a prospect, exemplifying the sample efficiency problem. In this work, we introduce Saturn, which leverages the Augmented Memory algorithm and demonstrates the first application of the Mamba architecture for generative molecular design. We elucidate how experience replay with data augmentation improves sample efficiency and how Mamba synergistically exploits this mechanism. Saturn outperforms 22 models on multi-parameter optimization tasks relevant to drug discovery and may possess sufficient sample efficiency to consider the prospect of directly optimizing high-fidelity oracles.",
        "subjects": [
            "q-bio.BM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17075",
        "abstract url": "https://arxiv.org/abs/2405.17075",
        "title": "Interaction-Force Transport Gradient Flows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a new type of gradient flow geometries over non-negative and probability measures motivated via a principled construction that combines the optimal transport and interaction forces modeled by reproducing kernels. Concretely, we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD Riemannian metric tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization studied by Arbel et al. [2019]. Furthermore, we prove that the spherical IFT gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.",
        "subjects": [
            "cs.LG",
            "math.AP",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17079",
        "abstract url": "https://arxiv.org/abs/2405.17079",
        "title": "Learning with User-Level Local Differential Privacy",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "User-level privacy is important in distributed systems. Previous research primarily focuses on the central model, while the local models have received much less attention. Under the central model, user-level DP is strictly stronger than the item-level one. However, under the local model, the relationship between user-level and item-level LDP becomes more complex, thus the analysis is crucially different. In this paper, we first analyze the mean estimation problem and then apply it to stochastic optimization, classification, and regression. In particular, we propose adaptive strategies to achieve optimal performance at all privacy levels. Moreover, we also obtain information-theoretic lower bounds, which show that the proposed methods are minimax optimal up to logarithmic factors. Unlike the central DP model, where user-level DP always leads to slower convergence, our result shows that under the local model, the convergence rates are nearly the same between user-level and item-level cases for distributions with bounded support. For heavy-tailed distributions, the user-level rate is even faster than the item-level one.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17081",
        "abstract url": "https://arxiv.org/abs/2405.17081",
        "title": "Effective Layer Pruning Through Similarity Metric Perspective",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks have been the predominant paradigm in machine learning for solving cognitive tasks. Such models, however, are restricted by a high computational overhead, limiting their applicability and hindering advancements in the field. Extensive research demonstrated that pruning structures from these models is a straightforward approach to reducing network complexity. In this direction, most efforts focus on removing weights or filters. Studies have also been devoted to layer pruning as it promotes superior computational gains. However, layer pruning often hurts the network predictive ability (i.e., accuracy) at high compression rates. This work introduces an effective layer-pruning strategy that meets all underlying properties pursued by pruning methods. Our method estimates the relative importance of a layer using the Centered Kernel Alignment (CKA) metric, employed to measure the similarity between the representations of the unpruned model and a candidate layer for pruning. We confirm the effectiveness of our method on standard architectures and benchmarks, in which it outperforms existing layer-pruning strategies and other state-of-the-art pruning techniques. Particularly, we remove more than 75% of computation while improving predictive ability. At higher compression regimes, our method exhibits negligible accuracy drop, while other methods notably deteriorate model accuracy. Apart from these benefits, our pruned models exhibit robustness to adversarial and out-of-distribution samples.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Code available at: github.com/IanPons/CKA-Layer-Pruning"
    },
    {
        "paper id": "2405.17094",
        "abstract url": "https://arxiv.org/abs/2405.17094",
        "title": "Dual feature reduction for the sparse-group lasso and its adaptive variant",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The sparse-group lasso performs both variable and group selection, making simultaneous use of the strengths of the lasso and group lasso. It has found widespread use in genetics, a field that regularly involves the analysis of high-dimensional data, due to its sparse-group penalty, which allows it to utilize grouping information. However, the sparse-group lasso can be computationally more expensive than both the lasso and group lasso, due to the added shrinkage complexity, and its additional hyper-parameter that needs tuning. In this paper a novel dual feature reduction method, Dual Feature Reduction (DFR), is presented that uses strong screening rules for the sparse-group lasso and the adaptive sparse-group lasso to reduce their input space before optimization. DFR applies two layers of screening and is based on the dual norms of the sparse-group lasso and adaptive sparse-group lasso. Through synthetic and real numerical studies, it is shown that the proposed feature reduction approach is able to drastically reduce the computational cost in many different scenarios.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "46 pages, 18 figures, 41 tables"
    },
    {
        "paper id": "2405.17108",
        "abstract url": "https://arxiv.org/abs/2405.17108",
        "title": "Finding good policies in average-reward Markov Decision Processes without prior knowledge",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We revisit the identification of an $\\varepsilon$-optimal policy in average-reward Markov Decision Processes (MDP). In such MDPs, two measures of complexity have appeared in the literature: the diameter, $D$, and the optimal bias span, $H$, which satisfy $H\\leq D$. Prior work have studied the complexity of $\\varepsilon$-optimal policy identification only when a generative model is available. In this case, it is known that there exists an MDP with $D \\simeq H$ for which the sample complexity to output an $\\varepsilon$-optimal policy is $\u03a9(SAD/\\varepsilon^2)$ where $S$ and $A$ are the sizes of the state and action spaces. Recently, an algorithm with a sample complexity of order $SAH/\\varepsilon^2$ has been proposed, but it requires the knowledge of $H$. We first show that the sample complexity required to estimate $H$ is not bounded by any function of $S,A$ and $H$, ruling out the possibility to easily make the previous algorithm agnostic to $H$. By relying instead on a diameter estimation procedure, we propose the first algorithm for $(\\varepsilon,\u03b4)$-PAC policy identification that does not need any form of prior knowledge on the MDP. Its sample complexity scales in $SAD/\\varepsilon^2$ in the regime of small $\\varepsilon$, which is near-optimal. In the online setting, our first contribution is a lower bound which implies that a sample complexity polynomial in $H$ cannot be achieved in this setting. Then, we propose an online algorithm with a sample complexity in $SAD^2/\\varepsilon^2$, as well as a novel approach based on a data-dependent stopping rule that we believe is promising to further reduce this bound.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17120",
        "abstract url": "https://arxiv.org/abs/2405.17120",
        "title": "Dual VC Dimension Obstructs Sample Compression by Embeddings",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work studies embedding of arbitrary VC classes in well-behaved VC classes, focusing particularly on extremal classes. Our main result expresses an impossibility: such embeddings necessarily require a significant increase in dimension. In particular, we prove that for every $d$ there is a class with VC dimension $d$ that cannot be embedded in any extremal class of VC dimension smaller than exponential in $d$. In addition to its independent interest, this result has an important implication in learning theory, as it reveals a fundamental limitation of one of the most extensively studied approaches to tackling the long-standing sample compression conjecture. Concretely, the approach proposed by Floyd and Warmuth entails embedding any given VC class into an extremal class of a comparable dimension, and then applying an optimal sample compression scheme for extremal classes. However, our results imply that this strategy would in some cases result in a sample compression scheme at least exponentially larger than what is predicted by the sample compression conjecture. The above implications follow from a general result we prove: any extremal class with VC dimension $d$ has dual VC dimension at most $2d+1$. This bound is exponentially smaller than the classical bound $2^{d+1}-1$ of Assouad, which applies to general concept classes (and is known to be unimprovable for some classes). We in fact prove a stronger result, establishing that $2d+1$ upper bounds the dual Radon number of extremal classes. This theorem represents an abstraction of the classical Radon theorem for convex sets, extending its applicability to a wider combinatorial framework, without relying on the specifics of Euclidean convexity. The proof utilizes the topological method and is primarily based on variants of the Topological Radon Theorem.",
        "subjects": [
            "cs.DM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17151",
        "abstract url": "https://arxiv.org/abs/2405.17151",
        "title": "Smoke and Mirrors in Causal Downstream Tasks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where we assume binary effects that are recorded as high-dimensional images in a Randomized Controlled Trial (RCT). Despite being the simplest possible setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences. All code and data will be released.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17152",
        "abstract url": "https://arxiv.org/abs/2405.17152",
        "title": "CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion. Existing work mainly chooses neighboring intersections as collaborators. However, quite an amount of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate. To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy. Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features. Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods. The code is available at https://github.com/AnonymousAccountss/CoSLight.",
        "subjects": [
            "cs.MA",
            "cs.AI"
        ],
        "comment": "Accepted by KDD 2024"
    },
    {
        "paper id": "2405.17164",
        "abstract url": "https://arxiv.org/abs/2405.17164",
        "title": "WeiPer: OOD Detection using Weight Perturbations of Class Projections",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in out-of-distribution (OOD) detection on image data show that pre-trained neural network classifiers can separate in-distribution (ID) from OOD data well, leveraging the class-discriminative ability of the model itself. Methods have been proposed that either use logit information directly or that process the model's penultimate layer activations. With \"WeiPer\", we introduce perturbations of the class projections in the final fully connected layer which creates a richer representation of the input. We show that this simple trick can improve the OOD detection performance of a variety of methods and additionally propose a distance-based method that leverages the properties of the augmented WeiPer space. We achieve state-of-the-art OOD detection results across multiple benchmarks of the OpenOOD framework, especially pronounced in difficult settings in which OOD samples are positioned close to the training set distribution. We support our findings with theoretical motivations and empirical observations, and run extensive ablations to provide insights into why WeiPer works.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17182",
        "abstract url": "https://arxiv.org/abs/2405.17182",
        "title": "Exploring the Performance of Continuous-Time Dynamic Link Prediction Algorithms",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.SI"
            ]
        ],
        "abstract": "Dynamic Link Prediction (DLP) addresses the prediction of future links in evolving networks. However, accurately portraying the performance of DLP algorithms poses challenges that might impede progress in the field. Importantly, common evaluation pipelines usually calculate ranking or binary classification metrics, where the scores of observed interactions (positives) are compared with those of randomly generated ones (negatives). However, a single metric is not sufficient to fully capture the differences between DLP algorithms, and is prone to overly optimistic performance evaluation. Instead, an in-depth evaluation should reflect performance variations across different nodes, edges, and time segments. In this work, we contribute tools to perform such a comprehensive evaluation. (1) We propose Birth-Death diagrams, a simple but powerful visualization technique that illustrates the effect of time-based train-test splitting on the difficulty of DLP on a given dataset. (2) We describe an exhaustive taxonomy of negative sampling methods that can be used at evaluation time. (3) We carry out an empirical study of the effect of the different negative sampling strategies. Our comparison between heuristics and state-of-the-art memory-based methods on various real-world datasets confirms a strong effect of using different negative sampling strategies on the test Area Under the Curve (AUC). Moreover, we conduct a visual exploration of the prediction, with additional insights on which different types of errors are prominent over time.",
        "subjects": [
            "cs.SI",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17198",
        "abstract url": "https://arxiv.org/abs/2405.17198",
        "title": "Convex Relaxation for Solving Large-Margin Classifiers in Hyperbolic Space",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hyperbolic spaces have increasingly been recognized for their outstanding performance in handling data with inherent hierarchical structures compared to their Euclidean counterparts. However, learning in hyperbolic spaces poses significant challenges. In particular, extending support vector machines to hyperbolic spaces is in general a constrained non-convex optimization problem. Previous and popular attempts to solve hyperbolic SVMs, primarily using projected gradient descent, are generally sensitive to hyperparameters and initializations, often leading to suboptimal solutions. In this work, by first rewriting the problem into a polynomial optimization, we apply semidefinite relaxation and sparse moment-sum-of-squares relaxation to effectively approximate the optima. From extensive empirical experiments, these methods are shown to perform better than the projected gradient descent approach.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17211",
        "abstract url": "https://arxiv.org/abs/2405.17211",
        "title": "Spectral-Refiner: Fine-Tuning of Accurate Spatiotemporal Neural Operator for Turbulent Flows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in operator-type neural networks have shown promising results in approximating the solutions of spatiotemporal Partial Differential Equations (PDEs). However, these neural networks often entail considerable training expenses, and may not always achieve the desired accuracy required in many scientific and engineering disciplines. In this paper, we propose a new Spatiotemporal Fourier Neural Operator (SFNO) that learns maps between Bochner spaces, and a new learning framework to address these issues. This new paradigm leverages wisdom from traditional numerical PDE theory and techniques to refine the pipeline of commonly adopted end-to-end neural operator training and evaluations. Specifically, in the learning problems for the turbulent flow modeling by the Navier-Stokes Equations (NSE), the proposed architecture initiates the training with a few epochs for SFNO, concluding with the freezing of most model parameters. Then, the last linear spectral convolution layer is fine-tuned without the frequency truncation. The optimization uses a negative Sobolev norm for the first time as the loss in operator learning, defined through a reliable functional-type \\emph{a posteriori} error estimator whose evaluation is almost exact thanks to the Parseval identity. This design allows the neural operators to effectively tackle low-frequency errors while the relief of the de-aliasing filter addresses high-frequency errors. Numerical experiments on commonly used benchmarks for the 2D NSE demonstrate significant improvements in both computational efficiency and accuracy, compared to end-to-end evaluation and traditional numerical PDE solvers.",
        "subjects": [
            "cs.LG",
            "math.NA",
            "physics.flu-dyn"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17222",
        "abstract url": "https://arxiv.org/abs/2405.17222",
        "title": "A Retrospective of the Tutorial on Opportunities and Challenges of Online Deep Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning algorithms have become indispensable in today's world. They support and accelerate the way we make decisions based on the data at hand. This acceleration means that data structures that were valid at one moment could no longer be valid in the future. With these changing data structures, it is necessary to adapt machine learning (ML) systems incrementally to the new data. This is done with the use of online learning or continuous ML technologies. While deep learning technologies have shown exceptional performance on predefined datasets, they have not been widely applied to online, streaming, and continuous learning. In this retrospective of our tutorial titled Opportunities and Challenges of Online Deep Learning held at ECML PKDD 2023, we provide a brief overview of the opportunities but also the potential pitfalls for the application of neural networks in online learning environments using the frameworks River and Deep-River.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted for publication on ECML-PKDD 2023 joint Post-Workshop Proceeding"
    },
    {
        "paper id": "2405.17233",
        "abstract url": "https://arxiv.org/abs/2405.17233",
        "title": "CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance. Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios. Code will be released soon.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17240",
        "abstract url": "https://arxiv.org/abs/2405.17240",
        "title": "Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth",
        "rating": "0.5",
        "keywords": [
            [
                "synthesizing"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The absence of real targets to guide the model training is one of the main problems with the makeup transfer task. Most existing methods tackle this problem by synthesizing pseudo ground truths (PGTs). However, the generated PGTs are often sub-optimal and their imprecision will eventually lead to performance degradation. To alleviate this issue, in this paper, we propose a novel Content-Style Decoupled Makeup Transfer (CSD-MT) method, which works in a purely unsupervised manner and thus eliminates the negative effects of generating PGTs. Specifically, based on the frequency characteristics analysis, we assume that the low-frequency (LF) component of a face image is more associated with its makeup style information, while the high-frequency (HF) component is more related to its content details. This assumption allows CSD-MT to decouple the content and makeup style information in each face image through the frequency decomposition. After that, CSD-MT realizes makeup transfer by maximizing the consistency of these two types of information between the transferred result and input images, respectively. Two newly designed loss functions are also introduced to further improve the transfer performance. Extensive quantitative and qualitative analyses show the effectiveness of our CSD-MT method. Our code is available at https://github.com/Snowfallingplum/CSD-MT.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by CVPR2024"
    },
    {
        "paper id": "2405.17243",
        "abstract url": "https://arxiv.org/abs/2405.17243",
        "title": "Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Both entropy-minimizing and entropy-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy. However, neither method alone results in an agent that will consistently learn intelligent behavior across environments. In an effort to find a single entropy-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective online, depending on the entropy conditions by framing the choice as a multi-armed bandit problem. We devise a novel intrinsic feedback signal for the bandit, which captures the agent's ability to control the entropy in its environment. We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high- and low-entropy regimes and can learn skillful behaviors in benchmark tasks. Videos of the trained agents and summarized findings can be found on our project page https://sites.google.com/view/surprise-adaptive-agents",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Published at the Reinforcement Learning Conference 2024"
    },
    {
        "paper id": "2405.17245",
        "abstract url": "https://arxiv.org/abs/2405.17245",
        "title": "Galaxy: A Resource-Efficient Collaborative Edge AI System for In-situ Transformer Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users' privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity-aware parallelism planning for fully exploiting the resource potential. Furthermore, Galaxy devises a tile-based fine-grained overlapping of communication and computation to mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments. Extensive evaluation based on prototype implementation demonstrates that Galaxy remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 2.5x end-to-end latency reduction.",
        "subjects": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "comment": "Accepted by IEEE International Conference on Computer Communications 2024"
    },
    {
        "paper id": "2405.17248",
        "abstract url": "https://arxiv.org/abs/2405.17248",
        "title": "Transformer In-Context Learning for Categorical Data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent research has sought to understand Transformers through the lens of in-context learning with functional data. We extend that line of work with the goal of moving closer to language models, considering categorical outcomes, nonlinear underlying models, and nonlinear attention. The contextual data are of the form $\\textsf{C}=(x_1,c_1,\\dots,x_N,c_{N})$ where each $c_i\\in\\{0,\\dots,C-1\\}$ is drawn from a categorical distribution that depends on covariates $x_i\\in\\mathbb{R}^d$. Contextual outcomes in the $m$th set of contextual data, $\\textsf{C}_m$, are modeled in terms of latent function $f_m(x)\\in\\textsf{F}$, where $\\textsf{F}$ is a functional class with $(C-1)$-dimensional vector output. The probability of observing class $c\\in\\{0,\\dots,C-1\\}$ is modeled in terms of the output components of $f_m(x)$ via the softmax. The Transformer parameters may be trained with $M$ contextual examples, $\\{\\textsf{C}_m\\}_{m=1,M}$, and the trained model is then applied to new contextual data $\\textsf{C}_{M+1}$ for new $f_{M+1}(x)\\in\\textsf{F}$. The goal is for the Transformer to constitute the probability of each category $c\\in\\{0,\\dots,C-1\\}$ for a new query $x_{N_{M+1}+1}$. We assume each component of $f_m(x)$ resides in a reproducing kernel Hilbert space (RKHS), specifying $\\textsf{F}$. Analysis and an extensive set of experiments suggest that on its forward pass the Transformer (with attention defined by the RKHS kernel) implements a form of gradient descent of the underlying function, connected to the latent vector function associated with the softmax. We present what is believed to be the first real-world demonstration of this few-shot-learning methodology, using the ImageNet dataset.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17277",
        "abstract url": "https://arxiv.org/abs/2405.17277",
        "title": "Gradients of Functions of Large Matrices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Tuning scientific and probabilistic machine learning models -- for example, partial differential equations, Gaussian processes, or Bayesian neural networks -- often relies on evaluating functions of matrices whose size grows with the data set or the number of parameters. While the state-of-the-art for evaluating these quantities is almost always based on Lanczos and Arnoldi iterations, the present work is the first to explain how to differentiate these workhorses of numerical linear algebra efficiently. To get there, we derive previously unknown adjoint systems for Lanczos and Arnoldi iterations, implement them in JAX, and show that the resulting code can compete with Diffrax when it comes to differentiating PDEs, GPyTorch for selecting Gaussian process models and beats standard factorisation methods for calibrating Bayesian neural networks. All this is achieved without any problem-specific code optimisation. Find the code at https://github.com/pnkraemer/experiments-lanczos-adjoints and install the library with pip install matfree.",
        "subjects": [
            "cs.LG",
            "math.NA",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17283",
        "abstract url": "https://arxiv.org/abs/2405.17283",
        "title": "Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Current state-of-the-art synchrony-based models encode object bindings with complex-valued activations and compute with real-valued weights in feedforward architectures. We argue for the computational advantages of a recurrent architecture with complex-valued weights. We propose a fully convolutional autoencoder, SynCx, that performs iterative constraint satisfaction: at each iteration, a hidden layer bottleneck encodes statistically regular configurations of features in particular phase relationships; over iterations, local constraints propagate and the model converges to a globally consistent configuration of phase assignments. Binding is achieved simply by the matrix-vector product operation between complex-valued weights and activations, without the need for additional mechanisms that have been incorporated into current synchrony-based models. SynCx outperforms or is strongly competitive with current models for unsupervised object discovery. SynCx also avoids certain systematic grouping errors of current models, such as the inability to separate similarly colored objects without additional supervision.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": "minor typo fixed"
    },
    {
        "paper id": "2405.17287",
        "abstract url": "https://arxiv.org/abs/2405.17287",
        "title": "Opinion-Guided Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Human guidance is often desired in reinforcement learning to improve the performance of the learning agent. However, human insights are often mere opinions and educated guesses rather than well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial informedness or ignorance about a problem, they also emerge earlier than hard evidence could be produced. Thus, guiding reinforcement learning agents through opinions offers the potential for more performant learning processes, but comes with the challenge of modeling and managing opinions in a formal way. In this article, we present a method to guide reinforcement learning agents through opinions. To this end, we provide an end-to-end method to model and manage advisors' opinions. To assess the utility of the approach, we evaluate it with synthetic and human advisors, at different levels of uncertainty, and under multiple advise strategies. Our results indicate that opinions, even if uncertain, improve the performance of reinforcement learning agents, resulting in higher rewards, more efficient exploration, and a better reinforced policy. Although we demonstrate our approach in a simplified topological running example, our approach is applicable to complex problems with higher dimensions as well.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17293",
        "abstract url": "https://arxiv.org/abs/2405.17293",
        "title": "Efficient Ensembles Improve Training Data Attribution",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Training data attribution (TDA) methods aim to quantify the influence of individual training data points on the model predictions, with broad applications in data-centric AI, such as mislabel detection, data selection, and copyright compensation. However, existing methods in this field, which can be categorized as retraining-based and gradient-based, have struggled with the trade-off between computational efficiency and attribution efficacy. Retraining-based methods can accurately attribute complex non-convex models but are computationally prohibitive, while gradient-based methods are efficient but often fail for non-convex models. Recent research has shown that augmenting gradient-based methods with ensembles of multiple independently trained models can achieve significantly better attribution efficacy. However, this approach remains impractical for very large-scale applications. In this work, we discover that expensive, fully independent training is unnecessary for ensembling the gradient-based methods, and we propose two efficient ensemble strategies, DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative to naive independent ensemble. These strategies significantly reduce training time (up to 80%), serving time (up to 60%), and space cost (up to 80%) while maintaining similar attribution efficacy to the naive independent ensemble. Our extensive experimental results demonstrate that the proposed strategies are effective across multiple TDA methods on diverse datasets and models, including generative settings, significantly advancing the Pareto frontier of TDA methods with better computational efficiency and attribution efficacy.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17374",
        "abstract url": "https://arxiv.org/abs/2405.17374",
        "title": "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Safety alignment is the key to guiding the behaviors of large language models (LLMs) that are in line with human preferences and restrict harmful behaviors at inference time, but recent studies show that it can be easily compromised by finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as \"safety basin\": randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. LLM safety landscape also highlights the system prompt's critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17378",
        "abstract url": "https://arxiv.org/abs/2405.17378",
        "title": "RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated potential in assisting with Register Transfer Level (RTL) design tasks. Nevertheless, there remains to be a significant gap in benchmarks that accurately reflect the complexity of real-world RTL projects. To address this, this paper presents RTL-Repo, a benchmark specifically designed to evaluate LLMs on large-scale RTL design projects. RTL-Repo includes a comprehensive dataset of more than 4000 Verilog code samples extracted from public GitHub repositories, with each sample providing the full context of the corresponding repository. We evaluate several state-of-the-art models on the RTL-Repo benchmark, including GPT-4, GPT-3.5, Starcoder2, alongside Verilog-specific models like VeriGen and RTLCoder, and compare their performance in generating Verilog code for complex projects. The RTL-Repo benchmark provides a valuable resource for the hardware design community to assess and compare LLMs' performance in real-world RTL design scenarios and train LLMs specifically for Verilog code generation in complex, multi-file RTL projects. RTL-Repo is open-source and publicly available on Github.",
        "subjects": [
            "cs.LG",
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17391",
        "abstract url": "https://arxiv.org/abs/2405.17391",
        "title": "Dataset-learning duality and emergent criticality",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables. During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases). For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass. We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning). In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems. We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables. In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function.",
        "subjects": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.NE"
        ],
        "comment": "27 pages, 9 figures"
    },
    {
        "paper id": "2405.17399",
        "abstract url": "https://arxiv.org/abs/2405.17399",
        "title": "Transformers Can Do Arithmetic with the Right Embeddings",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17406",
        "abstract url": "https://arxiv.org/abs/2405.17406",
        "title": "Deep Learning Calabi-Yau four folds with hybrid and recurrent neural network architectures",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we report the results of applying deep learning based on hybrid convolutional-recurrent and purely recurrent neural network architectures to the dataset of almost one million complete intersection Calabi-Yau four-folds (CICY4) to machine-learn their four Hodge numbers $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$. In particular, we explored and experimented with twelve different neural network models, nine of which are convolutional-recurrent (CNN-RNN) hybrids with the RNN unit being either GRU (Gated Recurrent Unit) or Long Short Term Memory (LSTM). The remaining four models are purely recurrent neural networks based on LSTM. In terms of the $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$ prediction accuracies, at 72% training ratio, our best performing individual model is CNN-LSTM-400, a hybrid CNN-LSTM with the LSTM hidden size of 400, which obtained 99.74%, 98.07%, 95.19%, 81.01%, our second best performing individual model is LSTM-448, an LSTM-based model with the hidden size of 448, which obtained 99.74%, 97.51%, 94.24%, and 78.63%. These results were improved by forming ensembles of the top two, three or even four models. Our best ensemble, consisting of the top three models, achieved the accuracies of 99.80%, 98.40%, 95.80%, 83.02%. At 80% training ratio, the top two performing models LSTM-448 and LSTM-424 are both LSTM-based with the hidden sizes of 448 and 424. Compared with the 72% training ratio, there is a significant improvement of accuracies, which reached 99.85%, 98.66%, 96.26%, 84.77% for the best individual model and 99.88%, 98.91%, 96.96%, 86.78% for the best ensemble.",
        "subjects": [
            "hep-th",
            "cs.LG",
            "math.AG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17410",
        "abstract url": "https://arxiv.org/abs/2405.17410",
        "title": "The Peripatetic Hater: Predicting Movement Among Hate Subreddits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI",
                "cs.CY"
            ]
        ],
        "abstract": "Many online hate groups exist to disparage others based on race, gender identity, sex, or other characteristics. The accessibility of these communities allows users to join multiple types of hate groups (e.g., a racist community and misogynistic community), which calls into question whether these peripatetic users could be further radicalized compared to users that stay in one type of hate group. However, little is known about the dynamics of joining multiple types of hate groups, nor the effect of these groups on peripatetic users. In this paper, we develop a new method to classify hate subreddits, and the identities they disparage, which we use to better understand how users become peripatetic (join different types of hate subreddits). The hate classification technique utilizes human-validated LLMs to extract the protected identities attacked, if any, across 168 subreddits. We then cluster identity-attacking subreddits to discover three broad categories of hate: racist, anti-LGBTQ, and misogynistic. We show that becoming active in a user's first hate subreddit can cause them to become active in additional hate subreddits of a different category. We also find that users who join additional hate subreddits, especially of a different category, become more active in hate subreddits as a whole and develop a wider hate group lexicon. We are therefore motivated to train an AI model that we find usefully predicts the hate categories users will become active in based on post text read and written. The accuracy of this model may be partly driven by peripatetic users often using the language of hate subreddits they eventually join. Overall, these results highlight the unique risks associated with hate communities on a social media platform, as discussion of alternative targets of hate may lead users to target more protected identities.",
        "subjects": [
            "cs.SI",
            "cs.CY",
            "cs.HC"
        ],
        "comment": "11 pages, 6 figures"
    },
    {
        "paper id": "2405.17412",
        "abstract url": "https://arxiv.org/abs/2405.17412",
        "title": "Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper shows that the dimensionality reduction methods, UMAP and t-SNE, can be approximately recast as MAP inference methods corresponding to a generalized Wishart-based model introduced in ProbDR. This interpretation offers deeper theoretical insights into these algorithms, while introducing tools with which similar dimensionality reduction methods can be studied.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Workshop preprint"
    },
    {
        "paper id": "2405.17512",
        "abstract url": "https://arxiv.org/abs/2405.17512",
        "title": "On Fairness of Low-Rank Adaptation of Large Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Low-rank adaptation of large models, particularly LoRA, has gained traction due to its computational efficiency. This efficiency, contrasted with the prohibitive costs of full-model fine-tuning, means that practitioners often turn to LoRA and sometimes without a complete understanding of its ramifications. In this study, we focus on fairness and ask whether LoRA has an unexamined impact on utility, calibration, and resistance to membership inference across different subgroups (e.g., genders, races, religions) compared to a full-model fine-tuning baseline. We present extensive experiments across vision and language domains and across classification and generation tasks using ViT-Base, Swin-v2-Large, Llama-2 7B, and Mistral 7B. Intriguingly, experiments suggest that while one can isolate cases where LoRA exacerbates model bias across subgroups, the pattern is inconsistent -- in many cases, LoRA has equivalent or even improved fairness compared to the base model or its full fine-tuning baseline. We also examine the complications of evaluating fine-tuning fairness relating to task design and model token bias, calling for more careful fairness evaluations in future work.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17516",
        "abstract url": "https://arxiv.org/abs/2405.17516",
        "title": "Time Elastic Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We introduce and detail an atypical neural network architecture, called time elastic neural network (teNN), for multivariate time series classification. The novelty compared to classical neural network architecture is that it explicitly incorporates time warping ability, as well as a new way of considering attention. In addition, this architecture is capable of learning a dropout strategy, thus optimizing its own architecture.Behind the design of this architecture, our overall objective is threefold: firstly, we are aiming at improving the accuracy of instance based classification approaches that shows quite good performances as far as enough training data is available. Secondly we seek to reduce the computational complexity inherent to these methods to improve their scalability. Ideally, we seek to find an acceptable balance between these first two criteria. And finally, we seek to enhance the explainability of the decision provided by this kind of neural architecture.The experiment demonstrates that the stochastic gradient descent implemented to train a teNN is quite effective. To the extent that the selection of some critical meta-parameters is correct, convergence is generally smooth and fast.While maintaining good accuracy, we get a drastic gain in scalability by first reducing the required number of reference time series, i.e. the number of teNN cells required. Secondly, we demonstrate that, during the training process, the teNN succeeds in reducing the number of neurons required within each cell. Finally, we show that the analysis of the activation and attention matrices as well as the reference time series after training provides relevant information to interpret and explain the classification results.The comparative study that we have carried out and which concerns around thirty diverse and multivariate datasets shows that the teNN obtains results comparable to those of the state of the art, in particular similar to those of a network mixing LSTM and CNN architectures for example.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17529",
        "abstract url": "https://arxiv.org/abs/2405.17529",
        "title": "Clip Body and Tail Separately: High Probability Guarantees for DPSGD with Heavy Tails",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Differentially Private Stochastic Gradient Descent (DPSGD) is widely utilized to preserve training data privacy in deep learning, which first clips the gradients to a predefined norm and then injects calibrated noise into the training procedure. Existing DPSGD works typically assume the gradients follow sub-Gaussian distributions and design various clipping mechanisms to optimize training performance. However, recent studies have shown that the gradients in deep learning exhibit a heavy-tail phenomenon, that is, the tails of the gradient have infinite variance, which may lead to excessive clipping loss to the gradients with existing DPSGD mechanisms. To address this problem, we propose a novel approach, Discriminative Clipping~(DC)-DPSGD, with two key designs. First, we introduce a subspace identification technique to distinguish between body and tail gradients. Second, we present a discriminative clipping mechanism that applies different clipping thresholds for body and tail gradients to reduce the clipping loss. Under the non-convex condition, \\ourtech{} reduces the empirical gradient norm from {${\\mathbb{O}\\left(\\log^{\\max(0,\u03b8-1)}(T/\u03b4)\\log^{2\u03b8}(\\sqrt{T})\\right)}$} to {${\\mathbb{O}\\left(\\log(\\sqrt{T})\\right)}$} with heavy-tailed index $\u03b8\\geq 1/2$, iterations $T$, and arbitrary probability $\u03b4$. Extensive experiments on four real-world datasets demonstrate that our approach outperforms three baselines by up to 9.72\\% in terms of accuracy.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17534",
        "abstract url": "https://arxiv.org/abs/2405.17534",
        "title": "SMR: State Memory Replay for Long Sequence Modeling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17544",
        "abstract url": "https://arxiv.org/abs/2405.17544",
        "title": "Towards Human-AI Complementarity with Predictions Sets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Decision support systems based on prediction sets have proven to be effective at helping human experts solve classification tasks. Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets. In this paper, we first show that the prediction sets constructed using conformal prediction are, in general, suboptimal in terms of average accuracy. Then, we show that the problem of finding the optimal prediction sets under which the human experts achieve the highest average accuracy is NP-hard. More strongly, unless P = NP, we show that the problem is hard to approximate to any factor less than the size of the label set. However, we introduce a simple and efficient greedy algorithm that, for a large class of expert models and non-conformity scores, is guaranteed to find prediction sets that provably offer equal or greater performance than those constructed using conformal prediction. Further, using a simulation study with both synthetic and real expert predictions, we demonstrate that, in practice, our greedy algorithm finds near-optimal prediction sets offering greater performance than conformal prediction.",
        "subjects": [
            "cs.LG",
            "cs.CY",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17556",
        "abstract url": "https://arxiv.org/abs/2405.17556",
        "title": "Probabilistic Verification of Neural Networks using Branch and Bound",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Probabilistic verification of neural networks is concerned with formally analysing the output distribution of a neural network under a probability distribution of the inputs. Examples of probabilistic verification include verifying the demographic parity fairness notion or quantifying the safety of a neural network. We present a new algorithm for the probabilistic verification of neural networks based on an algorithm for computing and iteratively refining lower and upper bounds on probabilities over the outputs of a neural network. By applying state-of-the-art bound propagation and branch and bound techniques from non-probabilistic neural network verification, our algorithm significantly outpaces existing probabilistic verification algorithms, reducing solving times for various benchmarks from the literature from tens of minutes to tens of seconds. Furthermore, our algorithm compares favourably even to dedicated algorithms for restricted subsets of probabilistic verification. We complement our empirical evaluation with a theoretical analysis, proving that our algorithm is sound and, under mildly restrictive conditions, also complete when using a suitable set of heuristics.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "16 pages, 2 figures, 22 pages references and appendix, including 4 more figures"
    },
    {
        "paper id": "2405.17571",
        "abstract url": "https://arxiv.org/abs/2405.17571",
        "title": "Bluesky: Network Topology, Polarisation, and Algorithmic Curation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Bluesky is a nascent ``Twitter-like'' and decentralized social media network with novel features and unprecedented data access. This paper provides a characterization of the network, studying the political leaning, polarization, network structure, and algorithmic curation mechanisms of five million users. The dataset spans from the website's first release in February of 2023. Users of the new social media site are predominantly left-center leaning and share little to no links associated with questionable sources. In contrast to the homogeneous political stance, we find significant issues-based divergence by studying opinions related to the Israel-Palestine conflict. Two clear homophilic clusters emerge: Pro-Palestinian voices make up the plurality of messages related to the conflict and the proportion has increased with a lessening of interest. We investigate multiple layers of the multi-scale Bluesky network based on replies, likes, reposts, and follows, highlighting differences and similarities between the layers. We differentiate between persistent and non-persistent interactions and measure metrics of network topology over time. All networks are heavy-tailed, clustered, and connected by short paths. We showcase all feeds - algorithmic content recommenders - created for and by users. A large number of custom feeds have been created but their uptake by users is limited. Multiple popular feeds aim to provide similar feeds that are neither topical nor chronological. We conclude by claiming that Bluesky - for all its novel features - is very similar in terms of its network structure to existing and larger social media sites and provides unprecedented research opportunities for social scientists, network scientists, and political scientists alike.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17576",
        "abstract url": "https://arxiv.org/abs/2405.17576",
        "title": "Container pre-marshalling problem minimizing CV@R under uncertainty of ship arrival times",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper is concerned with the container pre-marshalling problem, which involves relocating containers in the storage area so that they can be efficiently loaded onto ships without reshuffles. In reality, however, ship arrival times are affected by various external factors, which can cause the order of container retrieval to be different from the initial plan. To represent such uncertainty, we generate multiple scenarios from a multivariate probability distribution of ship arrival times. We derive a mixed-integer linear optimization model to find an optimal container layout such that the conditional value-at-risk is minimized for the number of misplaced containers responsible for reshuffles. Moreover, we devise an exact algorithm based on the cutting-plane method to handle large-scale problems. Numerical experiments using synthetic datasets demonstrate that our method can produce high-quality container layouts compared with the conventional robust optimization model. Additionally, our algorithm can speed up the computation of solving large-scale problems.",
        "subjects": [
            "math.OC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17580",
        "abstract url": "https://arxiv.org/abs/2405.17580",
        "title": "Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The training dynamics of linear networks are well studied in two distinct setups: the lazy regime and balanced/active regime, depending on the initialization and width of the network. We provide a surprisingly simple unyfing formula for the evolution of the learned matrix that contains as special cases both lazy and balanced regimes but also a mixed regime in between the two. In the mixed regime, a part of the network is lazy while the other is balanced. More precisely the network is lazy along singular values that are below a certain threshold and balanced along those that are above the same threshold. At initialization, all singular values are lazy, allowing for the network to align itself with the task, so that later in time, when some of the singular value cross the threshold and become active they will converge rapidly (convergence in the balanced regime is notoriously difficult in the absence of alignment). The mixed regime is the `best of both worlds': it converges from any random initialization (in contrast to balanced dynamics which require special initialization), and has a low rank bias (absent in the lazy dynamics). This allows us to prove an almost complete phase diagram of training behavior as a function of the variance at initialization and the width, for a MSE training task.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17583",
        "abstract url": "https://arxiv.org/abs/2405.17583",
        "title": "Understanding Forgetting in Continual Learning with Linear Regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Continual learning, focused on sequentially learning multiple tasks, has gained significant attention recently. Despite the tremendous progress made in the past, the theoretical understanding, especially factors contributing to catastrophic forgetting, remains relatively unexplored. In this paper, we provide a general theoretical analysis of forgetting in the linear regression model via Stochastic Gradient Descent (SGD) applicable to both underparameterized and overparameterized regimes. Our theoretical framework reveals some interesting insights into the intricate relationship between task sequence and algorithmic parameters, an aspect not fully captured in previous studies due to their restrictive assumptions. Specifically, we demonstrate that, given a sufficiently large data size, the arrangement of tasks in a sequence, where tasks with larger eigenvalues in their population data covariance matrices are trained later, tends to result in increased forgetting. Additionally, our findings highlight that an appropriate choice of step size will help mitigate forgetting in both underparameterized and overparameterized settings. To validate our theoretical analysis, we conducted simulation experiments on both linear regression models and Deep Neural Networks (DNNs). Results from these simulations substantiate our theoretical findings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "To be published in The 41st International Conference on Machine Learning"
    },
    {
        "paper id": "2405.17618",
        "abstract url": "https://arxiv.org/abs/2405.17618",
        "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can introduce additional difficulty. Differing preferences can complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. To enhance training robustness, RL has adopted techniques from supervised learning, such as ensembles and layer normalization. In this work, we improve the stability of RL training by adapting the reverse cross entropy (RCE) from supervised learning for noisy data to define a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with and without added noise with especially notable performance in SPPO across different hyperparameters. Furthermore, we validate the benefits of the symmetric RL loss when using SPPO for large language models through improved performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR summarization tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17625",
        "abstract url": "https://arxiv.org/abs/2405.17625",
        "title": "Matrix Low-Rank Trust Region Policy Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Most methods in reinforcement learning use a Policy Gradient (PG) approach to learn a parametric stochastic policy that maps states to actions. The standard approach is to implement such a mapping via a neural network (NN) whose parameters are optimized using stochastic gradient descent. However, PG methods are prone to large policy updates that can render learning inefficient. Trust region algorithms, like Trust Region Policy Optimization (TRPO), constrain the policy update step, ensuring monotonic improvements. This paper introduces low-rank matrix-based models as an efficient alternative for estimating the parameters of TRPO algorithms. By gathering the stochastic policy's parameters into a matrix and applying matrix-completion techniques, we promote and enforce low rank. Our numerical studies demonstrate that low-rank matrix-based policy models effectively reduce both computational and sample complexities compared to NN models, while maintaining comparable aggregated rewards.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17626",
        "abstract url": "https://arxiv.org/abs/2405.17626",
        "title": "Matrix Low-Rank Approximation For Policy Gradient Methods",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Estimating a policy that maps states to actions is a central problem in reinforcement learning. Traditionally, policies are inferred from the so called value functions (VFs), but exact VF computation suffers from the curse of dimensionality. Policy gradient (PG) methods bypass this by learning directly a parametric stochastic policy. Typically, the parameters of the policy are estimated using neural networks (NNs) tuned via stochastic gradient descent. However, finding adequate NN architectures can be challenging, and convergence issues are common as well. In this paper, we put forth low-rank matrix-based models to estimate efficiently the parameters of PG algorithms. We collect the parameters of the stochastic policy into a matrix, and then, we leverage matrix-completion techniques to promote (enforce) low rank. We demonstrate via numerical studies how low-rank matrix-based policy models reduce the computational and sample complexities relative to NN models, while achieving a similar aggregated reward.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17627",
        "abstract url": "https://arxiv.org/abs/2405.17627",
        "title": "Salutary Labeling with Zero Human Annotation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Active learning strategically selects informative unlabeled data points and queries their ground truth labels for model training. The prevailing assumption underlying this machine learning paradigm is that acquiring these ground truth labels will optimally enhance model performance. However, this assumption may not always hold true or maximize learning capacity, particularly considering the costly labor annotations required for ground truth labels. In contrast to traditional ground truth labeling, this paper proposes salutary labeling, which automatically assigns the most beneficial labels to the most informative samples without human annotation. Specifically, we utilize the influence function, a tool for estimating sample influence, to select newly added samples and assign their salutary labels by choosing the category that maximizes their positive influence. This process eliminates the need for human annotation. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our salutary labeling approach over traditional active learning strategies. Additionally, we provide several in-depth explorations and practical applications of large language model (LLM) fine-tuning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17628",
        "abstract url": "https://arxiv.org/abs/2405.17628",
        "title": "Tensor Low-rank Approximation of Finite-horizon Value Functions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The goal of reinforcement learning is estimating a policy that maps states to actions and maximizes the cumulative reward of a Markov Decision Process (MDP). This is oftentimes achieved by estimating first the optimal (reward) value function (VF) associated with each state-action pair. When the MDP has an infinite horizon, the optimal VFs and policies are stationary under mild conditions. However, in finite-horizon MDPs, the VFs (hence, the policies) vary with time. This poses a challenge since the number of VFs to estimate grows not only with the size of the state-action space but also with the time horizon. This paper proposes a non-parametric low-rank stochastic algorithm to approximate the VFs of finite-horizon MDPs. First, we represent the (unknown) VFs as a multi-dimensional array, or tensor, where time is one of the dimensions. Then, we use rewards sampled from the MDP to estimate the optimal VFs. More precisely, we use the (truncated) PARAFAC decomposition to design an online low-rank algorithm that recovers the entries of the tensor of VFs. The size of the low-rank PARAFAC model grows additively with respect to each of its dimensions, rendering our approach efficient, as demonstrated via numerical experiments.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17637",
        "abstract url": "https://arxiv.org/abs/2405.17637",
        "title": "The Economic Implications of Large Language Model Selection on Earnings and Return on Investment: A Decision Theoretic Model",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Selecting language models in business contexts requires a careful analysis of the final financial benefits of the investment. However, the emphasis of academia and industry analysis of LLM is solely on performance. This work introduces a framework to evaluate LLMs, focusing on the earnings and return on investment aspects that should be taken into account in business decision making. We use a decision-theoretic approach to compare the financial impact of different LLMs, considering variables such as the cost per token, the probability of success in the specific task, and the gain and losses associated with LLMs use. The study reveals how the superior accuracy of more expensive models can, under certain conditions, justify a greater investment through more significant earnings but not necessarily a larger RoI. This article provides a framework for companies looking to optimize their technology choices, ensuring that investment in cutting-edge technology aligns with strategic financial objectives. In addition, we discuss how changes in operational variables influence the economics of using LLMs, offering practical insights for enterprise settings, finding that the predicted gain and loss and the different probabilities of success and failure are the variables that most impact the sensitivity of the models.",
        "subjects": [
            "cs.AI",
            "cs.CE"
        ],
        "comment": "27 pages, 13 figures"
    },
    {
        "paper id": "2405.17638",
        "abstract url": "https://arxiv.org/abs/2405.17638",
        "title": "The surprising efficiency of temporal difference learning for rare event prediction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We quantify the efficiency of temporal difference (TD) learning over the direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement learning, with an emphasis on estimation of quantities related to rare events. Policy evaluation is complicated in the rare event setting by the long timescale of the event and by the need for \\emph{relative accuracy} in estimates of very small values. Specifically, we focus on least-squares TD (LSTD) prediction for finite state Markov chains, and show that LSTD can achieve relative accuracy far more efficiently than MC. We prove a central limit theorem for the LSTD estimator and upper bound the \\emph{relative asymptotic variance} by simple quantities characterizing the connectivity of states relative to the transition probabilities between them. Using this bound, we show that, even when both the timescale of the rare event and the relative accuracy of the MC estimator are exponentially large in the number of states, LSTD maintains a fixed level of relative accuracy with a total number of observed transitions of the Markov chain that is only \\emph{polynomially} large in the number of states.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17640",
        "abstract url": "https://arxiv.org/abs/2405.17640",
        "title": "Probabilistically Plausible Counterfactual Explanations with Normalizing Flows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We present PPCEF, a novel method for generating probabilistically plausible counterfactual explanations (CFs). PPCEF advances beyond existing methods by combining a probabilistic formulation that leverages the data distribution with the optimization of plausibility within a unified framework. Compared to reference approaches, our method enforces plausibility by directly optimizing the explicit density function without assuming a particular family of parametrized distributions. This ensures CFs are not only valid (i.e., achieve class change) but also align with the underlying data's probability density. For that purpose, our approach leverages normalizing flows as powerful density estimators to capture the complex high-dimensional data distribution. Furthermore, we introduce a novel loss that balances the trade-off between achieving class change and maintaining closeness to the original instance while also incorporating a probabilistic plausibility term. PPCEF's unconstrained formulation allows for efficient gradient-based optimization with batch processing, leading to orders of magnitude faster computation compared to prior methods. Moreover, the unconstrained formulation of PPCEF allows for the seamless integration of future constraints tailored to specific counterfactual properties. Finally, extensive evaluations demonstrate PPCEF's superiority in generating high-quality, probabilistically plausible counterfactual explanations in high-dimensional tabular settings. This makes PPCEF a powerful tool for not only interpreting complex machine learning models but also for improving fairness, accountability, and trust in AI systems.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17642",
        "abstract url": "https://arxiv.org/abs/2405.17642",
        "title": "Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Growing regulatory and societal pressures demand increased transparency in AI, particularly in understanding the decisions made by complex machine learning models. Counterfactual Explanations (CFs) have emerged as a promising technique within Explainable AI (xAI), offering insights into individual model predictions. However, to understand the systemic biases and disparate impacts of AI models, it is crucial to move beyond local CFs and embrace global explanations, which offer a~holistic view across diverse scenarios and populations. Unfortunately, generating Global Counterfactual Explanations (GCEs) faces challenges in computational complexity, defining the scope of \"global,\" and ensuring the explanations are both globally representative and locally plausible. We introduce a novel unified approach for generating Local, Group-wise, and Global Counterfactual Explanations for differentiable classification models via gradient-based optimization to address these challenges. This framework aims to bridge the gap between individual and systemic insights, enabling a deeper understanding of model decisions and their potential impact on diverse populations. Our approach further innovates by incorporating a probabilistic plausibility criterion, enhancing actionability and trustworthiness. By offering a cohesive solution to the optimization and plausibility challenges in GCEs, our work significantly advances the interpretability and accountability of AI models, marking a step forward in the pursuit of transparent AI.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17666",
        "abstract url": "https://arxiv.org/abs/2405.17666",
        "title": "Structured Partial Stochasticity in Bayesian Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian neural network posterior distributions have a great number of modes that correspond to the same network function. The abundance of such modes can make it difficult for approximate inference methods to do their job. Recent work has demonstrated the benefits of partial stochasticity for approximate inference in Bayesian neural networks; inference can be less costly and performance can sometimes be improved. I propose a structured way to select the deterministic subset of weights that removes neuron permutation symmetries, and therefore the corresponding redundant posterior modes. With a drastically simplified posterior distribution, the performance of existing approximate inference schemes is found to be greatly improved.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17667",
        "abstract url": "https://arxiv.org/abs/2405.17667",
        "title": "Hunting for Polluted White Dwarfs and Other Treasures with Gaia XP Spectra and Unsupervised Machine Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "White dwarfs (WDs) polluted by exoplanetary material provide the unprecedented opportunity to directly observe the interiors of exoplanets. However, spectroscopic surveys are often limited by brightness constraints, and WDs tend to be very faint, making detections of large populations of polluted WDs difficult. In this paper, we aim to increase considerably the number of WDs with multiple metals in their atmospheres. Using 96,134 WDs with Gaia DR3 BP/RP (XP) spectra, we constructed a 2D map using an unsupervised machine learning technique called Uniform Manifold Approximation and Projection (UMAP) to organize the WDs into identifiable spectral regions. The polluted WDs are among the distinct spectral groups identified in our map. We have shown that this selection method could potentially increase the number of known WDs with 5 or more metal species in their atmospheres by an order of magnitude. Such systems are essential for characterizing exoplanet diversity and geology.",
        "subjects": [
            "astro-ph.SR",
            "astro-ph.EP",
            "cs.LG"
        ],
        "comment": "16 pages, 10 figures, submitted to ApJ on May 20, 2024"
    },
    {
        "paper id": "2405.17672",
        "abstract url": "https://arxiv.org/abs/2405.17672",
        "title": "Exploring Loss Design Techniques For Decision Tree Robustness To Label Noise",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the real world, data is often noisy, affecting not only the quality of features but also the accuracy of labels. Current research on mitigating label errors stems primarily from advances in deep learning, and a gap exists in exploring interpretable models, particularly those rooted in decision trees. In this study, we investigate whether ideas from deep learning loss design can be applied to improve the robustness of decision trees. In particular, we show that loss correction and symmetric losses, both standard approaches, are not effective. We argue that other directions need to be explored to improve the robustness of decision trees to label noise.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17691",
        "abstract url": "https://arxiv.org/abs/2405.17691",
        "title": "Ontology-Enhanced Decision-Making for Autonomous Agents in Dynamic and Partially Observable Environments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Agents, whether software or hardware, perceive their environment through sensors and act using actuators, often operating in dynamic, partially observable settings. They face challenges like incomplete and noisy data, unforeseen situations, and the need to adapt goals in real-time. Traditional reasoning and ML methods, including Reinforcement Learning (RL), help but are limited by data needs, predefined goals, and extensive exploration periods. Ontologies offer a solution by integrating diverse information sources, enhancing decision-making in complex environments. This thesis introduces an ontology-enhanced decision-making model (OntoDeM) for autonomous agents. OntoDeM enriches agents' domain knowledge, allowing them to interpret unforeseen events, generate or adapt goals, and make better decisions. Key contributions include: 1. An ontology-based method to improve agents' real-time observations using prior knowledge. 2. The OntoDeM model for handling dynamic, unforeseen situations by evolving or generating new goals. 3. Implementation and evaluation in four real-world applications, demonstrating its effectiveness. Compared to traditional and advanced learning algorithms, OntoDeM shows superior performance in improving agents' observations and decision-making in dynamic, partially observable environments.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "PhD thesis"
    },
    {
        "paper id": "2405.17693",
        "abstract url": "https://arxiv.org/abs/2405.17693",
        "title": "Tamed Langevin sampling under weaker conditions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Motivated by applications to deep learning which often fail standard Lipschitz smoothness requirements, we examine the problem of sampling from distributions that are not log-concave and are only weakly dissipative, with log-gradients allowed to grow superlinearly at infinity. In terms of structure, we only assume that the target distribution satisfies either a log-Sobolev or a Poincar\u00e9 inequality and a local Lipschitz smoothness assumption with modulus growing possibly polynomially at infinity. This set of assumptions greatly exceeds the operational limits of the \"vanilla\" unadjusted Langevin algorithm (ULA), making sampling from such distributions a highly involved affair. To account for this, we introduce a taming scheme which is tailored to the growth and decay properties of the target distribution, and we provide explicit non-asymptotic guarantees for the proposed sampler in terms of the Kullback-Leibler (KL) divergence, total variation, and Wasserstein distance to the target distribution.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.NA",
            "math.OC",
            "math.PR"
        ],
        "comment": "32 pages, 2 figures"
    },
    {
        "paper id": "2405.17697",
        "abstract url": "https://arxiv.org/abs/2405.17697",
        "title": "P4: Towards private, personalized, and Peer-to-Peer learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Personalized learning is a proposed approach to address the problem of data heterogeneity in collaborative machine learning. In a decentralized setting, the two main challenges of personalization are client clustering and data privacy. In this paper, we address these challenges by developing P4 (Personalized Private Peer-to-Peer) a method that ensures that each client receives a personalized model while maintaining differential privacy guarantee of each client's local dataset during and after the training. Our approach includes the design of a lightweight algorithm to identify similar clients and group them in a private, peer-to-peer (P2P) manner. Once grouped, we develop differentially-private knowledge distillation for clients to co-train with minimal impact on accuracy. We evaluate our proposed method on three benchmark datasets (FEMNIST or Federated EMNIST, CIFAR-10 and CIFAR-100) and two different neural network architectures (Linear and CNN-based networks) across a range of privacy parameters. The results demonstrate the potential of P4, as it outperforms the state-of-the-art of differential private P2P by up to 40 percent in terms of accuracy. We also show the practicality of P4 by implementing it on resource constrained devices, and validating that it has minimal overhead, e.g., about 7 seconds to run collaborative training between two clients.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17700",
        "abstract url": "https://arxiv.org/abs/2405.17700",
        "title": "Learning Social Welfare Functions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Is it possible to understand or imitate a policy maker's rationale by looking at past decisions they made? We formalize this question as the problem of learning social welfare functions belonging to the well-studied family of power mean functions. We focus on two learning tasks; in the first, the input is vectors of utilities of an action (decision or policy) for individuals in a group and their associated social welfare as judged by a policy maker, whereas in the second, the input is pairwise comparisons between the welfares associated with a given pair of utility vectors. We show that power mean functions are learnable with polynomial sample complexity in both cases, even if the comparisons are social welfare information is noisy. Finally, we design practical algorithms for these tasks and evaluate their performance.",
        "subjects": [
            "cs.GT",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17739",
        "abstract url": "https://arxiv.org/abs/2405.17739",
        "title": "The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Novice programmers often struggle through programming problem solving due to a lack of metacognitive awareness and strategies. Previous research has shown that novices can encounter multiple metacognitive difficulties while programming. Novices are typically unaware of how these difficulties are hindering their progress. Meanwhile, many novices are now programming with generative AI (GenAI), which can provide complete solutions to most introductory programming problems, code suggestions, hints for next steps when stuck, and explain cryptic error messages. Its impact on novice metacognition has only started to be explored. Here we replicate a previous study that examined novice programming problem solving behavior and extend it by incorporating GenAI tools. Through 21 lab sessions consisting of participant observation, interview, and eye tracking, we explore how novices are coding with GenAI tools. Although 20 of 21 students completed the assigned programming problem, our findings show an unfortunate divide in the use of GenAI tools between students who accelerated and students who struggled. Students who accelerated were able to use GenAI to create code they already intended to make and were able to ignore unhelpful or incorrect inline code suggestions. But for students who struggled, our findings indicate that previously known metacognitive difficulties persist, and that GenAI unfortunately can compound them and even introduce new metacognitive difficulties. Furthermore, struggling students often expressed cognitive dissonance about their problem solving ability, thought they performed better than they did, and finished with an illusion of competence. Based on our observations from both groups, we propose ways to scaffold the novice GenAI experience and make suggestions for future work.",
        "subjects": [
            "cs.AI",
            "cs.HC"
        ],
        "comment": "Accepted to ICER 2024"
    },
    {
        "paper id": "2405.17741",
        "abstract url": "https://arxiv.org/abs/2405.17741",
        "title": "LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent literature has found that an effective method to customize or further improve large language models (LLMs) is to add dynamic adapters, such as low-rank adapters (LoRA) with Mixture-of-Experts (MoE) structures. Though such dynamic adapters incur modest computational complexity, they surprisingly lead to huge inference latency overhead, slowing down the decoding speed by 2.5+ times. In this paper, we analyze the fine-grained costs of the dynamic adapters and find that the fragmented CUDA kernel calls are the root cause. Therefore, we propose LoRA-Switch, a system-algorithm co-designed architecture for efficient dynamic adapters. Unlike most existing dynamic structures that adopt layer-wise or block-wise dynamic routing, LoRA-Switch introduces a token-wise routing mechanism. It switches the LoRA adapters and weights for each token and merges them into the backbone for inference. For efficiency, this switching is implemented with an optimized CUDA kernel, which fuses the merging operations for all LoRA adapters at once. Based on experiments with popular open-source LLMs on common benchmarks, our approach has demonstrated similar accuracy improvement as existing dynamic adapters, while reducing the decoding latency by more than 2.4 times.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17761",
        "abstract url": "https://arxiv.org/abs/2405.17761",
        "title": "Double Variance Reduction: A Smoothing Trick for Composite Optimization Problems without First-Order Gradient",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Variance reduction techniques are designed to decrease the sampling variance, thereby accelerating convergence rates of first-order (FO) and zeroth-order (ZO) optimization methods. However, in composite optimization problems, ZO methods encounter an additional variance called the coordinate-wise variance, which stems from the random gradient estimation. To reduce this variance, prior works require estimating all partial derivatives, essentially approximating FO information. This approach demands O(d) function evaluations (d is the dimension size), which incurs substantial computational costs and is prohibitive in high-dimensional scenarios. This paper proposes the Zeroth-order Proximal Double Variance Reduction (ZPDVR) method, which utilizes the averaging trick to reduce both sampling and coordinate-wise variances. Compared to prior methods, ZPDVR relies solely on random gradient estimates, calls the stochastic zeroth-order oracle (SZO) in expectation $\\mathcal{O}(1)$ times per iteration, and achieves the optimal $\\mathcal{O}(d(n + \u03ba)\\log (\\frac{1}\u03b5))$ SZO query complexity in the strongly convex and smooth setting, where $\u03ba$ represents the condition number and $\u03b5$ is the desired accuracy. Empirical results validate ZPDVR's linear convergence and demonstrate its superior performance over other related methods.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17776",
        "abstract url": "https://arxiv.org/abs/2405.17776",
        "title": "The Binary Quantized Neural Network for Dense Prediction via Specially Designed Upsampling and Attention",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning-based information processing consumes long time and requires huge computing resources, especially for dense prediction tasks which require an output for each pixel, like semantic segmentation and salient object detection. There are mainly two challenges for quantization of dense prediction tasks. Firstly, directly applying the upsampling operation that dense prediction tasks require is extremely crude and causes unacceptable accuracy reduction. Secondly, the complex structure of dense prediction networks means it is difficult to maintain a fast speed as well as a high accuracy when performing quantization. In this paper, we propose an effective upsampling method and an efficient attention computation strategy to transfer the success of the binary neural networks (BNN) from single prediction tasks to dense prediction tasks. Firstly, we design a simple and robust multi-branch parallel upsampling structure to achieve the high accuracy. Then we further optimize the attention method which plays an important role in segmentation but has huge computation complexity. Our attention method can reduce the computational complexity by a factor of one hundred times but retain the original effect. Experiments on Cityscapes, KITTI road, and ECSSD fully show the effectiveness of our work.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "30 pages, 6 figures"
    },
    {
        "paper id": "2405.17796",
        "abstract url": "https://arxiv.org/abs/2405.17796",
        "title": "Offline Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Motivated by the recent discovery of a statistical and computational reduction from contextual bandits to offline regression (Simchi-Levi and Xu, 2021), we address the general (stochastic) Contextual Markov Decision Process (CMDP) problem with horizon H (as known as CMDP with H layers). In this paper, we introduce a reduction from CMDPs to offline density estimation under the realizability assumption, i.e., a model class M containing the true underlying CMDP is provided in advance. We develop an efficient, statistically near-optimal algorithm requiring only O(HlogT) calls to an offline density estimation algorithm (or oracle) across all T rounds of interaction. This number can be further reduced to O(HloglogT) if T is known in advance. Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class. A notable feature of our algorithm is the design of a layerwise exploration-exploitation tradeoff tailored to address the layerwise structure of CMDPs. Additionally, our algorithm is versatile and applicable to pure exploration tasks in reward-free reinforcement learning.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17823",
        "abstract url": "https://arxiv.org/abs/2405.17823",
        "title": "Spectral Truncation Kernels: Noncommutativity in $C^*$-algebraic Kernel Machines",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we propose a new class of positive definite kernels based on the spectral truncation, which has been discussed in the fields of noncommutative geometry and $C^*$-algebra. We focus on kernels whose inputs and outputs are functions and generalize existing kernels, such as polynomial, product, and separable kernels, by introducing a truncation parameter $n$ that describes the noncommutativity of the products appearing in the kernels. When $n$ goes to infinity, the proposed kernels tend to the existing commutative kernels. If $n$ is finite, they exhibit different behavior, and the noncommutativity induces interactions along the data function domain. We show that the truncation parameter $n$ is a governing factor leading to performance enhancement: by setting an appropriate $n$, we can balance the representation power and the complexity of the representation space. The flexibility of the proposed class of kernels allows us to go beyond previous commutative kernels.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.OA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.19369",
        "abstract url": "https://arxiv.org/abs/2405.19369",
        "title": "Sublinear Cuts are the Exception in BDF-GIRGs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "The introduction of geometry has proven instrumental in the efforts towards more realistic models for real-world networks. In Geometric Inhomogeneous Random Graphs (GIRGs), Euclidean Geometry induces clustering of the vertices, which is widely observed in networks in the wild. Euclidean Geometry in multiple dimensions however restricts proximity of vertices to those cases where vertices are close in each coordinate. We introduce a large class of GIRG extensions, called BDF-GIRGs, which capture arbitrary hierarchies of the coordinates within the distance function of the vertex feature space. These distance functions have the potential to allow more realistic modeling of the complex formation of social ties in real-world networks, where similarities between people lead to connections. Here, similarity with respect to certain features, such as familial kinship or a shared workplace, suffices for the formation of ties. It is known that - while many key properties of GIRGs, such as log-log average distance and sparsity, are independent of the distance function - the Euclidean metric induces small separators, i.e. sublinear cuts of the unique giant component in GIRGs, whereas no such sublinear separators exist under the component-wise minimum distance. Building on work of Lengler and Todorovi\u0107, we give a complete classification for the existence of small separators in BDF-GIRGs. We further show that BDF-GIRGs all fulfill a stochastic triangle inequality and thus also exhibit clustering.",
        "subjects": [
            "cs.SI",
            "math.CO",
            "math.MG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.20350",
        "abstract url": "https://arxiv.org/abs/2405.20350",
        "title": "Linear Function Approximation as a Computationally Efficient Method to Solve Classical Reinforcement Learning Challenges",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural Network based approximations of the Value function make up the core of leading Policy Based methods such as Trust Regional Policy Optimization (TRPO) and Proximal Policy Optimization (PPO). While this adds significant value when dealing with very complex environments, we note that in sufficiently low State and action space environments, a computationally expensive Neural Network architecture offers marginal improvement over simpler Value approximation methods. We present an implementation of Natural Actor Critic algorithms with actor updates through Natural Policy Gradient methods. This paper proposes that Natural Policy Gradient (NPG) methods with Linear Function Approximation as a paradigm for value approximation may surpass the performance and speed of Neural Network based models such as TRPO and PPO within these environments. Over Reinforcement Learning benchmarks Cart Pole and Acrobot, we observe that our algorithm trains much faster than complex neural network architectures, and obtains an equivalent or greater result. This allows us to recommend the use of NPG methods with Linear Function Approximation over TRPO and PPO for both traditional and sparse reward low dimensional problems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.20769",
        "abstract url": "https://arxiv.org/abs/2405.20769",
        "title": "Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of computing tight privacy guarantees for the composition of subsampled differentially private mechanisms. Recent algorithms can numerically compute the privacy parameters to arbitrary precision but must be carefully applied. Our main contribution is to address two common points of confusion. First, some privacy accountants assume that the privacy guarantees for the composition of a subsampled mechanism are determined by self-composing the worst-case datasets for the uncomposed mechanism. We show that this is not true in general. Second, Poisson subsampling is sometimes assumed to have similar privacy guarantees compared to sampling without replacement. We show that the privacy guarantees may in fact differ significantly between the two sampling schemes. In particular, we give an example of hyperparameters that result in $\\varepsilon \\approx 1$ for Poisson subsampling and $\\varepsilon > 10$ for sampling without replacement. This occurs for some parameters that could realistically be chosen for DP-SGD.",
        "subjects": [
            "cs.CR",
            "cs.DS",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16848",
        "abstract url": "https://arxiv.org/abs/2405.16848",
        "title": "A re-calibration method for object detection with multi-modal alignment bias in autonomous driving",
        "rating": "0",
        "keywords": [
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-modal object detection in autonomous driving has achieved great breakthroughs due to the usage of fusing complementary information from different sensors. The calibration in fusion between sensors such as LiDAR and camera is always supposed to be precise in previous work. However, in reality, calibration matrices are fixed when the vehicles leave the factory, but vibration, bumps, and data lags may cause calibration bias. As the research on the calibration influence on fusion detection performance is relatively few, flexible calibration dependency multi-sensor detection method has always been attractive. In this paper, we conducted experiments on SOTA detection method EPNet++ and proved slight bias on calibration can reduce the performance seriously. We also proposed a re-calibration model based on semantic segmentation which can be combined with a detection algorithm to improve the performance and robustness of multi-modal calibration bias.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2405.16858",
        "abstract url": "https://arxiv.org/abs/2405.16858",
        "title": "Estimating Depth of Monocular Panoramic Image with Teacher-Student Model Fusing Equirectangular and Spherical Representations",
        "rating": "0",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Disconnectivity and distortion are the two problems which must be coped with when processing 360 degrees equirectangular images. In this paper, we propose a method of estimating the depth of monocular panoramic image with a teacher-student model fusing equirectangular and spherical representations. In contrast with the existing methods fusing an equirectangular representation with a cube map representation or tangent representation, a spherical representation is a better choice because a sampling on a sphere is more uniform and can also cope with distortion more effectively. In this processing, a novel spherical convolution kernel computing with sampling points on a sphere is developed to extract features from the spherical representation, and then, a Segmentation Feature Fusion(SFF) methodology is utilized to combine the features with ones extracted from the equirectangular representation. In contrast with the existing methods using a teacher-student model to obtain a lighter model of depth estimation, we use a teacher-student model to learn the latent features of depth images. This results in a trained model which estimates the depth map of an equirectangular image using not only the feature maps extracted from an input equirectangular image but also the distilled knowledge learnt from the ground truth of depth map of a training set. In experiments, the proposed method is tested on several well-known 360 monocular depth estimation benchmark datasets, and outperforms the existing methods for the most evaluation indexes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16869",
        "abstract url": "https://arxiv.org/abs/2405.16869",
        "title": "Mixture of Modality Knowledge Experts for Robust Multi-modal Knowledge Graph Completion",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Multi-modal knowledge graph completion (MMKGC) aims to automatically discover new knowledge triples in the given multi-modal knowledge graphs (MMKGs), which is achieved by collaborative modeling the structural information concealed in massive triples and the multi-modal features of the entities. Existing methods tend to focus on crafting elegant entity-wise multi-modal fusion strategies, yet they overlook the utilization of multi-perspective features concealed within the modalities under diverse relational contexts. To address this issue, we introduce a novel MMKGC framework with Mixture of Modality Knowledge experts (MoMoK for short) to learn adaptive multi-modal embedding under intricate relational contexts. We design relation-guided modality knowledge experts to acquire relation-aware modality embeddings and integrate the predictions from multi-modalities to achieve comprehensive decisions. Additionally, we disentangle the experts by minimizing their mutual information. Experiments on four public MMKG benchmarks demonstrate the outstanding performance of MoMoK under complex scenarios.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Work in progress. Code and data will be released at https://github.com/zjukg/MoMoK"
    },
    {
        "paper id": "2405.16890",
        "abstract url": "https://arxiv.org/abs/2405.16890",
        "title": "PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating compact and sharply detailed 3D meshes poses a significant challenge for current 3D generative models. Different from extracting dense meshes from neural representation, some recent works try to model the native mesh distribution (i.e., a set of triangles), which generates more compact results as humans crafted. However, due to the complexity and variety of mesh topology, these methods are typically limited to small datasets with specific categories and are hard to extend. In this paper, we introduce a generic and scalable mesh generation framework PivotMesh, which makes an initial attempt to extend the native mesh generation to large-scale datasets. We employ a transformer-based auto-encoder to encode meshes into discrete tokens and decode them from face level to vertex level hierarchically. Subsequently, to model the complex typology, we first learn to generate pivot vertices as coarse mesh representation and then generate the complete mesh tokens with the same auto-regressive Transformer. This reduces the difficulty compared with directly modeling the mesh distribution and further improves the model controllability. PivotMesh demonstrates its versatility by effectively learning from both small datasets like Shapenet, and large-scale datasets like Objaverse and Objaverse-xl. Extensive experiments indicate that PivotMesh can generate compact and sharp 3D meshes across various categories, highlighting its great potential for native mesh modeling.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project website: https://whaohan.github.io/pivotmesh"
    },
    {
        "paper id": "2405.16909",
        "abstract url": "https://arxiv.org/abs/2405.16909",
        "title": "A Cross-Dataset Study for Text-based 3D Human Motion Retrieval",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We provide results of our study on text-based 3D human motion retrieval and particularly focus on cross-dataset generalization. Due to practical reasons such as dataset-specific human body representations, existing works typically benchmarkby training and testing on partitions from the same dataset. Here, we employ a unified SMPL body format for all datasets, which allows us to perform training on one dataset, testing on the other, as well as training on a combination of datasets. Our results suggest that there exist dataset biases in standard text-motion benchmarks such as HumanML3D, KIT Motion-Language, and BABEL. We show that text augmentations help close the domain gap to some extent, but the gap remains. We further provide the first zero-shot action recognition results on BABEL, without using categorical action labels during training, opening up a new avenue for future research.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16923",
        "abstract url": "https://arxiv.org/abs/2405.16923",
        "title": "SA-GS: Semantic-Aware Gaussian Splatting for Large Scene Reconstruction with Geometry Constrain",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the emergence of Gaussian Splats, recent efforts have focused on large-scale scene geometric reconstruction. However, most of these efforts either concentrate on memory reduction or spatial space division, neglecting information in the semantic space. In this paper, we propose a novel method, named SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware 3D Gaussian Splats. Specifically, we leverage prior information stored in large vision models such as SAM and DINO to generate semantic masks. We then introduce a geometric complexity measurement function to serve as soft regularization, guiding the shape of each Gaussian Splat within specific semantic areas. Additionally, we present a method that estimates the expected number of Gaussian Splats in different semantic areas, effectively providing a lower bound for Gaussian Splats in these areas. Subsequently, we extract the point cloud using a novel probability density-based extraction method, transforming Gaussian Splats into a point cloud crucial for downstream tasks. Our method also offers the potential for detailed semantic inquiries while maintaining high image-based reconstruction results. We provide extensive experiments on publicly available large-scale scene reconstruction datasets with highly accurate point clouds as ground truth and our novel dataset. Our results demonstrate the superiority of our method over current state-of-the-art Gaussian Splats reconstruction methods by a significant margin in terms of geometric-based measurement metrics. Code and additional results will soon be available on our project page.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Might need more comparison, will be add later"
    },
    {
        "paper id": "2405.16933",
        "abstract url": "https://arxiv.org/abs/2405.16933",
        "title": "Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-Augmented Generation (RAG) offers a cost-effective approach to injecting real-time knowledge into large language models (LLMs). Nevertheless, constructing and validating high-quality knowledge repositories require considerable effort. We propose a pre-retrieval framework named Pseudo-Graph Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students by providing them with abundant raw reading materials and encouraging them to engage in autonomous reading to record factual information in their own words. The resulting concise, well-organized mental indices are interconnected through common topics or complementary facts to form a pseudo-graph database. During the retrieval phase, PG-RAG mimics the human behavior in flipping through notes, identifying fact paths and subsequently exploring the related contexts. Adhering to the principle of the path taken by many is the best, it integrates highly corroborated fact paths to provide a structured and refined sub-graph assisting LLMs. We validated PG-RAG on three specialized question-answering datasets. In single-document tasks, PG-RAG significantly outperformed the current best baseline, KGP-LLaMA, across all key evaluation metrics, with an average overall performance improvement of 11.6%. Specifically, its BLEU score increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In multi-document scenarios, the average metrics of PG-RAG were at least 2.35% higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed stable improvements of around 7.55% and 12.75%, respectively. Our code: https://github.com/IAAR-Shanghai/PGRAG.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16940",
        "abstract url": "https://arxiv.org/abs/2405.16940",
        "title": "Adversarial Attacks on Both Face Recognition and Face Anti-spoofing Models",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Adversarial attacks on Face Recognition (FR) systems have proven highly effective in compromising pure FR models, yet adversarial examples may be ineffective to the complete FR systems as Face Anti-Spoofing (FAS) models are often incorporated and can detect a significant number of them. To address this under-explored and essential problem, we propose a novel setting of adversarially attacking both FR and FAS models simultaneously, aiming to enhance the practicability of adversarial attacks on FR systems. In particular, we introduce a new attack method, namely Style-aligned Distribution Biasing (SDB), to improve the capacity of black-box attacks on both FR and FAS models. Specifically, our SDB framework consists of three key components. Firstly, to enhance the transferability of FAS models, we design a Distribution-aware Score Biasing module to optimize adversarial face examples away from the distribution of spoof images utilizing scores. Secondly, to mitigate the substantial style differences between live images and adversarial examples initialized with spoof images, we introduce an Instance Style Alignment module that aligns the style of adversarial examples with live images. In addition, to alleviate the conflicts between the gradients of FR and FAS models, we propose a Gradient Consistency Maintenance module to minimize disparities between the gradients using Hessian approximation. Extensive experiments showcase the superiority of our proposed attack method to state-of-the-art adversarial attacks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16947",
        "abstract url": "https://arxiv.org/abs/2405.16947",
        "title": "Zero-Shot Video Semantic Segmentation based on Pre-Trained Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce the first zero-shot approach for Video Semantic Segmentation (VSS) based on pre-trained diffusion models. A growing research direction attempts to employ diffusion models to perform downstream vision tasks by exploiting their deep understanding of image semantics. Yet, the majority of these approaches have focused on image-related tasks like semantic correspondence and segmentation, with less emphasis on video tasks such as VSS. Ideally, diffusion-based image semantic segmentation approaches can be applied to videos in a frame-by-frame manner. However, we find their performance on videos to be subpar due to the absence of any modeling of temporal information inherent in the video data. To this end, we tackle this problem and introduce a framework tailored for VSS based on pre-trained image and video diffusion models. We propose building a scene context model based on the diffusion features, where the model is autoregressively updated to adapt to scene changes. This context model predicts per-frame coarse segmentation maps that are temporally consistent. To refine these maps further, we propose a correspondence-based refinement strategy that aggregates predictions temporally, resulting in more confident predictions. Finally, we introduce a masked modulation approach to upsample the coarse maps to the full resolution at a high quality. Experiments show that our proposed approach outperforms existing zero-shot image semantic segmentation approaches significantly on various VSS benchmarks without any training or fine-tuning. Moreover, it rivals supervised VSS approaches on the VSPW dataset despite not being explicitly trained for VSS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project webpage: https://qianwangx.github.io/VidSeg_diffusion/"
    },
    {
        "paper id": "2405.16952",
        "abstract url": "https://arxiv.org/abs/2405.16952",
        "title": "A Variance-Preserving Interpolation Approach for Diffusion Models with Applications to Single Channel Speech Enhancement and Recognition",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "In this paper, we propose a variance-preserving interpolation framework to improve diffusion models for single-channel speech enhancement (SE) and automatic speech recognition (ASR). This new variance-preserving interpolation diffusion model (VPIDM) approach requires only 25 iterative steps and obviates the need for a corrector, an essential element in the existing variance-exploding interpolation diffusion model (VEIDM). Two notable distinctions between VPIDM and VEIDM are the scaling function of the mean of state variables and the constraint imposed on the variance relative to the mean's scale. We conduct a systematic exploration of the theoretical mechanism underlying VPIDM and develop insights regarding VPIDM's applications in SE and ASR using VPIDM as a frontend. Our proposed approach, evaluated on two distinct data sets, demonstrates VPIDM's superior performances over conventional discriminative SE algorithms. Furthermore, we assess the performance of the proposed model under varying signal-to-noise ratio (SNR) levels. The investigation reveals VPIDM's improved robustness in target noise elimination when compared to VEIDM. Furthermore, utilizing the mid-outputs of both VPIDM and VEIDM results in enhanced ASR accuracies, thereby highlighting the practical efficacy of our proposed approach.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16960",
        "abstract url": "https://arxiv.org/abs/2405.16960",
        "title": "DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to Unsupervised Monocular Depth Estimation",
        "rating": "0",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "There has been a recent surge of interest in learning to perceive depth from monocular videos in an unsupervised fashion. A key challenge in this field is achieving robust and accurate depth estimation in challenging scenarios, particularly in regions with weak textures or where dynamic objects are present. This study makes three major contributions by delving deeply into dense correspondence priors to provide existing frameworks with explicit geometric constraints. The first novelty is a contextual-geometric depth consistency loss, which employs depth maps triangulated from dense correspondences based on estimated ego-motion to guide the learning of depth perception from contextual information, since explicitly triangulated depth maps capture accurate relative distances among pixels. The second novelty arises from the observation that there exists an explicit, deducible relationship between optical flow divergence and depth gradient. A differential property correlation loss is, therefore, designed to refine depth estimation with a specific emphasis on local variations. The third novelty is a bidirectional stream co-adjustment strategy that enhances the interaction between rigid and optical flows, encouraging the former towards more accurate correspondence and making the latter more adaptable across various scenarios under the static scene hypotheses. DCPI-Depth, a framework that incorporates all these innovative components and couples two bidirectional and collaborative streams, achieves state-of-the-art performance and generalizability across multiple public datasets, outperforming all existing prior arts. Specifically, it demonstrates accurate depth estimation in texture-less and dynamic regions, and shows more reasonable smoothness.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "13 pages, 7 figures"
    },
    {
        "paper id": "2405.17004",
        "abstract url": "https://arxiv.org/abs/2405.17004",
        "title": "Efficient Visual Fault Detection for Freight Train via Neural Architecture Search with Data Volume Robustness",
        "rating": "0",
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning-based fault detection methods have achieved significant success. In visual fault detection of freight trains, there exists a large characteristic difference between inter-class components (scale variance) but intra-class on the contrary, which entails scale-awareness for detectors. Moreover, the design of task-specific networks heavily relies on human expertise. As a consequence, neural architecture search (NAS) that automates the model design process gains considerable attention because of its promising performance. However, NAS is computationally intensive due to the large search space and huge data volume. In this work, we propose an efficient NAS-based framework for visual fault detection of freight trains to search for the task-specific detection head with capacities of multi-scale representation. First, we design a scale-aware search space for discovering an effective receptive field in the head. Second, we explore the robustness of data volume to reduce search costs based on the specifically designed search space, and a novel sharing strategy is proposed to reduce memory and further improve search efficiency. Extensive experimental results demonstrate the effectiveness of our method with data volume robustness, which achieves 46.8 and 47.9 mAP on the Bottom View and Side View datasets, respectively. Our framework outperforms the state-of-the-art approaches and linearly decreases the search costs with reduced data volumes.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "11 pages, 8 figures"
    },
    {
        "paper id": "2405.17028",
        "abstract url": "https://arxiv.org/abs/2405.17028",
        "title": "RSET: Remapping-based Sorting Method for Emotion Transfer Speech Synthesis",
        "rating": "0",
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Although current Text-To-Speech (TTS) models are able to generate high-quality speech samples, there are still challenges in developing emotion intensity controllable TTS. Most existing TTS models achieve emotion intensity control by extracting intensity information from reference speeches. Unfortunately, limited by the lack of modeling for intra-class emotion intensity and the model's information decoupling capability, the generated speech cannot achieve fine-grained emotion intensity control and suffers from information leakage issues. In this paper, we propose an emotion transfer TTS model, which defines a remapping-based sorting method to model intra-class relative intensity information, combined with Mutual Information (MI) to decouple speaker and emotion information, and synthesizes expressive speeches with perceptible intensity differences. Experiments show that our model achieves fine-grained emotion control while preserving speaker information.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Accepted by the 8th APWeb-WAIM International Joint Conference on Web and Big Data"
    },
    {
        "paper id": "2405.17037",
        "abstract url": "https://arxiv.org/abs/2405.17037",
        "title": "BDC-Occ: Binarized Deep Convolution Unit For Binarized Occupancy Network",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing 3D occupancy networks demand significant hardware resources, hindering the deployment of edge devices. Binarized Neural Networks (BNN) offer substantially reduced computational and memory requirements. However, their performance decreases notably compared to full-precision networks. Moreover, it is challenging to enhance the performance of binarized models by increasing the number of binarized convolutional layers, which limits their practicability for 3D occupancy prediction. To bridge these gaps, we propose a novel binarized deep convolution (BDC) unit that effectively enhances performance while increasing the number of binarized convolutional layers. Firstly, through theoretical analysis, we demonstrate that 1 \\times 1 binarized convolutions introduce minimal binarization errors. Therefore, additional binarized convolutional layers are constrained to 1 \\times 1 in the BDC unit. Secondly, we introduce the per-channel weight branch to mitigate the impact of binarization errors from unimportant channel features on the performance of binarized models, thereby improving performance while increasing the number of binarized convolutional layers. Furthermore, we decompose the 3D occupancy network into four convolutional modules and utilize the proposed BDC unit to binarize these modules. Our BDC-Occ model is created by applying the proposed BDC unit to binarize the existing 3D occupancy networks. Comprehensive quantitative and qualitative experiments demonstrate that the proposed BDC-Occ is the state-of-the-art binarized 3D occupancy network algorithm.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "19 pages, 8 figures"
    },
    {
        "paper id": "2405.17044",
        "abstract url": "https://arxiv.org/abs/2405.17044",
        "title": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone. However, how interesting are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, a system that uses an evolving knowledge graph built from more than 58 million scientific papers to generate personalized research ideas via an interface to GPT-4. We conducted a large-scale human evaluation with over 100 research group leaders from the Max Planck Society, who ranked more than 4,000 personalized research ideas based on their level of interest. This evaluation allows us to understand the relationships between scientific interest and the core properties of the knowledge graph. We find that data-efficient machine learning can predict research interest with high precision, allowing us to optimize the interest-level of generated research ideas. This work represents a step towards an artificial scientific muse that could catalyze unforeseen collaborations and suggest interesting avenues for scientists.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.DL",
            "cs.LG"
        ],
        "comment": "10 pages; 5 figures"
    },
    {
        "paper id": "2405.17069",
        "abstract url": "https://arxiv.org/abs/2405.17069",
        "title": "Training-free Editioning of Text-to-Image Models",
        "rating": "0",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Inspired by the software industry's practice of offering different editions or versions of a product tailored to specific user groups or use cases, we propose a novel task, namely, training-free editioning, for text-to-image models. Specifically, we aim to create variations of a base text-to-image model without retraining, enabling the model to cater to the diverse needs of different user groups or to offer distinct features and functionalities. To achieve this, we propose that different editions of a given text-to-image model can be formulated as concept subspaces in the latent space of its text encoder (e.g., CLIP). In such a concept subspace, all points satisfy a specific user need (e.g., generating images of a cat lying on the grass/ground/falling leaves). Technically, we apply Principal Component Analysis (PCA) to obtain the desired concept subspaces from representative text embedding that correspond to a specific user need or requirement. Projecting the text embedding of a given prompt into these low-dimensional subspaces enables efficient model editioning without retraining. Intuitively, our proposed editioning paradigm enables a service provider to customize the base model into its \"cat edition\" (or other editions) that restricts image generation to cats, regardless of the user's prompt (e.g., dogs, people, etc.). This introduces a new dimension for product differentiation, targeted functionality, and pricing strategies, unlocking novel business models for text-to-image generators. Extensive experimental results demonstrate the validity of our approach and its potential to enable a wide range of customized text-to-image model editions across various domains and applications.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17074",
        "abstract url": "https://arxiv.org/abs/2405.17074",
        "title": "Towards Ultra-High-Definition Image Deraining: A Benchmark and An Efficient Method",
        "rating": "0",
        "keywords": [
            [
                "Deraining"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite significant progress has been made in image deraining, existing approaches are mostly carried out on low-resolution images. The effectiveness of these methods on high-resolution images is still unknown, especially for ultra-high-definition (UHD) images, given the continuous advancement of imaging devices. In this paper, we focus on the task of UHD image deraining, and contribute the first large-scale UHD image deraining dataset, 4K-Rain13k, that contains 13,000 image pairs at 4K resolution. Based on this dataset, we conduct a benchmark study on existing methods for processing UHD images. Furthermore, we develop an effective and efficient vision MLP-based architecture (UDR-Mixer) to better solve this task. Specifically, our method contains two building components: a spatial feature rearrangement layer that captures long-range information of UHD images, and a frequency feature modulation layer that facilitates high-quality UHD image reconstruction. Extensive experimental results demonstrate that our method performs favorably against the state-of-the-art approaches while maintaining a lower model complexity. The code and dataset will be available at https://github.com/cschenxiang/UDR-Mixer.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17082",
        "abstract url": "https://arxiv.org/abs/2405.17082",
        "title": "Ensembling Diffusion Models via Adaptive Feature Aggregation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community. These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities. Leveraging multiple high-quality models to produce stronger generation ability is valuable, but has not been extensively studied. Existing methods primarily adopt parameter merging strategies to produce a new static model. However, they overlook the fact that the divergent denoising capabilities of the models may dynamically change across different states, such as when experiencing different prompts, initial noises, denoising steps, and spatial locations. In this paper, we propose a novel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically adjusts the contributions of multiple models at the feature level according to various states (i.e., prompts, initial noises, denoising steps, and spatial locations), thereby keeping the advantages of multiple diffusion models, while suppressing their disadvantages. Specifically, we design a lightweight Spatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the block-wise intermediate features from multiple U-Net denoisers into a unified one. The core idea lies in dynamically producing an individual attention map for each model's features by comprehensively considering various states. It is worth noting that only SABW is trainable with about 50 million parameters, while other models are frozen. Both the quantitative and qualitative experiments demonstrate the effectiveness of our proposed Adaptive Feature Aggregation method. The code is available at https://github.com/tenvence/afa/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17097",
        "abstract url": "https://arxiv.org/abs/2405.17097",
        "title": "Evaluation of Multi-task Uncertainties in Joint Semantic Segmentation and Monocular Depth Estimation",
        "rating": "0",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "While a number of promising uncertainty quantification methods have been proposed to address the prevailing shortcomings of deep neural networks like overconfidence and lack of explainability, quantifying predictive uncertainties in the context of joint semantic segmentation and monocular depth estimation has not been explored yet. Since many real-world applications are multi-modal in nature and, hence, have the potential to benefit from multi-task learning, this is a substantial gap in current literature. To this end, we conduct a comprehensive series of experiments to study how multi-task learning influences the quality of uncertainty estimates in comparison to solving both tasks separately.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Submitted to Forum Bildverarbeitung 2024. arXiv admin note: substantial text overlap with arXiv:2402.10580"
    },
    {
        "paper id": "2405.17098",
        "abstract url": "https://arxiv.org/abs/2405.17098",
        "title": "Q-value Regularized Transformer for Offline Reinforcement Learning",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Recent advancements in offline reinforcement learning (RL) have underscored the capabilities of Conditional Sequence Modeling (CSM), a paradigm that learns the action distribution based on history trajectory and target returns for each state. However, these methods often struggle with stitching together optimal trajectories from sub-optimal ones due to the inconsistency between the sampled returns within individual trajectories and the optimal returns across multiple trajectories. Fortunately, Dynamic Programming (DP) methods offer a solution by leveraging a value function to approximate optimal future returns for each state, while these techniques are prone to unstable learning behaviors, particularly in long-horizon and sparse-reward scenarios. Building upon these insights, we propose the Q-value regularized Transformer (QT), which combines the trajectory modeling ability of the Transformer with the predictability of optimal future returns from DP methods. QT learns an action-value function and integrates a term maximizing action-values into the training loss of CSM, which aims to seek optimal actions that align closely with the behavior policy. Empirical evaluations on D4RL benchmark datasets demonstrate the superiority of QT over traditional DP and CSM methods, highlighting the potential of QT to enhance the state-of-the-art in offline RL.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Published at ICML 2024"
    },
    {
        "paper id": "2405.17100",
        "abstract url": "https://arxiv.org/abs/2405.17100",
        "title": "Sok: Comprehensive Security Overview, Challenges, and Future Directions of Voice-Controlled Systems",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The integration of Voice Control Systems (VCS) into smart devices and their growing presence in daily life accentuate the importance of their security. Current research has uncovered numerous vulnerabilities in VCS, presenting significant risks to user privacy and security. However, a cohesive and systematic examination of these vulnerabilities and the corresponding solutions is still absent. This lack of comprehensive analysis presents a challenge for VCS designers in fully understanding and mitigating the security issues within these systems. Addressing this gap, our study introduces a hierarchical model structure for VCS, providing a novel lens for categorizing and analyzing existing literature in a systematic manner. We classify attacks based on their technical principles and thoroughly evaluate various attributes, such as their methods, targets, vectors, and behaviors. Furthermore, we consolidate and assess the defense mechanisms proposed in current research, offering actionable recommendations for enhancing VCS security. Our work makes a significant contribution by simplifying the complexity inherent in VCS security, aiding designers in effectively identifying and countering potential threats, and setting a foundation for future advancements in VCS security research.",
        "subjects": [
            "cs.CR",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17110",
        "abstract url": "https://arxiv.org/abs/2405.17110",
        "title": "Superpixelwise Low-rank Approximation based Partial Label Learning for Hyperspectral Image Classification",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Insufficient prior knowledge of a captured hyperspectral image (HSI) scene may lead the experts or the automatic labeling systems to offer incorrect labels or ambiguous labels (i.e., assigning each training sample to a group of candidate labels, among which only one of them is valid; this is also known as partial label learning) during the labeling process. Accordingly, how to learn from such data with ambiguous labels is a problem of great practical importance. In this paper, we propose a novel superpixelwise low-rank approximation (LRA)-based partial label learning method, namely SLAP, which is the first to take into account partial label learning in HSI classification. SLAP is mainly composed of two phases: disambiguating the training labels and acquiring the predictive model. Specifically, in the first phase, we propose a superpixelwise LRA-based model, preparing the affinity graph for the subsequent label propagation process while extracting the discriminative representation to enhance the following classification task of the second phase. Then to disambiguate the training labels, label propagation propagates the labeling information via the affinity graph of training pixels. In the second phase, we take advantage of the resulting disambiguated training labels and the discriminative representations to enhance the classification performance. The extensive experiments validate the advantage of the proposed SLAP method over state-of-the-art methods.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "0"
    },
    {
        "paper id": "2405.17136",
        "abstract url": "https://arxiv.org/abs/2405.17136",
        "title": "PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Social VR platforms enable social, economic, and creative activities by allowing users to create and share their own virtual spaces. In social VR, photography within a VR scene is an important indicator of visitors' activities. Although automatic identification of photo spots within a VR scene can facilitate the process of creating a VR scene and enhance the visitor experience, there are challenges in quantitatively evaluating photos taken in the VR scene and efficiently exploring the large VR scene. We propose PanoTree, an automated photo-spot explorer in VR scenes. To assess the aesthetics of images captured in VR scenes, a deep scoring network is trained on a large dataset of photos collected by a social VR platform to determine whether humans are likely to take similar photos. Furthermore, we propose a Hierarchical Optimistic Optimization (HOO)-based search algorithm to efficiently explore 3D VR spaces with the reward from the scoring network. Our user study shows that the scoring network achieves human-level performance in distinguishing randomly taken images from those taken by humans. In addition, we show applications using the explored photo spots, such as automatic thumbnail generation, support for VR world creation, and visitor flow planning within a VR scene.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "12pages, 12 figures"
    },
    {
        "paper id": "2405.17149",
        "abstract url": "https://arxiv.org/abs/2405.17149",
        "title": "LCM: Locally Constrained Compact Point Cloud Model for Masked Point Modeling",
        "rating": "0",
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The pre-trained point cloud model based on Masked Point Modeling (MPM) has exhibited substantial improvements across various tasks. However, these models heavily rely on the Transformer, leading to quadratic complexity and limited decoder, hindering their practice application. To address this limitation, we first conduct a comprehensive analysis of existing Transformer-based MPM, emphasizing the idea that redundancy reduction is crucial for point cloud analysis. To this end, we propose a Locally constrained Compact point cloud Model (LCM) consisting of a locally constrained compact encoder and a locally constrained Mamba-based decoder. Our encoder replaces self-attention with our local aggregation layers to achieve an elegant balance between performance and efficiency. Considering the varying information density between masked and unmasked patches in the decoder inputs of MPM, we introduce a locally constrained Mamba-based decoder. This decoder ensures linear complexity while maximizing the perception of point cloud geometry information from unmasked patches with higher information density. Extensive experimental results show that our compact model significantly surpasses existing Transformer-based models in both performance and efficiency, especially our LCM-based Point-MAE model, compared to the Transformer-based model, achieved an improvement of 2.24%, 0.87%, and 0.94% in performance on the three variants of ScanObjectNN while reducing parameters by 88% and computation by 73%.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17158",
        "abstract url": "https://arxiv.org/abs/2405.17158",
        "title": "PatchScaler: An Efficient Patch-independent Diffusion Model for Super-Resolution",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models significantly improve the quality of super-resolved images with their impressive content generation capabilities. However, the huge computational costs limit the applications of these methods.Recent efforts have explored reasonable inference acceleration to reduce the number of sampling steps, but the computational cost remains high as each step is performed on the entire image.This paper introduces PatchScaler, a patch-independent diffusion-based single image super-resolution (SR) method, designed to enhance the efficiency of the inference process.The proposed method is motivated by the observation that not all the image patches within an image need the same sampling steps for reconstructing high-resolution images.Based on this observation, we thus develop a Patch-adaptive Group Sampling (PGS) to divide feature patches into different groups according to the patch-level reconstruction difficulty and dynamically assign an appropriate sampling configuration for each group so that the inference speed can be better accelerated.In addition, to improve the denoising ability at each step of the sampling, we develop a texture prompt to guide the estimations of the diffusion model by retrieving high-quality texture priors from a patch-independent reference texture memory.Experiments show that our PatchScaler achieves favorable performance in both quantitative and qualitative evaluations with fast inference speed.Our code and model are available at \\url{https://github.com/yongliuy/PatchScaler}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17181",
        "abstract url": "https://arxiv.org/abs/2405.17181",
        "title": "Spectral regularization for adversarially-robust representation learning",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The vulnerability of neural network classifiers to adversarial attacks is a major obstacle to their deployment in safety-critical applications. Regularization of network parameters during training can be used to improve adversarial robustness and generalization performance. Usually, the network is regularized end-to-end, with parameters at all layers affected by regularization. However, in settings where learning representations is key, such as self-supervised learning (SSL), layers after the feature representation will be discarded when performing inference. For these models, regularizing up to the feature space is more suitable. To this end, we propose a new spectral regularizer for representation learning that encourages black-box adversarial robustness in downstream classification tasks. In supervised classification settings, we show empirically that this method is more effective in boosting test accuracy and robustness than previously-proposed methods that regularize all layers of the network. We then show that this method improves the adversarial robustness of classifiers using representations learned with self-supervised training or transferred from another classification task. In all, our work begins to unveil how representational structure affects adversarial robustness.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "15 + 15 pages, 8 + 11 figures"
    },
    {
        "paper id": "2405.17191",
        "abstract url": "https://arxiv.org/abs/2405.17191",
        "title": "MCGAN: Enhancing GAN Training with Regression-Based Generator Loss",
        "rating": "0",
        "keywords": [
            [
                "GAN"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generative adversarial networks (GANs) have emerged as a powerful tool for generating high-fidelity data. However, the main bottleneck of existing approaches is the lack of supervision on the generator training, which often results in undamped oscillation and unsatisfactory performance. To address this issue, we propose an algorithm called Monte Carlo GAN (MCGAN). This approach, utilizing an innovative generative loss function, termly the regression loss, reformulates the generator training as a regression task and enables the generator training by minimizing the mean squared error between the discriminator's output of real data and the expected discriminator of fake data. We demonstrate the desirable analytic properties of the regression loss, including discriminability and optimality, and show that our method requires a weaker condition on the discriminator for effective generator training. These properties justify the strength of this approach to improve the training stability while retaining the optimality of GAN by leveraging strong supervision of the regression loss. Numerical results on CIFAR-10 and CIFAR-100 datasets demonstrate that the proposed MCGAN significantly and consistently improves the existing state-of-the-art GAN models in terms of quality, accuracy, training stability, and learned latent space. Furthermore, the proposed algorithm exhibits great flexibility for integrating with a variety of backbone models to generate spatial images, temporal time-series, and spatio-temporal video data.",
        "subjects": [
            "cs.CV",
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17201",
        "abstract url": "https://arxiv.org/abs/2405.17201",
        "title": "Diagnosing the Compositional Knowledge of Vision Language Models from a Game-Theoretic View",
        "rating": "0",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "Diagnosing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Compositional reasoning capabilities are usually considered as fundamental skills to characterize human perception. Recent studies show that current Vision Language Models (VLMs) surprisingly lack sufficient knowledge with respect to such capabilities. To this end, we propose to thoroughly diagnose the composition representations encoded by VLMs, systematically revealing the potential cause for this weakness. Specifically, we propose evaluation methods from a novel game-theoretic view to assess the vulnerability of VLMs on different aspects of compositional understanding, e.g., relations and attributes. Extensive experimental results demonstrate and validate several insights to understand the incapabilities of VLMs on compositional reasoning, which provide useful and reliable guidance for future studies. The deliverables will be updated at https://vlms-compositionality-gametheory.github.io/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "21 pages, 8 figures"
    },
    {
        "paper id": "2405.17249",
        "abstract url": "https://arxiv.org/abs/2405.17249",
        "title": "Assessing LLMs Suitability for Knowledge Graph Completion",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent work shown the capability of Large Language Models (LLMs) to solve tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in Zero- or Few-Shot paradigms. However, they are known to hallucinate answers, or output results in a non-deterministic manner, thus leading to wrongly reasoned responses, even if they satisfy the user's demands. To highlight opportunities and challenges in knowledge graphs-related tasks, we experiment with two distinguished LLMs, namely Mixtral-8x7B-Instruct-v0.1, and gpt-3.5-turbo-0125, on Knowledge Graph Completion for static knowledge graphs, using prompts constructed following the TELeR taxonomy, in Zero- and One-Shot contexts, on a Task-Oriented Dialogue system use case. When evaluated using both strict and flexible metrics measurement manners, our results show that LLMs could be fit for such a task if prompts encapsulate sufficient information and relevant examples.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Evaluating Mixtral-8x7B-Instruct-v0.1 and gpt-3.5-turbo-0125 for Knowledge Graph Completion task with prompts formatted according to the TELeR taxonomy"
    },
    {
        "paper id": "2405.17257",
        "abstract url": "https://arxiv.org/abs/2405.17257",
        "title": "Surface reconstruction of sampled textiles via Morse theory",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we study the perception problem for garments using tools from computational topology: the identification of their geometry and position in space from point-cloud samples, as obtained e.g. with 3D scanners. We present a reconstruction algorithm based on a direct topological study of the sampled textile surface that allows us to obtain a cellular decomposition of it via a Morse function. No intermediate triangulation or local implicit equations are used, avoiding reconstruction-induced artifices. No a priori knowledge of the surface topology, density or regularity of the point-sample is required to run the algorithm. The results are a piecewise decomposition of the surface as a union of Morse cells (i.e. topological disks), suitable for tasks such as noise-filtering or mesh-independent reparametrization, and a cell complex of small rank determining the surface topology. This algorithm can be applied to smooth surfaces with or without boundary, embedded in an ambient space of any dimension.",
        "subjects": [
            "cs.CG",
            "cs.CV",
            "math.AT"
        ],
        "comment": "40 pages, 17 figures, 1 table, 1 algorithm, 1 appendix"
    },
    {
        "paper id": "2405.17261",
        "abstract url": "https://arxiv.org/abs/2405.17261",
        "title": "Does Diffusion Beat GAN in Image Super Resolution?",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "GAN",
                "Super Resolution"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "There is a prevalent opinion in the recent literature that Diffusion-based models outperform GAN-based counterparts on the Image Super Resolution (ISR) problem. However, in most studies, Diffusion-based ISR models were trained longer and utilized larger networks than the GAN baselines. This raises the question of whether the superiority of Diffusion models is due to the Diffusion paradigm being better suited for the ISR task or if it is a consequence of the increased scale and computational resources used in contemporary studies. In our work, we compare Diffusion-based and GAN-based Super Resolution under controlled settings, where both approaches are matched in terms of architecture, model and dataset size, and computational budget. We show that a GAN-based model can achieve results comparable to a Diffusion-based model. Additionally, we explore the impact of design choices such as text conditioning and augmentation on the performance of ISR models, showcasing their effect on several downstream tasks. We will release the inference code and weights of our scaled GAN.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17306",
        "abstract url": "https://arxiv.org/abs/2405.17306",
        "title": "Controllable Longer Image Animation with Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating realistic animated videos from static images is an important area of research in computer vision. Methods based on physical simulation and motion prediction have achieved notable advances, but they are often limited to specific object textures and motion trajectories, failing to exhibit highly complex environments and physical dynamics. In this paper, we introduce an open-domain controllable image animation method using motion priors with video diffusion models. Our method achieves precise control over the direction and speed of motion in the movable region by extracting the motion field information from videos and learning moving trajectories and strengths. Current pretrained video generation models are typically limited to producing very short videos, typically less than 30 frames. In contrast, we propose an efficient long-duration video generation method based on noise reschedule specifically tailored for image animation tasks, facilitating the creation of videos over 100 frames in length while maintaining consistency in content scenery and motion coordination. Specifically, we decompose the denoise process into two distinct phases: the shaping of scene contours and the refining of motion details. Then we reschedule the noise to control the generated frame sequences maintaining long-distance noise correlation. We conducted extensive experiments with 10 baselines, encompassing both commercial tools and academic methodologies, which demonstrate the superiority of our method. Our project page: https://wangqiang9.github.io/Controllable.github.io/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "https://wangqiang9.github.io/Controllable.github.io/"
    },
    {
        "paper id": "2405.17351",
        "abstract url": "https://arxiv.org/abs/2405.17351",
        "title": "DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for Refocusing,Defocus Rendering and Blur Removal",
        "rating": "0",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "3D",
                "Gaussian Splatting",
                "Depth"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting-based techniques have recently advanced 3D scene reconstruction and novel view synthesis, achieving high-quality real-time rendering. However, these approaches are inherently limited by the underlying pinhole camera assumption in modeling the images and hence only work for All-in-Focus (AiF) sharp image inputs. This severely affects their applicability in real-world scenarios where images often exhibit defocus blur due to the limited depth-of-field (DOF) of imaging devices. Additionally, existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of DOF effects. To address these challenges, we introduce DOF-GS that allows for rendering adjustable DOF effects, removing defocus blur as well as refocusing of 3D scenes, all from multi-view images degraded by defocus blur. To this end, we re-imagine the traditional Gaussian Splatting pipeline by employing a finite aperture camera model coupled with explicit, differentiable defocus rendering guided by the Circle-of-Confusion (CoC). The proposed framework provides for dynamic adjustment of DOF effects by changing the aperture and focal distance of the underlying camera model on-demand. It also enables rendering varying DOF effects of 3D scenes post-optimization, and generating AiF images from defocused training images. Furthermore, we devise a joint optimization strategy to further enhance details in the reconstructed scenes by jointly optimizing rendered defocused and AiF images. Our experimental results indicate that DOF-GS produces high-quality sharp all-in-focus renderings conditioned on inputs compromised by defocus blur, with the training process incurring only a modest increase in GPU memory consumption. We further demonstrate the applications of the proposed method for adjustable defocus rendering and refocusing of the 3D scene from input images degraded by defocus blur.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17369",
        "abstract url": "https://arxiv.org/abs/2405.17369",
        "title": "Predict joint angle of body parts based on sequence pattern recognition",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The way organs are positioned and moved in the workplace can cause pain and physical harm. Therefore, ergonomists use ergonomic risk assessments based on visual observation of the workplace, or review pictures and videos taken in the workplace. Sometimes the workers in the photos are not in perfect condition. Some parts of the workers' bodies may not be in the camera's field of view, could be obscured by objects, or by self-occlusion, this is the main problem in 2D human posture recognition. It is difficult to predict the position of body parts when they are not visible in the image, and geometric mathematical methods are not entirely suitable for this purpose. Therefore, we created a dataset with artificial images of a 3D human model, specifically for painful postures, and real human photos from different viewpoints. Each image we captured was based on a predefined joint angle for each 3D model or human model. We created various images, including images where some body parts are not visible. Nevertheless, the joint angle is estimated beforehand, so we could study the case by converting the input images into the sequence of joint connections between predefined body parts and extracting the desired joint angle with a convolutional neural network. In the end, we obtained root mean square error (RMSE) of 12.89 and mean absolute error (MAE) of 4.7 on the test dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17376",
        "abstract url": "https://arxiv.org/abs/2405.17376",
        "title": "Federating Dynamic Models using Early-Exit Architectures for Automatic Speech Recognition on Heterogeneous Clients",
        "rating": "0",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Automatic speech recognition models require large amounts of speech recordings for training. However, the collection of such data often is cumbersome and leads to privacy concerns. Federated learning has been widely used as an effective decentralized technique that collaboratively learns a shared prediction model while keeping the data local on different clients. Unfortunately, client devices often feature limited computation and communication resources leading to practical difficulties for large models. In addition, the heterogeneity that characterizes edge devices makes it sub-optimal to generate a single model that fits all of them. Differently from the recent literature, where multiple models with different architectures are used, in this work, we propose using dynamical architectures which, employing early-exit solutions, can adapt their processing (i.e. traversed layers) depending on the input and on the operation conditions. This solution falls in the realm of partial training methods and brings two benefits: a single model is used on a variety of devices; federating the models after local training is straightforward. Experiments on public datasets show that our proposed approach is effective and can be combined with basic federated learning strategies.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "The paper is under review in Future Generation Computer Systems Journal"
    },
    {
        "paper id": "2405.17401",
        "abstract url": "https://arxiv.org/abs/2405.17401",
        "title": "RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "comment": "Preprint. Under review"
    },
    {
        "paper id": "2405.17427",
        "abstract url": "https://arxiv.org/abs/2405.17427",
        "title": "Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in multimodal large language models (LLMs) have shown their potential in various domains, especially concept reasoning. Despite these developments, applications in understanding 3D environments remain limited. This paper introduces Reason3D, a novel LLM designed for comprehensive 3D understanding. Reason3D takes point cloud data and text prompts as input to produce textual responses and segmentation masks, facilitating advanced tasks like 3D reasoning segmentation, hierarchical searching, express referring, and question answering with detailed mask outputs. Specifically, we propose a hierarchical mask decoder to locate small objects within expansive scenes. This decoder initially generates a coarse location estimate covering the object's general area. This foundational estimation facilitates a detailed, coarse-to-fine segmentation strategy that significantly enhances the precision of object identification and segmentation. Experiments validate that Reason3D achieves remarkable results on large-scale ScanNet and Matterport3D datasets for 3D express referring, 3D question answering, and 3D reasoning segmentation tasks. Code and models are available at: https://github.com/KuanchihHuang/Reason3D.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://KuanchihHuang.github.io/project/reason3d"
    },
    {
        "paper id": "2405.17523",
        "abstract url": "https://arxiv.org/abs/2405.17523",
        "title": "Locally Testing Model Detections for Semantic Global Concepts",
        "rating": "0",
        "keywords": [
            [
                "automated driving"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Ensuring the quality of black-box Deep Neural Networks (DNNs) has become ever more significant, especially in safety-critical domains such as automated driving. While global concept encodings generally enable a user to test a model for a specific concept, linking global concept encodings to the local processing of single network inputs reveals their strengths and limitations. Our proposed framework global-to-local Concept Attribution (glCA) uses approaches from local (why a specific prediction originates) and global (how a model works generally) eXplainable Artificial Intelligence (xAI) to test DNNs for a predefined semantical concept locally. The approach allows for conditioning local, post-hoc explanations on predefined semantic concepts encoded as linear directions in the model's latent space. Pixel-exact scoring concerning the global concept usage assists the tester in further understanding the model processing of single data points for the selected concept. Our approach has the advantage of fully covering the model-internal encoding of the semantic concept and allowing the localization of relevant concept-related information. The results show major differences in the local perception and usage of individual global concept encodings and demand for further investigations regarding obtaining thorough semantic concept encodings.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17532",
        "abstract url": "https://arxiv.org/abs/2405.17532",
        "title": "ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent text-to-image customization works have been proven successful in generating images of given concepts by fine-tuning the diffusion models on a few examples. However, these methods tend to overfit the concepts, resulting in failure to create the concept under multiple conditions (e.g. headphone is missing when generating a <sks> dog wearing a headphone'). Interestingly, we notice that the base model before fine-tuning exhibits the capability to compose the base concept with other elements (e.g. a dog wearing a headphone) implying that the compositional ability only disappears after personalization tuning. Inspired by this observation, we present ClassDiffusion, a simple technique that leverages a semantic preservation loss to explicitly regulate the concept space when learning the new concept. Despite its simplicity, this helps avoid semantic drift when fine-tuning on the target concepts. Extensive qualitative and quantitative experiments demonstrate that the use of semantic preservation loss effectively improves the compositional abilities of the fine-tune models. In response to the ineffective evaluation of CLIP-T metrics, we introduce BLIP2-T metric, a more equitable and effective evaluation metric for this particular domain. We also provide in-depth empirical study and theoretical analysis to better understand the role of the proposed loss. Lastly, we also extend our ClassDiffusion to personalized video generation, demonstrating its flexibility.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17609",
        "abstract url": "https://arxiv.org/abs/2405.17609",
        "title": "GarmentCodeData: A Dataset of 3D Made-to-Measure Garments With Sewing Patterns",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent research interest in the learning-based processing of garments, from virtual fitting to generation and reconstruction, stumbles on a scarcity of high-quality public data in the domain. We contribute to resolving this need by presenting the first large-scale synthetic dataset of 3D made-to-measure garments with sewing patterns, as well as its generation pipeline. GarmentCodeData contains 115,000 data points that cover a variety of designs in many common garment categories: tops, shirts, dresses, jumpsuits, skirts, pants, etc., fitted to a variety of body shapes sampled from a custom statistical body model based on CAESAR, as well as a standard reference body shape, applying three different textile materials. To enable the creation of datasets of such complexity, we introduce a set of algorithms for automatically taking tailor's measures on sampled body shapes, sampling strategies for sewing pattern design, and propose an automatic, open-source 3D garment draping pipeline based on a fast XPBD simulator, while contributing several solutions for collision resolution and drape correctness to enable scalability. Dataset: http://hdl.handle.net/20.500.11850/673889",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17661",
        "abstract url": "https://arxiv.org/abs/2405.17661",
        "title": "RefDrop: Controllable Consistency in Image or Video Generation via Reference Feature Guidance",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "There is a rapidly growing interest in controlling consistency across multiple generated images using diffusion models. Among various methods, recent works have found that simply manipulating attention modules by concatenating features from multiple reference images provides an efficient approach to enhancing consistency without fine-tuning. Despite its popularity and success, few studies have elucidated the underlying mechanisms that contribute to its effectiveness. In this work, we reveal that the popular approach is a linear interpolation of image self-attention and cross-attention between synthesized content and reference features, with a constant rank-1 coefficient. Motivated by this observation, we find that a rank-1 coefficient is not necessary and simplifies the controllable generation mechanism. The resulting algorithm, which we coin as RefDrop, allows users to control the influence of reference context in a direct and precise manner. Besides further enhancing consistency in single-subject image generation, our method also enables more interesting applications, such as the consistent generation of multiple subjects, suppressing specific features to encourage more diverse content, and high-quality personalized video generation by boosting temporal consistency. Even compared with state-of-the-art image-prompt-based generators, such as IP-Adapter, RefDrop is competitive in terms of controllability and quality while avoiding the need to train a separate image encoder for feature injection from reference images, making it a versatile plug-and-play solution for any image or video diffusion model.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17680",
        "abstract url": "https://arxiv.org/abs/2405.17680",
        "title": "Deciphering Movement: Unified Trajectory Generation Model for Multi-Agent",
        "rating": "0",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding multi-agent behavior is critical across various fields. The conventional approach involves analyzing agent movements through three primary tasks: trajectory prediction, imputation, and spatial-temporal recovery. Considering the unique input formulation and constraint of these tasks, most existing methods are tailored to address only one specific task. However, in real-world applications, these scenarios frequently occur simultaneously. Consequently, methods designed for one task often fail to adapt to others, resulting in performance drops. To overcome this limitation, we propose a Unified Trajectory Generation model, UniTraj, that processes arbitrary trajectories as masked inputs, adaptable to diverse scenarios. Specifically, we introduce a Ghost Spatial Masking (GSM) module embedded within a Transformer encoder for spatial feature extraction. We further extend recent successful State Space Models (SSMs), particularly the Mamba model, into a Bidirectional Temporal Mamba to effectively capture temporal dependencies. Additionally, we incorporate a Bidirectional Temporal Scaled (BTS) module to comprehensively scan trajectories while maintaining the temporal missing relationships within the sequence. We curate and benchmark three practical sports game datasets, Basketball-U, Football-U, and Soccer-U, for evaluation. Extensive experiments demonstrate the superior performance of our model. To the best of our knowledge, this is the first work that addresses this unified problem through a versatile generative framework, thereby enhancing our understanding of multi-agent movement. Our datasets, code, and model weights are available at https://github.com/colorfulfuture/UniTraj-pytorch.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Datasets, code, and model weights at available at: https://github.com/colorfulfuture/UniTraj-pytorch"
    },
    {
        "paper id": "2405.17704",
        "abstract url": "https://arxiv.org/abs/2405.17704",
        "title": "Consistency Regularisation for Unsupervised Domain Adaptation in Monocular Depth Estimation",
        "rating": "0",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In monocular depth estimation, unsupervised domain adaptation has recently been explored to relax the dependence on large annotated image-based depth datasets. However, this comes at the cost of training multiple models or requiring complex training protocols. We formulate unsupervised domain adaptation for monocular depth estimation as a consistency-based semi-supervised learning problem by assuming access only to the source domain ground truth labels. To this end, we introduce a pairwise loss function that regularises predictions on the source domain while enforcing perturbation consistency across multiple augmented views of the unlabelled target samples. Importantly, our approach is simple and effective, requiring only training of a single model in contrast to the prior work. In our experiments, we rely on the standard depth estimation benchmarks KITTI and NYUv2 to demonstrate state-of-the-art results compared to related approaches. Furthermore, we analyse the simplicity and effectiveness of our approach in a series of ablation studies. The code is available at \\url{https://github.com/AmirMaEl/SemiSupMDE}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to Conference on Lifelong Learning Agents (CoLLAs) 2024"
    },
    {
        "paper id": "2405.17719",
        "abstract url": "https://arxiv.org/abs/2405.17719",
        "title": "EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?",
        "rating": "0",
        "keywords": [
            [
                "text-to-video"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Egocentric video-language pretraining is a crucial paradigm to advance the learning of egocentric hand-object interactions (EgoHOI). Despite the great success on existing testbeds, these benchmarks focus more on closed-set visual concepts or limited scenarios. Due to the occurrence of diverse EgoHOIs in the real world, we propose an open-vocabulary benchmark named EgoHOIBench to reveal the diminished performance of current egocentric video-language models (EgoVLM) on fined-grained concepts, indicating that these models still lack a full spectrum of egocentric understanding. We attribute this performance gap to insufficient fine-grained supervision and strong bias towards understanding objects rather than temporal dynamics in current methods. To tackle these issues, we introduce a novel asymmetric contrastive objective for EgoHOI named EgoNCE++. For video-to-text loss, we enhance text supervision through the generation of negative captions by leveraging the in-context learning of large language models to perform HOI-related word substitution. For text-to-video loss, we propose an object-centric positive video sampling strategy that aggregates video representations by the same nouns. Our extensive experiments demonstrate that EgoNCE++ significantly boosts open-vocabulary HOI recognition, multi-instance retrieval, and action recognition tasks across various egocentric models, with improvements of up to +26.55%. Our code is available at https://github.com/xuboshen/EgoNCEpp.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code: https://github.com/xuboshen/EgoNCEpp"
    },
    {
        "paper id": "2405.17774",
        "abstract url": "https://arxiv.org/abs/2405.17774",
        "title": "Gradually Vanishing Gap in Prototypical Network for Unsupervised Domain Adaptation",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Unsupervised domain adaptation (UDA) is a critical problem for transfer learning, which aims to transfer the semantic information from labeled source domain to unlabeled target domain. Recent advancements in UDA models have demonstrated significant generalization capabilities on the target domain. However, the generalization boundary of UDA models remains unclear. When the domain discrepancy is too large, the model can not preserve the distribution structure, leading to distribution collapse during the alignment. To address this challenge, we propose an efficient UDA framework named Gradually Vanishing Gap in Prototypical Network (GVG-PN), which achieves transfer learning from both global and local perspectives. From the global alignment standpoint, our model generates a domain-biased intermediate domain that helps preserve the distribution structures. By entangling cross-domain features, our model progressively reduces the risk of distribution collapse. However, only relying on global alignment is insufficient to preserve the distribution structure. To further enhance the inner relationships of features, we introduce the local perspective. We utilize the graph convolutional network (GCN) as an intuitive method to explore the internal relationships between features, ensuring the preservation of manifold structures and generating domain-biased prototypes. Additionally, we consider the discriminability of the inner relationships between features. We propose a pro-contrastive loss to enhance the discriminability at the prototype level by separating hard negative pairs. By incorporating both GCN and the pro-contrastive loss, our model fully explores fine-grained semantic relationships. Experiments on several UDA benchmarks validated that the proposed GVG-PN can clearly outperform the SOTA models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17788",
        "abstract url": "https://arxiv.org/abs/2405.17788",
        "title": "Enhancing Road Safety: Real-Time Detection of Driver Distraction through Convolutional Neural Networks",
        "rating": "0",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "As we navigate our daily commutes, the threat posed by a distracted driver is at a large, resulting in a troubling rise in traffic accidents. Addressing this safety concern, our project harnesses the analytical power of Convolutional Neural Networks (CNNs), with a particular emphasis on the well-established models VGG16 and VGG19. These models are acclaimed for their precision in image recognition and are meticulously tested for their ability to detect nuances in driver behavior under varying environmental conditions. Through a comparative analysis against an array of CNN architectures, this study seeks to identify the most efficient model for real-time detection of driver distractions. The ultimate aim is to incorporate the findings into vehicle safety systems, significantly boosting their capability to prevent accidents triggered by inattention. This research not only enhances our understanding of automotive safety technologies but also marks a pivotal step towards creating vehicles that are intuitively aligned with driver behaviors, ensuring safer roads for all.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17814",
        "abstract url": "https://arxiv.org/abs/2405.17814",
        "title": "FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in Text-to-Image Models",
        "rating": "0",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The rapid development and reduced barriers to entry for Text-to-Image (T2I) models have raised concerns about the biases in their outputs, but existing research lacks a holistic definition and evaluation framework of biases, limiting the enhancement of debiasing techniques. To address this issue, we introduce FAIntbench, a holistic and precise benchmark for biases in T2I models. In contrast to existing benchmarks that evaluate bias in limited aspects, FAIntbench evaluate biases from four dimensions: manifestation of bias, visibility of bias, acquired attributes, and protected attributes. We applied FAIntbench to evaluate seven recent large-scale T2I models and conducted human evaluation, whose results demonstrated the effectiveness of FAIntbench in identifying various biases. Our study also revealed new research questions about biases, including the side-effect of distillation. The findings presented here are preliminary, highlighting the potential of FAIntbench to advance future research aimed at mitigating the biases in T2I models. Our benchmark is publicly available to ensure the reproducibility.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17818",
        "abstract url": "https://arxiv.org/abs/2405.17818",
        "title": "Hyperspectral and multispectral image fusion with arbitrary resolution through self-supervised representations",
        "rating": "0",
        "keywords": [
            [
                "super-resolution"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The fusion of a low-resolution hyperspectral image (LR-HSI) with a high-resolution multispectral image (HR-MSI) has emerged as an effective technique for achieving HSI super-resolution (SR). Previous studies have mainly concentrated on estimating the posterior distribution of the latent high-resolution hyperspectral image (HR-HSI), leveraging an appropriate image prior and likelihood computed from the discrepancy between the latent HSI and observed images. Low rankness stands out for preserving latent HSI characteristics through matrix factorization among the various priors. However, this method only enhances resolution within the dimensions of the two modalities. To overcome this limitation, we propose a novel continuous low-rank factorization (CLoRF) by integrating two neural representations into the matrix factorization, capturing spatial and spectral information, respectively. This approach enables us to harness both the low rankness from the matrix factorization and the continuity from neural representation in a self-supervised manner. Theoretically, we prove the low-rank property and Lipschitz continuity in the proposed continuous low-rank factorization. Experimentally, our method significantly surpasses existing techniques and achieves user-desired resolutions without the need for neural network retraining.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17825",
        "abstract url": "https://arxiv.org/abs/2405.17825",
        "title": "Diffusion Model Patching via Mixture-of-Prompts",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "We present Diffusion Model Patching (DMP), a simple method to boost the performance of pre-trained diffusion models that have already reached convergence, with a negligible increase in parameters. DMP inserts a small, learnable set of prompts into the model's input space while keeping the original model frozen. The effectiveness of DMP is not merely due to the addition of parameters but stems from its dynamic gating mechanism, which selects and combines a subset of learnable prompts at every step of the generative process (e.g., reverse denoising steps). This strategy, which we term \"mixture-of-prompts\", enables the model to draw on the distinct expertise of each prompt, essentially \"patching\" the model's functionality at every step with minimal yet specialized parameters. Uniquely, DMP enhances the model by further training on the same dataset on which it was originally trained, even in a scenario where significant improvements are typically not expected due to model convergence. Experiments show that DMP significantly enhances the converged FID of DiT-L/2 on FFHQ 256x256 by 10.38%, achieved with only a 1.43% parameter increase and 50K additional training iterations.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Project page: https://sangminwoo.github.io/DMP/"
    },
    {
        "paper id": "2405.16837",
        "abstract url": "https://arxiv.org/abs/2405.16837",
        "title": "Enhancing Accuracy in Generative Models via Knowledge Transfer",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates the accuracy of generative models and the impact of knowledge transfer on their generation precision. Specifically, we examine a generative model for a target task, fine-tuned using a pre-trained model from a source task. Building on the \"Shared Embedding\" concept, which bridges the source and target tasks, we introduce a novel framework for transfer learning under distribution metrics such as the Kullback-Leibler divergence. This framework underscores the importance of leveraging inherent similarities between diverse tasks despite their distinct data distributions. Our theory suggests that the shared structures can augment the generation accuracy for a target task, reliant on the capability of a source model to identify shared structures and effective knowledge transfer from source to target learning. To demonstrate the practical utility of this framework, we explore the theoretical implications for two specific generative models: diffusion and normalizing flows. The results show enhanced performance in both models over their non-transfer counterparts, indicating advancements for diffusion models and providing fresh insights into normalizing flows in transfer and non-transfer settings. These results highlight the significant contribution of knowledge transfer in boosting the generation capabilities of these models.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16852",
        "abstract url": "https://arxiv.org/abs/2405.16852",
        "title": "EM Distillation for One-step Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "While diffusion models can learn complex distributions, sampling requires a computationally expensive iterative process. Existing distillation methods enable efficient sampling, but have notable limitations, such as performance degradation with very few sampling steps, reliance on training data access, or mode-seeking optimization that may fail to capture the full distribution. We propose EM Distillation (EMD), a maximum likelihood-based approach that distills a diffusion model to a one-step generator model with minimal loss of perceptual quality. Our approach is derived through the lens of Expectation-Maximization (EM), where the generator parameters are updated using samples from the joint distribution of the diffusion teacher prior and inferred generator latents. We develop a reparametrized sampling scheme and a noise cancellation technique that together stabilizes the distillation process. We further reveal an interesting connection of our method with existing methods that minimize mode-seeking KL. EMD outperforms existing one-step generative methods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares favorably with prior work on distilling text-to-image diffusion models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16876",
        "abstract url": "https://arxiv.org/abs/2405.16876",
        "title": "Transfer Learning for Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models, a specific type of generative model, have achieved unprecedented performance in recent years and consistently produce high-quality synthetic samples. A critical prerequisite for their notable success lies in the presence of a substantial number of training samples, which can be impractical in real-world applications due to high collection costs or associated risks. Consequently, various finetuning and regularization approaches have been proposed to transfer knowledge from existing pre-trained models to specific target domains with limited data. This paper introduces the Transfer Guided Diffusion Process (TGDP), a novel approach distinct from conventional finetuning and regularization methods. We prove that the optimal diffusion model for the target domain integrates pre-trained diffusion models on the source domain with additional guidance from a domain classifier. We further extend TGDP to a conditional version for modeling the joint distribution of data and its corresponding labels, together with two additional regularization terms to enhance the model performance. We validate the effectiveness of TGDP on Gaussian mixture simulations and on real electrocardiogram (ECG) datasets.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "24 pages"
    },
    {
        "paper id": "2405.16879",
        "abstract url": "https://arxiv.org/abs/2405.16879",
        "title": "Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Feature transformation is to derive a new feature set from original features to augment the AI power of data. In many science domains such as material performance screening, while feature transformation can model material formula interactions and compositions and discover performance drivers, supervised labels are collected from expensive and lengthy experiments. This issue motivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior literature, such as manual transformation, supervised feedback guided search, and PCA, either relies on domain knowledge or expensive supervised feedback, or suffers from large search space, or overlooks non-linear feature-feature interactions. UFTL imposes a major challenge on existing methods: how to design a new unsupervised paradigm that captures complex feature interactions and avoids large search space? To fill this gap, we connect graph, contrastive, and generative learning to develop a measurement-pretrain-finetune paradigm for UFTL. For unsupervised feature set utility measurement, we propose a feature value consistency preservation perspective and develop a mean discounted cumulative gain like unsupervised metric to evaluate feature set utility. For unsupervised feature set representation pretraining, we regard a feature set as a feature-feature interaction graph, and develop an unsupervised graph contrastive learning encoder to embed feature sets into vectors. For generative transformation finetuning, we regard a feature set as a feature cross sequence and feature transformation as sequential generation. We develop a deep generative feature transformation model that coordinates the pretrained feature set encoder and the gradient information extracted from a feature set utility evaluator to optimize a transformed feature generator.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16902",
        "abstract url": "https://arxiv.org/abs/2405.16902",
        "title": "Predicting from a Different Perspective in Re-ranking Model for Inductive Knowledge Graph Completion",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Rule-induction models have been shown great power in the inductive setting of knowledge graph completion. In this setting, the models are tested on a knowledge graph entirely composed of unseen entities. These models learn relation patterns as rules by utilizing subgraphs. The same input but different rules cause differences in the model's predictions. In this paper, we focus on this behavior of the model. We propose a re-ranking-based model called ReDistLP (Re-ranking with a Distinct Model for Link Prediction). This model enhances the effectiveness of re-ranking by leveraging the difference in the predictions between the initial retriever and the re-ranker. ReDistLP outperforms the state-of-the-art methods in 2 out of 3 datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 2 figures"
    },
    {
        "paper id": "2405.16918",
        "abstract url": "https://arxiv.org/abs/2405.16918",
        "title": "The Uncanny Valley: Exploring Adversarial Robustness from a Flatness Perspective",
        "rating": "-0.5",
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Flatness of the loss surface not only correlates positively with generalization but is also related to adversarial robustness, since perturbations of inputs relate non-linearly to perturbations of weights. In this paper, we empirically analyze the relation between adversarial examples and relative flatness with respect to the parameters of one layer. We observe a peculiar property of adversarial examples: during an iterative first-order white-box attack, the flatness of the loss surface measured around the adversarial example first becomes sharper until the label is flipped, but if we keep the attack running it runs into a flat uncanny valley where the label remains flipped. We find this phenomenon across various model architectures and datasets. Our results also extend to large language models (LLMs), but due to the discrete nature of the input space and comparatively weak attacks, the adversarial examples rarely reach a truly flat region. Most importantly, this phenomenon shows that flatness alone cannot explain adversarial robustness unless we can also guarantee the behavior of the function around the examples. We theoretically connect relative flatness to adversarial robustness by bounding the third derivative of the loss surface, underlining the need for flatness in combination with a low global Lipschitz constant for a robust model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16971",
        "abstract url": "https://arxiv.org/abs/2405.16971",
        "title": "A Correlation- and Mean-Aware Loss Function and Benchmarking Framework to Improve GAN-based Tabular Data Synthesis",
        "rating": "-0.5",
        "keywords": [
            [
                "GAN",
                "Synthesis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Advancements in science rely on data sharing. In medicine, where personal data are often involved, synthetic tabular data generated by generative adversarial networks (GANs) offer a promising avenue. However, existing GANs struggle to capture the complexities of real-world tabular data, which often contain a mix of continuous and categorical variables with potential imbalances and dependencies. We propose a novel correlation- and mean-aware loss function designed to address these challenges as a regularizer for GANs. To ensure a rigorous evaluation, we establish a comprehensive benchmarking framework using ten real-world datasets and eight established tabular GAN baselines. The proposed loss function demonstrates statistically significant improvements over existing methods in capturing the true data distribution, significantly enhancing the quality of synthetic data generated with GANs. The benchmarking framework shows that the enhanced synthetic data quality leads to improved performance in downstream machine learning (ML) tasks, ultimately paving the way for easier data sharing.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "n.a"
    },
    {
        "paper id": "2405.16978",
        "abstract url": "https://arxiv.org/abs/2405.16978",
        "title": "OSLO: One-Shot Label-Only Membership Inference Attacks",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs), which accurately infer a given sample's membership in a target model's training set with high precision using just \\emph{a single query}, where the target model only returns the predicted hard label. This is in contrast to state-of-the-art label-only attacks which require $\\sim6000$ queries, yet get attack precisions lower than OSLO's. OSLO leverages transfer-based black-box adversarial attacks. The core idea is that a member sample exhibits more resistance to adversarial perturbations than a non-member. We compare OSLO against state-of-the-art label-only attacks and demonstrate that, despite requiring only one query, our method significantly outperforms previous attacks in terms of precision and true positive rate (TPR) under the same false positive rates (FPR). For example, compared to previous label-only MIAs, OSLO achieves a TPR that is 7$\\times$ to 28$\\times$ stronger under a 0.1\\% FPR on CIFAR10 for a ResNet model. We evaluated multiple defense mechanisms against OSLO.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17034",
        "abstract url": "https://arxiv.org/abs/2405.17034",
        "title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectrum. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17035",
        "abstract url": "https://arxiv.org/abs/2405.17035",
        "title": "Glauber Generative Model: Discrete Diffusion Models via Binary Classification",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce the Glauber Generative Model (GGM), a new class of discrete diffusion models, to obtain new samples from a distribution given samples from a discrete space. GGM deploys a discrete Markov chain called the heat bath dynamics (or the Glauber dynamics) to denoise a sequence of noisy tokens to a sample from a joint distribution of discrete tokens. Our novel conceptual framework provides an exact reduction of the task of learning the denoising Markov chain to solving a class of binary classification tasks. More specifically, the model learns to classify a given token in a noisy sequence as signal or noise. In contrast, prior works on discrete diffusion models either solve regression problems to learn importance ratios, or minimize loss functions given by variational approximations. We apply GGM to language modeling and image generation, where images are discretized using image tokenizers like VQGANs. We show that it outperforms existing discrete diffusion models in language generation, and demonstrates strong performance for image generation without using dataset-specific image tokenizers. We also show that our model is capable of performing well in zero-shot control settings like text and image infilling.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17047",
        "abstract url": "https://arxiv.org/abs/2405.17047",
        "title": "Interpretable Robotic Manipulation from Language",
        "rating": "-0.5",
        "keywords": [
            [
                "voxel"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Humans naturally employ linguistic instructions to convey knowledge, a process that proves significantly more complex for machines, especially within the context of multitask robotic manipulation environments. Natural language, moreover, serves as the primary medium through which humans acquire new knowledge, presenting a potentially intuitive bridge for translating concepts understandable by humans into formats that can be learned by machines. In pursuit of facilitating this integration, we introduce an explainable behavior cloning agent, named Ex-PERACT, specifically designed for manipulation tasks. This agent is distinguished by its hierarchical structure, which incorporates natural language to enhance the learning process. At the top level, the model is tasked with learning a discrete skill code, while at the bottom level, the policy network translates the problem into a voxelized grid and maps the discretized actions to voxel grids. We evaluate our method across eight challenging manipulation tasks utilizing the RLBench benchmark, demonstrating that Ex-PERACT not only achieves competitive policy performance but also effectively bridges the gap between human instructions and machine execution in complex environments.",
        "subjects": [
            "cs.RO",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17049",
        "abstract url": "https://arxiv.org/abs/2405.17049",
        "title": "Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper explores methods for verifying the properties of Binary Neural Networks (BNNs), focusing on robustness against adversarial attacks. Despite their lower computational and memory needs, BNNs, like their full-precision counterparts, are also sensitive to input perturbations. Established methods for solving this problem are predominantly based on Satisfiability Modulo Theories and Mixed-Integer Linear Programming techniques, which are characterized by NP complexity and often face scalability issues. We introduce an alternative approach using Semidefinite Programming relaxations derived from sparse Polynomial Optimization. Our approach, compatible with continuous input space, not only mitigates numerical issues associated with floating-point calculations but also enhances verification scalability through the strategic use of tighter first-order semidefinite relaxations. We demonstrate the effectiveness of our method in verifying robustness against both $\\|.\\|_\\infty$ and $\\|.\\|_2$-based adversarial attacks.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": "22 pages, 2 figures, 7 tables"
    },
    {
        "paper id": "2405.17050",
        "abstract url": "https://arxiv.org/abs/2405.17050",
        "title": "HeNCler: Node Clustering in Heterophilous Graphs through Learned Asymmetric Similarity",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Clustering nodes in heterophilous graphs presents unique challenges due to the asymmetric relationships often overlooked by traditional methods, which moreover assume that good clustering corresponds to high intra-cluster and low inter-cluster connectivity. To address these issues, we introduce HeNCler - a novel approach for Heterophilous Node Clustering. Our method begins by defining a weighted kernel singular value decomposition to create an asymmetric similarity graph, applicable to both directed and undirected graphs. We further establish that the dual problem of this formulation aligns with asymmetric kernel spectral clustering, interpreting learned graph similarities without relying on homophily. We demonstrate the ability to solve the primal problem directly, circumventing the computational difficulties of the dual approach. Experimental evidence confirms that HeNCler significantly enhances performance in node clustering tasks within heterophilous graph contexts.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17068",
        "abstract url": "https://arxiv.org/abs/2405.17068",
        "title": "The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient Discretization for Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization. Langevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the simplest and most studied algorithm. LMC can suffer from slow convergence - requiring a large number of steps of small step-size to obtain good quality samples. This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps. Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions. However, important applications such as diffusion models involve non-log concave densities and contain time varying drift. We propose its variant, the Poisson Midpoint Method, which approximates a small step-size LMC with large step-sizes. We prove that this can obtain a quadratic speed up of LMC under very weak assumptions. We apply our method to diffusion models for image generation and show that it maintains the quality of DDPM with 1000 neural network calls with just 50-80 neural network calls and outperforms ODE based methods with similar compute.",
        "subjects": [
            "cs.LG",
            "math.NA",
            "stat.ML"
        ],
        "comment": "\"One often meets his destiny on the road he takes to avoid it\" - Master Oogway. My destiny seems to be to write triangle inequalities for the rest of my life"
    },
    {
        "paper id": "2405.17072",
        "abstract url": "https://arxiv.org/abs/2405.17072",
        "title": "A novel framework for systematic propositional formula simplification based on existential graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents a novel simplification calculus for propositional logic derived from Peirce's existential graphs' rules of inference and implication graphs. Our rules can be applied to propositional logic formulae in nested form, are equivalence-preserving, guarantee a monotonically decreasing number of variables, clauses and literals, and maximise the preservation of structural problem information. Our techniques can also be seen as higher-level SAT preprocessing, and we show how one of our rules (TWSR) generalises and streamlines most of the known equivalence-preserving SAT preprocessing methods. In addition, we propose a simplification procedure based on the systematic application of two of our rules (EPR and TWSR) which is solver-agnostic and can be used to simplify large Boolean satisfiability problems and propositional formulae in arbitrary form, and we provide a formal analysis of its algorithmic complexity in terms of space and time. Finally, we show how our rules can be further extended with a novel n-ary implication graph to capture all known equivalence-preserving preprocessing procedures.",
        "subjects": [
            "cs.LO",
            "cs.AI",
            "math.LO"
        ],
        "comment": "19 pages, 12 figures. Under consideration in Theory and Practice of Logic Programming (TPLP)"
    },
    {
        "paper id": "2405.17111",
        "abstract url": "https://arxiv.org/abs/2405.17111",
        "title": "Diffusion Bridge AutoEncoders for Unsupervised Representation Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from a sample and to adjust the dimensionality of a latent variable z. Meanwhile, this auxiliary structure invokes information split problem because the diffusion and the auxiliary encoder would divide the information from the sample into two representations for each model. Particularly, the information modeled by the diffusion becomes over-regularized because of the static prior distribution on xT. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enable z-dependent endpoint xT inference through a feed-forward architecture. This structure creates an information bottleneck at z, so xT becomes dependent on z in its generation. This results in two consequences: 1) z holds the full information of samples, and 2) xT becomes a learnable distribution, not static any further. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17176",
        "abstract url": "https://arxiv.org/abs/2405.17176",
        "title": "DreamMat: High-quality PBR Material Generation with Geometry- and Light-aware Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "2D diffusion model, which often contains unwanted baked-in shading effects and results in unrealistic rendering effects in the downstream applications. Generating Physically Based Rendering (PBR) materials instead of just RGB textures would be a promising solution. However, directly distilling the PBR material parameters from 2D diffusion models still suffers from incorrect material decomposition, such as baked-in shading effects in albedo. We introduce DreamMat, an innovative approach to resolve the aforementioned problem, to generate high-quality PBR materials from text descriptions. We find out that the main reason for the incorrect material distillation is that large-scale 2D diffusion models are only trained to generate final shading colors, resulting in insufficient constraints on material decomposition during distillation. To tackle this problem, we first finetune a new light-aware 2D diffusion model to condition on a given lighting environment and generate the shading results on this specific lighting condition. Then, by applying the same environment lights in the material distillation, DreamMat can generate high-quality PBR materials that are not only consistent with the given geometry but also free from any baked-in shading effects in albedo. Extensive experiments demonstrate that the materials produced through our methods exhibit greater visual appeal to users and achieve significantly superior rendering quality compared to baseline methods, which are preferable for downstream tasks such as game and film production.",
        "subjects": [
            "cs.GR",
            "cs.AI"
        ],
        "comment": "Accepted to SIGGRAPH 2024"
    },
    {
        "paper id": "2405.17253",
        "abstract url": "https://arxiv.org/abs/2405.17253",
        "title": "Gaussian Embedding of Temporal Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Representing the nodes of continuous-time temporal graphs in a low-dimensional latent space has wide-ranging applications, from prediction to visualization. Yet, analyzing continuous-time relational data with timestamped interactions introduces unique challenges due to its sparsity. Merely embedding nodes as trajectories in the latent space overlooks this sparsity, emphasizing the need to quantify uncertainty around the latent positions. In this paper, we propose TGNE (\\textbf{T}emporal \\textbf{G}aussian \\textbf{N}etwork \\textbf{E}mbedding), an innovative method that bridges two distinct strands of literature: the statistical analysis of networks via Latent Space Models (LSM)\\cite{Hoff2002} and temporal graph machine learning. TGNE embeds nodes as piece-wise linear trajectories of Gaussian distributions in the latent space, capturing both structural information and uncertainty around the trajectories. We evaluate TGNE's effectiveness in reconstructing the original graph and modelling uncertainty. The results demonstrate that TGNE generates competitive time-varying embedding locations compared to common baselines for reconstructing unobserved edge interactions based on observed edges. Furthermore, the uncertainty estimates align with the time-varying degree distribution in the network, providing valuable insights into the temporal dynamics of the graph. To facilitate reproducibility, we provide an open-source implementation of TGNE at \\url{https://github.com/aida-ugent/tgne}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17279",
        "abstract url": "https://arxiv.org/abs/2405.17279",
        "title": "Socially-Aware Shared Control Navigation for Assistive Mobile Robots in the Built Environment",
        "rating": "-0.5",
        "keywords": [
            [
                "Navigation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "As the number of Persons with Disabilities (PWD), particularly those with one or more physical impairments, increases, there is an increasing demand for assistive robotic technologies that can support independent mobility in the built environment and reduce the burden on caregivers. Current assistive mobility platforms (e.g., robotic wheelchairs) often fail to incorporate user preferences and control, leading to reduced trust and efficiency. Existing shared control algorithms do not allow the incorporation of the user control preferences inside the navigation framework or the path planning algorithm. In addition, existing dynamic local planner algorithms for robotic wheelchairs do not take into account the social spaces of people, potentially leading such platforms to infringe upon these areas and cause discomfort. To address these concerns, this work introduces a novel socially-aware shared autonomy-based navigation system for assistive mobile robotic platforms. Our navigation framework comprises a Global Planner and a Local Planner. To implement the Global Planner, the proposed approach introduces a novel User Preference Field (UPF) theory within its global planning framework, explicitly acknowledging user preferences to adeptly navigate away from congested areas. For the Local Planner, we propose a Socially-aware Shared Control-based Model Predictive Control with Dynamic Control Barrier Function (SS-MPC-DCBF) to adjust movements in real-time, integrating user preferences for safer, more autonomous navigation. Evaluation results show that our Global Planner aligns closely with user preferences compared to baselines, and our Local Planner demonstrates enhanced safety and efficiency in dynamic and static scenarios. This integrated approach fosters trust and autonomy, crucial for the acceptance of assistive mobility technologies in the built environment.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.HC"
        ],
        "comment": "42 pages, 14 figures"
    },
    {
        "paper id": "2405.17311",
        "abstract url": "https://arxiv.org/abs/2405.17311",
        "title": "Probabilistic Graph Rewiring via Virtual Nodes",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning. Despite their effectiveness, MPNNs face challenges such as under-reaching and over-squashing, where limited receptive fields and structural bottlenecks hinder information flow in the graph. While graph transformers hold promise in addressing these issues, their scalability is limited due to quadratic complexity regarding the number of nodes, rendering them impractical for larger graphs. Here, we propose \\emph{implicitly rewired message-passing neural networks} (IPR-MPNNs), a novel approach that integrates \\emph{implicit} probabilistic graph rewiring into MPNNs. By introducing a small number of virtual nodes, i.e., adding additional nodes to a given graph and connecting them to existing nodes, in a differentiable, end-to-end manner, IPR-MPNNs enable long-distance message propagation, circumventing quadratic complexity. Theoretically, we demonstrate that IPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we validate our approach by showcasing its ability to mitigate under-reaching and over-squashing effects, achieving state-of-the-art performance across multiple graph datasets. Notably, IPR-MPNNs outperform graph transformers while maintaining significantly faster computational efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2310.02156"
    },
    {
        "paper id": "2405.17346",
        "abstract url": "https://arxiv.org/abs/2405.17346",
        "title": "Prompt Optimization with Human Feedback",
        "rating": "-0.5",
        "keywords": [
            [
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable performances in various tasks. However, the performance of LLMs heavily depends on the input prompt, which has given rise to a number of recent works on prompt optimization. However, previous works often require the availability of a numeric score to assess the quality of every prompt. Unfortunately, when a human user interacts with a black-box LLM, attaining such a score is often infeasible and unreliable. Instead, it is usually significantly easier and more reliable to obtain preference feedback from a human user, i.e., showing the user the responses generated from a pair of prompts and asking the user which one is preferred. Therefore, in this paper, we study the problem of prompt optimization with human feedback (POHF), in which we aim to optimize the prompt for a black-box LLM using only human preference feedback. Drawing inspiration from dueling bandits, we design a theoretically principled strategy to select a pair of prompts to query for preference feedback in every iteration, and hence introduce our algorithm named automated POHF (APOHF). We apply our APOHF algorithm to various tasks, including optimizing user instructions, prompt optimization for text-to-image generative models, and response optimization with human feedback (i.e., further refining the response using a variant of our APOHF). The results demonstrate that our APOHF can efficiently find a good prompt using a small number of preference feedback instances. Our code can be found at \\url{https://github.com/xqlin98/APOHF}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Preprint, 18 pages"
    },
    {
        "paper id": "2405.17370",
        "abstract url": "https://arxiv.org/abs/2405.17370",
        "title": "Model-Agnostic Zeroth-Order Policy Optimization for Meta-Learning of Ergodic Linear Quadratic Regulators",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Meta-learning has been proposed as a promising machine learning topic in recent years, with important applications to image classification, robotics, computer games, and control systems. In this paper, we study the problem of using meta-learning to deal with uncertainty and heterogeneity in ergodic linear quadratic regulators. We integrate the zeroth-order optimization technique with a typical meta-learning method, proposing an algorithm that omits the estimation of policy Hessian, which applies to tasks of learning a set of heterogeneous but similar linear dynamic systems. The induced meta-objective function inherits important properties of the original cost function when the set of linear dynamic systems are meta-learnable, allowing the algorithm to optimize over a learnable landscape without projection onto the feasible set. We provide a convergence result for the exact gradient descent process by analyzing the boundedness and smoothness of the gradient for the meta-objective, which justify the proposed algorithm with gradient estimation error being small. We also provide a numerical example to corroborate this perspective.",
        "subjects": [
            "eess.SY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17372",
        "abstract url": "https://arxiv.org/abs/2405.17372",
        "title": "BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction",
        "rating": "-0.5",
        "keywords": [
            [
                "Autonomous Driving"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Simulating realistic interactions among traffic agents is crucial for efficiently validating the safety of autonomous driving systems. Existing leading simulators primarily use an encoder-decoder structure to encode the historical trajectories for future simulation. However, such a paradigm complicates the model architecture, and the manual separation of history and future trajectories leads to low data utilization. To address these challenges, we propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a decoder-only, autoregressive architecture designed to simulate the sequential motion of multiple agents. Crucially, our approach discards the traditional separation between \"history\" and \"future,\" treating each time step as the \"current\" one, resulting in a simpler, more parameter- and data-efficient design that scales seamlessly with data and computation. Additionally, we introduce the Next-Patch Prediction Paradigm (NP3), which enables models to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. BehaviorGPT ranks first across several metrics on the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in multi-agent and agent-map interactions. We outperformed state-of-the-art models with a realism score of 0.741 and improved the minADE metric to 1.540, with an approximately 91.6% reduction in model parameters.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17377",
        "abstract url": "https://arxiv.org/abs/2405.17377",
        "title": "How Does Perfect Fitting Affect Representation Learning? On the Training Dynamics of Representations in Deep Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we elucidate how representations in deep neural networks (DNNs) evolve during training. We focus on overparameterized learning settings where the training continues much after the trained DNN starts to perfectly fit its training data. We examine the evolution of learned representations along the entire training process, including its perfect fitting regime, and with respect to the epoch-wise double descent phenomenon. We explore the representational similarity of DNN layers, each layer with respect to its own representations throughout the training process. For this, we use two similarity metrics: (1) The centered kernel alignment (CKA) similarity; (2) Similarity of decision regions of linear classifier probes that we train for the DNN layers. Our extensive experiments discover training dynamics patterns that can emerge in layers depending on the relative layer-depth, DNN width, and architecture. We show that representations at the deeper layers evolve much more in the training when an epoch-wise double descent occurs. For Vision Transformer, we show that the perfect fitting threshold creates a transition in the evolution of representations across all the encoder blocks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17403",
        "abstract url": "https://arxiv.org/abs/2405.17403",
        "title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Training diffusion models is always a computation-intensive task. In this paper, we introduce a novel speed-up method for diffusion model training, called, which is based on a closer look at time steps. Our key findings are: i) Time steps can be empirically divided into acceleration, deceleration, and convergence areas based on the process increment. ii) These time steps are imbalanced, with many concentrated in the convergence area. iii) The concentrated steps provide limited benefits for diffusion training. To address this, we design an asymmetric sampling strategy that reduces the frequency of steps from the convergence area while increasing the sampling probability for steps from other areas. Additionally, we propose a weighting strategy to emphasize the importance of time steps with rapid-change process increments. As a plug-and-play and architecture-agnostic approach, SpeeD consistently achieves 3-times acceleration across various diffusion architectures, datasets, and tasks. Notably, due to its simple design, our approach significantly reduces the cost of diffusion model training with minimal overhead. Our research enables more researchers to train diffusion models at a lower cost.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17404",
        "abstract url": "https://arxiv.org/abs/2405.17404",
        "title": "Spectral Greedy Coresets for Graph Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The ubiquity of large-scale graphs in node-classification tasks significantly hinders the real-world applications of Graph Neural Networks (GNNs). Node sampling, graph coarsening, and dataset condensation are effective strategies for enhancing data efficiency. However, owing to the interdependence of graph nodes, coreset selection, which selects subsets of the data examples, has not been successfully applied to speed up GNN training on large graphs, warranting special treatment. This paper studies graph coresets for GNNs and avoids the interdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs around a node) based on their spectral embeddings. We decompose the coreset selection problem for GNNs into two phases: a coarse selection of widely spread ego graphs and a refined selection to diversify their topologies. We design a greedy algorithm that approximately optimizes both objectives. Our spectral greedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates the need for model pre-training, and applies to low-homophily graphs. Extensive experiments on ten datasets demonstrate that SGGC outperforms other coreset methods by a wide margin, generalizes well across GNN architectures, and is much faster than graph condensation.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17514",
        "abstract url": "https://arxiv.org/abs/2405.17514",
        "title": "AbstractBeam: Enhancing Bottom-Up Program Synthesis using Library Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Synthesis"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "LambdaBeam is a state-of-the-art execution-guided algorithm for program synthesis that incorporates higher-order functions, lambda functions, and iterative loops into the Domain-Specific Language (DSL). LambdaBeam generates every program from the start. Yet, many program blocks or subprograms occur frequently in a given domain, e.g., loops to traverse a list. Thus, repeating programs can be used to enhance the synthesis algorithm. However, LambdaBeam fails to leverage this potential. For this purpose, we introduce AbstractBeam: A novel program synthesis framework that employs Library Learning to identify such program repetitions, integrates them into the DSL, and thus utilizes their potential to boost LambdaBeam's synthesis algorithm. Our experimental evaluations demonstrate that AbstractBeam significantly improves LambdaBeam's performance in the LambdaBeam integer list manipulation domain. Additionally, AbstractBeam's program generation is more efficient compared to LambdaBeam's synthesis. Finally, our findings indicate that Library Learning is effective in domains not specifically crafted to highlight its benefits.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17522",
        "abstract url": "https://arxiv.org/abs/2405.17522",
        "title": "Efficient Model Compression for Hierarchical Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL), as an emerging collaborative learning paradigm, has garnered significant attention due to its capacity to preserve privacy within distributed learning systems. In these systems, clients collaboratively train a unified neural network model using their local datasets and share model parameters rather than raw data, enhancing privacy. Predominantly, FL systems are designed for mobile and edge computing environments where training typically occurs over wireless networks. Consequently, as model sizes increase, the conventional FL frameworks increasingly consume substantial communication resources. To address this challenge and improve communication efficiency, this paper introduces a novel hierarchical FL framework that integrates the benefits of clustered FL and model compression. We present an adaptive clustering algorithm that identifies a core client and dynamically organizes clients into clusters. Furthermore, to enhance transmission efficiency, each core client implements a local aggregation with compression (LC aggregation) algorithm after collecting compressed models from other clients within the same cluster. Simulation results affirm that our proposed algorithms not only maintain comparable predictive accuracy but also significantly reduce energy consumption relative to existing FL mechanisms.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17535",
        "abstract url": "https://arxiv.org/abs/2405.17535",
        "title": "Calibrated Dataset Condensation for Faster Hyperparameter Search",
        "rating": "-0.5",
        "keywords": [
            [
                "architecture search"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Dataset condensation can be used to reduce the computational cost of training multiple models on a large dataset by condensing the training dataset into a small synthetic set. State-of-the-art approaches rely on matching the model gradients between the real and synthetic data. However, there is no theoretical guarantee of the generalizability of the condensed data: data condensation often generalizes poorly across hyperparameters/architectures in practice. This paper considers a different condensation objective specifically geared toward hyperparameter search. We aim to generate a synthetic validation dataset so that the validation-performance rankings of the models, with different hyperparameters, on the condensed and original datasets are comparable. We propose a novel hyperparameter-calibrated dataset condensation (HCDC) algorithm, which obtains the synthetic validation dataset by matching the hyperparameter gradients computed via implicit differentiation and efficient inverse Hessian approximation. Experiments demonstrate that the proposed framework effectively maintains the validation-performance rankings of models and speeds up hyperparameter/architecture search for tasks on both images and graphs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17538",
        "abstract url": "https://arxiv.org/abs/2405.17538",
        "title": "Bayesian RG Flow in Neural Network Field Theories",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Neural Network Field Theory correspondence (NNFT) is a mapping from neural network (NN) architectures into the space of statistical field theories (SFTs). The Bayesian renormalization group (BRG) is an information-theoretic coarse graining scheme that generalizes the principles of the Exact Renormalization Group (ERG) to arbitrarily parameterized probability distributions, including those of NNs. In BRG, coarse graining is performed in parameter space with respect to an information-theoretic distinguishability scale set by the Fisher information metric. In this paper, we unify NNFT and BRG to form a powerful new framework for exploring the space of NNs and SFTs, which we coin BRG-NNFT. With BRG-NNFT, NN training dynamics can be interpreted as inducing a flow in the space of SFTs from the information-theoretic `IR' $\\rightarrow$ `UV'. Conversely, applying an information-shell coarse graining to the trained network's parameters induces a flow in the space of SFTs from the information-theoretic `UV' $\\rightarrow$ `IR'. When the information-theoretic cutoff scale coincides with a standard momentum scale, BRG is equivalent to ERG. We demonstrate the BRG-NNFT correspondence on two analytically tractable examples. First, we construct BRG flows for trained, infinite-width NNs, of arbitrary depth, with generic activation functions. As a special case, we then restrict to architectures with a single infinitely-wide layer, scalar outputs, and generalized cos-net activations. In this case, we show that BRG coarse-graining corresponds exactly to the momentum-shell ERG flow of a free scalar SFT. Our analytic results are corroborated by a numerical experiment in which an ensemble of asymptotically wide NNs are trained and subsequently renormalized using an information-shell BRG scheme.",
        "subjects": [
            "hep-th",
            "cond-mat.dis-nn",
            "cs.LG"
        ],
        "comment": "46 pages, 10 figures, 2 tables"
    },
    {
        "paper id": "2405.17541",
        "abstract url": "https://arxiv.org/abs/2405.17541",
        "title": "Approximately-symmetric neural networks for quantum spin liquids",
        "rating": "-0.5",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose and analyze a family of approximately-symmetric neural networks for quantum spin liquid problems. These tailored architectures are parameter-efficient, scalable, and significantly out-perform existing symmetry-unaware neural network architectures. Utilizing the mixed-field toric code model, we demonstrate that our approach is competitive with the state-of-the-art tensor network and quantum Monte Carlo methods. Moreover, at the largest system sizes (N=480), our method allows us to explore Hamiltonians with sign problems beyond the reach of both quantum Monte Carlo and finite-size matrix-product states. The network comprises an exactly symmetric block following a non-symmetric block, which we argue learns a transformation of the ground state analogous to quasiadiabatic continuation. Our work paves the way toward investigating quantum spin liquid problems within interpretable neural network architectures",
        "subjects": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cond-mat.str-el",
            "cs.LG"
        ],
        "comment": "5+10 pages"
    },
    {
        "paper id": "2405.17573",
        "abstract url": "https://arxiv.org/abs/2405.17573",
        "title": "Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We study Leaky ResNets, which interpolate between ResNets ($\\tilde{L}=0$) and Fully-Connected nets ($\\tilde{L}\\to\\infty$) depending on an 'effective depth' hyper-parameter $\\tilde{L}$. In the infinite depth limit, we study 'representation geodesics' $A_{p}$: continuous paths in representation space (similar to NeuralODEs) from input $p=0$ to output $p=1$ that minimize the parameter norm of the network. We give a Lagrangian and Hamiltonian reformulation, which highlight the importance of two terms: a kinetic energy which favors small layer derivatives $\\partial_{p}A_{p}$ and a potential energy that favors low-dimensional representations, as measured by the 'Cost of Identity'. The balance between these two forces offers an intuitive understanding of feature learning in ResNets. We leverage this intuition to explain the emergence of a bottleneck structure, as observed in previous work: for large $\\tilde{L}$ the potential energy dominates and leads to a separation of timescales, where the representation jumps rapidly from the high dimensional inputs to a low-dimensional representation, move slowly inside the space of low-dimensional representations, before jumping back to the potentially high-dimensional outputs. Inspired by this phenomenon, we train with an adaptive layer step-size to adapt to the separation of timescales.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17612",
        "abstract url": "https://arxiv.org/abs/2405.17612",
        "title": "A note on the error analysis of data-driven closure models for large eddy simulations of turbulence",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we provide a mathematical formulation for error propagation in flow trajectory prediction using data-driven turbulence closure modeling. Under the assumption that the predicted state of a large eddy simulation prediction must be close to that of a subsampled direct numerical simulation, we retrieve an upper bound for the prediction error when utilizing a data-driven closure model. We also demonstrate that this error is significantly affected by the time step size and the Jacobian which play a role in amplifying the initial one-step error made by using the closure. Our analysis also shows that the error propagates exponentially with rollout time and the upper bound of the system Jacobian which is itself influenced by the Jacobian of the closure formulation. These findings could enable the development of new regularization techniques for ML models based on the identified error-bound terms, improving their robustness and reducing error propagation.",
        "subjects": [
            "physics.flu-dyn",
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17724",
        "abstract url": "https://arxiv.org/abs/2405.17724",
        "title": "ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion",
                "Synthesis"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent research in tabular data synthesis has focused on single tables, whereas real-world applications often involve complex data with tens or hundreds of interconnected tables. Previous approaches to synthesizing multi-relational (multi-table) data fall short in two key aspects: scalability for larger datasets and capturing long-range dependencies, such as correlations between attributes spread across different tables. Inspired by the success of diffusion models in tabular data modeling, we introduce $\\textbf{C}luster$ $\\textbf{La}tent$ $\\textbf{Va}riable$ $guided$ $\\textbf{D}enoising$ $\\textbf{D}iffusion$ $\\textbf{P}robabilistic$ $\\textbf{M}odels$ (ClavaDDPM). This novel approach leverages clustering labels as intermediaries to model relationships between tables, specifically focusing on foreign key constraints. ClavaDDPM leverages the robust generation capabilities of diffusion models while incorporating efficient algorithms to propagate the learned latent variables across tables. This enables ClavaDDPM to capture long-range dependencies effectively. Extensive evaluations on multi-table datasets of varying sizes show that ClavaDDPM significantly outperforms existing methods for these long-range dependencies while remaining competitive on utility metrics for single-table data.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17725",
        "abstract url": "https://arxiv.org/abs/2405.17725",
        "title": "Color Shift Estimation-and-Correction for Image Enhancement",
        "rating": "-0.5",
        "keywords": [
            [
                "Image Enhancement"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Images captured under sub-optimal illumination conditions may contain both over- and under-exposures. Current approaches mainly focus on adjusting image brightness, which may exacerbate the color tone distortion in under-exposed areas and fail to restore accurate colors in over-exposed regions. We observe that over- and under-exposed regions display opposite color tone distribution shifts with respect to each other, which may not be easily normalized in joint modeling as they usually do not have ``normal-exposed'' regions/pixels as reference. In this paper, we propose a novel method to enhance images with both over- and under-exposures by learning to estimate and correct such color shifts. Specifically, we first derive the color feature maps of the brightened and darkened versions of the input image via a UNet-based network, followed by a pseudo-normal feature generator to produce pseudo-normal color feature maps. We then propose a novel COlor Shift Estimation (COSE) module to estimate the color shifts between the derived brightened (or darkened) color feature maps and the pseudo-normal color feature maps. The COSE module corrects the estimated color shifts of the over- and under-exposed regions separately. We further propose a novel COlor MOdulation (COMO) module to modulate the separately corrected colors in the over- and under-exposed regions to produce the enhanced image. Comprehensive experiments show that our method outperforms existing approaches. Project webpage: https://github.com/yiyulics/CSEC.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR2024 accepted paper"
    },
    {
        "paper id": "2405.17728",
        "abstract url": "https://arxiv.org/abs/2405.17728",
        "title": "Facilitating Holistic Evaluations with LLMs: Insights from Scenario-Based Experiments",
        "rating": "-0.5",
        "keywords": [
            [
                "synthesize"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Workshop courses designed to foster creativity are gaining popularity. However, achieving a holistic evaluation that accommodates diverse perspectives is challenging, even for experienced faculty teams. Adequate discussion is essential to integrate varied assessments, but faculty often lack the time for such deliberations. Deriving an average score without discussion undermines the purpose of a holistic evaluation. This paper explores the use of a Large Language Model (LLM) as a facilitator to integrate diverse faculty assessments. Scenario-based experiments were conducted to determine if the LLM could synthesize diverse evaluations and explain the underlying theories to faculty. The results were noteworthy, showing that the LLM effectively facilitated faculty discussions. Additionally, the LLM demonstrated the capability to generalize and create evaluation criteria from a single scenario based on its learned domain knowledge.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17750",
        "abstract url": "https://arxiv.org/abs/2405.17750",
        "title": "Magnitude-based Neuron Pruning for Backdoor Defens",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. In this paper, we investigate the correlation between backdoor behavior and neuron magnitude, and find that backdoor neurons deviate from the magnitude-saliency correlation of the model. The deviation inspires us to propose a Magnitude-based Neuron Pruning (MNP) method to detect and prune backdoor neurons. Specifically, MNP uses three magnitude-guided objective functions to manipulate the magnitude-saliency correlation of backdoor neurons, thus achieving the purpose of exposing backdoor behavior, eliminating backdoor neurons and preserving clean neurons, respectively. Experiments show our pruning strategy achieves state-of-the-art backdoor defense performance against a variety of backdoor attacks with a limited amount of clean data, demonstrating the crucial role of magnitude for guiding backdoor defenses.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17765",
        "abstract url": "https://arxiv.org/abs/2405.17765",
        "title": "PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild",
        "rating": "-0.5",
        "keywords": [
            [
                "Quality Assessment"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Video quality assessment (VQA) is a challenging problem due to the numerous factors that can affect the perceptual quality of a video, \\eg, content attractiveness, distortion type, motion pattern, and level. However, annotating the Mean opinion score (MOS) for videos is expensive and time-consuming, which limits the scale of VQA datasets, and poses a significant obstacle for deep learning-based methods. In this paper, we propose a VQA method named PTM-VQA, which leverages PreTrained Models to transfer knowledge from models pretrained on various pre-tasks, enabling benefits for VQA from different aspects. Specifically, we extract features of videos from different pretrained models with frozen weights and integrate them to generate representation. Since these models possess various fields of knowledge and are often trained with labels irrelevant to quality, we propose an Intra-Consistency and Inter-Divisibility (ICID) loss to impose constraints on features extracted by multiple pretrained models. The intra-consistency constraint ensures that features extracted by different pretrained models are in the same unified quality-aware latent space, while the inter-divisibility introduces pseudo clusters based on the annotation of samples and tries to separate features of samples from different clusters. Furthermore, with a constantly growing number of pretrained models, it is crucial to determine which models to use and how to use them. To address this problem, we propose an efficient scheme to select suitable candidates. Models with better clustering performance on VQA datasets are chosen to be our candidates. Extensive experiments demonstrate the effectiveness of the proposed method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "CVPR 2024, 11 pages, 4 figures, 7 tables"
    },
    {
        "paper id": "2405.17768",
        "abstract url": "https://arxiv.org/abs/2405.17768",
        "title": "Revisiting the Message Passing in Heterophilous Graph Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors. However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNNs). Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success. This raises the question: why does message passing remain effective on heterophilous graphs? To answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism. Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes. Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs. To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix. A thorough evaluation involving 10 benchmark datasets and comparative analysis against 13 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17779",
        "abstract url": "https://arxiv.org/abs/2405.17779",
        "title": "Online Analytic Exemplar-Free Continual Learning with Large Models for Imbalanced Autonomous Driving Task",
        "rating": "-0.5",
        "keywords": [
            [
                "Autonomous Driving"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the field of autonomous driving, even a meticulously trained model can encounter failures when faced with unfamiliar sceanrios. One of these scenarios can be formulated as an online continual learning (OCL) problem. That is, data come in an online fashion, and models are updated according to these streaming data. Two major OCL challenges are catastrophic forgetting and data imbalance. To address these challenges, in this paper, we propose an Analytic Exemplar-Free Online Continual Learning (AEF-OCL). The AEF-OCL leverages analytic continual learning principles and employs ridge regression as a classifier for features extracted by a large backbone network. It solves the OCL problem by recursively calculating the analytical solution, ensuring an equalization between the continual learning and its joint-learning counterpart, and works without the need to save any used samples (i.e., exemplar-free). Additionally, we introduce a Pseudo-Features Generator (PFG) module that recursively estimates the deviation of real features. The PFG generates offset pseudo-features following a normal distribution, thereby addressing the data imbalance issue. Experimental results demonstrate that despite being an exemplar-free strategy, our method outperforms various methods on the autonomous driving SODA10M dataset. Source code is available at https://github.com/ZHUANGHP/Analytic-continual-learning.",
        "subjects": [
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.20774",
        "abstract url": "https://arxiv.org/abs/2405.20774",
        "title": "Exploring Backdoor Attacks against Large Language Model-based Decision Making",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown significant promise in decision-making tasks when fine-tuned on specific applications, leveraging their inherent common sense and reasoning abilities learned from vast amounts of data. However, these systems are exposed to substantial safety and security risks during the fine-tuning phase. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-enabled Decision-making systems (BALD), systematically exploring how such attacks can be introduced during the fine-tuning phase across various channels. Specifically, we propose three attack mechanisms and corresponding backdoor optimization methods to attack different components in the LLM-based decision-making pipeline: word injection, scenario manipulation, and knowledge injection. Word injection embeds trigger words directly into the query prompt. Scenario manipulation occurs in the physical environment, where a high-level backdoor semantic scenario triggers the attack. Knowledge injection conducts backdoor attacks on retrieval augmented generation (RAG)-based LLM systems, strategically injecting word triggers into poisoned knowledge while ensuring the information remains factually accurate for stealthiness. We conduct extensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using two datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and stealthiness of our backdoor triggers and mechanisms. Finally, we critically assess the strengths and weaknesses of our proposed approaches, highlight the inherent vulnerabilities of LLMs in decision-making tasks, and evaluate potential defenses to safeguard LLM-based decision making systems.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "27 pages, including main paper, references, and appendix"
    },
    {
        "paper id": "2405.16850",
        "abstract url": "https://arxiv.org/abs/2405.16850",
        "title": "UniCompress: Enhancing Multi-Data Medical Image Compression with Knowledge Distillation",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "CT"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In the field of medical image compression, Implicit Neural Representation (INR) networks have shown remarkable versatility due to their flexible compression ratios, yet they are constrained by a one-to-one fitting approach that results in lengthy encoding times. Our novel method, ``\\textbf{UniCompress}'', innovatively extends the compression capabilities of INR by being the first to compress multiple medical data blocks using a single INR network. By employing wavelet transforms and quantization, we introduce a codebook containing frequency domain information as a prior input to the INR network. This enhances the representational power of INR and provides distinctive conditioning for different image blocks. Furthermore, our research introduces a new technique for the knowledge distillation of implicit representations, simplifying complex model knowledge into more manageable formats to improve compression ratios. Extensive testing on CT and electron microscopy (EM) datasets has demonstrated that UniCompress outperforms traditional INR methods and commercial compression solutions like HEVC, especially in complex and high compression scenarios. Notably, compared to existing INR techniques, UniCompress achieves a 4$\\sim$5 times increase in compression speed, marking a significant advancement in the field of medical image compression. Codes will be publicly available.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16874",
        "abstract url": "https://arxiv.org/abs/2405.16874",
        "title": "CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "avatar"
            ],
            [
                "diffusion",
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The dataset will be released as soon as possible"
    },
    {
        "paper id": "2405.16877",
        "abstract url": "https://arxiv.org/abs/2405.16877",
        "title": "Are Self-Attentions Effective for Time Series Forecasting?",
        "rating": "-1",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Time series forecasting is crucial for applications across multiple domains and various scenarios. Although Transformer models have dramatically shifted the landscape of forecasting, their effectiveness remains debated. Recent findings have indicated that simpler linear models might outperform complex Transformer-based approaches, highlighting the potential for more streamlined architectures. In this paper, we shift focus from the overall architecture of the Transformer to the effectiveness of self-attentions for time series forecasting. To this end, we introduce a new architecture, Cross-Attention-only Time Series transformer (CATS), that rethinks the traditional Transformer framework by eliminating self-attention and leveraging cross-attention mechanisms instead. By establishing future horizon-dependent parameters as queries and enhanced parameter sharing, our model not only improves long-term forecasting accuracy but also reduces the number of parameters and memory usage. Extensive experiment across various datasets demonstrates that our model achieves superior performance with the lowest mean squared error and uses fewer parameters compared to existing models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "20 pages, 14 figures, 13 tables. Submitted to NeurIPS 2024 (under review)"
    },
    {
        "paper id": "2405.16888",
        "abstract url": "https://arxiv.org/abs/2405.16888",
        "title": "Part123: Part-aware 3D Reconstruction from a Single-view Image",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, the emergence of diffusion models has opened up new opportunities for single-view reconstruction. However, all the existing methods represent the target object as a closed mesh devoid of any structural information, thus neglecting the part-based structure, which is crucial for many downstream applications, of the reconstructed shape. Moreover, the generated meshes usually suffer from large noises, unsmooth surfaces, and blurry textures, making it challenging to obtain satisfactory part segments using 3D segmentation techniques. In this paper, we present Part123, a novel framework for part-aware 3D reconstruction from a single-view image. We first use diffusion models to generate multiview-consistent images from a given image, and then leverage Segment Anything Model (SAM), which demonstrates powerful generalization ability on arbitrary objects, to generate multiview segmentation masks. To effectively incorporate 2D part-based information into 3D reconstruction and handle inconsistency, we introduce contrastive learning into a neural rendering framework to learn a part-aware feature space based on the multiview segmentation masks. A clustering-based algorithm is also developed to automatically derive 3D part segmentation results from the reconstructed models. Experiments show that our method can generate 3D models with high-quality segmented parts on various objects. Compared to existing unstructured reconstruction methods, the part-aware 3D models from our method benefit some important applications, including feature-preserving reconstruction, primitive fitting, and 3D shape editing.",
        "subjects": [
            "cs.GR",
            "cs.CV"
        ],
        "comment": "Accepted to SIGGRAPH 2024 (conference track),webpage: https://liuar0512.github.io/part123_official_page/"
    },
    {
        "paper id": "2405.16905",
        "abstract url": "https://arxiv.org/abs/2405.16905",
        "title": "Privacy and Security Trade-off in Interconnected Systems with Known or Unknown Privacy Noise Covariance",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "This paper is concerned with the security problem for interconnected systems, where each subsystem is required to detect local attacks using locally available information and the information received from its neighboring subsystems. Moreover, we consider that there exists an additional eavesdropper being able to infer the private information by eavesdropping transmitted data between subsystems. Then, a privacy-preserving method is employed by adding privacy noise to transmitted data, and the privacy level is measured by mutual information. Nevertheless, adding privacy noise to transmitted data may affect the detection performance metrics such as detection probability and false alarm probability. Thus, we theoretically analyze the trade-off between the privacy and the detection performance. An optimization problem with maximizing both the degree of privacy preservation and the detection probability is established to obtain the covariance of the privacy noise. In addition, the attack detector of each subsystem may not obtain all information about the privacy noise. We further theoretically analyze the trade-off between the privacy and the false alarm probability when the attack detector has no knowledge of the privacy noise covariance. An optimization problem with maximizing the degree of privacy preservation with guaranteeing a bound of false alarm distortion level is established to obtain {\\color{black}{the covariance of the privacy noise}}. Moreover, to analyze the effect of the privacy noise on the detection probability, we consider that each subsystem can estimate the unknown privacy noise covariance by the secondary data. Based on the estimated covariance, we construct another attack detector and analyze how the privacy noise affects its detection performance. Finally, a numerical example is provided to verify the effectiveness of theoretical results.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16907",
        "abstract url": "https://arxiv.org/abs/2405.16907",
        "title": "GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning",
        "rating": "-1",
        "keywords": [
            [
                "diffusion",
                "synthesizing"
            ],
            [
                "Trajectory"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions. Data augmentation techniques, such as noise injection and data synthesizing, aim to improve Q-function approximation by smoothing the learned state-action region. However, these methods often fall short of directly improving the quality of offline datasets, leading to suboptimal results. In response, we introduce \\textbf{GTA}, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible. GTA applies a diffusion model within the data augmentation framework. GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value. Our results show that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms in both dense and sparse reward settings. Furthermore, we conduct a quality analysis of data augmented by GTA and demonstrate that GTA improves the quality of the data. Our code is available at https://github.com/Jaewoopudding/GTA",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted (Spotlight) to ICLR 2024 Workshop on Generative Models for Decision Making. Jaewoo Lee and Sujin Yun are equal contribution authors"
    },
    {
        "paper id": "2405.16914",
        "abstract url": "https://arxiv.org/abs/2405.16914",
        "title": "Rigorous Simulation-based Testing for Autonomous Driving Systems -- Targeting the Achilles' Heel of Four Open Autopilots",
        "rating": "-1",
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ]
        ],
        "abstract": "Simulation-based testing remains the main approach for validating Autonomous Driving Systems. We propose a rigorous test method based on breaking down scenarios into simple ones, taking into account the fact that autopilots make decisions according to traffic rules whose application depends on local knowledge and context. This leads us to consider the autopilot as a dynamic system receiving three different types of vistas as input, each characterizing a specific driving operation and a corresponding control policy. The test method for the considered vista types generates test cases for critical configurations that place the vehicle under test in critical situations characterized by the transition from cautious behavior to progression in order to clear an obstacle. The test cases thus generated are realistic, i.e., they determine the initial conditions from which safe control policies are possible, based on knowledge of the vehicle's dynamic characteristics. Constraint analysis identifies the most critical test cases, whose success implies the validity of less critical ones. Test coverage can therefore be greatly simplified. Critical test cases reveal major defects in Apollo, Autoware, and the Carla and LGSVL autopilots. Defects include accidents, software failures, and traffic rule violations that would be difficult to detect by random simulation, as the test cases lead to situations characterized by finely-tuned parameters of the vehicles involved, such as their relative position and speed. Our results corroborate real-life observations and confirm that autonomous driving systems still have a long way to go before offering acceptable safety guarantees.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "57 pages, 19 figures, 29 tables"
    },
    {
        "paper id": "2405.16932",
        "abstract url": "https://arxiv.org/abs/2405.16932",
        "title": "CudaSIFT-SLAM: multiple-map visual SLAM for full procedure mapping in real human endoscopy",
        "rating": "-1",
        "keywords": [
            [
                "SLAM"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Monocular visual simultaneous localization and mapping (V-SLAM) is nowadays an irreplaceable tool in mobile robotics and augmented reality, where it performs robustly. However, human colonoscopies pose formidable challenges like occlusions, blur, light changes, lack of texture, deformation, water jets or tool interaction, which result in very frequent tracking losses. ORB-SLAM3, the top performing multiple-map V-SLAM, is unable to recover from them by merging sub-maps or relocalizing the camera, due to the poor performance of its place recognition algorithm based on ORB features and DBoW2 bag-of-words. We present CudaSIFT-SLAM, the first V-SLAM system able to process complete human colonoscopies in real-time. To overcome the limitations of ORB-SLAM3, we use SIFT instead of ORB features and replace the DBoW2 direct index with the more computationally demanding brute-force matching, being able to successfully match images separated in time for relocation and map merging. Real-time performance is achieved thanks to CudaSIFT, a GPU implementation for SIFT extraction and brute-force matching. We benchmark our system in the C3VD phantom colon dataset, and in a full real colonoscopy from the Endomapper dataset, demonstrating the capabilities to merge sub-maps and relocate in them, obtaining significantly longer sub-maps. Our system successfully maps in real-time 88 % of the frames in the C3VD dataset. In a real screening colonoscopy, despite the much higher prevalence of occluded and blurred frames, the mapping coverage is 53 % in carefully explored areas and 38 % in the full sequence, a 70 % improvement over ORB-SLAM3.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "10 pages, 10 figures, 6 tables, under revision"
    },
    {
        "paper id": "2405.16936",
        "abstract url": "https://arxiv.org/abs/2405.16936",
        "title": "Construction of birational trilinear volumes via tensor rank criteria",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "We provide effective methods to construct and manipulate trilinear birational maps $\u03c6:(\\mathbb{P}^1)^3\\dashrightarrow \\mathbb{P}^3$ by establishing a novel connection between birationality and tensor rank. These yield four families of nonlinear birational transformations between 3D spaces that can be operated with enough flexibility for applications in computer-aided geometric design. More precisely, we describe the geometric constraints on the defining control points of the map that are necessary for birationality, and present constructions for such configurations. For adequately constrained control points, we prove that birationality is achieved if and only if a certain $2\\times 2\\times 2$ tensor has rank one. As a corollary, we prove that the locus of weights that ensure birationality is $\\mathbb{P}^1\\times\\mathbb{P}^1\\times\\mathbb{P}^1$. Additionally, we provide formulas for the inverse $\u03c6^{-1}$ as well as the explicit defining equations of the irreducible components of the base loci. Finally, we introduce a notion of \"distance to birationality\" for trilinear rational maps, and explain how to continuously deform birational maps.",
        "subjects": [
            "math.AG",
            "cs.SC"
        ],
        "comment": "30 pages, 12 figures"
    },
    {
        "paper id": "2405.16938",
        "abstract url": "https://arxiv.org/abs/2405.16938",
        "title": "Remote control system of a binary tree of switches -- I. constraints and inequalities",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "We study a tree coloring model introduced by Guidon (2018), initially based on an analogy with a remote control system of a rail yard, seen as a switch tree. For a given rooted tree, we formalize the constraints on the coloring, in particular on the minimum number of colors, and on the distribution of the nodes among colors. We show that the sequence $(a_1,a_2,a_3,\\cdots)$, where $a_i$ denotes the number of nodes with color $i$, satisfies a set of inequalities which only involve the sequence $(n_0,n_1,n_2,\\cdots)$ where $n_i$ denotes the number of nodes with height $i$. By coloring the nodes according to their depth, we deduce that these inequalities also apply to the sequence $(d_0,d_1,d_2,\\cdots)$ where $d_i$ denotes the number of nodes with depth $i$.",
        "subjects": [
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16945",
        "abstract url": "https://arxiv.org/abs/2405.16945",
        "title": "Joint Channel, Data and Radar Parameter Estimation for AFDM Systems in Doubly-Dispersive Channels",
        "rating": "-1",
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "We propose new schemes for joint channel and data estimation (JCDE) and radar parameter estimation (RPE) in doubly-dispersive channels, such that integrated sensing and communications (ISAC) is enabled by user equipment (UE) independently performing JCDE, and base stations (BSs) performing RPE. The contributed JCDE and RPE schemes are designed for waveforms known to perform well in doubly-dispersive channels, under a unified model that captures the features of either legacy orthogonal frequency division multiplexing (OFDM), state-of-the-art (SotA) orthogonal time frequency space (OTFS), and next-generation affine frequency division multiplexing (AFDM) systems. The proposed JCDE algorithm is based on a Bayesian parametric bilinear Gaussian belief propagation (PBiGaBP) framework first proposed for OTFS and here shown to apply to all aforementioned waveforms, while the RPE scheme is based on a new probabilistic data association (PDA) approach incorporating a Bernoulli-Gaussian denoising, optimized via expectation maximization (EM). Simulation results demonstrate that JCDE in AFDM systems utilizing a single pilot per block significantly outperforms the SotA alternative even if the latter is granted a substantial power advantage. Similarly, the AFDM-based RPE scheme is found to outperform the OTFS-based approach, as well as the sparse Bayesian learning (SBL) technique, regardless of the waveform used.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IEEE Transactions on Wireless Communications"
    },
    {
        "paper id": "2405.16959",
        "abstract url": "https://arxiv.org/abs/2405.16959",
        "title": "A Machine Learning Approach to Analyze the Effects of Alzheimer's Disease on Handwriting through Lognormal Features",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosis",
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Alzheimer's disease is one of the most incisive illnesses among the neurodegenerative ones, and it causes a progressive decline in cognitive abilities that, in the worst cases, becomes severe enough to interfere with daily life. Currently, there is no cure, so an early diagnosis is strongly needed to try and slow its progression through medical treatments. Handwriting analysis is considered a potential tool for detecting and understanding certain neurological conditions, including Alzheimer's disease. While handwriting analysis alone cannot provide a definitive diagnosis of Alzheimer's, it may offer some insights and be used for a comprehensive assessment. The Sigma-lognormal model is conceived for movement analysis and can also be applied to handwriting. This model returns a set of lognormal parameters as output, which forms the basis for the computation of novel and significant features. This paper presents a machine learning approach applied to handwriting features extracted through the sigma-lognormal model. The aim is to develop a support system to help doctors in the diagnosis and study of Alzheimer, evaluate the effectiveness of the extracted features and finally study the relation among them.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16973",
        "abstract url": "https://arxiv.org/abs/2405.16973",
        "title": "Collective Perception Datasets for Autonomous Driving: A Comprehensive Review",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "To ensure safe operation of autonomous vehicles in complex urban environments, complete perception of the environment is necessary. However, due to environmental conditions, sensor limitations, and occlusions, this is not always possible from a single point of view. To address this issue, collective perception is an effective method. Realistic and large-scale datasets are essential for training and evaluating collective perception methods. This paper provides the first comprehensive technical review of collective perception datasets in the context of autonomous driving. The survey analyzes existing V2V and V2X datasets, categorizing them based on different criteria such as sensor modalities, environmental conditions, and scenario variety. The focus is on their applicability for the development of connected automated vehicles. This study aims to identify the key criteria of all datasets and to present their strengths, weaknesses, and anomalies. Finally, this survey concludes by making recommendations regarding which dataset is most suitable for collective 3D object detection, tracking, and semantic segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at IEEE IV 2024"
    },
    {
        "paper id": "2405.16982",
        "abstract url": "https://arxiv.org/abs/2405.16982",
        "title": "Robust kernel-free quadratic surface twin support vector machine with capped $L_1$-norm distance metric",
        "rating": "-1",
        "keywords": [
            [
                "support vector machine"
            ]
        ],
        "abstract": "Twin support vector machine (TSVM) is a very classical and practical classifier for pattern classification. However, the traditional TSVM has two limitations. Firstly, it uses the L_2-norm distance metric that leads to its sensitivity to outliers. Second, it needs to select the appropriate kernel function and the kernel parameters for nonlinear classification. To effectively avoid these two problems, this paper proposes a robust capped L_1-norm kernel-free quadratic surface twin support vector machine (CL_1QTSVM). The strengths of our model are briefly summarized as follows. 1) The robustness of our model is further improved by employing the capped L_1 norm distance metric. 2) Our model is a kernel-free method that avoids the time-consuming process of selecting appropriate kernel functions and kernel parameters. 3) The introduction of L_2-norm regularization term to improve the generalization ability of the model. 4) To efficiently solve the proposed model, an iterative algorithm is developed. 5) The convergence, time complexity and existence of locally optimal solutions of the developed algorithms are further discussed. Numerical experiments on numerous types of datasets validate the classification performance and robustness of the proposed model.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16994",
        "abstract url": "https://arxiv.org/abs/2405.16994",
        "title": "Vision-and-Language Navigation Generative Pretrained Transformer",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Navigation"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "In the Vision-and-Language Navigation (VLN) field, agents are tasked with navigating real-world scenes guided by linguistic instructions. Enabling the agent to adhere to instructions throughout the process of navigation represents a significant challenge within the domain of VLN. To address this challenge, common approaches often rely on encoders to explicitly record past locations and actions, increasing model complexity and resource consumption. Our proposal, the Vision-and-Language Navigation Generative Pretrained Transformer (VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory sequence dependencies, bypassing the need for historical encoding modules. This method allows for direct historical information access through trajectory sequence, enhancing efficiency. Furthermore, our model separates the training process into offline pre-training with imitation learning and online fine-tuning with reinforcement learning. This distinction allows for more focused training objectives and improved performance. Performance assessments on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art encoder-based models.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16997",
        "abstract url": "https://arxiv.org/abs/2405.16997",
        "title": "Program Synthesis is $\u03a3_3^0$-Complete",
        "rating": "-1",
        "keywords": [
            [
                "Synthesis"
            ]
        ],
        "abstract": "This paper considers program synthesis in the context of computational hardness, asking the question: How hard is it to determine whether a given synthesis problem has a solution or not? To answer this question, this paper studies program synthesis for a basic imperative, Turing-complete language IMP, for which this paper proves that program synthesis is $\u03a3_3^0$-\\emph{complete} in the arithmetical hierarchy. The proof of this fact relies on a fully constructive encoding of program synthesis (which is typically formulated as a second-order query) as a first-order formula in the standard model of arithmetic (i.e., Peano arithmetic). Constructing such a formula then allows us to reduce the decision problem for COF (the set of functions which diverge only on a finite set of inputs), which is well-known to be a $\u03a3_3^0$-complete problem, into the constructed first-order representation of synthesis. In addition to this main result, we also consider the hardness of variants of synthesis problems, such as those introduced in previous work to make program synthesis more tractable (e.g., synthesis over finite examples). To the best of our knowledge, this paper is the first to give a first-order characterization of program synthesis in general, and precisely define the computability of synthesis problems and their variants.",
        "subjects": [
            "cs.LO",
            "cs.CC",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17002",
        "abstract url": "https://arxiv.org/abs/2405.17002",
        "title": "UIT-DarkCow team at ImageCLEFmedical Caption 2024: Diagnostic Captioning for Radiology Images Efficiency with Transformer Models",
        "rating": "-1",
        "keywords": [
            [
                "biomedical",
                "medical",
                "healthcare",
                "clinical",
                "Radiology"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Purpose: This study focuses on the development of automated text generation from radiology images, termed diagnostic captioning, to assist medical professionals in reducing clinical errors and improving productivity. The aim is to provide tools that enhance report quality and efficiency, which can significantly impact both clinical practice and deep learning research in the biomedical field. Methods: In our participation in the ImageCLEFmedical2024 Caption evaluation campaign, we explored caption prediction tasks using advanced Transformer-based models. We developed methods incorporating Transformer encoder-decoder and Query Transformer architectures. These models were trained and evaluated to generate diagnostic captions from radiology images. Results: Experimental evaluations demonstrated the effectiveness of our models, with the VisionDiagnostor-BioBART model achieving the highest BERTScore of 0.6267. This performance contributed to our team, DarkCow, achieving third place on the leaderboard. Conclusion: Our diagnostic captioning models show great promise in aiding medical professionals by generating high-quality reports efficiently. This approach can facilitate better data processing and performance optimization in medical imaging departments, ultimately benefiting healthcare delivery.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17013",
        "abstract url": "https://arxiv.org/abs/2405.17013",
        "title": "MotionLLM: Multimodal Motion-Language Learning with Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in Multimodal Large Language Models (MM-LLMs) have demonstrated promising potential in terms of generalization and robustness when applied to different modalities. While previous works have already achieved 3D human motion generation using various approaches including language modeling, they mostly % are mostly carefully designed use specialized architecture and are restricted to single-human motion generation. Inspired by the success of MM-LLMs, we propose MotionLLM, a simple and general framework that can achieve single-human, multi-human motion generation, and motion captioning by fine-tuning pre-trained LLMs. Specifically, we encode and quantize motions into discrete LLM-understandable tokens, which results in a unified vocabulary consisting of both motion and text tokens. With only 1--3% parameters of the LLMs trained by using adapters, our single-human motion generation achieves comparable results to those diffusion models and other trained-from-scratch transformer-based models. Additionally, we show that our approach is scalable and flexible, allowing easy extension to multi-human motion generation through autoregressive generation of single-human motions. Project page: https://knoxzhao.github.io/MotionLLM",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://knoxzhao.github.io/MotionLLM"
    },
    {
        "paper id": "2405.17018",
        "abstract url": "https://arxiv.org/abs/2405.17018",
        "title": "Structural cohesive element for the modelling of delamination in composite laminates without the cohesive zone limit",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "Delamination is a critical mode of failure that occurs between plies in a composite laminate. The cohesive element, developed based on the cohesive zone model, is widely used for modeling delamination. However, standard cohesive elements suffer from a well-known limit on the mesh density-the element size must be much smaller than the cohesive zone size. This work develops a new set of elements for modelling composite plies and their interfaces in 3D. A triangular Kirchhoff-Love shell element is developed for orthotropic materials to model the plies. A structural cohesive element, conforming to the shell elements of the plies, is developed to model the interface delamination. The proposed method is verified and validated on the classical benchmark problems of Mode I, Mode II, and mixed-mode delamination of unidirectional laminates, as well as on the single-leg bending problem of a multi-directional laminate. All the results show that the element size in the proposed models can be ten times larger than that in the standard cohesive element models, with more than 90% reduction in CPU time, while retaining prediction accuracy. This would then allow more effective and efficient modeling of delamination in composites without worrying about the cohesive zone limit on the mesh density.",
        "subjects": [
            "cs.CE",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17019",
        "abstract url": "https://arxiv.org/abs/2405.17019",
        "title": "Bounding Random Test Set Size with Computational Learning Theory",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving"
            ]
        ],
        "abstract": "Random testing approaches work by generating inputs at random, or by selecting inputs randomly from some pre-defined operational profile. One long-standing question that arises in this and other testing contexts is as follows: When can we stop testing? At what point can we be certain that executing further tests in this manner will not explore previously untested (and potentially buggy) software behaviors? This is analogous to the question in Machine Learning, of how many training examples are required in order to infer an accurate model. In this paper we show how probabilistic approaches to answer this question in Machine Learning (arising from Computational Learning Theory) can be applied in our testing context. This enables us to produce an upper bound on the number of tests that are required to achieve a given level of adequacy. We are the first to enable this from only knowing the number of coverage targets (e.g. lines of code) in the source code, without needing to observe a sample test executions. We validate this bound on a large set of Java units, and an autonomous driving system.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Preprint, accepted to FSE 2024"
    },
    {
        "paper id": "2405.17020",
        "abstract url": "https://arxiv.org/abs/2405.17020",
        "title": "From Compliant to Rigid Contact Simulation: a Unified and Efficient Approach",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things. Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions. Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective. Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged. Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet). In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation. It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way. To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically. By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo. We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios. Our code is made open-source at https://github.com/Simple-Robotics/Simple.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17030",
        "abstract url": "https://arxiv.org/abs/2405.17030",
        "title": "SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "Autonomous Driving",
                "lidar",
                "radar"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel synthetically generated multi-modal dataset, SCaRL, to enable the training and validation of autonomous driving solutions. Multi-modal datasets are essential to attain the robustness and high accuracy required by autonomous systems in applications such as autonomous driving. As deep learning-based solutions are becoming more prevalent for object detection, classification, and tracking tasks, there is great demand for datasets combining camera, lidar, and radar sensors. Existing real/synthetic datasets for autonomous driving lack synchronized data collection from a complete sensor suite. SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar. SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions. SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors. The dataset can be accessed here: https://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Accepted in International Conference on Microwaves for Intelligent Mobility - 16.&17. April 2024 - Boppard near Koblenz, Germany"
    },
    {
        "paper id": "2405.17080",
        "abstract url": "https://arxiv.org/abs/2405.17080",
        "title": "A Two-Level Stochastic Model for the Lateral Movement of Vehicles Within Their Lane Under Homogeneous Traffic Conditions",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving",
                "vehicle"
            ]
        ],
        "abstract": "The lateral position of vehicles within their lane is a decisive factor for the range of vision of vehicle sensors. This, in turn, is crucial for a vehicle's ability to perceive its environment and gain a high situational awareness by processing the collected information. When aiming for increasing levels of vehicle autonomy, this situational awareness becomes more and more important. Thus, when validating an autonomous driving function the representativeness of the submicroscopic behavior such as the lateral offset has to be ensured. With simulations being an essential part of the validation of autonomous driving functions, models describing these phenomena are required. Possible applications are the enhancement of microscopic traffic simulations and the maneuver-based approach for scenario-based testing. This paper presents a two-level stochastic approach to model the lateral movement of vehicles within their lane during road-following maneuvers under homogeneous traffic conditions. A Markov model generates the coarse lateral offset profile. It is superposed with a noise model for the fine movements. Both models are set up using real-world data. The evaluation of the model shows promising qualitative and quantitative results, the potential for enhancements and extreme low computation times (10000 times faster than real time).",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 12 figures"
    },
    {
        "paper id": "2405.17083",
        "abstract url": "https://arxiv.org/abs/2405.17083",
        "title": "F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "synthesizing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The neural radiance field (NeRF) has made significant strides in representing 3D scenes and synthesizing novel views. Despite its advancements, the high computational costs of NeRF have posed challenges for its deployment in resource-constrained environments and real-time applications. As an alternative to NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers rapid rendering speeds while maintaining excellent image quality. However, as it represents objects and scenes using a myriad of Gaussians, it requires substantial storage to achieve high-quality representation. To mitigate the storage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel approach that drastically reduces storage requirements while preserving image quality. Inspired by classical matrix and tensor factorization techniques, our method represents and approximates dense clusters of Gaussians with significantly fewer Gaussians through efficient factorization. We aim to efficiently represent dense 3D Gaussians by approximating them with a limited amount of information for each axis and their combinations. This method allows us to encode a substantially large number of Gaussians along with their essential attributes -- such as color, scale, and rotation -- necessary for rendering using a relatively small number of elements. Extensive experimental results demonstrate that F-3DGS achieves a significant reduction in storage costs while maintaining comparable quality in rendered images.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Our project page including code is available at https://xiangyu1sun.github.io/Factorize-3DGS/"
    },
    {
        "paper id": "2405.17088",
        "abstract url": "https://arxiv.org/abs/2405.17088",
        "title": "Phase Transitions in the Output Distribution of Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another. Analogous phenomena have recently been observed in large language models. Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities.",
        "subjects": [
            "cs.LG",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "21 pages, 4 figures"
    },
    {
        "paper id": "2405.17102",
        "abstract url": "https://arxiv.org/abs/2405.17102",
        "title": "DINO-SD: Champion Solution for ICRA 2024 RoboDepth Challenge",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Surround-view depth estimation is a crucial task aims to acquire the depth maps of the surrounding views. It has many applications in real world scenarios such as autonomous driving, AR/VR and 3D reconstruction, etc. However, given that most of the data in the autonomous driving dataset is collected in daytime scenarios, this leads to poor depth model performance in the face of out-of-distribution(OoD) data. While some works try to improve the robustness of depth model under OoD data, these methods either require additional training data or lake generalizability. In this report, we introduce the DINO-SD, a novel surround-view depth estimation model. Our DINO-SD does not need additional data and has strong robustness. Our DINO-SD get the best performance in the track4 of ICRA 2024 RoboDepth Challenge.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Outstanding Champion in the RoboDepth Challenge (ICRA24) https://robodrive-24.github.io/"
    },
    {
        "paper id": "2405.17141",
        "abstract url": "https://arxiv.org/abs/2405.17141",
        "title": "MVMS-RCN: A Dual-Domain Unfolding CT Reconstruction with Multi-sparse-view and Multi-scale Refinement-correction",
        "rating": "-1",
        "keywords": [
            [
                "CT",
                "X-ray",
                "clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "X-ray Computed Tomography (CT) is one of the most important diagnostic imaging techniques in clinical applications. Sparse-view CT imaging reduces the number of projection views to a lower radiation dose and alleviates the potential risk of radiation exposure. Most existing deep learning (DL) and deep unfolding sparse-view CT reconstruction methods: 1) do not fully use the projection data; 2) do not always link their architecture designs to a mathematical theory; 3) do not flexibly deal with multi-sparse-view reconstruction assignments. This paper aims to use mathematical ideas and design optimal DL imaging algorithms for sparse-view tomography reconstructions. We propose a novel dual-domain deep unfolding unified framework that offers a great deal of flexibility for multi-sparse-view CT reconstruction with different sampling views through a single model. This framework combines the theoretical advantages of model-based methods with the superior reconstruction performance of DL-based methods, resulting in the expected generalizability of DL. We propose a refinement module that utilizes unfolding projection domain to refine full-sparse-view projection errors, as well as an image domain correction module that distills multi-scale geometric error corrections to reconstruct sparse-view CT. This provides us with a new way to explore the potential of projection information and a new perspective on designing network architectures. All parameters of our proposed framework are learnable end to end, and our method possesses the potential to be applied to plug-and-play reconstruction. Extensive experiments demonstrate that our framework is superior to other existing state-of-the-art methods. Our source codes are available at https://github.com/fanxiaohong/MVMS-RCN.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "12 pages, submitted"
    },
    {
        "paper id": "2405.17171",
        "abstract url": "https://arxiv.org/abs/2405.17171",
        "title": "Coordinating robotized construction using advanced robotic simulation: The case of collaborative brick wall assembly",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Utilizing robotic systems in the construction industry is gaining popularity due to their build time, precision, and efficiency. In this paper, we introduce a system that allows the coordination of multiple manipulator robots for construction activities. As a case study, we chose robotic brick wall assembly. By utilizing a multi robot system where arm manipulators collaborate with each other, the entirety of a potentially long wall can be assembled simultaneously. However, the reduction of overall bricklaying time is dependent on the minimization of time required for each individual manipulator. In this paper, we execute the simulation with various placements of material and the robots base, as well as different robot configurations, to determine the optimal position of the robot and material and the best configuration for the robot. The simulation results provide users with insights into how to find the best placement of robots and raw materials for brick wall assembly.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2405.17172",
        "abstract url": "https://arxiv.org/abs/2405.17172",
        "title": "Partitioning complete geometric graphs into plane subgraphs",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A \\emph{complete geometric graph} consists of a set $P$ of $n$ points in the plane, in general position, and all segments (edges) connecting them. It is a well known question of Bose, Hurtado, Rivera-Campo, and Wood, whether there exists a positive constant $c<1$, such that every complete geometric graph on $n$ points can be partitioned into at most $cn$ plane graphs (that is, noncrossing subgraphs). We answer this question in the affirmative in the special case where the underlying point set $P$ is \\emph{dense}, which means that the ratio between the maximum and the minimum distances in $P$ is of the order of $\u0398(\\sqrt{n})$.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "10 pages, 4 figures"
    },
    {
        "paper id": "2405.17187",
        "abstract url": "https://arxiv.org/abs/2405.17187",
        "title": "Memorize What Matters: Emergent Scene Decomposition from Multitraverse",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "robotics"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory. This selective retention is crucial for robotic perception, localization, and mapping. To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised, camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation. Our key observation is that the environment remains consistent across traversals, while objects frequently change. This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition. More specifically, 3DGM formulates multitraverse environmental mapping as a robust differentiable rendering problem, treating pixels of the environment and objects as inliers and outliers, respectively. Using robust feature distillation, feature residuals mining, and robust optimization, 3DGM jointly performs 2D segmentation and 3D mapping without human intervention. We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Extensive results verify the effectiveness and potential of our method for self-driving and robotics.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "comment": "Project page: https://3d-gaussian-mapping.github.io; Code and data: https://github.com/NVlabs/3DGM"
    },
    {
        "paper id": "2405.17193",
        "abstract url": "https://arxiv.org/abs/2405.17193",
        "title": "Anisotropic Gauss Reconstruction for Unoriented Point Clouds",
        "rating": "-1",
        "keywords": [
            [
                "point cloud"
            ]
        ],
        "abstract": "Unoriented surface reconstructions based on the Gauss formula have attracted much attention due to their elegant mathematical formulation and excellent performance. However, the isotropic characteristics of the formulation limit their capacity to leverage the anisotropic information within the point cloud. In this work, we propose a novel anisotropic formulation by introducing a convection term in the original Laplace operator. By choosing different velocity vectors, the anisotropic feature can be exploited to construct more effective linear equations. Moreover, an adaptive selection strategy is introduced for the velocity vector to further enhance the orientation and reconstruction performance of thin structures. Extensive experiments demonstrate that our method achieves state-of-the-art performance and manages various challenging situations, especially for models with thin structures or small holes. The source code will be released on GitHub.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "17pages;14figures"
    },
    {
        "paper id": "2405.17236",
        "abstract url": "https://arxiv.org/abs/2405.17236",
        "title": "From Text to Blueprint: Leveraging Text-to-Image Tools for Floor Plan Creation",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion",
                "synthesis",
                "Text-to-Image"
            ]
        ],
        "abstract": "Artificial intelligence is revolutionizing architecture through text-to-image synthesis, converting textual descriptions into detailed visual representations. We explore AI-assisted floor plan design, focusing on technical background, practical methods, and future directions. Using tools like, Stable Diffusion, AI leverages models such as Generative Adversarial Networks and Variational Autoencoders to generate complex and functional floorplans designs. We evaluates these AI models' effectiveness in generating residential floor plans from text prompts. Through experiments with reference images, text prompts, and sketches, we assess the strengths and limitations of current text-to-image technology in architectural visualization. Architects can use these AI tools to streamline design processes, create multiple design options, and enhance creativity and collaboration. We highlight AI's potential to drive smarter, more efficient floorplan design, contributing to ongoing discussions on AI integration in the design profession and its future impact.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "13 pages, 7 figures"
    },
    {
        "paper id": "2405.17241",
        "abstract url": "https://arxiv.org/abs/2405.17241",
        "title": "NeurTV: Total Variation on the Neural Domain",
        "rating": "-1",
        "keywords": [
            [
                "hyperspectral images"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Recently, we have witnessed the success of total variation (TV) for many imaging applications. However, traditional TV is defined on the original pixel domain, which limits its potential. In this work, we suggest a new TV regularization defined on the neural domain. Concretely, the discrete data is continuously and implicitly represented by a deep neural network (DNN), and we use the derivatives of DNN outputs w.r.t. input coordinates to capture local correlations of data. As compared with classical TV on the original domain, the proposed TV on the neural domain (termed NeurTV) enjoys two advantages. First, NeurTV is not limited to meshgrid but is suitable for both meshgrid and non-meshgrid data. Second, NeurTV can more exactly capture local correlations across data for any direction and any order of derivatives attributed to the implicit and continuous nature of neural domain. We theoretically reinterpret NeurTV under the variational approximation framework, which allows us to build the connection between classical TV and NeurTV and inspires us to develop variants (e.g., NeurTV with arbitrary resolution and space-variant NeurTV). Extensive numerical experiments with meshgrid data (e.g., color and hyperspectral images) and non-meshgrid data (e.g., point clouds and spatial transcriptomics) showcase the effectiveness of the proposed methods.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17250",
        "abstract url": "https://arxiv.org/abs/2405.17250",
        "title": "\"Pass the butter\": A study on desktop-classic multitasking robotic arm based on advanced YOLOv7 and BERT",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "In recent years, various intelligent autonomous robots have begun to appear in daily life and production. Desktop-level robots are characterized by their flexible deployment, rapid response, and suitability for light workload environments. In order to meet the current societal demand for service robot technology, this study proposes using a miniaturized desktop-level robot (by ROS) as a carrier, locally deploying a natural language model (NLP-BERT), and integrating visual recognition (CV-YOLO) and speech recognition technology (ASR-Whisper) as inputs to achieve autonomous decision-making and rational action by the desktop robot. Three comprehensive experiments were designed to validate the robotic arm, and the results demonstrate excellent performance using this approach across all three experiments. In Task 1, the execution rates for speech recognition and action performance were 92.6% and 84.3%, respectively. In Task 2, the highest execution rates under the given conditions reached 92.1% and 84.6%, while in Task 3, the highest execution rates were 95.2% and 80.8%, respectively. Therefore, it can be concluded that the proposed solution integrating ASR, NLP, and other technologies on edge devices is feasible and provides a technical and engineering foundation for realizing multimodal desktop-level robots.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17251",
        "abstract url": "https://arxiv.org/abs/2405.17251",
        "title": "GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "synthesis",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating novel views from a single image remains a challenging task due to the complexity of 3D scenes and the limited diversity in the existing multi-view datasets to train a model on. Recent research combining large-scale text-to-image (T2I) models with monocular depth estimation (MDE) has shown promise in handling in-the-wild images. In these methods, an input view is geometrically warped to novel views with estimated depth maps, then the warped image is inpainted by T2I models. However, they struggle with noisy depth maps and loss of semantic details when warping an input view to novel viewpoints. In this paper, we propose a novel approach for single-shot novel view synthesis, a semantic-preserving generative warping framework that enables T2I generative models to learn where to warp and where to generate, through augmenting cross-view attention with self-attention. Our approach addresses the limitations of existing methods by conditioning the generative model on source view images and incorporating geometric warping signals. Qualitative and quantitative evaluations demonstrate that our model outperforms existing methods in both in-domain and out-of-domain scenarios. Project page is available at https://GenWarp-NVS.github.io/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://GenWarp-NVS.github.io"
    },
    {
        "paper id": "2405.17270",
        "abstract url": "https://arxiv.org/abs/2405.17270",
        "title": "Towards Accurate Ego-lane Identification with Early Time Series Classification",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving",
                "vehicle"
            ]
        ],
        "abstract": "Accurate and timely determination of a vehicle's current lane within a map is a critical task in autonomous driving systems. This paper utilizes an Early Time Series Classification (ETSC) method to achieve precise and rapid ego-lane identification in real-world driving data. The method begins by assessing the similarities between map and lane markings perceived by the vehicle's camera using measurement model quality metrics. These metrics are then fed into a selected ETSC method, comprising a probabilistic classifier and a tailored trigger function, optimized via multi-objective optimization to strike a balance between early prediction and accuracy. Our solution has been evaluated on a comprehensive dataset consisting of 114 hours of real-world traffic data, collected across 5 different countries by our test vehicles. Results show that by leveraging road lane-marking geometry and lane-marking type derived solely from a camera, our solution achieves an impressive accuracy of 99.6%, with an average prediction time of only 0.84 seconds.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2405.17280",
        "abstract url": "https://arxiv.org/abs/2405.17280",
        "title": "A Library for Automatic Natural Language Generation of Spanish Texts",
        "rating": "-1",
        "keywords": [
            [
                "grammar"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this article we present a novel system for natural language generation (NLG) of Spanish sentences from a minimum set of meaningful words (such as nouns, verbs and adjectives) which, unlike other state-of-the-art solutions, performs the NLG task in a fully automatic way, exploiting both knowledge-based and statistical approaches. Relying on its linguistic knowledge of vocabulary and grammar, the system is able to generate complete, coherent and correctly spelled sentences from the main word sets presented by the user. The system, which was designed to be integrable, portable and efficient, can be easily adapted to other languages by design and can feasibly be integrated in a wide range of digital devices. During its development we also created a supplementary lexicon for Spanish, aLexiS, with wide coverage and high precision, as well as syntactic trees from a freely available definite-clause grammar. The resulting NLG library has been evaluated both automatically and manually (annotation). The system can potentially be used in different application domains such as augmentative communication and automatic generation of administrative reports or news.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17297",
        "abstract url": "https://arxiv.org/abs/2405.17297",
        "title": "Enhanced Automotive Radar Collaborative Sensing By Exploiting Constructive Interference",
        "rating": "-1",
        "keywords": [
            [
                "Radar",
                "vehicle"
            ]
        ],
        "abstract": "Automotive radar emerges as a crucial sensor for autonomous vehicle perception. As more cars are equipped radars, radar interference is an unavoidable challenge. Unlike conventional approaches such as interference mitigation and interference-avoiding technologies, this paper introduces an innovative collaborative sensing scheme with multiple automotive radars that exploits constructive interference. Through collaborative sensing, our method optimally aligns cross-path interference signals from other radars with another radar's self-echo signals, thereby significantly augmenting its target detection capabilities. This approach alleviates the need for extensive raw data sharing between collaborating radars. Instead, only an optimized weighting matrix needs to be exchanged between the radars. This approach considerably decreases the data bandwidth requirements for the wireless channel, making it a more feasible and practical solution for automotive radar collaboration. Numerical results demonstrate the effectiveness of the constructive interference approach for enhanced object detection capability.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "paper accepted by IEEE SAM Workshop 2024"
    },
    {
        "paper id": "2405.17304",
        "abstract url": "https://arxiv.org/abs/2405.17304",
        "title": "Stochastic Omega-Regular Verification and Control with Supermartingales",
        "rating": "-1",
        "keywords": [
            [
                "synthesis"
            ]
        ],
        "abstract": "We present for the first time a supermartingale certificate for $\u03c9$-regular specifications. We leverage the Robbins & Siegmund convergence theorem to characterize supermartingale certificates for the almost-sure acceptance of Streett conditions on general stochastic processes, which we call Streett supermartingales. This enables effective verification and control of discrete-time stochastic dynamical models with infinite state space under $\u03c9$-regular and linear temporal logic specifications. Our result generalises reachability, safety, reach-avoid, persistence and recurrence specifications; our contribution applies to discrete-time stochastic dynamical models and probabilistic programs with discrete and continuous state spaces and distributions, and carries over to deterministic models and programs. We provide a synthesis algorithm for control policies and Streett supermartingales as proof certificates for $\u03c9$-regular objectives, which is sound and complete for supermartingales and control policies with polynomial templates and any stochastic dynamical model whose post-expectation is expressible as a polynomial. We additionally provide an optimisation of our algorithm that reduces the problem to satisfiability modulo theories, under the assumption that templates and post-expectation are in piecewise linear form. We have built a prototype and have demonstrated the efficacy of our approach on several exemplar $\u03c9$-regular verification and control synthesis problems.",
        "subjects": [
            "cs.LO",
            "eess.SY"
        ],
        "comment": "The conference version of this manuscript appeared at CAV'24"
    },
    {
        "paper id": "2405.17314",
        "abstract url": "https://arxiv.org/abs/2405.17314",
        "title": "Maximizing Phylogenetic Diversity under Ecological Constraints: A Parameterized Complexity Study",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In the NP-hard Optimizing PD with Dependencies (PDD) problem, the input consists of a phylogenetic tree $T$ over a set of taxa $X$, a food-web that describes the prey-predator relationships in $X$, and integers $k$ and $D$. The task is to find a set $S$ of $k$ species that is viable in the food-web such that the subtree of $T$ obtained by retaining only the vertices of $S$ has total edge weight at least $D$. Herein, viable means that for every predator taxon of $S$, the set $S$ contains at least one prey taxon. We provide the first systematic analysis of PDD and its special case s-PDD from a parameterized complexity perspective. For solution-size related parameters, we show that PDD is FPT with respect to $D$ and with respect to $k$ plus the height of the phylogenetic tree. Moreover, we consider structural parameterizations of the food-web. For example, we show an FPT-algorithm for the parameter that measures the vertex deletion distance to graphs where every connected component is a complete graph. Finally, we show that s-PDD admits an FPT-algorithm for the treewidth of the food-web. This disproves a conjecture of Faller et al. [Annals of Combinatorics, 2011] who conjectured that s-PDD is NP-hard even when the food-web is a tree.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17315",
        "abstract url": "https://arxiv.org/abs/2405.17315",
        "title": "All-day Depth Completion",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "point cloud",
                "Depth"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a method for depth estimation under different illumination conditions, i.e., day and night time. As photometry is uninformative in regions under low-illumination, we tackle the problem through a multi-sensor fusion approach, where we take as input an additional synchronized sparse point cloud (i.e., from a LiDAR) projected onto the image plane as a sparse depth map, along with a camera image. The crux of our method lies in the use of the abundantly available synthetic data to first approximate the 3D scene structure by learning a mapping from sparse to (coarse) dense depth maps along with their predictive uncertainty - we term this, SpaDe. In poorly illuminated regions where photometric intensities do not afford the inference of local shape, the coarse approximation of scene depth serves as a prior; the uncertainty map is then used with the image to guide refinement through an uncertainty-driven residual learning (URL) scheme. The resulting depth completion network leverages complementary strengths from both modalities - depth is sparse but insensitive to illumination and in metric scale, and image is dense but sensitive with scale ambiguity. SpaDe can be used in a plug-and-play fashion, which allows for 25% improvement when augmented onto existing methods to preprocess sparse depth. We demonstrate URL on the nuScenes dataset where we improve over all baselines by an average 11.65% in all-day scenarios, 11.23% when tested specifically for daytime, and 13.12% for nighttime scenes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2405.17336",
        "abstract url": "https://arxiv.org/abs/2405.17336",
        "title": "XFormParser: A Simple and Effective Multimodal Multilingual Semi-structured Form Parser",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the domain of document AI, semi-structured form parsing plays a crucial role. This task leverages techniques from key information extraction (KIE), dealing with inputs that range from plain text to intricate modal data comprising images and structural layouts. The advent of pre-trained multimodal models has driven the extraction of key information from form documents in different formats such as PDFs and images. Nonetheless, the endeavor of form parsing is still encumbered by notable challenges like subpar capabilities in multi-lingual parsing and diminished recall in contexts rich in text and visuals. In this work, we introduce a simple but effective \\textbf{M}ultimodal and \\textbf{M}ultilingual semi-structured \\textbf{FORM} \\textbf{PARSER} (\\textbf{XFormParser}), which is anchored on a comprehensive pre-trained language model and innovatively amalgamates semantic entity recognition (SER) and relation extraction (RE) into a unified framework, enhanced by a novel staged warm-up training approach that employs soft labels to significantly refine form parsing accuracy without amplifying inference overhead. Furthermore, we have developed a groundbreaking benchmark dataset, named InDFormBench, catering specifically to the parsing requirements of multilingual forms in various industrial contexts. Through rigorous testing on established multilingual benchmarks and InDFormBench, XFormParser has demonstrated its unparalleled efficacy, notably surpassing the state-of-the-art (SOTA) models in RE tasks within language-specific setups by achieving an F1 score improvement of up to 1.79\\%. Our framework exhibits exceptionally improved performance across tasks in both multi-language and zero-shot contexts when compared to existing SOTA benchmarks. The code is publicly available at https://github.com/zhbuaa0/layoutlmft.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages, 3 figures, 6 tables"
    },
    {
        "paper id": "2405.17368",
        "abstract url": "https://arxiv.org/abs/2405.17368",
        "title": "Fusing uncalibrated IMUs and handheld smartphone video to reconstruct knee kinematics",
        "rating": "-1",
        "keywords": [
            [
                "biomechanically",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video and wearable sensor data provide complementary information about human movement. Video provides a holistic understanding of the entire body in the world while wearable sensors provide high-resolution measurements of specific body segments. A robust method to fuse these modalities and obtain biomechanically accurate kinematics would have substantial utility for clinical assessment and monitoring. While multiple video-sensor fusion methods exist, most assume that a time-intensive, and often brittle, sensor-body calibration process has already been performed. In this work, we present a method to combine handheld smartphone video and uncalibrated wearable sensor data at their full temporal resolution. Our monocular, video-only, biomechanical reconstruction already performs well, with only several degrees of error at the knee during walking compared to markerless motion capture. Reconstructing from a fusion of video and wearable sensor data further reduces this error. We validate this in a mixture of people with no gait impairments, lower limb prosthesis users, and individuals with a history of stroke. We also show that sensor data allows tracking through periods of visual occlusion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to International Conference on Biomedical Robotics and Biomechatronics 2024"
    },
    {
        "paper id": "2405.17393",
        "abstract url": "https://arxiv.org/abs/2405.17393",
        "title": "EASI-Tex: Edge-Aware Mesh Texturing from Single Image",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel approach for single-image mesh texturing, which employs a diffusion model with judicious conditioning to seamlessly transfer an object's texture from a single RGB image to a given 3D mesh object. We do not assume that the two objects belong to the same category, and even if they do, there can be significant discrepancies in their geometry and part proportions. Our method aims to rectify the discrepancies by conditioning a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to generate textures that respect the underlying geometry of the mesh and the input texture without any optimization or training. We also introduce Image Inversion, a novel technique to quickly personalize the diffusion model for a single concept using a single image, for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully. Experimental results demonstrate the efficiency and effectiveness of our edge-aware single-image mesh texturing approach, coined EASI-Tex, in preserving the details of the input texture on diverse 3D objects, while respecting their geometry.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ACM Transactions on Graphics (Proceedings of SIGGRAPH), 2024. Project Page: https://sairajk.github.io/easi-tex/"
    },
    {
        "paper id": "2405.17397",
        "abstract url": "https://arxiv.org/abs/2405.17397",
        "title": "Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional Encoding",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding human behavior fundamentally relies on accurate 3D human pose estimation. Graph Convolutional Networks (GCNs) have recently shown promising advancements, delivering state-of-the-art performance with rather lightweight architectures. In the context of graph-structured data, leveraging the eigenvectors of the graph Laplacian matrix for positional encoding is effective. Yet, the approach does not specify how to handle scenarios where edges in the input graph are missing. To this end, we propose a novel positional encoding technique, PerturbPE, that extracts consistent and regular components from the eigenbasis. Our method involves applying multiple perturbations and taking their average to extract the consistent and regular component from the eigenbasis. PerturbPE leverages the Rayleigh-Schrodinger Perturbation Theorem (RSPT) for calculating the perturbed eigenvectors. Employing this labeling technique enhances the robustness and generalizability of the model. Our results support our theoretical findings, e.g. our experimental analysis observed a performance enhancement of up to $12\\%$ on the Human3.6M dataset in instances where occlusion resulted in the absence of one edge. Furthermore, our novel approach significantly enhances performance in scenarios where two edges are missing, setting a new benchmark for state-of-the-art.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17405",
        "abstract url": "https://arxiv.org/abs/2405.17405",
        "title": "Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion",
                "GAN",
                "synthesize"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel approach for generating high-quality, spatio-temporally coherent human videos from a single image under arbitrary viewpoints. Our framework combines the strengths of U-Nets for accurate condition injection and diffusion transformers for capturing global correlations across viewpoints and time. The core is a cascaded 4D transformer architecture that factorizes attention across views, time, and spatial dimensions, enabling efficient modeling of the 4D space. Precise conditioning is achieved by injecting human identity, camera parameters, and temporal signals into the respective transformers. To train this model, we curate a multi-dimensional dataset spanning images, videos, multi-view data and 3D/4D scans, along with a multi-dimensional training strategy. Our approach overcomes the limitations of previous methods based on GAN or UNet-based diffusion models, which struggle with complex motions and viewpoint changes. Through extensive experiments, we demonstrate our method's ability to synthesize realistic, coherent and free-view human videos, paving the way for advanced multimedia applications in areas such as virtual reality and animation. Our project website is https://human4dit.github.io.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Our project website is https://human4dit.github.io"
    },
    {
        "paper id": "2405.17414",
        "abstract url": "https://arxiv.org/abs/2405.17414",
        "title": "Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images. Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories make strides towards it. Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories. Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications. We introduce collaborative video diffusion (CVD) as an important step towards this vision. The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism. Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments. Project page: https://collaborativevideodiffusion.github.io/.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17421",
        "abstract url": "https://arxiv.org/abs/2405.17421",
        "title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds",
        "rating": "-1",
        "keywords": [
            [
                "Gaussian Splatting"
            ],
            [
                "synthesize"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce 4D Motion Scaffolds (MoSca), a neural information processing system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models, lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions / deformations. The scene geometry and appearance are then disentangled from the deformation field, and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera poses can be seamlessly initialized and refined during the dynamic rendering process, without the need for other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "project page: https://www.cis.upenn.edu/~leijh/projects/mosca"
    },
    {
        "paper id": "2405.17425",
        "abstract url": "https://arxiv.org/abs/2405.17425",
        "title": "From Neurons to Neutrons: A Case Study in Interpretability",
        "rating": "-1",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Mechanistic Interpretability (MI) promises a path toward fully understanding how neural networks make their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters. Does this mean neuron-level interpretability techniques have limited applicability? We argue that high-dimensional neural networks can learn low-dimensional representations of their training data that are useful beyond simply making good predictions. Such representations can be understood through the mechanistic interpretability lens and provide insights that are surprisingly faithful to human-derived domain knowledge. This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it. As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data.",
        "subjects": [
            "cs.LG",
            "nucl-th"
        ],
        "comment": "International Conference on Machine Learning (ICML) 2024"
    },
    {
        "paper id": "2405.17429",
        "abstract url": "https://arxiv.org/abs/2405.17429",
        "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "voxel"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and semantics of the surrounding scene and is an important task for the robustness of vision-centric autonomous driving. Most existing methods employ dense grids such as voxels as scene representations, which ignore the sparsity of occupancy and the diversity of object scales and thus lead to unbalanced allocation of resources. To address this, we propose an object-centric representation to describe 3D scenes with sparse 3D semantic Gaussians where each Gaussian represents a flexible region of interest and its semantic features. We aggregate information from images through the attention mechanism and iteratively refine the properties of 3D Gaussians including position, covariance, and semantics. We then propose an efficient Gaussian-to-voxel splatting method to generate 3D occupancy predictions, which only aggregates the neighboring Gaussians for a certain position. We conduct extensive experiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental results demonstrate that GaussianFormer achieves comparable performance with state-of-the-art methods with only 17.8% - 24.8% of their memory consumption. Code is available at: https://github.com/huang-yh/GaussianFormer.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Code is available at: https://github.com/huang-yh/GaussianFormer"
    },
    {
        "paper id": "2405.17520",
        "abstract url": "https://arxiv.org/abs/2405.17520",
        "title": "Advancing Medical Image Segmentation with Mini-Net: A Lightweight Solution Tailored for Efficient Segmentation of Medical Images",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "diagnosis"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Accurate segmentation of anatomical structures and abnormalities in medical images is crucial for computer-aided diagnosis and analysis. While deep learning techniques excel at this task, their computational demands pose challenges. Additionally, some cutting-edge segmentation methods, though effective for general object segmentation, may not be optimised for medical images. To address these issues, we propose Mini-Net, a lightweight segmentation network specifically designed for medical images. With fewer than 38,000 parameters, Mini-Net efficiently captures both high- and low-frequency features, enabling real-time applications in various medical imaging scenarios. We evaluate Mini-Net on various datasets, including DRIVE, STARE, ISIC-2016, ISIC-2018, and MoNuSeg, demonstrating its robustness and good performance compared to state-of-the-art methods.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17533",
        "abstract url": "https://arxiv.org/abs/2405.17533",
        "title": "PAE: LLM-based Product Attribute Extraction for E-Commerce Fashion Trends",
        "rating": "-1",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Product attribute extraction is an growing field in e-commerce business, with several applications including product ranking, product recommendation, future assortment planning and improving online shopping customer experiences. Understanding the customer needs is critical part of online business, specifically fashion products. Retailers uses assortment planning to determine the mix of products to offer in each store and channel, stay responsive to market dynamics and to manage inventory and catalogs. The goal is to offer the right styles, in the right sizes and colors, through the right channels. When shoppers find products that meet their needs and desires, they are more likely to return for future purchases, fostering customer loyalty. Product attributes are a key factor in assortment planning. In this paper we present PAE, a product attribute extraction algorithm for future trend reports consisting text and images in PDF format. Most existing methods focus on attribute extraction from titles or product descriptions or utilize visual information from existing product images. Compared to the prior works, our work focuses on attribute extraction from PDF files where upcoming fashion trends are explained. This work proposes a more comprehensive framework that fully utilizes the different modalities for attribute extraction and help retailers to plan the assortment in advance. Our contributions are three-fold: (a) We develop PAE, an efficient framework to extract attributes from unstructured data (text and images); (b) We provide catalog matching methodology based on BERT representations to discover the existing attributes using upcoming attribute values; (c) We conduct extensive experiments with several baselines and show that PAE is an effective, flexible and on par or superior (avg 92.5% F1-Score) framework to existing state-of-the-art for attribute value extraction task.",
        "subjects": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Attribute Extraction, PDF files, Bert Embedding, Hashtag, Large Language Model (LLM), Text and Images"
    },
    {
        "paper id": "2405.17537",
        "abstract url": "https://arxiv.org/abs/2405.17537",
        "title": "BIOSCAN-CLIP: Bridging Vision and Genomics for Biodiversity Monitoring at Scale",
        "rating": "-1",
        "keywords": [
            [
                "BIOSCAN-CLIP",
                "health",
                "DNA"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for the taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, DNA barcodes, and textual data in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 11% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "comment": "16 pages with 9 figures"
    },
    {
        "paper id": "2405.17569",
        "abstract url": "https://arxiv.org/abs/2405.17569",
        "title": "Discriminant audio properties in deep learning based respiratory insufficiency detection in Brazilian Portuguese",
        "rating": "-1",
        "keywords": [
            [
                "biomarker"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This work investigates Artificial Intelligence (AI) systems that detect respiratory insufficiency (RI) by analyzing speech audios, thus treating speech as a RI biomarker. Previous works collected RI data (P1) from COVID-19 patients during the first phase of the pandemic and trained modern AI models, such as CNNs and Transformers, which achieved $96.5\\%$ accuracy, showing the feasibility of RI detection via AI. Here, we collect RI patient data (P2) with several causes besides COVID-19, aiming at extending AI-based RI detection. We also collected control data from hospital patients without RI. We show that the considered models, when trained on P1, do not generalize to P2, indicating that COVID-19 RI has features that may not be found in all RI types.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "5 pages, 2 figures, 1 table. Published in Artificial Intelligence in Medicine (AIME) 2023"
    },
    {
        "paper id": "2405.17590",
        "abstract url": "https://arxiv.org/abs/2405.17590",
        "title": "Optimizing Layout of Recursive Datatypes with Marmoset",
        "rating": "-1",
        "keywords": [
            [
                "synthesize"
            ]
        ],
        "abstract": "While programmers know that the low-level memory representation of data structures can have significant effects on performance, compiler support to optimize the layout of those structures is an under-explored field. Prior work has optimized the layout of individual, non-recursive structures without considering how collections of those objects in linked or recursive data structures are laid out. This work introduces Marmoset, a compiler that optimizes the layouts of algebraic datatypes, with a special focus on producing highly optimized, packed data layouts where recursive structures can be traversed with minimal pointer chasing. Marmoset performs an analysis of how a recursive ADT is used across functions to choose a global layout that promotes simple, strided access for that ADT in memory. It does so by building and solving a constraint system to minimize an abstract cost model, yielding a predicted efficient layout for the ADT. Marmoset then builds on top of Gibbon, a prior compiler for packed, mostly-serial representations, to synthesize optimized ADTs. We show experimentally that Marmoset is able to choose optimal layouts across a series of microbenchmarks and case studies, outperforming both Gibbons baseline approach, as well as MLton, a Standard ML compiler that uses traditional pointer-heavy representations.",
        "subjects": [
            "cs.PL",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17602",
        "abstract url": "https://arxiv.org/abs/2405.17602",
        "title": "Augmenting Textual Generation via Topology Aware Retrieval",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Despite the impressive advancements of Large Language Models (LLMs) in generating text, they are often limited by the knowledge contained in the input and prone to producing inaccurate or hallucinated content. To tackle these issues, Retrieval-augmented Generation (RAG) is employed as an effective strategy to enhance the available knowledge base and anchor the responses in reality by pulling additional texts from external databases. In real-world applications, texts are often linked through entities within a graph, such as citations in academic papers or comments in social networks. This paper exploits these topological relationships to guide the retrieval process in RAG. Specifically, we explore two kinds of topological connections: proximity-based, focusing on closely connected nodes, and role-based, which looks at nodes sharing similar subgraph structures. Our empirical research confirms their relevance to text relationships, leading us to develop a Topology-aware Retrieval-augmented Generation framework. This framework includes a retrieval module that selects texts based on their topological relationships and an aggregation module that integrates these texts into prompts to stimulate LLMs for text generation. We have curated established text-attributed networks and conducted comprehensive experiments to validate the effectiveness of this framework, demonstrating its potential to enhance RAG with topological awareness.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17613",
        "abstract url": "https://arxiv.org/abs/2405.17613",
        "title": "A Framework for Multi-modal Learning: Jointly Modeling Inter- & Intra-Modality Dependencies",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Supervised multi-modal learning involves mapping multiple modalities to a target label. Previous studies in this field have concentrated on capturing in isolation either the inter-modality dependencies (the relationships between different modalities and the label) or the intra-modality dependencies (the relationships within a single modality and the label). We argue that these conventional approaches that rely solely on either inter- or intra-modality dependencies may not be optimal in general. We view the multi-modal learning problem from the lens of generative models where we consider the target as a source of multiple modalities and the interaction between them. Towards that end, we propose inter- & intra-modality modeling (I2M2) framework, which captures and integrates both the inter- and intra-modality dependencies, leading to more accurate predictions. We evaluate our approach using real-world healthcare and vision-and-language datasets with state-of-the-art models, demonstrating superior performance over traditional methods focusing only on one type of modality dependency.",
        "subjects": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17654",
        "abstract url": "https://arxiv.org/abs/2405.17654",
        "title": "Data-Driven Personalized Energy Consumption Range Estimation for Plug-in Hybrid Electric Vehicles in Urban Traffic",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "In urban traffic environments, driver behaviors exhibit considerable diversity in vehicle operation, encompassing a range of acceleration and braking maneuvers as well as adherence to traffic regulations, such as speed limits. It is well-established that these intrinsic driving behaviors significantly influence vehicle energy consumption. Therefore, establishing a quantitative relationship between driver behavior and energy usage is essential for identifying energy-efficient driving practices and optimizing routes within urban traffic. This study introduces a data-driven approach to predict the equivalent fuel consumption of a plug-in hybrid electric vehicle (PHEV) based on an integrated model of driver behavior and vehicle energy consumption. Unlike traditional models that provide point predictions of fuel consumption, this approach uses Conformalized Quantile Regression (CQR) to offer prediction intervals that capture the variability and uncertainty in fuel consumption. These intervals reflect changes in fuel consumption, as well as variations in driver behavior, and vehicle and route conditions. To develop this model, driver-specific data were collected through a driver-in-the-loop simulator, which tested different human drivers responses. The CQR model was then trained and validated using the experimental data from the driver-in-the-loop simulator, augmented by the synthetic data generated from Monte Carlo simulations conducted using a calibrated microscopic driver behavior and vehicle energy model. The CQR model was evaluated by comparing its predictions of equivalent fuel consumption intervals with those of baseline prediction interval methods that rely solely on conformal prediction.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17665",
        "abstract url": "https://arxiv.org/abs/2405.17665",
        "title": "Enhanced Robot Arm at the Edge with NLP and Vision Systems",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "Robot"
            ]
        ],
        "abstract": "This paper introduces a \"proof of concept\" for a new approach to assistive robotics, integrating edge computing with Natural Language Processing (NLP) and computer vision to enhance the interaction between humans and robotic systems. Our \"proof of concept\" demonstrates the feasibility of using large language models (LLMs) and vision systems in tandem for interpreting and executing complex commands conveyed through natural language. This integration aims to improve the intuitiveness and accessibility of assistive robotic systems, making them more adaptable to the nuanced needs of users with disabilities. By leveraging the capabilities of edge computing, our system has the potential to minimize latency and support offline capability, enhancing the autonomy and responsiveness of assistive robots. Experimental results from our implementation on a robotic arm show promising outcomes in terms of accurate intent interpretation and object manipulation based on verbal commands. This research lays the groundwork for future developments in assistive robotics, focusing on creating highly responsive, user-centric systems that can significantly improve the quality of life for individuals with disabilities.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17670",
        "abstract url": "https://arxiv.org/abs/2405.17670",
        "title": "Deployment of NLP and LLM Techniques to Control Mobile Robots at the Edge: A Case Study Using GPT-4-Turbo and LLaMA 2",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "This paper investigates the possibility of intuitive human-robot interaction through the application of Natural Language Processing (NLP) and Large Language Models (LLMs) in mobile robotics. We aim to explore the feasibility of using these technologies for edge-based deployment, where traditional cloud dependencies are eliminated. The study specifically contrasts the performance of GPT-4-Turbo, which requires cloud connectivity, with an offline-capable, quantized version of LLaMA 2 (LLaMA 2-7B.Q5 K M). Our results show that GPT-4-Turbo delivers superior performance in interpreting and executing complex commands accurately, whereas LLaMA 2 exhibits significant limitations in consistency and reliability of command execution. Communication between the control computer and the mobile robot is established via a Raspberry Pi Pico W, which wirelessly receives commands from the computer without internet dependency and transmits them through a wired connection to the robot's Arduino controller. This study highlights the potential and challenges of implementing LLMs and NLP at the edge, providing groundwork for future research into fully autonomous and network-independent robotic systems. For video demonstrations and source code, please refer to: https://tinyurl.com/RobocupSym2024.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17677",
        "abstract url": "https://arxiv.org/abs/2405.17677",
        "title": "Understanding differences in applying DETR to natural and medical images",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transformer-based detectors have shown success in computer vision tasks with natural images. These models, exemplified by the Deformable DETR, are optimized through complex engineering strategies tailored to the typical characteristics of natural scenes. However, medical imaging data presents unique challenges such as extremely large image sizes, fewer and smaller regions of interest, and object classes which can be differentiated only through subtle differences. This study evaluates the applicability of these transformer-based design choices when applied to a screening mammography dataset that represents these distinct medical imaging data characteristics. Our analysis reveals that common design choices from the natural image domain, such as complex encoder architectures, multi-scale feature fusion, query initialization, and iterative bounding box refinement, do not improve and sometimes even impair object detection performance in medical imaging. In contrast, simpler and shallower architectures often achieve equal or superior results. This finding suggests that the adaptation of transformer models for medical imaging data requires a reevaluation of standard practices, potentially leading to more efficient and specialized frameworks for medical diagnosis.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17698",
        "abstract url": "https://arxiv.org/abs/2405.17698",
        "title": "BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos",
        "rating": "-1",
        "keywords": [
            [
                "Drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior. Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras. To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. This study presents a novel dataset from drone videos for baboon detection, tracking, and behavior recognition. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The tracking dataset is derived from the detection dataset, where all bounding boxes are assigned the same ID throughout the video. This process resulted in half an hour of very dense tracking data. The behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal; each mini-scene was manually annotated with 12 distinct behavior types, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62\\% for the YOLOv8-X detection model, multiple object tracking precision (MOTA) of 63.81\\% for the BotSort tracking algorithm, and micro top-1 accuracy of 63.97\\% for the X3D behavior recognition model. Using deep learning to classify wildlife behavior from drone footage facilitates non-invasive insight into the collective behavior of an entire group.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Dataset will be published shortly"
    },
    {
        "paper id": "2405.17710",
        "abstract url": "https://arxiv.org/abs/2405.17710",
        "title": "Does Geo-co-location Matter? A Case Study of Public Health Conversations during COVID-19",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "Health"
            ],
            [
                "cs.SI",
                "cs.CL"
            ]
        ],
        "abstract": "Social media platforms like Twitter (now X) have been pivotal in information dissemination and public engagement, especially during COVID-19. A key goal for public health experts was to encourage prosocial behavior that could impact local outcomes such as masking and social distancing. Given the importance of local news and guidance during COVID-19, the objective of our research is to analyze the effect of localized engagement, on social media conversations. This study examines the impact of geographic co-location, as a proxy for localized engagement between public health experts (PHEs) and the public, on social media. We analyze a Twitter conversation dataset from January 2020 to November 2021, comprising over 19 K tweets from nearly five hundred PHEs, along with approximately 800 K replies from 350 K participants. Our findings reveal that geo-co-location is associated with higher engagement rates, especially in conversations on topics including masking, lockdowns, and education, and in conversations with academic and medical professionals. Lexical features associated with emotion and personal experiences were more common in geo-co-located contexts. This research provides insights into how geographic co-location influences social media engagement and can inform strategies to improve public health messaging.",
        "subjects": [
            "cs.SI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17729",
        "abstract url": "https://arxiv.org/abs/2405.17729",
        "title": "Hierarchical Action Recognition: A Contrastive Video-Language Approach with Hierarchical Interactions",
        "rating": "-1",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video recognition remains an open challenge, requiring the identification of diverse content categories within videos. Mainstream approaches often perform flat classification, overlooking the intrinsic hierarchical structure relating categories. To address this, we formalize the novel task of hierarchical video recognition, and propose a video-language learning framework tailored for hierarchical recognition. Specifically, our framework encodes dependencies between hierarchical category levels, and applies a top-down constraint to filter recognition predictions. We further construct a new fine-grained dataset based on medical assessments for rehabilitation of stroke patients, serving as a challenging benchmark for hierarchical recognition. Through extensive experiments, we demonstrate the efficacy of our approach for hierarchical recognition, significantly outperforming conventional methods, especially for fine-grained subcategories. The proposed framework paves the way for hierarchical modeling in video understanding tasks, moving beyond flat categorization.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17737",
        "abstract url": "https://arxiv.org/abs/2405.17737",
        "title": "The HTTP Garden: Discovering Parsing Vulnerabilities in HTTP/1.1 Implementations by Differential Fuzzing of Request Streams",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "HTTP/1.1 parsing discrepancies have been the basis for numerous classes of attacks against web servers. Previous techniques for discovering HTTP parsing discrepancies have focused on blackbox differential testing of HTTP gateway servers, despite evidence that the most significant parsing anomalies occur within origin servers. While these techniques can detect some vulnerabilities, not all parsing discrepancy-related vulnerabilities are detectable by examining a gateway server's output alone. Our system, the HTTP Garden, examines both origin servers' interpretations and gateway servers' transformations of HTTP requests. It also includes a coverage-guided differential fuzzer for HTTP/1.1 origin servers that is capable of mutating all components of a request stream, paired with an interactive REPL that facilitates the automatic discovery of meaningful HTTP parsing discrepancies and the rapid development of those discrepancies into attack payloads. Using our tool, we have discovered and reported over 100 HTTP parsing bugs in popular web servers, of which 68 have been fixed following our reports. We designate 39 of these to be exploitable. We release the HTTP Garden to the public on GitHub under a free software license to allow researchers to further explore new parser discrepancy-based attacks against HTTP/1.1 servers.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17740",
        "abstract url": "https://arxiv.org/abs/2405.17740",
        "title": "MobileConvRec: A Conversational Dataset for Mobile Apps Recommendations",
        "rating": "-1",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Existing recommendation systems have focused on two paradigms: 1- historical user-item interaction-based recommendations and 2- conversational recommendations. Conversational recommendation systems facilitate natural language dialogues between users and the system, allowing the system to solicit users' explicit needs while enabling users to inquire about recommendations and provide feedback. Due to substantial advancements in natural language processing, conversational recommendation systems have gained prominence. Existing conversational recommendation datasets have greatly facilitated research in their respective domains. Despite the exponential growth in mobile users and apps in recent years, research in conversational mobile app recommender systems has faced substantial constraints. This limitation can primarily be attributed to the lack of high-quality benchmark datasets specifically tailored for mobile apps. To facilitate research for conversational mobile app recommendations, we introduce MobileConvRec. MobileConvRec simulates conversations by leveraging real user interactions with mobile apps on the Google Play store, originally captured in large-scale mobile app recommendation dataset MobileRec. The proposed conversational recommendation dataset synergizes sequential user-item interactions, which reflect implicit user preferences, with comprehensive multi-turn conversations to effectively grasp explicit user needs. MobileConvRec consists of over 12K multi-turn recommendation-related conversations spanning 45 app categories. Moreover, MobileConvRec presents rich metadata for each app such as permissions data, security and privacy-related information, and binary executables of apps, among others. We demonstrate that MobileConvRec can serve as an excellent testbed for conversational mobile app recommendation through a comparative study of several pre-trained large language models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17743",
        "abstract url": "https://arxiv.org/abs/2405.17743",
        "title": "ORLM: Training Large Language Models for Optimization Modeling",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for tackling complex Operations Research (OR) problem by providing the capacity in automating optimization modeling. However, current methodologies heavily rely on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs, raising data privacy concerns that could be prohibitive in industry applications. To tackle this issue, we propose training open-source LLMs for optimization modeling. We identify four critical requirements for the training dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process for creating synthetic data tailored to specific requirements. We also introduce the IndustryOR benchmark, the first industrial benchmark for testing LLMs on solving real-world OR problems. We apply the data from OR-Instruct to various open-source LLMs of 7b size (termed as ORLMs), resulting in a significantly improved capability for optimization modeling. Our best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data are available at \\url{https://github.com/Cardinal-Operations/ORLM}.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CE",
            "cs.LG"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2405.17756",
        "abstract url": "https://arxiv.org/abs/2405.17756",
        "title": "Motion-Informed Deep Learning for Brain MR Image Reconstruction Framework",
        "rating": "-1",
        "keywords": [
            [
                "MRI",
                "clinical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the frequently occurring artifacts due to patient movements during scanning. Motion is estimated to be present in approximately 30% of clinical MRI scans; however, motion has not been explicitly modeled within deep learning image reconstruction models. Deep learning (DL) algorithms have been demonstrated to be effective for both the image reconstruction task and the motion correction task, but the two tasks are considered separately. The image reconstruction task involves removing undersampling artifacts such as noise and aliasing artifacts, whereas motion correction involves removing artifacts including blurring, ghosting, and ringing. In this work, we propose a novel method to simultaneously accelerate imaging and correct motion. This is achieved by integrating a motion module into the deep learning-based MRI reconstruction process, enabling real-time detection and correction of motion. We model motion as a tightly integrated auxiliary layer in the deep learning model during training, making the deep learning model 'motion-informed'. During inference, image reconstruction is performed from undersampled raw k-space data using a trained motion-informed DL model. Experimental results demonstrate that the proposed motion-informed deep learning image reconstruction network outperformed the conventional image reconstruction network for motion-degraded MRI datasets.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "physics.med-ph"
        ],
        "comment": "22 pages, 7 figures, 4 tables"
    },
    {
        "paper id": "2405.17757",
        "abstract url": "https://arxiv.org/abs/2405.17757",
        "title": "NASPrecision: Neural Architecture Search-Driven Multi-Stage Learning for Surface Roughness Prediction in Ultra-Precision Machining",
        "rating": "-1",
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ]
        ],
        "abstract": "Accurate surface roughness prediction is critical for ensuring high product quality, especially in areas like manufacturing and aerospace, where the smallest imperfections can compromise performance or safety. However, this is challenging due to complex, non-linear interactions among variables, which is further exacerbated with limited and imbalanced datasets. Existing methods using traditional machine learning algorithms require extensive domain knowledge for feature engineering and substantial human intervention for model selection. To address these issues, we propose NASPrecision, a Neural Architecture Search (NAS)-Driven Multi-Stage Learning Framework. This innovative approach autonomously identifies the most suitable features and models for various surface roughness prediction tasks and significantly enhances the performance by multi-stage learning. Our framework operates in three stages: 1) architecture search stage, employing NAS to automatically identify the most effective model architecture; 2) initial training stage, where we train the neural network for initial predictions; 3) refinement stage, where a subsequent model is appended to refine and capture subtle variations overlooked by the initial training stage. In light of limited and imbalanced datasets, we adopt a generative data augmentation technique to balance and generate new data by learning the underlying data distribution. We conducted experiments on three distinct real-world datasets linked to different machining techniques. Results show improvements in Mean Absolute Percentage Error (MAPE), Root Mean Square Error (RMSE), and Standard Deviation (STD) by 18%, 31%, and 22%, respectively. This establishes it as a robust and general solution for precise surface roughness prediction, potentially boosting production efficiency and product quality in key industries while minimizing domain expertise and human intervention.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17759",
        "abstract url": "https://arxiv.org/abs/2405.17759",
        "title": "Wireless Federated Learning over Resource-Constrained Networks: Digital versus Analog Transmissions",
        "rating": "-1",
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "To enable wireless federated learning (FL) in communication resource-constrained networks, two communication schemes, i.e., digital and analog ones, are effective solutions. In this paper, we quantitatively compare these two techniques, highlighting their essential differences as well as respectively suitable scenarios. We first examine both digital and analog transmission schemes, together with a unified and fair comparison framework under imbalanced device sampling, strict latency targets, and transmit power constraints. A universal convergence analysis under various imperfections is established for evaluating the performance of FL over wireless networks. These analytical results reveal that the fundamental difference between the digital and analog communications lies in whether communication and computation are jointly designed or not. The digital scheme decouples the communication design from FL computing tasks, making it difficult to support uplink transmission from massive devices with limited bandwidth and hence the performance is mainly communication-limited. In contrast, the analog communication allows over-the-air computation (AirComp) and achieves better spectrum utilization. However, the computation-oriented analog transmission reduces power efficiency, and its performance is sensitive to computation errors from imperfect channel state information (CSI). Furthermore, device sampling for both schemes are optimized and differences in sampling optimization are analyzed. Numerical results verify the theoretical analysis and affirm the superior performance of the sampling optimization.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Accepted by IEEE TWC. arXiv admin note: text overlap with arXiv:2402.09657"
    },
    {
        "paper id": "2405.17769",
        "abstract url": "https://arxiv.org/abs/2405.17769",
        "title": "Microsaccade-inspired Event Camera for Robotics",
        "rating": "-1",
        "keywords": [
            [
                "Event Camera"
            ],
            [
                "Robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Neuromorphic vision sensors or event cameras have made the visual perception of extremely low reaction time possible, opening new avenues for high-dynamic robotics applications. These event cameras' output is dependent on both motion and texture. However, the event camera fails to capture object edges that are parallel to the camera motion. This is a problem intrinsic to the sensor and therefore challenging to solve algorithmically. Human vision deals with perceptual fading using the active mechanism of small involuntary eye movements, the most prominent ones called microsaccades. By moving the eyes constantly and slightly during fixation, microsaccades can substantially maintain texture stability and persistence. Inspired by microsaccades, we designed an event-based perception system capable of simultaneously maintaining low reaction time and stable texture. In this design, a rotating wedge prism was mounted in front of the aperture of an event camera to redirect light and trigger events. The geometrical optics of the rotating wedge prism allows for algorithmic compensation of the additional rotational motion, resulting in a stable texture appearance and high informational output independent of external motion. The hardware device and software solution are integrated into a system, which we call Artificial MIcrosaccade-enhanced EVent camera (AMI-EV). Benchmark comparisons validate the superior data quality of AMI-EV recordings in scenarios where both standard cameras and event cameras fail to deliver. Various real-world experiments demonstrate the potential of the system to facilitate robotics perception both for low-level and high-level vision tasks.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "Published on Science Robotics June 2024 issue"
    },
    {
        "paper id": "2405.17780",
        "abstract url": "https://arxiv.org/abs/2405.17780",
        "title": "Unmasking Vulnerabilities: Cardinality Sketches under Adaptive Inputs",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Cardinality sketches are popular data structures that enhance the efficiency of working with large data sets. The sketches are randomized representations of sets that are only of logarithmic size but can support set merges and approximate cardinality (i.e., distinct count) queries. When queries are not adaptive, that is, they do not depend on preceding query responses, the design provides strong guarantees of correctly answering a number of queries exponential in the sketch size $k$. In this work, we investigate the performance of cardinality sketches in adaptive settings and unveil inherent vulnerabilities. We design an attack against the ``standard'' estimators that constructs an adversarial input by post-processing responses to a set of simple non-adaptive queries of size linear in the sketch size $k$. Empirically, our attack used only $4k$ queries with the widely used HyperLogLog (HLL++)~\\citep{hyperloglog:2007,hyperloglogpractice:EDBT2013} sketch. The simple attack technique suggests it can be effective with post-processed natural workloads. Finally and importantly, we demonstrate that the vulnerability is inherent as \\emph{any} estimator applied to known sketch structures can be attacked using a number of queries that is quadratic in $k$, matching a generic upper bound.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17793",
        "abstract url": "https://arxiv.org/abs/2405.17793",
        "title": "SafeguardGS: 3D Gaussian Primitive Pruning While Avoiding Catastrophic Scene Destruction",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting (3DGS) has made a significant stride in novel view synthesis, demonstrating top-notch rendering quality while achieving real-time rendering speed. However, the excessively large number of Gaussian primitives resulting from 3DGS' suboptimal densification process poses a major challenge, slowing down frame-per-second (FPS) and demanding considerable memory cost, making it unfavorable for low-end devices. To cope with this issue, many follow-up studies have suggested various pruning techniques, often in combination with different score functions, to optimize rendering performance. Nonetheless, a comprehensive discussion regarding their effectiveness and implications across all techniques is missing. In this paper, we first categorize 3DGS pruning techniques into two types: Cross-view pruning and pixel-wise pruning, which differ in their approaches to rank primitives. Our subsequent experiments reveal that while cross-view pruning leads to disastrous quality drops under extreme Gaussian primitives decimation, the pixel-wise pruning technique not only sustains relatively high rendering quality with minuscule performance degradation but also provides a reasonable minimum boundary for pruning. Building on this observation, we further propose multiple variations of score functions and empirically discover that the color-weighted score function outperforms others for discriminating insignificant primitives for rendering. We believe our research provides valuable insights for optimizing 3DGS pruning strategies for future works.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Comprehensive experiments are in progress"
    },
    {
        "paper id": "2405.17804",
        "abstract url": "https://arxiv.org/abs/2405.17804",
        "title": "Detection-Correction Structure via General Language Model for Grammatical Error Correction",
        "rating": "-1",
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Grammatical error correction (GEC) is a task dedicated to rectifying texts with minimal edits, which can be decoupled into two components: detection and correction. However, previous works have predominantly focused on direct correction, with no prior efforts to integrate both into a single model. Moreover, the exploration of the detection-correction paradigm by large language models (LLMs) remains underdeveloped. This paper introduces an integrated detection-correction structure, named DeCoGLM, based on the General Language Model (GLM). The detection phase employs a fault-tolerant detection template, while the correction phase leverages autoregressive mask infilling for localized error correction. Through the strategic organization of input tokens and modification of attention masks, we facilitate multi-task learning within a single model. Our model demonstrates competitive performance against the state-of-the-art models on English and Chinese GEC datasets. Further experiments present the effectiveness of the detection-correction structure in LLMs, suggesting a promising direction for GEC.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Long paper. Accepted by ACL 2024 Main Conference"
    },
    {
        "paper id": "2405.17811",
        "abstract url": "https://arxiv.org/abs/2405.17811",
        "title": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "synthesis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Neural 3D representations such as Neural Radiance Fields (NeRF), excel at producing photo-realistic rendering results but lack the flexibility for manipulation and editing which is crucial for content creation. Previous works have attempted to address this issue by deforming a NeRF in canonical space or manipulating the radiance field based on an explicit mesh. However, manipulating NeRF is not highly controllable and requires a long training and inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely high-fidelity novel view synthesis can be achieved using an explicit point-based 3D representation with much faster training and rendering speed. However, there is still a lack of effective means to manipulate 3DGS freely while maintaining rendering quality. In this work, we aim to tackle the challenge of achieving manipulable photo-realistic rendering. We propose to utilize a triangular mesh to manipulate 3DGS directly with self-adaptation. This approach reduces the need to design various algorithms for different types of Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding and adapting method, we can achieve 3DGS manipulation and preserve high-fidelity rendering after manipulation. Our approach is capable of handling large deformations, local manipulations, and soft body simulations while keeping high-quality rendering. Furthermore, we demonstrate that our method is also effective with inaccurate meshes extracted from 3DGS. Experiments conducted demonstrate the effectiveness of our method and its superiority over baseline approaches.",
        "subjects": [
            "cs.GR",
            "cs.CV"
        ],
        "comment": "Project page here: https://gaoxiangjun.github.io/mani_gs/"
    },
    {
        "paper id": "2405.17819",
        "abstract url": "https://arxiv.org/abs/2405.17819",
        "title": "An optimal chromatic bound for ($P_2+P_3$, gem)-free graphs",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Given a graph $G$, the parameters $\u03c7(G)$ and $\u03c9(G)$ respectively denote the chromatic number and the clique number of $G$. A function $f : \\mathbb{N} \\rightarrow \\mathbb{N}$ such that $f(1) = 1$ and $f(x) \\geq x$, for all $x \\in \\mathbb{N}$ is called a $\u03c7$-binding function for the given class of graphs $\\cal{G}$ if every $G \\in \\cal{G}$ satisfies $\u03c7(G) \\leq f(\u03c9(G))$, and the \\emph{smallest $\u03c7$-binding function} $f^*$ for $\\cal{G}$ is defined as $f^*(x) := \\max\\{\u03c7(G)\\mid G\\in {\\cal G} \\mbox{ and } \u03c9(G)=x\\}$. In general, the problem of obtaining the smallest $\u03c7$-binding function for the given class of graphs seems to be extremely hard, and only a few classes of graphs are studied in this direction. In this paper, we study the class of ($P_2+ P_3$, gem)-free graphs, and prove that the function $\u03c6:\\mathbb{N}\\rightarrow \\mathbb{N}$ defined by $\u03c6(1)=1$, $\u03c6(2)=4$, $\u03c6(3)=6$ and $\u03c6(x)=\\left\\lceil\\frac{1}{4}(5x-1)\\right\\rceil$, for $x\\geq 4$ is the smallest $\u03c7$-binding function for the class of ($P_2+ P_3$, gem)-free graphs.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17824",
        "abstract url": "https://arxiv.org/abs/2405.17824",
        "title": "mTREE: Multi-Level Text-Guided Representation End-to-End Learning for Whole Slide Image Analysis",
        "rating": "-1",
        "keywords": [
            [
                "survival",
                "Whole Slide"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-modal learning adeptly integrates visual and textual data, but its application to histopathology image and text analysis remains challenging, particularly with large, high-resolution images like gigapixel Whole Slide Images (WSIs). Current methods typically rely on manual region labeling or multi-stage learning to assemble local representations (e.g., patch-level) into global features (e.g., slide-level). However, there is no effective way to integrate multi-scale image representations with text data in a seamless end-to-end process. In this study, we introduce Multi-Level Text-Guided Representation End-to-End Learning (mTREE). This novel text-guided approach effectively captures multi-scale WSI representations by utilizing information from accompanying textual pathology information. mTREE innovatively combines - the localization of key areas (global-to-local) and the development of a WSI-level image-text representation (local-to-global) - into a unified, end-to-end learning framework. In this model, textual information serves a dual purpose: firstly, functioning as an attention map to accurately identify key areas, and secondly, acting as a conduit for integrating textual features into the comprehensive representation of the image. Our study demonstrates the effectiveness of mTREE through quantitative analyses in two image-related tasks: classification and survival prediction, showcasing its remarkable superiority over baselines.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.18448",
        "abstract url": "https://arxiv.org/abs/2405.18448",
        "title": "Multi-objective Representation for Numbers in Clinical Narratives Using CamemBERT-bio",
        "rating": "-1",
        "keywords": [
            [
                "bio",
                "medical",
                "Clinical",
                "physiological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This research aims to classify numerical values extracted from medical documents across seven distinct physiological categories, employing CamemBERT-bio. Previous studies suggested that transformer-based models might not perform as well as traditional NLP models in such tasks. To enhance CamemBERT-bio's performances, we introduce two main innovations: integrating keyword embeddings into the model and adopting a number-agnostic strategy by excluding all numerical data from the text. The implementation of label embedding techniques refines the attention mechanisms, while the technique of using a `numerical-blind' dataset aims to bolster context-centric learning. Another key component of our research is determining the criticality of extracted numerical data. To achieve this, we utilized a simple approach that involves verifying if the value falls within the established standard ranges. Our findings are encouraging, showing substantial improvements in the effectiveness of CamemBERT-bio, surpassing conventional methods with an F1 score of 0.89. This represents an over 20\\% increase over the 0.73 $F_1$ score of traditional approaches and an over 9\\% increase over the 0.82 $F_1$ score of state-of-the-art approaches. All this was achieved despite using small and imbalanced training datasets.",
        "subjects": [
            "cs.CL",
            "eess.SP"
        ],
        "comment": "Under the revision. arXiv admin note: substantial text overlap with arXiv:2404.10171"
    },
    {
        "paper id": "2405.16901",
        "abstract url": "https://arxiv.org/abs/2405.16901",
        "title": "Recurrent and Convolutional Neural Networks in Classification of EEG Signal for Guided Imagery and Mental Workload Detection",
        "rating": "-1.5",
        "keywords": [
            [
                "EEG"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Guided Imagery technique is reported to be used by therapists all over the world in order to increase the comfort of patients suffering from a variety of disorders from mental to oncology ones and proved to be successful in numerous of ways. Possible support for the therapists can be estimation of the time at which subject goes into deep relaxation. This paper presents the results of the investigations of a cohort of 26 students exposed to Guided Imagery relaxation technique and mental task workloads conducted with the use of dense array electroencephalographic amplifier. The research reported herein aimed at verification whether it is possible to detect differences between those two states and to classify them using deep learning methods and recurrent neural networks such as EEGNet, Long Short-Term Memory-based classifier, 1D Convolutional Neural Network and hybrid model of 1D Convolutional Neural Network and Long Short-Term Memory. The data processing pipeline was presented from the data acquisition, through the initial data cleaning, preprocessing and postprocessing. The classification was based on two datasets: one of them using 26 so-called cognitive electrodes and the other one using signal collected from 256 channels. So far there have not been such comparisons in the application being discussed. The classification results are presented by the validation metrics such as: accuracy, recall, precision, F1-score and loss for each case. It turned out that it is not necessary to collect signals from all electrodes as classification of the cognitive ones gives the results similar to those obtained for the full signal and extending input to 256 channels does not add much value. In Disscussion there were proposed an optimal classifier as well as some suggestions concerning the prospective development of the project.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "In review"
    },
    {
        "paper id": "2405.16926",
        "abstract url": "https://arxiv.org/abs/2405.16926",
        "title": "Is Cambodia the World's Largest Cashew Producer?",
        "rating": "-1.5",
        "keywords": [
            [
                "satellite",
                "agricultural"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Cambodia's agricultural landscape is rapidly transforming, particularly in the cashew sector. Despite the country's rapid emergence and ambition to become the largest cashew producer, comprehensive data on plantation areas and the environmental impacts of this expansion are lacking. This study addresses the gap in detailed land use data for cashew plantations in Cambodia and assesses the implications of agricultural advancements. We collected over 80,000 training polygons across Cambodia to train a convolutional neural network using high-resolution optical and SAR satellite data for precise cashew plantation mapping. Our findings indicate that Cambodia ranks among the top five in terms of cultivated area and the top three in global cashew production, driven by high yields. Significant cultivated areas are located in Kampong Thom, Kratie, and Ratanak Kiri provinces. Balancing rapid agricultural expansion with environmental stewardship, particularly forest conservation, is crucial. Cambodia's cashew production is poised for further growth, driven by high-yielding trees and premium nuts. However, sustainable expansion requires integrating agricultural practices with economic and environmental strategies to enhance local value and protect forested areas. Advanced mapping technologies offer comprehensive tools to support these objectives and ensure the sustainable development of Cambodia's cashew industry.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "13 pages, 5 figures, 4 tables"
    },
    {
        "paper id": "2405.16929",
        "abstract url": "https://arxiv.org/abs/2405.16929",
        "title": "Uncertainty Management in the Construction of Knowledge Graphs: a Survey",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Knowledge Graphs (KGs) are a major asset for companies thanks to their great flexibility in data representation and their numerous applications, e.g., vocabulary sharing, Q/A or recommendation systems. To build a KG it is a common practice to rely on automatic methods for extracting knowledge from various heterogeneous sources. But in a noisy and uncertain world, knowledge may not be reliable and conflicts between data sources may occur. Integrating unreliable data would directly impact the use of the KG, therefore such conflicts must be resolved. This could be done manually by selecting the best data to integrate. This first approach is highly accurate, but costly and time-consuming. That is why recent efforts focus on automatic approaches, which represents a challenging task since it requires handling the uncertainty of extracted knowledge throughout its integration into the KG. We survey state-of-the-art approaches in this direction and present constructions of both open and enterprise KGs and how their quality is maintained. We then describe different knowledge extraction methods, introducing additional uncertainty. We also discuss downstream tasks after knowledge acquisition, including KG completion using embedding models, knowledge alignment, and knowledge fusion in order to address the problem of knowledge uncertainty in KG construction. We conclude with a discussion on the remaining challenges and perspectives when constructing a KG taking into account uncertainty.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16946",
        "abstract url": "https://arxiv.org/abs/2405.16946",
        "title": "Biological Neurons Compete with Deep Reinforcement Learning in Sample Efficiency in a Simulated Gameworld",
        "rating": "-1.5",
        "keywords": [
            [
                "Biological"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "How do biological systems and machine learning algorithms compare in the number of samples required to show significant improvements in completing a task? We compared the learning efficiency of in vitro biological neural networks to the state-of-the-art deep reinforcement learning (RL) algorithms in a simplified simulation of the game `Pong'. Using DishBrain, a system that embodies in vitro neural networks with in silico computation using a high-density multi-electrode array, we contrasted the learning rate and the performance of these biological systems against time-matched learning from three state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same game environment. This allowed a meaningful comparison between biological neural systems and deep RL. We find that when samples are limited to a real-world time course, even these very simple biological cultures outperformed deep RL algorithms across various game performance characteristics, implying a higher sample efficiency. Ultimately, even when tested across multiple types of information input to assess the impact of higher dimensional data input, biological neurons showcased faster learning than all deep reinforcement learning agents.",
        "subjects": [
            "q-bio.NC",
            "cs.AI"
        ],
        "comment": "13 Pages, 6 Figures - 38 Supplementary Pages, 6 Supplementary Figures, 4 Supplementary Tables"
    },
    {
        "paper id": "2405.17003",
        "abstract url": "https://arxiv.org/abs/2405.17003",
        "title": "Graph Condensation for Open-World Graph Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by KDD 2024"
    },
    {
        "paper id": "2405.17042",
        "abstract url": "https://arxiv.org/abs/2405.17042",
        "title": "LabObf: A Label Protection Scheme for Vertical Federated Learning Through Label Obfuscation",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Split learning, as one of the most common architectures in vertical federated learning, has gained widespread use in industry due to its privacy-preserving characteristics. In this architecture, the party holding the labels seeks cooperation from other parties to improve model performance due to insufficient feature data. Each of these participants has a self-defined bottom model to learn hidden representations from its own feature data and uploads the embedding vectors to the top model held by the label holder for final predictions. This design allows participants to conduct joint training without directly exchanging data. However, existing research points out that malicious participants may still infer label information from the uploaded embeddings, leading to privacy leakage. In this paper, we first propose an embedding extension attack that manually modifies embeddings to undermine existing defense strategies, which rely on constraining the correlation between the embeddings uploaded by participants and the labels. Subsequently, we propose a new label obfuscation defense strategy, called `LabObf', which randomly maps each original one-hot vector label to multiple numerical soft labels with values intertwined, significantly increasing the difficulty for attackers to infer the labels. We conduct experiments on four different types of datasets, and the results show that LabObf can reduce the attacker's success rate to near random guessing while maintaining an acceptable model accuracy.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17070",
        "abstract url": "https://arxiv.org/abs/2405.17070",
        "title": "Efficient mid-term forecasting of hourly electricity load using generalized additive models",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Accurate mid-term (weeks to one year) hourly electricity load forecasts are essential for strategic decision-making in power plant operation, ensuring supply security and grid stability, and energy trading. While numerous models effectively predict short-term (hours to a few days) hourly load, mid-term forecasting solutions remain scarce. In mid-term load forecasting, besides daily, weekly, and annual seasonal and autoregressive effects, capturing weather and holiday effects, as well as socio-economic non-stationarities in the data, poses significant modeling challenges. To address these challenges, we propose a novel forecasting method using Generalized Additive Models (GAMs) built from interpretable P-splines and enhanced with autoregressive post-processing. This model uses smoothed temperatures, Error-Trend-Seasonal (ETS) modeled non-stationary states, a nuanced representation of holiday effects with weekday variations, and seasonal information as input. The proposed model is evaluated on load data from 24 European countries. This analysis demonstrates that the model not only has significantly enhanced forecasting accuracy compared to state-of-the-art methods but also offers valuable insights into the influence of individual components on predicted load, given its full interpretability. Achieving performance akin to day-ahead TSO forecasts in fast computation times of a few seconds for several years of hourly data underscores the model's potential for practical application in the power system industry.",
        "subjects": [
            "stat.AP",
            "cs.LG",
            "econ.GN",
            "q-fin.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17163",
        "abstract url": "https://arxiv.org/abs/2405.17163",
        "title": "Injecting Hamiltonian Architectural Bias into Deep Graph Networks for Long-Range Propagation",
        "rating": "-1.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems. We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components. Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time. Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17234",
        "abstract url": "https://arxiv.org/abs/2405.17234",
        "title": "Benchmarking General Purpose In-Context Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In-context learning (ICL) capabilities are becoming increasingly appealing for building general intelligence due to their sample efficiency and independence from artificial optimization skills. To enhance generalization, biological neural systems primarily inherit learning capabilities and subsequently refine their memory, acquiring diverse skills and knowledge through extensive lifelong experiences. This process gives rise to the concept of general-purpose in-context learning (GPICL). Compared to standard ICL, GPICL addresses a broader range of tasks, extends learning horizons, and starts at a lower zero-shot baseline. We introduce two lightweight but insightful benchmarks specifically crafted to train and evaluate GPICL functionalities. Each benchmark includes a vast number of tasks characterized by significant task variance and minimal transferable knowledge among tasks, facilitating lifelong in-context learning through continuous generation and interaction. These features pose significant challenges for models that rely on context or interactions to improve their proficiency, including language models, decision models, and world models. Our experiments reveal that parameter scale alone may not be crucial for ICL or GPICL, suggesting alternative approaches such as increasing the scale of contexts and memory states.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17272",
        "abstract url": "https://arxiv.org/abs/2405.17272",
        "title": "DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max Vehicle Routing Problems",
        "rating": "-1.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "Navigation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The min-max vehicle routing problem (min-max VRP) traverses all given customers by assigning several routes and aims to minimize the length of the longest route. Recently, reinforcement learning (RL)-based sequential planning methods have exhibited advantages in solving efficiency and optimality. However, these methods fail to exploit the problem-specific properties in learning representations, resulting in less effective features for decoding optimal routes. This paper considers the sequential planning process of min-max VRPs as two coupled optimization tasks: customer partition for different routes and customer navigation in each route (i.e., partition and navigation). To effectively process min-max VRP instances, we present a novel attention-based Partition-and-Navigation encoder (P&N Encoder) that learns distinct embeddings for partition and navigation. Furthermore, we utilize an inherent symmetry in decoding routes and develop an effective agent-permutation-symmetric (APS) loss function. Experimental results demonstrate that the proposed Decoupling-Partition-Navigation (DPN) method significantly surpasses existing learning-based methods in both single-depot and multi-depot min-max VRPs. Our code is available at",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17325",
        "abstract url": "https://arxiv.org/abs/2405.17325",
        "title": "Novel Approaches for ML-Assisted Particle Track Reconstruction and Hit Clustering",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Track reconstruction is a vital aspect of High-Energy Physics (HEP) and plays a critical role in major experiments. In this study, we delve into unexplored avenues for particle track reconstruction and hit clustering. Firstly, we enhance the algorithmic design effort by utilising a simplified simulator (REDVID) to generate training data that is specifically composed for simplicity. We demonstrate the effectiveness of this data in guiding the development of optimal network architectures. Additionally, we investigate the application of image segmentation networks for this task, exploring their potential for accurate track reconstruction. Moreover, we approach the task from a different perspective by treating it as a hit sequence to track sequence translation problem. Specifically, we explore the utilisation of Transformer architectures for tracking purposes. Our preliminary findings are covered in detail. By considering this novel approach, we aim to uncover new insights and potential advancements in track reconstruction. This research sheds light on previously unexplored methods and provides valuable insights for the field of particle track reconstruction and hit clustering in HEP.",
        "subjects": [
            "hep-ex",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17333",
        "abstract url": "https://arxiv.org/abs/2405.17333",
        "title": "Conditioning on Time is All You Need for Synthetic Survival Data Generation",
        "rating": "-1.5",
        "keywords": [
            [
                "Survival"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Synthetic data generation holds considerable promise, offering avenues to enhance privacy, fairness, and data accessibility. Despite the availability of various methods for generating synthetic tabular data, challenges persist, particularly in specialized applications such as survival analysis. One significant obstacle in survival data generation is censoring, which manifests as not knowing the precise timing of observed (target) events for certain instances. Existing methods face difficulties in accurately reproducing the real distribution of event times for both observed (uncensored) events and censored events, i.e., the generated event-time distributions do not accurately match the underlying distributions of the real data. So motivated, we propose a simple paradigm to produce synthetic survival data by generating covariates conditioned on event times (and censoring indicators), thus allowing one to reuse existing conditional generative models for tabular data without significant computational overhead, and without making assumptions about the (usually unknown) generation mechanism underlying censoring. We evaluate this method via extensive experiments on real-world datasets. Our methodology outperforms multiple competitive baselines at generating survival data, while improving the performance of downstream survival models trained on it and tested on real data.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17366",
        "abstract url": "https://arxiv.org/abs/2405.17366",
        "title": "EM-GANSim: Real-time and Accurate EM Simulation Using Conditional GANs for 3D Indoor Scenes",
        "rating": "-1.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "GAN"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a novel machine-learning (ML) approach (EM-GANSim) for real-time electromagnetic (EM) propagation that is used for wireless communication simulation in 3D indoor environments. Our approach uses a modified conditional Generative Adversarial Network (GAN) that incorporates encoded geometry and transmitter location while adhering to the electromagnetic propagation theory. The overall physically-inspired learning is able to predict the power distribution in 3D scenes, which is represented using heatmaps. Our overall accuracy is comparable to ray tracing-based EM simulation, as evidenced by lower mean squared error values. Furthermore, our GAN-based method drastically reduces the computation time, achieving a 5X speedup on complex benchmarks. In practice, it can compute the signal strength in a few milliseconds on any location in 3D indoor environments. We also present a large dataset of 3D models and EM ray tracing-simulated heatmaps. To the best of our knowledge, EM-GANSim is the first real-time algorithm for EM simulation in complex 3D indoor environments. We plan to release the code and the dataset.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": "10 pages, 8 figures, 5 tables"
    },
    {
        "paper id": "2405.17420",
        "abstract url": "https://arxiv.org/abs/2405.17420",
        "title": "Survival of the Fittest Representation: A Case Study with Modular Addition",
        "rating": "-1.5",
        "keywords": [
            [
                "Survival"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "When a neural network can learn multiple distinct algorithms to solve a task, how does it \"choose\" between them during training? To approach this question, we take inspiration from ecology: when multiple species coexist, they eventually reach an equilibrium where some survive while others die out. Analogously, we suggest that a neural network at initialization contains many solutions (representations and algorithms), which compete with each other under pressure from resource constraints, with the \"fittest\" ultimately prevailing. To investigate this Survival of the Fittest hypothesis, we conduct a case study on neural networks performing modular addition, and find that these networks' multiple circular representations at different Fourier frequencies undergo such competitive dynamics, with only a few circles surviving at the end. We find that the frequencies with high initial signals and gradients, the \"fittest,\" are more likely to survive. By increasing the embedding dimension, we also observe more surviving frequencies. Inspired by the Lotka-Volterra equations describing the dynamics between species, we find that the dynamics of the circles can be nicely characterized by a set of linear differential equations. Our results with modular addition show that it is possible to decompose complicated representations into simpler components, along with their basic interactions, to offer insight on the training dynamics of representations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17509",
        "abstract url": "https://arxiv.org/abs/2405.17509",
        "title": "Reference Neural Operators: Learning the Smooth Dependence of Solutions of PDEs on Geometric Deformations",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "For partial differential equations on domains of arbitrary shapes, existing works of neural operators attempt to learn a mapping from geometries to solutions. It often requires a large dataset of geometry-solution pairs in order to obtain a sufficiently accurate neural operator. However, for many industrial applications, e.g., engineering design optimization, it can be prohibitive to satisfy the requirement since even a single simulation may take hours or days of computation. To address this issue, we propose reference neural operators (RNO), a novel way of implementing neural operators, i.e., to learn the smooth dependence of solutions on geometric deformations. Specifically, given a reference solution, RNO can predict solutions corresponding to arbitrary deformations of the referred geometry. This approach turns out to be much more data efficient. Through extensive experiments, we show that RNO can learn the dependence across various types and different numbers of geometry objects with relatively small datasets. RNO outperforms baseline models in accuracy by a large lead and achieves up to 80% error reduction.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17525",
        "abstract url": "https://arxiv.org/abs/2405.17525",
        "title": "SmoothGNN: Smoothing-based GNN for Unsupervised Node Anomaly Detection",
        "rating": "-1.5",
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The smoothing issue leads to indistinguishable node representations, which poses a significant challenge in the field of graph learning. However, this issue also presents an opportunity to reveal underlying properties behind different types of nodes, which have been overlooked in previous studies. Through empirical and theoretical analysis of real-world node anomaly detection (NAD) datasets, we observe that anomalous and normal nodes show different patterns in the smoothing process, which can be leveraged to enhance NAD tasks. Motivated by these findings, in this paper, we propose a novel unsupervised NAD framework. Specifically, according to our theoretical analysis, we design a Smoothing Learning Component. Subsequently, we introduce a Smoothing-aware Spectral Graph Neural Network, which establishes the connection between the spectral space of graphs and the smoothing process. Additionally, we demonstrate that the Dirichlet Energy, which reflects the smoothness of a graph, can serve as coefficients for node representations across different dimensions of the spectral space. Building upon these observations and analyses, we devise a novel anomaly measure for the NAD task. Extensive experiments on 9 real-world datasets show that SmoothGNN outperforms the best rival by an average of 14.66% in AUC and 7.28% in Precision, with 75x running time speed-up, which validates the effectiveness and efficiency of our framework.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17527",
        "abstract url": "https://arxiv.org/abs/2405.17527",
        "title": "Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep models have recently emerged as a promising tool to solve partial differential equations (PDEs), known as neural PDE solvers. While neural solvers trained from either simulation data or physics-informed loss can solve the PDEs reasonably well, they are mainly restricted to a specific set of PDEs, e.g. a certain equation or a finite set of coefficients. This bottleneck limits the generalizability of neural solvers, which is widely recognized as its major advantage over numerical solvers. In this paper, we present the Universal PDE solver (Unisolver) capable of solving a wide scope of PDEs by leveraging a Transformer pre-trained on diverse data and conditioned on diverse PDEs. Instead of simply scaling up data and parameters, Unisolver stems from the theoretical analysis of the PDE-solving process. Our key finding is that a PDE solution is fundamentally under the control of a series of PDE components, e.g. equation symbols, coefficients, and initial and boundary conditions. Inspired by the mathematical structure of PDEs, we define a complete set of PDE components and correspondingly embed them as domain-wise (e.g. equation symbols) and point-wise (e.g. boundaries) conditions for Transformer PDE solvers. Integrating physical insights with recent Transformer advances, Unisolver achieves consistent state-of-the-art results on three challenging large-scale benchmarks, showing impressive gains and endowing favorable generalizability and scalability.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17566",
        "abstract url": "https://arxiv.org/abs/2405.17566",
        "title": "A deep-learning algorithm to disentangle self-interacting dark matter and AGN feedback models",
        "rating": "-1.5",
        "keywords": [
            [
                "X-ray"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Different models of dark matter can alter the distribution of mass in galaxy clusters in a variety of ways. However, so can uncertain astrophysical feedback mechanisms. Here we present a Machine Learning method that ''learns'' how the impact of dark matter self-interactions differs from that of astrophysical feedback in order to break this degeneracy and make inferences on dark matter. We train a Convolutional Neural Network on images of galaxy clusters from hydro-dynamic simulations. In the idealised case our algorithm is 80% accurate at identifying if a galaxy cluster harbours collisionless dark matter, dark matter with $\u03c3_{\\rm DM}/m = 0.1$cm$^2/$g or with $\u03c3_{DM}/m = 1$cm$^2$/g. Whilst we find adding X-ray emissivity maps does not improve the performance in differentiating collisional dark matter, it does improve the ability to disentangle different models of astrophysical feedback. We include noise to resemble data expected from Euclid and Chandra and find our model has a statistical error of < 0.01cm$^2$/g and that our algorithm is insensitive to shape measurement bias and photometric redshift errors. This method represents a new way to analyse data from upcoming telescopes that is an order of magnitude more precise and many orders faster, enabling us to explore the dark matter parameter space like never before.",
        "subjects": [
            "astro-ph.CO",
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": "Accepted Nature Astronomy"
    },
    {
        "paper id": "2405.17575",
        "abstract url": "https://arxiv.org/abs/2405.17575",
        "title": "Interpretable Prognostics with Concept Bottleneck Models",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning approaches have recently been extensively explored for the prognostics of industrial assets. However, they still suffer from a lack of interpretability, which hinders their adoption in safety-critical applications. To improve their trustworthiness, explainable AI (XAI) techniques have been applied in prognostics, primarily to quantify the importance of input variables for predicting the remaining useful life (RUL) using post-hoc attribution methods. In this work, we propose the application of Concept Bottleneck Models (CBMs), a family of inherently interpretable neural network architectures based on concept explanations, to the task of RUL prediction. Unlike attribution methods, which explain decisions in terms of low-level input features, concepts represent high-level information that is easily understandable by users. Moreover, once verified in actual applications, CBMs enable domain experts to intervene on the concept activations at test-time. We propose using the different degradation modes of an asset as intermediate concepts. Our case studies on the New Commercial Modular AeroPropulsion System Simulation (N-CMAPSS) aircraft engine dataset for RUL prediction demonstrate that the performance of CBMs can be on par or superior to black-box models, while being more interpretable, even when the available labeled concepts are limited. Code available at \\href{https://github.com/EPFL-IMOS/concept-prognostics/}{\\url{github.com/EPFL-IMOS/concept-prognostics/}}.",
        "subjects": [
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2405.17582",
        "abstract url": "https://arxiv.org/abs/2405.17582",
        "title": "Building a temperature forecasting model for the city with the regression neural network (RNN)",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, a study by environmental organizations in the world and Vietnam shows that weather change is quite complex. global warming has become a serious problem in the modern world, which is a concern for scientists. last century, it was difficult to forecast the weather due to missing weather monitoring stations and technological limitations. this made it hard to collect data for building predictive models to make accurate simulations. in Vietnam, research on weather forecast models is a recent development, having only begun around 2000. along with advancements in computer science, mathematical models are being built and applied with machine learning techniques to create more accurate and reliable predictive models. this article will summarize the research and solutions for applying recurrent neural networks to forecast urban temperatures.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2405.17607",
        "abstract url": "https://arxiv.org/abs/2405.17607",
        "title": "Advancing Cultural Inclusivity: Optimizing Embedding Spaces for Balanced Music Recommendations",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Popularity bias in music recommendation systems -- where artists and tracks with the highest listen counts are recommended more often -- can also propagate biases along demographic and cultural axes. In this work, we identify these biases in recommendations for artists from underrepresented cultural groups in prototype-based matrix factorization methods. Unlike traditional matrix factorization methods, prototype-based approaches are interpretable. This allows us to directly link the observed bias in recommendations for minority artists (the effect) to specific properties of the embedding space (the cause). We mitigate popularity bias in music recommendation through capturing both users' and songs' cultural nuances in the embedding space. To address these challenges while maintaining recommendation quality, we propose two novel enhancements to the embedding space: i) we propose an approach to filter-out the irrelevant prototypes used to represent each user and item to improve generalizability, and ii) we introduce regularization techniques to reinforce a more uniform distribution of prototypes within the embedding space. Our results demonstrate significant improvements in reducing popularity bias and enhancing demographic and cultural fairness in music recommendations while achieving competitive -- if not better -- overall performance.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17631",
        "abstract url": "https://arxiv.org/abs/2405.17631",
        "title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments",
        "rating": "-1.5",
        "keywords": [
            [
                "BioDiscoveryAgent"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. Here, we develop BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function. Moreover, BioDiscoveryAgent achieves an average of 18% improvement in detecting desired phenotypes across five datasets, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' capabilities.",
        "subjects": [
            "cs.AI",
            "cs.CE",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17656",
        "abstract url": "https://arxiv.org/abs/2405.17656",
        "title": "Alignment is Key for Applying Diffusion Models to Retrosynthesis",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Retrosynthesis, the task of identifying precursors for a given molecule, can be naturally framed as a conditional graph generation task. Diffusion models are a particularly promising modelling approach, enabling post-hoc conditioning and trading off quality for speed during generation. We show mathematically that permutation equivariant denoisers severely limit the expressiveness of graph diffusion models and thus their adaptation to retrosynthesis. To address this limitation, we relax the equivariance requirement such that it only applies to aligned permutations of the conditioning and the generated graphs obtained through atom mapping. Our new denoiser achieves the highest top-$1$ accuracy ($54.7$\\%) across template-free and template-based methods on USPTO-50k. We also demonstrate the ability for flexible post-training conditioning and good sample quality with small diffusion step counts, highlighting the potential for interactive applications and additional controls for multi-step planning.",
        "subjects": [
            "cs.LG",
            "q-bio.QM"
        ],
        "comment": "28 pages, 9 figures"
    },
    {
        "paper id": "2405.17676",
        "abstract url": "https://arxiv.org/abs/2405.17676",
        "title": "Utilising a Quantum Hybrid Solver for Bi-objective Quadratic Assignment Problems",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The intersection between quantum computing and optimisation has been an area of interest in recent years. There have been numerous studies exploring the application of quantum and quantum-hybrid solvers to various optimisation problems. This work explores scalarisation methods within the context of solving the bi-objective quadratic assignment problem using a quantum-hybrid solver. We show results that are consistent with previous research on a different Ising machine.",
        "subjects": [
            "quant-ph",
            "cs.AI"
        ],
        "comment": "4 pages"
    },
    {
        "paper id": "2405.17696",
        "abstract url": "https://arxiv.org/abs/2405.17696",
        "title": "Physics-guided Full Waveform Inversion using Encoder-Solver Convolutional Neural Networks",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Full Waveform Inversion (FWI) is an inverse problem for estimating the wave velocity distribution in a given domain, based on observed data on the boundaries. The inversion is computationally demanding because we are required to solve multiple forward problems, either in time or frequency domains, to simulate data that are then iteratively fitted to the observed data. We consider FWI in the frequency domain, where the Helmholtz equation is used as a forward model, and its repeated solution is the main computational bottleneck of the inversion process. To ease this cost, we integrate a learning process of an encoder-solver preconditioner that is based on convolutional neural networks (CNNs). The encoder-solver is trained to effectively precondition the discretized Helmholtz operator given velocity medium parameters. Then, by re-training the CNN between the iterations of the optimization process, the encoder-solver is adapted to the iteratively evolving velocity medium as part of the inversion. Without retraining, the performance of the solver deteriorates as the medium changes. Using our light retraining procedures, we obtain the forward simulations effectively throughout the process. We demonstrate our approach to solving FWI problems using 2D geophysical models with high-frequency data.",
        "subjects": [
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": "24 pages, 9 figures"
    },
    {
        "paper id": "2405.17734",
        "abstract url": "https://arxiv.org/abs/2405.17734",
        "title": "Towards Efficient Disaster Response via Cost-effective Unbiased Class Rate Estimation through Neyman Allocation Stratified Sampling Active Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "remote sensing",
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the rapid development of earth observation technology, we have entered an era of massively available satellite remote-sensing data. However, a large amount of satellite remote sensing data lacks a label or the label cost is too high to hinder the potential of AI technology mining satellite data. Especially in such an emergency response scenario that uses satellite data to evaluate the degree of disaster damage. Disaster damage assessment encountered bottlenecks due to excessive focus on the damage of a certain building in a specific geographical space or a certain area on a larger scale. In fact, in the early days of disaster emergency response, government departments were more concerned about the overall damage rate of the disaster area instead of single-building damage, because this helps the government decide the level of emergency response. We present an innovative algorithm that constructs Neyman stratified random sampling trees for binary classification and extends this approach to multiclass problems. Through extensive experimentation on various datasets and model structures, our findings demonstrate that our method surpasses both passive and conventional active learning techniques in terms of class rate estimation and model enhancement with only 30\\%-60\\% of the annotation cost of simple sampling. It effectively addresses the 'sampling bias' challenge in traditional active learning strategies and mitigates the 'cold start' dilemma. The efficacy of our approach is further substantiated through application to disaster evaluation tasks using Xview2 Satellite imagery, showcasing its practical utility in real-world contexts.",
        "subjects": [
            "cs.LG",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17746",
        "abstract url": "https://arxiv.org/abs/2405.17746",
        "title": "Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective",
        "rating": "-1.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. Most of the existing defense methods rely on defined rules and focus on neuron's local properties, ignoring the exploration and optimization of pruning policies. To address this gap, we propose an Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN) and Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP first models the target DNN as graphs based on neuron connectivity, and then uses GNN-based RL agents to learn graph embeddings and find a suitable pruning policy. To the best of our knowledge, this is the first attempt to employ GNN and RL for optimizing pruning policies in the field of backdoor defense. Experiments show, with a small amount of clean data, ONP can effectively prune the backdoor neurons implanted by a set of backdoor attacks at the cost of negligible performance degradation, achieving a new state-of-the-art performance for backdoor mitigation.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17766",
        "abstract url": "https://arxiv.org/abs/2405.17766",
        "title": "SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals",
        "rating": "-1.5",
        "keywords": [
            [
                "cardiac",
                "physiological"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Sleep is a complex physiological process evaluated through various modalities recording electrical brain, cardiac, and respiratory activities. We curate a large polysomnography dataset from over 14,000 participants comprising over 100,000 hours of multi-modal sleep recordings. Leveraging this extensive dataset, we developed SleepFM, the first multi-modal foundation model for sleep analysis. We show that a novel leave-one-out approach for contrastive learning significantly improves downstream task performance compared to representations from standard pairwise contrastive learning. A logistic regression model trained on SleepFM's learned embeddings outperforms an end-to-end trained convolutional neural network (CNN) on sleep stage classification (macro AUROC 0.88 vs 0.72 and macro AUPRC 0.72 vs 0.48) and sleep disordered breathing detection (AUROC 0.85 vs 0.69 and AUPRC 0.77 vs 0.61). Notably, the learned embeddings achieve 48% top-1 average accuracy in retrieving the corresponding recording clips of other modalities from 90,000 candidates. This work demonstrates the value of holistic multi-modal sleep modeling to fully capture the richness of sleep recordings. SleepFM is open source and available at https://github.com/rthapa84/sleepfm-codebase.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17802",
        "abstract url": "https://arxiv.org/abs/2405.17802",
        "title": "Multi-level Interaction Modeling for Protein Mutational Effect Prediction",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Protein-protein interactions are central mediators in many biological processes. Accurately predicting the effects of mutations on interactions is crucial for guiding the modulation of these interactions, thereby playing a significant role in therapeutic development and drug discovery. Mutations generally affect interactions hierarchically across three levels: mutated residues exhibit different sidechain conformations, which lead to changes in the backbone conformation, eventually affecting the binding affinity between proteins. However, existing methods typically focus only on sidechain-level interaction modeling, resulting in suboptimal predictions. In this work, we propose a self-supervised multi-level pre-training framework, ProMIM, to fully capture all three levels of interactions with well-designed pretraining objectives. Experiments show ProMIM outperforms all the baselines on the standard benchmark, especially on mutations where significant changes in backbone conformations may occur. In addition, leading results from zero-shot evaluations for SARS-CoV-2 mutational effect prediction and antibody optimization underscore the potential of ProMIM as a powerful next-generation tool for developing novel therapeutic approaches and new drugs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16851",
        "abstract url": "https://arxiv.org/abs/2405.16851",
        "title": "Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "biologically"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation. A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally. This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning. It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation. Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks. The spiking model is theoretically estimated to achieve $20\\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning. The code is available at https://github.com/pkuxmq/GRSNN.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted by ICML 2024"
    },
    {
        "paper id": "2405.16868",
        "abstract url": "https://arxiv.org/abs/2405.16868",
        "title": "RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs. However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available. In reality, cameras may be highly noisy, obscured or even failed during the collaboration. In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vectors for foregrounds with appropriate positions. To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios. Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity settings. Our code and datasets will be available soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16871",
        "abstract url": "https://arxiv.org/abs/2405.16871",
        "title": "Multi-Behavior Generative Recommendation",
        "rating": "-2",
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "Multi-behavior sequential recommendation (MBSR) aims to incorporate behavior types of interactions for better recommendations. Existing approaches focus on the next-item prediction objective, neglecting the value of integrating the target behavior type into the learning objective. In this paper, we propose MBGen, a novel Multi-Behavior sequential Generative recommendation framework. We formulate the MBSR task into a consecutive two-step process: (1) given item sequences, MBGen first predicts the next behavior type to frame the user intention, (2) given item sequences and a target behavior type, MBGen then predicts the next items. To model such a two-step process, we tokenize both behaviors and items into tokens and construct one single token sequence with both behaviors and items placed interleaved. Furthermore, MBGen learns to autoregressively generate the next behavior and item tokens in a unified generative recommendation paradigm, naturally enabling a multi-task capability. Additionally, we exploit the heterogeneous nature of token sequences in the generative recommendation and propose a position-routed sparse architecture to efficiently and effectively scale up models. Extensive experiments on public datasets demonstrate that MBGen significantly outperforms existing MBSR models across multiple tasks.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16895",
        "abstract url": "https://arxiv.org/abs/2405.16895",
        "title": "Anonymization Prompt Learning for Facial Privacy-Preserving Text-to-Image Generation",
        "rating": "-2",
        "keywords": [
            [
                "diffusion",
                "Text-to-Image"
            ],
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-to-image diffusion models, such as Stable Diffusion, generate highly realistic images from text descriptions. However, the generation of certain content at such high quality raises concerns. A prominent issue is the accurate depiction of identifiable facial images, which could lead to malicious deepfake generation and privacy violations. In this paper, we propose Anonymization Prompt Learning (APL) to address this problem. Specifically, we train a learnable prompt prefix for text-to-image diffusion models, which forces the model to generate anonymized facial identities, even when prompted to produce images of specific individuals. Extensive quantitative and qualitative experiments demonstrate the successful anonymization performance of APL, which anonymizes any specific individuals without compromising the quality of non-identity-specific image generation. Furthermore, we reveal the plug-and-play property of the learned prompt prefix, enabling its effective application across different pretrained text-to-image models for transferrable privacy and security protection against the risks of deepfakes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "15 pages, 8 figures and 5 tables"
    },
    {
        "paper id": "2405.16965",
        "abstract url": "https://arxiv.org/abs/2405.16965",
        "title": "Timeliness of Status Update System: The Effect of Parallel Transmission Using Heterogeneous Updating Devices",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Timely status updating is the premise of emerging interaction-based applications in the Internet of Things (IoT). Using redundant devices to update the status of interest is a promising method to improve the timeliness of information. However, parallel status updating leads to out-of-order arrivals at the monitor, significantly challenging timeliness analysis. This work studies the Age of Information (AoI) of a multi-queue status update system where multiple devices monitor the same physical process. Specifically, two systems are considered: the Basic System, which only has type-1 devices that are ad hoc devices located close to the source, and the Hybrid System, which contains additional type-2 devices that are infrastructure-based devices located in fixed points compared to the Basic System. Using the Stochastic Hybrid Systems (SHS) framework, a mathematical model that combines discrete and continuous dynamics, we derive the expressions of the average AoI of the considered two systems in closed form. Numerical results verify the accuracy of the analysis. It is shown that when the number and parameters of the type-1 devices/type-2 devices are fixed, the logarithm of average AoI will linearly decrease with the logarithm of the total arrival rate of type-2 devices or that of the number of type-1 devices under specific condition. It has also been demonstrated that the proposed systems can significantly outperform the FCFS M/M/N status update system.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16988",
        "abstract url": "https://arxiv.org/abs/2405.16988",
        "title": "An experimental study of the response time in an edge-cloud continuum with ClusterLink",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "In this paper, we conduct an experimental study to provide a general sense of the application response time implications that inter-cluster communication experiences at the edge at the example of a specific IoT-edge-cloud contiuum solution from the EU Project ICOS called ClusterLink. We create an environment to emulate different networking topologies that include multiple cloud or edge sites scenarios, and conduct a set of tests to compare the application response times via ClusterLink to direct communications in relation to node distances and request/response payload size. Our results show that, in an edge context, ClusterLink does not introduce a significant processing overhead to the communication for small payloads as compared to cloud. For higher payloads and on comparably more aged consumer hardware, ClusterLink version 0.2 introduces communication overhead relative to the delay experienced on the link.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Submitted to IECCONT workshop co-hosted with Euro-Par 2024 https://2024.euro-par.org/workshops/workshops/"
    },
    {
        "paper id": "2405.17007",
        "abstract url": "https://arxiv.org/abs/2405.17007",
        "title": "Waveforms for Computing Over the Air",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Over-the-air computation (AirComp) leverages the signal-superposition characteristic of wireless multiple access channels to perform mathematical computations. Initially introduced to enhance communication reliability in interference channels and wireless sensor networks, AirComp has more recently found applications in task-oriented communications, namely, for wireless distributed learning and in wireless control systems. Its adoption aims to address latency challenges arising from an increased number of edge devices or IoT devices accessing the constrained wireless spectrum. This paper focuses on the physical layer of these systems, specifically on the waveform and the signal processing aspects at the transmitter and receiver to meet the challenges that AirComp presents within the different contexts and use cases.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Feature article submitted at the IEEE Signal Processing Magazine"
    },
    {
        "paper id": "2405.17008",
        "abstract url": "https://arxiv.org/abs/2405.17008",
        "title": "Quantum-safe Edge Applications: How to Secure Computation in Distributed Computing Systems",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "The advent of distributed computing systems will offer great flexibility for application workloads, while also imposing more attention to security, where the future advent and adoption of quantum technology can introduce new security threats. For this reason, the Multi-access Edge Computing (MEC) working group at ETSI has recently started delving into security aspects, especially motivated by the upcoming reality of the MEC federation, which involves services made of application instances belonging to different systems (thus, different trust domains). On the other side, Quantum Key Distribution (QKD) can help strengthen the level of security by enabling the exchange of secure keys through an unconditionally secure protocol, e.g., to secure communication between REST clients and servers in distributed computing systems at the edge. In this paper, we propose a technical solution to achieve this goal, building on standard specifications, namely ETSI MEC and ETSI QKD, and discussing the gaps and limitations of current technology, which hamper full-fledged in-field deployment and mass adoption. Furthermore, we provide our look-ahead view on the future of secure distributed computing through the enticing option of federating edge computing domains.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Accepted for presentation at IEEE PIMRC 2024"
    },
    {
        "paper id": "2405.17024",
        "abstract url": "https://arxiv.org/abs/2405.17024",
        "title": "Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals",
        "rating": "-2",
        "keywords": [
            [
                "EEG"
            ]
        ],
        "abstract": "Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc. Since these EEG data were usually collected with well-designed paradigms in labs, the reliability and robustness of the corresponding decoding methods were doubted by some researchers, and they argued that such decoding accuracy was overestimated due to the inherent temporal autocorrelation of EEG signals. However, the coupling between the stimulus-driven neural responses and the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth. Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation. In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework. EEG data were recorded from watermelons to remove stimulus-driven neural responses. Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted. The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets. Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features. These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal autocorrelation.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17127",
        "abstract url": "https://arxiv.org/abs/2405.17127",
        "title": "Motion Primitives Planning For Center-Articulated Vehicles",
        "rating": "-2",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "robot",
                "navigation"
            ]
        ],
        "abstract": "Autonomous navigation across unstructured terrains, including forests and construction areas, faces unique challenges due to intricate obstacles and the element of the unknown. Lacking pre-existing maps, these scenarios necessitate a motion planning approach that combines agility with efficiency. Critically, it must also incorporate the robot's kinematic constraints to navigate more effectively through complex environments. This work introduces a novel planning method for center-articulated vehicles (CAV), leveraging motion primitives within a receding horizon planning framework using onboard sensing. The approach commences with the offline creation of motion primitives, generated through forward simulations that reflect the distinct kinematic model of center-articulated vehicles. These primitives undergo evaluation through a heuristic-based scoring function, facilitating the selection of the most suitable path for real-time navigation. To augment this planning process, we develop a pose-stabilizing controller, tailored to the kinematic specifications of center-articulated vehicles. During experiments, our method demonstrates a $67\\%$ improvement in SPL (Success Rate weighted by Path Length) performance over existing strategies. Furthermore, its efficacy was validated through real-world experiments conducted with a tree harvester vehicle - SAHA.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 9 figures"
    },
    {
        "paper id": "2405.17138",
        "abstract url": "https://arxiv.org/abs/2405.17138",
        "title": "CMOSS: A Reliable, Motif-based Columnar Molecular Storage System",
        "rating": "-2",
        "keywords": [
            [
                "DNA"
            ]
        ],
        "abstract": "The surge in demand for cost-effective, durable long-term archival media, coupled with density limitations of contemporary magnetic media, has resulted in synthetic DNA emerging as a promising new alternative. Despite its benefits, storing data on DNA poses several challenges as the technology used for reading/writing data and achieving random access on DNA are highly error prone. In order to deal with such errors, it is important to design efficient pipelines that can carefully use redundancy to mask errors without amplifying overall cost. In this work, we present Columnar MOlecular Storage System (CMOSS), a novel, end-to-end DNA storage pipeline that can provide error-tolerant data storage at low read/write costs. CMOSS differs from SOTA on three fronts (i) a motif-based, vertical layout in contrast to nucleotide-based horizontal layout used by SOTA, (ii) merged consensus calling and decoding enabled by the vertical layout, and (iii) a flexible, fixed-size, block-based data organization for random access over DNA storage in contrast to the variable-sized, object-based access used by SOTA. Using an in-depth evaluation via simulation studies and real wet-lab experiments, we demonstrate the benefits of various CMOSS design choices. We make the entire pipeline together with the read datasets openly available to the community for faithful reproduction and furthering research.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17140",
        "abstract url": "https://arxiv.org/abs/2405.17140",
        "title": "SDL-MVS: View Space and Depth Deformable Learning Paradigm for Multi-View Stereo Reconstruction in Remote Sensing",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Research on multi-view stereo based on remote sensing images has promoted the development of large-scale urban 3D reconstruction. However, remote sensing multi-view image data suffers from the problems of occlusion and uneven brightness between views during acquisition, which leads to the problem of blurred details in depth estimation. To solve the above problem, we re-examine the deformable learning method in the Multi-View Stereo task and propose a novel paradigm based on view Space and Depth deformable Learning (SDL-MVS), aiming to learn deformable interactions of features in different view spaces and deformably model the depth ranges and intervals to enable high accurate depth estimation. Specifically, to solve the problem of view noise caused by occlusion and uneven brightness, we propose a Progressive Space deformable Sampling (PSS) mechanism, which performs deformable learning of sampling points in the 3D frustum space and the 2D image space in a progressive manner to embed source features to the reference feature adaptively. To further optimize the depth, we introduce Depth Hypothesis deformable Discretization (DHD), which achieves precise positioning of the depth prior by adaptively adjusting the depth range hypothesis and performing deformable discretization of the depth interval hypothesis. Finally, our SDL-MVS achieves explicit modeling of occlusion and uneven brightness faced in multi-view stereo through the deformable learning paradigm of view space and depth, achieving accurate multi-view depth estimation. Extensive experiments on LuoJia-MVS and WHU datasets show that our SDL-MVS reaches state-of-the-art performance. It is worth noting that our SDL-MVS achieves an MAE error of 0.086, an accuracy of 98.9% for <0.6m, and 98.9% for <3-interval on the LuoJia-MVS dataset under the premise of three views as input.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17167",
        "abstract url": "https://arxiv.org/abs/2405.17167",
        "title": "Partitioned Hankel-based Diffusion Models for Few-shot Low-dose CT Reconstruction",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "CT",
                "clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Low-dose computed tomography (LDCT) plays a vital role in clinical applications by mitigating radiation risks. Nevertheless, reducing radiation doses significantly degrades image quality. Concurrently, common deep learning methods demand extensive data, posing concerns about privacy, cost, and time constraints. Consequently, we propose a few-shot low-dose CT reconstruction method using Partitioned Hankel-based Diffusion (PHD) models. During the prior learning stage, the projection data is first transformed into multiple partitioned Hankel matrices. Structured tensors are then extracted from these matrices to facilitate prior learning through multiple diffusion models. In the iterative reconstruction stage, an iterative stochastic differential equation solver is employed along with data consistency constraints to update the acquired projection data. Furthermore, penalized weighted least-squares and total variation techniques are introduced to enhance the resulting image quality. The results approximate those of normal-dose counterparts, validating PHD model as an effective and practical model for reducing artifacts and noise while preserving image quality.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17262",
        "abstract url": "https://arxiv.org/abs/2405.17262",
        "title": "Deep Feature Gaussian Processes for Single-Scene Aerosol Optical Depth Reconstruction",
        "rating": "-2",
        "keywords": [
            [
                "Depth"
            ],
            [
                "Remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Remote sensing data provide a low-cost solution for large-scale monitoring of air pollution via the retrieval of aerosol optical depth (AOD), but is often limited by cloud contamination. Existing methods for AOD reconstruction rely on temporal information. However, for remote sensing data at high spatial resolution, multi-temporal observations are often unavailable. In this letter, we take advantage of deep representation learning from convolutional neural networks and propose Deep Feature Gaussian Processes (DFGP) for single-scene AOD reconstruction. By using deep learning, we transform the variables to a feature space with better explainable power. By using Gaussian processes, we explicitly consider the correlation between observed AOD and missing AOD in spatial and feature domains. Experiments on two AOD datasets with real-world cloud patterns showed that the proposed method outperformed deep CNN and random forest, achieving R$^2$ of 0.7431 on MODIS AOD and R$^2$ of 0.9211 on EMIT AOD, compared to deep CNN's R$^2$ of 0.6507 and R$^2$ of 0.8619. The proposed methods increased R$^2$ by over 0.35 compared to the popular random forest in AOD reconstruction. The data and code used in this study are available at \\url{https://skrisliu.com/dfgp}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to IEEE GEOSCIENCE AND REMOTE SENSING LETTERS"
    },
    {
        "paper id": "2405.17278",
        "abstract url": "https://arxiv.org/abs/2405.17278",
        "title": "EF-Calib: Spatiotemporal Calibration of Event- and Frame-Based Cameras Using Continuous-Time Trajectories",
        "rating": "-2",
        "keywords": [
            [
                "Event camera"
            ],
            [
                "bio-inspired"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Event camera, a bio-inspired asynchronous triggered camera, offers promising prospects for fusion with frame-based cameras owing to its low latency and high dynamic range. However, calibrating stereo vision systems that incorporate both event and frame-based cameras remains a significant challenge. In this letter, we present EF-Calib, a spatiotemporal calibration framework for event- and frame-based cameras using continuous-time trajectories. A novel calibration pattern applicable to both camera types and the corresponding event recognition algorithm is proposed. Leveraging the asynchronous nature of events, a derivable piece-wise B-spline to represent camera pose continuously is introduced, enabling calibration for intrinsic parameters, extrinsic parameters, and time offset, with analytical Jacobians provided. Various experiments are carried out to evaluate the calibration performance of EF-Calib, including calibration experiments for intrinsic parameters, extrinsic parameters, and time offset. Experimental results show that EF-Calib achieves the most accurate intrinsic parameters compared to current SOTA, the close accuracy of the extrinsic parameters compared to the frame-based results, and accurate time offset estimation. EF-Calib provides a convenient and accurate toolbox for calibrating the system that fuses events and frames. The code of this paper will also be open-sourced at: https://github.com/wsakobe/EF-Calib.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2405.17296",
        "abstract url": "https://arxiv.org/abs/2405.17296",
        "title": "Coupling Light with Matter for Identifying Dominant Subnetworks",
        "rating": "-2",
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "We present a novel light-matter platform that uses complex-valued oscillator networks, a form of physical neural networks, to identify dominant subnetworks and uncover indirect correlations within larger networks. This approach offers significant advantages, including low energy consumption, high processing speed, and the immediate identification of co- and counter-regulated nodes without post-processing. The effectiveness of this approach is demonstrated through its application to biological networks, and we also propose its applicability to a wide range of other network types.",
        "subjects": [
            "cond-mat.dis-nn",
            "cond-mat.other",
            "cs.ET",
            "physics.optics",
            "quant-ph"
        ],
        "comment": "8 pages, 12 figures"
    },
    {
        "paper id": "2405.17363",
        "abstract url": "https://arxiv.org/abs/2405.17363",
        "title": "Optimized thread-block arrangement in a GPU implementation of a linear solver for atmospheric chemistry mechanisms",
        "rating": "-2",
        "keywords": [
            [
                "chemistry",
                "chemical"
            ]
        ],
        "abstract": "Earth system models (ESM) demand significant hardware resources and energy consumption to solve atmospheric chemistry processes. Recent studies have shown improved performance from running these models on GPU accelerators. Nonetheless, there is room for improvement in exploiting even more GPU resources. This study proposes an optimized distribution of the chemical solver's computational load on the GPU, named Block-cells. Additionally, we evaluate different configurations for distributing the computational load in an NVIDIA GPU. We use the linear solver from the Chemistry Across Multiple Phases (CAMP) framework as our test bed. An intermediate-complexity chemical mechanism under typical atmospheric conditions is used. Results demonstrate a 35x speedup compared to the single-CPU thread reference case. Even using the full resources of the node (40 physical cores) on the reference case, the Block-cells version outperforms them by 50%. The Block-cells approach shows promise in alleviating the computational burden of chemical solvers on GPU architectures.",
        "subjects": [
            "cs.AR",
            "cs.DC",
            "cs.PF",
            "cs.SE"
        ],
        "comment": "Accepted manuscript"
    },
    {
        "paper id": "2405.17387",
        "abstract url": "https://arxiv.org/abs/2405.17387",
        "title": "Batteryless BLE and Light-based IoT Sensor Nodes for Reliable Environmental Sensing",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "The sustainable design of Internet of Things (IoT) networks encompasses considerations related to energy efficiency and autonomy as well as considerations related to reliable communications, ensuring no energy is wasted on undelivered data. Under these considerations, this work proposes the design and implementation of energy-efficient Bluetooth Low Energy (BLE) and Light-based IoT (LIoT) batteryless IoT sensor nodes powered by an indoor light Energy Harvesting Unit (EHU). Our design intends to integrate these nodes into a sensing network to improve its reliability by combining both technologies and taking advantage of their features. The nodes incorporate state-of-the-art components, such as low-power sensors and efficient System-on-Chips (SoCs). Moreover, we design a strategy for adaptive switching between active and sleep cycles as a function of the available energy, allowing the IoT nodes to continuously operate without batteries. Our results show that by adapting the duty cycle of the BLE and LIoT nodes depending on the environment's light intensity, we can ensure a continuous and reliable node operation. In particular, measurements show that our proposed BLE and LIoT node designs are able to communicate with an IoT gateway in a bidirectional way, every 19.3 and 624.6 seconds, respectively, in an energy-autonomous and reliable manner.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "6 pages, 9 figures, accepted for publication in the IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC 2024), Valencia, Spain"
    },
    {
        "paper id": "2405.17398",
        "abstract url": "https://arxiv.org/abs/2405.17398",
        "title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
        "rating": "-2",
        "keywords": [
            [
                "autonomous driving",
                "trajectory"
            ],
            [
                "diagnosis"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Code and model: https://github.com/OpenDriveLab/Vista, video demos: https://vista-demo.github.io"
    },
    {
        "paper id": "2405.17422",
        "abstract url": "https://arxiv.org/abs/2405.17422",
        "title": "Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Synthesis"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "3D object detection aims to recover the 3D information of concerning objects and serves as the fundamental task of autonomous driving perception. Its performance greatly depends on the scale of labeled training data, yet it is costly to obtain high-quality annotations for point cloud data. While conventional methods focus on generating pseudo-labels for unlabeled samples as supplements for training, the structural nature of 3D point cloud data facilitates the composition of objects and backgrounds to synthesize realistic scenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS) method to generate adaptive synthetic scenes to improve the generalization of the detection models. We obtain pseudo-labels for unlabeled objects and generate diverse scenes with different compositions of objects and backgrounds. As the scene synthesis is sensitive to the quality of pseudo-labels, we further propose a hardness-aware strategy to reduce the effect of low-quality pseudo-labels and maintain a dynamic pseudo-database to ensure the diversity and quality of synthetic scenes. Extensive experimental results on the widely used KITTI and Waymo datasets demonstrate the superiority of the proposed HASS method, which outperforms existing semi-supervised learning methods on 3D object detection. Code: https://github.com/wzzheng/HASS.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Code is available at: https://github.com/wzzheng/HASS"
    },
    {
        "paper id": "2405.17518",
        "abstract url": "https://arxiv.org/abs/2405.17518",
        "title": "Assessment of Left Atrium Motion Deformation Through Full Cardiac Cycle",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "biomarkers",
                "clinical",
                "Cardiac"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Unlike Right Atrium (RA), Left Atrium (LA) presents distinctive challenges, including much thinner myocardial walls, complex and irregular morphology, as well as diversity in individual's structure, making off-the-shelf methods designed for the Left Ventricle (LV) may not work in the context of the left atrium. To overcome aforementioned challenges, we are the first to present comprehensive technical workflow designed for 4D registration modeling to automatically analyze LA motion using high-resolution 3D Cine MR images. We integrate segmentation network and 4D registration process to precisely delineate LA segmentation throughout the full cardiac cycle. Additionally, an image 4D registration network is employed to extract LA displacement vector fields (DVFs). Our findings show the potential of proposed end to end framework in providing clinicians with novel regional biomarkers for left atrium motion tracking and deformation, carrying significant clinical implications.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17616",
        "abstract url": "https://arxiv.org/abs/2405.17616",
        "title": "Design of a Rectangular Linear Microstrip Patch Antenna Array for 5G Communication",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "This paper presents the design and characterization of a rectangular microstrip patch antenna array optimized for operation within the Ku-band frequency range. The antenna array is impedance-matched to 50 Ohms and utilizes a microstrip line feeding mechanism for excitation. The design maintains compact dimensions, with the overall antenna occupying an area of 29.5x7 mm. The antenna structure is modelled on an R03003 substrate material, featuring a dielectric constant of 3, a low-loss tangent of 0.0009, and a thickness of 1.574 mm. The substrate is backed by a conducting ground plane, and the array consists of six radiating patch elements positioned on top. Evaluation of the designed antenna array reveals a resonant frequency of 18GHz, with a -10 dB impedance bandwidth extending over 700MHz. The antenna demonstrates a high gain of 7.51dBi, making it well-suited for applications in 5G and future communication systems. Its compact form factor, cost-effectiveness, and broad impedance and radiation coverage further underscore its potential in these domains.",
        "subjects": [
            "eess.SY",
            "eess.SP",
            "physics.app-ph"
        ],
        "comment": "4 pages, 5 figures, 2 tables"
    },
    {
        "paper id": "2405.17635",
        "abstract url": "https://arxiv.org/abs/2405.17635",
        "title": "Enhancing Resiliency of Integrated Space-Air-Ground-Sea Networks with Renewable Energies: A Use Case After the 2023 T\u00fcrkiye Earthquake",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Natural disasters can have catastrophic consequences, a poignant example is the series of $7.7$ and $7.6$ magnitude earthquakes that devastated T\u00fcrkiye on February 6, 2023. To limit the damage, it is essential to maintain the communications infrastructure to ensure individuals impacted by the disaster can receive critical information. The disastrous earthquakes in T\u00fcrkiye have revealed the importance of considering communications and energy solutions together to build resilient and sustainable infrastructure. Thus, this paper proposes an integrated space-air-ground-sea network architecture that utilizes various communications and energy-enabling technologies. This study aims to contribute to the development of robust and sustainable disaster-response frameworks. In light of the T\u00fcrkiye earthquakes, two methods for network management are proposed: the first aims to ensure sustainability in the pre-disaster phase and the second aims to maintain communications during the in-disaster phase. In these frameworks, communications technologies such as High Altitude Platform Station(s)(HAPS), which are among the key enablers to unlock the potential of 6G networks, and energy technologies such as Renewable Energy Sources (RES), Battery Energy Storage Systems (BESSs), and Electric Vehicles (EVs) have been used as the prominent technologies. By simulating a case study, we demonstrate the performance of a proposed framework for providing network resiliency. The paper concludes with potential challenges and future directions to achieve a disaster-resilient network architecture solution.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "accepted in IEEE COMMAG"
    },
    {
        "paper id": "2405.17650",
        "abstract url": "https://arxiv.org/abs/2405.17650",
        "title": "An Analysis of Performance Bottlenecks in MRI Pre-Processing",
        "rating": "-2",
        "keywords": [
            [
                "MRI",
                "clinical"
            ]
        ],
        "abstract": "Magnetic Resonance Image (MRI) pre-processing is a critical step for neuroimaging analysis. However, the computational cost of MRI pre-processing pipelines is a major bottleneck for large cohort studies and some clinical applications. While High-Performance Computing (HPC) and, more recently, Deep Learning have been adopted to accelerate the computations, these techniques require costly hardware and are not accessible to all researchers. Therefore, it is important to understand the performance bottlenecks of MRI pre-processing pipelines to improve their performance. Using Intel VTune profiler, we characterized the bottlenecks of several commonly used MRI-preprocessing pipelines from the ANTs, FSL, and FreeSurfer toolboxes. We found that few functions contributed to most of the CPU time, and that linear interpolation was the largest contributor. Data access was also a substantial bottleneck. We identified a bug in the ITK library that impacts the performance of ANTs pipeline in single-precision and a potential issue with the OpenMP scaling in FreeSurfer recon-all. Our results provide a reference for future efforts to optimize MRI pre-processing pipelines.",
        "subjects": [
            "cs.PF"
        ],
        "comment": "8 pages, 7 figures, 3 tables, 1 listing"
    },
    {
        "paper id": "2405.17659",
        "abstract url": "https://arxiv.org/abs/2405.17659",
        "title": "Enhancing Global Sensitivity and Uncertainty Quantification in Medical Image Reconstruction with Monte Carlo Arbitrary-Masked Mamba",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning has been extensively applied in medical image reconstruction, where Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) represent the predominant paradigms, each possessing distinct advantages and inherent limitations: CNNs exhibit linear complexity with local sensitivity, whereas ViTs demonstrate quadratic complexity with global sensitivity. The emerging Mamba has shown superiority in learning visual representation, which combines the advantages of linear scalability and global sensitivity. In this study, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with wavelet decomposition for joint medical image reconstruction and uncertainty estimation. A novel Arbitrary Scan Masking (ASM) mechanism ``masks out'' redundant information to introduce randomness for further uncertainty estimation. Compared to the commonly used Monte Carlo (MC) dropout, our proposed MC-ASM provides an uncertainty map without the need for hyperparameter tuning and mitigates the performance drop typically observed when applying dropout to low-level tasks. For further texture preservation and better perceptual quality, we employ the wavelet transformation into MambaMIR and explore its variant based on the Generative Adversarial Network, namely MambaMIR-GAN. Comprehensive experiments have been conducted for multiple representative medical image reconstruction tasks, demonstrating that the proposed MambaMIR and MambaMIR-GAN outperform other baseline and state-of-the-art methods in different reconstruction tasks, where MambaMIR achieves the best reconstruction fidelity and MambaMIR-GAN has the best perceptual quality. In addition, our MC-ASM provides uncertainty maps as an additional tool for clinicians, while mitigating the typical performance drop caused by the commonly used dropout.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17673",
        "abstract url": "https://arxiv.org/abs/2405.17673",
        "title": "Fast Samplers for Inverse Problems in Iterative Refinement Models",
        "rating": "-2",
        "keywords": [
            [
                "diffusion",
                "inpainting",
                "super-resolution"
            ],
            [
                "image restoration"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving inverse problems, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only pre-trained diffusion or flow-matching models. We present Conditional Conjugate Integrators, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method's performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like 4$\\times$ super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as 5 conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code and models will be publicly available at https://github.com/mandt-lab/CI2RM.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17705",
        "abstract url": "https://arxiv.org/abs/2405.17705",
        "title": "DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam Videos",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "synthesis"
            ],
            [
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present DC-Gaussian, a new method for generating novel views from in-vehicle dash cam videos. While neural rendering techniques have made significant strides in driving scenarios, existing methods are primarily designed for videos collected by autonomous vehicles. However, these videos are limited in both quantity and diversity compared to dash cam videos, which are more widely used across various types of vehicles and capture a broader range of scenarios. Dash cam videos often suffer from severe obstructions such as reflections and occlusions on the windshields, which significantly impede the application of neural rendering techniques. To address this challenge, we develop DC-Gaussian based on the recent real-time neural rendering technique 3D Gaussian Splatting (3DGS). Our approach includes an adaptive image decomposition module to model reflections and occlusions in a unified manner. Additionally, we introduce illumination-aware obstruction modeling to manage reflections and occlusions under varying lighting conditions. Lastly, we employ a geometry-guided Gaussian enhancement strategy to improve rendering details by incorporating additional geometry priors. Experiments on self-captured and public dash cam videos show that our method not only achieves state-of-the-art performance in novel view synthesis, but also accurately reconstructing captured scenes getting rid of obstructions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages,7 figures;project page: https://linhanwang.github.io/dcgaussian/"
    },
    {
        "paper id": "2405.17711",
        "abstract url": "https://arxiv.org/abs/2405.17711",
        "title": "RealityEffects: Augmenting 3D Volumetric Videos with Object-Centric Annotation and Dynamic Visual Effects",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "trajectory"
            ]
        ],
        "abstract": "This paper introduces RealityEffects, a desktop authoring interface designed for editing and augmenting 3D volumetric videos with object-centric annotations and visual effects. RealityEffects enhances volumetric capture by introducing a novel method for augmenting captured physical motion with embedded, responsive visual effects, referred to as object-centric augmentation. In RealityEffects, users can interactively attach various visual effects to physical objects within the captured 3D scene, enabling these effects to dynamically move and animate in sync with the corresponding physical motion and body movements. The primary contribution of this paper is the development of a taxonomy for such object-centric augmentations, which includes annotated labels, highlighted objects, ghost effects, and trajectory visualization. This taxonomy is informed by an analysis of 120 edited videos featuring object-centric visual effects. The findings from our user study confirm that our direct manipulation techniques lower the barriers to editing and annotating volumetric captures, thereby enhancing interactive and engaging viewing experiences of 3D volumetric videos.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "DIS 2024"
    },
    {
        "paper id": "2405.17716",
        "abstract url": "https://arxiv.org/abs/2405.17716",
        "title": "Soft Multipath Information-Based UWB Tracking in Cluttered Scenarios: Preliminaries and Validations",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "In this paper, we investigate ultra-wideband (UWB) localization and tracking in cluttered environments. Instead of mitigating the multipath, we exploit the specular reflections to enhance the localizability and improve the positioning accuracy. With the assistance of the multipath, it is also possible to achieve localization purposes using fewer anchors or when the line-of-sight propagations are blocked. Rather than using single-value distance, angle, or Doppler estimates for the localization, we model the likelihoods of both the line-of-sight and specular multipath components, namely soft multipath information, and propose the multipath-assisted probabilistic UWB tracking algorithm. Experimental results in a cluttered industrial scenario show that the proposed algorithm achieves 46.4 cm and 33.1 cm 90th percentile errors in the cases of 3 and 4 anchors, respectively, which outperforms conventional methods with more than 61.8% improvement given fewer anchors and strong multipath effect.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17720",
        "abstract url": "https://arxiv.org/abs/2405.17720",
        "title": "MindFormer: A Transformer Architecture for Multi-Subject Brain Decoding via fMRI",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "fMRI"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Research efforts to understand neural signals have been ongoing for many years, with visual decoding from fMRI signals attracting considerable attention. Particularly, the advent of image diffusion models has advanced the reconstruction of images from fMRI data significantly. However, existing approaches often introduce inter- and intra- subject variations in the reconstructed images, which can compromise accuracy. To address current limitations in multi-subject brain decoding, we introduce a new Transformer architecture called MindFormer. This model is specifically designed to generate fMRI-conditioned feature vectors that can be used for conditioning Stable Diffusion model. More specifically, MindFormer incorporates two key innovations: 1) a novel training strategy based on the IP-Adapter to extract semantically meaningful features from fMRI signals, and 2) a subject specific token and linear layer that effectively capture individual differences in fMRI signals while synergistically combines multi subject fMRI data for training. Our experimental results demonstrate that Stable Diffusion, when integrated with MindFormer, produces semantically consistent images across different subjects. This capability significantly surpasses existing models in multi-subject brain decoding. Such advancements not only improve the accuracy of our reconstructions but also deepen our understanding of neural processing variations among individuals.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17753",
        "abstract url": "https://arxiv.org/abs/2405.17753",
        "title": "Regression Equilibrium in Electricity Markets",
        "rating": "-2",
        "keywords": [
            [
                "forecasting"
            ]
        ],
        "abstract": "Renewable power producers participating in electricity markets build forecasting models independently, relying on their own data, model and feature preferences. In this paper, we argue that in renewable-dominated markets, such an uncoordinated approach to forecasting results in substantial opportunity costs for stochastic producers and additional operating costs for the power system. As a solution, we introduce Regression Equilibrium--a welfare-optimal state of electricity markets under uncertainty, where profit-seeking stochastic producers do not benefit by unilaterally deviating from their equilibrium forecast models. While the regression equilibrium maximizes the private welfare, i.e., the average profit of stochastic producers across the day-ahead and real-time markets, it also aligns with the socially optimal, least-cost dispatch solution for the system. We base the equilibrium analysis on the theory of variational inequalities, providing results on the existence and uniqueness of regression equilibrium in energy-only markets. We also devise two methods for computing the regression equilibrium: centralized optimization and a decentralized ADMM-based algorithm that preserves the privacy of regression datasets.",
        "subjects": [
            "eess.SY",
            "econ.GN",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17754",
        "abstract url": "https://arxiv.org/abs/2405.17754",
        "title": "Differential Voltage Analysis and Patterns in Parallel-Connected Pairs of Imbalanced Cells",
        "rating": "-2",
        "keywords": [
            [
                "Diagnosing"
            ]
        ],
        "abstract": "Diagnosing imbalances in capacity and resistance within parallel-connected cells in battery packs is critical for battery management and fault detection, but it is challenging given that individual currents flowing into each cell are often unmeasured. This work introduces a novel method useful for identifying imbalances in capacity and resistance within a pair of parallel-connected cells using only voltage and current measurements from the pair. Our method utilizes differential voltage analysis (DVA) when the pair is under constant current discharge and demonstrates that features of the pair's differential voltage curve (dV/dQ), namely its mid-to-high SOC dV/dQ peak's height and skewness, are sensitive to imbalances in capacity and resistance. We analyze and explain how and why these dV/dQ peak shape features change in response to these imbalances, highlighting that the underlying current imbalance dynamics resulting from these imbalances contribute to these changes. Ultimately, we demonstrate that dV/dQ peak shape features can identify the product of capacity imbalance and resistance imbalance, but cannot uniquely identify the imbalances. This work lays the groundwork for identifying imbalances in capacity and resistance in parallel-connected cell groups in battery packs, where commonly only a single current sensor is placed for each parallel cell group.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted to American Control Conference (ACC), Toronto, Canada, July 2024"
    },
    {
        "paper id": "2405.17773",
        "abstract url": "https://arxiv.org/abs/2405.17773",
        "title": "Towards a Generalist and Blind RGB-X Tracker",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "thermal"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the emergence of a single large model capable of successfully solving a multitude of tasks in NLP, there has been growing research interest in achieving similar goals in computer vision. On the one hand, most of these generic models, referred to as generalist vision models, aim at producing unified outputs serving different tasks. On the other hand, some existing models aim to combine different input types (aka data modalities), which are then processed by a single large model. Yet, this step of combination remains specialized, which falls short of serving the initial ambition. In this paper, we showcase that such specialization (during unification) is unnecessary, in the context of RGB-X video object tracking. Our single model tracker, termed XTrack, can remain blind to any modality X during inference time. Our tracker employs a mixture of modal experts comprising those dedicated to shared commonality and others capable of flexibly performing reasoning conditioned on input modality. Such a design ensures the unification of input modalities towards a common latent space, without weakening the modality-specific information representation. With this idea, our training process is extremely simple, integrating multi-label classification loss with a routing function, thereby effectively aligning and unifying all modalities together, even from only paired data. Thus, during inference, we can adopt any modality without relying on the inductive bias of the modal prior and achieve generalist performance. Without any bells and whistles, our generalist and blind tracker can achieve competitive performance compared to well-established modal-specific models on 5 benchmarks across 3 auxiliary modalities, covering commonly used depth, thermal, and event data.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17795",
        "abstract url": "https://arxiv.org/abs/2405.17795",
        "title": "Dataset Regeneration for Sequential Recommendation",
        "rating": "-2",
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the \\textbf{model-centric} paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of \\textbf{data-centric} AI, we propose a novel data-centric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The code can be found at \\textcolor{blue}{\\url{https://anonymous.4open.science/r/KDD2024-86EA/}}",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17817",
        "abstract url": "https://arxiv.org/abs/2405.17817",
        "title": "Benchmarking Skeleton-based Motion Encoder Models for Clinical Applications: Estimating Parkinson's Disease Severity in Walking Sequences",
        "rating": "-2",
        "keywords": [
            [
                "Skeleton"
            ],
            [
                "biomechanical",
                "Disease",
                "Clinical",
                "pathological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This study investigates the application of general human motion encoders trained on large-scale human motion datasets for analyzing gait patterns in PD patients. Although these models have learned a wealth of human biomechanical knowledge, their effectiveness in analyzing pathological movements, such as parkinsonian gait, has yet to be fully validated. We propose a comparative framework and evaluate six pre-trained state-of-the-art human motion encoder models on their ability to predict the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS-III) gait scores from motion capture data. We compare these against a traditional gait feature-based predictive model in a recently released large public PD dataset, including PD patients on and off medication. The feature-based model currently shows higher weighted average accuracy, precision, recall, and F1-score. Motion encoder models with closely comparable results demonstrate promise for scalability and efficiency in clinical settings. This potential is underscored by the enhanced performance of the encoder model upon fine-tuning on PD training set. Four of the six human motion models examined provided prediction scores that were significantly different between on- and off-medication states. This finding reveals the sensitivity of motion encoder models to nuanced clinical changes. It also underscores the necessity for continued customization of these models to better capture disease-specific features, thereby reducing the reliance on labor-intensive feature engineering. Lastly, we establish a benchmark for the analysis of skeleton-based motion encoder models in clinical settings. To the best of our knowledge, this is the first study to provide a benchmark that enables state-of-the-art models to be tested and compete in a clinical context. Codes and benchmark leaderboard are available at code.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16861",
        "abstract url": "https://arxiv.org/abs/2405.16861",
        "title": "NCIDiff: Non-covalent Interaction-generative Diffusion Model for Improving Reliability of 3D Molecule Generation Inside Protein Pocket",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion",
                "inpainting"
            ],
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Advancements in deep generative modeling have changed the paradigm of drug discovery. Among such approaches, target-aware methods that exploit 3D structures of protein pockets were spotlighted for generating ligand molecules with their plausible binding modes. While docking scores superficially assess the quality of generated ligands, closer inspection of the binding structures reveals the inconsistency in local interactions between a pocket and generated ligands. Here, we address the issue by explicitly generating non-covalent interactions (NCIs), which are universal patterns throughout protein-ligand complexes. Our proposed model, NCIDiff, simultaneously denoises NCI types of protein-ligand edges along with a 3D graph of a ligand molecule during the sampling. With the NCI-generating strategy, our model generates ligands with more reliable NCIs, especially outperforming the baseline diffusion-based models. We further adopted inpainting techniques on NCIs to further improve the quality of the generated molecules. Finally, we showcase the applicability of NCIDiff on drug design tasks for real-world settings with specialized objectives by guiding the generation process with desired NCI patterns.",
        "subjects": [
            "q-bio.BM",
            "cs.LG",
            "physics.bio-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17059",
        "abstract url": "https://arxiv.org/abs/2405.17059",
        "title": "Comparative Study of Machine Learning Algorithms in Detecting Cardiovascular Diseases",
        "rating": "-2.5",
        "keywords": [
            [
                "SVM",
                "Support Vector Machine"
            ],
            [
                "medical",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The detection of cardiovascular diseases (CVD) using machine learning techniques represents a significant advancement in medical diagnostics, aiming to enhance early detection, accuracy, and efficiency. This study explores a comparative analysis of various machine learning algorithms, including Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and XGBoost. By utilising a structured workflow encompassing data collection, preprocessing, model selection and hyperparameter tuning, training, evaluation, and choice of the optimal model, this research addresses the critical need for improved diagnostic tools. The findings highlight the efficacy of ensemble methods and advanced algorithms in providing reliable predictions, thereby offering a comprehensive framework for CVD detection that can be readily implemented and adapted in clinical settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17132",
        "abstract url": "https://arxiv.org/abs/2405.17132",
        "title": "Your decision path does matter in pre-training industrial recommenders with multi-source behaviors",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "industrial",
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Online service platforms offering a wide range of services through miniapps have become crucial for users who visit these platforms with clear intentions to find services they are interested in. Aiming at effective content delivery, cross-domain recommendation are introduced to learn high-quality representations by transferring behaviors from data-rich scenarios. However, these methods overlook the impact of the decision path that users take when conduct behaviors, that is, users ultimately exhibit different behaviors based on various intents. To this end, we propose HIER, a novel Hierarchical decIsion path Enhanced Representation learning for cross-domain recommendation. With the help of graph neural networks for high-order topological information of the knowledge graph between multi-source behaviors, we further adaptively learn decision paths through well-designed exemplar-level and information bottleneck based contrastive learning. Extensive experiments in online and offline environments show the superiority of HIER.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17170",
        "abstract url": "https://arxiv.org/abs/2405.17170",
        "title": "Forecasting Four Business Cycle Phases Using Machine Learning: A Case Study of US and EuroZone",
        "rating": "-2.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Understanding the business cycle is crucial for building economic stability, guiding business planning, and informing investment decisions. The business cycle refers to the recurring pattern of expansion and contraction in economic activity over time. Economic analysis is inherently complex, incorporating a myriad of factors (such as macroeconomic indicators, political decisions). This complexity makes it challenging to fully account for all variables when determining the current state of the economy and predicting its future trajectory in the upcoming months. The objective of this study is to investigate the capacity of machine learning models in automatically analyzing the state of the economic, with the goal of forecasting business phases (expansion, slowdown, recession and recovery) in the United States and the EuroZone. We compared three different machine learning approaches to classify the phases of the business cycle, and among them, the Multinomial Logistic Regression (MLR) achieved the best results. Specifically, MLR got the best results by achieving the accuracy of 65.25% (Top1) and 84.74% (Top2) for the EuroZone and 75% (Top1) and 92.14% (Top2) for the United States. These results demonstrate the potential of machine learning techniques to predict business cycles accurately, which can aid in making informed decisions in the fields of economics and finance.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17282",
        "abstract url": "https://arxiv.org/abs/2405.17282",
        "title": "R-ODE: Ricci Curvature Tells When You Will be Informed",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Information diffusion prediction is fundamental to understand the structure and organization of the online social networks, and plays a crucial role to blocking rumor spread, influence maximization, political propaganda, etc. So far, most existing solutions primarily predict the next user who will be informed with historical cascades, but ignore an important factor in the diffusion process - the time. Such limitation motivates us to pose the problem of the time-aware personalized information diffusion prediction for the first time, telling the time when the target user will be informed. In this paper, we address this problem from a fresh geometric perspective of Ricci curvature, and propose a novel Ricci-curvature regulated Ordinary Differential Equation (R-ODE). In the diffusion process, R-ODE considers that the inter-correlated users are organized in a dynamic system in the representation space, and the cascades give the observations sampled from the continuous realm. At each infection time, the message diffuses along the largest Ricci curvature, signifying less transportation effort. In the continuous realm, the message triggers users' movement, whose trajectory in the space is parameterized by an ODE with graph neural network. Consequently, R-ODE predicts the infection time of a target user by the movement trajectory learnt from the observations. Extensive experiments evaluate the personalized time prediction ability of R-ODE, and show R-ODE outperforms the state-of-the-art baselines.",
        "subjects": [
            "cs.SI",
            "cs.LG"
        ],
        "comment": "Accepted by SIGIR 2024"
    },
    {
        "paper id": "2405.17309",
        "abstract url": "https://arxiv.org/abs/2405.17309",
        "title": "Survey of Graph Neural Network for Internet of Things and NextG Networks",
        "rating": "-2.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "6G",
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The exponential increase in Internet of Things (IoT) devices coupled with 6G pushing towards higher data rates and connected devices has sparked a surge in data. Consequently, harnessing the full potential of data-driven machine learning has become one of the important thrusts. In addition to the advancement in wireless technology, it is important to efficiently use the resources available and meet the users' requirements. Graph Neural Networks (GNNs) have emerged as a promising paradigm for effectively modeling and extracting insights which inherently exhibit complex network structures due to its high performance and accuracy, scalability, adaptability, and resource efficiency. There is a lack of a comprehensive survey that focuses on the applications and advances GNN has made in the context of IoT and Next Generation (NextG) networks. To bridge that gap, this survey starts by providing a detailed description of GNN's terminologies, architecture, and the different types of GNNs. Then we provide a comprehensive survey of the advancements in applying GNNs for IoT from the perspective of data fusion and intrusion detection. Thereafter, we survey the impact GNN has made in improving spectrum awareness. Next, we provide a detailed account of how GNN has been leveraged for networking and tactical systems. Through this survey, we aim to provide a comprehensive resource for researchers to learn more about GNN in the context of wireless networks, and understand its state-of-the-art use cases while contrasting to other machine learning approaches. Finally, we also discussed the challenges and wide range of future research directions to further motivate the use of GNN for IoT and NextG Networks.",
        "subjects": [
            "cs.LG",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17708",
        "abstract url": "https://arxiv.org/abs/2405.17708",
        "title": "OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators",
        "rating": "-2.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Offline policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy's performance by leveraging historical interaction data collected from other policies. Evaluating a new policy online without a confident estimate of its performance can lead to costly, unsafe, or hazardous outcomes, especially in education and healthcare. Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training. Unfortunately, choosing the best OPE algorithm for each task and domain is still unclear. In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure. We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation. Additionally, we demonstrate that when compared to alternative approaches, our estimator can be used to select higher-performing policies in healthcare and robotics. Our work contributes to improving ease of use for a general-purpose, estimator-agnostic, off-policy evaluation framework for offline RL.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2405.17782",
        "abstract url": "https://arxiv.org/abs/2405.17782",
        "title": "Post-Fair Federated Learning: Achieving Group and Community Fairness in Federated Learning via Post-processing",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "health"
            ],
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Federated Learning (FL) is a distributed machine learning framework in which a set of local communities collaboratively learn a shared global model while retaining all training data locally within each community. Two notions of fairness have recently emerged as important issues for federated learning: group fairness and community fairness. Group fairness requires that a model's decisions do not favor any particular group based on a set of legally protected attributes such as race or gender. Community fairness requires that global models exhibit similar levels of performance (accuracy) across all collaborating communities. Both fairness concepts can coexist within an FL framework, but the existing literature has focused on either one concept or the other. This paper proposes and analyzes a post-processing fair federated learning (FFL) framework called post-FFL. Post-FFL uses a linear program to simultaneously enforce group and community fairness while maximizing the utility of the global model. Because Post-FFL is a post-processing approach, it can be used with existing FL training pipelines whose convergence properties are well understood. This paper uses post-FFL on real-world datasets to mimic how hospital networks, for example, use federated learning to deliver community health care. Theoretical results bound the accuracy lost when post-FFL enforces both notion of fairness. Experimental results illustrate that post-FFL simultaneously improves both group and community fairness in FL. Moreover, post-FFL outperforms the existing in-processing fair federated learning in terms of improving both notions of fairness, communication efficiency and computation cost.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16849",
        "abstract url": "https://arxiv.org/abs/2405.16849",
        "title": "Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D Generation",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "Physics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we introduce a novel approach for creating controllable dynamics in 3D-generated Gaussians using casually captured reference videos. Our method transfers the motion of objects from reference videos to a variety of generated 3D Gaussians across different categories, ensuring precise and customizable motion transfer. We achieve this by employing blend skinning-based non-parametric shape reconstruction to extract the shape and motion of reference objects. This process involves segmenting the reference objects into motion-related parts based on skinning weights and establishing shape correspondences with generated target shapes. To address shape and temporal inconsistencies prevalent in existing methods, we integrate physical simulation, driving the target shapes with matched motion. This integration is optimized through a displacement loss to ensure reliable and genuine dynamics. Our approach supports diverse reference inputs, including humans, quadrupeds, and articulated objects, and can generate dynamics of arbitrary length, providing enhanced fidelity and applicability. Unlike methods heavily reliant on diffusion video generation models, our technique offers specific and high-quality motion transfer, maintaining both shape integrity and temporal consistency.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Our project page: https://Sync4D.github.io"
    },
    {
        "paper id": "2405.16867",
        "abstract url": "https://arxiv.org/abs/2405.16867",
        "title": "Clustering-based Learning for UAV Tracking and Pose Estimation",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "LiDAR",
                "flight"
            ],
            [
                "UAV"
            ],
            [
                "cs.AI"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "UAV tracking and pose estimation plays an imperative role in various UAV-related missions, such as formation control and anti-UAV measures. Accurately detecting and tracking UAVs in a 3D space remains a particularly challenging problem, as it requires extracting sparse features of micro UAVs from different flight environments and continuously matching correspondences, especially during agile flight. Generally, cameras and LiDARs are the two main types of sensors used to capture UAV trajectories in flight. However, both sensors have limitations in UAV classification and pose estimation. This technical report briefly introduces the method proposed by our team \"NTU-ICG\" for the CVPR 2024 UG2+ Challenge Track 5. This work develops a clustering-based learning detection approach, CL-Det, for UAV tracking and pose estimation using two types of LiDARs, namely Livox Avia and LiDAR 360. We combine the information from the two data sources to locate drones in 3D. We first align the timestamps of Livox Avia data and LiDAR 360 data and then separate the point cloud of objects of interest (OOIs) from the environment. The point cloud of OOIs is clustered using the DBSCAN method, with the midpoint of the largest cluster assumed to be the UAV position. Furthermore, we utilize historical estimations to fill in missing data. The proposed method shows competitive pose estimation performance and ranks 5th on the final leaderboard of the CVPR 2024 UG2+ Challenge.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Submitted Report of CVPR 2024 UG2+ Challenge Track 5"
    },
    {
        "paper id": "2405.16942",
        "abstract url": "https://arxiv.org/abs/2405.16942",
        "title": "PASTA: Pathology-Aware MRI to PET Cross-Modal Translation with Diffusion Models",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "diagnosing",
                "MRI",
                "pathological"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Positron emission tomography (PET) is a well-established functional imaging technique for diagnosing brain disorders. However, PET's high costs and radiation exposure limit its widespread use. In contrast, magnetic resonance imaging (MRI) does not have these limitations. Although it also captures neurodegenerative changes, MRI is a less sensitive diagnostic tool than PET. To close this gap, we aim to generate synthetic PET from MRI. Herewith, we introduce PASTA, a novel pathology-aware image translation framework based on conditional diffusion models. Compared to the state-of-the-art methods, PASTA excels in preserving both structural and pathological details in the target modality, which is achieved through its highly interactive dual-arm architecture and multi-modal condition integration. A cycle exchange consistency and volumetric generation strategy elevate PASTA's capability to produce high-quality 3D PET scans. Our qualitative and quantitative results confirm that the synthesized PET scans from PASTA not only reach the best quantitative scores but also preserve the pathology correctly. For Alzheimer's classification, the performance of synthesized scans improves over MRI by 4%, almost reaching the performance of actual PET. Code is available at https://github.com/ai-med/PASTA.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17015",
        "abstract url": "https://arxiv.org/abs/2405.17015",
        "title": "A Deep-NN Beamforming Approach for Dual Function Radar-Communication THz UAV",
        "rating": "-3",
        "keywords": [
            [
                "Radar"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "In this paper, we consider a scenario with one UAV equipped with a ULA, which sends combined information and sensing signals to communicate with multiple GBS and, at the same time, senses potential targets placed within an interested area on the ground. We aim to jointly design the transmit beamforming with the GBS association to optimize communication performance while ensuring high sensing accuracy. We propose a predictive beamforming framework based on a dual DNN solution to solve the formulated nonconvex optimization problem. A first DNN is trained to produce the required beamforming matrix for any point of the UAV flying area in a reduced time compared to state-of-the-art beamforming optimizers. A second DNN is trained to learn the optimal mapping from the input features, power, and EIRP constraints to the GBS association decision. Finally, we provide an extensive simulation analysis to corroborate the proposed approach and show the benefits of EIRP, SINR performance and computational speed.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17016",
        "abstract url": "https://arxiv.org/abs/2405.17016",
        "title": "$\\text{Di}^2\\text{Pose}$: Discrete Diffusion Model for Occluded 3D Human Pose Estimation",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "biomechanically"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Continuous diffusion models have demonstrated their effectiveness in addressing the inherent uncertainty and indeterminacy in monocular 3D human pose estimation (HPE). Despite their strengths, the need for large search spaces and the corresponding demand for substantial training data make these models prone to generating biomechanically unrealistic poses. This challenge is particularly noticeable in occlusion scenarios, where the complexity of inferring 3D structures from 2D images intensifies. In response to these limitations, we introduce the Discrete Diffusion Pose ($\\text{Di}^2\\text{Pose}$), a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model. Specifically, $\\text{Di}^2\\text{Pose}$ employs a two-stage process: it first converts 3D poses into a discrete representation through a \\emph{pose quantization step}, which is subsequently modeled in latent space through a \\emph{discrete diffusion process}. This methodological innovation restrictively confines the search space towards physically viable configurations and enhances the model's capability to comprehend how occlusions affect human pose within the latent space. Extensive evaluations conducted on various benchmarks (e.g., Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17426",
        "abstract url": "https://arxiv.org/abs/2405.17426",
        "title": "Benchmarking and Improving Bird's Eye View Perception Robustness in Autonomous Driving",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in bird's eye view (BEV) representations have shown remarkable promise for in-vehicle 3D perception. However, while these methods have achieved impressive results on standard benchmarks, their robustness in varied conditions remains insufficiently assessed. In this study, we present RoboBEV, an extensive benchmark suite designed to evaluate the resilience of BEV algorithms. This suite incorporates a diverse set of camera corruption types, each examined over three severity levels. Our benchmarks also consider the impact of complete sensor failures that occur when using multi-modal models. Through RoboBEV, we assess 33 state-of-the-art BEV-based perception models spanning tasks like detection, map segmentation, depth estimation, and occupancy prediction. Our analyses reveal a noticeable correlation between the model's performance on in-distribution datasets and its resilience to out-of-distribution challenges. Our experimental results also underline the efficacy of strategies like pre-training and depth-free BEV transformations in enhancing robustness against out-of-distribution data. Furthermore, we observe that leveraging extensive temporal information significantly improves the model's robustness. Based on our observations, we design an effective robustness enhancement strategy based on the CLIP model. The insights from this study pave the way for the development of future BEV models that seamlessly combine accuracy with real-world robustness.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Preprint; 17 pages, 13 figures, 11 tables; Code at this https URL: https://github.com/Daniel-xsy/RoboBEV"
    },
    {
        "paper id": "2405.17579",
        "abstract url": "https://arxiv.org/abs/2405.17579",
        "title": "Harnessing Natural Oscillations for High-Speed, Efficient Asymmetrical Locomotion in Quadrupedal Robots",
        "rating": "-3",
        "keywords": [
            [
                "robotics",
                "robot"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "This study explores the dynamics of asymmetrical bounding gaits in quadrupedal robots, focusing on the integration of torso pitching and hip motion to enhance speed and stability. Traditional control strategies often enforce a fixed posture, minimizing natural body movements to simplify the control problem. However, this approach may overlook the inherent dynamical advantages found in natural locomotion. By considering the robot as two interconnected segments, we concentrate on stance leg motion while allowing passive torso oscillation, drawing inspiration from natural dynamics and underactuated robotics principles. Our control scheme employs Linear Inverted Pendulum (LIP) and Spring-Loaded Inverted Pendulum (SLIP) models to govern front and rear leg movements independently. This approach has been validated through extensive simulations and hardware experiments, demonstrating successful high-speed locomotion with top speeds nearing 4 m/s and reduced ground reaction forces, indicating a more efficient gait. Furthermore, unlike conventional methods, our strategy leverages natural torso oscillations to aid leg circulation and stride length, aligning robot dynamics more closely with biological counterparts. Our findings suggest that embracing the natural dynamics of quadrupedal movement, particularly in asymmetrical gaits like bounding, can lead to more stable, efficient, and high-speed robotic locomotion. This investigation lays the groundwork for future studies on versatile and dynamic quadrupedal gaits and their potential applications in scenarios demanding rapid and effective locomotion.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17629",
        "abstract url": "https://arxiv.org/abs/2405.17629",
        "title": "Lindenmayer graph languages, first-order theories and expanders",
        "rating": "-3",
        "keywords": [
            [
                "graph"
            ],
            [
                "grammar"
            ]
        ],
        "abstract": "Combinatorial generation of expander families and Lindenmayer-style development models are both parallel in nature. Both can be handled within proposed parallel graph grammar formalism. Their first-order properties can then be checked by encompassing the generated graph language into an appropriate automatic structure.",
        "subjects": [
            "cs.FL",
            "math.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17657",
        "abstract url": "https://arxiv.org/abs/2405.17657",
        "title": "Robust Perception and Navigation of Autonomous Surface Vehicles in Challenging Environments",
        "rating": "-3",
        "keywords": [
            [
                "Navigation"
            ],
            [
                "remote sensing"
            ]
        ],
        "abstract": "Research on coastal regions traditionally involves methods like manual sampling, monitoring buoys, and remote sensing, but these methods face challenges in spatially and temporally diverse regions of interest. Autonomous surface vehicles (ASVs) with artificial intelligence (AI) are being explored, and recognized by the International Maritime Organization (IMO) as vital for future ecosystem understanding. However, there is not yet a mature technology for autonomous environmental monitoring due to typically complex coastal situations: (1) many static (e.g., buoy, dock) and dynamic (e.g., boats) obstacles not compliant with the rules of the road (COLREGs); (2) uncharted or uncertain information (e.g., non-updated nautical chart); and (3) high-cost ASVs not accessible to the community and citizen science while resulting in technology illiteracy. To address the above challenges, my research involves both system and algorithmic development: (1) a robotic boat system for stable and reliable in-water monitoring, (2) maritime perception to detect and track obstacles (such as buoys, and boats), and (3) navigational decision-making with multiple-obstacle avoidance and multi-objective optimization.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "rss pioneer 2024"
    },
    {
        "paper id": "2405.17681",
        "abstract url": "https://arxiv.org/abs/2405.17681",
        "title": "Synthesizing JSON Schema Transformers",
        "rating": "-3",
        "keywords": [
            [
                "Synthesizing"
            ],
            [
                "healthcare"
            ]
        ],
        "abstract": "JSON (JavaScript Object Notation) is a data encoding that allows structured data to be used in a standardized and straightforward manner across systems. Schemas for JSON-formatted data can be constructed using the JSON Schema standard, which describes the data types, structure, and meaning of JSON-formatted data. JSON is commonly used for storing and transmitting information such as program configurations, web API requests and responses, or remote procedure calls; or data records, such as healthcare information or other structured documents. Since JSON is a plaintext format with potentially highly complex definitions, it can be an arduous process to change code which handles structured JSON data when its storage or transmission schemas are modified. Our work describes a program synthesis method to generate a program that accepts data conforming to a given input JSON Schema and automatically converts it to conform to a resulting, target JSON Schema. We use a top-down, type-directed approach to search for programs using a set of rewrite rules which constrain the ways in which a schema can be modified without unintended data loss or corruption. Once a satisfying sequence of rewrites has been found, we pass an intermediate representation of the rewrite sequence to a code generation backend, which synthesizes a program which executes the data transformation. This system allows users to quickly and efficiently modify or augment their existing systems in safe ways at their interfaces.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17702",
        "abstract url": "https://arxiv.org/abs/2405.17702",
        "title": "A Two-sided Model for EV Market Dynamics and Policy Implications",
        "rating": "-3",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "The diffusion of Electric Vehicles (EVs) plays a pivotal role in mitigating greenhouse gas emissions, particularly in the U.S., where ambitious zero-emission and carbon neutrality objectives have been set. In pursuit of these goals, many states have implemented a range of incentive policies aimed at stimulating EV adoption and charging infrastructure development, especially public EV charging stations (EVCS). This study examines the indirect network effect observed between EV adoption and EVCS deployment within urban landscapes. We developed a two-sided log-log regression model with historical data on EV purchases and EVCS development to quantify this effect. To test the robustness, we then conducted a case study of the EV market in Los Angeles (LA) County, which suggests that a 1% increase in EVCS correlates with a 0.35% increase in EV sales. Additionally, we forecasted the future EV market dynamics in LA County, revealing a notable disparity between current policies and the targeted 80% EV market share for private cars by 2045. To bridge this gap, we proposed a combined policy recommendation that enhances EV incentives by 60% and EVCS rebates by 66%, facilitating the achievement of future EV market objectives.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Conference preprint, 8 pages, 3 figures"
    },
    {
        "paper id": "2405.18449",
        "abstract url": "https://arxiv.org/abs/2405.18449",
        "title": "Adaptive Multiscale Retinal Diagnosis: A Hybrid Trio-Model Approach for Comprehensive Fundus Multi-Disease Detection Leveraging Transfer Learning and Siamese Networks",
        "rating": "-3",
        "keywords": [
            [
                "Diagnosis",
                "Disease",
                "Retinal"
            ],
            [
                "haze"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "WHO has declared that more than 2.2 billion people worldwide are suffering from visual disorders, such as media haze, glaucoma, and drusen. At least 1 billion of these cases could have been either prevented or successfully treated, yet they remain unaddressed due to poverty, a lack of specialists, inaccurate ocular fundus diagnoses by ophthalmologists, or the presence of a rare disease. To address this, the research has developed the Hybrid Trio-Network Model Algorithm for accurately diagnosing 12 distinct common and rare eye diseases. This algorithm utilized the RFMiD dataset of 3,200 fundus images and the Binary Relevance Method to detect diseases separately, ensuring expandability and avoiding incorrect correlations. Each detector, incorporating finely tuned hyperparameters to optimize performance, consisted of three feature components: A classical transfer learning CNN model, a two-stage CNN model, and a Siamese Network. The diagnosis was made using features extracted through this Trio-Model with Ensembled Machine Learning algorithms. The proposed model achieved an average accuracy of 97% and an AUC score of 0.96. Compared to past benchmark studies, an increase of over 10% in the F1-score was observed for most diseases. Furthermore, using the Siamese Network, the model successfully made predictions in diseases like optic disc pallor, which past studies failed to predict due to low confidence. This diagnostic tool presents a stable, adaptive, cost-effective, efficient, accessible, and fast solution for globalizing early detection of both common and rare diseases.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17051",
        "abstract url": "https://arxiv.org/abs/2405.17051",
        "title": "BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics",
        "rating": "-3.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "Physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Data-driven deep learning has emerged as the new paradigm to model complex physical space-time systems. These data-driven methods learn patterns by optimizing statistical metrics and tend to overlook the adherence to physical laws, unlike traditional model-driven numerical methods. Thus, they often generate predictions that are not physically realistic. On the other hand, by sampling a large amount of high quality predictions from a data-driven model, some predictions will be more physically plausible than the others and closer to what will happen in the future. Based on this observation, we propose \\emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical alignment of data-driven space-time forecasting models. The key of BeamVQ is to train model on self-generated samples filtered with physics-aware metrics. To be flexibly support different backbone architectures, BeamVQ leverages a code bank to transform any encoder-decoder model to the continuous state space into discrete codes. Afterwards, it iteratively employs beam search to sample high-quality sequences, retains those with the highest physics-aware scores, and trains model on the new dataset. Comprehensive experiments show that BeamVQ not only gave an average statistical skill score boost for more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17339",
        "abstract url": "https://arxiv.org/abs/2405.17339",
        "title": "Physics-Informed Real NVP for Satellite Power System Fault Detection",
        "rating": "-3.5",
        "keywords": [
            [
                "Satellite"
            ],
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The unique challenges posed by the space environment, characterized by extreme conditions and limited accessibility, raise the need for robust and reliable techniques to identify and prevent satellite faults. Fault detection methods in the space sector are required to ensure mission success and to protect valuable assets. In this context, this paper proposes an Artificial Intelligence (AI) based fault detection methodology and evaluates its performance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an Electrical Power System (EPS) dataset, crafted in laboratory by NASA. Our study focuses on the application of a physics-informed (PI) real-valued non-volume preserving (Real NVP) model for fault detection in space systems. The efficacy of this method is systematically compared against other AI approaches such as Gated Recurrent Unit (GRU) and Autoencoder-based techniques. Results show that our physics-informed approach outperforms existing methods of fault detection, demonstrating its suitability for addressing the unique challenges of satellite EPS sub-system faults. Furthermore, we unveil the competitive advantage of physics-informed loss in AI models to address specific space needs, namely robustness, reliability, and power constraints, crucial for space exploration and satellite missions.",
        "subjects": [
            "cs.LG",
            "cs.RO"
        ],
        "comment": "Accepted at International Conference on Advanced Intelligent Mechatronics (AIM) 2024"
    },
    {
        "paper id": "2405.17352",
        "abstract url": "https://arxiv.org/abs/2405.17352",
        "title": "Assessing the significance of longitudinal data in Alzheimer's Disease forecasting",
        "rating": "-3.5",
        "keywords": [
            [
                "Disease",
                "clinical"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this study, we employ a transformer encoder model to characterize the significance of longitudinal patient data for forecasting the progression of Alzheimer's Disease (AD). Our model, Longitudinal Forecasting Model for Alzheimer's Disease (LongForMAD), harnesses the comprehensive temporal information embedded in sequences of patient visits that incorporate multimodal data, providing a deeper understanding of disease progression than can be drawn from single-visit data alone. We present an empirical analysis across two patient groups-Cognitively Normal (CN) and Mild Cognitive Impairment (MCI)-over a span of five follow-up years. Our findings reveal that models incorporating more extended patient histories can outperform those relying solely on present information, suggesting a deeper historical context is critical in enhancing predictive accuracy for future AD progression. Our results support the incorporation of longitudinal data in clinical settings to enhance the early detection and monitoring of AD. Our code is available at \\url{https://github.com/batuhankmkaraman/LongForMAD}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16873",
        "abstract url": "https://arxiv.org/abs/2405.16873",
        "title": "ContrastAlign: Toward Robust BEV Feature Alignment via Contrastive Learning for Multi-Modal 3D Object Detection",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "LiDAR"
            ],
            [
                "graph"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the field of 3D object detection tasks, fusing heterogeneous features from LiDAR and camera sensors into a unified Bird's Eye View (BEV) representation is a widely adopted paradigm. However, existing methods are often compromised by imprecise sensor calibration, resulting in feature misalignment in LiDAR-camera BEV fusion. Moreover, such inaccuracies result in errors in depth estimation for the camera branch, ultimately causing misalignment between LiDAR and camera BEV features. In this work, we propose a novel ContrastAlign approach that utilizes contrastive learning to enhance the alignment of heterogeneous modalities, thereby improving the robustness of the fusion process. Specifically, our approach includes the L-Instance module, which directly outputs LiDAR instance features within LiDAR BEV features. Then, we introduce the C-Instance module, which predicts camera instance features through RoI (Region of Interest) pooling on the camera BEV features. We propose the InstanceFusion module, which utilizes contrastive learning to generate similar instance features across heterogeneous modalities. We then use graph matching to calculate the similarity between the neighboring camera instance features and the similarity instance features to complete the alignment of instance features. Our method achieves state-of-the-art performance, with an mAP of 70.3%, surpassing BEVFusion by 1.8% on the nuScenes validation set. Importantly, our method outperforms BEVFusion by 7.3% under conditions with misalignment noise.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16889",
        "abstract url": "https://arxiv.org/abs/2405.16889",
        "title": "Extraction of In-Phase and Quadrature Components by Time-Encoding Sampling",
        "rating": "-4",
        "keywords": [
            [
                "biologically-inspired"
            ],
            [
                "thermal"
            ]
        ],
        "abstract": "Time encoding machine (TEM) is a biologically-inspired scheme to perform signal sampling using timing. In this paper, we study its application to the sampling of bandpass signals. We propose an integrate-and-fire TEM scheme by which the in-phase (I) and quadrature (Q) components are extracted through reconstruction. We design the TEM according to the signal bandwidth and amplitude instead of upper-edge frequency and amplitude as in the case of bandlimited/lowpass signals. We show that the I and Q components can be perfectly reconstructed from the TEM measurements if the minimum firing rate is equal to the Landau's rate of the signal. For the reconstruction of I and Q components, we develop an alternating projection onto convex sets (POCS) algorithm in which two POCS algorithms are alternately iterated. For the algorithm analysis, we define a solution space of vector-valued signals and prove that the proposed reconstruction algorithm converges to the correct unique solution in the noiseless case. The proposed TEM can operate regardless of the center frequencies of the bandpass signals. This is quite different from traditional bandpass sampling, where the center frequency should be carefully allocated for Landau's rate and its variations have the negative effect on the sampling performance. In addition, the proposed TEM achieves certain reconstructed signal-to-noise-plus-distortion ratios for small firing rates in thermal noise, which is unavoidably present and will be aliased to the Nyquist band in the traditional sampling such that high sampling rates are required. We demonstrate the reconstruction performance and substantiate our claims via simulation experiments.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "30 pages, 8 figures"
    },
    {
        "paper id": "2405.17073",
        "abstract url": "https://arxiv.org/abs/2405.17073",
        "title": "Soft Two-degree-of-freedom Dielectric Elastomer Position Sensor Exhibiting Linear Behavior",
        "rating": "-4",
        "keywords": [
            [
                "multi-DOF"
            ],
            [
                "robot"
            ],
            [
                "MRI"
            ]
        ],
        "abstract": "Soft robots could bring robotic systems to new horizons, by enabling safe human-machine interaction. For precise control, these soft structures require high level position feedback that is not easily achieved through conventional one-degree-of-freedom (DOF) sensing apparatus. In this paper, a soft two-DOF dielectric elastomer (DE) sensor is specifically designed to provide accurate position feedback for a soft polymer robotic manipulator. The technology is exemplified on a soft robot intended for MRI-guided prostate interventions. DEs are chosen for their major advantages of softness, high strains, low cost and embedded multiple-DOF sensing capability, providing excellent system integration. A geometrical model of the proposed DE sensor is developed and compared to experimental results in order to understand sensor mechanics. Using a differential measurement approach, a handmade prototype provided linear sensory behavior and 0.2 mm accuracy on two-DOF. This correlates to a 0.7\\% error over the sensor's 30 mm x 30 mm planar range, demonstrating the outstanding potential of DE technology for accurate multi-DOF position sensing.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17150",
        "abstract url": "https://arxiv.org/abs/2405.17150",
        "title": "Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things",
        "rating": "-4",
        "keywords": [
            [
                "6G",
                "IoT"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "Low earth orbit (LEO) satellite internet of things (IoT) is a promising way achieving global Internet of Everything, and thus has been widely recognized as an important component of sixth-generation (6G) wireless networks. Yet, due to high-speed movement of the LEO satellite, it is challenging to acquire timely channel state information (CSI) and design effective multibeam precoding for various IoT applications. To this end, this paper provides a deep learning (DL)-based joint channel prediction and multibeam precoding scheme under adverse environments, e.g., high Doppler shift, long propagation delay, and low satellite payload. {Specifically, this paper first designs a DL-based channel prediction scheme by using convolutional neural networks (CNN) and long short term memory (LSTM), which predicts the CSI of current time slot according to that of previous time slots. With the predicted CSI, this paper designs a DL-based robust multibeam precoding scheme by using a channel augmentation method based on variational auto-encoder (VAE).} Finally, extensive simulation results confirm the effectiveness and robustness of the proposed scheme in LEO satellite IoT.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "IEEE Transactions on Wireless Communications, 2024"
    },
    {
        "paper id": "2405.17419",
        "abstract url": "https://arxiv.org/abs/2405.17419",
        "title": "MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities",
        "rating": "-4",
        "keywords": [
            [
                "synthesis"
            ],
            [
                "autonomous driving"
            ],
            [
                "robot"
            ],
            [
                "surgery"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on image data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection. To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, NP-Mix, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements A2D to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin. Our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Code and MultiOOD benchmark: https://github.com/donghao51/MultiOOD"
    },
    {
        "paper id": "2405.17543",
        "abstract url": "https://arxiv.org/abs/2405.17543",
        "title": "The MQT Handbook: A Summary of Design Automation Tools and Software for Quantum Computing",
        "rating": "-4",
        "keywords": [
            [
                "chemistry"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum computers are becoming a reality and numerous quantum computing applications with a near-term perspective (e.g., for finance, chemistry, machine learning, and optimization) and with a long-term perspective (e.g., for cryptography or unstructured search) are currently being investigated. However, designing and realizing potential applications for these devices in a scalable fashion requires automated, efficient, and user-friendly software tools that cater to the needs of end users, engineers, and physicists at every level of the entire quantum software stack. Many of the problems to be tackled in that regard are similar to design problems from the classical realm for which sophisticated design automation tools have been developed in the previous decades. The Munich Quantum Toolkit (MQT) is a collection of software tools for quantum computing developed by the Chair for Design Automation at the Technical University of Munich which explicitly utilizes this design automation expertise. Our overarching objective is to provide solutions for design tasks across the entire quantum software stack. This entails high-level support for end users in realizing their applications, efficient methods for the classical simulation, compilation, and verification of quantum circuits, tools for quantum error correction, support for physical design, and more. These methods are supported by corresponding data structures (such as decision diagrams) and core methods (such as SAT encodings/solvers). All of the developed tools are available as open-source implementations and are hosted on https://github.com/cda-tum.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": "8 pages, 4 figures, a live version of this document is available at https://mqt.readthedocs.io"
    },
    {
        "paper id": "2405.17600",
        "abstract url": "https://arxiv.org/abs/2405.17600",
        "title": "Spatial Spinal Fixation: A Transformative Approach Using a Unique Robot-Assisted Steerable Drilling System and Flexible Pedicle Screw",
        "rating": "-4",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot"
            ],
            [
                "CT"
            ]
        ],
        "abstract": "Spinal fixation procedures are currently limited by the rigidity of the existing instruments and pedicle screws leading to fixation failures and rigid pedicle screw pull out. Leveraging our recently developed Concentric Tube Steerable Drilling Robot (CT-SDR) in integration with a robotic manipulator, to address the aforementioned issue, here we introduce the transformative concept of Spatial Spinal Fixation (SSF) using a unique Flexible Pedicle Screw (FPS). The proposed SSF procedure enables planar and out-of-plane placement of the FPS throughout the full volume of the vertebral body. In other words, not only does our fixation system provide the option of drilling in-plane and out-of-plane trajectories, it also enables implanting the FPS inside linear (represented by an I-shape) and/or non-linear (represented by J-shape) trajectories. To thoroughly evaluate the functionality of our proposed robotic system and the SSF procedure, we have performed various experiments by drilling different I-J and J-J drilling trajectory pairs into our custom-designed L3 vertebral phantoms and analyzed the accuracy of the procedure using various metrics.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "6 pages, 4 figures"
    },
    {
        "paper id": "2405.17603",
        "abstract url": "https://arxiv.org/abs/2405.17603",
        "title": "Towards Biomechanical Evaluation of a Transformative Additively Manufactured Flexible Pedicle Screw for Robotic Spinal Fixation",
        "rating": "-4",
        "keywords": [
            [
                "Biomechanical",
                "surgery"
            ],
            [
                "mineral"
            ]
        ],
        "abstract": "Vital for spinal fracture treatment, pedicle screw fixation is the gold standard for spinal fixation procedures. Nevertheless, due to the screw pullout and loosening issues, this surgery often fails to be effective for patients suffering from osteoporosis (i.e., having low bone mineral density). These failures can be attributed to the rigidity of existing drilling instruments and pedicle screws forcing clinicians to place these implants into the osteoporotic regions of the vertebral body. To address this critical issue, we have developed a steerable drilling robotic system and evaluated its performance in drilling various J- and U-shape trajectories. Complementary to this robotic system, in this paper, we propose design, additive manufacturing, and biomechanical evaluation of a transformative flexible pedicle screw (FPS) that can be placed in pre-drilled straight and curved trajectories. To evaluate the performance of the proposed flexible implant, we designed and fabricated two different types of FPSs using the direct metal laser sintering (DMLS) process. Utilizing our unique experimental setup and ASTM standards, we then performed various pullout experiments on these FPSs to evaluate and analyze their biomechanical performance implanted in straight trajectories.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "6 pages, 7 figures, Accepted for publication at the 2024 International Symposium on Medical Robotics"
    },
    {
        "paper id": "2405.17606",
        "abstract url": "https://arxiv.org/abs/2405.17606",
        "title": "A Patient-Specific Framework for Autonomous Spinal Fixation via a Steerable Drilling Robot",
        "rating": "-4",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot",
                "navigation"
            ],
            [
                "CT"
            ]
        ],
        "abstract": "In this paper, with the goal of enhancing the minimally invasive spinal fixation procedure in osteoporotic patients, we propose a first-of-its-kind image-guided robotic framework for performing an autonomous and patient-specific procedure using a unique concentric tube steerable drilling robot (CT-SDR). Particularly, leveraging a CT-SDR, we introduce the concept of J-shape drilling based on a pre-operative trajectory planned in CT scan of a patient followed by appropriate calibration, registration, and navigation steps to safely execute this trajectory in real-time using our unique robotic setup. To thoroughly evaluate the performance of our framework, we performed several experiments on two different vertebral phantoms designed based on CT scan of real patients.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 3 figures"
    },
    {
        "paper id": "2405.17324",
        "abstract url": "https://arxiv.org/abs/2405.17324",
        "title": "Leveraging Offline Data in Linear Latent Bandits",
        "rating": "-4.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "healthcare"
            ],
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Sequential decision-making domains such as recommender systems, healthcare and education often have unobserved heterogeneity in the population that can be modeled using latent bandits $-$ a framework where an unobserved latent state determines the model for a trajectory. While the latent bandit framework is compelling, the extent of its generality is unclear. We first address this by establishing a de Finetti theorem for decision processes, and show that $\\textit{every}$ exchangeable and coherent stateless decision process is a latent bandit. The latent bandit framework lends itself particularly well to online learning with offline datasets, a problem of growing interest in sequential decision-making. One can leverage offline latent bandit data to learn a complex model for each latent state, so that an agent can simply learn the latent state online to act optimally. We focus on a linear model for a latent bandit with $d_A$-dimensional actions, where the latent states lie in an unknown $d_K$-dimensional subspace for $d_K \\ll d_A$. We present SOLD, a novel principled method to learn this subspace from short offline trajectories with guarantees. We then provide two methods to leverage this subspace online: LOCAL-UCB and ProBALL-UCB. We demonstrate that LOCAL-UCB enjoys $\\tilde O(\\min(d_A\\sqrt{T}, d_K\\sqrt{T}(1+\\sqrt{d_AT/d_KN})))$ regret guarantees, where the effective dimension is lower when the size $N$ of the offline dataset is larger. ProBALL-UCB enjoys a slightly weaker guarantee, but is more practical and computationally efficient. Finally, we establish the efficacy of our methods using experiments on both synthetic data and real-life movie recommendation data from MovieLens.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "40 pages. 14 pages for main paper, 26 pages for references + appendix"
    },
    {
        "paper id": "2405.17060",
        "abstract url": "https://arxiv.org/abs/2405.17060",
        "title": "Graph Neural Networks on Quantum Computers",
        "rating": "-5.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "recommendation"
            ],
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
        "subjects": [
            "quant-ph",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "50 Pages, 22 Figures"
    },
    {
        "paper id": "2405.17065",
        "abstract url": "https://arxiv.org/abs/2405.17065",
        "title": "Model-Driven Engineering for Quantum Programming: A Case Study on Ground State Energy Calculation",
        "rating": "-6",
        "keywords": [
            [
                "medical"
            ],
            [
                "chemistry"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "This study introduces a novel framework that brings together two main Quantum Programming methodologies, gate-based Quantum Computing and Quantum Annealing, by applying the Model-Driven Engineering principles. This aims to enhance the adaptability, design and scalability of quantum programs, facilitating their design and operation across diverse computing platforms. A notable achievement of this research is the development of a mapping method for programs between gate-based quantum computers and quantum annealers which can lead to the automatic transformation of these programs. Specifically, this method is applied to the Variational Quantum Eigensolver Algorithm and Quantum Anneling Ising Model, targeting ground state solutions. Finding ground-state solutions is crucial for a wide range of scientific applications, ranging from simulating chemistry lab experiments to medical applications, such as vaccine development. The success of this application demonstrates Model-Driven Engineering for Quantum Programming frameworks's practical viability and sets a clear path for quantum Computing's broader use in solving intricate problems.",
        "subjects": [
            "quant-ph",
            "cs.PL",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16854",
        "abstract url": "https://arxiv.org/abs/2405.16854",
        "title": "Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Multi-agent reinforcement learning (MARL) is employed to develop autonomous agents that can learn to adopt cooperative or competitive strategies within complex environments. However, the linear increase in the number of agents leads to a combinatorial explosion of the action space, which may result in algorithmic instability, difficulty in convergence, or entrapment in local optima. While researchers have designed a variety of effective algorithms to compress the action space, these methods also introduce new challenges, such as the need for manually designed prior knowledge or reliance on the structure of the problem, which diminishes the applicability of these techniques. In this paper, we introduce Evolutionary action SPAce Reduction with Knowledge (eSpark), an exploration function generation framework driven by large language models (LLMs) to boost exploration and prune unnecessary actions in MARL. Using just a basic prompt that outlines the overall task and setting, eSpark is capable of generating exploration functions in a zero-shot manner, identifying and pruning redundant or irrelevant state-action pairs, and then achieving autonomous improvement from policy feedback. In reinforcement learning tasks involving inventory management and traffic light control encompassing a total of 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in all scenarios, achieving an average performance gain of 34.4% and 9.9% in the two types of tasks respectively. Additionally, eSpark has proven to be capable of managing situations with a large number of agents, securing a 29.7% improvement in scalability challenges that featured over 500 agents. The code can be found in https://github.com/LiuZhihao2022/eSpark.git.",
        "subjects": [
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16864",
        "abstract url": "https://arxiv.org/abs/2405.16864",
        "title": "Sparsity comparison of polytopal finite element methods",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this work we compare crucial parameters for efficiency of different finite element methods for solving partial differential equations (PDEs) on polytopal meshes. We consider the Virtual Element Method (VEM) and different Discontinuous Galerkin (DG) methods, namely the Hybrid DG and Trefftz DG methods. The VEM is a conforming method, that can be seen as a generalization of the classic finite element method to arbitrary polytopal meshes. DG methods are non-conforming methods that offer high flexibility, but also come with high computational costs. Hybridization reduces these costs by introducing additional facet variables, onto which the computational costs can be transfered to. Trefftz DG methods achieve a similar reduction in complexity by selecting a special and smaller set of basis functions on each element. The association of computational costs to different geometrical entities (elements or facets) leads to differences in the performance of these methods on different grid types. This paper aims to compare the dependency of these approaches across different grid configurations.",
        "subjects": [
            "math.NA",
            "cs.CC",
            "cs.CE"
        ],
        "comment": "15 pages, 12 figures, 15 tables"
    },
    {
        "paper id": "2405.16866",
        "abstract url": "https://arxiv.org/abs/2405.16866",
        "title": "Hierarchical Rank-One Sequence Convexification for the Relaxation of Variational Problems with Microstructures",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents an efficient algorithm for the approximation of the rank-one convex hull in the context of nonlinear solid mechanics. It is based on hierarchical rank-one sequences and simultaneously provides first and second derivative information essential for the calculation of mechanical stresses and the computational minimization of discretized energies. For materials, whose microstructure can be well approximated in terms of laminates and where each laminate stage achieves energetic optimality with respect to the current stage, the approximate envelope coincides with the rank-one convex envelope. Although the proposed method provides only an upper bound for the rank-one convex hull, a careful examination of the resulting constraints shows a decent applicability in mechanical problems. Various aspects of the algorithm are discussed, including the restoration of rotational invariance, microstructure reconstruction, comparisons with other semi-convexification algorithms, and mesh independency. Overall, this paper demonstrates the efficiency of the algorithm for both, well-established mathematical benchmark problems as well as nonconvex isotropic finite-strain continuum damage models in two and three dimensions. Thereby, for the first time, a feasible concurrent numerical relaxation is established for an incremental, dissipative large-strain model with relevant applications in engineering problems.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16875",
        "abstract url": "https://arxiv.org/abs/2405.16875",
        "title": "Digitalization in Infrastructure Construction Projects: A PRISMA-Based Review of Benefits and Obstacles",
        "rating": "-10",
        "keywords": [],
        "abstract": "The current study presents a comprehensive review of the benefits and barriers associated with the adoption of Building Information Modeling (BIM) in infrastructure projects, focusing on the period from 2013 to 2023. The research explores the manifold advantages offered by BIM, spanning the entire project life cycle, including planning, design, construction, maintenance, and sustainability. Notably, BIM enhances collaboration, facilitates real-time data-driven decision-making, and leads to substantial cost and time savings. In parallel, a systematic literature review was conducted to identify and categorize the barriers hindering BIM adoption within the infrastructure industry. Eleven studies were selected for in-depth analysis, revealing a total of 74 obstacles. Through synthetic analysis and thematic clustering, seven primary impediments to BIM adoption were identified, encompassing challenges related to education/training, resistance to change, business value clarity, perceived cost, lack of standards and guidelines, lack of mandates, and lack of initiatives. This review explores the benefits and barriers in the industry that are facing BIM adoption in infrastructure projects, giving an important perspective toward improving effective BIM adoption strategies, policies, and standards. Future directions for research and industry development are outlined, including efforts to enhance education and training, promote standardization, advocate for policy and mandates, and integrate BIM with emerging technologies.",
        "subjects": [
            "cs.RO",
            "cs.CE",
            "cs.ET"
        ],
        "comment": "Part of Doctoral Dissertation"
    },
    {
        "paper id": "2405.16880",
        "abstract url": "https://arxiv.org/abs/2405.16880",
        "title": "Systematic Literature Review of Commercial Participation in Open Source Software",
        "rating": "-10",
        "keywords": [],
        "abstract": "Open source software (OSS) has been playing a fundamental role in not only information technology but also our social lives. Attracted by various advantages of OSS, increasing commercial companies take extensive participation in open source development and have had a broad impact. This paper provides a comprehensive systematic literature review (SLR) of existing research on company participation in OSS. We collected 92 papers and organized them based on their research topics, which cover three main directions, i.e., participation motivation, contribution model, and impact on OSS development. We found the explored motivations of companies are mainly from economic, technological, and social aspects. Existing studies categorize companies' contribution models in OSS projects mainly through their objectives and how they shape OSS communities. Researchers also explored how commercial participation affects OSS development. We conclude with research challenges and promising research directions on commercial participation in OSS. This study contributes to a comprehensive understanding of commercial participation in OSS development.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16881",
        "abstract url": "https://arxiv.org/abs/2405.16881",
        "title": "Half-duplex communication complexity with adversary can be less than the classical communication complexity",
        "rating": "-10",
        "keywords": [],
        "abstract": "Half-duplex communication complexity with adversary was defined in [Hoover, K., Impagliazzo, R., Mihajlin, I., Smal, A. V. Half-Duplex Communication Complexity, ISAAC 2018.] Half-duplex communication protocols generalize classical protocols defined by Andrew Yao in [Yao, A. C.-C. Some Complexity Questions Related to Distributive Computing (Preliminary Report), STOC 1979]. It has been unknown so far whether the communication complexities defined by these models are different or not. In the present paper we answer this question: we exhibit a function whose half-duplex communication complexity with adversary is strictly less than its classical communication complexity.",
        "subjects": [
            "cs.CC",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16893",
        "abstract url": "https://arxiv.org/abs/2405.16893",
        "title": "Cross Far- and Near-Field Channel Measurement and Modeling in Extremely Large-scale Antenna Array (ELAA) Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Technologies like ultra-massive multiple-input-multiple-output (UM-MIMO) and reconfigurable intelligent surfaces (RISs) are of special interest to meet the key performance indicators of future wireless systems including ubiquitous connectivity and lightning-fast data rates. One of their common features, the extremely large-scale antenna array (ELAA) systems with hundreds or thousands of antennas, give rise to near-field (NF) propagation and bring new challenges to channel modeling and characterization. In this paper, a cross-field channel model for ELAA systems is proposed, which improves the statistical model in 3GPP TR 38.901 by refining the propagation path with its first and last bounces and differentiating the characterization of parameters like path loss, delay, and angles in near- and far-fields. A comprehensive analysis of cross-field boundaries and closed-form expressions of corresponding NF or FF parameters are provided. Furthermore, cross-field experiments carried out in a typical indoor scenario at 300 GHz verify the variation of MPC parameters across the antenna array, and demonstrate the distinction of channels between different antenna elements. Finally, detailed generation procedures of the cross-field channel model are provided, based on which simulations and analysis on NF probabilities and channel coefficients are conducted for $4\\times4$, $8\\times8$, $16\\times16$, and $9\\times21$ uniform planar arrays at different frequency bands.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "14 pages, 33 figures"
    },
    {
        "paper id": "2405.16896",
        "abstract url": "https://arxiv.org/abs/2405.16896",
        "title": "Unconventional complexity classes in unconventional computing (extended abstract)",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many unconventional computing models, including some that appear to be quite different from traditional ones such as Turing machines, happen to characterise either the complexity class P or PSPACE when working in deterministic polynomial time (and in the maximally parallel way, where this applies). We discuss variants of cellular automata and membrane systems that escape this dichotomy and characterise intermediate complexity classes, usually defined in terms of Turing machines with oracles, as well as some possible reasons why this happens.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16911",
        "abstract url": "https://arxiv.org/abs/2405.16911",
        "title": "CycloDSP: A cyclostationary signal analysis tool for GNU Radio",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we present a first attempt to incorporate in the GNU Radio ecosystem a tool called CycloDSP, devoted to the analysis of complex-valued cyclostationary signals. Such signals are ubiquitous in communication and signal processing, exhibiting periodic or almost periodic statistics that are characterized by a countable set of cycle frequencies, which are related to the main signal periodicities. Common cycle frequencies for modulated signals are multiple of the baudrate and/or combination of the carrier frequency and baudrate. Conventional estimation strategies for cyclostationary signal analysis typically exhibit a high computational burden. Many approaches aimed at reducing complexity exploit fast Fourier transform (FFT) algorithms, which are very efficient for batch data but are not suited for continuously streaming data, typically encountered in software-defined radio (SDR) applications. The aim of this paper is to develop an out-of-tree (OOT) GNU Radio module containing a set of building block functions, aimed at estimating functions, such as the cyclic correlation functions, typically employed for cyclostationary signal analysis. The proposed implementation must be designed so as to ensure high efficiency in processing continuous data streams of complex samples, both in terms of computational load and data storage, in order to be executed on general purpose computers. We tested our implementation by estimating in real-time the second-order cyclic statistics of a Gaussian minimum-shift keying (GMSK) modulated signal, which belongs to a class of signals commonly employed in aeronautical telemetry datalinks.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "8 pages, 6 figures, 1 table, published in proceedings of 2024 IEEE Aerospace Conference (Aeroconf)"
    },
    {
        "paper id": "2405.16917",
        "abstract url": "https://arxiv.org/abs/2405.16917",
        "title": "LRAMM -- Low precision approximates GEMM via RSVD",
        "rating": "-10",
        "keywords": [],
        "abstract": "Matrix multiplication computation acceleration has been a research hotspot across various domains. Due to the characteristics of some applications, approximate matrix multiplication can achieve significant performance improvements without losing much precision. In this paper, we propose LRAMM - a high-performance matrix multiplication approximation algorithm that combines mixed-precision quantized matrix multiplication with RSVD techniques, further enhancing efficiency within the error range of low-precision matrix multiplication by utilizing matrix low-rank decomposition technology.",
        "subjects": [
            "math.NA",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16968",
        "abstract url": "https://arxiv.org/abs/2405.16968",
        "title": "Remote control system of a binary tree of switches -- II. balancing for a perfect binary tree",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study a tree coloring model introduced by Guidon (2018), initially based on an analogy with a remote control system of a rail yard, seen as switches on a binary tree. For a given binary tree, we formalize the constraints on the coloring, in particular the distribution of the nodes among colors. Following Guidon, we are interested in balanced colorings i.e. colorings which minimize the maximum size of the subsets of the tree nodes distributed by color. With his method, we present balanced colorings for trees of height up to 7. But his method seems difficult to apply for trees of greater height. Also we present another method which gives solutions for arbitrarily large trees. We illustrate it with a balanced coloring for height 8. In the appendix, we give the exact formulas and the asymptotic behavior of the number of colorings as a function of the height of the tree.",
        "subjects": [
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.16981",
        "abstract url": "https://arxiv.org/abs/2405.16981",
        "title": "Characterising Developer Sentiment in Software Components: An Exploratory Study of Gentoo",
        "rating": "-10",
        "keywords": [],
        "abstract": "Collaborative software development happens in teams, that cooperate on shared artefacts, and discuss development on online platforms. Due to the complexity of development and the variety of teams, software components often act as effective containers for parallel work and teams. Past research has shown how communication between team members, especially in an open-source environment, can become extremely toxic, and lead to members leaving the development team. This has a direct effect on the evolution and maintenance of the project in which the former members were active in. The purpose of our study is two-fold: first, we propose an approach to evaluate, at a finer granularity, the positive and negative emotions in the communication between developers; and second, we aim to characterise a project's development paths, or components, as more or less impacted by the emotions. Our analysis evaluates single sentences rather than whole messages as the finest granularity of communication. The previous study found that the high positivity or negativity at the sentence level may indirectly impact the writer him/herself, or the reader. In this way, we could highlight specific paths of Gentoo as the most affected by negative emotions, and show how negative emotions have evolved and changed along the same paths. By joining the analysis of the mailing lists, from which we derive the sentiment of the developers, with the information derived from the development logs, we obtained a longitudinal picture of how development paths have been historically affected by positive or negative emotions. Our study shows that, in recent years, negative emotions have generally decreased in the communication between Gentoo developers. We also show how file paths, as collaborative software development artefacts, were more or less impacted by the emotions of the developers.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17001",
        "abstract url": "https://arxiv.org/abs/2405.17001",
        "title": "Delta-modular ILP Problems of Bounded Co-dimension, Discrepancy, and Convolution",
        "rating": "-10",
        "keywords": [],
        "abstract": "For $k, n \\geq 0$, and $c \\in Z^n$, we consider ILP problems \\begin{gather*} \\max\\bigl\\{ c^\\top x \\colon A x = b,\\, x \\in Z^n_{\\geq 0} \\bigr\\}\\text{ with $A \\in Z^{k \\times n}$, $rank(A) = k$, $b \\in Z^{k}$ and} \\max\\bigl\\{ c^\\top x \\colon A x \\leq b,\\, x \\in Z^n \\bigr\\} \\text{ with $A \\in Z^{(n+k) \\times n}$, $rank(A) = n$, $b \\in Z^{n+k}$.} \\end{gather*} The first problem is called an \\emph{ILP problem in the standard form of the codimension $k$}, and the second problem is called an \\emph{ILP problem in the canonical form with $n+k$ constraints.} We show that, for any sufficiently large $\u0394$, both problems can be solved with $$ 2^{O(k)} \\cdot (f_{k,d} \\cdot \u0394)^2 / 2^{\u03a9\\bigl(\\sqrt{\\log(f_{k,d} \\cdot \u0394)}\\bigr)} $$ operations, where $ f_{k,d} = \\min \\Bigl\\{ k^{k/2}, \\bigl(\\log k \\cdot \\log (d + k)\\bigr)^{k/2} \\Bigr\\} $, $d$ is the dimension of a corresponding polyhedron and $\u0394$ is the maximum absolute value of $rank(A) \\times rank(A)$ sub-determinants of $A$. As our second main result, we show that the feasibility variants of both problems can be solved with $$ 2^{O(k)} \\cdot f_{k,d} \\cdot \u0394\\cdot \\log^3(f_{k,d} \\cdot \u0394) $$ operations. The constant $f_{k,d}$ can be replaced by other constant $g_{k,\u0394} = \\bigl(\\log k \\cdot \\log (k \u0394)\\bigr)^{k/2}$ that depends only on $k$ and $\u0394$. Additionally, we consider different partial cases with $k=0$ and $k=1$, which have interesting applications. As a result of independent interest, we propose an $n^2/2^{\u03a9\\bigl(\\sqrt{\\log n}\\bigr)}$-time algorithm for the tropical convolution problem on sequences, indexed by elements of a finite Abelian group of the order $n$. This result is obtained, reducing the above problem to the matrix multiplication problem on a tropical semiring and using seminal algorithm by R. Williams.",
        "subjects": [
            "cs.CC",
            "cs.DS",
            "math.AC",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17071",
        "abstract url": "https://arxiv.org/abs/2405.17071",
        "title": "Reliable Sub-Nyquist Spectrum Sensing via Conformal Risk Control",
        "rating": "-10",
        "keywords": [],
        "abstract": "Detecting occupied subbands is a key task for wireless applications such as unlicensed spectrum access. Recently, detection methods were proposed that extract per-subband features from sub-Nyquist baseband samples and then apply thresholding mechanisms based on held-out data. Such existing solutions can only provide guarantees in terms of false negative rate (FNR) in the asymptotic regime of large held-out data sets. In contrast, this work proposes a threshold mechanism-based conformal risk control (CRC), a method recently introduced in statistics. The proposed CRC-based thresholding technique formally meets user-specified FNR constraints, irrespective of the size of the held-out data set. By applying the proposed CRC-based framework to both reconstruction-based and classification-based sub-Nyquist spectrum sensing techniques, it is verified via experimental results that CRC not only provides theoretical guarantees on the FNR but also offers competitive true negative rate (TNR) performance.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "submitted for a journal publication"
    },
    {
        "paper id": "2405.17093",
        "abstract url": "https://arxiv.org/abs/2405.17093",
        "title": "DeeperImpact: Optimizing Sparse Learned Index Structures",
        "rating": "-10",
        "keywords": [],
        "abstract": "A lot of recent work has focused on sparse learned indexes that use deep neural architectures to significantly improve retrieval quality while keeping the efficiency benefits of the inverted index. While such sparse learned structures achieve effectiveness far beyond those of traditional inverted index-based rankers, there is still a gap in effectiveness to the best dense retrievers, or even to sparse methods that leverage more expensive optimizations such as query expansion and query term weighting. We focus on narrowing this gap by revisiting and optimizing DeepImpact, a sparse retrieval approach that uses DocT5Query for document expansion followed by a BERT language model to learn impact scores for document terms. We first reinvestigate the expansion process and find that the recently proposed Doc2Query query filtration does not enhance retrieval quality when used with DeepImpact. Instead, substituting T5 with a fine-tuned Llama 2 model for query prediction results in a considerable improvement. Subsequently, we study training strategies that have proven effective for other models, in particular the use of hard negatives, distillation, and pre-trained CoCondenser model initialization. Our results significantly narrow the effectiveness gap with the most effective versions of SPLADE.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17109",
        "abstract url": "https://arxiv.org/abs/2405.17109",
        "title": "Left-Linear Completion with AC Axioms",
        "rating": "-10",
        "keywords": [],
        "abstract": "We revisit completion modulo equational theories for left-linear term rewrite systems where unification modulo the theory is avoided and the normal rewrite relation can be used in order to decide validity questions. To that end, we give a new correctness proof for finite runs and establish a simulation result between the two inference systems known from the literature. Given a concrete reduction order, novel canonicity results show that the resulting complete systems are unique up to the representation of their rules' right-hand sides. Furthermore, we show how left-linear AC completion can be simulated by general AC completion. In particular, this result allows us to switch from the former to the latter at any point during a completion process.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17112",
        "abstract url": "https://arxiv.org/abs/2405.17112",
        "title": "Using continuation methods to analyse the difficulty of problems solved by Ising machines",
        "rating": "-10",
        "keywords": [],
        "abstract": "Ising machines are dedicated hardware solvers of NP-hard optimization problems. However, they do not always find the most optimal solution. The probability of finding this optimal solution depends on the problem at hand. Using continuation methods, we show that this is closely linked to the bifurcation sequence of the optimal solution. From this bifurcation analysis, we can determine the effectiveness of solution schemes. Moreover, we find that the proper choice of implementation of the Ising machine can drastically change this bifurcation sequence and therefore vastly increase the probability of finding the optimal solution.",
        "subjects": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ],
        "comment": "21 pages, 9 figures and 2 tables. At present time (27th of May 2024), this submission is under review in the journal 'Nature Communications'"
    },
    {
        "paper id": "2405.17114",
        "abstract url": "https://arxiv.org/abs/2405.17114",
        "title": "Holographic MIMO Systems, Their Channel Estimation and Performance",
        "rating": "-10",
        "keywords": [],
        "abstract": "Holographic multiple-input multiple-output (MIMO) systems constitute a promising technology in support of next-generation wireless communications, thus paving the way for a smart programmable radio environment. However, despite its significant potential, further fundamental issues remain to be addressed, such as the acquisition of accurate channel information. Indeed, the conventional angular-domain channel representation is no longer adequate for characterizing the sparsity inherent in holographic MIMO channels. To fill this knowledge gap, in this article, we conceive a decomposition and reconstruction (DeRe)-based framework for facilitating the estimation of sparse channels in holographic MIMOs. In particular, the channel parameters involved in the steering vector, namely the azimuth and elevation angles plus the distance (AED), are decomposed for independently constructing their own covariance matrices. Then, the acquisition of each parameter can be formulated as a compressive sensing (CS) problem by harnessing the covariance matrix associated with each individual parameter. We demonstrate that our solution exhibits an improved performance and imposes a reduced pilot overhead, despite its reduced complexity. Finally, promising open research topics are highlighted to bridge the gap between the theory and the practical employment of holographic MIMO schemes.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "This article has been accepted for publication in IEEE VTM"
    },
    {
        "paper id": "2405.17147",
        "abstract url": "https://arxiv.org/abs/2405.17147",
        "title": "Large Language Models (LLMs): Deployment, Tokenomics and Sustainability",
        "rating": "-10",
        "keywords": [],
        "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly impacted human-computer interaction, epitomized by the release of GPT-4o, which introduced comprehensive multi-modality capabilities. In this paper, we first explored the deployment strategies, economic considerations, and sustainability challenges associated with the state-of-the-art LLMs. More specifically, we discussed the deployment debate between Retrieval-Augmented Generation (RAG) and fine-tuning, highlighting their respective advantages and limitations. After that, we quantitatively analyzed the requirement of xPUs in training and inference. Additionally, for the tokenomics of LLM services, we examined the balance between performance and cost from the quality of experience (QoE)'s perspective of end users. Lastly, we envisioned the future hybrid architecture of LLM processing and its corresponding sustainability concerns, particularly in the environmental carbon footprint impact. Through these discussions, we provided a comprehensive overview of the operational and strategic considerations essential for the responsible development and deployment of LLMs.",
        "subjects": [
            "cs.MM"
        ],
        "comment": "Accepted by IEEE CTSoc-NCT"
    },
    {
        "paper id": "2405.17180",
        "abstract url": "https://arxiv.org/abs/2405.17180",
        "title": "Boolean Gates Based on Liquid Marbles",
        "rating": "-10",
        "keywords": [],
        "abstract": "Liquid Marbles are liquid droplets encapsulated by hydrophobic powder particles. They offer an efficient approach to handling liquids due to their non-wetting nature. In this work, starting from the interaction gate proposed in the literature, we describe how the logic gates AND, XOR, OR, NOT, NAND, and NOR could be realized. Given the irreversibility and non-conservativeness of classical gates, we also discuss a possible implementation of the Toffoli gate, a reversible gate, and of the Fredkin gate, a reversible and conservative gate.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17184",
        "abstract url": "https://arxiv.org/abs/2405.17184",
        "title": "A Pioneering Roadmap for ML-Driven Algorithmic Advancements in Electrical Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "To advance control, operation and planning tools of electrical networks with ML is not straightforward. 110 experts were surveyed showing where and how ML algorithmis could advance. This paper assesses this survey and research environment. Then it develops an innovation roadmap that helps align our research community towards a goal-oriented realisation of the opportunities that AI upholds. This paper finds that the R\\&D environment of system operators (and the surrounding research ecosystem) needs adaptation to enable faster developments with AI while maintaining high testing quality and safety. This roadmap may interest research centre managers in system operators, academics, and labs dedicated to advancing the next generation of tooling for electrical networks.",
        "subjects": [
            "eess.SY",
            "eess.SP"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2405.17190",
        "abstract url": "https://arxiv.org/abs/2405.17190",
        "title": "SoK: Leveraging Transformers for Malware Analysis",
        "rating": "-10",
        "keywords": [],
        "abstract": "The introduction of transformers has been an important breakthrough for AI research and application as transformers are the foundation behind Generative AI. A promising application domain for transformers is cybersecurity, in particular the malware domain analysis. The reason is the flexibility of the transformer models in handling long sequential features and understanding contextual relationships. However, as the use of transformers for malware analysis is still in the infancy stage, it is critical to evaluate, systematize, and contextualize existing literature to foster future research. This Systematization of Knowledge (SoK) paper aims to provide a comprehensive analysis of transformer-based approaches designed for malware analysis. Based on our systematic analysis of existing knowledge, we structure and propose taxonomies based on: (a) how different transformers are adapted, organized, and modified across various use cases; and (b) how diverse feature types and their representation capabilities are reflected. We also provide an inventory of datasets used to explore multiple research avenues in the use of transformers for malware analysis and discuss open challenges with future research directions. We believe that this SoK paper will assist the research community in gaining detailed insights from existing work and will serve as a foundational resource for implementing novel research using transformers for malware analysis.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17196",
        "abstract url": "https://arxiv.org/abs/2405.17196",
        "title": "Instability and Efficiency of Non-cooperative Games",
        "rating": "-10",
        "keywords": [],
        "abstract": "It is well known that a non-cooperative game may have multiple equilibria. In this paper we consider the efficiency of games, measured by the ratio between the aggregate payoff over all Nash equilibria and that over all admissible controls. Such efficiency operator is typically unstable with respect to small perturbation of the game. This seemingly bad property can actually be a good news in practice: it is possible that a small change of the game mechanism may improve the efficiency of the game dramatically. We shall introduce a game mediator with limited resources and investigate the mechanism designs aiming to improve the efficiency.",
        "subjects": [
            "cs.GT",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17199",
        "abstract url": "https://arxiv.org/abs/2405.17199",
        "title": "Physically Consistent Modeling & Identification of Nonlinear Friction with Dissipative Gaussian Processes",
        "rating": "-10",
        "keywords": [],
        "abstract": "Friction modeling has always been a challenging problem due to the complexity of real physical systems. Although a few state-of-the-art structured data-driven methods show their efficiency in nonlinear system modeling, deterministic passivity as one of the significant characteristics of friction is rarely considered in these methods. To address this issue, we propose a Gaussian Process based model that preserves the inherent structural properties such as passivity. A matrix-vector physical structure is considered in our approaches to ensure physical consistency, in particular, enabling a guarantee of positive semi-definiteness of the damping matrix. An aircraft benchmark simulation is employed to demonstrate the efficacy of our methodology. Estimation accuracy and data efficiency are increased substantially by considering and enforcing more structured physical knowledge. Also, the fulfillment of the dissipative nature of the aerodynamics is validated numerically.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "accepted by L4DC 2024"
    },
    {
        "paper id": "2405.17227",
        "abstract url": "https://arxiv.org/abs/2405.17227",
        "title": "Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL). Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces. Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their extensive data requirements. We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC). The policy efficiently learns high-level locomotion strategies, such as gait selection and step positioning, without the need for full dynamics simulations. This control architecture enables humanoid robots to dynamically navigate discrete terrains, making strategic locomotion decisions (e.g., walking, jumping, and leaping) based on ground height maps. Our results demonstrate that this integrated control architecture achieves dynamic locomotion with significantly fewer training samples than conventional RL-based methods and can be transferred to different humanoid platforms without additional training. The control architecture has been extensively tested in dynamic simulations, accomplishing terrain height-based dynamic locomotion for three different robots.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17229",
        "abstract url": "https://arxiv.org/abs/2405.17229",
        "title": "InsigHTable: Insight-driven Hierarchical Table Visualization with Reinforcement Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Embedding visual representations within original hierarchical tables can mitigate additional cognitive load stemming from the division of users' attention. The created hierarchical table visualizations can help users understand and explore complex data with multi-level attributes. However, because of many options available for transforming hierarchical tables and selecting subsets for embedding, the design space of hierarchical table visualizations becomes vast, and the construction process turns out to be tedious, hindering users from constructing hierarchical table visualizations with many data insights efficiently. We propose InsigHTable, a mixed-initiative and insight-driven hierarchical table transformation and visualization system. We first define data insights within hierarchical tables, which consider the hierarchical structure in the table headers. Since hierarchical table visualization construction is a sequential decision-making process, InsigHTable integrates a deep reinforcement learning framework incorporating an auxiliary rewards mechanism. This mechanism addresses the challenge of sparse rewards in constructing hierarchical table visualizations. Within the deep reinforcement learning framework, the agent continuously optimizes its decision-making process to create hierarchical table visualizations to uncover more insights by collaborating with analysts. We demonstrate the usability and effectiveness of InsigHTable through two case studies and sets of experiments. The results validate the effectiveness of the deep reinforcement learning framework and show that InsigHTable can facilitate users to construct hierarchical table visualizations and understand underlying data insights.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17238",
        "abstract url": "https://arxiv.org/abs/2405.17238",
        "title": "LLM-Assisted Static Analysis for Detecting Security Vulnerabilities",
        "rating": "-10",
        "keywords": [],
        "abstract": "Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice. While large language models (or LLMs) have shown impressive code generation capabilities, they cannot do complex reasoning over code to detect such vulnerabilities, especially because this task requires whole-repository analysis. In this work, we propose IRIS, the first approach that systematically combines LLMs with static analysis to perform whole-repository reasoning to detect security vulnerabilities. We curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. These projects are complex, with an average of 300,000 lines of code and a maximum of up to 7 million. Out of 120 vulnerabilities in CWE-Bench-Java, IRIS detects 69 using GPT-4, while the state-of-the-art static analysis tool only detects 27. Further, IRIS also significantly reduces the number of false alarms (by more than 80% in the best case).",
        "subjects": [
            "cs.CR",
            "cs.PL",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17263",
        "abstract url": "https://arxiv.org/abs/2405.17263",
        "title": "ReStorEdge: An edge computing system with reuse semantics",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper investigates an edge computing system where requests are processed by a set of replicated edge servers. We investigate a class of applications where similar queries produce identical results. To reduce processing overhead on the edge servers we store the results of previous computations and return them when new queries are sufficiently similar to earlier ones that produced the results, avoiding the necessity of processing every new query. We implement a similarity-based data classification system, which we evaluate based on real-world datasets of images and voice queries. We evaluate a range of orchestration strategies to distribute queries and cached results between edge nodes and show that the throughput of queries over a system of distributed edge nodes can be increased by 25-33%, increasing its capacity for higher workloads.",
        "subjects": [
            "cs.ET",
            "cs.DB",
            "cs.DC",
            "cs.NI"
        ],
        "comment": "12 pages, submitted for publication in IEEE Transactions on Emerging Topics in Computing"
    },
    {
        "paper id": "2405.17291",
        "abstract url": "https://arxiv.org/abs/2405.17291",
        "title": "Revised Optimal design of power electronic transformer based on hybrid MMC under over-modulation operation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The bridge arm of the hybrid modular multilevel converter (MMC) is composed of half-bridge and full-bridge sub-modules cascaded together. Compared with the half-bridge MMC, it can operate in the boost-AC mode, where the modulation index can be higher than 1, and the DC voltage and the AC voltage level are no longer mutually constrained; compared with the full-bridge MMC, it has lower switching device costs and losses. When the hybrid MMC boost-AC mode is used in the power electronic transformer, the degree of freedom in system design is improved, and the cost and volume of the power electronic transformer system can be further reduced. This paper analyzes how to make full use of the newly added modulation index of freedom introduced by the boost-AC hybrid MMC to optimize the power electronic transformer system, and finally gives the optimal modulation index selection scheme of the hybrid MMC for different optimization objectives.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2405.17295",
        "abstract url": "https://arxiv.org/abs/2405.17295",
        "title": "In-sensor Computing ANN Capacitive Sensors",
        "rating": "-10",
        "keywords": [],
        "abstract": "This letter proposes an in-sensor computing multiply-and-accumulate (MAC) circuit based on capacitance. The MAC circuits can constitute an artificial neural network(ANN) layer and be operated as ANN classifiers and autoencoders. The proposed circuit is a promising scheme for capacitive ANN image sensors, showing competitively high efficiency and lower power.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17307",
        "abstract url": "https://arxiv.org/abs/2405.17307",
        "title": "Peer2PIR: Private Queries for IPFS",
        "rating": "-10",
        "keywords": [],
        "abstract": "The InterPlanetary File System (IPFS) is a peer-to-peer network for storing data in a distributed file system, hosting over 190,000 peers spanning 152 countries. Despite its prominence, the privacy properties that IPFS offers to peers are severely limited. Any query within the network leaks to other peers the content for which a peer is querying. We address IPFS' privacy leakage across three functionalities (peer routing, provider advertisements, and content retrieval), ultimately empowering peers to privately navigate and retrieve content in the network. We argue that private information retrieval (PIR) is the most suitable tool for our task. Our work highlights and addresses novel challenges inherent to integrating PIR into distributed systems. We present our new, private protocols and demonstrate that they incur minimal overheads compared to IPFS today. We also include a systematic comparison of state-of-art PIR protocols in the context of distributed systems which may be of independent interest.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17322",
        "abstract url": "https://arxiv.org/abs/2405.17322",
        "title": "Evaluation of computational and energy performance in matrix multiplication algorithms on CPU and GPU using MKL, cuBLAS and SYCL",
        "rating": "-10",
        "keywords": [],
        "abstract": "Matrix multiplication is fundamental in the backpropagation algorithm used to train deep neural network models. Libraries like Intel's MKL or NVIDIA's cuBLAS implemented new and optimized matrix multiplication techniques that increase performance and reduce computational costs. These techniques can also be implemented in CUDA and SYCL and functions with AVX2 and AVX512 instructions, which have lower performance but better precision. The study compares execution times and power consumption using PAPI and PERF and compares accuracy for different matrix sizes. Comparisons were made on architectures such as third and fourth-generation Intel CPUs and NVIDIA V100 and A100 GPUs. The MKL library showed the best performance with a slight loss of precision, while OpenMP and SYCL on the CPU implementation showed the best accuracy but a loss of performance. On the other hand, the results on GPU showed that cuBLAS with tensor cores had the best performance; however, it had a cost in accuracy. The cuBLAS library without these specialized cores shows minimal performance loss and much higher accuracy. The data obtained on different architectures showed that the CPU could achieve performance close to that obtained on the GPU with increased power consumption. These results are conditional on certain hardware specifications, such as the number of cores, clock frequency, processor generation for the CPU, and the speed and bandwidth of the PCI bus and device architecture (compute capability) for the GPU.",
        "subjects": [
            "cs.DC",
            "cs.AR"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2405.17329",
        "abstract url": "https://arxiv.org/abs/2405.17329",
        "title": "Joint MIMO Transceiver and Reflector Design for Reconfigurable Intelligent Surface-Assisted Communication",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we consider a reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output communication system with multiple antennas at both the base station (BS) and the user. We plan to maximize the achievable rate through jointly optimizing the transmit precoding matrix, the receive combining matrix, and the RIS reflection matrix under the constraints of the transmit power at the BS and the unit-modulus reflection at the RIS. Regarding the non-trivial problem form, we initially reformulate it into an considerable problem to make it tractable by utilizing the relationship between the achievable rate and the weighted minimum mean squared error. Next, the transmit precoding matrix, the receive combining matrix, and the RIS reflection matrix are alternately optimized. In particular, the optimal transmit precoding matrix and receive combining matrix are obtained in closed forms. Furthermore, a pair of computationally efficient methods are proposed for the RIS reflection matrix, namely the semi-definite relaxation (SDR) method and the successive closed form (SCF) method. We theoretically prove that both methods are ensured to converge, and the SCF-based algorithm is able to converges to a Karush-Kuhn-Tucker point of the problem.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "14 pages, 12 figures"
    },
    {
        "paper id": "2405.17334",
        "abstract url": "https://arxiv.org/abs/2405.17334",
        "title": "Serial Monopoly on Blockchains with Quasi-patient Users",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces and investigates an extension of the price dynamics in serial monopoly blockchain described in Nisan [Nis23], tailored to accommodate quasi-patient users. Our model reflects users' diminishing interest in having their transactions added to the ledger over time, resulting in only a fraction $\u03b4$ of the current demand persisting in the subsequent round. The framework presented by Lavi et al. [LSZ22], where users are impatient and derive utility only from immediate transaction inclusion in the next block, corresponds to $\u03b4=0$. Fully patient users who wait forever as in [Nis23], correspond to $\u03b4=1$ in our model. This work provides new bounds on the price dynamics for the more interesting case $\u03b4\\in(0,1)$, showing somewhat unexpected effects on the dynamics itself. While the dynamics for the fully patient case is essentially \"oblivious\" of the structure of the daily demand curve, this is no longer true for finite $\u03b4< 1$. Moreover, the dynamics undergoes a \"transition phase\" where for some $\u03b4$ it behaves as in the fully patient setting ($\u03b4=1$), and for some smaller values $\u03b4'<\u03b4$ it stops \"oscillating\" and stays at the highest (\"monopolist\") price. We provide quantitative bounds and analytical results that apply to different demand functions showing that the bounds for $\u03b4=1$ are not tight in general, for $\u03b4<1$. These provide guarantees on the minimum (\"admission\") price such that transaction willing to pay that price are eventually included (and those who do not want are never included).",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17342",
        "abstract url": "https://arxiv.org/abs/2405.17342",
        "title": "Measuring Exploration: Review and Systematic Evaluation of Modelling to Generate Alternatives Methods in Macro-Energy Systems Planning Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "As decarbonization agendas mature, macro-energy systems modelling studies have increasingly focused on enhanced decision support methods that move beyond least-cost modelling to improve consideration of additional objectives and tradeoffs. One candidate is Modeling to Generate Alternatives (MGA), which systematically explores new objectives without explicit stakeholder elicitation. Previous literature lacks both a comprehensive review of MGA vector selection methods in large-scale energy system models and comparative testing of their relative efficacies in this setting. To fill this gap, this paper provides a comprehensive review of the MGA literature, identifying at least seven MGA vector selection methodologies and carrying out a systematic evaluation of four: Hop-Skip-Jump, Random Vector, Variable Min/Max, and Modelling All Alternatives. We examine each method's runtime, parallelizability, new solution discovery efficiency, and spatial exploration in lower dimensional (N <= 100) spaces, as well as spatial exploration in a three-zone, 8760-hour capacity expansion model case. Through these tests, we find Random Vector provides the broadest exploration of the near-optimal feasible region and Variable Min/Max provides the most extreme results, while the two tie on computational speed. We thus propose a new Hybrid vector selection approach combining the two methods to take advantage of the strengths of each. Additional analysis is provided on MGA variable selection, in which we demonstrate MGA problems formulated over generation variables fail to retain cost-optimal dispatch and are thus not reflective of real operations of equivalent hypothetical capacity choices. As such, we recommend future studies utilize a parallelized combined vector approach over the set of capacity variables for best results in computational speed and spatial exploration while retaining optimal dispatch.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17365",
        "abstract url": "https://arxiv.org/abs/2405.17365",
        "title": "DR-CGRA: Supporting Loop-Carried Dependencies in CGRAs Without Spilling Intermediate Values",
        "rating": "-10",
        "keywords": [],
        "abstract": "Coarse-grain reconfigurable architectures (CGRAs) are gaining traction thanks to their performance and power efficiency. Utilizing CGRAs to accelerate the execution of tight loops holds great potential for achieving significant overall performance gains, as a substantial portion of program execution time is dedicated to tight loops. But loop parallelization using CGRAs is challenging because of loop-carried data dependencies. Traditionally, loop-carried dependencies are handled by spilling dependent values out of the reconfigurable array to a memory medium and then feeding them back to the grid. Spilling the values and feeding them back into the grid imposes additional latencies and logic that impede performance and limit parallelism. In this paper, we present the Dependency Resolved CGRA (DR-CGRA) architecture that is designed to accelerate the execution of tight loops. DR-CGRA, which is based on a massively-multithreaded CGRA, runs each iteration as a separate CGRA thread and maps loop-carried data dependencies to inter-thread communication inside the grid. This design ensures the passage of data-dependent values across loop iterations without spilling them out of the grid. The proposed DR-CGRA architecture was evaluated on various SPEC CPU 2017 benchmarks. The results demonstrated significant performance improvements, with an average speedup ranging from 2.1 to 4.5 and an overall average of 3.1 when compared to state-of-the-art CGRA architecture.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17528",
        "abstract url": "https://arxiv.org/abs/2405.17528",
        "title": "The logistic queue model: theoretical properties and performance evaluation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The advent of digital twins (DT) for the control and management of communication networks requires accurate and fast methods to estimate key performance indicators (KPI) needed for autonomous decision-making. Among several alternatives, queuing theory can be applied to model a real network as a queue system that propagates entities representing network traffic. By using fluid flow queue simulation and numerical methods, a good trade-off between accuracy and execution time can be obtained. In this work, we present the formal derivation and mathematical properties of a continuous fluid flow queuing model called the logistic queue model. We give novel proofs showing that this queue model has all the theoretical properties one should expect such as positivity of the queue and first-in first-out (FIFO) property. Moreover, extensions are presented in order to model different characteristics of telecommunication networks, including finite buffer sizes and propagation of flows with different priorities. Numerical results are presented to validate the accuracy and improved performance of our approach in contrast to traditional discrete event simulation, using synthetic traffic generated with the characteristics of real captured network traffic. Finally, we evaluate a DT built using a queue system based on the logistic queue model and demonstrate its applicability to estimate KPIs of an emulated real network under different traffic conditions.",
        "subjects": [
            "cs.NI",
            "cs.CE"
        ],
        "comment": "36 pages, 12 figures, 2 tables"
    },
    {
        "paper id": "2405.17594",
        "abstract url": "https://arxiv.org/abs/2405.17594",
        "title": "Towards Achieving Cooperation Compliance of Human Drivers in Mixed Traffic",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider a mixed-traffic environment in transportation systems, where Connected and Automated Vehicles (CAVs) coexist with potentially non-cooperative Human-Driven Vehicles (HDVs). We develop a cooperation compliance control framework to incentivize HDVs to align their behavior with socially optimal objectives using a ``refundable toll'' scheme so as to achieve a desired compliance probability for all non-compliant HDVs through a feedback control mechanism combining global with local (individual) components. We apply this scheme to the lane-changing problem, where a ``Social Planner'' provides references to the HDVs, measures their state errors, and induces cooperation compliance for safe lane-changing through a refundable toll approach. Simulation results are included to show the effectiveness of our cooperation compliance controller in terms of improved compliance and lane-changing maneuver safety and efficiency when non-cooperative HDVs are present.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17595",
        "abstract url": "https://arxiv.org/abs/2405.17595",
        "title": "Element-Free Probability Distributions and Random Partitions",
        "rating": "-10",
        "keywords": [],
        "abstract": "An \"element-free\" probability distribution is what remains of a probability distribution after we forget the elements to which the probabilities were assigned. These objects naturally arise in Bayesian statistics, in situations where elements are used as labels and their specific identity is not important. This paper develops the structural theory of element-free distributions, using multisets and category theory. We give operations for moving between element-free and ordinary distributions, and we show that these operations commute with multinomial sampling. We then exploit this theory to prove two representation theorems. These theorems show that element-free distributions provide a natural representation for key random structures in Bayesian nonparametric clustering: exchangeable random partitions, and random distributions parametrized by a base measure.",
        "subjects": [
            "cs.LO",
            "math.CT",
            "math.PR",
            "math.ST"
        ],
        "comment": "To appear at LICS 2024"
    },
    {
        "paper id": "2405.17599",
        "abstract url": "https://arxiv.org/abs/2405.17599",
        "title": "A Mobility Equity Metric for Multi-Modal Intelligent Transportation Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we introduce a metric to evaluate the equity in mobility and a routing framework to enhance the metric within multi-modal intelligent transportation systems. The mobility equity metric (MEM) simultaneously accounts for service accessibility and transportation costs to quantify the equity and fairness in a transportation network. Finally, we develop a system planner integrated with MEM that aims to distribute travel demand for the transportation network, resulting in a socially optimal mobility system. Our framework results in a transportation network that is efficient in terms of travel time, improves accessibility, and ensures equity in transportation.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 7 figures"
    },
    {
        "paper id": "2405.17636",
        "abstract url": "https://arxiv.org/abs/2405.17636",
        "title": "Single-Fiber Optical Frequency Domain Reflectometry Shape Sensing of Continuum Manipulators with Planar Bending",
        "rating": "-10",
        "keywords": [],
        "abstract": "To address the challenges associated with shape sensing of continuum manipulators (CMs) using Fiber Bragg Grating (FBG) optical fibers, we feature a unique shape sensing assembly utilizing solely a single Optical Frequency Domain Reflectometry (OFDR) fiber attached to a flat nitinol wire (NiTi). Integrating this easy-to-manufacture unique sensor with a long and soft CM with 170 mm length, we performed different experiments to evaluate its shape reconstruction ability. Results demonstrate phenomenal shape reconstruction accuracy for both C-shape (< 2 mm tip error, < 1.2 mm shape error) and J-shape (< 3.4 mm tip error, < 2.3 mm shape error) experiments.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17664",
        "abstract url": "https://arxiv.org/abs/2405.17664",
        "title": "Adaptive Device-Edge Collaboration on DNN Inference in AIoT: A Digital Twin-Assisted Approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "Device-edge collaboration on deep neural network (DNN) inference is a promising approach to efficiently utilizing network resources for supporting artificial intelligence of things (AIoT) applications. In this paper, we propose a novel digital twin (DT)-assisted approach to device-edge collaboration on DNN inference that determines whether and when to stop local inference at a device and upload the intermediate results to complete the inference on an edge server. Instead of determining the collaboration for each DNN inference task only upon its generation, multi-step decision-making is performed during the on-device inference to adapt to the dynamic computing workload status at the device and the edge server. To enhance the adaptivity, a DT is constructed to evaluate all potential offloading decisions for each DNN inference task, which provides augmented training data for a machine learning-assisted decision-making algorithm. Then, another DT is constructed to estimate the inference status at the device to avoid frequently fetching the status information from the device, thus reducing the signaling overhead. We also derive necessary conditions for optimal offloading decisions to reduce the offloading decision space. Simulation results demon-strate the outstanding performance of our DT-assisted approach in terms of balancing the tradeoff among inference accuracy, delay, and energy consumption.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17685",
        "abstract url": "https://arxiv.org/abs/2405.17685",
        "title": "Cryogenic Characterization of Low-Frequency Noise in 40-nm CMOS",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents an extensive characterization of the low-frequency noise (LFN) at room temperature (RT) and cryogenic temperature (4.2 K) of 40-nm bulk-CMOS transistors. The noise is measured over a wide range of bias conditions and geometries to generate a comprehensive overview of LFN in this technology. While the RT results are in-line with the literature and the foundry models, the cryogenic behavior diverges in many aspects. These deviations include changes with respect to RT in magnitude and bias dependence that are conditional on transistor type and geometry, and even an additional systematic Lorentzian feature that is common among individual devices. Furthermore, we find the scaling of the average LFN with the area and its variability to be similar between RT and 4.2 K, with the cryogenic scaling reported systematically for the first time. The findings suggest that, as no consistent decrease of LFN at lower temperatures is observed while the white noise is reduced, the impact of LFN for precision analog design at cryogenic temperatures gains a more predominant role.",
        "subjects": [
            "physics.app-ph",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17690",
        "abstract url": "https://arxiv.org/abs/2405.17690",
        "title": "Data Makes Better Data Scientists",
        "rating": "-10",
        "keywords": [],
        "abstract": "With the goal of identifying common practices in data science projects, this paper proposes a framework for logging and understanding incremental code executions in Jupyter notebooks. This framework aims to allow reasoning about how insights are generated in data science and extract key observations into best data science practices in the wild. In this paper, we show an early prototype of this framework and ran an experiment to log a machine learning project for 25 undergraduate students.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17692",
        "abstract url": "https://arxiv.org/abs/2405.17692",
        "title": "Fully Subexponential Time Approximation Scheme for Product Partition",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper we study the Product Partition Problem (PPP), i.e. we are given a set of $n$ natural numbers represented on $m$ bits each and we are asked if a subset exists such that the product of the numbers in the subset equals the product of the numbers not in the subset. Our approach is to obtain the integer factorization of each number. This is the subexponential step. We then form a matrix with the exponents of the primes and propose a novel procedure which modifies the given numbers in such a way that their integer factorization contains sufficient primes to facilitate the search for the solution to the partition problem, while maintaining a similar product. We show that the required time and memory to run the proposed algorithm is subexponential.",
        "subjects": [
            "cs.DS",
            "math.OC"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2405.12555"
    },
    {
        "paper id": "2405.17694",
        "abstract url": "https://arxiv.org/abs/2405.17694",
        "title": "Bias Detection Via Signaling",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior. In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior. Since we often cannot observe the agent's beliefs directly, we take an approach inspired by information design. Specifically, we measure an agent's bias by designing a signaling scheme and observing the actions they take in response to different signals, assuming that they are maximizing their own expected utility; our goal is to detect bias with a minimum number of signals. Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17701",
        "abstract url": "https://arxiv.org/abs/2405.17701",
        "title": "Compression and In-Situ Query Processing for Fine-Grained Array Lineage",
        "rating": "-10",
        "keywords": [],
        "abstract": "Tracking data lineage is important for data integrity, reproducibility, and debugging data science workflows. However, fine-grained lineage (i.e., at a cell level) is challenging to store, even for the smallest datasets. This paper introduces DSLog, a storage system that efficiently stores, indexes, and queries array data lineage, agnostic to capture methodology. A main contribution is our new compression algorithm, named ProvRC, that compresses captured lineage relationships. Using ProvRC for lineage compression result in a significant storage reduction over functions with simple spatial regularity, beating alternative columnar-store baselines by up to 2000x}. We also show that ProvRC facilitates in-situ query processing that allows forward and backward lineage queries without decompression - in the optimal case, surpassing baselines by 20x in query latency on random numpy pipelines.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17723",
        "abstract url": "https://arxiv.org/abs/2405.17723",
        "title": "TableDC: Deep Clustering for Tabular Data",
        "rating": "-10",
        "keywords": [],
        "abstract": "Deep clustering (DC), a fusion of deep representation learning and clustering, has recently demonstrated positive results in data science, particularly text processing and computer vision. However, joint optimization of feature learning and data distribution in the multi-dimensional space is domain-specific, so existing DC methods struggle to generalize to other application domains (such as data integration and cleaning). In data management tasks, where high-density embeddings and overlapping clusters dominate, a data management-specific DC algorithm should be able to interact better with the data properties for supporting data cleaning and integration tasks. This paper presents a deep clustering algorithm for tabular data (TableDC) that reflects the properties of data management applications, particularly schema inference, entity resolution, and domain discovery. To address overlapping clusters, TableDC integrates Mahalanobis distance, which considers variance and correlation within the data, offering a similarity method suitable for tables, rows, or columns in high-dimensional latent spaces. TableDC provides flexibility for the final clustering assignment and shows higher tolerance to outliers through its heavy-tailed Cauchy distribution as the similarity kernel. The proposed similarity measure is particularly beneficial where the embeddings of raw data are densely packed and exhibit high degrees of overlap. Data cleaning tasks may involve a large number of clusters, which affects the scalability of existing DC methods. TableDC's self-supervised module efficiently learns data embeddings with a large number of clusters compared to existing benchmarks, which scale in quadratic time. We evaluated TableDC with several existing DC, Standard Clustering (SC), and state-of-the-art bespoke methods over benchmark datasets. TableDC consistently outperforms existing DC, SC, and bespoke methods.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17731",
        "abstract url": "https://arxiv.org/abs/2405.17731",
        "title": "Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the era of big data, conventional RDBMS models have become impractical for handling colossal workloads. Consequently, NoSQL databases have emerged as the preferred storage solutions for executing processing-intensive Online Analytical Processing (OLAP) tasks. Within the realm of NoSQL databases, various classifications exist based on their data storage mechanisms, making it challenging to select the most suitable one for a given OLAP workload. While each NoSQL database boasts distinct advantages, inherent scalability, adaptability to diverse data formats, and high data availability are universally recognized benefits crucial for managing OLAP workloads effectively. Existing research predominantly evaluates individual databases within custom data pipeline setups, lacking a standardized approach for comparative analysis across different databases to identify the optimal data pipeline for OLAP workloads. In this paper, we present our experimental insights into how various NoSQL databases handle OLAP workloads within a standardized data processing pipeline. Our experimental pipeline comprises Apache Spark for large-scale transformations, data cleansing, and schema normalization, diverse NoSQL databases as data stores, and a Business Intelligence tool for data analysis and visualization.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17777",
        "abstract url": "https://arxiv.org/abs/2405.17777",
        "title": "RREH: Reconstruction Relations Embedded Hashing for Semi-Paired Cross-Modal Retrieval",
        "rating": "-10",
        "keywords": [],
        "abstract": "Known for efficient computation and easy storage, hashing has been extensively explored in cross-modal retrieval. The majority of current hashing models are predicated on the premise of a direct one-to-one mapping between data points. However, in real practice, data correspondence across modalities may be partially provided. In this research, we introduce an innovative unsupervised hashing technique designed for semi-paired cross-modal retrieval tasks, named Reconstruction Relations Embedded Hashing (RREH). RREH assumes that multi-modal data share a common subspace. For paired data, RREH explores the latent consistent information of heterogeneous modalities by seeking a shared representation. For unpaired data, to effectively capture the latent discriminative features, the high-order relationships between unpaired data and anchors are embedded into the latent subspace, which are computed by efficient linear reconstruction. The anchors are sampled from paired data, which improves the efficiency of hash learning. The RREH trains the underlying features and the binary encodings in a unified framework with high-order reconstruction relations preserved. With the well devised objective function and discrete optimization algorithm, RREH is designed to be scalable, making it suitable for large-scale datasets and facilitating efficient cross-modal retrieval. In the evaluation process, the proposed is tested with partially paired data to establish its superiority over several existing methods.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by the 20th International Conference on Intelligent Computing (ICIC 2024)"
    },
    {
        "paper id": "2405.17789",
        "abstract url": "https://arxiv.org/abs/2405.17789",
        "title": "On the Downlink Average Energy Efficiency of Non-Stationary XL-MIMO",
        "rating": "-10",
        "keywords": [],
        "abstract": "Extra large-scale multiple-input multiple-output (XL-MIMO) is a key technology for future wireless communication systems. This paper considers the effects of visibility region (VR) at the base station (BS) in a non-stationary multi-user XL-MIMO scenario, where only partial antennas can receive users' signal. In time division duplexing (TDD) mode, we first estimate the VR at the BS by detecting the energy of the received signal during uplink training phase. The probabilities of two detection errors are derived and the uplink channel on the detected VR is estimated. In downlink data transmission, to avoid cumbersome Monte-Carlo trials, we derive a deterministic approximate expression for ergodic {average energy efficiency (EE)} with the regularized zero-forcing (RZF) precoding. In frequency division duplexing (FDD) mode, the VR is estimated in uplink training and then the channel information of detected VR is acquired from the feedback channel. In downlink data transmission, the approximation of ergodic average {EE} is also derived with the RZF precoding. Invoking approximate results, we propose an alternate optimization algorithm to design the detection threshold and the pilot length in both TDD and FDD modes. The numerical results reveal the impacts of VR estimation error on ergodic average {EE} and demonstrate the effectiveness of our proposed algorithm.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "13 pages, 11 figures"
    },
    {
        "paper id": "2405.17794",
        "abstract url": "https://arxiv.org/abs/2405.17794",
        "title": "LNS2+RL: Combining Multi-agent Reinforcement Learning with Large Neighborhood Search in Multi-agent Path Finding",
        "rating": "-10",
        "keywords": [],
        "abstract": "Multi-Agent Path Finding (MAPF) is a critical component of logistics and warehouse management, which focuses on planning collision-free paths for a team of robots in a known environment. Recent work introduced a novel MAPF approach, LNS2, which proposed to repair a quickly-obtainable set of infeasible paths via iterative re-planning, by relying on a fast, yet lower-quality, priority-based planner. At the same time, there has been a recent push for Multi-Agent Reinforcement Learning (MARL) based MAPF algorithms, which let agents learn decentralized policies that exhibit improved cooperation over such priority planning, although inevitably remaining slower. In this paper, we introduce a new MAPF algorithm, LNS2+RL, which combines the distinct yet complementary characteristics of LNS2 and MARL to effectively balance their individual limitations and get the best from both worlds. During early iterations, LNS2+RL relies on MARL for low-level re-planning, which we show eliminates collisions much more than a priority-based planner. There, our MARL-based planner allows agents to reason about past and future/predicted information to gradually learn cooperative decision-making through a finely designed curriculum learning. At later stages of planning, LNS2+RL adaptively switches to priority-based planning to quickly resolve the remaining collisions, naturally trading-off solution quality and computational efficiency. Our comprehensive experiments on challenging tasks across various team sizes, world sizes, and map structures consistently demonstrate the superior performance of LNS2+RL compared to many MAPF algorithms, including LNS2, LaCAM, and EECBS, where LNS2+RL shows significantly better performance in complex scenarios. We finally experimentally validate our algorithm in a hybrid simulation of a warehouse mockup involving a team of 100 (real-world and simulated) robots.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17798",
        "abstract url": "https://arxiv.org/abs/2405.17798",
        "title": "Learn to be Fair without Labels: a Distribution-based Learning Framework for Fair Ranking",
        "rating": "-10",
        "keywords": [],
        "abstract": "Ranking algorithms as an essential component of retrieval systems have been constantly improved in previous studies, especially regarding relevance-based utilities. In recent years, more and more research attempts have been proposed regarding fairness in rankings due to increasing concerns about potential discrimination and the issue of echo chamber. These attempts include traditional score-based methods that allocate exposure resources to different groups using pre-defined scoring functions or selection strategies and learning-based methods that learn the scoring functions based on data samples. Learning-based models are more flexible and achieve better performance than traditional methods. However, most of the learning-based models were trained and tested on outdated datasets where fairness labels are barely available. State-of-art models utilize relevance-based utility scores as a substitute for the fairness labels to train their fairness-aware loss, where plugging in the substitution does not guarantee the minimum loss. This inconsistency challenges the model's accuracy and performance, especially when learning is achieved by gradient descent. Hence, we propose a distribution-based fair learning framework (DLF) that does not require labels by replacing the unavailable fairness labels with target fairness exposure distributions. Experimental studies on TREC fair ranking track dataset confirm that our proposed framework achieves better fairness performance while maintaining better control over the fairness-relevance trade-off than state-of-art fair ranking frameworks.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "ICTIR'23"
    },
    {
        "paper id": "2405.17801",
        "abstract url": "https://arxiv.org/abs/2405.17801",
        "title": "Bandwidth Efficient Cache Selection and Content Advertisement",
        "rating": "-10",
        "keywords": [],
        "abstract": "Caching is extensively used in various networking environments to optimize performance by reducing latency, bandwidth, and energy consumption. To optimize performance, caches often advertise their content using indicators, which are data structures that trade space efficiency for accuracy. However, this tradeoff introduces the risk of false indications. Existing solutions for cache content advertisement and cache selection often lead to inefficiencies, failing to adapt to dynamic network conditions. This paper introduces SALSA2, a Scalable Adaptive and Learning-based Selection and Advertisement Algorithm, which addresses these limitations through a dynamic and adaptive approach. SALSA2 accurately estimates mis-indication probabilities by considering inter-cache dependencies and dynamically adjusts the size and frequency of indicator advertisements to minimize transmission overhead while maintaining high accuracy. Our extensive simulation study, conducted using a variety of real-world cache traces, demonstrates that SALSA2 achieves up to 84\\% bandwidth savings compared to the state-of-the-art solution and close-to-optimal service cost in most scenarios. These results highlight SALSA2's effectiveness in enhancing cache management, making it a robust and versatile solution for modern networking challenges.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17812",
        "abstract url": "https://arxiv.org/abs/2405.17812",
        "title": "Lyndon pairs and the lexicographically greatest perfect necklace",
        "rating": "-10",
        "keywords": [],
        "abstract": "Fix a finite alphabet. A necklace is a circular word. For positive integers $n$ and~$k$, a necklace is $(n,k)$-perfect if all words of length $n$ occur $k$ times but at positions with different congruence modulo $k$, for any convention of the starting position. We define the notion of a Lyndon pair and we use it to construct the lexicographically greatest $(n,k)$-perfect necklace, for any $n$ and $k$ such that $n$ divides~$k$ or $k$ divides~$n$. Our construction generalizes Fredricksen and Maiorana's construction of the lexicographically greatest de Bruijn sequence of order $n$, based on the concatenation of the Lyndon words whose length divide $n$.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2405.17813",
        "abstract url": "https://arxiv.org/abs/2405.17813",
        "title": "The Impacts of Data, Ordering, and Intrinsic Dimensionality on Recall in Hierarchical Navigable Small Worlds",
        "rating": "-10",
        "keywords": [],
        "abstract": "Vector search systems, pivotal in AI applications, often rely on the Hierarchical Navigable Small Worlds (HNSW) algorithm. However, the behaviour of HNSW under real-world scenarios using vectors generated with deep learning models remains under-explored. Existing Approximate Nearest Neighbours (ANN) benchmarks and research typically has an over-reliance on simplistic datasets like MNIST or SIFT1M and fail to reflect the complexity of current use-cases. Our investigation focuses on HNSW's efficacy across a spectrum of datasets, including synthetic vectors tailored to mimic specific intrinsic dimensionalities, widely-used retrieval benchmarks with popular embedding models, and proprietary e-commerce image data with CLIP models. We survey the most popular HNSW vector databases and collate their default parameters to provide a realistic fixed parameterisation for the duration of the paper. We discover that the recall of approximate HNSW search, in comparison to exact K Nearest Neighbours (KNN) search, is linked to the vector space's intrinsic dimensionality and significantly influenced by the data insertion sequence. Our methodology highlights how insertion order, informed by measurable properties such as the pointwise Local Intrinsic Dimensionality (LID) or known categories, can shift recall by up to 12 percentage points. We also observe that running popular benchmark datasets with HNSW instead of KNN can shift rankings by up to three positions for some models. This work underscores the need for more nuanced benchmarks and design considerations in developing robust vector search systems using approximate vector search algorithms. This study presents a number of scenarios with varying real world applicability which aim to better increase understanding and future development of ANN algorithms and embedding",
        "subjects": [
            "cs.IR"
        ],
        "comment": "15 pages, 2 figures"
    },
    {
        "paper id": "2405.17827",
        "abstract url": "https://arxiv.org/abs/2405.17827",
        "title": "DanceGen: Supporting Choreography Ideation and Prototyping with Generative AI",
        "rating": "-10",
        "keywords": [],
        "abstract": "Choreography creation requires high proficiency in artistic and technical skills. Choreographers typically go through four stages to create a dance piece: preparation, studio, performance, and reflection. This process is often individualized, complicated, and challenging due to multiple constraints at each stage. To assist choreographers, most prior work has focused on designing digital tools to support the last three stages of the choreography process, with the preparation stage being the least explored. To address this research gap, we introduce an AI-based approach to assist the preparation stage by supporting ideation, creating choreographic prototypes, and documenting creative attempts and outcomes. We address the limitations of existing AI-based motion generation methods for ideation by allowing generated sequences to be edited and modified in an interactive web interface. This capability is motivated by insights from a formative study we conducted with seven choreographers. We evaluated our system's functionality, benefits, and limitations with six expert choreographers. Results highlight the usability of our system, with users reporting increased efficiency, expanded creative possibilities, and an enhanced iterative process. We also identified areas for improvement, such as the relationship between user intent and AI outcome, intuitive and flexible user interaction design, and integration with existing physical choreography prototyping workflows. By reflecting on the evaluation results, we present three insights that aim to inform the development of future AI systems that can empower choreographers.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "ACM Conference on Designing Interactive Systems (DIS '24)"
    }
]