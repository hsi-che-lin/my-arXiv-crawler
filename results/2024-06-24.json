[
    {
        "paper id": "2406.16449",
        "abstract url": "https://arxiv.org/abs/2406.16449",
        "title": "Evaluating and Analyzing Relationship Hallucinations in LVLMs",
        "rating": "2.5",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset's long-tail distribution significantly impacts LVLMs' understanding of visual relationships. Furthermore, our analysis reveals that current LVLMs tend to disregard visual content and overly rely on the common sense knowledge of Large Language Models. They also struggle with reasoning about spatial relationships based on contextual information.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ICML2024"
    },
    {
        "paper id": "2406.16346",
        "abstract url": "https://arxiv.org/abs/2406.16346",
        "title": "Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks",
        "rating": "2",
        "keywords": [
            [
                "visual language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Large language models (LLMs) and large visual language models (LVLMs) have been at the forefront of the artificial intelligence field, particularly for tasks like text generation, video captioning, and question-answering. Typically, it is more applicable to train these models on broader knowledge bases or datasets to increase generalizability, learn relationships between topics, and recognize patterns. Instead, we propose to provide instructional datasets specific to the task of each modality within a distinct domain and then fine-tune the parameters of the model using LORA. With our approach, we can eliminate all noise irrelevant to the given task while also ensuring that the model generates with enhanced precision. For this work, we use Video-LLaVA to generate recipes given cooking videos without transcripts. Video-LLaVA's multimodal architecture allows us to provide cooking images to its image encoder, cooking videos to its video encoder, and general cooking questions to its text encoder. Thus, we aim to remove all noise unrelated to cooking while improving our model's capabilities to generate specific ingredient lists and detailed instructions. As a result, our approach to fine-tuning Video-LLaVA leads to gains over the baseline Video-LLaVA by 2% on the YouCook2 dataset. While this may seem like a marginal increase, our model trains on an image instruction dataset 2.5% the size of Video-LLaVA's and a video instruction dataset 23.76% of Video-LLaVA's.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16678",
        "abstract url": "https://arxiv.org/abs/2406.16678",
        "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are available at https://huggingface.co/segment-any-text under the MIT license.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16851",
        "abstract url": "https://arxiv.org/abs/2406.16851",
        "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
        "rating": "2",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "We present LoCoVQA, a dynamic benchmark generator for evaluating long-context extractive reasoning in vision language models (VLMs). LoCoVQA augments test examples for mathematical reasoning, VQA, and character recognition tasks with increasingly long visual contexts composed of both in-distribution and out-of-distribution distractor images. Across these tasks, a diverse set of VLMs rapidly lose performance as the visual context length grows, often exhibiting a striking exponential decay trend. This test assesses how well VLMs can ignore irrelevant information when answering queries -- a task that is quite easy for language models (LMs) in the text domain -- demonstrating that current state-of-the-art VLMs lack this essential capability for many long-context applications.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2406.17115",
        "abstract url": "https://arxiv.org/abs/2406.17115",
        "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Despite the rapid progress and outstanding performance of Large Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the issue of hallucination, i.e., LVLMs tend to generate responses that are inconsistent with the corresponding visual inputs. To evaluate the degree of hallucination in LVLMs, previous works have proposed a series of benchmarks featuring different types of tasks and evaluation metrics. However, we find that the quality of the existing hallucination benchmarks varies, with some suffering from problems, e.g., inconsistent evaluation results under repeated tests, and misalignment with human evaluation. To this end, we propose a Hallucination benchmark Quality Measurement framework (HQM), which leverages various indicators to assess the reliability and validity of existing hallucination benchmarks separately. Specifically, for reliability we explore test-retest reliability and parallel-forms reliability, while for validity we examine criterion validity and coverage of hallucination types. Furthermore, based on the results of our quality measurement, we construct a High-Quality Hallucination Benchmark (HQH) for LVLMs. We conduct an extensive evaluation of over 10 representative LVLMs, including GPT-4o and Gemini-Vision-Pro, to provide an in-depth analysis of the hallucination issues in existing models. Our benchmark is publicly available at https://github.com/HQHBench/HQHBench.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17126",
        "abstract url": "https://arxiv.org/abs/2406.17126",
        "title": "MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Spurious bias, a tendency to use spurious correlations between non-essential input attributes and target variables for predictions, has revealed a severe robustness pitfall in deep learning models trained on single modality data. Multimodal Large Language Models (MLLMs), which integrate both vision and language models, have demonstrated strong capability in joint vision-language understanding. However, whether spurious biases are prevalent in MLLMs remains under-explored. We mitigate this gap by analyzing the spurious biases in a multimodal setting, uncovering the specific test data patterns that can manifest this problem when biases in the vision model cascade into the alignment between visual and text tokens in MLLMs. To better understand this problem, we introduce MM-SpuBench, a comprehensive visual question-answering (VQA) benchmark designed to evaluate MLLMs' reliance on nine distinct categories of spurious correlations from five open-source image datasets. The VQA dataset is built from human-understandable concept information (attributes). Leveraging this benchmark, we conduct a thorough evaluation of current state-of-the-art MLLMs. Our findings illuminate the persistence of the reliance on spurious correlations from these models and underscore the urge for new methodologies to mitigate spurious biases. To support the MLLM robustness research, we release our VQA benchmark at https://huggingface.co/datasets/mmbench/MM-SpuBench.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17245",
        "abstract url": "https://arxiv.org/abs/2406.17245",
        "title": "Unlocking Continual Learning Abilities in Language Models",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "efficient finetuning"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL). Existing approaches usually address the issue by incorporating old task data or task-wise inductive bias into LMs. However, old data and accurate task information are often unavailable or costly to collect, hindering the availability of current CL approaches for LMs. To address this limitation, we introduce $\\textbf{MIGU}$ ($\\textbf{M}$agn$\\textbf{I}$tude-based $\\textbf{G}$radient $\\textbf{U}$pdating for continual learning), a rehearsal-free and task-label-free method that only updates the model parameters with large magnitudes of output in LMs' linear layers. MIGU is based on our observation that the L1-normalized magnitude distribution of the output in LMs' linear layers is different when the LM models deal with different task data. By imposing this simple constraint on the gradient update process, we can leverage the inherent behaviors of LMs, thereby unlocking their innate CL abilities. Our experiments demonstrate that MIGU is universally applicable to all three LM architectures (T5, RoBERTa, and Llama2), delivering state-of-the-art or on-par performance across continual finetuning and continual pre-training settings on four CL benchmarks. For example, MIGU brings a 15.2% average accuracy improvement over conventional parameter-efficient finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly integrate with all three existing CL types to further enhance performance. Code is available at \\href{https://github.com/wenyudu/MIGU}{this https URL}.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "preprint, 19 pages"
    },
    {
        "paper id": "2406.17257",
        "abstract url": "https://arxiv.org/abs/2406.17257",
        "title": "Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual Text-to-Speech Adaptation",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient"
            ],
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Different languages have distinct phonetic systems and vary in their prosodic features making it challenging to develop a Text-to-Speech (TTS) model that can effectively synthesise speech in multilingual settings. Furthermore, TTS architecture needs to be both efficient enough to capture nuances in multiple languages and efficient enough to be practical for deployment. The standard approach is to build transformer based model such as SpeechT5 and train it on large multilingual dataset. As the size of these models grow the conventional fine-tuning for adapting these model becomes impractical due to heavy computational cost. In this paper, we proposes to integrate parameter-efficient transfer learning (PETL) methods such as adapters and hypernetwork with TTS architecture for multilingual speech synthesis. Notably, in our experiments PETL methods able to achieve comparable or even better performance compared to full fine-tuning with only $\\sim$2.5\\% tunable parameters.The code and samples are available at: https://anonymous.4open.science/r/multilingualTTS-BA4C.",
        "subjects": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16319",
        "abstract url": "https://arxiv.org/abs/2406.16319",
        "title": "Modelled Multivariate Overlap: A method for measuring vowel merger",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "Interspeech"
            ]
        ],
        "abstract": "This paper introduces a novel method for quantifying vowel overlap. There is a tension in previous work between using multivariate measures, such as those derived from empirical distributions, and the ability to control for unbalanced data and extraneous factors, as is possible when using fitted model parameters. The method presented here resolves this tension by jointly modelling all acoustic dimensions of interest and by simulating distributions from the model to compute a measure of vowel overlap. An additional benefit of this method is that computation of uncertainty becomes straightforward. We evaluate this method on corpus speech data targeting the PIN-PEN merger in four dialects of English and find that using modelled distributions to calculate Bhattacharyya affinity substantially improves results compared to empirical distributions, while the difference between multivariate and univariate modelling is subtle.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to Interspeech 2024"
    },
    {
        "paper id": "2406.16384",
        "abstract url": "https://arxiv.org/abs/2406.16384",
        "title": "High-resolution open-vocabulary object 6D pose estimation",
        "rating": "1.5",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "6D"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The generalisation to unseen objects in the 6D pose estimation task is very challenging. While Vision-Language Models (VLMs) enable using natural language descriptions to support 6D pose estimation of unseen objects, these solutions underperform compared to model-based methods. In this work we present Horyon, an open-vocabulary VLM-based architecture that addresses relative pose estimation between two scenes of an unseen object, described by a textual prompt only. We use the textual prompt to identify the unseen object in the scenes and then obtain high-resolution multi-scale features. These features are used to extract cross-scene matches for registration. We evaluate our model on a benchmark with a large variety of unseen objects across four datasets, namely REAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves state-of-the-art performance on all datasets, outperforming by 12.6 in Average Recall the previous best-performing approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Technical report. Extension of CVPR paper \"Open-vocabulary object 6D pose estimation\". Project page: https://jcorsetti.github.io/oryon"
    },
    {
        "paper id": "2406.16808",
        "abstract url": "https://arxiv.org/abs/2406.16808",
        "title": "Exploring the Capability of Mamba in Speech Applications",
        "rating": "1.5",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ],
            [
                "Interspeech"
            ]
        ],
        "abstract": "This paper explores the capability of Mamba, a recently proposed architecture based on state space models (SSMs), as a competitive alternative to Transformer-based models. In the speech domain, well-designed Transformer-based models, such as the Conformer and E-Branchformer, have become the de facto standards. Extensive evaluations have demonstrated the effectiveness of these Transformer-based models across a wide range of speech tasks. In contrast, the evaluation of SSMs has been limited to a few tasks, such as automatic speech recognition (ASR) and speech synthesis. In this paper, we compared Mamba with state-of-the-art Transformer variants for various speech applications, including ASR, text-to-speech, spoken language understanding, and speech summarization. Experimental evaluations revealed that Mamba achieves comparable or better performance than Transformer-based models, and demonstrated its efficiency in long-form speech processing.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Accepted at Interspeech 2024"
    },
    {
        "paper id": "2406.17005",
        "abstract url": "https://arxiv.org/abs/2406.17005",
        "title": "PVUW 2024 Challenge on Complex Video Understanding: Methods and Results",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Pixel-level Video Understanding in the Wild Challenge (PVUW) focus on complex video understanding. In this CVPR 2024 workshop, we add two new tracks, Complex Video Object Segmentation Track based on MOSE dataset and Motion Expression guided Video Segmentation track based on MeViS dataset. In the two new tracks, we provide additional videos and annotations that feature challenging elements, such as the disappearance and reappearance of objects, inconspicuous small objects, heavy occlusions, and crowded environments in MOSE. Moreover, we provide a new motion expression guided video segmentation dataset MeViS to study the natural language-guided video understanding in complex environments. These new videos, sentences, and annotations enable us to foster the development of a more comprehensive and robust pixel-level understanding of video scenes in complex environments and realistic scenarios. The MOSE challenge had 140 registered teams in total, 65 teams participated the validation phase and 12 teams made valid submissions in the final challenge phase. The MeViS challenge had 225 registered teams in total, 50 teams participated the validation phase and 5 teams made valid submissions in the final challenge phase.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "MOSE Challenge: https://henghuiding.github.io/MOSE/ChallengeCVPR2024, MeViS Challenge: https://henghuiding.github.io/MeViS/ChallengeCVPR2024"
    },
    {
        "paper id": "2406.17261",
        "abstract url": "https://arxiv.org/abs/2406.17261",
        "title": "TRAWL: Tensor Reduced and Approximated Weights for Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large language models (LLMs) have fundamentally transformed artificial intelligence, catalyzing recent advancements while imposing substantial environmental and computational burdens. We introduce TRAWL (Tensor Reduced and Approximated Weights for Large Language Models), a novel methodology for optimizing LLMs through tensor decomposition. TRAWL leverages diverse strategies to exploit matrices within transformer-based architectures, realizing notable performance enhancements without necessitating retraining. The most significant improvements were observed through a layer-by-layer intervention strategy, particularly when applied to fully connected weights of the final layers, yielding up to 16% enhancement in accuracy without the need for additional data or fine-tuning. These results underscore the importance of targeted and adaptive techniques in increasing the efficiency and effectiveness of large language model optimization, thereby promoting the development of more sustainable and accessible AI systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "8 pages, 5 figures. Submitted to EMNLP 2024 and under review"
    },
    {
        "paper id": "2406.17272",
        "abstract url": "https://arxiv.org/abs/2406.17272",
        "title": "A Comprehensive Solution to Connect Speech Encoder and Large Language Model for ASR",
        "rating": "1.5",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent works have shown promising results in connecting speech encoders to large language models (LLMs) for speech recognition. However, several limitations persist, including limited fine-tuning options, a lack of mechanisms to enforce speech-text alignment, and high insertion errors especially in domain mismatch conditions. This paper presents a comprehensive solution to address these issues. We begin by investigating more thoughtful fine-tuning schemes. Next, we propose a matching loss to enhance alignment between modalities. Finally, we explore training and inference methods to mitigate high insertion errors. Experimental results on the Librispeech corpus demonstrate that partially fine-tuning the encoder and LLM using parameter-efficient methods, such as LoRA, is the most cost-effective approach. Additionally, the matching loss improves modality alignment, enhancing performance. The proposed training and inference methods significantly reduce insertion errors.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16326",
        "abstract url": "https://arxiv.org/abs/2406.16326",
        "title": "RefXVC: Cross-Lingual Voice Conversion with Enhanced Reference Leveraging",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "This paper proposes RefXVC, a method for cross-lingual voice conversion (XVC) that leverages reference information to improve conversion performance. Previous XVC works generally take an average speaker embedding to condition the speaker identity, which does not account for the changing timbre of speech that occurs with different pronunciations. To address this, our method uses both global and local speaker embeddings to capture the timbre changes during speech conversion. Additionally, we observed a connection between timbre and pronunciation in different languages and utilized this by incorporating a timbre encoder and a pronunciation matching network into our model. Furthermore, we found that the variation in tones is not adequately reflected in a sentence, and therefore, we used multiple references to better capture the range of a speaker's voice. The proposed method outperformed existing systems in terms of both speech quality and speaker similarity, highlighting the effectiveness of leveraging reference information in cross-lingual voice conversion. The converted speech samples can be found on the website: \\url{http://refxvc.dn3point.com}",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Manuscript under review by TASLP"
    },
    {
        "paper id": "2406.16330",
        "abstract url": "https://arxiv.org/abs/2406.16330",
        "title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82\\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16332",
        "abstract url": "https://arxiv.org/abs/2406.16332",
        "title": "DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, there has been increasing interest in applying large language models (LLMs) as zero-shot passage rankers. However, few studies have explored how to select appropriate in-context demonstrations for the passage ranking task, which is the focus of this paper. Previous studies mainly apply a demonstration retriever to retrieve demonstrations and use top-$k$ demonstrations for in-context learning (ICL). Although effective, this approach overlooks the dependencies between demonstrations, leading to inferior performance of few-shot ICL in the passage ranking task. In this paper, we formulate the demonstration selection as a \\textit{retrieve-then-rerank} process and introduce the DemoRank framework. In this framework, we first use LLM feedback to train a demonstration retriever and construct a novel dependency-aware training samples to train a demonstration reranker to improve few-shot ICL. The construction of such training samples not only considers demonstration dependencies but also performs in an efficient way. Extensive experiments demonstrate DemoRank's effectiveness in in-domain scenarios and strong generalization to out-of-domain scenarios. Our codes are available at~\\url{https://github.com/8421BCD/DemoRank}.",
        "subjects": [
            "cs.IR",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16338",
        "abstract url": "https://arxiv.org/abs/2406.16338",
        "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have extended their capabilities to video understanding. Yet, these models are often plagued by \"hallucinations\", where irrelevant or nonsensical content is generated, deviating from the actual video context. This work introduces VideoHallucer, the first comprehensive benchmark for hallucination detection in large video-language models (LVLMs). VideoHallucer categorizes hallucinations into two main types: intrinsic and extrinsic, offering further subcategories for detailed analysis, including object-relation, temporal, semantic detail, extrinsic factual, and extrinsic non-factual hallucinations. We adopt an adversarial binary VideoQA method for comprehensive evaluation, where pairs of basic and hallucinated questions are crafted strategically. By evaluating eleven LVLMs on VideoHallucer, we reveal that i) the majority of current models exhibit significant issues with hallucinations; ii) while scaling datasets and parameters improves models' ability to detect basic visual cues and counterfactuals, it provides limited benefit for detecting extrinsic factual hallucinations; iii) existing models are more adept at detecting facts than identifying hallucinations. As a byproduct, these analyses further instruct the development of our self-PEP framework, achieving an average of 5.38% improvement in hallucination resistance across all model architectures.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16342",
        "abstract url": "https://arxiv.org/abs/2406.16342",
        "title": "ADVSCORE: A Metric for the Evaluation and Creation of Adversarial Benchmarks",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Adversarial benchmarks validate model abilities by providing samples that fool models but not humans. However, despite the proliferation of datasets that claim to be adversarial, there does not exist an established metric to evaluate how adversarial these datasets are. To address this lacuna, we introduce ADVSCORE, a metric which quantifies how adversarial and discriminative an adversarial dataset is and exposes the features that make data adversarial. We then use ADVSCORE to underpin a dataset creation pipeline that incentivizes writing a high-quality adversarial dataset. As a proof of concept, we use ADVSCORE to collect an adversarial question answering (QA) dataset, ADVQA, from our pipeline. The high-quality questions in ADVQA surpasses three adversarial benchmarks across domains at fooling several models but not humans. We validate our result based on difficulty estimates from 9,347 human responses on four datasets and predictions from three models. Moreover, ADVSCORE uncovers which adversarial tactics used by human writers fool models (e.g., GPT-4) but not humans. Through ADVSCORE and its analyses, we offer guidance on revealing language model vulnerabilities and producing reliable adversarial examples.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2401.11185"
    },
    {
        "paper id": "2406.16356",
        "abstract url": "https://arxiv.org/abs/2406.16356",
        "title": "Evaluation of Instruction-Following Ability for Large Language Models on Story-Ending Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Instruction-tuned Large Language Models (LLMs) have achieved remarkable performance across various benchmark tasks. While providing instructions to LLMs for guiding their generations is user-friendly, assessing their instruction-following capabilities is still unclarified due to a lack of evaluation metrics. In this paper, we focus on evaluating the instruction-following ability of LLMs in the context of story-ending generation, which requires diverse and context-specific instructions. We propose an automatic evaluation pipeline that utilizes a machine reading comprehension (MRC) model to determine whether the generated story-ending reflects instruction. Our findings demonstrate that our proposed metric aligns with human evaluation. Furthermore, our experiments confirm that recent open-source LLMs can achieve instruction-following performance close to GPT-3.5, as assessed through automatic evaluation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16358",
        "abstract url": "https://arxiv.org/abs/2406.16358",
        "title": "Approximate DCT and Quantization Techniques for Energy-Constrained Image Sensors",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Recent expansions in multimedia devices gather enormous amounts of real-time images for processing and inference. The images are first compressed using compression schemes, like JPEG, to reduce storage costs and power for transmitting the captured data. Due to inherent error resilience and imperceptibility in images, JPEG can be approximated to reduce the required computation power and area. This work demonstrates the first end-to-end approximation computing-based optimization of JPEG hardware using i) an approximate division realized using bit-shift operators to reduce the complexity of the quantization block, ii) loop perforation, and iii) precision scaling on top of a multiplier-less fast DCT architecture to achieve an extremely energy-efficient JPEG compression unit which will be a perfect fit for power/bandwidth-limited scenario. Furthermore, a gradient descent-based heuristic composed of two conventional approximation strategies, i.e., Precision Scaling and Loop Perforation, is implemented for tuning the degree of approximation to trade off energy consumption with the quality degradation of the decoded image. The entire RTL design is coded in Verilog HDL, synthesized, mapped to TSMC 65nm CMOS technology, and simulated using Cadence Spectre Simulator under 25$^{\\circ}$\\textbf{C}, TT corner. The approximate division approach achieved around $\\textbf{28\\%}$ reduction in the active design area. The heuristic-based approximation technique combined with accelerator optimization achieves a significant energy reduction of $\\textbf{36\\%}$ for a minimal image quality degradation of $\\textbf{2\\%}$ SAD. Simulation results also show that the proposed architecture consumes 15uW at the DCT and quantization stages to compress a colored 480p image at 6fps.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16360",
        "abstract url": "https://arxiv.org/abs/2406.16360",
        "title": "MIRReS: Multi-bounce Inverse Rendering using Reservoir Sampling",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present MIRReS, a novel two-stage inverse rendering framework that jointly reconstructs and optimizes the explicit geometry, material, and lighting from multi-view images. Unlike previous methods that rely on implicit irradiance fields or simplified path tracing algorithms, our method extracts an explicit geometry (triangular mesh) in stage one, and introduces a more realistic physically-based inverse rendering model that utilizes multi-bounce path tracing and Monte Carlo integration. By leveraging multi-bounce path tracing, our method effectively estimates indirect illumination, including self-shadowing and internal reflections, which improves the intrinsic decomposition of shape, material, and lighting. Moreover, we incorporate reservoir sampling into our framework to address the noise in Monte Carlo integration, enhancing convergence and facilitating gradient-based optimization with low sample counts. Through qualitative and quantitative evaluation of several scenarios, especially in challenging scenarios with complex shadows, we demonstrate that our method achieves state-of-the-art performance on decomposition results. Additionally, our optimized explicit geometry enables applications such as scene editing, relighting, and material editing with modern graphics engines or CAD software. The source code is available at https://brabbitdousha.github.io/MIRReS/",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "16 pages, 14 figures"
    },
    {
        "paper id": "2406.16372",
        "abstract url": "https://arxiv.org/abs/2406.16372",
        "title": "UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot Cross-Lingual Natural Language Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Cross-lingual representation learning transfers knowledge from resource-rich data to resource-scarce ones to improve the semantic understanding abilities of different languages. However, previous works rely on shallow unsupervised data generated by token surface matching, regardless of the global context-aware semantics of the surrounding text tokens. In this paper, we propose an Unsupervised Pseudo Semantic Data Augmentation (UniPSDA) mechanism for cross-lingual natural language understanding to enrich the training data without human interventions. Specifically, to retrieve the tokens with similar meanings for the semantic data augmentation across different languages, we propose a sequential clustering process in 3 stages: within a single language, across multiple languages of a language family, and across languages from multiple language families. Meanwhile, considering the multi-lingual knowledge infusion with context-aware semantics while alleviating computation burden, we directly replace the key constituents of the sentences with the above-learned multi-lingual family knowledge, viewed as pseudo-semantic. The infusion process is further optimized via three de-biasing techniques without introducing any neural parameters. Extensive experiments demonstrate that our model consistently improves the performance on general zero-shot cross-lingual natural language understanding tasks, including sequence classification, information extraction, and question answering.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16377",
        "abstract url": "https://arxiv.org/abs/2406.16377",
        "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Despite the general capabilities of pre-trained large language models (LLMs), they still need further adaptation to better serve practical applications. In this paper, we demonstrate the interchangeability of three popular and distinct adaptation tools: parameter updating, reward modeling, and in-context prompting. This interchangeability establishes a triangular framework with six transformation directions, each of which facilitates a variety of applications. Our work offers a holistic view that unifies numerous existing studies and suggests potential research directions. We envision our work as a useful roadmap for future research on LLMs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16382",
        "abstract url": "https://arxiv.org/abs/2406.16382",
        "title": "UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions. With large language models (LLMs) demonstrating powerful capabilities between tasks, we can't help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions wtih the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16408",
        "abstract url": "https://arxiv.org/abs/2406.16408",
        "title": "A Symmetry Property of Christoffel Words",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Motivated by the theory of trapezoidal words, whose sequences of cardinality of factors by length are symmetric, we introduce a bivariate variant of this symmetry. We show that this symmetry characterizes Christoffel words, and establish other related results.",
        "subjects": [
            "math.CO",
            "cs.CL"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16422",
        "abstract url": "https://arxiv.org/abs/2406.16422",
        "title": "Exploring Cross-Domain Few-Shot Classification via Frequency-Aware Prompting",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Cross-Domain Few-Shot Learning has witnessed great stride with the development of meta-learning. However, most existing methods pay more attention to learning domain-adaptive inductive bias (meta-knowledge) through feature-wise manipulation or task diversity improvement while neglecting the phenomenon that deep networks tend to rely more on high-frequency cues to make the classification decision, which thus degenerates the robustness of learned inductive bias since high-frequency information is vulnerable and easy to be disturbed by noisy information. Hence in this paper, we make one of the first attempts to propose a Frequency-Aware Prompting method with mutual attention for Cross-Domain Few-Shot classification, which can let networks simulate the human visual perception of selecting different frequency cues when facing new recognition tasks. Specifically, a frequency-aware prompting mechanism is first proposed, in which high-frequency components of the decomposed source image are switched either with normal distribution sampling or zeroing to get frequency-aware augment samples. Then, a mutual attention module is designed to learn generalizable inductive bias under CD-FSL settings. More importantly, the proposed method is a plug-and-play module that can be directly applied to most off-the-shelf CD-FLS methods. Experimental results on CD-FSL benchmarks demonstrate the effectiveness of our proposed method as well as robustly improve the performance of existing CD-FLS methods. Resources at https://github.com/tinkez/FAP_CDFSC.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16427",
        "abstract url": "https://arxiv.org/abs/2406.16427",
        "title": "Dynamic Pseudo Label Optimization in Point-Supervised Nuclei Segmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning has achieved impressive results in nuclei segmentation, but the massive requirement for pixel-wise labels remains a significant challenge. To alleviate the annotation burden, existing methods generate pseudo masks for model training using point labels. However, the generated masks are inevitably different from the ground truth, and these dissimilarities are not handled reasonably during the network training, resulting in the subpar performance of the segmentation model. To tackle this issue, we propose a framework named DoNuSeg, enabling \\textbf{D}ynamic pseudo label \\textbf{O}ptimization in point-supervised \\textbf{Nu}clei \\textbf{Seg}mentation. Specifically, DoNuSeg takes advantage of class activation maps (CAMs) to adaptively capture regions with semantics similar to annotated points. To leverage semantic diversity in the hierarchical feature levels, we design a dynamic selection module to choose the optimal one among CAMs from different encoder blocks as pseudo masks. Meanwhile, a CAM-guided contrastive module is proposed to further enhance the accuracy of pseudo masks. In addition to exploiting the semantic information provided by CAMs, we consider location priors inherent to point labels, developing a task-decoupled structure for effectively differentiating nuclei. Extensive experiments demonstrate that DoNuSeg outperforms state-of-the-art point-supervised methods. The code is available at https://github.com/shinning0821/MICCAI24-DoNuSeg.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "early accepted by MICCAI2024"
    },
    {
        "paper id": "2406.16439",
        "abstract url": "https://arxiv.org/abs/2406.16439",
        "title": "Exploring Test-Time Adaptation for Object Detection in Continually Changing Environments",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "For real-world applications, neural network models are commonly deployed in dynamic environments, where the distribution of the target domain undergoes temporal changes. Continual Test-Time Adaptation (CTTA) has recently emerged as a promising technique to gradually adapt a source-trained model to test data drawn from a continually changing target domain. Despite recent advancements in addressing CTTA, two critical issues remain: 1) The use of a fixed threshold for pseudo-labeling in existing methodologies leads to the generation of low-quality pseudo-labels, as model confidence varies across categories and domains; 2) While current solutions utilize stochastic parameter restoration to mitigate catastrophic forgetting, their capacity to preserve critical information is undermined by its intrinsic randomness. To tackle these challenges, we present CTAOD, aiming to enhance the performance of detection models in CTTA scenarios. Inspired by prior CTTA works for effective adaptation, CTAOD is founded on the mean-teacher framework, characterized by three core components. Firstly, the object-level contrastive learning module tailored for object detection extracts object-level features using the teacher's region of interest features and optimizes them through contrastive learning. Secondly, the dynamic threshold strategy updates the category-specific threshold based on predicted confidence scores to improve the quality of pseudo-labels. Lastly, we design a data-driven stochastic restoration mechanism to selectively reset inactive parameters using the gradients as weights for a random mask matrix, thereby ensuring the retention of essential knowledge. We demonstrate the effectiveness of our approach on four CTTA tasks for object detection, where CTAOD outperforms existing methods, especially achieving a 3.0 mAP improvement on the Cityscapes-to-Cityscapes-C CTTA task.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16441",
        "abstract url": "https://arxiv.org/abs/2406.16441",
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by ACL 2024 (Main)"
    },
    {
        "paper id": "2406.16450",
        "abstract url": "https://arxiv.org/abs/2406.16450",
        "title": "Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models' parameter count and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFN), which are less studied than attention blocks. We consider three candidate linear layer approximations in the FFN by combining efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from the training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We first demonstrate they can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called \\textit{self-guided training}, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Experiments on the large RefinedWeb dataset show that our methods are both efficient and effective for training and inference. Interestingly, these structured FFNs exhibit steeper scaling curves than the original models. Further applying self-guided training to the structured matrices with 32\\% FFN parameters and 2.5$\\times$ speed-up enables only a 0.4 perplexity increase under the same training FLOPs. Finally, we develop the wide and structured networks surpassing the current medium-sized and large-sized Transformer in perplexity and throughput performance. Our code is available at \\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16464",
        "abstract url": "https://arxiv.org/abs/2406.16464",
        "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The prevalence of sarcasm in social media, conveyed through text-image combinations, presents significant challenges for sentiment analysis and intention mining. Current multi-modal sarcasm detection methods have been proven to struggle with biases from spurious cues, leading to a superficial understanding of the complex interactions between text and image. To address these issues, we propose InterCLIP-MEP, a robust framework for multi-modal sarcasm detection. InterCLIP-MEP introduces a refined variant of CLIP, Interactive CLIP (InterCLIP), as the backbone, enhancing sample representations by embedding cross-modality information in each encoder. Furthermore, a novel training strategy is designed to adapt InterCLIP for a Memory-Enhanced Predictor (MEP). MEP uses dynamic dual-channel memory to store valuable historical knowledge of test samples and then leverages this memory as a non-parametric classifier to derive the final prediction. By using InterCLIP to encode text-image interactions more effectively and incorporating MEP, InterCLIP-MEP offers a more robust recognition of multi-modal sarcasm. Experiments demonstrate that InterCLIP-MEP achieves state-of-the-art performance on the MMSD2.0 benchmark. Code and data are available at https://github.com/CoderChen01/InterCLIP-MEP.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures, 6 tables"
    },
    {
        "paper id": "2406.16476",
        "abstract url": "https://arxiv.org/abs/2406.16476",
        "title": "ResMaster: Mastering High-Resolution Image Generation via Structural and Fine-Grained Guidance",
        "rating": "1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models excel at producing high-quality images; however, scaling to higher resolutions, such as 4K, often results in over-smoothed content, structural distortions, and repetitive patterns. To this end, we introduce ResMaster, a novel, training-free method that empowers resolution-limited diffusion models to generate high-quality images beyond resolution restrictions. Specifically, ResMaster leverages a low-resolution reference image created by a pre-trained diffusion model to provide structural and fine-grained guidance for crafting high-resolution images on a patch-by-patch basis. To ensure a coherent global structure, ResMaster meticulously aligns the low-frequency components of high-resolution patches with the low-resolution reference at each denoising step. For fine-grained guidance, tailored image prompts based on the low-resolution reference and enriched textual prompts produced by a vision-language model are incorporated. This approach could significantly mitigate local pattern distortions and improve detail refinement. Extensive experiments validate that ResMaster sets a new benchmark for high-resolution image generation and demonstrates promising efficiency. The project page is https://shuweis.github.io/ResMaster .",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16478",
        "abstract url": "https://arxiv.org/abs/2406.16478",
        "title": "EMMI -- Empathic Multimodal Motivational Interviews Dataset: Analyses and Annotations",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The study of multimodal interaction in therapy can yield a comprehensive understanding of therapist and patient behavior that can be used to develop a multimodal virtual agent supporting therapy. This investigation aims to uncover how therapists skillfully blend therapy's task goal (employing classical steps of Motivational Interviewing) with the social goal (building a trusting relationship and expressing empathy). Furthermore, we seek to categorize patients into various ``types'' requiring tailored therapeutic approaches. To this intent, we present multimodal annotations of a corpus consisting of simulated motivational interviewing conversations, wherein actors portray the roles of patients and therapists. We introduce EMMI, composed of two publicly available MI corpora, AnnoMI and the Motivational Interviewing Dataset, for which we add multimodal annotations. We analyze these annotations to characterize functional behavior for developing a virtual agent performing motivational interviews emphasizing social and empathic behaviors. Our analysis found three clusters of patients expressing significant differences in behavior and adaptation of the therapist's behavior to those types. This shows the importance of a therapist being able to adapt their behavior depending on the current situation within the dialog and the type of user.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2406.16481",
        "abstract url": "https://arxiv.org/abs/2406.16481",
        "title": "Improving Quaternion Neural Networks with Quaternionic Activation Functions",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we propose novel quaternion activation functions where we modify either the quaternion magnitude or the phase, as an alternative to the commonly used split activation functions. We define criteria that are relevant for quaternion activation functions, and subsequently we propose our novel activation functions based on this analysis. Instead of applying a known activation function like the ReLU or Tanh on the quaternion elements separately, these activation functions consider the quaternion properties and respect the quaternion space $\\mathbb{H}$. In particular, all quaternion components are utilized to calculate all output components, carrying out the benefit of the Hamilton product in e.g. the quaternion convolution to the activation functions. The proposed activation functions can be incorporated in arbitrary quaternion valued neural networks trained with gradient descent techniques. We further discuss the derivatives of the proposed activation functions where we observe beneficial properties for the activation functions affecting the phase. Specifically, they prove to be sensitive on basically the whole input range, thus improved gradient flow can be expected. We provide an elaborate experimental evaluation of our proposed quaternion activation functions including comparison with the split ReLU and split Tanh on two image classification tasks using the CIFAR-10 and SVHN dataset. There, especially the quaternion activation functions affecting the phase consistently prove to provide better performance.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16490",
        "abstract url": "https://arxiv.org/abs/2406.16490",
        "title": "eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This study investigates the performance of the zero-shot method in classifying data using three large language models, alongside two models with large input token sizes and the two pre-trained models on legal data. Our main dataset comes from the domain of U.S. civil procedure. It includes summaries of legal cases, specific questions, potential answers, and detailed explanations for why each solution is relevant, all sourced from a book aimed at law students. By comparing different methods, we aimed to understand how effectively they handle the complexities found in legal datasets. Our findings show how well the zero-shot method of large language models can understand complicated data. We achieved our highest F1 score of 64% in these experiments.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16508",
        "abstract url": "https://arxiv.org/abs/2406.16508",
        "title": "Large Vocabulary Size Improves Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2406.16518",
        "abstract url": "https://arxiv.org/abs/2406.16518",
        "title": "Vision Mamba-based autonomous crack segmentation on concrete, asphalt, and masonry surfaces",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Convolutional neural networks (CNNs) and Transformers have shown advanced accuracy in crack detection under certain conditions. Yet, the fixed local attention can compromise the generalisation of CNNs, and the quadratic complexity of the global self-attention restricts the practical deployment of Transformers. Given the emergence of the new-generation architecture of Mamba, this paper proposes a Vision Mamba (VMamba)-based framework for crack segmentation on concrete, asphalt, and masonry surfaces, with high accuracy, generalisation, and less computational complexity. Having 15.6% - 74.5% fewer parameters, the encoder-decoder network integrated with VMamba could obtain up to 2.8% higher mDS than representative CNN-based models while showing about the same performance as Transformer-based models. Moreover, the VMamba-based encoder-decoder network could process high-resolution image input with up to 90.6% lower floating-point operations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "23 pages, 9 figures"
    },
    {
        "paper id": "2406.16521",
        "abstract url": "https://arxiv.org/abs/2406.16521",
        "title": "Carrot and Stick: Inducing Self-Motivation with Positive & Negative Feedback",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Positive thinking is thought to be an important component of self-motivation in various practical fields such as education and the workplace. Previous work, including sentiment transfer and positive reframing, has focused on the positive side of language. However, self-motivation that drives people to reach their goals has not yet been studied from a computational perspective. Moreover, negative feedback has not yet been explored, even though positive and negative feedback are both necessary to grow self-motivation. To facilitate self-motivation, we propose CArrot and STICk (CASTIC) dataset, consisting of 12,590 sentences with 5 different strategies for enhancing self-motivation. Our data and code are publicly available at here.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "10 pages, 8 figures"
    },
    {
        "paper id": "2406.16524",
        "abstract url": "https://arxiv.org/abs/2406.16524",
        "title": "The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge distillation (KD) has proven to be a successful strategy to improve the performance of a smaller model in many NLP tasks. However, most of the work in KD only explores monolingual scenarios. In this paper, we investigate the value of KD in multilingual settings. We find the significance of KD and model initialization by analyzing how well the student model acquires multilingual knowledge from the teacher model. Our proposed method emphasizes copying the teacher model's weights directly to the student model to enhance initialization. Our finding shows that model initialization using copy-weight from the fine-tuned teacher contributes the most compared to the distillation process itself across various multilingual settings. Furthermore, we demonstrate that efficient weight initialization preserves multilingual capabilities even in low-resource scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2406.16527",
        "abstract url": "https://arxiv.org/abs/2406.16527",
        "title": "SyROCCo: Enhancing Systematic Reviews using Machine Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "The sheer number of research outputs published every year makes systematic reviewing increasingly time- and resource-intensive. This paper explores the use of machine learning techniques to help navigate the systematic review process. ML has previously been used to reliably 'screen' articles for review - that is, identify relevant articles based on reviewers' inclusion criteria. The application of ML techniques to subsequent stages of a review, however, such as data extraction and evidence mapping, is in its infancy. We therefore set out to develop a series of tools that would assist in the profiling and analysis of 1,952 publications on the theme of 'outcomes-based contracting'. Tools were developed for the following tasks: assign publications into 'policy area' categories; identify and extract key information for evidence mapping, such as organisations, laws, and geographical information; connect the evidence base to an existing dataset on the same topic; and identify subgroups of articles that may share thematic content. An interactive tool using these techniques and a public dataset with their outputs have been released. Our results demonstrate the utility of ML techniques to enhance evidence accessibility and analysis within the systematic review processes. These efforts show promise in potentially yielding substantial efficiencies for future systematic reviewing and for broadening their analytical scope. Our work suggests that there may be implications for the ease with which policymakers and practitioners can access evidence. While ML techniques seem poised to play a significant role in bridging the gap between research and policy by offering innovative ways of gathering, accessing, and analysing data from systematic reviews, we also highlight their current limitations and the need to exercise caution in their application, particularly given the potential for errors and biases.",
        "subjects": [
            "cs.CL",
            "cs.CY",
            "cs.DL",
            "cs.LG"
        ],
        "comment": "28 pages, 5 figures. To appear in Data & Policy journal"
    },
    {
        "paper id": "2406.16528",
        "abstract url": "https://arxiv.org/abs/2406.16528",
        "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We investigate the abilities of a representative set of Large language Models (LLMs) to reason about cardinal directions (CDs). To do so, we create two datasets: the first, co-created with ChatGPT, focuses largely on recall of world knowledge about CDs; the second is generated from a set of templates, comprehensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first , second or third person. Even with a temperature setting of zero, Our experiments show that although LLMs are able to perform well in the simpler dataset, in the second more complex dataset no LLM is able to reliably determine the correct CD, even with a temperature setting of zero.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, 3 figures, 1 table. Short paper accepted by COSIT 24, The 16th Conference on Spatial Information Theory"
    },
    {
        "paper id": "2406.16535",
        "abstract url": "https://arxiv.org/abs/2406.16535",
        "title": "Token-based Decision Criteria Are Suboptimal in In-context Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In-Context Learning (ICL) typically utilizes classification criteria from probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LM's last hidden states. In detail, we use the nearest centroid classification on the hidden states, assigning the category of the nearest centroid previously observed from a few-shot calibration set to the test sample as the predicted label. Our experiments on 3 models and 10 classification datasets indicate that Hidden Calibration consistently outperforms current token-based calibrations by about 20%. Our further analysis demonstrates that Hidden Calibration finds better classification criteria with less inter-categories overlap, and LMs provide linearly separable intra-category clusters with the help of demonstrations, which supports Hidden Calibration and gives new insights into the conventional ICL.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "21 pages, 14 figures, 8 tables"
    },
    {
        "paper id": "2406.16536",
        "abstract url": "https://arxiv.org/abs/2406.16536",
        "title": "C-LLM: Learn to Check Chinese Spelling Errors Character by Character",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveal that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character. Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves an average improvement of 10% over existing methods. Specifically, it shows a 2.1% improvement in general scenarios and a significant 12% improvement in vertical domain scenarios, establishing state-of-the-art performance. The source code can be accessed at https://github.com/ktlKTL/C-LLM.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16537",
        "abstract url": "https://arxiv.org/abs/2406.16537",
        "title": "Character-Adapter: Prompt-Guided Region Control for High-Fidelity Character Customization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Customized image generation, which seeks to synthesize images with consistent characters, holds significant relevance for applications such as storytelling, portrait generation, and character design. However, previous approaches have encountered challenges in preserving characters with high-fidelity consistency due to inadequate feature extraction and concept confusion of reference characters. Therefore, we propose Character-Adapter, a plug-and-play framework designed to generate images that preserve the details of reference characters, ensuring high-fidelity consistency. Character-Adapter employs prompt-guided segmentation to ensure fine-grained regional features of reference characters and dynamic region-level adapters to mitigate concept confusion. Extensive experiments are conducted to validate the effectiveness of Character-Adapter. Both quantitative and qualitative results demonstrate that Character-Adapter achieves the state-of-the-art performance of consistent character generation, with an improvement of 24.8% compared with other methods",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16540",
        "abstract url": "https://arxiv.org/abs/2406.16540",
        "title": "Improving robustness to corruptions with multiplicative weight perturbations",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Deep neural networks (DNNs) excel on clean images but struggle with corrupted ones. Incorporating specific corruptions into the data augmentation pipeline can improve robustness to those corruptions but may harm performance on clean images and other types of distortion. In this paper, we introduce an alternative approach that improves the robustness of DNNs to a wide range of corruptions without compromising accuracy on clean images. We first demonstrate that input perturbations can be mimicked by multiplicative perturbations in the weight space. Leveraging this, we propose Data Augmentation via Multiplicative Perturbation (DAMP), a training method that optimizes DNNs under random multiplicative weight perturbations. We also examine the recently proposed Adaptive Sharpness-Aware Minimization (ASAM) and show that it optimizes DNNs under adversarial multiplicative weight perturbations. Experiments on image classification datasets (CIFAR-10/100, TinyImageNet and ImageNet) and neural network architectures (ResNet50, ViT-S/16) show that DAMP enhances model generalization performance in the presence of corruptions across different settings. Notably, DAMP is able to train a ViT-S/16 on ImageNet from scratch, reaching the top-1 error of 23.7% which is comparable to ResNet50 without extensive data augmentations.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2406.16544",
        "abstract url": "https://arxiv.org/abs/2406.16544",
        "title": "Hierarchical B-frame Video Coding for Long Group of Pictures",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Learned video compression methods already outperform VVC in the low-delay (LD) case, but the random-access (RA) scenario remains challenging. Most works on learned RA video compression either use HEVC as an anchor or compare it to VVC in specific test conditions, using RGB-PSNR metric instead of Y-PSNR and avoiding comprehensive evaluation. Here, we present an end-to-end learned video codec for random access that combines training on long sequences of frames, rate allocation designed for hierarchical coding and content adaptation on inference. We show that under common test conditions (JVET-CTC), it achieves results comparable to VTM (VVC reference software) in terms of YUV-PSNR BD-Rate on some classes of videos, and outperforms it on almost all test sets in terms of VMAF BD-Rate. On average it surpasses open LD and RA end-to-end solutions in terms of VMAF and YUV BD-Rates.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16554",
        "abstract url": "https://arxiv.org/abs/2406.16554",
        "title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated by this limit, we investigate building MoE models from existing dense large language models. Specifically, based on the well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert Construction, which partitions the parameters of original Feed-Forward Networks (FFNs) into multiple experts; (2) Continual Pre-training, which further trains the transformed MoE model and additional gate networks. In this paper, we comprehensively explore different methods for expert construction and various data sampling strategies for continual pre-training. After these stages, our LLaMA-MoE models could maintain language abilities and route the input tokens to specific experts with part of the parameters activated. Empirically, by training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense models that contain similar activation parameters. The source codes and models are available at https://github.com/pjlab-sys4nlp/llama-moe .",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16608",
        "abstract url": "https://arxiv.org/abs/2406.16608",
        "title": "When Invariant Representation Learning Meets Label Shift: Insufficiency and Theoretical Insights",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "As a crucial step toward real-world learning scenarios with changing environments, dataset shift theory and invariant representation learning algorithm have been extensively studied to relax the identical distribution assumption in classical learning setting. Among the different assumptions on the essential of shifting distributions, generalized label shift (GLS) is the latest developed one which shows great potential to deal with the complex factors within the shift. In this paper, we aim to explore the limitations of current dataset shift theory and algorithm, and further provide new insights by presenting a comprehensive understanding of GLS. From theoretical aspect, two informative generalization bounds are derived, and the GLS learner is proved to be sufficiently close to optimal target model from the Bayesian perspective. The main results show the insufficiency of invariant representation learning, and prove the sufficiency and necessity of GLS correction for generalization, which provide theoretical supports and innovations for exploring generalizable model under dataset shift. From methodological aspect, we provide a unified view of existing shift correction frameworks, and propose a kernel embedding-based correction algorithm (KECA) to minimize the generalization error and achieve successful knowledge transfer. Both theoretical results and extensive experiment evaluations demonstrate the sufficiency and necessity of GLS correction for addressing dataset shift and the superiority of proposed algorithm.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"
    },
    {
        "paper id": "2406.16615",
        "abstract url": "https://arxiv.org/abs/2406.16615",
        "title": "The Championship-Winning Solution for the 5th CLVISION Challenge 2024",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce our approach to the 5th CLVision Challenge, which presents distinctive challenges beyond traditional class incremental learning. Unlike standard settings, this competition features the recurrence of previously encountered classes and includes unlabeled data that may contain Out-of-Distribution (OOD) categories. Our approach is based on Winning Subnetworks to allocate independent parameter spaces for each task addressing the catastrophic forgetting problem in class incremental learning and employ three training strategies: supervised classification learning, unsupervised contrastive learning, and pseudo-label classification learning to fully utilize the information in both labeled and unlabeled data, enhancing the classification performance of each subnetwork. Furthermore, during the inference stage, we have devised an interaction strategy between subnetworks, where the prediction for a specific class of a particular sample is the average logits across different subnetworks corresponding to that class, leveraging the knowledge learned from different subnetworks on recurring classes to improve classification accuracy. These strategies can be simultaneously applied to the three scenarios of the competition, effectively solving the difficulties in the competition scenarios. Experimentally, our method ranks first in both the pre-selection and final evaluation stages, with an average accuracy of 0.4535 during the preselection stage and an average accuracy of 0.4805 during the final evaluation stage.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16620",
        "abstract url": "https://arxiv.org/abs/2406.16620",
        "title": "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding. However, processing extensive videos such as 24-hour CCTV footage or full-length films presents significant challenges due to the vast data and processing demands. Traditional methods, like extracting key frames or converting frames to text, often result in substantial information loss. To address these shortcomings, we develop OmAgent, efficiently stores and retrieves relevant video frames for specific queries, preserving the detailed content of videos. Additionally, it features an Divide-and-Conquer Loop capable of autonomous reasoning, dynamically invoking APIs and tools to enhance query processing and accuracy. This approach ensures robust video understanding, significantly reducing information loss. Experimental results affirm OmAgent's efficacy in handling various types of videos and complex tasks. Moreover, we have endowed it with greater autonomy and a robust tool-calling system, enabling it to accomplish even more intricate tasks.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16635",
        "abstract url": "https://arxiv.org/abs/2406.16635",
        "title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because the permanent removal of attention heads or neurons from LLMs can significantly degrade accuracy. Prior work has attempted to model contextual sparsity using neural networks trained to predict activation magnitudes, which can be used to dynamically prune structures with low predicted activation magnitude. In this paper, we look beyond magnitude-based pruning criteria to assess attention head and neuron importance in LLMs. We developed a novel predictor called ShadowLLM, which can shadow the LLM behavior and enforce better sparsity patterns, resulting in over 15% improvement in end-to-end accuracy without increasing latency compared to previous methods. ShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu framework. These enhancements are validated on models with up to 30 billion parameters. Our code is available at \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16655",
        "abstract url": "https://arxiv.org/abs/2406.16655",
        "title": "Large Language Models Are Cross-Lingual Knowledge-Free Reasoners",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models have demonstrated impressive reasoning capabilities across multiple languages. However, the relationship between capabilities in different languages is less explored. In this work, we decompose the process of reasoning tasks into two separated parts: knowledge retrieval and knowledge-free reasoning, and analyze the cross-lingual transferability of them. With adapted and constructed knowledge-free reasoning datasets, we show that the knowledge-free reasoning capability can be nearly perfectly transferred across various source-target language directions despite the secondary impact of resource in some specific target languages, while cross-lingual knowledge retrieval significantly hinders the transfer. Moreover, by analyzing the hidden states and feed-forward network neuron activation during the reasoning tasks, we show that higher similarity of hidden representations and larger overlap of activated neurons could explain the better cross-lingual transferability of knowledge-free reasoning than knowledge retrieval. Thus, we hypothesize that knowledge-free reasoning embeds in some language-shared mechanism, while knowledge is stored separately in different languages.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16672",
        "abstract url": "https://arxiv.org/abs/2406.16672",
        "title": "CAVE: Controllable Authorship Verification Explanations",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Authorship Verification (AV) (do two documents have the same author?) is essential for many sensitive real-life applications. AV is often used in proprietary domains that require a private, offline model, making SOTA online models like ChatGPT undesirable. Other SOTA systems use methods, e.g. Siamese Networks, that are uninterpretable, and hence cannot be trusted in high-stakes applications. In this work, we take the first step to address the above challenges with our model CAVE (Controllable Authorship Verification Explanations): CAVE generates free-text AV explanations that are controlled to be 1) structured (can be decomposed into sub-explanations with respect to relevant linguistic features), and 2) easily verified for explanation-label consistency (via intermediate labels in sub-explanations). In this work, we train a Llama-3-8B as CAVE; since there are no human-written corpora for AV explanations, we sample silver-standard explanations from GPT-4-TURBO and distill them into a pretrained Llama-3-8B. Results on three difficult AV datasets IMdB2, Blog-Auth, and FanFiction show that CAVE generates high quality explanations (as measured by automatic and human evaluation) as well as competitive task accuracies.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16674",
        "abstract url": "https://arxiv.org/abs/2406.16674",
        "title": "Computational Approaches to the Detection of Lesser-Known Rhetorical Figures: A Systematic Survey and Research Challenges",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Rhetorical figures play a major role in our everyday communication as they make text more interesting, more memorable, or more persuasive. Therefore, it is important to computationally detect rhetorical figures to fully understand the meaning of a text. We provide a comprehensive overview of computational approaches to lesser-known rhetorical figures. We explore the linguistic and computational perspectives on rhetorical figures, emphasizing their significance for the domain of Natural Language Processing. We present different figures in detail, delving into datasets, definitions, rhetorical functions, and detection approaches. We identified challenges such as dataset scarcity, language limitations, and reliance on rule-based methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Submitted to ACM Computing Surveys. 35 pages"
    },
    {
        "paper id": "2406.16690",
        "abstract url": "https://arxiv.org/abs/2406.16690",
        "title": "Scaling Laws for Linear Complexity Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for softmax attention for comparison. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Technical report. Yiran Zhong is the corresponding author"
    },
    {
        "paper id": "2406.16694",
        "abstract url": "https://arxiv.org/abs/2406.16694",
        "title": "Task Oriented In-Domain Data Augmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown superior performance in various applications and fields. To achieve better performance on specialized domains such as law and advertisement, LLMs are often continue pre-trained on in-domain data. However, existing approaches suffer from two major issues. First, in-domain data are scarce compared with general domain-agnostic data. Second, data used for continual pre-training are not task-aware, such that they may not be helpful to downstream applications. We propose TRAIT, a task-oriented in-domain data augmentation framework. Our framework is divided into two parts: in-domain data selection and task-oriented synthetic passage generation. The data selection strategy identifies and selects a large amount of in-domain data from general corpora, and thus significantly enriches domain knowledge in the continual pre-training data. The synthetic passages contain guidance on how to use domain knowledge to answer questions about downstream tasks. By training on such passages, the model aligns with the need of downstream applications. We adapt LLMs to two domains: advertisement and math. On average, TRAIT improves LLM performance by 8% in the advertisement domain and 7.5% in the math domain.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16714",
        "abstract url": "https://arxiv.org/abs/2406.16714",
        "title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks. As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically. Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students' learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor. The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude. More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks. Code and data are publicly available at https://github.com/thu-coai/AutoDetect.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16732",
        "abstract url": "https://arxiv.org/abs/2406.16732",
        "title": "CLIMATELI: Evaluating Entity Linking on Climate Change Data",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Climate Change (CC) is a pressing topic of global importance, attracting increasing attention across research fields, from social sciences to Natural Language Processing (NLP). CC is also discussed in various settings and communication platforms, from academic publications to social media forums. Understanding who and what is mentioned in such data is a first critical step to gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking), the first manually annotated CC dataset that links 3,087 entity spans to Wikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing entity linking (EL) systems on the CC topic across various genres and propose automated filtering methods for CC entities. We find that the performance of EL models notably lags behind humans at both token and entity levels. Testing within the scope of retaining or excluding non-nominal and/or non-CC entities particularly impacts the models' performances.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "8 pages, accepted at ClimateNLP 2024 workshop @ ACL 2024"
    },
    {
        "paper id": "2406.16740",
        "abstract url": "https://arxiv.org/abs/2406.16740",
        "title": "Learning the boundary-to-domain mapping using Lifting Product Fourier Neural Operators for partial differential equations",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Neural operators such as the Fourier Neural Operator (FNO) have been shown to provide resolution-independent deep learning models that can learn mappings between function spaces. For example, an initial condition can be mapped to the solution of a partial differential equation (PDE) at a future time-step using a neural operator. Despite the popularity of neural operators, their use to predict solution functions over a domain given only data over the boundary (such as a spatially varying Dirichlet boundary condition) remains unexplored. In this paper, we refer to such problems as boundary-to-domain problems; they have a wide range of applications in areas such as fluid mechanics, solid mechanics, heat transfer etc. We present a novel FNO-based architecture, named Lifting Product FNO (or LP-FNO) which can map arbitrary boundary functions defined on the lower-dimensional boundary to a solution in the entire domain. Specifically, two FNOs defined on the lower-dimensional boundary are lifted into the higher dimensional domain using our proposed lifting product layer. We demonstrate the efficacy and resolution independence of the proposed LP-FNO for the 2D Poisson equation.",
        "subjects": [
            "cs.LG",
            "math.NA"
        ],
        "comment": "Accepted by ICML 2024 AI for Science Workshop"
    },
    {
        "paper id": "2406.16743",
        "abstract url": "https://arxiv.org/abs/2406.16743",
        "title": "Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) can effectively reduce harmful responses from LLMs, they often require high-quality datasets and heavy computational overhead during model training. Another way to align language models is to modify the logit of tokens in model outputs without heavy training. Recent studies have shown that contrastive decoding can enhance the performance of language models by reducing the likelihood of confused tokens. However, these methods require the manual selection of contrastive models or instruction templates. To this end, we propose Adversarial Contrastive Decoding (ACD), an optimization-based framework to generate two opposite system prompts for prompt-based contrastive decoding. ACD only needs to apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min for each model) without training the target model. Experiments conducted on extensive models and benchmarks demonstrate that the proposed method achieves much better safety performance than previous model training-free decoding methods without sacrificing its original generation ability.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16746",
        "abstract url": "https://arxiv.org/abs/2406.16746",
        "title": "The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16747",
        "abstract url": "https://arxiv.org/abs/2406.16747",
        "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "preprint"
    },
    {
        "paper id": "2406.16748",
        "abstract url": "https://arxiv.org/abs/2406.16748",
        "title": "OCALM: Object-Centric Assessment with Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Properly defining a reward signal to efficiently train a reinforcement learning (RL) agent is a challenging task. Designing balanced objective functions from which a desired behavior can emerge requires expert knowledge, especially for complex environments. Learning rewards from human feedback or using large language models (LLMs) to directly provide rewards are promising alternatives, allowing non-experts to specify goals for the agent. However, black-box reward models make it difficult to debug the reward. In this work, we propose Object-Centric Assessment with Language Models (OCALM) to derive inherently interpretable reward functions for RL agents from natural language task descriptions. OCALM uses the extensive world-knowledge of LLMs while leveraging the object-centric nature common to many environments to derive reward functions focused on relational concepts, providing RL agents with the ability to derive policies from task descriptions.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "Accepted at the RLBRew Workshop at RLC 2024"
    },
    {
        "paper id": "2406.16751",
        "abstract url": "https://arxiv.org/abs/2406.16751",
        "title": "Towards Zero-Shot Text-To-Speech for Arabic Dialects",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Zero-shot multi-speaker text-to-speech (ZS-TTS) systems have advanced for English, however, it still lags behind due to insufficient resources. We address this gap for Arabic, a language of more than 450 million native speakers, by first adapting a sizeable existing dataset to suit the needs of speech synthesis. Additionally, we employ a set of Arabic dialect identification models to explore the impact of pre-defined dialect labels on improving the ZS-TTS model in a multi-dialect setting. Subsequently, we fine-tune the XTTS\\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc} model, an open-source architecture. We then evaluate our models on a dataset comprising 31 unseen speakers and an in-house dialectal dataset. Our automated and human evaluation results show convincing performance while capable of generating dialectal speech. Our study highlights significant potential for improvements in this emerging area of research in Arabic.",
        "subjects": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16758",
        "abstract url": "https://arxiv.org/abs/2406.16758",
        "title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16767",
        "abstract url": "https://arxiv.org/abs/2406.16767",
        "title": "The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The improved generative capabilities of large language models have made them a powerful tool for creative writing and storytelling. It is therefore important to quantitatively understand the nature of generated stories, and how they differ from human storytelling. We augment the Reddit WritingPrompts dataset with short stories generated by GPT-3.5, given the same prompts. We quantify and compare the emotional and descriptive features of storytelling from both generative processes, human and machine, along a set of six dimensions. We find that generated stories differ significantly from human stories along all six dimensions, and that human and machine generations display similar biases when grouped according to the narrative point-of-view and gender of the main protagonist. We release our dataset and code at https://github.com/KristinHuangg/gpt-writing-prompts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16777",
        "abstract url": "https://arxiv.org/abs/2406.16777",
        "title": "Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic Speech Recognition (ASR), Machine Translation (MT), and even End-to-End Speech Translation (ST). In this paper, we present KIT's offline submission in the constrained + LLM track by incorporating recently proposed techniques that can be added to any cascaded speech translation. Specifically, we integrate Mistral-7B\\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to enhance it in two ways. Firstly, we refine the ASR outputs by utilizing the N-best lists generated by our system and fine-tuning the LLM to predict the transcript accurately. Secondly, we refine the MT outputs at the document level by fine-tuning the LLM, leveraging both ASR and MT predictions to improve translation quality. We find that integrating the LLM into the ASR and MT systems results in an absolute improvement of $0.3\\%$ in Word Error Rate and $0.65\\%$ in COMET for tst2019 test set. In challenging test sets with overlapping speakers and background noise, we find that integrating LLM is not beneficial due to poor ASR performance. Here, we use ASR with chunked long-form decoding to improve context usage that may be unavailable when transcribing with Voice Activity Detection segmentation alone.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16778",
        "abstract url": "https://arxiv.org/abs/2406.16778",
        "title": "Finding Transformer Circuits with Edge Pruning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The path to interpreting a language model often proceeds via analysis of circuits -- sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they rely either on inefficient search algorithms or inaccurate approximations. In this paper, we frame automated circuit discovery as an optimization problem and propose *Edge Pruning* as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, it prunes the \\emph{edges} between components. Our method finds circuits in GPT-2 that use less than half the number of edges compared to circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient even with as many as 100K examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale that prior methods operate on. We use this setting for a case study comparing the mechanisms behind instruction prompting and in-context learning. We find two circuits with more than 99.96% sparsity that match the performance of the full model and reveal that the mechanisms in the two settings overlap substantially. Our case study shows that Edge Pruning is a practical and scalable tool for interpretability and sheds light on behaviors that only emerge in large models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "We release our code and data publicly at https://github.com/princeton-nlp/Edge-Pruning"
    },
    {
        "paper id": "2406.16779",
        "abstract url": "https://arxiv.org/abs/2406.16779",
        "title": "It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Natural language processing has seen rapid progress over the past decade. Due to the speed of developments, some practices get established without proper evaluation. Considering one such case and focusing on reading comprehension, we ask our first research question: 1) How does the order of inputs -- i.e., question and context -- affect model performance? Additionally, given recent advancements in input emphasis, we ask a second research question: 2) Does emphasizing either the question, the context, or both enhance performance? Experimenting with 9 large language models across 3 datasets, we find that presenting the context before the question improves model performance, with an accuracy increase of up to $31\\%$. Furthermore, emphasizing the context yields superior results compared to question emphasis, and in general, emphasizing parts of the input is particularly effective for addressing questions that models lack the parametric knowledge to answer. Experimenting with both prompt-based and attention-based emphasis methods, we additionally find that the best method is surprisingly simple: it only requires concatenating a few tokens to the input and results in an accuracy improvement of up to $36\\%$, allowing smaller models to outperform their significantly larger counterparts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to ACL Findings"
    },
    {
        "paper id": "2406.16782",
        "abstract url": "https://arxiv.org/abs/2406.16782",
        "title": "Confidence Aware Inverse Constrained Reinforcement Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "In coming up with solutions to real-world problems, humans implicitly adhere to constraints that are too numerous and complex to be specified completely. However, reinforcement learning (RL) agents need these constraints to learn the correct optimal policy in these settings. The field of Inverse Constraint Reinforcement Learning (ICRL) deals with this problem and provides algorithms that aim to estimate the constraints from expert demonstrations collected offline. Practitioners prefer to know a measure of confidence in the estimated constraints, before deciding to use these constraints, which allows them to only use the constraints that satisfy a desired level of confidence. However, prior works do not allow users to provide the desired level of confidence for the inferred constraints. This work provides a principled ICRL method that can take a confidence level with a set of expert demonstrations and outputs a constraint that is at least as constraining as the true underlying constraint with the desired level of confidence. Further, unlike previous methods, this method allows a user to know if the number of expert trajectories is insufficient to learn a constraint with a desired level of confidence, and therefore collect more expert trajectories as required to simultaneously learn constraints with the desired level of confidence and a policy that achieves the desired level of performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Paper to appear in ICML 2024"
    },
    {
        "paper id": "2406.16783",
        "abstract url": "https://arxiv.org/abs/2406.16783",
        "title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on high resource languages such as English. In this work, we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual, Multi-turn instruction finetuning dataset, called M2Lingual, to better align LLMs on a diverse set of languages and tasks. M2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds, covering 70 languages, 17 NLP tasks and general instruction-response pairs. LLMs finetuned with M2Lingual substantially outperform the majority of existing multilingual IFT datasets. Importantly, LLMs trained with M2Lingual consistently achieve competitive results across a wide variety of evaluation benchmarks compared to existing multilingual IFT datasets. Specifically, LLMs finetuned with M2Lingual achieve strong performance on our translated multilingual, multi-turn evaluation benchmark as well as a wide variety of multilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for its creation. M2Lingual repository - https://huggingface.co/datasets/ServiceNow-AI/M2Lingual",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "39 pages"
    },
    {
        "paper id": "2406.16784",
        "abstract url": "https://arxiv.org/abs/2406.16784",
        "title": "The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The transformer neural network architecture allows for autoregressive sequence-to-sequence modeling through the use of attention layers. It was originally created with the application of machine translation but has revolutionized natural language processing. Recently, transformers have also been applied across a wide variety of pattern recognition tasks, particularly in computer vision. In this literature review, we describe major advances in computer vision utilizing transformers. We then focus specifically on Multi-Object Tracking (MOT) and discuss how transformers are increasingly becoming competitive in state-of-the-art MOT works, yet still lag behind traditional deep learning methods.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "This report was written in November 2022, and may not contain more recent works since then"
    },
    {
        "paper id": "2406.16797",
        "abstract url": "https://arxiv.org/abs/2406.16797",
        "title": "Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Existing methods for adapting large language models (LLMs) to new tasks are not suited to multi-task adaptation because they modify all the model weights -- causing destructive interference between tasks. The resulting effects, such as catastrophic forgetting of earlier tasks, make it challenging to obtain good performance on multiple tasks at the same time. To mitigate this, we propose Lottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies and optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide range of challenging tasks such as instruction following, reasoning, math, and summarization. LoTA obtains better performance than full fine-tuning and low-rank adaptation (LoRA), and maintains good performance even after training on other tasks -- thus, avoiding catastrophic forgetting. By extracting and fine-tuning over lottery tickets (or sparse task vectors), LoTA also enables model merging over highly dissimilar tasks. Our code is made publicly available at https://github.com/kiddyboots216/lottery-ticket-adaptation.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16801",
        "abstract url": "https://arxiv.org/abs/2406.16801",
        "title": "RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The instruction-following ability of Large Language Models (LLMs) has cultivated a class of LLM-based systems capable of approaching complex tasks such as making edits to large code repositories. Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems. We propose RES-Q, a natural language instruction-based benchmark for evaluating $\\textbf{R}$epository $\\textbf{E}$diting $\\textbf{S}$ystems, which consists of 100 handcrafted repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system's ability to interpret the instruction, navigate the repository to gather relevant information, and construct an appropriate edit that satisfies the specified criteria. We argue that evaluating LLMs in this way addresses issues with traditional benchmarks and provides a more holistic assessment of a model's abilities. We evaluate various state-of-the-art LLMs as language agents in a repository-editing system built on Qurrent OS, our language agent development software. Despite their 1% pass@1 performance difference on HumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's capacity to differentiate model capability as traditional benchmarks approach saturation. We further investigate token efficiency, performance relationships with existing benchmarks, and interesting disparities between closed and open-source LLMs. Code and dataset are available at https://github.com/Qurrent-AI/RES-Q.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16829",
        "abstract url": "https://arxiv.org/abs/2406.16829",
        "title": "Understanding and Mitigating Tokenization Bias in Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "State-of-the-art language models are autoregressive and operate on subword units known as tokens. Specifically, one must encode the conditioning string into a list of tokens before passing to the language models for next-token prediction. We show that, for encoding schemes such as maximum prefix matching, tokenization induces a sampling bias that cannot be mitigated with more training or data. To counter this universal problem, we propose a novel algorithm to obtain unbiased estimates from a model that was trained on tokenized data. Our method does not require finetuning the model, and its complexity, defined as the number of model runs, scales linearly with the sequence length. As a consequence, we show that one can simulate token-free behavior from a tokenized language model. We empirically verify the correctness of our method through a Markov-chain setup, where it accurately recovers the transition probabilities, as opposed to the conventional method of directly prompting tokens into the language model.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16833",
        "abstract url": "https://arxiv.org/abs/2406.16833",
        "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising, and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) It is time-consuming and costly; 2) Conversation threads could be very long, increasing chances of noisy annotations; and 3) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human annotation process on the following two tasks while also providing reasoning: i) User Stance classification, which involves labeling a user's stance of a post in a conversation on a five-point scale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. The majority voting on zero-shot, one-shot, and few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available [https://anonymous.4open.science/r/USDC-0F7F].",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "32 pages, 18 figures"
    },
    {
        "paper id": "2406.16838",
        "abstract url": "https://arxiv.org/abs/2406.16838",
        "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16842",
        "abstract url": "https://arxiv.org/abs/2406.16842",
        "title": "Exploring Factual Entailment with NLI: A News Media Study",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We explore the relationship between factuality and Natural Language Inference (NLI) by introducing FactRel -- a novel annotation scheme that models \\textit{factual} rather than \\textit{textual} entailment, and use it to annotate a dataset of naturally occurring sentences from news articles. Our analysis shows that 84\\% of factually supporting pairs and 63\\% of factually undermining pairs do not amount to NLI entailment or contradiction, respectively, suggesting that factual relationships are more apt for analyzing media discourse. We experiment with models for pairwise classification on the new dataset, and find that in some cases, generating synthetic data with GPT-4 on the basis of the annotated dataset can improve performance. Surprisingly, few-shot learning with GPT-4 yields strong results on par with medium LMs (DeBERTa) trained on the labelled dataset. We hypothesize that these results indicate the fundamental dependence of this task on both world knowledge and advanced reasoning abilities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Presented at *SEM 2024"
    },
    {
        "paper id": "2406.16852",
        "abstract url": "https://arxiv.org/abs/2406.16852",
        "title": "Long Context Transfer from Language to Vision",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video sequences offer valuable temporal information, but existing large multimodal models (LMMs) fall short in understanding extremely long videos. Many works address this by reducing the number of visual tokens using visual resamplers. Alternatively, in this paper, we approach this problem from the perspective of the language model. By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training. We call this phenomenon long context transfer and carefully ablate its properties. To effectively measure LMMs' ability to generalize to long contexts in the vision modality, we develop V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model's NIAH test. Our proposed Long Video Assistant (LongVA) can process 2000 frames or over 200K visual tokens without additional complexities. With its extended context length, LongVA achieves state-of-the-art performance on Video-MME among 7B-scale models by densely sampling more input frames. Our work is open-sourced at https://github.com/EvolvingLMMs-Lab/LongVA.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16855",
        "abstract url": "https://arxiv.org/abs/2406.16855",
        "title": "DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Personalized image generation holds great promise in assisting humans in everyday work and life due to its impressive function in creatively generating personalized content. However, current evaluations either are automated but misalign with humans or require human evaluations that are time-consuming and expensive. In this work, we present DreamBench++, a human-aligned benchmark automated by advanced multimodal GPT models. Specifically, we systematically design the prompts to let GPT be both human-aligned and self-aligned, empowered with task reinforcement. Further, we construct a comprehensive dataset comprising diverse images and prompts. By benchmarking 7 modern generative models, we demonstrate that DreamBench++ results in significantly more human-aligned evaluation, helping boost the community with innovative findings.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://dreambenchplus.github.io/"
    },
    {
        "paper id": "2406.16858",
        "abstract url": "https://arxiv.org/abs/2406.16858",
        "title": "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16860",
        "abstract url": "https://arxiv.org/abs/2406.16860",
        "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Website at https://cambrian-mllm.github.io"
    },
    {
        "paper id": "2406.16866",
        "abstract url": "https://arxiv.org/abs/2406.16866",
        "title": "Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Referring expression comprehension (REC) involves localizing a target instance based on a textual description. Recent advancements in REC have been driven by large multimodal models (LMMs) like CogVLM, which achieved 92.44% accuracy on RefCOCO. However, this study questions whether existing benchmarks such as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive capabilities. We begin with a manual examination of these benchmarks, revealing high labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg, which undermines the authenticity of evaluations. We address this by excluding problematic instances and reevaluating several LMMs capable of handling the REC task, showing significant accuracy improvements, thus highlighting the impact of benchmark noise. In response, we introduce Ref-L4, a comprehensive REC benchmark, specifically designed to evaluate modern REC models. Ref-L4 is distinguished by four key features: 1) a substantial sample size with 45,341 annotations; 2) a diverse range of object categories with 365 distinct types and varying instance scales from 30 to 3,767; 3) lengthy referring expressions averaging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique words. We evaluate a total of 24 large models on Ref-L4 and provide valuable insights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as our Ref-L4 benchmark and evaluation code, are available at https://github.com/JierunChen/Ref-L4.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17047",
        "abstract url": "https://arxiv.org/abs/2406.17047",
        "title": "Enhancing Scientific Figure Captioning Through Cross-modal Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Scientific charts are essential tools for effectively communicating research findings, serving as a vital medium for conveying information and revealing data patterns. With the rapid advancement of science and technology, coupled with the advent of the big data era, the volume and diversity of scientific research data have surged, leading to an increase in the number and variety of charts. This trend presents new challenges for researchers, particularly in efficiently and accurately generating appropriate titles for these charts to better convey their information and results. Automatically generated chart titles can enhance information retrieval systems by providing precise data for detailed chart classification. As research in image captioning and text summarization matures, the automatic generation of scientific chart titles has gained significant attention. By leveraging natural language processing, machine learning, and multimodal techniques, it is possible to automatically extract key information from charts and generate accurate, concise titles that better serve the needs of researchers. This paper presents a novel approach to scientific chart title generation, demonstrating its effectiveness in improving the clarity and accessibility of research data.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "7 pages"
    },
    {
        "paper id": "2406.17095",
        "abstract url": "https://arxiv.org/abs/2406.17095",
        "title": "Attention Instruction: Amplifying Attention in the Middle via Prompting",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The context window of large language models has been extended to 128k tokens or more. However, language models still suffer from position bias and have difficulty in accessing and using the middle part of the context due to the lack of attention. We examine the relative position awareness of LLMs and the feasibility of mitigating disproportional attention through prompting. We augment the original task instruction with $\\texttt{attention instructions}$ that direct language models to allocate more attention towards a selected segment of the context. We conduct a comprehensive investigation on multi-document question answering task with both position-based and index-based instructions. We find that language models do not have relative position awareness of the context. Nevertheless, they demonstrate the capacity to adapt attention to a specific segment using matching indexes. Our analysis contributes to a deeper understanding of position bias in LLMs and provides a pathway to mitigate this bias by instruction, thus benefiting LLMs in locating and utilizing relevant information from retrieved documents in RAG applications.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17098",
        "abstract url": "https://arxiv.org/abs/2406.17098",
        "title": "Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Temporal distances lie at the heart of many algorithms for planning, control, and reinforcement learning that involve reaching goals, allowing one to estimate the transit time between two states. However, prior attempts to define such temporal distances in stochastic settings have been stymied by an important limitation: these prior approaches do not satisfy the triangle inequality. This is not merely a definitional concern, but translates to an inability to generalize and find shortest paths. In this paper, we build on prior work in contrastive learning and quasimetrics to show how successor features learned by contrastive learning (after a change of variables) form a temporal distance that does satisfy the triangle inequality, even in stochastic settings. Importantly, this temporal distance is computationally efficient to estimate, even in high-dimensional and stochastic settings. Experiments in controlled settings and benchmark suites demonstrate that an RL algorithm based on these new temporal distances exhibits combinatorial generalization (i.e., \"stitching\") and can sometimes learn more quickly than prior methods, including those based on quasimetrics.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Proceedings of the 41st International Conference on Machine Learning (ICML 2024)"
    },
    {
        "paper id": "2406.17103",
        "abstract url": "https://arxiv.org/abs/2406.17103",
        "title": "Maximum Likelihood Estimation of the Direction of Sound In A Reverberant Noisy Environment",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "We describe a new method for estimating the direction of sound in a reverberant environment from basic principles of sound propagation. The method utilizes SNR-adaptive features from time-delay and energy of the directional components after acoustic wave decomposition of the observed sound field to estimate the line-of-sight direction under noisy and reverberant conditions. The effectiveness of the approach is established with real-data of different microphone array configurations under various usage scenarios.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": "5 pages, 2 figures, conference"
    },
    {
        "paper id": "2406.17117",
        "abstract url": "https://arxiv.org/abs/2406.17117",
        "title": "Speeding Up Image Classifiers with Little Companions",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Scaling up neural networks has been a key recipe to the success of large language and vision models. However, in practice, up-scaled models can be disproportionately costly in terms of computations, providing only marginal improvements in performance; for example, EfficientViT-L3-384 achieves <2% improvement on ImageNet-1K accuracy over the base L1-224 model, while requiring $14\\times$ more multiply-accumulate operations (MACs). In this paper, we investigate scaling properties of popular families of neural networks for image classification, and find that scaled-up models mostly help with \"difficult\" samples. Decomposing the samples by difficulty, we develop a simple model-agnostic two-pass Little-Big algorithm that first uses a light-weight \"little\" model to make predictions of all samples, and only passes the difficult ones for the \"big\" model to solve. Good little companion achieve drastic MACs reduction for a wide variety of model families and scales. Without loss of accuracy or modification of existing models, our Little-Big models achieve MACs reductions of 76% for EfficientViT-L3-384, 81% for EfficientNet-B7-600, 71% for DeiT3-L-384 on ImageNet-1K. Little-Big also speeds up the InternImage-G-512 model by 62% while achieving 90% ImageNet-1K top-1 accuracy, serving both as a strong baseline and as a simple practical method for large model compression.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17124",
        "abstract url": "https://arxiv.org/abs/2406.17124",
        "title": "Investigating Confidence Estimation Measures for Speaker Diarization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Speaker diarization systems segment a conversation recording based on the speakers' identity. Such systems can misclassify the speaker of a portion of audio due to a variety of factors, such as speech pattern variation, background noise, and overlapping speech. These errors propagate to, and can adversely affect, downstream systems that rely on the speaker's identity, such as speaker-adapted speech recognition. One of the ways to mitigate these errors is to provide segment-level diarization confidence scores to downstream systems. In this work, we investigate multiple methods for generating diarization confidence scores, including those derived from the original diarization system and those derived from an external model. Our experiments across multiple datasets and diarization systems demonstrate that the most competitive confidence score methods can isolate ~30% of the diarization errors within segments with the lowest ~10% of confidence scores.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": "Accepted in INTERSPEECH 2024"
    },
    {
        "paper id": "2406.17135",
        "abstract url": "https://arxiv.org/abs/2406.17135",
        "title": "Testing network clustering algorithms with Natural Language Processing",
        "rating": "1",
        "keywords": [
            [
                "cs.SI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "The advent of online social networks has led to the development of an abundant literature on the study of online social groups and their relationship to individuals' personalities as revealed by their textual productions. Social structures are inferred from a wide range of social interactions. Those interactions form complex -- sometimes multi-layered -- networks, on which community detection algorithms are applied to extract higher order structures. The choice of the community detection algorithm is however hardily questioned in relation with the cultural production of the individual they classify. In this work, we assume the entangled nature of social networks and their cultural production to propose a definition of cultural based online social groups as sets of individuals whose online production can be categorized as social group-related. We take advantage of this apparently self-referential description of online social groups with a hybrid methodology that combines a community detection algorithm and a natural language processing classification algorithm. A key result of this analysis is the possibility to score community detection algorithms using their agreement with the natural language processing classification. A second result is that we can assign the opinion of a random user at >85% accuracy.",
        "subjects": [
            "cs.SI",
            "cs.CL",
            "cs.CY",
            "physics.soc-ph"
        ],
        "comment": "10 pages, 8 figures"
    },
    {
        "paper id": "2406.17146",
        "abstract url": "https://arxiv.org/abs/2406.17146",
        "title": "Vastextures: Vast repository of textures and PBR materials extracted from real-world images using unsupervised methods",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vastextures is a vast repository of 500,000 textures and PBR materials extracted from real-world images using an unsupervised process. The extracted materials and textures are extremely diverse and cover a vast range of real-world patterns, but at the same time less refined compared to existing repositories. The repository is composed of 2D textures cropped from natural images and SVBRDF/PBR materials generated from these textures. Textures and PBR materials are essential for CGI. Existing materials repositories focus on games, animation, and arts, that demand a limited amount of high-quality assets. However, virtual worlds and synthetic data are becoming increasingly important for training A.I systems for computer vision. This application demands a huge amount of diverse assets but at the same time less affected by noisy and unrefined assets. Vastexture aims to address this need by creating a free, huge, and diverse assets repository that covers as many real-world materials as possible. The materials are automatically extracted from natural images in two steps: 1) Automatically scanning a giant amount of images to identify and crop regions with uniform textures. This is done by splitting the image into a grid of cells and identifying regions in which all of the cells share a similar statistical distribution. 2) Extracting the properties of the PBR material from the cropped texture. This is done by randomly guessing every correlation between the properties of the texture image and the properties of the PBR material. The resulting PBR materials exhibit a vast amount of real-world patterns as well as unexpected emergent properties. Neutral nets trained on this repository outperformed nets trained using handcrafted assets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Vastexture was published as part of Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data, refer to this work in citations. This document gives a more detailed and technical discussion of this repository"
    },
    {
        "paper id": "2406.17148",
        "abstract url": "https://arxiv.org/abs/2406.17148",
        "title": "Unambiguous Recognition Should Not Rely Solely on Natural Language Training",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In LaTeX text recognition using Transformer-based architectures, this paper identifies certain \"bias\" issues. For instance, $e-t$ is frequently misrecognized as $e^{-t}$. This bias stems from the inherent characteristics of the dataset. To mitigate this bias, we propose a LaTeX printed text recognition model trained on a mixed dataset of pseudo-formulas and pseudo-text. The model employs a Swin Transformer as the encoder and a RoBERTa model as the decoder. Experimental results demonstrate that this approach reduces \"bias\", enhancing the accuracy and robustness of text recognition. For clear images, the model strictly adheres to the image content; for blurred images, it integrates both image and contextual information to produce reasonable recognition results.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17158",
        "abstract url": "https://arxiv.org/abs/2406.17158",
        "title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While retrieval performance for classical QA tasks is well explored, their capabilities for heterogeneous complex retrieval tasks, especially in an open-domain setting, and the impact on downstream QA performance, are relatively unexplored. To address this, in this work, we propose a benchmark composing diverse complex QA tasks and provide a toolkit to evaluate state-of-the-art pre-trained dense and sparse retrieval models in an open-domain setting. We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre-trained dense retrieval models. In addition, since context-based reasoning is critical for solving complex QA tasks, we also evaluate the reasoning capabilities of LLMs and the impact of retrieval performance on their reasoning capabilities. Through experiments, we observe that much progress is to be made in retrieval for complex QA to improve downstream QA performance. Our software and related data can be accessed at https://github.com/VenkteshV/DEXTER",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": "under submission, 22 pages"
    },
    {
        "paper id": "2406.17162",
        "abstract url": "https://arxiv.org/abs/2406.17162",
        "title": "Virtual Mines -- Component-level recycling of printed circuit boards using deep learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This contribution gives an overview of an ongoing project using machine learning and computer vision components for improving the electronic waste recycling process. In circular economy, the \"virtual mines\" concept refers to production cycles where interesting raw materials are reclaimed in an efficient and cost-effective manner from end-of-life items. In particular, the growth of e-waste, due to the increasingly shorter life cycle of hi-tech goods, is a global problem. In this paper, we describe a pipeline based on deep learning model to recycle printed circuit boards at the component level. A pre-trained YOLOv5 model is used to analyze the results of the locally developed dataset. With a different distribution of class instances, YOLOv5 managed to achieve satisfactory precision and recall, with the ability to optimize with large component instances.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2406.17163",
        "abstract url": "https://arxiv.org/abs/2406.17163",
        "title": "Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLM) have achieved remarkable success in natural language generation but lesser focus has been given to their applicability in decision making tasks such as classification. We show that LLMs like LLaMa can achieve high performance on large multi-class classification tasks but still make classification errors and worse, generate out-of-vocabulary class labels. To address these critical issues, we introduce Paraphrase and AGgregate (PAG)-LLM approach wherein an LLM generates multiple paraphrases of the input query (parallel queries), performs multi-class classification for the original query and each paraphrase, and at the end aggregate all the classification labels based on their confidence scores. We evaluate PAG-LLM on two large multi-class classication datasets: CLINC, and Banking and show 22.7% and 15.1% error reduction. We show that PAG-LLM is especially effective for hard examples where LLM is uncertain, and reduces the critical misclassification and hallucinated label generation errors",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at SIGIR 2024"
    },
    {
        "paper id": "2406.17185",
        "abstract url": "https://arxiv.org/abs/2406.17185",
        "title": "Vaporetto: Efficient Japanese Tokenization Based on Improved Pointwise Linear Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper proposes an approach to improve the runtime efficiency of Japanese tokenization based on the pointwise linear classification (PLC) framework, which formulates the whole tokenization process as a sequence of linear classification problems. Our approach optimizes tokenization by leveraging the characteristics of the PLC framework and the task definition. Our approach involves (1) composing multiple classifications into array-based operations, (2) efficient feature lookup with memory-optimized automata, and (3) three orthogonal pre-processing methods for reducing actual score calculation. Thus, our approach makes the tokenization speed 5.7 times faster than the current approach based on the same model without decreasing tokenization accuracy. Our implementation is available at https://github.com/daac-tools/vaporetto under the MIT or Apache-2.0 license.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17186",
        "abstract url": "https://arxiv.org/abs/2406.17186",
        "title": "CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented Analysis Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Legal professionals need to write analyses that rely on citations to relevant precedents, i.e., previous case decisions. Intelligent systems assisting legal professionals in writing such documents provide great benefits but are challenging to design. Such systems need to help locate, summarize, and reason over salient precedents in order to be useful. To enable systems for such tasks, we work with legal professionals to transform a large open-source legal corpus into a dataset supporting two important backbone tasks: information retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC (Case Law Evaluation Retrieval Corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. We benchmark state-of-the-art models on CLERC, showing that current approaches still struggle: GPT-4o generates analyses with the highest ROUGE F-scores but hallucinates the most, while zero-shot IR models only achieve 48.3% recall@1000.",
        "subjects": [
            "cs.CL",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17190",
        "abstract url": "https://arxiv.org/abs/2406.17190",
        "title": "Sound Tagging in Infant-centric Home Soundscapes",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Certain environmental noises have been associated with negative developmental outcomes for infants and young children. Though classifying or tagging sound events in a domestic environment is an active research area, previous studies focused on data collected from a non-stationary microphone placed in the environment or from the perspective of adults. Further, many of these works ignore infants or young children in the environment or have data collected from only a single family where noise from the fixed sound source can be moderate at the infant's position or vice versa. Thus, despite the recent success of large pre-trained models for noise event detection, the performance of these models on infant-centric noise soundscapes in the home is yet to be explored. To bridge this gap, we have collected and labeled noises in home soundscapes from 22 families in an unobtrusive manner, where the data are collected through an infant-worn recording device. In this paper, we explore the performance of a large pre-trained model (Audio Spectrogram Transformer [AST]) on our noise-conditioned infant-centric environmental data as well as publicly available home environmental datasets. Utilizing different training strategies such as resampling, utilizing public datasets, mixing public and infant-centric training sets, and data augmentation using noise and masking, we evaluate the performance of a large pre-trained model on sparse and imbalanced infant-centric data. Our results show that fine-tuning the large pre-trained model by combining our collected dataset with public datasets increases the F1-score from 0.11 (public datasets) and 0.76 (collected datasets) to 0.84 (combined datasets) and Cohen's Kappa from 0.013 (public datasets) and 0.77 (collected datasets) to 0.83 (combined datasets) compared to only training with public or collected datasets, respectively.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": "Accepted in IEEE/ACM CHASE 2024"
    },
    {
        "paper id": "2406.17224",
        "abstract url": "https://arxiv.org/abs/2406.17224",
        "title": "Large Language Models are Interpretable Learners",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision-making. While symbolic rules offer interpretability, they often lack expressiveness, whereas neural networks excel in performance but are known for being black boxes. In this paper, we show a combination of Large Language Models (LLMs) and symbolic programs can bridge this gap. In the proposed LLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language prompts provides a massive set of interpretable modules that can transform raw input into natural language concepts. Symbolic programs then integrate these modules into an interpretable decision rule. To train LSPs, we develop a divide-and-conquer approach to incrementally build the program from scratch, where the learning process of each step is guided by LLMs. To evaluate the effectiveness of LSPs in extracting interpretable and accurate knowledge from data, we introduce IL-Bench, a collection of diverse tasks, including both synthetic and real-world scenarios across different modalities. Empirical results demonstrate LSP's superior performance compared to traditional neurosymbolic programs and vanilla automatic prompt tuning methods. Moreover, as the knowledge learned by LSP is a combination of natural language descriptions and symbolic rules, it is easily transferable to humans (interpretable), and other LLMs, and generalizes well to out-of-distribution samples.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.SC"
        ],
        "comment": "Preliminary Version, Code at [this url](https://github.com/ruocwang/llm-symbolic-program)"
    },
    {
        "paper id": "2406.17232",
        "abstract url": "https://arxiv.org/abs/2406.17232",
        "title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 18 topics loading on two non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17233",
        "abstract url": "https://arxiv.org/abs/2406.17233",
        "title": "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method recompiles the LLM's decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 7.35\\% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 55.03\\%.",
        "subjects": [
            "cs.SE",
            "cs.CL"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2406.17250",
        "abstract url": "https://arxiv.org/abs/2406.17250",
        "title": "A benchmark for 2D foetal brain ultrasound analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Brain development involves a sequence of structural changes from early stages of the embryo until several months after birth. Currently, ultrasound is the established technique for screening due to its ability to acquire dynamic images in real-time without radiation and to its cost-efficiency. However, identifying abnormalities remains challenging due to the difficulty in interpreting foetal brain images. In this work we present a set of 104 2D foetal brain ultrasound images acquired during the 20th week of gestation that have been co-registered to a common space from a rough skull segmentation. The images are provided both on the original space and template space centred on the ellipses of all the subjects. Furthermore, the images have been annotated to highlight landmark points from structures of interest to analyse brain development. Both the final atlas template with probabilistic maps and the original images can be used to develop new segmentation techniques, test registration approaches for foetal brain ultrasound, extend our work to longitudinal datasets and to detect anomalies in new images.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17255",
        "abstract url": "https://arxiv.org/abs/2406.17255",
        "title": "MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by ACL 2024, Main Conference"
    },
    {
        "paper id": "2406.17260",
        "abstract url": "https://arxiv.org/abs/2406.17260",
        "title": "Mitigating Hallucination in Fictional Character Role-Play",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Role-playing has wide-ranging applications in customer support, embodied agents, computational social science, etc. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with more than 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. The code and the dataset will be available at https://github.com/NafisSadeq/rolefact.git.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17262",
        "abstract url": "https://arxiv.org/abs/2406.17262",
        "title": "D2LLM: Decomposed and Distilled Large Language Models for Semantic Search",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The key challenge in semantic search is to create models that are both accurate and efficient in pinpointing relevant sentences for queries. While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications. In this paper, we present D2LLMs-Decomposed and Distilled LLMs for semantic search-that combines the best of both worlds. We decompose a cross-encoder into an efficient bi-encoder integrated with Pooling by Multihead Attention and an Interaction Emulation Module, achieving nuanced understanding and pre-computability. Knowledge from the LLM is distilled into this model using contrastive, rank, and feature imitation techniques. Our experiments show that D2LLM surpasses five leading baselines in terms of all metrics across three tasks, particularly improving NLI task performance by at least 6.45%. The source code is available at https://github.com/codefuse-ai/D2LLM.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17266",
        "abstract url": "https://arxiv.org/abs/2406.17266",
        "title": "AG-LSEC: Audio Grounded Lexical Speaker Error Correction",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL",
                "eess.AS"
            ]
        ],
        "abstract": "Speaker Diarization (SD) systems are typically audio-based and operate independently of the ASR system in traditional speech transcription pipelines and can have speaker errors due to SD and/or ASR reconciliation, especially around speaker turns and regions of speech overlap. To reduce these errors, a Lexical Speaker Error Correction (LSEC), in which an external language model provides lexical information to correct the speaker errors, was recently proposed. Though the approach achieves good Word Diarization error rate (WDER) improvements, it does not use any additional acoustic information and is prone to miscorrections. In this paper, we propose to enhance and acoustically ground the LSEC system with speaker scores directly derived from the existing SD pipeline. This approach achieves significant relative WDER reductions in the range of 25-40% over the audio-based SD, ASR system and beats the LSEC system by 15-25% relative on RT03-CTS, Callhome American English and Fisher datasets.",
        "subjects": [
            "eess.AS",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted at INTERSPEECH 2024"
    },
    {
        "paper id": "2406.17274",
        "abstract url": "https://arxiv.org/abs/2406.17274",
        "title": "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "63 pages, 41 figures, 11 tables"
    },
    {
        "paper id": "2406.17276",
        "abstract url": "https://arxiv.org/abs/2406.17276",
        "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a \"draft and then verify\" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16386",
        "abstract url": "https://arxiv.org/abs/2406.16386",
        "title": "Automatically Generating UI Code from Screenshot: A Divide-and-Conquer-Based Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Websites are critical in today's digital world, with over 1.11 billion currently active and approximately 252,000 new sites launched daily. Converting website layout design into functional UI code is a time-consuming yet indispensable step of website development. Manual methods of converting visual designs into functional code present significant challenges, especially for non-experts. To explore automatic design-to-code solutions, we first conduct a motivating study on GPT-4o and identify three types of issues in generating UI code: element omission, element distortion, and element misarrangement. We further reveal that a focus on smaller visual segments can help multimodal large language models (MLLMs) mitigate these failures in the generation process. In this paper, we propose DCGen, a divide-and-conquer-based approach to automate the translation of webpage design to UI code. DCGen starts by dividing screenshots into manageable segments, generating descriptions for each segment, and then reassembling them into complete UI code for the entire screenshot. We conduct extensive testing with a dataset comprised of real-world websites and various MLLMs and demonstrate that DCGen achieves up to a 14% improvement in visual similarity over competing methods. To the best of our knowledge, DCGen is the first segment-aware prompt-based approach for generating UI code directly from screenshots.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16437",
        "abstract url": "https://arxiv.org/abs/2406.16437",
        "title": "Theory on Mixture-of-Experts in Continual Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Continual learning (CL) has garnered significant attention because of its ability to adapt to new tasks that arrive over time. Catastrophic forgetting (of old tasks) has been identified as a major issue in CL, as the model adapts to new tasks. The Mixture-of-Experts (MoE) model has recently been shown to effectively mitigate catastrophic forgetting in CL, by employing a gating network to sparsify and distribute diverse tasks among multiple experts. However, there is a lack of theoretical analysis of MoE and its impact on the learning performance in CL. This paper provides the first theoretical results to characterize the impact of MoE in CL via the lens of overparameterized linear regression tasks. We establish the benefit of MoE over a single expert by proving that the MoE model can diversify its experts to specialize in different tasks, while its router learns to select the right expert for each task and balance the loads across all experts. Our study further suggests an intriguing fact that the MoE in CL needs to terminate the update of the gating network after sufficient training rounds to attain system convergence, which is not needed in the existing MoE studies that do not consider the continual task arrival. Furthermore, we provide explicit expressions for the expected forgetting and overall generalization error to characterize the benefit of MoE in the learning performance in CL. Interestingly, adding more experts requires additional rounds before convergence, which may not enhance the learning performance. Finally, we conduct experiments on both synthetic and real datasets to extend these insights from linear models to deep neural networks (DNNs), which also shed light on the practical algorithm design for MoE in CL.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16453",
        "abstract url": "https://arxiv.org/abs/2406.16453",
        "title": "Learning in Wilson-Cowan model for metapopulation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The Wilson-Cowan model for metapopulation, a Neural Mass Network Model, treats different subcortical regions of the brain as connected nodes, with connections representing various types of structural, functional, or effective neuronal connectivity between these regions. Each region comprises interacting populations of excitatory and inhibitory cells, consistent with the standard Wilson-Cowan model. By incorporating stable attractors into such a metapopulation model's dynamics, we transform it into a learning algorithm capable of achieving high image and text classification accuracy. We test it on MNIST and Fashion MNIST, in combination with convolutional neural networks, on CIFAR-10 and TF-FLOWERS, and, in combination with a transformer architecture (BERT), on IMDB, always showing high classification accuracy. These numerical evaluations illustrate that minimal modifications to the Wilson-Cowan model for metapopulation can reveal unique and previously unobserved dynamics.",
        "subjects": [
            "q-bio.NC",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16456",
        "abstract url": "https://arxiv.org/abs/2406.16456",
        "title": "Automated Privacy-Preserving Techniques via Meta-Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Sharing private data for learning tasks is pivotal for transparent and secure machine learning applications. Many privacy-preserving techniques have been proposed for this task aiming to transform the data while ensuring the privacy of individuals. Some of these techniques have been incorporated into tools, whereas others are accessed through various online platforms. However, such tools require manual configuration, which can be complex and time-consuming. Moreover, they require substantial expertise, potentially restricting their use to those with advanced technical knowledge. In this paper, we propose AUTOPRIV, the first automated privacy-preservation method, that eliminates the need for any manual configuration. AUTOPRIV employs meta-learning to automate the de-identification process, facilitating the secure release of data for machine learning tasks. The main goal is to anticipate the predictive performance and privacy risk of a large set of privacy configurations. We provide a ranked list of the most promising solutions, which are likely to achieve an optimal approximation within a new domain. AUTOPRIV is highly effective as it reduces computational complexity and energy consumption considerably.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "12 pages, 6 figures, 3 tables"
    },
    {
        "paper id": "2406.16468",
        "abstract url": "https://arxiv.org/abs/2406.16468",
        "title": "The Hidden Pitfalls of the Cosine Similarity Loss",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We show that the gradient of the cosine similarity between two points goes to zero in two under-explored settings: (1) if a point has large magnitude or (2) if the points are on opposite ends of the latent space. Counterintuitively, we prove that optimizing the cosine similarity between points forces them to grow in magnitude. Thus, (1) is unavoidable in practice. We then observe that these derivations are extremely general -- they hold across deep learning architectures and for many of the standard self-supervised learning (SSL) loss functions. This leads us to propose cut-initialization: a simple change to network initialization that helps all studied SSL methods converge faster.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16484",
        "abstract url": "https://arxiv.org/abs/2406.16484",
        "title": "Robust prediction under missingness shifts",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Prediction becomes more challenging with missing covariates. What method is chosen to handle missingness can greatly affect how models perform. In many real-world problems, the best prediction performance is achieved by models that can leverage the informative nature of a value being missing. Yet, the reasons why a covariate goes missing can change once a model is deployed in practice. If such a missingness shift occurs, the conditional probability of a value being missing differs in the target data. Prediction performance in the source data may no longer be a good selection criterion, and approaches that do not rely on informative missingness may be preferable. However, we show that the Bayes predictor remains unchanged by ignorable shifts for which the probability of missingness only depends on observed data. Any consistent estimator of the Bayes predictor may therefore result in robust prediction under those conditions, although we show empirically that different methods appear robust to different types of shifts. If the missingness shift is non-ignorable, the Bayes predictor may change due to the shift. While neither approach recovers the Bayes predictor in this case, we found empirically that disregarding missingness was most beneficial when it was highly informative.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16486",
        "abstract url": "https://arxiv.org/abs/2406.16486",
        "title": "Towards Comprehensive Preference Data Collection for Reward Modeling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models (LLMs) with human preferences, thereby enhancing the quality of responses generated. A critical component of RLHF is the reward model, which is trained on preference data and outputs a scalar reward during the inference stage. However, the collection of preference data still lacks thorough investigation. Recent studies indicate that preference data is collected either by AI or humans, where chosen and rejected instances are identified among pairwise responses. We question whether this process effectively filters out noise and ensures sufficient diversity in collected data. To address these concerns, for the first time, we propose a comprehensive framework for preference data collection, decomposing the process into four incremental steps: Prompt Generation, Response Generation, Response Filtering, and Human Labeling. This structured approach ensures the collection of high-quality preferences while reducing reliance on human labor. We conducted comprehensive experiments based on the data collected at different stages, demonstrating the effectiveness of the proposed data collection method.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16505",
        "abstract url": "https://arxiv.org/abs/2406.16505",
        "title": "$\\text{Alpha}^2$: Discovering Logical Formulaic Alphas using Deep Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Alphas are pivotal in providing signals for quantitative trading. The industry highly values the discovery of formulaic alphas for their interpretability and ease of analysis, compared with the expressive yet overfitting-prone black-box alphas. In this work, we focus on discovering formulaic alphas. Prior studies on automatically generating a collection of formulaic alphas were mostly based on genetic programming (GP), which is known to suffer from the problems of being sensitive to the initial population, converting to local optima, and slow computation speed. Recent efforts employing deep reinforcement learning (DRL) for alpha discovery have not fully addressed key practical considerations such as alpha correlations and validity, which are crucial for their effectiveness. In this work, we propose a novel framework for alpha discovery using DRL by formulating the alpha discovery process as program construction. Our agent, $\\text{Alpha}^2$, assembles an alpha program optimized for an evaluation metric. A search algorithm guided by DRL navigates through the search space based on value estimates for potential alpha outcomes. The evaluation metric encourages both the performance and the diversity of alphas for a better final trading strategy. Our formulation of searching alphas also brings the advantage of pre-calculation dimensional analysis, ensuring the logical soundness of alphas, and pruning the vast search space to a large extent. Empirical experiments on real-world stock markets demonstrates $\\text{Alpha}^2$'s capability to identify a diverse set of logical and effective alphas, which significantly improves the performance of the final trading strategy. The code of our method is available at https://github.com/x35f/alpha2.",
        "subjects": [
            "q-fin.CP",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16526",
        "abstract url": "https://arxiv.org/abs/2406.16526",
        "title": "NARRepair: Non-Autoregressive Code Generation Model for Automatic Program Repair",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the advancement of deep learning techniques, the performance of Automatic Program Repair(APR) techniques has reached a new level. Previous deep learning-based APR techniques essentially modified program sentences in the Autoregressive(AR) manner, which predicts future values based on past values. Due to the manner of word-by-word generation, the AR-based APR technique has a huge time delay. This negative consequence overshadows the widespread adoption of APR techniques in real-life software development. To address the issue, we aim to apply the Non-Autoregressive(NAR) method to the APR task, which can output target code in a parallel manner to avoid huge inference delays. To effectively adapt the NAR manner for the APR task, we in this paper propose NARRepair, the first customized NAR code generation model for the APR task. The NARRepair features three major novelties, including 1) using repair actions to alleviate the over-correction issue, 2) extracting dependency information from AST to alleviate the issue of lacking inter-word dependency information, 3) employing two-stage decoding to alleviate the issue of lacking contextual information. We evaluated NARRepair on three widely used datasets in the APR community, and the results show that our technique can significantly improve the inference speed while maintaining high repair accuracy.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16530",
        "abstract url": "https://arxiv.org/abs/2406.16530",
        "title": "Conditional Bayesian Quadrature",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a novel approach for estimating conditional or parametric expectations in the setting where obtaining samples or evaluating integrands is costly. Through the framework of probabilistic numerical methods (such as Bayesian quadrature), our novel approach allows to incorporates prior information about the integrands especially the prior smoothness knowledge about the integrands and the conditional expectation. As a result, our approach provides a way of quantifying uncertainty and leads to a fast convergence rate, which is confirmed both theoretically and empirically on challenging tasks in Bayesian sensitivity analysis, computational finance and decision making under uncertainty.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16555",
        "abstract url": "https://arxiv.org/abs/2406.16555",
        "title": "Homomorphisms and Embeddings of STRIPS Planning Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Determining whether two STRIPS planning instances are isomorphic is the simplest form of comparison between planning instances. It is also a particular case of the problem concerned with finding an isomorphism between a planning instance $P$ and a sub-instance of another instance $P_0$ . One application of such a mapping is to efficiently produce a compiled form containing all solutions to P from a compiled form containing all solutions to $P_0$. We also introduce the notion of embedding from an instance $P$ to another instance $P_0$, which allows us to deduce that $P_0$ has no solution-plan if $P$ is unsolvable. In this paper, we study the complexity of these problems. We show that the first is GI-complete, and can thus be solved, in theory, in quasi-polynomial time. While we prove the remaining problems to be NP-complete, we propose an algorithm to build an isomorphism, when possible. We report extensive experimental trials on benchmark problems which demonstrate conclusively that applying constraint propagation in preprocessing can greatly improve the efficiency of a SAT solver.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16557",
        "abstract url": "https://arxiv.org/abs/2406.16557",
        "title": "Efficient k-means with Individual Fairness via Exponential Tilting",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "In location-based resource allocation scenarios, the distances between each individual and the facility are desired to be approximately equal, thereby ensuring fairness. Individually fair clustering is often employed to achieve the principle of treating all points equally, which can be applied in these scenarios. This paper proposes a novel algorithm, tilted k-means (TKM), aiming to achieve individual fairness in clustering. We integrate the exponential tilting into the sum of squared errors (SSE) to formulate a novel objective function called tilted SSE. We demonstrate that the tilted SSE can generalize to SSE and employ the coordinate descent and first-order gradient method for optimization. We propose a novel fairness metric, the variance of the distances within each cluster, which can alleviate the Matthew Effect typically caused by existing fairness metrics. Our theoretical analysis demonstrates that the well-known k-means++ incurs a multiplicative error of O(k log k), and we establish the convergence of TKM under mild conditions. In terms of fairness, we prove that the variance generated by TKM decreases with a scaled hyperparameter. In terms of efficiency, we demonstrate the time complexity is linear with the dataset size. Our experiments demonstrate that TKM outperforms state-of-the-art methods in effectiveness, fairness, and efficiency.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16560",
        "abstract url": "https://arxiv.org/abs/2406.16560",
        "title": "GNNTAL:A Novel Model for Identifying Critical Nodes in Complex Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Identification of critical nodes is a prominent topic in the study of complex networks. Numerous methods have been proposed, yet most exhibit inherent limitations. Traditional approaches primarily analyze specific structural features of the network; however, node influence is typically the result of a combination of multiple factors. Machine learning-based methods struggle to effectively represent the complex characteristics of network structures through suitable embedding techniques and require substantial data for training, rendering them prohibitively costly for large-scale networks. To address these challenges, this paper presents an active learning model based on GraphSAGE and Transformer, named GNNTAL. This model is initially pre-trained on random or synthetic networks and subsequently fine-tuned on real-world networks by selecting a few representative nodes using K-Means clustering and uncertainty sampling. This approach offers two main advantages: (1) it significantly reduces training costs; (2) it simultaneously incorporates both local and global features. A series of comparative experiments conducted on twelve real-world networks demonstrate that GNNTAL achieves superior performance. Additionally, this paper proposes an influence maximization method based on the predictions of the GNNTAL model, which achieves optimal performance without the need for complex computations. Finally, the paper analyses certain limitations of the GNNTAL model and suggests potential solutions.",
        "subjects": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16571",
        "abstract url": "https://arxiv.org/abs/2406.16571",
        "title": "Differentiable Distributionally Robust Optimization Layers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, there has been a growing research interest in decision-focused learning, which embeds optimization problems as a layer in learning pipelines and demonstrates a superior performance than the prediction-focused approach. However, for distributionally robust optimization (DRO), a popular paradigm for decision-making under uncertainty, it is still unknown how to embed it as a layer, i.e., how to differentiate decisions with respect to an ambiguity set. In this paper, we develop such differentiable DRO layers for generic mixed-integer DRO problems with parameterized second-order conic ambiguity sets and discuss its extension to Wasserstein ambiguity sets. To differentiate the mixed-integer decisions, we propose a novel dual-view methodology by handling continuous and discrete parts of decisions via different principles. Specifically, we construct a differentiable energy-based surrogate to implement the dual-view methodology and use importance sampling to estimate its gradient. We further prove that such a surrogate enjoys the asymptotic convergency under regularization. As an application of the proposed differentiable DRO layers, we develop a novel decision-focused learning pipeline for contextual distributionally robust decision-making tasks and compare it with the prediction-focused approach in experiments.",
        "subjects": [
            "math.OC",
            "cs.AI",
            "cs.LG",
            "eess.SY"
        ],
        "comment": "In Forty-first International Conference on Machine Learning (2024)"
    },
    {
        "paper id": "2406.16578",
        "abstract url": "https://arxiv.org/abs/2406.16578",
        "title": "QuadrupedGPT: Towards a Versatile Quadruped Agent in Open-ended Worlds",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "While pets offer companionship, their limited intelligence restricts advanced reasoning and autonomous interaction with humans. Considering this, we propose QuadrupedGPT, a versatile agent designed to master a broad range of complex tasks with agility comparable to that of a pet. To achieve this goal, the primary challenges include: i) effectively leveraging multimodal observations for decision-making; ii) mastering agile control of locomotion and path planning; iii) developing advanced cognition to execute long-term objectives. QuadrupedGPT processes human command and environmental contexts using a large multimodal model (LMM). Empowered by its extensive knowledge base, our agent autonomously assigns appropriate parameters for adaptive locomotion policies and guides the agent in planning a safe but efficient path towards the goal, utilizing semantic-aware terrain analysis. Moreover, QuadrupedGPT is equipped with problem-solving capabilities that enable it to decompose long-term goals into a sequence of executable subgoals through high-level reasoning. Extensive experiments across various benchmarks confirm that QuadrupedGPT can adeptly handle multiple tasks with intricate instructions, demonstrating a significant step towards the versatile quadruped agents in open-ended worlds. Our website and codes can be found at https://quadruped-hub.github.io/Quadruped-GPT/.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2406.16606",
        "abstract url": "https://arxiv.org/abs/2406.16606",
        "title": "Cherry on the Cake: Fairness is NOT an Optimization Problem",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Fair cake-cutting is a mathematical subfield that studies the problem of fairly dividing a resource among a number of participants. The so-called ``cake,'' as an object, represents any resource that can be distributed among players. This concept is connected to supervised multi-label classification: any dataset can be thought of as a cake that needs to be distributed, where each label is a player that receives its share of the dataset. In particular, any efficient cake-cutting solution for the dataset is equivalent to an optimal decision function. Although we are not the first to demonstrate this connection, the important ramifications of this parallel seem to have been partially forgotten. We revisit these classical results and demonstrate how this connection can be prolifically used for fairness in machine learning problems. Understanding the set of achievable fair decisions is a fundamental step in finding optimal fair solutions and satisfying fairness requirements. By employing the tools of cake-cutting theory, we have been able to describe the behavior of optimal fair decisions, which, counterintuitively, often exhibit quite unfair properties. Specifically, in order to satisfy fairness constraints, it is sometimes preferable, in the name of optimality, to purposefully make mistakes and deny giving the positive label to deserving individuals in a community in favor of less worthy individuals within the same community. This practice is known in the literature as cherry-picking and has been described as ``blatantly unfair.''",
        "subjects": [
            "cs.LG",
            "cs.CY",
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16609",
        "abstract url": "https://arxiv.org/abs/2406.16609",
        "title": "Evaluating the Robustness of Deep-Learning Algorithm-Selection Models by Evolving Adversarial Instances",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks (DNN) are increasingly being used to perform algorithm-selection in combinatorial optimisation domains, particularly as they accommodate input representations which avoid designing and calculating features. Mounting evidence from domains that use images as input shows that deep convolutional networks are vulnerable to adversarial samples, in which a small perturbation of an instance can cause the DNN to misclassify. However, it remains unknown as to whether deep recurrent networks (DRN) which have recently been shown promise as algorithm-selectors in the bin-packing domain are equally vulnerable. We use an evolutionary algorithm (EA) to find perturbations of instances from two existing benchmarks for online bin packing that cause trained DRNs to misclassify: adversarial samples are successfully generated from up to 56% of the original instances depending on the dataset. Analysis of the new misclassified instances sheds light on the `fragility' of some training instances, i.e. instances where it is trivial to find a small perturbation that results in a misclassification and the factors that influence this. Finally, the method generates a large number of new instances misclassified with a wide variation in confidence, providing a rich new source of training data to create more robust models.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "To appear in the proceedings of the 18th International Conference on Parallel Problem Solving from Nature (PPSN 2024)"
    },
    {
        "paper id": "2406.16619",
        "abstract url": "https://arxiv.org/abs/2406.16619",
        "title": "No More Sliding-Windows: Dynamic Functional Connectivity Based On Random Convolutions Without Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the field of dynamic functional connectivity, the sliding-window method is widely used and its stability is generally recognized. However, the sliding-window method's data processing within the window is overly simplistic, which to some extent limits its effectiveness. This study proposes a feature expansion method based on random convolution, which achieves better and more noise-resistant results than the sliding-window method without requiring training. Experiments on simulated data show that the dynamic functional connectivity matrix and time series obtained using the random convolution method have a higher degree of fit (95.59\\%) with the standard answers within shorter time windows, compared to the sliding-window method (45.99\\%). Gender difference studies on real data also reveal that the random convolution method uncovers more gender differences than the sliding-window method. Through theoretical analysis, we propose a more comprehensive convolutional functional connectivity computation model, with the sliding-window method being a special case of this model, thereby opening up vast potential for research methods in dynamic functional connectivity.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16626",
        "abstract url": "https://arxiv.org/abs/2406.16626",
        "title": "Hacking a surrogate model approach to XAI",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In recent years, the number of new applications for highly complex AI systems has risen significantly. Algorithmic decision-making systems (ADMs) are one of such applications, where an AI system replaces the decision-making process of a human expert. As one approach to ensure fairness and transparency of such systems, explainable AI (XAI) has become more important. One variant to achieve explainability are surrogate models, i.e., the idea to train a new simpler machine learning model based on the input-output-relationship of a black box model. The simpler machine learning model could, for example, be a decision tree, which is thought to be intuitively understandable by humans. However, there is not much insight into how well the surrogate model approximates the black box. Our main assumption is that a good surrogate model approach should be able to bring such a discriminating behavior to the attention of humans; prior to our research we assumed that a surrogate decision tree would identify such a pattern on one of its first levels. However, in this article we show that even if the discriminated subgroup - while otherwise being the same in all categories - does not get a single positive decision from the black box ADM system, the corresponding question of group membership can be pushed down onto a level as low as wanted by the operator of the system. We then generalize this finding to pinpoint the exact level of the tree on which the discriminating question is asked and show that in a more realistic scenario, where discrimination only occurs to some fraction of the disadvantaged group, it is even more feasible to hide such discrimination. Our approach can be generalized easily to other surrogate models.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "24 pages, 7 figures"
    },
    {
        "paper id": "2406.16659",
        "abstract url": "https://arxiv.org/abs/2406.16659",
        "title": "Data-driven Modeling in Metrology -- A Short Introduction, Current Developments and Future Perspectives",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Mathematical models are vital to the field of metrology, playing a key role in the derivation of measurement results and the calculation of uncertainties from measurement data, informed by an understanding of the measurement process. These models generally represent the correlation between the quantity being measured and all other pertinent quantities. Such relationships are used to construct measurement systems that can interpret measurement data to generate conclusions and predictions about the measurement system itself. Classic models are typically analytical, built on fundamental physical principles. However, the rise of digital technology, expansive sensor networks, and high-performance computing hardware have led to a growing shift towards data-driven methodologies. This trend is especially prominent when dealing with large, intricate networked sensor systems in situations where there is limited expert understanding of the frequently changing real-world contexts. Here, we demonstrate the variety of opportunities that data-driven modeling presents, and how they have been already implemented in various real-world applications.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": "31 pages, Preprint"
    },
    {
        "paper id": "2406.16666",
        "abstract url": "https://arxiv.org/abs/2406.16666",
        "title": "Cubic regularized subspace Newton for non-convex optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper addresses the optimization problem of minimizing non-convex continuous functions, which is relevant in the context of high-dimensional machine learning applications characterized by over-parametrization. We analyze a randomized coordinate second-order method named SSCN which can be interpreted as applying cubic regularization in random subspaces. This approach effectively reduces the computational complexity associated with utilizing second-order information, rendering it applicable in higher-dimensional scenarios. Theoretically, we establish convergence guarantees for non-convex functions, with interpolating rates for arbitrary subspace sizes and allowing inexact curvature estimation. When increasing subspace size, our complexity matches $\\mathcal{O}(\u03b5^{-3/2})$ of the cubic regularization (CR) rate. Additionally, we propose an adaptive sampling scheme ensuring exact convergence rate of $\\mathcal{O}(\u03b5^{-3/2}, \u03b5^{-3})$ to a second-order stationary point, even without sampling all coordinates. Experimental results demonstrate substantial speed-ups achieved by SSCN compared to conventional first-order methods.",
        "subjects": [
            "cs.LG",
            "math.NA",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16676",
        "abstract url": "https://arxiv.org/abs/2406.16676",
        "title": "Unveiling Cognitive Constraints in Language Production: Extracting and Validating the Active Ego Network of Words",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "The \"ego network of words\" model captures structural properties in language production associated with cognitive constraints. While previous research focused on the layer-based structure and its semantic properties, this paper argues that an essential element, the concept of an active network, is missing. The active part of the ego network of words only includes words that are regularly used by individuals, akin to the ego networks in the social domain, where the active part includes relationships regularly nurtured by individuals and hence demanding cognitive effort. In this work, we define a methodology for extracting the active part of the ego network of words and validate it using interview transcripts and tweets. The robustness of our method to varying input data sizes and temporal stability is demonstrated. We also demonstrate that without the active network concept (and a tool for properly extracting the active network from data), the \"ego network of words\" model is not able to properly estimate the cognitive effort involved and it becomes vulnerable to the amount of data considered (leading to the disappearance of the layered structure in large datasets). Our results are well-aligned with prior analyses of the ego network of words, where the limitation of the data collected led automatically (and implicitly) to approximately consider the active part of the network only. Moreover, the validation on the transcripts dataset (MediaSum) highlights the generalizability of the model across diverse domains and the ingrained cognitive constraints in language usage.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "Accepted for publication in IEEE Transactions on Computational Social Systems. Partly supported by projects SoBigData.it (PNRR IR0000013) and ICSC (PNRR CN00000013)"
    },
    {
        "paper id": "2406.16689",
        "abstract url": "https://arxiv.org/abs/2406.16689",
        "title": "Coding schemes in neural networks learning classification tasks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural networks posses the crucial ability to generate meaningful representations of task-dependent features. Indeed, with appropriate scaling, supervised learning in neural networks can result in strong, task-dependent feature learning. However, the nature of the emergent representations, which we call the `coding scheme', is still unclear. To understand the emergent coding scheme, we investigate fully-connected, wide neural networks learning classification tasks using the Bayesian framework where learning shapes the posterior distribution of the network weights. Consistent with previous findings, our analysis of the feature learning regime (also known as `non-lazy', `rich', or `mean-field' regime) shows that the networks acquire strong, data-dependent features. Surprisingly, the nature of the internal representations depends crucially on the neuronal nonlinearity. In linear networks, an analog coding scheme of the task emerges. Despite the strong representations, the mean predictor is identical to the lazy case. In nonlinear networks, spontaneous symmetry breaking leads to either redundant or sparse coding schemes. Our findings highlight how network properties such as scaling of weights and neuronal nonlinearity can profoundly influence the emergent representations.",
        "subjects": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16698",
        "abstract url": "https://arxiv.org/abs/2406.16698",
        "title": "Learning Interpretable Fair Representations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Numerous approaches have been recently proposed for learning fair representations that mitigate unfair outcomes in prediction tasks. A key motivation for these methods is that the representations can be used by third parties with unknown objectives. However, because current fair representations are generally not interpretable, the third party cannot use these fair representations for exploration, or to obtain any additional insights, besides the pre-contracted prediction tasks. Thus, to increase data utility beyond prediction tasks, we argue that the representations need to be fair, yet interpretable. We propose a general framework for learning interpretable fair representations by introducing an interpretable \"prior knowledge\" during the representation learning process. We implement this idea and conduct experiments with ColorMNIST and Dsprite datasets. The results indicate that in addition to being interpretable, our representations attain slightly higher accuracy and fairer outcomes in a downstream classification task compared to state-of-the-art fair representations.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16707",
        "abstract url": "https://arxiv.org/abs/2406.16707",
        "title": "Probabilistic Subgoal Representations for Hierarchical Reinforcement learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In goal-conditioned hierarchical reinforcement learning (HRL), a high-level policy specifies a subgoal for the low-level policy to reach. Effective HRL hinges on a suitable subgoal represen tation function, abstracting state space into latent subgoal space and inducing varied low-level behaviors. Existing methods adopt a subgoal representation that provides a deterministic mapping from state space to latent subgoal space. Instead, this paper utilizes Gaussian Processes (GPs) for the first probabilistic subgoal representation. Our method employs a GP prior on the latent subgoal space to learn a posterior distribution over the subgoal representation functions while exploiting the long-range correlation in the state space through learnable kernels. This enables an adaptive memory that integrates long-range subgoal information from prior planning steps allowing to cope with stochastic uncertainties. Furthermore, we propose a novel learning objective to facilitate the simultaneous learning of probabilistic subgoal representations and policies within a unified framework. In experiments, our approach outperforms state-of-the-art baselines in standard benchmarks but also in environments with stochastic elements and under diverse reward conditions. Additionally, our model shows promising capabilities in transferring low-level policies across different tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16728",
        "abstract url": "https://arxiv.org/abs/2406.16728",
        "title": "CausalMMM: Learning Causal Structure for Marketing Mix Modeling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In online advertising, marketing mix modeling (MMM) is employed to predict the gross merchandise volume (GMV) of brand shops and help decision-makers to adjust the budget allocation of various advertising channels. Traditional MMM methods leveraging regression techniques can fail in handling the complexity of marketing. Although some efforts try to encode the causal structures for better prediction, they have the strict restriction that causal structures are prior-known and unchangeable. In this paper, we define a new causal MMM problem that automatically discovers the interpretable causal structures from data and yields better GMV predictions. To achieve causal MMM, two essential challenges should be addressed: (1) Causal Heterogeneity. The causal structures of different kinds of shops vary a lot. (2) Marketing Response Patterns. Various marketing response patterns i.e., carryover effect and shape effect, have been validated in practice. We argue that causal MMM needs dynamically discover specific causal structures for different shops and the predictions should comply with the prior known marketing response patterns. Thus, we propose CausalMMM that integrates Granger causality in a variational inference framework to measure the causal relationships between different channels and predict the GMV with the regularization of both temporal and saturation marketing response patterns. Extensive experiments show that CausalMMM can not only achieve superior performance of causal structure learning on synthetic datasets with improvements of 5.7%\\sim 7.1%, but also enhance the GMV prediction results on a representative E-commerce platform.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "WSDM 2024, full version"
    },
    {
        "paper id": "2406.16738",
        "abstract url": "https://arxiv.org/abs/2406.16738",
        "title": "Inducing Group Fairness in LLM-Based Decisions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Prompting Large Language Models (LLMs) has created new and interesting means for classifying textual data. While evaluating and remediating group fairness is a well-studied problem in classifier fairness literature, some classical approaches (e.g., regularization) do not carry over, and some new opportunities arise (e.g., prompt-based remediation). We measure fairness of LLM-based classifiers on a toxicity classification task, and empirically show that prompt-based classifiers may lead to unfair decisions. We introduce several remediation techniques and benchmark their fairness and performance trade-offs. We hope our work encourages more research on group fairness in LLM-based classifiers.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16745",
        "abstract url": "https://arxiv.org/abs/2406.16745",
        "title": "Bandits with Preference Feedback: A Stackelberg Game Perspective",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.GT",
            "stat.ML"
        ],
        "comment": "30 pages, 8 figures"
    },
    {
        "paper id": "2406.16749",
        "abstract url": "https://arxiv.org/abs/2406.16749",
        "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed trial-to-trial variability.",
        "subjects": [
            "cs.LG",
            "q-bio.NC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16768",
        "abstract url": "https://arxiv.org/abs/2406.16768",
        "title": "WARP: On the Benefits of Weight Averaged Rewarded Policies",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning from human feedback (RLHF) aligns large language models (LLMs) by encouraging their generations to have high rewards, using a reward model trained on human preferences. To prevent the forgetting of pre-trained knowledge, RLHF usually incorporates a KL regularization; this forces the policy to remain close to its supervised fine-tuned initialization, though it hinders the reward optimization. To tackle the trade-off between KL and reward, in this paper we introduce a novel alignment strategy named Weight Averaged Rewarded Policies (WARP). WARP merges policies in the weight space at three distinct stages. First, it uses the exponential moving average of the policy as a dynamic anchor in the KL regularization. Second, it applies spherical interpolation to merge independently fine-tuned policies into a new enhanced one. Third, it linearly interpolates between this merged model and the initialization, to recover features from pre-training. This procedure is then applied iteratively, with each iteration's final model used as an advanced initialization for the next, progressively refining the KL-reward Pareto front, achieving superior rewards at fixed KL. Experiments with GEMMA policies validate that WARP improves their quality and alignment, outperforming other open-source LLMs.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "11 main pages (34 pages with Appendix)"
    },
    {
        "paper id": "2406.16781",
        "abstract url": "https://arxiv.org/abs/2406.16781",
        "title": "A Carrying Capacity Calculator for Pedestrians Using OpenStreetMap Data: Application to Urban Tourism and Public Spaces",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Determining the carrying capacity of urban tourism destinations and public spaces is essential for sustainable management. This paper presents an online tool that calculates pedestrian carrying capacities for user-defined areas based on OpenStreetMap (OSM) data. The tool considers physical, real, and effective carrying capacities by incorporating parameters such as area per pedestrian, rotation factor, corrective factors, and management capacity. The carrying capacity calculator aids in balancing environmental, economic, social, and experiential factors to prevent overcrowding and preserve the quality of life for residents and visitors. This tool is particularly useful for tourism destination management, urban planning, and event management, ensuring positive visitor experiences and sustainable infrastructure development. We detail the implementation of the calculator, its underlying algorithm, and its application to the Santa Maria Maior parish in Lisbon, highlighting its effectiveness in managing urban tourism and public spaces.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16791",
        "abstract url": "https://arxiv.org/abs/2406.16791",
        "title": "Enabling more efficient and cost-effective AI/ML systems with Collective Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and reproducible optimization tournaments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this white paper, I present my community effort to automatically co-design cheaper, faster and more energy-efficient software and hardware for AI, ML and other popular workloads with the help of the Collective Mind framework (CM), virtualized MLOps, MLPerf benchmarks and reproducible optimization tournaments. I developed CM to modularize, automate and virtualize the tedious process of building, running, profiling and optimizing complex applications across rapidly evolving open-source and proprietary AI/ML models, datasets, software and hardware. I achieved that with the help of portable, reusable and technology-agnostic automation recipes (ResearchOps) for MLOps and DevOps (CM4MLOps) discovered in close collaboration with academia and industry when reproducing more than 150 research papers and organizing the 1st mass-scale community benchmarking of ML and AI systems using CM and MLPerf. I donated CM and CM4MLOps to MLCommons to help connect academia and industry to learn how to build and run AI and other emerging workloads in the most efficient and cost-effective way using a common and technology-agnostic automation, virtualization and reproducibility framework while unifying knowledge exchange, protecting everyone's intellectual property, enabling portable skills, and accelerating transfer of the state-of-the-art research to production. My long-term vision is to make AI accessible to everyone by making it a commodity automatically produced from the most suitable open-source and proprietary components from different vendors based on user demand, requirements and constraints such as cost, latency, throughput, accuracy, energy, size and other important characteristics.",
        "subjects": [
            "cs.LG",
            "cs.ET",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16793",
        "abstract url": "https://arxiv.org/abs/2406.16793",
        "title": "Adam-mini: Use Fewer Learning Rates To Gain More",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We propose Adam-mini, an optimizer that achieves on-par or better performance than AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by cutting down the learning rate resources in Adam (i.e., $1/\\sqrt{v}$). We find that $\\geq$ 90% of these learning rates in $v$ could be harmlessly removed if we (1) carefully partition the parameters into blocks following our proposed principle on Hessian structure; (2) assign a single but good learning rate to each parameter block. We further find that, for each of these parameter blocks, there exists a single high-quality learning rate that can outperform Adam, provided that sufficient resources are available to search it out. We then provide one cost-effective way to find good learning rates and propose Adam-mini. Empirically, we verify that Adam-mini performs on par or better than AdamW on various language models sized from 125M to 7B for pre-training, supervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini also alleviates communication overheads among GPUs and CPUs, thereby increasing throughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW when pre-training Llama2-7B on $2\\times$ A800-80GB GPUs, which saves 33% wall-clock time for pre-training.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16802",
        "abstract url": "https://arxiv.org/abs/2406.16802",
        "title": "Improved Regret Bounds for Bandits with Expert Advice",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this research note, we revisit the bandits with expert advice problem. Under a restricted feedback model, we prove a lower bound of order $\\sqrt{K T \\ln(N/K)}$ for the worst-case regret, where $K$ is the number of actions, $N>K$ the number of experts, and $T$ the time horizon. This matches a previously known upper bound of the same order and improves upon the best available lower bound of $\\sqrt{K T (\\ln N) / (\\ln K)}$. For the standard feedback model, we prove a new instance-based upper bound that depends on the agreement between the experts and provides a logarithmic improvement compared to prior results.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16834",
        "abstract url": "https://arxiv.org/abs/2406.16834",
        "title": "Concentration Inequalities for $(f,\u0393)$-GANs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative adversarial networks (GANs) are unsupervised learning methods for training a generator distribution to produce samples that approximate those drawn from a target distribution. Many such methods can be formulated as minimization of a metric or divergence. Recent works have proven the statistical consistency of GANs that are based on integral probability metrics (IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. IPMs are defined by optimizing a linear functional (difference of expectations) over a space of discriminators. A much larger class of GANs, which allow for the use of nonlinear objective functionals, can be constructed using $(f,\u0393)$-divergences; these generalize and interpolate between IPMs and $f$-divergences (e.g., KL or $\u03b1$-divergences). Instances of $(f,\u0393)$-GANs have been shown to exhibit improved performance in a number of applications. In this work we study the statistical consistency of $(f,\u0393)$-GANs for general $f$ and $\u0393$. Specifically, we derive finite-sample concentration inequalities. These derivations require novel arguments due to nonlinearity of the objective functional. We demonstrate that our new results reduce to the known results for IPM-GANs in the appropriate limit while also significantly extending the domain of applicability of this theory.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "21 pages"
    },
    {
        "paper id": "2406.16846",
        "abstract url": "https://arxiv.org/abs/2406.16846",
        "title": "Data Debiasing with Datamodels (D3M): Improving Subgroup Robustness via Data Selection",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Machine learning models can fail on subgroups that are underrepresented during training. While techniques such as dataset balancing can improve performance on underperforming groups, they require access to training group annotations and can end up removing large portions of the dataset. In this paper, we introduce Data Debiasing with Datamodels (D3M), a debiasing approach which isolates and removes specific training examples that drive the model's failures on minority groups. Our approach enables us to efficiently train debiased classifiers while removing only a small number of examples, and does not require training group annotations or additional hyperparameter tuning.",
        "subjects": [
            "cs.LG",
            "cs.CY",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16989",
        "abstract url": "https://arxiv.org/abs/2406.16989",
        "title": "Retrieval-Augmented Mixture of LoRA Experts for Uploadable Machine Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Low-Rank Adaptation (LoRA) offers an efficient way to fine-tune large language models (LLMs). Its modular and plug-and-play nature allows the integration of various domain-specific LoRAs, enhancing LLM capabilities. Open-source platforms like Huggingface and Modelscope have introduced a new computational paradigm, Uploadable Machine Learning (UML). In UML, contributors use decentralized data to train specialized adapters, which are then uploaded to a central platform to improve LLMs. This platform uses these domain-specific adapters to handle mixed-task requests requiring personalized service. Previous research on LoRA composition either focuses on specific tasks or fixes the LoRA selection during training. However, in UML, the pool of LoRAs is dynamically updated with new uploads, requiring a generalizable selection mechanism for unseen LoRAs. Additionally, the mixed-task nature of downstream requests necessitates personalized services. To address these challenges, we propose Retrieval-Augmented Mixture of LoRA Experts (RAMoLE), a framework that adaptively retrieves and composes multiple LoRAs based on input prompts. RAMoLE has three main components: LoraRetriever for identifying and retrieving relevant LoRAs, an on-the-fly MoLE mechanism for coordinating the retrieved LoRAs, and efficient batch inference for handling heterogeneous requests. Experimental results show that RAMoLE consistently outperforms baselines, highlighting its effectiveness and scalability.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2402.09997"
    },
    {
        "paper id": "2406.16990",
        "abstract url": "https://arxiv.org/abs/2406.16990",
        "title": "AND: Audio Network Dissection for Interpreting Deep Acoustic Models",
        "rating": "0.5",
        "keywords": [
            [
                "unlearning"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Neuron-level interpretations aim to explain network behaviors and properties by investigating neurons responsive to specific perceptual or structural input patterns. Although there is emerging work in the vision and language domains, none is explored for acoustic models. To bridge the gap, we introduce $\\textit{AND}$, the first $\\textbf{A}$udio $\\textbf{N}$etwork $\\textbf{D}$issection framework that automatically establishes natural language explanations of acoustic neurons based on highly-responsive audio. $\\textit{AND}$ features the use of LLMs to summarize mutual acoustic features and identities among audio. Extensive experiments are conducted to verify $\\textit{AND}$'s precise and informative descriptions. In addition, we demonstrate a potential use of $\\textit{AND}$ for audio machine unlearning by conducting concept-specific pruning based on the generated descriptions. Finally, we highlight two acoustic model behaviors with analysis by $\\textit{AND}$: (i) models discriminate audio with a combination of basic acoustic features rather than high-level abstract concepts; (ii) training strategies affect model behaviors and neuron interpretability -- supervised training guides neurons to gradually narrow their attention, while self-supervised learning encourages neurons to be polysemantic for exploring high-level features.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": "Accepted by ICML'24"
    },
    {
        "paper id": "2406.16999",
        "abstract url": "https://arxiv.org/abs/2406.16999",
        "title": "Identifying Easy Instances to Improve Efficiency of ML Pipelines for Algorithm-Selection",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Algorithm-selection (AS) methods are essential in order to obtain the best performance from a portfolio of solvers over large sets of instances. However, many AS methods rely on an analysis phase, e.g. where features are computed by sampling solutions and used as input in a machine-learning model. For AS to be efficient, it is therefore important that this analysis phase is not computationally expensive. We propose a method for identifying easy instances which can be solved quickly using a generalist solver without any need for algorithm-selection. This saves computational budget associated with feature-computation which can then be used elsewhere in an AS pipeline, e.g., enabling additional function evaluations on hard problems. Experiments on the BBOB dataset in two settings (batch and streaming) show that identifying easy instances results in substantial savings in function evaluations. Re-allocating the saved budget to hard problems provides gains in performance compared to both the virtual best solver (VBS) computed with the original budget, the single best solver (SBS) and a trained algorithm-selector.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": "To appear in the proceedings of the 18th International Conference on Parallel Problem Solving from Nature (PPSN 2024)"
    },
    {
        "paper id": "2406.17058",
        "abstract url": "https://arxiv.org/abs/2406.17058",
        "title": "Bayesian Deep ICE",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep Independent Component Estimation (DICE) has many applications in modern day machine learning as a feature engineering extraction method. We provide a novel latent variable representation of independent component analysis that enables both point estimates via expectation-maximization (EM) and full posterior sampling via Markov Chain Monte Carlo (MCMC) algorithms. Our methodology also applies to flow-based methods for nonlinear feature extraction. We discuss how to implement conditional posteriors and envelope-based methods for optimization. Through this representation hierarchy, we unify a number of hitherto disjoint estimation procedures. We illustrate our methodology and algorithms on a numerical example. Finally, we conclude with directions for future research.",
        "subjects": [
            "stat.ME",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17096",
        "abstract url": "https://arxiv.org/abs/2406.17096",
        "title": "Model-Free Robust Reinforcement Learning with Sample Complexity Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Distributionally Robust Reinforcement Learning (DR-RL) aims to derive a policy optimizing the worst-case performance within a predefined uncertainty set. Despite extensive research, previous DR-RL algorithms have predominantly favored model-based approaches, with limited availability of model-free methods offering convergence guarantees or sample complexities. This paper proposes a model-free DR-RL algorithm leveraging the Multi-level Monte Carlo (MLMC) technique to close such a gap. Our innovative approach integrates a threshold mechanism that ensures finite sample requirements for algorithmic implementation, a significant improvement than previous model-free algorithms. We develop algorithms for uncertainty sets defined by total variation, Chi-square divergence, and KL divergence, and provide finite sample analyses under all three cases. Remarkably, our algorithms represent the first model-free DR-RL approach featuring finite sample complexity for total variation and Chi-square divergence uncertainty sets, while also offering an improved sample complexity and broader applicability compared to existing model-free DR-RL algorithms for the KL divergence model. The complexities of our method establish the tightest results for all three uncertainty models in model-free DR-RL, underscoring the effectiveness and efficiency of our algorithm, and highlighting its potential for practical applications.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "UAI 2024"
    },
    {
        "paper id": "2406.17125",
        "abstract url": "https://arxiv.org/abs/2406.17125",
        "title": "A Wiener process perspective on local intrinsic dimension estimation methods",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Local intrinsic dimension (LID) estimation methods have received a lot of attention in recent years thanks to the progress in deep neural networks and generative modeling. In opposition to old non-parametric methods, new methods use generative models to approximate diffused dataset density and scale the methods to high-dimensional datasets like images. In this paper, we investigate the recent state-of-the-art parametric LID estimation methods from the perspective of the Wiener process. We explore how these methods behave when their assumptions are not met. We give an extended mathematical description of those methods and their error as a function of the probability density of the data.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17131",
        "abstract url": "https://arxiv.org/abs/2406.17131",
        "title": "Bayesian temporal biclustering with applications to multi-subject neuroscience studies",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of analyzing multivariate time series collected on multiple subjects, with the goal of identifying groups of subjects exhibiting similar trends in their recorded measurements over time as well as time-varying groups of associated measurements. To this end, we propose a Bayesian model for temporal biclustering featuring nested partitions, where a time-invariant partition of subjects induces a time-varying partition of measurements. Our approach allows for data-driven determination of the number of subject and measurement clusters as well as estimation of the number and location of changepoints in measurement partitions. To efficiently perform model fitting and posterior estimation with Markov Chain Monte Carlo, we derive a blocked update of measurements' cluster-assignment sequences. We illustrate the performance of our model in two applications to functional magnetic resonance imaging data and to an electroencephalogram dataset. The results indicate that the proposed model can combine information from potentially many subjects to discover a set of interpretable, dynamic patterns. Experiments on simulated data compare the estimation performance of the proposed model against ground-truth values and other statistical methods, showing that it performs well at identifying ground-truth subject and measurement clusters even when no subject or time dependence is present.",
        "subjects": [
            "stat.ME",
            "cs.LG",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17144",
        "abstract url": "https://arxiv.org/abs/2406.17144",
        "title": "An information-geometric approach for network decomposition using the q-state Potts model",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Complex networks are critical in many scientific, technological, and societal contexts due to their ability to represent and analyze intricate systems with interdependent components. Often, after labeling the nodes of a network with a community detection algorithm, its modular organization emerges, allowing a better understanding of the underlying structure by uncovering hidden relationships. In this paper, we introduce a novel information-geometric framework for the filtering and decomposition of networks whose nodes have been labeled. Our approach considers the labeled network as the outcome of a Markov random field modeled by a q-state Potts model. According to information geometry, the first and second order Fisher information matrices are related to the metric and curvature tensor of the parametric space of a statistical model. By computing an approximation to the local shape operator, the proposed methodology is able to identify low and high information nodes, allowing the decomposition of the labeled network in two complementary subgraphs. Hence, we call this method as the LO-HI decomposition. Experimental results with several kinds of networks show that the high information subgraph is often related to edges and boundaries, while the low information subgraph is a smoother version of the network, in the sense that the modular structure is improved.",
        "subjects": [
            "cs.IT",
            "cs.SI",
            "math-ph",
            "nlin.AO",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17150",
        "abstract url": "https://arxiv.org/abs/2406.17150",
        "title": "Peirce in the Machine: How Mixture of Experts Models Perform Hypothesis Construction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Mixture of experts is a prediction aggregation method in machine learning that aggregates the predictions of specialized experts. This method often outperforms Bayesian methods despite the Bayesian having stronger inductive guarantees. We argue that this is due to the greater functional capacity of mixture of experts. We prove that in a limiting case of mixture of experts will have greater capacity than equivalent Bayesian methods, which we vouchsafe through experiments on non-limiting cases. Finally, we conclude that mixture of experts is a type of abductive reasoning in the Peircian sense of hypothesis construction.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "31 pages"
    },
    {
        "paper id": "2406.17167",
        "abstract url": "https://arxiv.org/abs/2406.17167",
        "title": "Learning on Transformers is Provable Low-Rank and Sparse: A One-layer Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Efficient training and inference algorithms, such as low-rank adaption and model pruning, have shown impressive performance for learning Transformer-based large foundation models. However, due to the technical challenges of the non-convex optimization caused by the complicated architecture of Transformers, the theoretical study of why these methods can be applied to learn Transformers is mostly elusive. To the best of our knowledge, this paper shows the first theoretical analysis of the property of low-rank and sparsity of one-layer Transformers by characterizing the trained model after convergence using stochastic gradient descent. By focusing on a data model based on label-relevant and label-irrelevant patterns, we quantify that the gradient updates of trainable parameters are low-rank, which depends on the number of label-relevant patterns. We also analyze how model pruning affects the generalization while improving computation efficiency and conclude that proper magnitude-based pruning has a slight effect on the testing performance. We implement numerical experiments to support our findings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "IEEE SAM Workshop 2024"
    },
    {
        "paper id": "2406.17184",
        "abstract url": "https://arxiv.org/abs/2406.17184",
        "title": "Minimax Optimality in Contextual Dynamic Pricing with General Valuation Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Dynamic pricing, the practice of adjusting prices based on contextual factors, has gained significant attention due to its impact on revenue maximization. In this paper, we address the contextual dynamic pricing problem, which involves pricing decisions based on observable product features and customer characteristics. We propose a novel algorithm that achieves improved regret bounds while minimizing assumptions about the problem. Our algorithm discretizes the unknown noise distribution and combines the upper confidence bounds with a layered data partitioning technique to effectively regulate regret in each episode. These techniques effectively control the regret associated with pricing decisions, leading to the minimax optimality. Specifically, our algorithm achieves a regret upper bound of $\\tilde{\\mathcal{O}}(\u03c1_{\\mathcal{V}}^{\\frac{1}{3}}(\u03b4) T^{\\frac{2}{3}})$, where $\u03c1_{\\mathcal{V}}(\u03b4)$ represents the estimation error of the valuation function. Importantly, this bound matches the lower bound up to logarithmic terms, demonstrating the minimax optimality of our approach. Furthermore, our method extends beyond linear valuation models commonly used in dynamic pricing by considering general function spaces. We simplify the estimation process by reducing it to general offline regression oracles, making implementation more straightforward.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "29 pages"
    },
    {
        "paper id": "2406.17188",
        "abstract url": "https://arxiv.org/abs/2406.17188",
        "title": "Geometric Median (GM) Matching for Robust Data Pruning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Data pruning, the combinatorial task of selecting a small and informative subset from a large dataset, is crucial for mitigating the enormous computational costs associated with training data-hungry modern deep learning models at scale. Since large-scale data collections are invariably noisy, developing data pruning strategies that remain robust even in the presence of corruption is critical in practice. Unfortunately, the existing heuristics for (robust) data pruning lack theoretical coherence and rely on heroic assumptions, that are, often unattainable, by the very nature of the problem setting. Moreover, these strategies often yield sub-optimal neural scaling laws even compared to random sampling, especially in scenarios involving strong corruption and aggressive pruning rates -- making provably robust data pruning an open challenge. In response, in this work, we propose Geometric Median ($\\gm$) Matching -- a herding~\\citep{welling2009herding} style greedy algorithm -- that yields a $k$-subset such that the mean of the subset approximates the geometric median of the (potentially) noisy dataset. Theoretically, we show that $\\gm$ Matching enjoys an improved $\\gO(1/k)$ scaling over $\\gO(1/\\sqrt{k})$ scaling of uniform sampling; while achieving the optimal breakdown point of 1/2 even under arbitrary corruption. Extensive experiments across popular deep learning benchmarks indicate that $\\gm$ Matching consistently outperforms prior state-of-the-art; the gains become more profound at high rates of corruption and aggressive pruning rates; making $\\gm$ Matching a strong baseline for future research in robust data pruning.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17215",
        "abstract url": "https://arxiv.org/abs/2406.17215",
        "title": "Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of Daline",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The integration of experiment technologies with large language models (LLMs) is transforming scientific research, offering AI capabilities beyond specialized problem-solving to becoming research assistants for human scientists. In power systems, simulations are essential for research. However, LLMs face significant challenges in power system simulations due to limited pre-existing knowledge and the complexity of power grids. To address this issue, this work proposes a modular framework that integrates expertise from both the power system and LLM domains. This framework enhances LLMs' ability to perform power system simulations on previously unseen tools. Validated using 34 simulation tasks in Daline, a (optimal) power flow simulation and linearization toolbox not yet exposed to LLMs, the proposed framework improved GPT-4o's simulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o web interface's 33.8% accuracy (with the entire knowledge base uploaded). These results highlight the potential of LLMs as research assistants in power systems.",
        "subjects": [
            "eess.SY",
            "cs.AI"
        ],
        "comment": "All the supplementary files mentioned in the manuscript will be open-source upon acceptance"
    },
    {
        "paper id": "2406.17263",
        "abstract url": "https://arxiv.org/abs/2406.17263",
        "title": "Efficient, Multimodal, and Derivative-Free Bayesian Inference With Fisher-Rao Gradient Flows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we study efficient approximate sampling for probability distributions known up to normalization constants. We specifically focus on a problem class arising in Bayesian inference for large-scale inverse problems in science and engineering applications. The computational challenges we address with the proposed methodology are: (i) the need for repeated evaluations of expensive forward models; (ii) the potential existence of multiple modes; and (iii) the fact that gradient of, or adjoint solver for, the forward model might not be feasible. While existing Bayesian inference methods meet some of these challenges individually, we propose a framework that tackles all three systematically. Our approach builds upon the Fisher-Rao gradient flow in probability space, yielding a dynamical system for probability densities that converges towards the target distribution at a uniform exponential rate. This rapid convergence is advantageous for the computational burden outlined in (i). We apply Gaussian mixture approximations with operator splitting techniques to simulate the flow numerically; the resulting approximation can capture multiple modes thus addressing (ii). Furthermore, we employ the Kalman methodology to facilitate a derivative-free update of these Gaussian components and their respective weights, addressing the issue in (iii). The proposed methodology results in an efficient derivative-free sampler flexible enough to handle multi-modal distributions: Gaussian Mixture Kalman Inversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically and numerically in several experiments with multimodal target distributions, including proof-of-concept and two-dimensional examples, as well as a large-scale application: recovering the Navier-Stokes initial condition from solution data at positive times.",
        "subjects": [
            "cs.LG",
            "math.DS",
            "math.NA"
        ],
        "comment": "42 pages, 9 figures"
    },
    {
        "paper id": "2406.17809",
        "abstract url": "https://arxiv.org/abs/2406.17809",
        "title": "Towards a Science Exocortex",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Artificial intelligence (AI) methods are poised to revolutionize intellectual work, with generative AI enabling automation of text analysis, text generation, and simple decision making or reasoning. The impact to science is only just beginning, but the opportunity is significant since scientific research relies fundamentally on extended chains of cognitive work. Here, we review the state of the art in agentic AI systems, and discuss how these methods could be extended to have even greater impact on science. We propose the development of an exocortex, a synthetic extension of a person's cognition. A science exocortex could be designed as a swarm of AI agents, with each agent individually streamlining specific researcher tasks, and whose inter-communication leads to emergent behavior that greatly extend the researcher's cognition and volition.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "29 pages, 5 figures, 1 table"
    },
    {
        "paper id": "2406.17811",
        "abstract url": "https://arxiv.org/abs/2406.17811",
        "title": "CATBench: A Compiler Autotuning Benchmarking Suite for Black-box Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian optimization is a powerful method for automating tuning of compilers. The complex landscape of autotuning provides a myriad of rarely considered structural challenges for black-box optimizers, and the lack of standardized benchmarks has limited the study of Bayesian optimization within the domain. To address this, we present CATBench, a comprehensive benchmarking suite that captures the complexities of compiler autotuning, ranging from discrete, conditional, and permutation parameter types to known and unknown binary constraints, as well as both multi-fidelity and multi-objective evaluations. The benchmarks in CATBench span a range of machine learning-oriented computations, from tensor algebra to image processing and clustering, and uses state-of-the-art compilers, such as TACO and RISE/ELEVATE. CATBench offers a unified interface for evaluating Bayesian optimization algorithms, promoting reproducibility and innovation through an easy-to-use, fully containerized setup of both surrogate and real-world compiler optimization tasks. We validate CATBench on several state-of-the-art algorithms, revealing their strengths and weaknesses and demonstrating the suite's potential for advancing both Bayesian optimization and compiler autotuning research.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17813",
        "abstract url": "https://arxiv.org/abs/2406.17813",
        "title": "Unsupervised Concept Drift Detection from Deep Learning Representations in Real-time",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Concept Drift is a phenomenon in which the underlying data distribution and statistical properties of a target domain change over time, leading to a degradation of the model's performance. Consequently, models deployed in production require continuous monitoring through drift detection techniques. Most drift detection methods to date are supervised, i.e., based on ground-truth labels. However, true labels are usually not available in many real-world scenarios. Although recent efforts have been made to develop unsupervised methods, they often lack the required accuracy, have a complexity that makes real-time implementation in production environments difficult, or are unable to effectively characterize drift. To address these challenges, we propose DriftLens, an unsupervised real-time concept drift detection framework. It works on unstructured data by exploiting the distribution distances of deep learning representations. DriftLens can also provide drift characterization by analyzing each label separately. A comprehensive experimental evaluation is presented with multiple deep learning classifiers for text, image, and speech. Results show that (i) DriftLens performs better than previous methods in detecting drift in $11/13$ use cases; (ii) it runs at least 5 times faster; (iii) its detected drift value is very coherent with the amount of drift (correlation $\\geq 0.85$); (iv) it is robust to parameter changes.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16320",
        "abstract url": "https://arxiv.org/abs/2406.16320",
        "title": "What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation",
        "rating": "0",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "Facial"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Vision-Language Models (VLMs) have gained community-spanning prominence due to their ability to integrate visual and textual inputs to perform complex tasks. Despite their success, the internal decision-making processes of these models remain opaque, posing challenges in high-stakes applications. To address this, we introduce NOTICE, the first Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE incorporates a Semantic Minimal Pairs (SMP) framework for image corruption and Symmetric Token Replacement (STR) for text. This approach enables semantically meaningful causal mediation analysis for both modalities, providing a robust method for analyzing multimodal integration within models like BLIP. Our experiments on the SVO-Probes, MIT-States, and Facial Expression Recognition datasets reveal crucial insights into VLM decision-making, identifying the significant role of middle-layer cross-attention heads. Further, we uncover a set of ``universal cross-attention heads'' that consistently contribute across tasks and modalities, each performing distinct functions such as implicit image segmentation, object inhibition, and outlier inhibition. This work paves the way for more transparent and interpretable multimodal systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16359",
        "abstract url": "https://arxiv.org/abs/2406.16359",
        "title": "Improving Generative Adversarial Networks for Video Super-Resolution",
        "rating": "0",
        "keywords": [
            [
                "GAN",
                "Super-Resolution"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In this research, we explore different ways to improve generative adversarial networks for video super-resolution tasks from a base single image super-resolution GAN model. Our primary objective is to identify potential techniques that enhance these models and to analyze which of these techniques yield the most significant improvements. We evaluate our results using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Our findings indicate that the most effective techniques include temporal smoothing, long short-term memory (LSTM) layers, and a temporal loss function. The integration of these methods results in an 11.97% improvement in PSNR and an 8% improvement in SSIM compared to the baseline video super-resolution generative adversarial network (GAN) model. This substantial improvement suggests potential further applications to enhance current state-of-the-art models.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16374",
        "abstract url": "https://arxiv.org/abs/2406.16374",
        "title": "KEHRL: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning",
        "rating": "0",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation triples from knowledge graphs (KGs) and integrate these external data sources into language models via self-supervised learning. Previous works treat knowledge enhancement as two independent operations, i.e., knowledge injection and knowledge integration. In this paper, we propose to learn Knowledge-Enhanced language representations with Hierarchical Reinforcement Learning (KEHRL), which jointly addresses the problems of detecting positions for knowledge injection and integrating external knowledge into the model in order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a high-level reinforcement learning (RL) agent utilizes both internal and prior knowledge to iteratively detect essential positions in texts for knowledge injection, which filters out less meaningful entities to avoid diverting the knowledge learning direction. Once the entity positions are selected, a relevant triple filtration module is triggered to perform low-level RL to dynamically refine the triples associated with polysemic entities through binary-valued actions. Experiments validate KEHRL's effectiveness in probing factual knowledge and enhancing the model's performance on various natural language understanding tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16416",
        "abstract url": "https://arxiv.org/abs/2406.16416",
        "title": "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multilingual knowledge editing (MKE) aims to simultaneously revise factual knowledge across multilingual languages within large language models (LLMs). However, most existing MKE methods just adapt existing monolingual editing methods to multilingual scenarios, overlooking the deep semantic connections of the same factual knowledge between different languages, thereby limiting edit performance. To address this issue, we first investigate how LLMs represent multilingual factual knowledge and discover that the same factual knowledge in different languages generally activates a shared set of neurons, which we call language-agnostic factual neurons. These neurons represent the semantic connections between multilingual knowledge and are mainly located in certain layers. Inspired by this finding, we propose a new MKE method by locating and modifying Language-Agnostic Factual Neurons (LAFN) to simultaneously edit multilingual knowledge. Specifically, we first generate a set of paraphrases for each multilingual knowledge to be edited to precisely locate the corresponding language-agnostic factual neurons. Then we optimize the update values for modifying these located neurons to achieve simultaneous modification of the same factual knowledge in multiple languages. Experimental results on Bi-ZsRE and MzsRE benchmarks demonstrate that our method outperforms existing MKE methods and achieves remarkable edit performance, indicating the importance of considering the semantic connections among multilingual knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 4 figures, 7 tables"
    },
    {
        "paper id": "2406.16469",
        "abstract url": "https://arxiv.org/abs/2406.16469",
        "title": "Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration",
        "rating": "0",
        "keywords": [
            [
                "vision-language",
                "VLM"
            ],
            [
                "diagnosis"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "To create culturally inclusive vision-language models (VLMs), the foremost requirement is developing a test benchmark that can diagnose the models' ability to respond to questions reflecting cultural elements. This paper addresses the necessity for such benchmarks, noting that existing research has relied on human annotators' manual efforts, which impedes diversity and efficiency. We propose a semi-automated pipeline for constructing cultural VLM benchmarks to enhance diversity and efficiency. This pipeline leverages human-VLM collaboration, where VLMs generate questions based on guidelines, human-annotated examples, and image-wise relevant knowledge, which are then reviewed by native speakers for quality and cultural relevance. The effectiveness of our adaptable pipeline is demonstrated through a specific application: creating a dataset tailored to Korean culture, dubbed K-Viscuit. The resulting benchmark features two types of questions: Type 1 questions measure visual recognition abilities, while Type 2 assess fine-grained visual reasoning skills. This ensures a thorough diagnosis of VLM models across various aspects. Our evaluation using K-Viscuit revealed that open-source models notably lag behind proprietary models in understanding Korean culture, highlighting areas for improvement. We provided diverse analyses of VLM performance across different cultural aspects. Besides, we explored the potential of incorporating external knowledge retrieval to enhance the generation process, suggesting future directions for improving cultural interpretation ability of VLMs. Our dataset and code will be made publicly available.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16489",
        "abstract url": "https://arxiv.org/abs/2406.16489",
        "title": "Deepfake tweets automatic detection",
        "rating": "0",
        "keywords": [
            [
                "Deepfake"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This study addresses the critical challenge of detecting DeepFake tweets by leveraging advanced natural language processing (NLP) techniques to distinguish between genuine and AI-generated texts. Given the increasing prevalence of misinformation, our research utilizes the TweepFake dataset to train and evaluate various machine learning models. The objective is to identify effective strategies for recognizing DeepFake content, thereby enhancing the integrity of digital communications. By developing reliable methods for detecting AI-generated misinformation, this work contributes to a more trustworthy online information environment.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16501",
        "abstract url": "https://arxiv.org/abs/2406.16501",
        "title": "UNICAD: A Unified Approach for Attack Detection, Noise Reduction and Novel Class Identification",
        "rating": "0",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "As the use of Deep Neural Networks (DNNs) becomes pervasive, their vulnerability to adversarial attacks and limitations in handling unseen classes poses significant challenges. The state-of-the-art offers discrete solutions aimed to tackle individual issues covering specific adversarial attack scenarios, classification or evolving learning. However, real-world systems need to be able to detect and recover from a wide range of adversarial attacks without sacrificing classification accuracy and to flexibly act in {\\bf unseen} scenarios. In this paper, UNICAD, is proposed as a novel framework that integrates a variety of techniques to provide an adaptive solution. For the targeted image classification, UNICAD achieves accurate image classification, detects unseen classes, and recovers from adversarial attacks using Prototype and Similarity-based DNNs with denoising autoencoders. Our experiments performed on the CIFAR-10 dataset highlight UNICAD's effectiveness in adversarial mitigation and unseen class classification, outperforming traditional models.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16529",
        "abstract url": "https://arxiv.org/abs/2406.16529",
        "title": "Towards Better Graph-based Cross-document Relation Extraction via Non-bridge Entity Enhancement and Prediction Debiasing",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Cross-document Relation Extraction aims to predict the relation between target entities located in different documents. In this regard, the dominant models commonly retain useful information for relation prediction via bridge entities, which allows the model to elaborately capture the intrinsic interdependence between target entities. However, these studies ignore the non-bridge entities, each of which co-occurs with only one target entity and offers the semantic association between target entities for relation prediction. Besides, the commonly-used dataset--CodRED contains substantial NA instances, leading to the prediction bias during inference. To address these issues, in this paper, we propose a novel graph-based cross-document RE model with non-bridge entity enhancement and prediction debiasing. Specifically, we use a unified entity graph to integrate numerous non-bridge entities with target entities and bridge entities, modeling various associations between them, and then use a graph recurrent network to encode this graph. Finally, we introduce a novel debiasing strategy to calibrate the original prediction distribution. Experimental results on the closed and open settings show that our model significantly outperforms all baselines, including the GPT-3.5-turbo and InstructUIE, achieving state-of-the-art performance. Particularly, our model obtains 66.23% and 55.87% AUC points in the official leaderboard\\footnote{\\url{https://codalab.lisn.upsaclay.fr/competitions/3770#results}} under the two settings, respectively, ranking the first place in all submissions since December 2023. Our code is available at https://github.com/DeepLearnXMU/CoRE-NEPD.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to ACL 2024 Findings"
    },
    {
        "paper id": "2406.16531",
        "abstract url": "https://arxiv.org/abs/2406.16531",
        "title": "GIM: A Million-scale Benchmark for Generative Image Manipulation Detection and Localization",
        "rating": "0",
        "keywords": [
            [
                "image editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The extraordinary ability of generative models emerges as a new trend in image editing and generating realistic images, posing a serious threat to the trustworthiness of multimedia data and driving the research of image manipulation detection and location(IMDL). However, the lack of a large-scale data foundation makes IMDL task unattainable. In this paper, a local manipulation pipeline is designed, incorporating the powerful SAM, ChatGPT and generative models. Upon this basis, We propose the GIM dataset, which has the following advantages: 1) Large scale, including over one million pairs of AI-manipulated images and real images. 2) Rich Image Content, encompassing a broad range of image classes 3) Diverse Generative Manipulation, manipulated images with state-of-the-art generators and various manipulation tasks. The aforementioned advantages allow for a more comprehensive evaluation of IMDL methods, extending their applicability to diverse images. We introduce two benchmark settings to evaluate the generalization capability and comprehensive performance of baseline methods. In addition, we propose a novel IMDL framework, termed GIMFormer, which consists of a ShadowTracer, Frequency-Spatial Block (FSB), and a Multi-window Anomalous Modelling (MWAM) Module. Extensive experiments on the GIM demonstrate that GIMFormer surpasses previous state-of-the-art works significantly on two different benchmarks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code page: https://github.com/chenyirui/GIM"
    },
    {
        "paper id": "2406.16562",
        "abstract url": "https://arxiv.org/abs/2406.16562",
        "title": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models",
        "rating": "0",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The recent advancements in text-to-image generative models have been remarkable. Yet, the field suffers from a lack of evaluation metrics that accurately reflect the performance of these models, particularly lacking fine-grained metrics that can guide the optimization of the models. In this paper, we propose EvalAlign, a metric characterized by its accuracy, stability, and fine granularity. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) pre-trained on extensive datasets. We develop evaluation protocols that focus on two key dimensions: image faithfulness and text-image alignment. Each protocol comprises a set of detailed, fine-grained instructions linked to specific scoring options, enabling precise manual scoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to align closely with human evaluative judgments, resulting in a robust evaluation model. Our comprehensive tests across 24 text-to-image generation models demonstrate that EvalAlign not only provides superior metric stability but also aligns more closely with human preferences than existing metrics, confirming its effectiveness and utility in model assessment.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "Github Repository: https://github.com/SAIS-FUXI/EvalAlign"
    },
    {
        "paper id": "2406.16583",
        "abstract url": "https://arxiv.org/abs/2406.16583",
        "title": "Personalized federated learning based on feature fusion",
        "rating": "0",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Federated learning enables distributed clients to collaborate on training while storing their data locally to protect client privacy. However, due to the heterogeneity of data, models, and devices, the final global model may need to perform better for tasks on each client. Communication bottlenecks, data heterogeneity, and model heterogeneity have been common challenges in federated learning. In this work, we considered a label distribution skew problem, a type of data heterogeneity easily overlooked. In the context of classification, we propose a personalized federated learning approach called pFedPM. In our process, we replace traditional gradient uploading with feature uploading, which helps reduce communication costs and allows for heterogeneous client models. These feature representations play a role in preserving privacy to some extent. We use a hyperparameter $a$ to mix local and global features, which enables us to control the degree of personalization. We also introduced a relation network as an additional decision layer, which provides a non-linear learnable classifier to predict labels. Experimental results show that, with an appropriate setting of $a$, our scheme outperforms several recent FL methods on MNIST, FEMNIST, and CRIFAR10 datasets and achieves fewer communications.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16592",
        "abstract url": "https://arxiv.org/abs/2406.16592",
        "title": "Toward Fairer Face Recognition Datasets",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Face recognition and verification are two computer vision tasks whose performance has progressed with the introduction of deep representations. However, ethical, legal, and technical challenges due to the sensitive character of face data and biases in real training datasets hinder their development. Generative AI addresses privacy by creating fictitious identities, but fairness problems persist. We promote fairness by introducing a demographic attributes balancing mechanism in generated training datasets. We experiment with an existing real dataset, three generated training datasets, and the balanced versions of a diffusion-based dataset. We propose a comprehensive evaluation that considers accuracy and fairness equally and includes a rigorous regression-based statistical analysis of attributes. The analysis shows that balancing reduces demographic unfairness. Also, a performance gap persists despite generation becoming more accurate with time. The proposed balancing method and comprehensive verification evaluation promote fairer and transparent face recognition and verification.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16605",
        "abstract url": "https://arxiv.org/abs/2406.16605",
        "title": "CLEAR: Can Language Models Really Understand Causal Graphs?",
        "rating": "0",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Causal reasoning is a cornerstone of how humans interpret the world. To model and reason about causality, causal graphs offer a concise yet effective solution. Given the impressive advancements in language models, a crucial question arises: can they really understand causal graphs? To this end, we pioneer an investigation into language models' understanding of causal graphs. Specifically, we develop a framework to define causal graph understanding, by assessing language models' behaviors through four practical criteria derived from diverse disciplines (e.g., philosophy and psychology). We then develop CLEAR, a novel benchmark that defines three complexity levels and encompasses 20 causal graph-based tasks across these levels. Finally, based on our framework and benchmark, we conduct extensive experiments on six leading language models and summarize five empirical findings. Our results indicate that while language models demonstrate a preliminary understanding of causal graphs, significant potential for improvement remains. Our project website is at https://github.com/OpenCausaLab/CLEAR.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16623",
        "abstract url": "https://arxiv.org/abs/2406.16623",
        "title": "Articulate your NeRF: Unsupervised articulated object modeling via conditional view synthesis",
        "rating": "0",
        "keywords": [
            [
                "voxel",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a novel unsupervised method to learn the pose and part-segmentation of articulated objects with rigid parts. Given two observations of an object in different articulation states, our method learns the geometry and appearance of object parts by using an implicit model from the first observation, distils the part segmentation and articulation from the second observation while rendering the latter observation. Additionally, to tackle the complexities in the joint optimization of part segmentation and articulation, we propose a voxel grid-based initialization strategy and a decoupled optimization procedure. Compared to the prior unsupervised work, our model obtains significantly better performance, and generalizes to objects with multiple parts while it can be efficiently from few views for the latter observation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages for the maincontent, excluding references and supplementaries"
    },
    {
        "paper id": "2406.16633",
        "abstract url": "https://arxiv.org/abs/2406.16633",
        "title": "MLAAN: Scaling Supervised Local Learning with Multilaminar Leap Augmented Auxiliary Network",
        "rating": "0",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "biocompatibility"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "End-to-end (E2E) training approaches are commonly plagued by high memory consumption, reduced efficiency in training, challenges in model parallelization, and suboptimal biocompatibility. Local learning is considered a novel interactive training method that holds promise as an alternative to E2E. Nonetheless, conventional local learning methods fall short in achieving high model accuracy due to inadequate local inter-module interactions. In this paper, we introduce a new model known as the Scaling Supervised Local Learning with Multilaminar Leap Augmented Auxiliary Network (MLAAN). MLAAN features an innovative supervised local learning approach coupled with a robust reinforcement module. This dual-component design enables the MLAAN to integrate smoothly with established local learning techniques, thereby enhancing the efficacy of the foundational methods. The method simultaneously acquires the local and global features of the model separately by constructing an independent auxiliary network and a cascade auxiliary network on the one hand and incorporates a leap augmented module, which serves to counteract the reduced learning capacity often associated with weaker supervision. This architecture not only augments the exchange of information amongst the local modules but also effectively mitigates the model's tendency toward myopia. The experimental evaluations conducted on four benchmark datasets, CIFAR-10, STL-10, SVHN, and ImageNet, demonstrate that the integration of MLAAN with existing supervised local learning methods significantly enhances the original methodologies. Of particular note, MLAAN enables local learning methods to comprehensively outperform end-to-end training approaches in terms of optimal performance while saving GPU memory.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16638",
        "abstract url": "https://arxiv.org/abs/2406.16638",
        "title": "Feature Fusion for Human Activity Recognition using Parameter-Optimized Multi-Stage Graph Convolutional Network and Transformer Models",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Human activity recognition (HAR) is a crucial area of research that involves understanding human movements using computer and machine vision technology. Deep learning has emerged as a powerful tool for this task, with models such as Convolutional Neural Networks (CNNs) and Transformers being employed to capture various aspects of human motion. One of the key contributions of this work is the demonstration of the effectiveness of feature fusion in improving HAR accuracy by capturing spatial and temporal features, which has important implications for the development of more accurate and robust activity recognition systems. The study uses sensory data from HuGaDB, PKU-MMD, LARa, and TUG datasets. Two model, the PO-MS-GCN and a Transformer were trained and evaluated, with PO-MS-GCN outperforming state-of-the-art models. HuGaDB and TUG achieved high accuracies and f1-scores, while LARa and PKU-MMD had lower scores. Feature fusion improved results across datasets.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "7 pages, 1 figure, conference"
    },
    {
        "paper id": "2406.16658",
        "abstract url": "https://arxiv.org/abs/2406.16658",
        "title": "Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin Methods",
        "rating": "0",
        "keywords": [
            [
                "inpainting"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "This paper studies two classes of sampling methods for the solution of inverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in sensitivity analysis, and Langevin methods, which are rooted in the Bayesian framework. The two classes of methods correspond to different assumptions and yield samples from different target distributions. We highlight the main conceptual and theoretical differences between the two approaches and compare them from a practical point of view by tackling two classical inverse problems in imaging: deblurring and inpainting. We show that the choice of the sampling method has a significant impact on the quality of the reconstruction and that the RTO method is more robust to the choice of the parameters.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16683",
        "abstract url": "https://arxiv.org/abs/2406.16683",
        "title": "Repulsive Score Distillation for Diverse Sampling of Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Score distillation sampling has been pivotal for integrating diffusion models into generation of complex visuals. Despite impressive results it suffers from mode collapse and lack of diversity. To cope with this challenge, we leverage the gradient flow interpretation of score distillation to propose Repulsive Score Distillation (RSD). In particular, we propose a variational framework based on repulsion of an ensemble of particles that promotes diversity. Using a variational approximation that incorporates a coupling among particles, the repulsion appears as a simple regularization that allows interaction of particles based on their relative pairwise similarity, measured e.g., via radial basis kernels. We design RSD for both unconstrained and constrained sampling scenarios. For constrained sampling we focus on inverse problems in the latent space that leads to an augmented variational formulation, that strikes a good balance between compute, quality and diversity. Our extensive experiments for text-to-image generation, and inverse problems demonstrate that RSD achieves a superior trade-off between diversity and quality compared with state-of-the-art alternatives.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16722",
        "abstract url": "https://arxiv.org/abs/2406.16722",
        "title": "Venturing into Uncharted Waters: The Navigation Compass from Transformer to Mamba",
        "rating": "0",
        "keywords": [
            [
                "Navigation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Transformer, a deep neural network architecture, has long dominated the field of natural language processing and beyond. Nevertheless, the recent introduction of Mamba challenges its supremacy, sparks considerable interest among researchers, and gives rise to a series of Mamba-based models that have exhibited notable potential. This survey paper orchestrates a comprehensive discussion, diving into essential research dimensions, covering: (i) the functioning of the Mamba mechanism and its foundation on the principles of structured state space models; (ii) the proposed improvements and the integration of Mamba with various networks, exploring its potential as a substitute for Transformers; (iii) the combination of Transformers and Mamba to compensate for each other's shortcomings. We have also made efforts to interpret Mamba and Transformer in the framework of kernel functions, allowing for a comparison of their mathematical nature within a unified context. Our paper encompasses the vast majority of improvements related to Mamba to date.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16776",
        "abstract url": "https://arxiv.org/abs/2406.16776",
        "title": "Instance Consistency Regularization for Semi-Supervised 3D Instance Segmentation",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large-scale datasets with point-wise semantic and instance labels are crucial to 3D instance segmentation but also expensive. To leverage unlabeled data, previous semi-supervised 3D instance segmentation approaches have explored self-training frameworks, which rely on high-quality pseudo labels for consistency regularization. They intuitively utilize both instance and semantic pseudo labels in a joint learning manner. However, semantic pseudo labels contain numerous noise derived from the imbalanced category distribution and natural confusion of similar but distinct categories, which leads to severe collapses in self-training. Motivated by the observation that 3D instances are non-overlapping and spatially separable, we ask whether we can solely rely on instance consistency regularization for improved semi-supervised segmentation. To this end, we propose a novel self-training network InsTeacher3D to explore and exploit pure instance knowledge from unlabeled data. We first build a parallel base 3D instance segmentation model DKNet, which distinguishes each instance from the others via discriminative instance kernels without reliance on semantic segmentation. Based on DKNet, we further design a novel instance consistency regularization framework to generate and leverage high-quality instance pseudo labels. Experimental results on multiple large-scale datasets show that the InsTeacher3D significantly outperforms prior state-of-the-art semi-supervised approaches. Code is available: https://github.com/W1zheng/InsTeacher3D.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 10 figures"
    },
    {
        "paper id": "2406.16807",
        "abstract url": "https://arxiv.org/abs/2406.16807",
        "title": "Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Human feedback plays a critical role in learning and refining reward models for text-to-image generation, but the optimal form the feedback should take for learning an accurate reward function has not been conclusively established. This paper investigates the effectiveness of fine-grained feedback which captures nuanced distinctions in image quality and prompt-alignment, compared to traditional coarse-grained feedback (for example, thumbs up/down or ranking between a set of options). While fine-grained feedback holds promise, particularly for systems catering to diverse societal preferences, we show that demonstrating its superiority to coarse-grained feedback is not automatic. Through experiments on real and synthetic preference data, we surface the complexities of building effective models due to the interplay of model choice, feedback type, and the alignment between human judgment and computational interpretation. We identify key challenges in eliciting and utilizing fine-grained feedback, prompting a reassessment of its assumed benefits and practicality. Our findings -- e.g., that fine-grained feedback can lead to worse models for a fixed budget, in some settings; however, in controlled settings with known attributes, fine grained rewards can indeed be more helpful -- call for careful consideration of feedback attributes and potentially beckon novel modeling approaches to appropriately unlock the potential value of fine-grained feedback in-the-wild.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16864",
        "abstract url": "https://arxiv.org/abs/2406.16864",
        "title": "StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This work addresses the challenge of high-quality surface normal estimation from monocular colored inputs (i.e., images and videos), a field which has recently been revolutionized by repurposing diffusion priors. However, previous attempts still struggle with stochastic inference, conflicting with the deterministic nature of the Image2Normal task, and costly ensembling step, which slows down the estimation process. Our method, StableNormal, mitigates the stochasticity of the diffusion process by reducing inference variance, thus producing \"Stable-and-Sharp\" normal estimates without any additional ensembling process. StableNormal works robustly under challenging imaging conditions, such as extreme lighting, blurring, and low quality. It is also robust against transparent and reflective surfaces, as well as cluttered scenes with numerous objects. Specifically, StableNormal employs a coarse-to-fine strategy, which starts with a one-step normal estimator (YOSO) to derive an initial normal guess, that is relatively coarse but reliable, then followed by a semantic-guided refinement process (SG-DRN) that refines the normals to recover geometric details. The effectiveness of StableNormal is demonstrated through competitive performance in standard datasets such as DIODE-indoor, iBims, ScannetV2 and NYUv2, and also in various downstream tasks, such as surface reconstruction and normal enhancement. These results evidence that StableNormal retains both the \"stability\" and \"sharpness\" for accurate normal estimation. StableNormal represents a baby attempt to repurpose diffusion priors for deterministic estimation. To democratize this, code and models have been publicly available in hf.co/Stable-X",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "comment": "HF Demo: hf.co/Stable-X, Video: https://www.youtube.com/watch?v=sylXTxG_U2U"
    },
    {
        "paper id": "2406.17074",
        "abstract url": "https://arxiv.org/abs/2406.17074",
        "title": "Reducing the Memory Footprint of 3D Gaussian Splatting",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian splatting provides excellent visual quality for novel view synthesis, with fast training and real-time rendering; unfortunately, the memory requirements of this method for storing and transmission are unreasonably high. We first analyze the reasons for this, identifying three main areas where storage can be reduced: the number of 3D Gaussian primitives used to represent a scene, the number of coefficients for the spherical harmonics used to represent directional radiance, and the precision required to store Gaussian primitive attributes. We present a solution to each of these issues. First, we propose an efficient, resolution-aware primitive pruning approach, reducing the primitive count by half. Second, we introduce an adaptive adjustment method to choose the number of coefficients used to represent directional radiance for each Gaussian primitive, and finally a codebook-based quantization method, together with a half-float representation for further memory reduction. Taken together, these three components result in a 27 reduction in overall size on disk on the standard datasets we tested, along with a 1.7 speedup in rendering speed. We demonstrate our method on standard datasets and show how our solution results in significantly reduced download times when using the method on a mobile device.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project website: https://repo-sam.inria.fr/fungraph/reduced_3dgs/"
    },
    {
        "paper id": "2406.17104",
        "abstract url": "https://arxiv.org/abs/2406.17104",
        "title": "Automated Adversarial Discovery for Safety Classifiers",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Safety classifiers are critical in mitigating toxicity on online forums such as social media and in chatbots. Still, they continue to be vulnerable to emergent, and often innumerable, adversarial attacks. Traditional automated adversarial data generation methods, however, tend to produce attacks that are not diverse, but variations of previously observed harm types. We formalize the task of automated adversarial discovery for safety classifiers - to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier. We measure progress on this task along two key axes (1) adversarial success: does the attack fool the classifier? and (2) dimensional diversity: does the attack represent a previously unseen harm type? Our evaluation of existing attack generation methods on the CivilComments toxicity task reveals their limitations: Word perturbation attacks fail to fool classifiers, while prompt-based LLM attacks have more adversarial success, but lack dimensional diversity. Even our best-performing prompt-based method finds new successful attacks on unseen harm dimensions of attacks only 5\\% of the time. Automatically finding new harmful dimensions of attack is crucial and there is substantial headroom for future research on our new task.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Published at Fourth Workshop on TrustworthyNLP (TrustNLP) at NAACL 2024"
    },
    {
        "paper id": "2406.17169",
        "abstract url": "https://arxiv.org/abs/2406.17169",
        "title": "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types--propositional, first-order, and non-monotonic--consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs. Data is available at https://github.com/Mihir3009/Multi-LogiEval.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "23 Pages"
    },
    {
        "paper id": "2406.17183",
        "abstract url": "https://arxiv.org/abs/2406.17183",
        "title": "POPCat: Propagation of particles for complex annotation tasks",
        "rating": "0",
        "keywords": [
            [
                "time efficient"
            ],
            [
                "industrial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Novel dataset creation for all multi-object tracking, crowd-counting, and industrial-based videos is arduous and time-consuming when faced with a unique class that densely populates a video sequence. We propose a time efficient method called POPCat that exploits the multi-target and temporal features of video data to produce a semi-supervised pipeline for segmentation or box-based video annotation. The method retains the accuracy level associated with human level annotation while generating a large volume of semi-supervised annotations for greater generalization. The method capitalizes on temporal features through the use of a particle tracker to expand the domain of human-provided target points. This is done through the use of a particle tracker to reassociate the initial points to a set of images that follow the labeled frame. A YOLO model is then trained with this generated data, and then rapidly infers on the target video. Evaluations are conducted on GMOT-40, AnimalTrack, and Visdrone-2019 benchmarks. These multi-target video tracking/detection sets contain multiple similar-looking targets, camera movements, and other features that would commonly be seen in \"wild\" situations. We specifically choose these difficult datasets to demonstrate the efficacy of the pipeline and for comparison purposes. The method applied on GMOT-40, AnimalTrack, and Visdrone shows a margin of improvement on recall/mAP50/mAP over the best results by a value of 24.5%/9.6%/4.8%, -/43.1%/27.8%, and 7.5%/9.4%/7.5% where metrics were collected.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 5 figures, Accepted in \"Conference on Robots and Vision 2024\""
    },
    {
        "paper id": "2406.17231",
        "abstract url": "https://arxiv.org/abs/2406.17231",
        "title": "CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17236",
        "abstract url": "https://arxiv.org/abs/2406.17236",
        "title": "LIPE: Learning Personalized Identity Prior for Non-rigid Image Editing",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "Image Editing",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although recent years have witnessed significant advancements in image editing thanks to the remarkable progress of text-to-image diffusion models, the problem of non-rigid image editing still presents its complexities and challenges. Existing methods often fail to achieve consistent results due to the absence of unique identity characteristics. Thus, learning a personalized identity prior might help with consistency in the edited results. In this paper, we explore a novel task: learning the personalized identity prior for text-based non-rigid image editing. To address the problems in jointly learning prior and editing the image, we present LIPE, a two-stage framework designed to customize the generative model utilizing a limited set of images of the same subject, and subsequently employ the model with learned prior for non-rigid image editing. Experimental results demonstrate the advantages of our approach in various editing scenarios over past related leading methods in qualitative and quantitative ways.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17241",
        "abstract url": "https://arxiv.org/abs/2406.17241",
        "title": "What Do the Circuits Mean? A Knowledge Edit View",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Edit"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In the field of language model interpretability, circuit discovery is gaining popularity. Despite this, the true meaning of these circuits remain largely unanswered. We introduce a novel method to learn their meanings as a holistic object through the lens of knowledge editing. We extract circuits in the GPT2-XL model using diverse text classification datasets, and use hierarchical relations datasets to explore knowledge editing in the circuits. Our findings indicate that these circuits contain entity knowledge but resist new knowledge more than complementary circuits during knowledge editing. Additionally, we examine the impact of circuit size, discovering that an ideal \"theoretical circuit\" where essential knowledge is concentrated likely incorporates more than 5% but less than 50% of the model's parameters. We also assess the overlap between circuits from different datasets, finding moderate similarities. What constitutes these circuits, then? We find that up to 60% of the circuits consist of layer normalization modules rather than attention or MLP modules, adding evidence to the ongoing debates regarding knowledge localization. In summary, our findings offer new insights into the functions of the circuits, and introduce research directions for further interpretability and safety research of language models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17246",
        "abstract url": "https://arxiv.org/abs/2406.17246",
        "title": "Beyond Silence: Bias Analysis through Loss and Asymmetric Approach in Audio Anti-Spoofing",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Current trends in audio anti-spoofing detection research strive to improve models' ability to generalize across unseen attacks by learning to identify a variety of spoofing artifacts. This emphasis has primarily focused on the spoof class. Recently, several studies have noted that the distribution of silence differs between the two classes, which can serve as a shortcut. In this paper, we extend class-wise interpretations beyond silence. We employ loss analysis and asymmetric methodologies to move away from traditional attack-focused and result-oriented evaluations towards a deeper examination of model behaviors. Our investigations highlight the significant differences in training dynamics between the two classes, emphasizing the need for future research to focus on robust modeling of the bonafide class.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": "5 pages, 1 figure, 5 tables"
    },
    {
        "paper id": "2406.17253",
        "abstract url": "https://arxiv.org/abs/2406.17253",
        "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Edit"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "As large language models (LLMs) are widely deployed, targeted editing of their knowledge has become a critical challenge. Recently, advancements in model editing techniques, such as Rank-One Model Editing (ROME), have paved the way for updating LLMs with new knowledge. However, the efficacy of these methods varies across different types of knowledge. This study investigates the capability of knowledge editing methods to incorporate new knowledge with varying degrees of \"perplexingness\", a term we use to describe the initial difficulty LLMs have in understanding new concepts. We begin by quantifying the \"perplexingness\" of target knowledge using pre-edit conditional probabilities, and assess the efficacy of edits through post-edit conditional probabilities. Utilizing the widely-used CounterFact dataset, we find significant negative correlations between the \"perplexingness\" of the new knowledge and the edit efficacy across all 12 scenarios. To dive deeper into this phenomenon, we introduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym pairs across diverse categories. Our analysis reveal that more abstract concepts (hypernyms) tend to be more perplexing than their specific counterparts (hyponyms). Further exploration into the influence of knowledge hierarchy on editing outcomes indicates that knowledge positioned at higher hierarchical levels is more challenging to modify in some scenarios. Our research highlights a previously overlooked aspect of LLM editing: the variable efficacy of editing methods in handling perplexing knowledge. By revealing how hierarchical relationships can influence editing outcomes, our findings offer new insights into the challenges of updating LLMs and pave the way for more nuanced approaches to model editing in the future.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17256",
        "abstract url": "https://arxiv.org/abs/2406.17256",
        "title": "Disentangled Motion Modeling for Video Frame Interpolation",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video frame interpolation (VFI) aims to synthesize intermediate frames in between existing frames to enhance visual smoothness and quality. Beyond the conventional methods based on the reconstruction loss, recent works employ the high quality generative models for perceptual quality. However, they require complex training and large computational cost for modeling on the pixel space. In this paper, we introduce disentangled Motion Modeling (MoMo), a diffusion-based approach for VFI that enhances visual quality by focusing on intermediate motion modeling. We propose disentangled two-stage training process, initially training a frame synthesis model to generate frames from input pairs and their optical flows. Subsequently, we propose a motion diffusion model, equipped with our novel diffusion U-Net architecture designed for optical flow, to produce bi-directional flows between frames. This method, by leveraging the simpler low-frequency representation of motions, achieves superior perceptual quality with reduced computational demands compared to generative modeling methods on the pixel space. Our method surpasses state-of-the-art methods in perceptual metrics across various benchmarks, demonstrating its efficacy and efficiency in VFI. Our code is available at: https://github.com/JHLew/MoMo",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17271",
        "abstract url": "https://arxiv.org/abs/2406.17271",
        "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs. Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed. In this work, we introduce Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity. Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data. Such newly generated test samples can have different levels of complexity while maintaining linguistic diversity similar to the original benchmarks. We further use a code-augmented LLM to ensure the label correctness of newly generated data. We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs. Experimental results show that almost all LLMs experience a performance decrease with increased complexity and certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit more biases when being evaluated via the data generated by DARG with higher complexity levels. These observations provide useful insights into how to dynamically and adaptively evaluate LLMs. The code is available at https://github.com/SALT-NLP/DARG.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16321",
        "abstract url": "https://arxiv.org/abs/2406.16321",
        "title": "Multimodal Graph Benchmark",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Associating unstructured data with structured information is crucial for real-world tasks that require relevance search. However, existing graph learning benchmarks often overlook the rich semantic information associate with each node. To bridge such gap, we introduce the Multimodal Graph Benchmark (MM-GRAPH), the first comprehensive multi-modal graph benchmark that incorporates both textual and visual information. MM-GRAPH surpasses previous efforts, which have primarily focused on text-attributed graphs with various connectivity patterns. MM-GRAPH consists of five graph learning datasets of various scales that are appropriate for different learning tasks. Their multimodal node features, enabling a more comprehensive evaluation of graph learning algorithms in real-world scenarios. To facilitate research on multimodal graph learning, we further provide an extensive study on the performance of various graph neural networks in the presence of features from various modalities. MM-GRAPH aims to foster research on multimodal graph learning and drive the development of more advanced and robust graph learning algorithms. By providing a diverse set of datasets and benchmarks, MM-GRAPH enables researchers to evaluate and compare their models in realistic settings, ultimately leading to improved performance on real-world applications that rely on multimodal graph data.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "https://mm-graph-benchmark.github.io/"
    },
    {
        "paper id": "2406.16355",
        "abstract url": "https://arxiv.org/abs/2406.16355",
        "title": "Compact Model Parameter Extraction via Derivative-Free Optimization",
        "rating": "-0.5",
        "keywords": [
            [
                "GaN"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we address the problem of compact model parameter extraction to simultaneously extract tens of parameters via derivative-free optimization. Traditionally, parameter extraction is performed manually by dividing the complete set of parameters into smaller subsets, each targeting different operational regions of the device, a process that can take several days or even weeks. Our approach streamlines this process by employing derivative-free optimization to identify a good parameter set that best fits the compact model without performing an exhaustive number of simulations. We further enhance the optimization process to address critical issues in device modeling by carefully choosing a loss function that evaluates model performance consistently across varying magnitudes by focusing on relative errors (as opposed to absolute errors), prioritizing accuracy in key operational regions of the device above a certain threshold, and reducing sensitivity to outliers. Furthermore, we utilize the concept of train-test split to assess the model fit and avoid overfitting. This is done by fitting 80% of the data and testing the model efficacy with the remaining 20%. We demonstrate the effectiveness of our methodology by successfully modeling two semiconductor devices: a diamond Schottky diode and a GaN-on-SiC HEMT, with the latter involving the ASM-HEMT DC model, which requires simultaneously extracting 35 model parameters to fit the model to the measured data. These examples demonstrate the effectiveness of our approach and showcase the practical benefits of derivative-free optimization in device modeling.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16525",
        "abstract url": "https://arxiv.org/abs/2406.16525",
        "title": "OAML: Outlier Aware Metric Learning for OOD Detection Enhancement",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Out-of-distribution (OOD) detection methods have been developed to identify objects that a model has not seen during training. The Outlier Exposure (OE) methods use auxiliary datasets to train OOD detectors directly. However, the collection and learning of representative OOD samples may pose challenges. To tackle these issues, we propose the Outlier Aware Metric Learning (OAML) framework. The main idea of our method is to use the k-NN algorithm and Stable Diffusion model to generate outliers for training at the feature level without making any distributional assumptions. To increase feature discrepancies in the semantic space, we develop a mutual information-based contrastive learning approach for learning from OOD data effectively. Both theoretical and empirical results confirm the effectiveness of this contrastive learning technique. Furthermore, we incorporate knowledge distillation into our learning framework to prevent degradation of in-distribution classification accuracy. The combination of contrastive learning and knowledge distillation algorithms significantly enhances the performance of OOD detection. Experimental results across various datasets show that our method significantly outperforms previous OE methods.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16552",
        "abstract url": "https://arxiv.org/abs/2406.16552",
        "title": "Inference of Sequential Patterns for Neural Message Passing in Temporal Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "The modelling of temporal patterns in dynamic graphs is an important current research issue in the development of time-aware GNNs. Whether or not a specific sequence of events in a temporal graph constitutes a temporal pattern not only depends on the frequency of its occurrence. We consider whether it deviates from what is expected in a temporal graph where timestamps are randomly shuffled. While accounting for such a random baseline is important to model temporal patterns, it has mostly been ignored by current temporal graph neural networks. To address this issue we propose HYPA-DBGNN, a novel two-step approach that combines (i) the inference of anomalous sequential patterns in time series data on graphs based on a statistically principled null model, with (ii) a neural message passing approach that utilizes a higher-order De Bruijn graph whose edges capture overrepresented sequential patterns. Our method leverages hypergeometric graph ensembles to identify anomalous edges within both first- and higher-order De Bruijn graphs, which encode the temporal ordering of events. The model introduces an inductive bias that enhances model interpretability. We evaluate our approach for static node classification using benchmark datasets and a synthetic dataset that showcases its ability to incorporate the observed inductive bias regarding over- and under-represented temporal edges. We demonstrate the framework's effectiveness in detecting similar patterns within empirical datasets, resulting in superior performance compared to baseline methods in node classification tasks. To the best of our knowledge, our work is the first to introduce statistically informed GNNs that leverage temporal and causal sequence anomalies. HYPA-DBGNN represents a path for bridging the gap between statistical graph inference and neural graph representation learning, with potential applications to static GNNs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.SI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16565",
        "abstract url": "https://arxiv.org/abs/2406.16565",
        "title": "Noisy Neighbors: Efficient membership inference attacks against LLMs",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The potential of transformer-based LLMs risks being hindered by privacy concerns due to their reliance on extensive datasets, possibly including sensitive information. Regulatory measures like GDPR and CCPA call for using robust auditing tools to address potential privacy issues, with Membership Inference Attacks (MIA) being the primary method for assessing LLMs' privacy risks. Differently from traditional MIA approaches, often requiring computationally intensive training of additional models, this paper introduces an efficient methodology that generates \\textit{noisy neighbors} for a target sample by adding stochastic noise in the embedding space, requiring operating the target model in inference mode only. Our findings demonstrate that this approach closely matches the effectiveness of employing shadow models, showing its usability in practical privacy auditing scenarios.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16687",
        "abstract url": "https://arxiv.org/abs/2406.16687",
        "title": "Link Prediction with Untrained Message Passing Layers",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Message passing neural networks (MPNNs) operate on graphs by exchanging information between neigbouring nodes. MPNNs have been successfully applied to various node-, edge-, and graph-level tasks in areas like molecular science, computer vision, natural language processing, and combinatorial optimization. However, most MPNNs require training on large amounts of labeled data, which can be costly and time-consuming. In this work, we explore the use of various untrained message passing layers in graph neural networks, i.e. variants of popular message passing architecture where we remove all trainable parameters that are used to transform node features in the message passing step. Focusing on link prediction, we find that untrained message passing layers can lead to competitive and even superior performance compared to fully trained MPNNs, especially in the presence of high-dimensional features. We provide a theoretical analysis of untrained message passing by relating the inner products of features implicitly produced by untrained message passing layers to path-based topological node similarity measures. As such, untrained message passing architectures can be viewed as a highly efficient and interpretable approach to link prediction.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16697",
        "abstract url": "https://arxiv.org/abs/2406.16697",
        "title": "Expected Runtime Comparisons Between Breadth-First Search and Constant-Depth Restarting Random Walks",
        "rating": "-0.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "When greedy search algorithms encounter a local minima or plateau, the search typically devolves into a breadth-first search (BrFS), or a local search technique is used in an attempt to find a way out. In this work, we formally analyze the performance of BrFS and constant-depth restarting random walks (RRW) -- two methods often used for finding exits to a plateau/local minima -- to better understand when each is best suited. In particular, we formally derive the expected runtime for BrFS in the case of a uniformly distributed set of goals at a given goal depth. We then prove RRW will be faster than BrFS on trees if there are enough goals at that goal depth. We refer to this threshold as the crossover point. Our bound shows that the crossover point grows linearly with the branching factor of the tree, the goal depth, and the error in the random walk depth, while the size of the tree grows exponentially in branching factor and goal depth. Finally, we discuss the practical implications and applicability of this bound.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "ICAPS 2024 Heuristics and Search for Domain-Independent Planning Workshop, 5 pages, 1 figure"
    },
    {
        "paper id": "2406.16708",
        "abstract url": "https://arxiv.org/abs/2406.16708",
        "title": "CausalFormer: An Interpretable Transformer for Temporal Causal Discovery",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Temporal causal discovery is a crucial task aimed at uncovering the causal relations within time series data. The latest temporal causal discovery methods usually train deep learning models on prediction tasks to uncover the causality between time series. They capture causal relations by analyzing the parameters of some components of the trained models, e.g., attention weights and convolution weights. However, this is an incomplete mapping process from the model parameters to the causality and fails to investigate the other components, e.g., fully connected layers and activation functions, that are also significant for causal discovery. To facilitate the utilization of the whole deep learning models in temporal causal discovery, we proposed an interpretable transformer-based causal discovery model termed CausalFormer, which consists of the causality-aware transformer and the decomposition-based causality detector. The causality-aware transformer learns the causal representation of time series data using a prediction task with the designed multi-kernel causal convolution which aggregates each input time series along the temporal dimension under the temporal priority constraint. Then, the decomposition-based causality detector interprets the global structure of the trained causality-aware transformer with the proposed regression relevance propagation to identify potential causal relations and finally construct the causal graph. Experiments on synthetic, simulated, and real datasets demonstrate the state-of-the-art performance of CausalFormer on discovering temporal causality. Our code is available at https://github.com/lingbai-kong/CausalFormer.",
        "subjects": [
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16716",
        "abstract url": "https://arxiv.org/abs/2406.16716",
        "title": "One-Class Learning with Adaptive Centroid Shift for Audio Deepfake Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "Deepfake"
            ],
            [
                "attacks"
            ],
            [
                "cs.SD",
                "eess.AS"
            ],
            [
                "Interspeech"
            ]
        ],
        "abstract": "As speech synthesis systems continue to make remarkable advances in recent years, the importance of robust deepfake detection systems that perform well in unseen systems has grown. In this paper, we propose a novel adaptive centroid shift (ACS) method that updates the centroid representation by continually shifting as the weighted average of bonafide representations. Our approach uses only bonafide samples to define their centroid, which can yield a specialized centroid for one-class learning. Integrating our ACS with one-class learning gathers bonafide representations into a single cluster, forming well-separated embeddings robust to unseen spoofing attacks. Our proposed method achieves an equal error rate (EER) of 2.19% on the ASVspoof 2021 deepfake dataset, outperforming all existing systems. Furthermore, the t-SNE visualization illustrates that our method effectively maps the bonafide embeddings into a single cluster and successfully disentangles the bonafide and spoof classes.",
        "subjects": [
            "eess.AS",
            "cs.CR",
            "cs.SD"
        ],
        "comment": "Accepted by Interspeech 2024"
    },
    {
        "paper id": "2406.16730",
        "abstract url": "https://arxiv.org/abs/2406.16730",
        "title": "Convolutional neural network for Lyman break galaxies classification and redshift regression in DESI (Dark Energy Spectroscopic Instrument)",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "DESI is a groundbreaking international project to observe more than 40 million quasars and galaxies over a 5-year period to create a 3D map of the sky. This map will enable us to probe multiple aspects of cosmology, from dark energy to neutrino mass. We are focusing here on one type of object observed by DESI, the Lyman Break Galaxies (LBGs). The aim is to use their spectra to determine whether they are indeed LBGs, and if so, to determine their distance from the Earth using a phenomenon called redshift. This will enable us to place these galaxies on the DESI 3D map. The aim is therefore to develop a convolutional neural network (CNN) inspired by QuasarNET (See arXiv:1808.09955), performing simultaneously a classification (LBG type or not) and a regression task (determine the redshift of the LBGs). Initially, data augmentation techniques such as shifting the spectra in wavelengths, adding noise to the spectra, or adding synthetic spectra were used to increase the model training dataset from 3,019 data to over 66,000. In a second phase, modifications to the QuasarNET architecture, notably through transfer learning and hyperparameter tuning with Bayesian optimization, boosted model performance. Gains of up to 26% were achieved on the Purity/Efficiency curve, which is used to evaluate model performance, particularly in areas with interesting redshifts, at low (around 2) and high (around 4) redshifts. The best model obtained an average score of 94%, compared with 75% for the initial model.",
        "subjects": [
            "astro-ph.CO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16816",
        "abstract url": "https://arxiv.org/abs/2406.16816",
        "title": "On the Impact of Sample Size in Reconstructing Noisy Graph Signals: A Theoretical Characterisation",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Reconstructing a signal on a graph from noisy observations of a subset of the vertices is a fundamental problem in the field of graph signal processing. This paper investigates how sample size affects reconstruction error in the presence of noise via an in-depth theoretical analysis of the two most common reconstruction methods in the literature, least-squares reconstruction (LS) and graph-Laplacian regularised reconstruction (GLR). Our theorems show that at sufficiently low signal-to-noise ratios (SNRs), under these reconstruction methods we may simultaneously decrease sample size and decrease average reconstruction error. We further show that at sufficiently low SNRs, for LS reconstruction we have a $\u039b$-shaped error curve and for GLR reconstruction, a sample size of $ \\mathcal{O}(\\sqrt{N})$, where $N$ is the total number of vertices, results in lower reconstruction error than near full observation. We present thresholds on the SNRs, $\u03c4$ and $\u03c4_{GLR}$, below which the error is non-monotonic, and illustrate these theoretical results with experiments across multiple random graph models, sampling schemes and SNRs. These results demonstrate that any decision in sample-size choice has to be made in light of the noise levels in the data.",
        "subjects": [
            "eess.SP",
            "cs.SI"
        ],
        "comment": "The paper arXiv:2307.00336v1 is the earlier, shorter conference version of this paper"
    },
    {
        "paper id": "2406.16992",
        "abstract url": "https://arxiv.org/abs/2406.16992",
        "title": "Make Graph Neural Networks Great Again: A Generic Integration Paradigm of Topology-Free Patterns for Traffic Speed Prediction",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Urban traffic speed prediction aims to estimate the future traffic speed for improving urban transportation services. Enormous efforts have been made to exploit Graph Neural Networks (GNNs) for modeling spatial correlations and temporal dependencies of traffic speed evolving patterns, regularized by graph topology.While achieving promising results, current traffic speed prediction methods still suffer from ignoring topology-free patterns, which cannot be captured by GNNs. To tackle this challenge, we propose a generic model for enabling the current GNN-based methods to preserve topology-free patterns. Specifically, we first develop a Dual Cross-Scale Transformer (DCST) architecture, including a Spatial Transformer and a Temporal Transformer, to preserve the cross-scale topology-free patterns and associated dynamics, respectively. Then, to further integrate both topology-regularized/-free patterns, we propose a distillation-style learning framework, in which the existing GNN-based methods are considered as the teacher model, and the proposed DCST architecture is considered as the student model. The teacher model would inject the learned topology-regularized patterns into the student model for integrating topology-free patterns. The extensive experimental results demonstrated the effectiveness of our methods.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted to IJCAI 2024"
    },
    {
        "paper id": "2406.17057",
        "abstract url": "https://arxiv.org/abs/2406.17057",
        "title": "At First Sight: Zero-Shot Classification of Astronomical Images with Large Multimodal Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "astronomy"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Vision-Language multimodal Models (VLMs) offer the possibility for zero-shot classification in astronomy: i.e. classification via natural language prompts, with no training. We investigate two models, GPT-4o and LLaVA-NeXT, for zero-shot classification of low-surface brightness galaxies and artifacts, as well as morphological classification of galaxies. We show that with natural language prompts these models achieved significant accuracy (above 80 percent typically) without additional training/fine tuning. We discuss areas that require improvement, especially for LLaVA-NeXT, which is an open source model. Our findings aim to motivate the astronomical community to consider VLMs as a powerful tool for both research and pedagogy, with the prospect that future custom-built or fine-tuned models could perform better.",
        "subjects": [
            "astro-ph.IM",
            "astro-ph.GA",
            "cs.AI"
        ],
        "comment": "5 pages, 3 images. Prepared for submission to RNAAS"
    },
    {
        "paper id": "2406.17092",
        "abstract url": "https://arxiv.org/abs/2406.17092",
        "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Safety backdoor attacks in large language models (LLMs) enable the stealthy triggering of unsafe behaviors while evading detection during normal interactions. The high dimensionality of potential triggers in the token space and the diverse range of malicious behaviors make this a critical challenge. We present BEEAR, a mitigation approach leveraging the insight that backdoor triggers induce relatively uniform drifts in the model's embedding space. Our bi-level optimization method identifies universal embedding perturbations that elicit unwanted behaviors and adjusts the model parameters to reinforce safe behaviors against these perturbations. Experiments show BEEAR reduces the success rate of RLHF time backdoor attacks from >95% to <1% and from 47% to 0% for instruction-tuning time backdoors targeting malicious code generation, without compromising model utility. Requiring only defender-defined safe and unwanted behaviors, BEEAR represents a step towards practical defenses against safety backdoors in LLMs, providing a foundation for further advancements in AI safety and security.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17114",
        "abstract url": "https://arxiv.org/abs/2406.17114",
        "title": "Inception: Efficiently Computable Misinformation Attacks on Markov Games",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study security threats to Markov games due to information asymmetry and misinformation. We consider an attacker player who can spread misinformation about its reward function to influence the robust victim player's behavior. Given a fixed fake reward function, we derive the victim's policy under worst-case rationality and present polynomial-time algorithms to compute the attacker's optimal worst-case policy based on linear programming and backward induction. Then, we provide an efficient inception (\"planting an idea in someone's mind\") attack algorithm to find the optimal fake reward function within a restricted set of reward functions with dominant strategies. Importantly, our methods exploit the universal assumption of rationality to compute attacks efficiently. Thus, our work exposes a security vulnerability arising from standard game assumptions under misinformation.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.GT"
        ],
        "comment": "Accepted to Reinforcement Learning Conference (RLC) 2024"
    },
    {
        "paper id": "2406.17145",
        "abstract url": "https://arxiv.org/abs/2406.17145",
        "title": "GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks (DNNs) continue to grow rapidly in size, making them infeasible to train on a single device. Pipeline parallelism is commonly used in existing DNN systems to support large-scale DNN training by partitioning a DNN into multiple stages, which concurrently perform DNN training for different micro-batches in a pipeline fashion. However, existing pipeline-parallel approaches only consider sequential pipeline stages and thus ignore the topology of a DNN, resulting in missed model-parallel opportunities. This paper presents graph pipeline parallelism (GPP), a new pipeline-parallel scheme that partitions a DNN into pipeline stages whose dependencies are identified by a directed acyclic graph. GPP generalizes existing sequential pipeline parallelism and preserves the inherent topology of a DNN to enable concurrent execution of computationally-independent operators, resulting in reduced memory requirement and improved GPU performance. In addition, we develop GraphPipe, a distributed system that exploits GPP strategies to enable performant and scalable DNN training. GraphPipe partitions a DNN into a graph of stages, optimizes micro-batch schedules for these stages, and parallelizes DNN training using the discovered GPP strategies. Evaluation on a variety of DNNs shows that GraphPipe outperforms existing pipeline-parallel systems such as PipeDream and Piper by up to 1.6X. GraphPipe also reduces the search time by 9-21X compared to PipeDream and Piper.",
        "subjects": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17168",
        "abstract url": "https://arxiv.org/abs/2406.17168",
        "title": "Reinforcement Learning via Auxiliary Task Distillation",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We present Reinforcement Learning via Auxiliary Task Distillation (AuxDistill), a new method that enables reinforcement learning (RL) to perform long-horizon robot control problems by distilling behaviors from auxiliary RL tasks. AuxDistill achieves this by concurrently carrying out multi-task RL with auxiliary tasks, which are easier to learn and relevant to the main task. A weighted distillation loss transfers behaviors from these auxiliary tasks to solve the main task. We demonstrate that AuxDistill can learn a pixels-to-actions policy for a challenging multi-stage embodied object rearrangement task from the environment reward without demonstrations, a learning curriculum, or pre-trained skills. AuxDistill achieves $2.3 \\times$ higher success than the previous state-of-the-art baseline in the Habitat Object Rearrangement benchmark and outperforms methods that use pre-trained skills and expert demonstrations.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17199",
        "abstract url": "https://arxiv.org/abs/2406.17199",
        "title": "Contrastive General Graph Matching with Adaptive Augmentation Sampling",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph matching has important applications in pattern recognition and beyond. Current approaches predominantly adopt supervised learning, demanding extensive labeled data which can be limited or costly. Meanwhile, self-supervised learning methods for graph matching often require additional side information such as extra categorical information and input features, limiting their application to the general case. Moreover, designing the optimal graph augmentations for self-supervised graph matching presents another challenge to ensure robustness and efficacy. To address these issues, we introduce a novel Graph-centric Contrastive framework for Graph Matching (GCGM), capitalizing on a vast pool of graph augmentations for contrastive learning, yet without needing any side information. Given the variety of augmentation choices, we further introduce a Boosting-inspired Adaptive Augmentation Sampler (BiAS), which adaptively selects more challenging augmentations tailored for graph matching. Through various experiments, our GCGM surpasses state-of-the-art self-supervised methods across various datasets, marking a significant step toward more effective, efficient and general graph matching.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17210",
        "abstract url": "https://arxiv.org/abs/2406.17210",
        "title": "Dynamic Metric Embedding into $\\ell_p$ Space",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "We give the first non-trivial decremental dynamic embedding of a weighted, undirected graph $G$ into $\\ell_p$ space. Given a weighted graph $G$ undergoing a sequence of edge weight increases, the goal of this problem is to maintain a (randomized) mapping $\u03c6: (G,d) \\to (X,\\ell_p)$ from the set of vertices of the graph to the $\\ell_p$ space such that for every pair of vertices $u$ and $v$, the expected distance between $\u03c6(u)$ and $\u03c6(v)$ in the $\\ell_p$ metric is within a small multiplicative factor, referred to as the distortion, of their distance in $G$. Our main result is a dynamic algorithm with expected distortion $O(\\log^2 n)$ and total update time $O\\left((m^{1+o(1)} \\log^2 W + Q)\\log(nW) \\right)$, where $W$ is the maximum weight of the edges, $Q$ is the total number of updates and $n, m$ denote the number of vertices and edges in $G$ respectively. This is the first result of its kind, extending the seminal result of Bourgain to the expanding field of dynamic algorithms. Moreover, we demonstrate that in the fully dynamic regime, where we tolerate edge insertions as well as deletions, no algorithm can explicitly maintain an embedding into $\\ell_p$ space that has a low distortion with high probability.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "Accepted to ICML 2024 (15 pages, 3 figures)"
    },
    {
        "paper id": "2406.17213",
        "abstract url": "https://arxiv.org/abs/2406.17213",
        "title": "Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence Coverage",
        "rating": "-0.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "News media structure their reporting of events or issues using certain perspectives. When describing an incident involving gun violence, for example, some journalists may focus on mental health or gun regulation, while others may emphasize the discussion of gun rights. Such perspectives are called \\say{frames} in communication research. We study, for the first time, the value of combining lead images and their contextual information with text to identify the frame of a given news article. We observe that using multiple modes of information(article- and image-derived features) improves prediction of news frames over any single mode of information when the images are relevant to the frames of the headlines. We also observe that frame image relevance is related to the ease of conveying frames via images, which we call frame concreteness. Additionally, we release the first multimodal news framing dataset related to gun violence in the U.S., curated and annotated by communication researchers. The dataset will allow researchers to further examine the use of multiple information modalities for studying media framing.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "published at Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
        "paper id": "2406.17219",
        "abstract url": "https://arxiv.org/abs/2406.17219",
        "title": "Facial Identity Anonymization via Intrinsic and Extrinsic Attention Distraction",
        "rating": "-0.5",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "The unprecedented capture and application of face images raise increasing concerns on anonymization to fight against privacy disclosure. Most existing methods may suffer from the problem of excessive change of the identity-independent information or insufficient identity protection. In this paper, we present a new face anonymization approach by distracting the intrinsic and extrinsic identity attentions. On the one hand, we anonymize the identity information in the feature space by distracting the intrinsic identity attention. On the other, we anonymize the visual clues (i.e. appearance and geometry structure) by distracting the extrinsic identity attention. Our approach allows for flexible and intuitive manipulation of face appearance and geometry structure to produce diverse results, and it can also be used to instruct users to perform personalized anonymization. We conduct extensive experiments on multiple datasets and demonstrate that our approach outperforms state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024: 12406-12415"
    },
    {
        "paper id": "2406.17228",
        "abstract url": "https://arxiv.org/abs/2406.17228",
        "title": "Greedy equivalence search for nonparametric graphical models",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "One of the hallmark achievements of the theory of graphical models and Bayesian model selection is the celebrated greedy equivalence search (GES) algorithm due to Chickering and Meek. GES is known to consistently estimate the structure of directed acyclic graph (DAG) models in various special cases including Gaussian and discrete models, which are in particular curved exponential families. A general theory that covers general nonparametric DAG models, however, is missing. Here, we establish the consistency of greedy equivalence search for general families of DAG models that satisfy smoothness conditions on the Markov factorization, and hence may not be curved exponential families, or even parametric. The proof leverages recent advances in nonparametric Bayes to construct a test for comparing misspecified DAG models that avoids arguments based on the Laplace approximation. Nonetheless, when the Laplace approximation is valid and a consistent scoring function exists, we recover the classical result. As a result, we obtain a general consistency theorem for GES applied to general DAG models.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16333",
        "abstract url": "https://arxiv.org/abs/2406.16333",
        "title": "Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The rapid advancement of Text-to-Image(T2I) generative models has enabled the synthesis of high-quality images guided by textual descriptions. Despite this significant progress, these models are often susceptible in generating contents that contradict the input text, which poses a challenge to their reliability and practical deployment. To address this problem, we introduce a novel diffusion-based framework to significantly enhance the alignment of generated images with their corresponding descriptions, addressing the inconsistency between visual output and textual input. Our framework is built upon a comprehensive analysis of inconsistency phenomena, categorizing them based on their manifestation in the image. Leveraging a state-of-the-art large language module, we first extract objects and construct a knowledge graph to predict the locations of these objects in potentially generated images. We then integrate a state-of-the-art controllable image generation model with a visual text generation module to generate an image that is consistent with the original prompt, guided by the predicted object locations. Through extensive experiments on an advanced multimodal hallucination benchmark, we demonstrate the efficacy of our approach in accurately generating the images without the inconsistency with the original prompt. The code can be accessed via https://github.com/TruthAI-Lab/PCIG.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16341",
        "abstract url": "https://arxiv.org/abs/2406.16341",
        "title": "EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "Health",
                "healthcare",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs. EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 3,943 entities across 105 clinical notes checked against database entries for consistency. EHRCon has two versions, one using the original MIMIC-III schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings. The code is available at https://github.com/dustn1259/EHRCon.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16362",
        "abstract url": "https://arxiv.org/abs/2406.16362",
        "title": "Open-Source Tool Based Framework for Automated Performance Evaluation of an AD Function",
        "rating": "-1",
        "keywords": [
            [
                "automated driving"
            ]
        ],
        "abstract": "As automation in the field of automated driving (AD) progresses, ensuring the safety and functionality of AD functions (ADFs) becomes crucial. Virtual scenario-based testing has emerged as a prevalent method for evaluating these systems, allowing for a wider range of testing environments and reproducibility of results. This approach involves AD-equipped test vehicles operating within predefined scenarios to achieve specific driving objectives. To comprehensively assess the impact of road network properties on the performance of an ADF, varying parameters such as intersection angle, curvature and lane width is essential. However, covering all potential scenarios is impractical, necessitating the identification of feasible parameter ranges and automated generation of corresponding road networks for simulation. Automating the workflow of road network generation, parameter variation, simulation, and evaluation leads to a comprehensive understanding of an ADF's behavior in diverse road network conditions. This paper aims to investigate the influence of road network parameters on the performance of a prototypical ADF through virtual scenario-based testing, ultimately advocating the importance of road topology in assuring safety and reliability of ADFs.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "III. International Conference on Electrical, Computer and Energy Technologies (ICECET 2023), 16 - 17 November 2023, Cape Town-South Africa"
    },
    {
        "paper id": "2406.16376",
        "abstract url": "https://arxiv.org/abs/2406.16376",
        "title": "Multi-Objective Global Path Planning for Lunar Exploration With a Quadruped Robot",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "In unstructured environments the best path is not always the shortest, but needs to consider various objectives like energy efficiency, risk of failure or scientific outcome. This paper proposes a global planner, based on the A* algorithm, capable of individually considering multiple layers of map data for different cost objectives. We introduce weights between the objectives, which can be adapted to achieve a variety of optimal paths. In order to find the best of these paths, a tool for statistical path analysis is presented. Our planner was tested on exemplary lunar topographies to propose two trajectories for exploring the Aristarchus Plateau. The optimized paths significantly reduce the risk of failure while yielding more scientific value compared to a manually planned paths in the same area. The planner and analysis tool are made open-source in order to simplify mission planning for planetary scientists.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 19 figures, IEEE conference iSpaRo 2024"
    },
    {
        "paper id": "2406.16390",
        "abstract url": "https://arxiv.org/abs/2406.16390",
        "title": "On the Confluence of Directed Graph Reductions Preserving Feedback Vertex Set Minimality",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "In graph theory, the minimum directed feedback vertex set (FVS) problem consists in identifying the smallest subsets of vertices in a directed graph whose deletion renders the directed graph acyclic. Although being known as NP-hard since 1972, this problem can be solved in a reasonable time on small instances, or on instances having special combinatorial structure. In this paper we investigate graph reductions preserving all or some minimum FVS and focus on their properties, especially the Church-Rosser property, also called confluence. The Church-Rosser property implies the irrelevance of reduction order, leading to a unique directed graph. The study seeks the largest subset of reductions with the Church-Rosser property and explores the adaptability of reductions to meet this criterion. Addressing these questions is crucial, as it may impact algorithmic implications, allowing for parallelization and speeding up sequential algorithms.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16397",
        "abstract url": "https://arxiv.org/abs/2406.16397",
        "title": "Uniform Sampling and Visualization of 3D Reluctant Walks",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "A family of walks confined to the first orthant whose defining stepset has drift outside of the region can be challenging to sample uniformly at random for large lengths. We address this by generalizing the 2D walk sampler of Lumbroso et al. to handle 3D walks restricted to the first orthant. The sampler includes a visualizer and means to animate the walks.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16415",
        "abstract url": "https://arxiv.org/abs/2406.16415",
        "title": "Counting Colored Tilings on Grids and Graphs",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "In this paper, we explore some generalizations of a counting problem related to tilings in grids of size 2xn, which was originally posed as a question on Mathematics Stack Exchange (Question 3972905). In particular, we consider this problem for the product of two graphs G and P(n), where P(n) is the path graph of n vertices. We give explicit bivariate generating functions for some specific cases.",
        "subjects": [
            "cs.DM",
            "math.CO"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16434",
        "abstract url": "https://arxiv.org/abs/2406.16434",
        "title": "Multi-threshold Deep Metric Learning for Facial Expression Recognition",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Effective expression feature representations generated by a triplet-based deep metric learning are highly advantageous for facial expression recognition (FER). The performance of triplet-based deep metric learning is contingent upon identifying the best threshold for triplet loss. Threshold validation, however, is tough and challenging, as the ideal threshold changes among datasets and even across classes within the same dataset. In this paper, we present the multi-threshold deep metric learning technique, which not only avoids the difficult threshold validation but also vastly increases the capacity of triplet loss learning to construct expression feature representations. We find that each threshold of the triplet loss intrinsically determines a distinctive distribution of inter-class variations and corresponds, thus, to a unique expression feature representation. Therefore, rather than selecting a single optimal threshold from a valid threshold range, we thoroughly sample thresholds across the range, allowing the representation characteristics manifested by thresholds within the range to be fully extracted and leveraged for FER. To realize this approach, we partition the embedding layer of the deep metric learning network into a collection of slices and model training these embedding slices as an end-to-end multi-threshold deep metric learning problem. Each embedding slice corresponds to a sample threshold and is learned by enforcing the corresponding triplet loss, yielding a set of distinct expression features, one for each embedding slice. It makes the embedding layer, which is composed of a set of slices, a more informative and discriminative feature, hence enhancing the FER accuracy. Extensive evaluations demonstrate the superior performance of the proposed approach on both posed and spontaneous facial expression datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "accepted by Pattern Recognition"
    },
    {
        "paper id": "2406.16442",
        "abstract url": "https://arxiv.org/abs/2406.16442",
        "title": "EmoLLM: Multimodal Emotional Understanding Meets Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-modal large language models (MLLMs) have achieved remarkable performance on objective multimodal perception tasks, but their ability to interpret subjective, emotionally nuanced multimodal content remains largely unexplored. Thus, it impedes their ability to effectively understand and react to the intricate emotions expressed by humans through multimodal media. To bridge this gap, we introduce EmoBench, the first comprehensive benchmark designed specifically to evaluate the emotional capabilities of MLLMs across five popular emotional tasks, using a diverse dataset of 287k images and videos paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a novel model for multimodal emotional understanding, incorporating with two core techniques. 1) Multi-perspective Visual Projection, it captures diverse emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it guides MLLMs to reason about emotions in the correct direction. Experimental results demonstrate that EmoLLM significantly elevates multimodal emotional understanding performance, with an average improvement of 12.1% across multiple foundation models on EmoBench. Our work contributes to the advancement of MLLMs by facilitating a deeper and more nuanced comprehension of intricate human emotions, paving the way for the development of artificial emotional intelligence capabilities with wide-ranging applications in areas such as human-computer interaction, mental health support, and empathetic AI systems. Code, data, and model will be released.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2406.16455",
        "abstract url": "https://arxiv.org/abs/2406.16455",
        "title": "Guardrails for avoiding harmful medical product recommendations and off-label promotion in generative AI models",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "health"
            ],
            [
                "cs.AI"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Generative AI (GenAI) models have demonstrated remarkable capabilities in a wide variety of medical tasks. However, as these models are trained using generalist datasets with very limited human oversight, they can learn uses of medical products that have not been adequately evaluated for safety and efficacy, nor approved by regulatory agencies. Given the scale at which GenAI may reach users, unvetted recommendations pose a public health risk. In this work, we propose an approach to identify potentially harmful product recommendations, and demonstrate it using a recent multimodal large language model.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "CVPR 2024 Responsible Generative AI (ReGenAI) workshop"
    },
    {
        "paper id": "2406.16459",
        "abstract url": "https://arxiv.org/abs/2406.16459",
        "title": "Suppressing Uncertainties in Degradation Estimation for Blind Super-Resolution",
        "rating": "-1",
        "keywords": [
            [
                "Depth"
            ],
            [
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The problem of blind image super-resolution aims to recover high-resolution (HR) images from low-resolution (LR) images with unknown degradation modes. Most existing methods model the image degradation process using blur kernels. However, this explicit modeling approach struggles to cover the complex and varied degradation processes encountered in the real world, such as high-order combinations of JPEG compression, blur, and noise. Implicit modeling for the degradation process can effectively overcome this issue, but a key challenge of implicit modeling is the lack of accurate ground truth labels for the degradation process to conduct supervised training. To overcome this limitations inherent in implicit modeling, we propose an \\textbf{U}ncertainty-based degradation representation for blind \\textbf{S}uper-\\textbf{R}esolution framework (\\textbf{USR}). By suppressing the uncertainty of local degradation representations in images, USR facilitated self-supervised learning of degradation representations. The USR consists of two components: Adaptive Uncertainty-Aware Degradation Extraction (AUDE) and a feature extraction network composed of Variable Depth Dynamic Convolution (VDDC) blocks. To extract Uncertainty-based Degradation Representation from LR images, the AUDE utilizes the Self-supervised Uncertainty Contrast module with Uncertainty Suppression Loss to suppress the inherent model uncertainty of the Degradation Extractor. Furthermore, VDDC block integrates degradation information through dynamic convolution. Rhe VDDC also employs an Adaptive Intensity Scaling operation that adaptively adjusts the degradation representation according to the network hierarchy, thereby facilitating the effective integration of degradation information. Quantitative and qualitative experiments affirm the superiority of our approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16473",
        "abstract url": "https://arxiv.org/abs/2406.16473",
        "title": "Seeking Certainty In Uncertainty: Dual-Stage Unified Framework Solving Uncertainty in Dynamic Facial Expression Recognition",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The contemporary state-of-the-art of Dynamic Facial Expression Recognition (DFER) technology facilitates remarkable progress by deriving emotional mappings of facial expressions from video content, underpinned by training on voluminous datasets. Yet, the DFER datasets encompass a substantial volume of noise data. Noise arises from low-quality captures that defy logical labeling, and instances that suffer from mislabeling due to annotation bias, engendering two principal types of uncertainty: the uncertainty regarding data usability and the uncertainty concerning label reliability. Addressing the two types of uncertainty, we have meticulously crafted a two-stage framework aiming at \\textbf{S}eeking \\textbf{C}ertain data \\textbf{I}n extensive \\textbf{U}ncertain data (SCIU). This initiative aims to purge the DFER datasets of these uncertainties, thereby ensuring that only clean, verified data is employed in training processes. To mitigate the issue of low-quality samples, we introduce the Coarse-Grained Pruning (CGP) stage, which assesses sample weights and prunes those deemed unusable due to their low weight. For samples with incorrect annotations, the Fine-Grained Correction (FGC) stage evaluates prediction stability to rectify mislabeled data. Moreover, SCIU is conceived as a universally compatible, plug-and-play framework, tailored to integrate seamlessly with prevailing DFER methodologies. Rigorous experiments across prevalent DFER datasets and against numerous benchmark methods substantiates SCIU's capacity to markedly elevate performance metrics.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16487",
        "abstract url": "https://arxiv.org/abs/2406.16487",
        "title": "Decomposing God Header File via Multi-View Graph Clustering",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "God Header File refers to a header file with large code size and wide file impact. Such files pose difficulties in code comprehension and slow down compilation, ultimately increasing the maintenance cost during software evolution. Although this concept is similar to God Class, existing refactoring methods for God Classes are inappropriate for God Header Files. The reason lies in the fact that the code elements in header files are mostly short declaration types, and build dependencies of the entire system should be considered with the aim of improving compilation efficiency. Meanwhile, these methods overlook the concern of cyclic dependencies, which holds immense importance in the God Header File decomposition. To address these challenges, this paper proposes a God Header File decomposing approach based on multi-view graph clustering. It first constructs a code element graph with multiple relationships. Then after coarsening the graph, a novel multi-view graph clustering algorithm is applied to identify clusters of closely related code elements, and a heuristic algorithm is introduced to address the cyclic dependencies in the clustering result. We evaluate our approach on a synthetic dataset as well as six real-world God Header Files from different projects. The results show that our approach could achieve 11.5% higher accuracy in comparison to existing God Class refactoring methods. Moreover, our decomposition results attain better modularity on all the real-world God Header Files and reduce recompilation time for historical commits by 15% to 60%.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Be accepted by ICSME 2024"
    },
    {
        "paper id": "2406.16495",
        "abstract url": "https://arxiv.org/abs/2406.16495",
        "title": "OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser",
        "rating": "-1",
        "keywords": [
            [
                "biomimetic"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent research has shown that combining Mamba with Transformer architecture, which has selective state space and quadratic self-attention mechanism, outperforms using Mamba or Transformer architecture alone in language modeling tasks. The quadratic self-attention mechanism effectively alleviates the shortcomings of selective state space in handling long-term dependencies of any element in the sequence. We propose a position information injection method that connects the selective state space model with the quadratic attention, and integrates these two architectures with hybrid experts with cross-sharing domains, so that we can enjoy the advantages of both. We design a new architecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser (OTCE), which can compete with well-known medium-scale open-source language models on a small scale in language modeling tasks.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16502",
        "abstract url": "https://arxiv.org/abs/2406.16502",
        "title": "LOGCAN++: Local-global class-aware network for semantic segmentation of remote sensing images",
        "rating": "-1",
        "keywords": [
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Remote sensing images usually characterized by complex backgrounds, scale and orientation variations, and large intra-class variance. General semantic segmentation methods usually fail to fully investigate the above issues, and thus their performances on remote sensing image segmentation are limited. In this paper, we propose our LOGCAN++, a semantic segmentation model customized for remote sensing images, which is made up of a Global Class Awareness (GCA) module and several Local Class Awareness (LCA) modules. The GCA module captures global representations for class-level context modeling to reduce the interference of background noise. The LCA module generates local class representations as intermediate perceptual elements to indirectly associate pixels with the global class representations, targeting at dealing with the large intra-class variance problem. In particular, we introduce affine transformations in the LCA module for adaptive extraction of local class representations to effectively tolerate scale and orientation variations in remotely sensed images. Extensive experiments on three benchmark datasets show that our LOGCAN++ outperforms current mainstream general and remote sensing semantic segmentation methods and achieves a better trade-off between speed and accuracy. Code is available at https://github.com/xwmaxwma/rssegmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2406.16513",
        "abstract url": "https://arxiv.org/abs/2406.16513",
        "title": "Multi-Modal Vision Transformers for Crop Mapping from Satellite Image Time Series",
        "rating": "-1",
        "keywords": [
            [
                "Satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Using images acquired by different satellite sensors has shown to improve classification performance in the framework of crop mapping from satellite image time series (SITS). Existing state-of-the-art architectures use self-attention mechanisms to process the temporal dimension and convolutions for the spatial dimension of SITS. Motivated by the success of purely attention-based architectures in crop mapping from single-modal SITS, we introduce several multi-modal multi-temporal transformer-based architectures. Specifically, we investigate the effectiveness of Early Fusion, Cross Attention Fusion and Synchronized Class Token Fusion within the Temporo-Spatial Vision Transformer (TSViT). Experimental results demonstrate significant improvements over state-of-the-art architectures with both convolutional and self-attention components.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 2 figures, 1 table. Accepted at IEEE International Geoscience and Remote Sensing Symposium (IGARSS) 2024. Our code is available at https://git.tu-berlin.de/rsim/mmtsvit"
    },
    {
        "paper id": "2406.16563",
        "abstract url": "https://arxiv.org/abs/2406.16563",
        "title": "Are there identifiable structural parts in the sentence embedding whole?",
        "rating": "-1",
        "keywords": [
            [
                "grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Sentence embeddings from transformer models encode in a fixed length vector much linguistic information. We explore the hypothesis that these embeddings consist of overlapping layers of information that can be separated, and on which specific types of information -- such as information about chunks and their structural and semantic properties -- can be detected. We show that this is the case using a dataset consisting of sentences with known chunk structure, and two linguistic intelligence datasets, solving which relies on detecting chunks and their grammatical number, and respectively, their semantic roles, and through analyses of the performance on the tasks and of the internal representations built during learning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "17 pages, 14 figures, 5 tables"
    },
    {
        "paper id": "2406.16567",
        "abstract url": "https://arxiv.org/abs/2406.16567",
        "title": "Data Augmentation of Multi-turn Psychological Dialogue via Knowledge-driven Progressive Thought Prompting",
        "rating": "-1",
        "keywords": [
            [
                "Psychological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Existing dialogue data augmentation (DA) techniques predominantly focus on augmenting utterance-level dialogues, which makes it difficult to take dialogue contextual information into account. The advent of large language models (LLMs) has simplified the implementation of multi-turn dialogues. Due to absence of professional understanding and knowledge, it remains challenging to deliver satisfactory performance in low-resource domain, like psychological dialogue dialogue. DA involves creating new training or prompting data based on the existing data, which help the model better understand and generate psychology-related responses. In this paper, we aim to address the issue of multi-turn dialogue data augmentation for boosted performance in the psychology domain. We propose a knowledge-driven progressive thought prompting method to guide LLM to generate multi-turn psychology-related dialogue. This method integrates a progressive thought generator, a psychology knowledge generator, and a multi-turn dialogue generator. The thought generated by the progressive thought generator serves as a prompt to prevent the generated dialogue from having significant semantic deviations, while the psychology knowledge generator produces psychological knowledge to serve as the dialogue history for the LLM, guiding the dialogue generator to create multi-turn psychological dialogue. To ensure the precision of multi-turn psychological dialogue generation by LLM, a meticulous professional evaluation is required. Extensive experiments conducted on three datasets related to psychological dialogue verify the effectiveness of the proposed method.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16593",
        "abstract url": "https://arxiv.org/abs/2406.16593",
        "title": "Measuring the Recyclability of Electronic Components to Assist Automatic Disassembly and Sorting Waste Printed Circuit Boards",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG",
                "cs.CY",
                "cs.CV"
            ]
        ],
        "abstract": "The waste of electrical and electronic equipment has been increased due to the fast evolution of technology products and competition of many IT sectors. Every year millions of tons of electronic waste are thrown into the environment which causes high consequences for human health. Therefore, it is crucial to control this waste flow using technology, especially using Artificial Intelligence but also reclamation of critical raw materials for new production processes. In this paper, we focused on the measurement of recyclability of waste electronic components (WECs) from waste printed circuit boards (WPCBs) using mathematical innovation model. This innovative approach evaluates both the recyclability and recycling difficulties of WECs, integrating an AI model for improved disassembly and sorting. Assessing the recyclability of individual electronic components present on WPCBs provides insight into the recovery potential of valuable materials and indicates the level of complexity involved in recycling in terms of economic worth and production utility. This novel measurement approach helps AI models in accurately determining the number of classes to be identified and sorted during the automated disassembly of discarded PCBs. It also facilitates the model in iterative training and validation of individual electronic components.",
        "subjects": [
            "cs.CV",
            "cs.CY",
            "cs.LG"
        ],
        "comment": "15 pages, 6 figures"
    },
    {
        "paper id": "2406.16611",
        "abstract url": "https://arxiv.org/abs/2406.16611",
        "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Since the emergence of the Transformer architecture, language model development has increased, driven by their promising potential. However, releasing these models into production requires properly understanding their behavior, particularly in sensitive domains such as medicine. Despite this need, the medical literature still lacks technical assessments of pre-trained language models, which are especially valuable in resource-constrained settings in terms of computational power or limited budget. To address this gap, we provide a comprehensive survey of language models in the medical domain. In addition, we selected a subset of these models for thorough evaluation, focusing on classification and text generation tasks. Our subset encompasses 53 models, ranging from 110 million to 13 billion parameters, spanning the three families of Transformer-based models and from diverse knowledge domains. This study employs a series of approaches for text classification together with zero-shot prompting instead of model training or fine-tuning, which closely resembles the limited resource setting in which many users of language models find themselves. Encouragingly, our findings reveal remarkable performance across various tasks and datasets, underscoring the latent potential of certain models to contain medical knowledge, even without domain specialization. Consequently, our study advocates for further exploration of model applications in medical contexts, particularly in resource-constrained settings. The code is available on https://github.com/anpoc/Language-models-in-medicine.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16641",
        "abstract url": "https://arxiv.org/abs/2406.16641",
        "title": "Vision-Language Consistency Guided Multi-modal Prompt Learning for Blind AI Generated Image Quality Assessment",
        "rating": "-1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "text-to-image"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recently, textual prompt tuning has shown inspirational performance in adapting Contrastive Language-Image Pre-training (CLIP) models to natural image quality assessment. However, such uni-modal prompt learning method only tunes the language branch of CLIP models. This is not enough for adapting CLIP models to AI generated image quality assessment (AGIQA) since AGIs visually differ from natural images. In addition, the consistency between AGIs and user input text prompts, which correlates with the perceptual quality of AGIs, is not investigated to guide AGIQA. In this letter, we propose vision-language consistency guided multi-modal prompt learning for blind AGIQA, dubbed CLIP-AGIQA. Specifically, we introduce learnable textual and visual prompts in language and vision branches of CLIP models, respectively. Moreover, we design a text-to-image alignment quality prediction task, whose learned vision-language consistency knowledge is used to guide the optimization of the above multi-modal prompts. Experimental results on two public AGIQA datasets demonstrate that the proposed method outperforms state-of-the-art quality assessment models. The source code is available at https://github.com/JunFu1995/CLIP-AGIQA.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted by IEEE Signal Processing Letter"
    },
    {
        "paper id": "2406.16647",
        "abstract url": "https://arxiv.org/abs/2406.16647",
        "title": "Delineating Half-Integrality of the Erd\u0151s-P\u00f3sa Property for Minors: the Case of Surfaces",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In 1986 Robertson and Seymour proved a generalization of the seminal result of Erd\u0151s and P\u00f3sa on the duality of packing and covering cycles: A graph has the Erd\u0151s-P\u00f3sa property for minors if and only if it is planar. In particular, for every non-planar graph $H$ they gave examples showing that the Erd\u0151s-P\u00f3sa property does not hold for $H.$ Recently, Liu confirmed a conjecture of Thomas and showed that every graph has the half-integral Erd\u0151s-P\u00f3sa property for minors. Liu's proof is non-constructive and to this date, with the exception of a small number of examples, no constructive proof is known. In this paper, we initiate the delineation of the half-integrality of the Erd\u0151s-P\u00f3sa property for minors. We conjecture that for every graph $H,$ there exists a unique (up to a suitable equivalence relation) graph parameter ${\\textsf{EP}}_H$ such that $H$ has the Erd\u0151s-P\u00f3sa property in a minor-closed graph class $\\mathcal{G}$ if and only if $\\sup\\{\\textsf{EP}_H(G) \\mid G\\in\\mathcal{G}\\}$ is finite. We prove this conjecture for the class $\\mathcal{H}$ of Kuratowski-connected shallow-vortex minors by showing that, for every non-planar $H\\in\\mathcal{H},$ the parameter ${\\sf EP}_H(G)$ is precisely the maximum order of a Robertson-Seymour counterexample to the Erd\u0151s-P\u00f3sa property of $H$ which can be found as a minor in $G.$ Our results are constructive and imply, for the first time, parameterized algorithms that find either a packing, or a cover, or one of the Robertson-Seymour counterexamples, certifying the existence of a half-integral packing for the graphs in $\\mathcal{H}.$",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16653",
        "abstract url": "https://arxiv.org/abs/2406.16653",
        "title": "Consistent Query Answering over SHACL Constraints",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "The Shapes Constraint Language (SHACL) was standardized by the World Wide Web as a constraint language to describe and validate RDF data graphs. SHACL uses the notion of shapes graph to describe a set of shape constraints paired with targets, that specify which nodes of the RDF graph should satisfy which shapes. An important question in practice is how to handle data graphs that do not validate the shapes graph. A solution is to tolerate the non-validation and find ways to obtain meaningful and correct answers to queries despite the non-validation. This is known as consistent query answering (CQA) and there is extensive literature on CQA in both the database and the KR setting. We study CQA in the context of SHACL for a fundamental fragment of the Semantic Web query language SPARQL. The goal of our work is a detailed complexity analysis of CQA for various semantics and possible restrictions on the acceptable repairs. It turns out that all considered variants of the problem are intractable, with complexities ranging between the first and third level of the polynomial hierarchy.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16661",
        "abstract url": "https://arxiv.org/abs/2406.16661",
        "title": "Towards Communication-Efficient Peer-to-Peer Networks",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We focus on designing Peer-to-Peer (P2P) networks that enable efficient communication. Over the last two decades, there has been substantial algorithmic research on distributed protocols for building P2P networks with various desirable properties such as high expansion, low diameter, and robustness to a large number of deletions. A key underlying theme in all of these works is to distributively build a \\emph{random graph} topology that guarantees the above properties. Moreover, the random connectivity topology is widely deployed in many P2P systems today, including those that implement blockchains and cryptocurrencies. However, a major drawback of using a random graph topology for a P2P network is that the random topology does not respect the \\emph{underlying} (Internet) communication topology. This creates a large \\emph{propagation delay}, which is a major communication bottleneck in modern P2P networks. In this paper, we work towards designing P2P networks that are communication-efficient (having small propagation delay) with provable guarantees. Our main contribution is an efficient, decentralized protocol, $\\textsc{Close-Weaver}$, that transforms a random graph topology embedded in an underlying Euclidean space into a topology that also respects the underlying metric. We then present efficient point-to-point routing and broadcast protocols that achieve essentially optimal performance with respect to the underlying space.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "33 pages, 7 figures, full version of paper to appear in ESA 2024"
    },
    {
        "paper id": "2406.16679",
        "abstract url": "https://arxiv.org/abs/2406.16679",
        "title": "Multi-Robot Collaborative Localization and Planning with Inter-Ranging",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Robots often use feature-based image tracking to identify their position in their surrounding environment; however, feature-based image tracking is prone to errors in low-textured and poorly lit environments. Specifically, we investigate a scenario where robots are tasked with exploring the surface of the Moon and are required to have an accurate estimate of their position to be able to correctly geotag scientific measurements. To reduce localization error, we complement traditional feature-based image tracking with ultra-wideband (UWB) distance measurements between the robots. The robots use an advanced mesh-ranging protocol that allows them to continuously share distance measurements amongst each other rather than relying on the common \"anchor\" and \"tag\" UWB architecture. We develop a decentralized multi-robot coordination algorithm that actively plans paths based on measurement line-of-sight vectors amongst all robots to minimize collective localization error. We then demonstrate the emergent behavior of the proposed multi-robot coordination algorithm both in simulation and hardware to lower a geometry-based uncertainty metric and reduce localization error.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2406.16695",
        "abstract url": "https://arxiv.org/abs/2406.16695",
        "title": "Geometry-Aware Score Distillation via 3D Consistent Noising and Gradient Consistency Modeling",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Score distillation sampling (SDS), the methodology in which the score from pretrained 2D diffusion models is distilled into 3D representation, has recently brought significant advancements in text-to-3D generation task. However, this approach is still confronted with critical geometric inconsistency problems such as the Janus problem. Starting from a hypothesis that such inconsistency problems may be induced by multiview inconsistencies between 2D scores predicted from various viewpoints, we introduce GSD, a simple and general plug-and-play framework for incorporating 3D consistency and therefore geometry awareness into the SDS process. Our methodology is composed of three components: 3D consistent noising, designed to produce 3D consistent noise maps that perfectly follow the standard Gaussian distribution, geometry-based gradient warping for identifying correspondences between predicted gradients of different viewpoints, and novel gradient consistency loss to optimize the scene geometry toward producing more consistent gradients. We demonstrate that our method significantly improves performance, successfully addressing the geometric inconsistency problems in text-to-3D generation task with minimal computation cost and being compatible with existing score distillation-based models. Our project page is available at https://ku-cvlab.github.io/GSD/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16701",
        "abstract url": "https://arxiv.org/abs/2406.16701",
        "title": "Demystifying the Effect of Receptive Field Size in U-Net Models for Medical Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "healthcare"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Medical image segmentation is a critical task in healthcare applications, and U-Nets have demonstrated promising results. This work delves into the understudied aspect of receptive field (RF) size and its impact on the U-Net and Attention U-Net architectures. This work explores several critical elements including the relationship between RF size, characteristics of the region of interest, and model performance, as well as the balance between RF size and computational costs for U-Net and Attention U-Net methods for different datasets. This work also proposes a mathematical notation for representing the theoretical receptive field (TRF) of a given layer in a network and proposes two new metrics - effective receptive field (ERF) rate and the Object rate to quantify the fraction of significantly contributing pixels within the ERF against the TRF area and assessing the relative size of the segmentation object compared to the TRF size respectively. The results demonstrate that there exists an optimal TRF size that successfully strikes a balance between capturing a wider global context and maintaining computational efficiency, thereby optimizing model performance. Interestingly, a distinct correlation is observed between the data complexity and the required TRF size; segmentation based solely on contrast achieved peak performance even with smaller TRF sizes, whereas more complex segmentation tasks necessitated larger TRFs. Attention U-Net models consistently outperformed their U-Net counterparts, highlighting the value of attention mechanisms regardless of TRF size. These novel insights present an invaluable resource for developing more efficient U-Net-based architectures for medical imaging and pave the way for future exploration. A tool is also developed that calculates the TRF for a U-Net (and Attention U-Net) model, and also suggest an appropriate TRF size for a given model and dataset.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "physics.med-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16710",
        "abstract url": "https://arxiv.org/abs/2406.16710",
        "title": "Portrait3D: 3D Head Generation from Single In-the-wild Portrait Image",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "diffusion",
                "GAN",
                "Inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "While recent works have achieved great success on one-shot 3D common object generation, high quality and fidelity 3D head generation from a single image remains a great challenge. Previous text-based methods for generating 3D heads were limited by text descriptions and image-based methods struggled to produce high-quality head geometry. To handle this challenging problem, we propose a novel framework, Portrait3D, to generate high-quality 3D heads while preserving their identities. Our work incorporates the identity information of the portrait image into three parts: 1) geometry initialization, 2) geometry sculpting, and 3) texture generation stages. Given a reference portrait image, we first align the identity features with text features to realize ID-aware guidance enhancement, which contains the control signals representing the face information. We then use the canny map, ID features of the portrait image, and a pre-trained text-to-normal/depth diffusion model to generate ID-aware geometry supervision, and 3D-GAN inversion is employed to generate ID-aware geometry initialization. Furthermore, with the ability to inject identity information into 3D head generation, we use ID-aware guidance to calculate ID-aware Score Distillation (ISD) for geometry sculpting. For texture generation, we adopt the ID Consistent Texture Inpainting and Refinement which progressively expands the view for texture inpainting to obtain an initialization UV texture map. We then use the id-aware guidance to provide image-level supervision for noisy multi-view images to obtain a refined texture map. Extensive experiments demonstrate that we can generate high-quality 3D heads with accurate geometry and texture from single in-the-wild portrait images. The project page is at https://jinkun-hao.github.io/Portrait3D/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "https://jinkun-hao.github.io/Portrait3D/"
    },
    {
        "paper id": "2406.16737",
        "abstract url": "https://arxiv.org/abs/2406.16737",
        "title": "A Digital Human Model for Symptom Progression of Vestibular Motion Sickness based on Subjective Vertical Conflict Theory",
        "rating": "-1",
        "keywords": [
            [
                "6DoF"
            ]
        ],
        "abstract": "Digital human models of motion sickness have been actively developed, among which models based on subjective vertical conflict (SVC) theory are the most actively studied. These models facilitate the prediction of motion sickness in various scenarios such as riding in a car. Most SVC theory models predict the motion sickness incidence (MSI), which is defined as the percentage of people who would vomit with the given specific motion stimulus. However, no model has been developed to describe milder forms of discomfort or specific symptoms of motion sickness, even though predicting milder symptoms is important for applications in automobiles and daily use vehicles. Therefore, the purpose of this study was to build a computational model of symptom progression of vestibular motion sickness based on SVC theory. We focused on a model of vestibular motion sickness with six degrees-of-freedom (6DoF) head motions. The model was developed by updating the output part of the state-of-the-art SVC model, termed the 6DoF-SVC (IN1) model, from MSI to the MIsery SCale (MISC), which is a subjective rating scale for symptom progression. We conducted an experiment to measure the progression of motion sickness during a straight fore-aft motion. It was demonstrated that our proposed method, with the parameters of the output parts optimized by the experimental results, fits well with the observed MISC.",
        "subjects": [
            "cs.HC",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16754",
        "abstract url": "https://arxiv.org/abs/2406.16754",
        "title": "The MRI Scanner as a Diagnostic: Image-less Active Sampling",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "MRI",
                "disease"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Despite the high diagnostic accuracy of Magnetic Resonance Imaging (MRI), using MRI as a Point-of-Care (POC) disease identification tool poses significant accessibility challenges due to the use of high magnetic field strength and lengthy acquisition times. We ask a simple question: Can we dynamically optimise acquired samples, at the patient level, according to an (automated) downstream decision task, while discounting image reconstruction? We propose an ML-based framework that learns an active sampling strategy, via reinforcement learning, at a patient-level to directly infer disease from undersampled k-space. We validate our approach by inferring Meniscus Tear in undersampled knee MRI data, where we achieve diagnostic performance comparable with ML-based diagnosis, using fully sampled k-space data. We analyse task-specific sampling policies, showcasing the adaptability of our active sampling approach. The introduced frugal sampling strategies have the potential to reduce high field strength requirements that in turn strengthen the viability of MRI-based POC disease identification and associated preliminary screening tools.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "eess.IV"
        ],
        "comment": "Accepted in MICCAI 2024"
    },
    {
        "paper id": "2406.16810",
        "abstract url": "https://arxiv.org/abs/2406.16810",
        "title": "PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs",
        "rating": "-1",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recently, machine unlearning, which seeks to erase specific data stored in the pre-trained or fine-tuned models, has emerged as a crucial protective measure for LLMs. However, unlearning approaches for LLMs that have been considered thus far have focused on the removal of independent data points and have not taken into account that the stored facts are logically connected to one another and form an implicit knowledge graph. To facilitate the development of structural unlearning methods, which are essential for the practical application of unlearning, we propose PISTOL, a pipeline for compiling multi-scenario datasets for benchmarking structural LLM unlearning. Additionally, leveraging sample datasets synthesized using PISTOL, we conducted benchmarks with four distinct unlearning methods on both Llama2-7B and Mistral-7B models. This analysis helps to illustrate the prevailing challenges in effectively and robustly removing highly inter-connected data, batched data, or data skewed towards a specific domain. It also highlights the choice of pre-trained model can impact unlearning performance. This work not only advances our understandings on the limitation of current LLMs unlearning methods and proposes future research directions, but also provides a replicable framework for ongoing exploration and validation in the field.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16812",
        "abstract url": "https://arxiv.org/abs/2406.16812",
        "title": "FlipDyn in Graphs: Resource Takeover Games in Graphs",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "We present \\texttt{FlipDyn-G}, a dynamic game model extending the \\texttt{FlipDyn} framework to a graph-based setting, where each node represents a dynamical system. This model captures the interactions between a defender and an adversary who strategically take over nodes in a graph to minimize (resp. maximize) a finite horizon additive cost. At any time, the \\texttt{FlipDyn} state is represented as the current node, and each player can transition the \\texttt{FlipDyn} state to a depending based on the connectivity from the current node. Such transitions are driven by the node dynamics, state, and node-dependent costs. This model results in a hybrid dynamical system where the discrete state (\\texttt{FlipDyn} state) governs the continuous state evolution and the corresponding state cost. Our objective is to compute the Nash equilibrium of this finite horizon zero-sum game on a graph. Our contributions are two-fold. First, we model and characterize the \\texttt{FlipDyn-G} game for general dynamical systems, along with the corresponding Nash equilibrium (NE) takeover strategies. Second, for scalar linear discrete-time dynamical systems with quadratic costs, we derive the NE takeover strategies and saddle-point values independent of the continuous state of the system. Additionally, for a finite state birth-death Markov chain (represented as a graph) under scalar linear dynamical systems, we derive analytical expressions for the NE takeover strategies and saddle-point values. We illustrate our findings through numerical studies involving epidemic models and linear dynamical systems with adversarial interactions.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "25 pages, 8 figures, submitted to GameSec 2024"
    },
    {
        "paper id": "2406.16815",
        "abstract url": "https://arxiv.org/abs/2406.16815",
        "title": "ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "RGBD",
                "avatar"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "High-fidelity 3D garment synthesis from text is desirable yet challenging for digital avatar creation. Recent diffusion-based approaches via Score Distillation Sampling (SDS) have enabled new possibilities but either intricately couple with human body or struggle to reuse. We introduce ClotheDreamer, a 3D Gaussian-based method for generating wearable, production-ready 3D garment assets from text prompts. We propose a novel representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate optimization. DCGS represents clothed avatar as one Gaussian model but freezes body Gaussian splats. To enhance quality and completeness, we incorporate bidirectional SDS to supervise clothed avatar and garment RGBD renderings respectively with pose conditions and propose a new pruning strategy for loose clothing. Our approach can also support custom clothing templates as input. Benefiting from our design, the synthetic 3D garment can be easily applied to virtual try-on and support physically accurate animation. Extensive experiments showcase our method's superior and competitive performance. Our project page is at https://ggxxii.github.io/clothedreamer.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://ggxxii.github.io/clothedreamer"
    },
    {
        "paper id": "2406.16822",
        "abstract url": "https://arxiv.org/abs/2406.16822",
        "title": "A Multi-Party, Multi-Blockchain Atomic Swap Protocol with Universal Adaptor Secret",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "The increasing complexity of digital asset transactions across multiple blockchains necessitates a robust atomic swap protocol that can securely handle more than two participants. Traditional atomic swap protocols, including those based on adaptor signatures, are vulnerable to malicious dropout attacks, which break atomicity and compromise the security of the transaction. This paper presents a novel multi-party atomic swap protocol that operates almost entirely off-chain, requiring only a single on-chain transaction for finalization. Our protocol leverages Schnorr-like signature verification and a universal adaptor secret to ensure atomicity and scalability across any number of participants and blockchains without the need for smart contracts or trusted third parties. By addressing key challenges such as collusion attacks and malicious dropouts, our protocol significantly enhances the security and efficiency of multi-party atomic swaps. Our contributions include the first scalable, fully off-chain protocol for atomic swaps involving any number of participants, adding zero overhead to native blockchains, and providing a practical and cost-effective solution for decentralized asset exchanges.",
        "subjects": [
            "cs.CR",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16828",
        "abstract url": "https://arxiv.org/abs/2406.16828",
        "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Did you try out the new Bing Search? Or maybe you fiddled around with Google AI~Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality -- we describe the details of our reusable framework, Ragnar\u00f6k, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\u00f6k, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\u00f6k framework and baselines to achieve a unified standard for future RAG systems.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16845",
        "abstract url": "https://arxiv.org/abs/2406.16845",
        "title": "RaTEScore: A Metric for Radiology Report Generation",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "clinical",
                "Radiology"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces a novel, entity-aware metric, termed as Radiological Report (Text) Evaluation (RaTEScore), to assess the quality of medical reports generated by AI models. RaTEScore emphasizes crucial medical entities such as diagnostic outcomes and anatomical details, and is robust against complex medical synonyms and sensitive to negation expressions. Technically, we developed a comprehensive medical NER dataset, RaTE-NER, and trained an NER model specifically for this purpose. This model enables the decomposition of complex radiological reports into constituent medical entities. The metric itself is derived by comparing the similarity of entity embeddings, obtained from a language model, based on their types and relevance to clinical significance. Our evaluations demonstrate that RaTEScore aligns more closely with human preference than existing metrics, validated both on established public benchmarks and our newly proposed RaTE-Eval benchmark.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16848",
        "abstract url": "https://arxiv.org/abs/2406.16848",
        "title": "Unsupervised Domain Adaptation for Pediatric Brain Tumor Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "clinical",
                "Tumor"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Significant advances have been made toward building accurate automatic segmentation models for adult gliomas. However, the performance of these models often degrades when applied to pediatric glioma due to their imaging and clinical differences (domain shift). Obtaining sufficient annotated data for pediatric glioma is typically difficult because of its rare nature. Also, manual annotations are scarce and expensive. In this work, we propose Domain-Adapted nnU-Net (DA-nnUNet) to perform unsupervised domain adaptation from adult glioma (source domain) to pediatric glioma (target domain). Specifically, we add a domain classifier connected with a gradient reversal layer (GRL) to a backbone nnU-Net. Once the classifier reaches a very high accuracy, the GRL is activated with the goal of transferring domain-invariant features from the classifier to the segmentation model while preserving segmentation accuracy on the source domain. The accuracy of the classifier slowly degrades to chance levels. No annotations are used in the target domain. The method is compared to 8 different supervised models using BraTS-Adult glioma (N=1251) and BraTS-PED glioma data (N=99). The proposed method shows notable performance enhancements in the tumor core (TC) region compared to the model that only uses adult data: ~32% better Dice scores and ~20 better 95th percentile Hausdorff distances. Moreover, our unsupervised approach shows no statistically significant difference compared to the practical upper bound model using manual annotations from both datasets in TC region. The code is shared at https://github.com/Fjr9516/DA_nnUNet.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "10 pages, 4 figures, conference"
    },
    {
        "paper id": "2406.16853",
        "abstract url": "https://arxiv.org/abs/2406.16853",
        "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
        "rating": "-1",
        "keywords": [
            [
                "quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Molecular modeling, a central topic in quantum mechanics, aims to accurately calculate the properties and simulate the behaviors of molecular systems. The molecular model is governed by physical laws, which impose geometric constraints such as invariance and equivariance to coordinate rotation and translation. While numerous deep learning approaches have been developed to learn molecular representations under these constraints, most of them are built upon heuristic and costly modules. We argue that there is a strong need for a general and flexible framework for learning both invariant and equivariant features. In this work, we introduce a novel Transformer-based molecular model called GeoMFormer to achieve this goal. Using the standard Transformer modules, two separate streams are developed to maintain and learn invariant and equivariant representations. Carefully designed cross-attention modules bridge the two streams, allowing information fusion and enhancing geometric modeling in each stream. As a general and flexible architecture, we show that many previous architectures can be viewed as special instantiations of GeoMFormer. Extensive experiments are conducted to demonstrate the power of GeoMFormer. All empirical results show that GeoMFormer achieves strong performance on both invariant and equivariant tasks of different types and scales. Code and models will be made publicly available at https://github.com/c-tl/GeoMFormer.",
        "subjects": [
            "cs.LG",
            "cond-mat.mtrl-sci",
            "cs.AI",
            "q-bio.BM"
        ],
        "comment": "25 pages, 13 tables, l figure; ICML 2024 camera ready version"
    },
    {
        "paper id": "2406.16862",
        "abstract url": "https://arxiv.org/abs/2406.16862",
        "title": "Dreamitate: Real-World Visuomotor Policy Learning via Video Generation",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "A key challenge in manipulation is learning a policy that can robustly generalize to diverse visual environments. A promising mechanism for learning robust policies is to leverage video generative models, which are pretrained on large-scale datasets of internet videos. In this paper, we propose a visuomotor policy learning framework that fine-tunes a video diffusion model on human demonstrations of a given task. At test time, we generate an example of an execution of the task conditioned on images of a novel scene, and use this synthesized execution directly to control the robot. Our key insight is that using common tools allows us to effortlessly bridge the embodiment gap between the human hand and the robot manipulator. We evaluate our approach on four tasks of increasing complexity and demonstrate that harnessing internet-scale generative models allows the learned policy to achieve a significantly higher degree of generalization than existing behavior cloning approaches.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "Project page: https://dreamitate.cs.columbia.edu/"
    },
    {
        "paper id": "2406.16863",
        "abstract url": "https://arxiv.org/abs/2406.16863",
        "title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion model has demonstrated remarkable capability in video generation, which further sparks interest in introducing trajectory control into the generation process. While existing works mainly focus on training-based methods (e.g., conditional adapter), we argue that diffusion model itself allows decent control over the generated content without requiring any training. In this study, we introduce a tuning-free framework to achieve trajectory-controllable video generation, by imposing guidance on both noise construction and attention computation. Specifically, 1) we first show several instructive phenomenons and analyze how initial noises influence the motion trajectory of generated content. 2) Subsequently, we propose FreeTraj, a tuning-free approach that enables trajectory control by modifying noise sampling and attention mechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger video generation with controllable trajectories. Equipped with these designs, users have the flexibility to provide trajectories manually or opt for trajectories automatically generated by the LLM trajectory planner. Extensive experiments validate the efficacy of our approach in enhancing the trajectory controllability of video diffusion models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: http://haonanqiu.com/projects/FreeTraj.html, Code Repo: https://github.com/arthur-qiu/FreeTraj"
    },
    {
        "paper id": "2406.17032",
        "abstract url": "https://arxiv.org/abs/2406.17032",
        "title": "Dwarf: Disease-weighted network for attention map refinement",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "healthcare",
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The interpretability of deep learning is crucial for evaluating the reliability of medical imaging models and reducing the risks of inaccurate patient recommendations. This study addresses the \"human out of the loop\" and \"trustworthiness\" issues in medical image analysis by integrating medical professionals into the interpretability process. We propose a disease-weighted attention map refinement network (Dwarf) that leverages expert feedback to enhance model relevance and accuracy. Our method employs cyclic training to iteratively improve diagnostic performance, generating precise and interpretable feature maps. Experimental results demonstrate significant improvements in interpretability and diagnostic accuracy across multiple medical imaging datasets. This approach fosters effective collaboration between AI systems and healthcare professionals, ultimately aiming to improve patient outcomes",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17038",
        "abstract url": "https://arxiv.org/abs/2406.17038",
        "title": "modeLing: A Novel Dataset for Testing Linguistic Reasoning in Language Models",
        "rating": "-1",
        "keywords": [
            [
                "grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce modeLing, a novel benchmark of Linguistics Olympiad-style puzzles which tests few-shot reasoning in AI systems. Solving these puzzles necessitates inferring aspects of a language's grammatical structure from a small number of examples. Such puzzles provide a natural testbed for language models, as they require compositional generalization and few-shot inductive reasoning. Consisting solely of new puzzles written specifically for this work, modeLing has no risk of appearing in the training data of existing AI systems: this ameliorates the risk of data leakage, a potential confounder for many prior evaluations of reasoning. Evaluating several large open source language models and GPT on our benchmark, we observe non-negligible accuracy, demonstrating few-shot emergent reasoning ability which cannot merely be attributed to shallow memorization. However, imperfect model performance suggests that modeLing can be used to measure further progress in linguistic reasoning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17051",
        "abstract url": "https://arxiv.org/abs/2406.17051",
        "title": "Leveraging Knowledge Distillation for Lightweight Skin Cancer Classification: Balancing Accuracy and Computational Efficiency",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "Cancer"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Skin cancer is a major concern to public health, accounting for one-third of the reported cancers. If not detected early, the cancer has the potential for severe consequences. Recognizing the critical need for effective skin cancer classification, we address the limitations of existing models, which are often too large to deploy in areas with limited computational resources. In response, we present a knowledge distillation based approach for creating a lightweight yet high-performing classifier. The proposed solution involves fusing three models, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective teacher model. The teacher model is then employed to guide a lightweight student model of size 2.03 MB. This student model is further compressed to 469.77 KB using 16-bit quantization, enabling smooth incorporation into edge devices. With six-stage image preprocessing, data augmentation, and a rigorous ablation study, the model achieves an impressive accuracy of 98.75% on the HAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and malignant skin cancers. With its high accuracy and compact size, our model appears to be a potential choice for accurate skin cancer classification, particularly in resource-constrained settings.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17055",
        "abstract url": "https://arxiv.org/abs/2406.17055",
        "title": "Large Language Models Assume People are More Rational than We Really are",
        "rating": "-1",
        "keywords": [
            [
                "psychological"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate -- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice -- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17089",
        "abstract url": "https://arxiv.org/abs/2406.17089",
        "title": "Spectral conditions of pancyclicity for t-tough graphs",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "More than 40 years ago Chv\u00e1tal introduced a new graph invariant, which he called graph toughness. From then on a lot of research has been conducted, mainly related to the relationship between toughness conditions and the existence of cyclic structures, in particular, determining whether the graph is Hamiltonian and pancyclic. A pancyclic graph is certainly Hamiltonian, but not conversely. Bondy in 1976, however, suggested the \"metaconjecture\" that almost any nontrivial condition on a graph which implies that the graph is Hamiltonian also implies that the graph is pancyclic. We confirm the Bondy conjecture for t-tough graphs in the case when $t\\in \\{ 1;2;3\\}$ in terms of the edge number, the spectral radius and the signless Laplacian spectral radius of the graph.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "8 pages, 3 figures"
    },
    {
        "paper id": "2406.17109",
        "abstract url": "https://arxiv.org/abs/2406.17109",
        "title": "GMT: Guided Mask Transformer for Leaf Instance Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "biology-related"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Leaf instance segmentation is a challenging multi-instance segmentation task, aiming to separate and delineate each leaf in an image of a plant. The delineation of each leaf is a necessary prerequisite task for several biology-related applications such as the fine-grained monitoring of plant growth, and crop yield estimation. The task is challenging because self-similarity of instances is high (similar shape and colour) and instances vary greatly in size under heavy occulusion. We believe that the key to overcoming the aforementioned challenges lies in the specific spatial patterns of leaf distribution. For example, leaves typically grow around the plant's center, with smaller leaves clustering and overlapped near this central point. In this paper, we propose a novel approach named Guided Mask Transformer (GMT), which contains three key components, namely Guided Positional Encoding (GPE), Guided Embedding Fusion Module (GEFM) and Guided Dynamic Positional Queries (GDPQ), to extend the meta-architecture of Mask2Former and incorporate with a set of harmonic guide functions. These guide functions are tailored to the pixel positions of instances and trained to separate distinct instances in an embedding space. The proposed GMT consistently outperforms State-of-the-Art models on three public plant datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17111",
        "abstract url": "https://arxiv.org/abs/2406.17111",
        "title": "Sound Field Synthesis with Acoustic Waves",
        "rating": "-1",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "We propose a practical framework to synthesize broadband sound-field on a small rigid surface based on the physics of sound propagation. The sound-field is generated as a composite map of two components: room component and device component; with acoustic plane waves as core element. This decoupling of room and device components significantly reduces the problem complexity and provide accurate rendering of the sound-field. We describe in details the theoretical foundations, efficient procedures, and engineering applications of the proposed framework. The effectiveness of the proposed framework is established through rigorous validation under different environment setups.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "16 pages, double-spaced, 5 figures"
    },
    {
        "paper id": "2406.17119",
        "abstract url": "https://arxiv.org/abs/2406.17119",
        "title": "Accelerating Phase Field Simulations Through a Hybrid Adaptive Fourier Neural Operator with U-Net Backbone",
        "rating": "-1",
        "keywords": [
            [
                "alloys"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Prolonged contact between a corrosive liquid and metal alloys can cause progressive dealloying. For such liquid-metal dealloying (LMD) process, phase field models have been developed. However, the governing equations often involve coupled non-linear partial differential equations (PDE), which are challenging to solve numerically. In particular, stiffness in the PDEs requires an extremely small time steps (e.g. $10^{-12}$ or smaller). This computational bottleneck is especially problematic when running LMD simulation until a late time horizon is required. This motivates the development of surrogate models capable of leaping forward in time, by skipping several consecutive time steps at-once. In this paper, we propose U-Shaped Adaptive Fourier Neural Operators (U-AFNO), a machine learning (ML) model inspired by recent advances in neural operator learning. U-AFNO employs U-Nets for extracting and reconstructing local features within the physical fields, and passes the latent space through a vision transformer (ViT) implemented in the Fourier space (AFNO). We use U-AFNOs to learn the dynamics mapping the field at a current time step into a later time step. We also identify global quantities of interest (QoI) describing the corrosion process (e.g. the deformation of the liquid-metal interface) and show that our proposed U-AFNO model is able to accurately predict the field dynamics, in-spite of the chaotic nature of LMD. Our model reproduces the key micro-structure statistics and QoIs with a level of accuracy on-par with the high-fidelity numerical solver. We also investigate the opportunity of using hybrid simulations, in which we alternate forward leap in time using the U-AFNO with high-fidelity time stepping. We demonstrate that while advantageous for some surrogate model design choices, our proposed U-AFNO model in fully auto-regressive settings consistently outperforms hybrid schemes.",
        "subjects": [
            "cs.CE",
            "cs.CV",
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17136",
        "abstract url": "https://arxiv.org/abs/2406.17136",
        "title": "Stable Tool-Use with Flexible Musculoskeletal Hands by Learning the Predictive Model of Sensor State Transition",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "The flexible under-actuated musculoskeletal hand is superior in its adaptability and impact resistance. On the other hand, since the relationship between sensors and actuators cannot be uniquely determined, almost all its controls are based on feedforward controls. When grasping and using a tool, the contact state of the hand gradually changes due to the inertia of the tool or impact of action, and the initial contact state is hardly kept. In this study, we propose a system that trains the predictive network of sensor state transition using the actual robot sensor information, and keeps the initial contact state by a feedback control using the network. We conduct experiments of hammer hitting, vacuuming, and brooming, and verify the effectiveness of this study.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at ICRA2020"
    },
    {
        "paper id": "2406.17153",
        "abstract url": "https://arxiv.org/abs/2406.17153",
        "title": "Computing User Equilibria for Schedule-Based Transit Networks with Hard Vehicle Capacities",
        "rating": "-1",
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "Modelling passenger assignments in public transport networks is a fundamental task for city planners, especially when deliberating network infrastructure decisions. A key aspect of a realistic model for passenger assignments is to integrate selfish routing behaviour of passengers on the one hand, and the limited vehicle capacities on the other hand. We formulate a side-constrained user equilibrium model in a schedule-based time-expanded transit network, where passengers are modelled via a continuum of non-atomic agents that want to travel with a fixed start time from a user-specific origin to a destination. An agent's route may comprise several rides along given lines, each using vehicles with hard loading capacities. We give a characterization of (side-constrained) user equilibria via a quasi-variational inequality and prove their existence by generalizing a well-known existence result of Bernstein and Smith (Transp. Sci., 1994). We further derive a polynomial time algorithm for single-commodity instances and an exact finite time algorithm for the multi-commodity case. Based on our quasi-variational characterization, we finally devise a fast heuristic computing user equilibria, which is tested on real-world instances based on data gained from the Hamburg S-Bahn system and the Swiss long-distance train network. It turns out that w.r.t. the total travel time, the computed user-equilibria are quite efficient compared to a system optimum, which neglects equilibrium constraints and only minimizes total travel time.",
        "subjects": [
            "cs.GT",
            "math.OC"
        ],
        "comment": "32 pages, 9 figures"
    },
    {
        "paper id": "2406.17159",
        "abstract url": "https://arxiv.org/abs/2406.17159",
        "title": "Exploring compressibility of transformer based text-to-music (TTM) models",
        "rating": "-1",
        "keywords": [
            [
                "music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "State-of-the art Text-To-Music (TTM) generative AI models are large and require desktop or server class compute, making them infeasible for deployment on mobile phones. This paper presents an analysis of trade-offs between model compression and generation performance of TTM models. We study compression through knowledge distillation and specific modifications that enable applicability over the various components of the TTM model (encoder, generative model and the decoder). Leveraging these methods we create TinyTTM (89.2M params) that achieves a FAD of 3.66 and KL of 1.32 on MusicBench dataset, better than MusicGen-Small (557.6M params) but not lower than MusicGen-small fine-tuned on MusicBench.",
        "subjects": [
            "eess.AS",
            "cs.MM",
            "cs.SD"
        ],
        "comment": "Proceedings of INTERSPEECH 2024"
    },
    {
        "paper id": "2406.17189",
        "abstract url": "https://arxiv.org/abs/2406.17189",
        "title": "Hierarchical Framework for Optimizing Wildfire Surveillance and Suppression using Human-Autonomous Teaming",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "The integration of manned and unmanned aircraft can help improve wildfire response. Wildfire containment failures occur when resources available to first responders, who execute the initial stages of wildfire management referred to as the initial attack, are ineffective or insufficient. Initial attack surveillance and suppression models have linked action spaces and objectives, making their optimization computationally challenging. The initial attack may be formulated as a multi-agent partially observable Markov decision process (MPOMDP). We divide the initial attack MPOMDP into surveillance and suppression processes with their respective planners operating on different, but constant, time scales. A hierarchical framework iterates between surveillance and suppression planners while also providing collision avoidance. This framework is exemplified by a set of multi-rotor unmanned aircraft surveying an initial attack fire while a manned helicopter suppresses the fire with a water bucket. Wildfire-specific solver extensions are formulated to reduce the otherwise vast action spaces. The hierarchical framework outperforms firefighting techniques and a myopic baseline by up to 242% for moderate wildfires and 60% for rapid wildfires when simulated in abstracted and actual case studies. We also validate the early dispatching of additional suppression assets using regression models to ensure wildfire containment to thresholds established by wildfire agencies.",
        "subjects": [
            "eess.SY",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17207",
        "abstract url": "https://arxiv.org/abs/2406.17207",
        "title": "VR-based Blockchain-enabled Data Visualization Framework For Manufacturing Industry",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This research proposes a blockchain-based data visualization framework integrated with VR to get manufacturing insights. This framework is implemented at the testbed of the Future Factories Lab at the University of South Carolina. The proposed system aims to enhance understanding, analysis, and decision-making processes by immersing users in a VR environment where complex manufacturing data stored using blockchain is translated into intuitive and interactive representations. The project focuses on two main components: blockchain and VR. Hyperledger Fabric is employed to establish a blockchain network, recording data from the Future Factories testbed. This system captures information from various sources, such as potentiometers on robot grippers to measure grip positioning, load cells to gauge pressure, emergency stop buttons, temperature, speed, and vibration sensors on the conveyors. Whenever predefined conditions are met, pertinent data, including sensor ID, timestamp, value, cause, and importance, is securely recorded in the blockchain, signaling the occurrence of a defect within the cell. Data retrieved from the blockchain system is accessed through 'GET' API requests. A VR application is developed using a cross-platform Unity game engine to visualize the data retrieved from the blockchain database. Meta Quest 3 is used as the targeted Head Mounted VR device. The VR application has two C# scripts: one script to query blockchain data using 'GET' API calls and another script converts the JSON object to text data to visualize in the VR system. The proposed system leverages blockchain technology and VR visualization to deliver immersive, actionable insights using secure data transmission. By embracing the proposed framework, manufacturers can unlock new potential for efficiency, sustainability, and resilience in today's increasingly complex and interconnected manufacturing workplace.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17218",
        "abstract url": "https://arxiv.org/abs/2406.17218",
        "title": "MIMO-OFDM ISAC Waveform Design for Range-Doppler Sidelobe Suppression",
        "rating": "-1",
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "Integrated sensing and communication (ISAC) is a key enabling technique for future wireless networks owing to its efficient hardware and spectrum utilization. In this paper, we focus on dual-functional waveform design for a multi-input multi-output (MIMO) orthogonal frequency division multiplexing (OFDM) ISAC system, which is considered to be a promising solution for practical deployment. Since the dual-functional waveform carries communication information, its random nature leads to high range-Doppler sidelobes in the ambiguity function, which in turn degrades radar sensing performance. To suppress range-Doppler sidelobes, we propose a novel symbol-level precoding (SLP) based waveform design for MIMO-OFDM ISAC systems by fully exploiting the temporal degrees of freedom (DoFs). Our goal is to minimize the range-Doppler integrated sidelobe level (ISL) while satisfying the constraints of target illumination power, multi-user communication quality of service (QoS), and constant-modulus transmission. To solve the resulting non-convex waveform design problem, we develop an efficient algorithm using the majorization-minimization (MM) and alternative direction method of multipliers (ADMM) methods. Simulation results show that the proposed waveform has significantly reduced range-Doppler sidelobes compared with signals designed only for communications and other baselines. In addition, the proposed waveform design achieves target detection and estimation performance close to that achievable by waveforms designed only for radar, which demonstrates the superiority of the proposed SLP-based ISAC approach.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "13 pages, 9 figures, submitted to IEEE TWC"
    },
    {
        "paper id": "2406.17223",
        "abstract url": "https://arxiv.org/abs/2406.17223",
        "title": "On Zero-Error Capacity of Graphs with One Edge",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "In this paper, we study the zero-error capacity of channels with memory, which are represented by graphs. We provide a method to construct code for any graph with one edge, thereby determining a lower bound on its zero-error capacity. Moreover, this code can achieve zero-error capacity when the symbols in a vertex with degree one are the same. We further apply our method to the one-edge graphs representing the binary channels with two memories. There are 28 possible graphs, which can be organized into 11 categories based on their symmetries. The code constructed by our method is proved to achieve the zero-error capacity for all these graphs except for the two graphs in Case 11.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17225",
        "abstract url": "https://arxiv.org/abs/2406.17225",
        "title": "Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide Pathological Images",
        "rating": "-1",
        "keywords": [
            [
                "Survival",
                "Whole Slide",
                "cancer",
                "tumor",
                "Pathological"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Survival prediction, utilizing pathological images and genomic profiles, is increasingly important in cancer analysis and prognosis. Despite significant progress, precise survival analysis still faces two main challenges: (1) The massive pixels contained in whole slide images (WSIs) complicate the process of pathological images, making it difficult to generate an effective representation of the tumor microenvironment (TME). (2) Existing multimodal methods often rely on alignment strategies to integrate complementary information, which may lead to information loss due to the inherent heterogeneity between pathology and genes. In this paper, we propose a Multimodal Cross-Task Interaction (MCTI) framework to explore the intrinsic correlations between subtype classification and survival analysis tasks. Specifically, to capture TME-related features in WSIs, we leverage the subtype classification task to mine tumor regions. Simultaneously, multi-head attention mechanisms are applied in genomic feature extraction, adaptively performing genes grouping to obtain task-related genomic embedding. With the joint representation of pathological images and genomic data, we further introduce a Transport-Guided Attention (TGA) module that uses optimal transport theory to model the correlation between subtype classification and survival analysis tasks, effectively transferring potential information. Extensive experiments demonstrate the superiority of our approaches, with MCTI outperforming state-of-the-art frameworks on three public benchmarks. \\href{https://github.com/jsh0792/MCTI}{https://github.com/jsh0792/MCTI}.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17229",
        "abstract url": "https://arxiv.org/abs/2406.17229",
        "title": "Self-Supervised Embeddings for Detecting Individual Symptoms of Depression",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Depression, a prevalent mental health disorder impacting millions globally, demands reliable assessment systems. Unlike previous studies that focus solely on either detecting depression or predicting its severity, our work identifies individual symptoms of depression while also predicting its severity using speech input. We leverage self-supervised learning (SSL)-based speech models to better utilize the small-sized datasets that are frequently encountered in this task. Our study demonstrates notable performance improvements by utilizing SSL embeddings compared to conventional speech features. We compare various types of SSL pretrained models to elucidate the type of speech information (semantic, speaker, or prosodic) that contributes the most in identifying different symptoms. Additionally, we evaluate the impact of combining multiple SSL embeddings on performance. Furthermore, we show the significance of multi-task learning for identifying depressive symptoms effectively.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": "Accepted at INTERSPEECH 2024"
    },
    {
        "paper id": "2406.17238",
        "abstract url": "https://arxiv.org/abs/2406.17238",
        "title": "Expansive Synthesis: Generating Large-Scale Datasets from Minimal Samples",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "graph"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The challenge of limited availability of data for training in machine learning arises in many applications and the impact on performance and generalization is serious. Traditional data augmentation methods aim to enhance training with a moderately sufficient data set. Generative models like Generative Adversarial Networks (GANs) often face problematic convergence when generating significant and diverse data samples. Diffusion models, though effective, still struggle with high computational cost and long training times. This paper introduces an innovative Expansive Synthesis model that generates large-scale, high-fidelity datasets from minimal samples. The proposed approach exploits expander graph mappings and feature interpolation to synthesize expanded datasets while preserving the intrinsic data distribution and feature structural relationships. The rationale of the model is rooted in the non-linear property of neural networks' latent space and in its capture by a Koopman operator to yield a linear space of features to facilitate the construction of larger and enriched consistent datasets starting with a much smaller dataset. This process is optimized by an autoencoder architecture enhanced with self-attention layers and further refined for distributional consistency by optimal transport. We validate our Expansive Synthesis by training classifiers on the generated datasets and comparing their performance to classifiers trained on larger, original datasets. Experimental results demonstrate that classifiers trained on synthesized data achieve performance metrics on par with those trained on full-scale datasets, showcasing the model's potential to effectively augment training data. This work represents a significant advancement in data generation, offering a robust solution to data scarcity and paving the way for enhanced data availability in machine learning applications.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "eess.IV"
        ],
        "comment": "14 pages. arXiv admin note: text overlap with arXiv:2405.13866"
    },
    {
        "paper id": "2406.17244",
        "abstract url": "https://arxiv.org/abs/2406.17244",
        "title": "A Near-Field Super-Resolution Network for Accelerating Antenna Characterization",
        "rating": "-1",
        "keywords": [
            [
                "Super-Resolution"
            ]
        ],
        "abstract": "We present a deep neural network-enabled method to accelerate near-field (NF) antenna measurement. We develop a Near-field Super-resolution Network (NFS-Net) to reconstruct significantly undersampled near-field data as high-resolution data, which considerably reduces the number of sampling points required for NF measurement and thus improves measurement efficiency. The high-resolution near-field data reconstructed by the network is further processed by a near-field-to-far-field (NF2FF) transformation to obtain far-field antenna radiation patterns. Our experiments demonstrate that the NFS-Net exhibits both accuracy and generalizability in restoring high-resolution near-field data from low-resolution input. The NF measurement workflow that combines the NFS-Net and the NF2FF algorithm enables accurate radiation pattern characterization with only 11% of the Nyquist rate samples. Though the experiments in this study are conducted on a planar setup with a uniform grid, the proposed method can serve as a universal strategy to accelerate measurements under different setups and conditions.",
        "subjects": [
            "eess.SP",
            "eess.SY"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2406.17254",
        "abstract url": "https://arxiv.org/abs/2406.17254",
        "title": "Scalp Diagnostic System With Label-Free Segmentation and Training-Free Image Translation",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Scalp diseases and alopecia affect millions of people around the world, underscoring the urgent need for early diagnosis and management of the disease. However, the development of a comprehensive AI-based diagnosis system encompassing these conditions remains an underexplored domain due to the challenges associated with data imbalance and the costly nature of labeling. To address these issues, we propose ScalpVision, an AI-driven system for the holistic diagnosis of scalp diseases and alopecia. In ScalpVision, effective hair segmentation is achieved using pseudo image-label pairs and an innovative prompting method in the absence of traditional hair masking labels. This approach is crucial for extracting key features such as hair thickness and count, which are then used to assess alopecia severity. Additionally, ScalpVision introduces DiffuseIT-M, a generative model adept at dataset augmentation while maintaining hair information, facilitating improved predictions of scalp disease severity. Our experimental results affirm ScalpVision's efficiency in diagnosing a variety of scalp conditions and alopecia, showcasing its potential as a valuable tool in dermatological care.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "IEEE Transactions on Medical Imaging (Under Review)"
    },
    {
        "paper id": "2406.17732",
        "abstract url": "https://arxiv.org/abs/2406.17732",
        "title": "EMVD dataset: a dataset of extreme vocal distortion techniques used in heavy metal",
        "rating": "-1",
        "keywords": [
            [
                "music"
            ],
            [
                "cs.AI",
                "cs.SD"
            ]
        ],
        "abstract": "In this paper, we introduce the Extreme Metal Vocals Dataset, which comprises a collection of recordings of extreme vocal techniques performed within the realm of heavy metal music. The dataset consists of 760 audio excerpts of 1 second to 30 seconds long, totaling about 100 min of audio material, roughly composed of 60 minutes of distorted voices and 40 minutes of clear voice recordings. These vocal recordings are from 27 different singers and are provided without accompanying musical instruments or post-processing effects. The distortion taxonomy within this dataset encompasses four distinct distortion techniques and three vocal effects, all performed in different pitch ranges. Performance of a state-of-the-art deep learning model is evaluated for two different classification tasks related to vocal techniques, demonstrating the potential of this resource for the audio processing community.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "physics.class-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16349",
        "abstract url": "https://arxiv.org/abs/2406.16349",
        "title": "AnnotatedTables: A Large Tabular Dataset with Language Model Annotations",
        "rating": "-1.5",
        "keywords": [
            [
                "Tabular",
                "SQL"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Tabular data is ubiquitous in real-world applications and abundant on the web, yet its annotation has traditionally required human labor, posing a significant scalability bottleneck for tabular machine learning. Our methodology can successfully annotate a large amount of tabular data and can be flexibly steered to generate various types of annotations based on specific research objectives, as we demonstrate with SQL annotation and input-target column annotation as examples. As a result, we release AnnotatedTables, a collection of 32,119 databases with LLM-generated annotations. The dataset includes 405,616 valid SQL programs, making it the largest SQL dataset with associated tabular data that supports query execution. To further demonstrate the value of our methodology and dataset, we perform two follow-up research studies. 1) We investigate whether LLMs can translate SQL programs to Rel programs, a database language previously unknown to LLMs, while obtaining the same execution results. Using our Incremental Prompt Engineering methods based on execution feedback, we show that LLMs can produce adequate translations with few-shot learning. 2) We evaluate the performance of TabPFN, a recent neural tabular classifier trained on Bayesian priors, on 2,720 tables with input-target columns identified and annotated by LLMs. On average, TabPFN performs on par with the baseline AutoML method, though the relative performance can vary significantly from one data table to another, making both models viable for practical applications depending on the situation. Our findings underscore the potential of LLMs in automating the annotation of large volumes of diverse tabular data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16351",
        "abstract url": "https://arxiv.org/abs/2406.16351",
        "title": "METRIK: Measurement-Efficient Randomized Controlled Trials using Transformers with Input Masking",
        "rating": "-1.5",
        "keywords": [
            [
                "Clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Clinical randomized controlled trials (RCTs) collect hundreds of measurements spanning various metric types (e.g., laboratory tests, cognitive/motor assessments, etc.) across 100s-1000s of subjects to evaluate the effect of a treatment, but do so at the cost of significant trial expense. To reduce the number of measurements, trial protocols can be revised to remove metrics extraneous to the study's objective, but doing so requires additional human labor and limits the set of hypotheses that can be studied with the collected data. In contrast, a planned missing design (PMD) can reduce the amount of data collected without removing any metric by imputing the unsampled data. Standard PMDs randomly sample data to leverage statistical properties of imputation algorithms, but are ad hoc, hence suboptimal. Methods that learn PMDs produce more sample-efficient PMDs, but are not suitable for RCTs because they require ample prior data (150+ subjects) to model the data distribution. Therefore, we introduce a framework called Measurement EfficienT Randomized Controlled Trials using Transformers with Input MasKing (METRIK), which, for the first time, calculates a PMD specific to the RCT from a modest amount of prior data (e.g., 60 subjects). Specifically, METRIK models the PMD as a learnable input masking layer that is optimized with a state-of-the-art imputer based on the Transformer architecture. METRIK implements a novel sampling and selection algorithm to generate a PMD that satisfies the trial designer's objective, i.e., whether to maximize sampling efficiency or imputation performance for a given sampling budget. Evaluated across five real-world clinical RCT datasets, METRIK increases the sampling efficiency of and imputation performance under the generated PMD by leveraging correlations over time and across metrics, thereby removing the need to manually remove metrics from the RCT.",
        "subjects": [
            "cs.LG",
            "stat.ME"
        ],
        "comment": "18 pages, 11 figures"
    },
    {
        "paper id": "2406.16357",
        "abstract url": "https://arxiv.org/abs/2406.16357",
        "title": "Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification",
        "rating": "-1.5",
        "keywords": [
            [
                "Architecture Search"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constraint scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose a Lightweight Graph Neural Architecture Search with Graph SparsIfication and Network Pruning (GASSIP) method. In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.SI"
        ],
        "comment": "Accepted by KDD 2024. The two first authors made equal contributions"
    },
    {
        "paper id": "2406.16388",
        "abstract url": "https://arxiv.org/abs/2406.16388",
        "title": "PenSLR: Persian end-to-end Sign Language Recognition Using Ensembling",
        "rating": "-1.5",
        "keywords": [
            [
                "Sign Language"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Sign Language Recognition (SLR) is a fast-growing field that aims to fill the communication gaps between the hearing-impaired and people without hearing loss. Existing solutions for Persian Sign Language (PSL) are limited to word-level interpretations, underscoring the need for more advanced and comprehensive solutions. Moreover, previous work on other languages mainly focuses on manipulating the neural network architectures or hardware configurations instead of benefiting from the aggregated results of multiple models. In this paper, we introduce PenSLR, a glove-based sign language system consisting of an Inertial Measurement Unit (IMU) and five flexible sensors powered by a deep learning framework capable of predicting variable-length sequences. We achieve this in an end-to-end manner by leveraging the Connectionist Temporal Classification (CTC) loss function, eliminating the need for segmentation of input signals. To further enhance its capabilities, we propose a novel ensembling technique by leveraging a multiple sequence alignment algorithm known as Star Alignment. Furthermore, we introduce a new PSL dataset, including 16 PSL signs with more than 3000 time-series samples in total. We utilize this dataset to evaluate the performance of our system based on four word-level and sentence-level metrics. Our evaluations show that PenSLR achieves a remarkable word accuracy of 94.58% and 96.70% in subject-independent and subject-dependent setups, respectively. These achievements are attributable to our ensembling algorithm, which not only boosts the word-level performance by 0.51% and 1.32% in the respective scenarios but also yields significant enhancements of 1.46% and 4.00%, respectively, in sentence-level accuracy.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16426",
        "abstract url": "https://arxiv.org/abs/2406.16426",
        "title": "Fault Detection for agents on power grid topology optimization: A Comprehensive analysis",
        "rating": "-1.5",
        "keywords": [
            [
                "survival"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The topology optimization of transmission networks using Deep Reinforcement Learning (DRL) has increasingly come into focus. Various researchers have proposed different DRL agents, which are often benchmarked on the Grid2Op environment from the Learning to Run a Power Network (L2RPN) challenges. The environments have many advantages with their realistic chronics and underlying power flow backends. However, the interpretation of agent survival or failure is not always clear, as there are a variety of potential causes. In this work, we focus on the failures of the power grid to identify patterns and detect them a priori. We collect the failed chronics of three different agents on the WCCI 2022 L2RPN environment, totaling about 40k data points. By clustering, we are able to detect five distinct clusters, identifying different failure types. Further, we propose a multi-class prediction approach to detect failures beforehand and evaluate five different models. Here, the Light Gradient-Boosting Machine (LightGBM) shows the best performance, with an accuracy of 86%. It also correctly identifies in 91% of the time failure and survival observations. Finally, we provide a detailed feature importance analysis that identifies critical features and regions in the grid.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SY"
        ],
        "comment": "11 Pages plus references and appendix. The appendix consist of additional material of the paper and is not included in the initial submission"
    },
    {
        "paper id": "2406.16479",
        "abstract url": "https://arxiv.org/abs/2406.16479",
        "title": "Emerging NeoHebbian Dynamics in Forward-Forward Learning: Implications for Neuromorphic Computing",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Advances in neural computation have predominantly relied on the gradient backpropagation algorithm (BP). However, the recent shift towards non-stationary data modeling has highlighted the limitations of this heuristic, exposing that its adaptation capabilities are far from those seen in biological brains. Unlike BP, where weight updates are computed through a reverse error propagation path, Hebbian learning dynamics provide synaptic updates using only information within the layer itself. This has spurred interest in biologically plausible learning algorithms, hypothesized to overcome BP's shortcomings. In this context, Hinton recently introduced the Forward-Forward Algorithm (FFA), which employs local learning rules for each layer and has empirically proven its efficacy in multiple data modeling tasks. In this work we argue that when employing a squared Euclidean norm as a goodness function driving the local learning, the resulting FFA is equivalent to a neo-Hebbian Learning Rule. To verify this result, we compare the training behavior of FFA in analog networks with its Hebbian adaptation in spiking neural networks. Our experiments demonstrate that both versions of FFA produce similar accuracy and latent distributions. The findings herein reported provide empirical evidence linking biological learning rules with currently used training algorithms, thus paving the way towards extrapolating the positive outcomes from FFA to Hebbian learning rules. Simultaneously, our results imply that analog networks trained under FFA could be directly applied to neuromorphic computing, leading to reduced energy usage and increased computational speed.",
        "subjects": [
            "cs.NE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16494",
        "abstract url": "https://arxiv.org/abs/2406.16494",
        "title": "Cross-domain Transfer of Valence Preferences via a Meta-optimization Approach",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Cross-domain recommendation offers a potential avenue for alleviating data sparsity and cold-start problems. Embedding and mapping, as a classic cross-domain research genre, aims to identify a common mapping function to perform representation transformation between two domains. Nevertheless, previous coarse-grained preference representations, non-personalized mapping functions, and excessive reliance on overlapping users limit their performance, especially in scenarios where overlapping users are sparse. To address aforementioned challenges, we propose a novel cross-domain approach, namely CVPM. CVPM formalizes cross-domain interest transfer as a hybrid architecture of parametric meta-learning and self-supervised learning, which not only transfers user preferences at a finer level, but also enables signal enhancement with the knowledge of non-overlapping users. Specifically, with deep insights into user preferences and valence preference theory, we believe that there exists significant difference between users' positive preferences and negative behaviors, and thus employ differentiated encoders to learn their distributions. In particular, we further utilize the pre-trained model and item popularity to sample pseudo-interaction items to ensure the integrity of both distributions. To guarantee the personalization of preference transfer, we treat each user's mapping as two parts, the common transformation and the personalized bias, where the network used to generate the personalized bias is output by a meta-learner. Furthermore, in addition to the supervised loss for overlapping users, we design contrastive tasks for non-overlapping users from both group and individual-levels to avoid model skew and enhance the semantics of representations. Exhaustive data analysis and extensive experimental results demonstrate the effectiveness and advancement of our proposed framework.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16590",
        "abstract url": "https://arxiv.org/abs/2406.16590",
        "title": "Forecasting with Deep Learning: Beyond Average of Average of Average Performance",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Accurate evaluation of forecasting models is essential for ensuring reliable predictions. Current practices for evaluating and comparing forecasting models focus on summarising performance into a single score, using metrics such as SMAPE. We hypothesize that averaging performance over all samples dilutes relevant information about the relative performance of models. Particularly, conditions in which this relative performance is different than the overall accuracy. We address this limitation by proposing a novel framework for evaluating univariate time series forecasting models from multiple perspectives, such as one-step ahead forecasting versus multi-step ahead forecasting. We show the advantages of this framework by comparing a state-of-the-art deep learning approach with classical forecasting techniques. While classical methods (e.g. ARIMA) are long-standing approaches to forecasting, deep neural networks (e.g. NHITS) have recently shown state-of-the-art forecasting performance in benchmark datasets. We conducted extensive experiments that show NHITS generally performs best, but its superiority varies with forecasting conditions. For instance, concerning the forecasting horizon, NHITS only outperforms classical approaches for multi-step ahead forecasting. Another relevant insight is that, when dealing with anomalies, NHITS is outperformed by methods such as Theta. These findings highlight the importance of aspect-based model evaluation.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16681",
        "abstract url": "https://arxiv.org/abs/2406.16681",
        "title": "A Comprehensive Review of Emerging Approaches in Machine Learning for De Novo PROTAC Design",
        "rating": "-1.5",
        "keywords": [
            [
                "disease"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Targeted protein degradation (TPD) is a rapidly growing field in modern drug discovery that aims to regulate the intracellular levels of proteins by harnessing the cell's innate degradation pathways to selectively target and degrade disease-related proteins. This strategy creates new opportunities for therapeutic intervention in cases where occupancy-based inhibitors have not been successful. Proteolysis-targeting chimeras (PROTACs) are at the heart of TPD strategies, which leverage the ubiquitin-proteasome system for the selective targeting and proteasomal degradation of pathogenic proteins. As the field evolves, it becomes increasingly apparent that the traditional methodologies for designing such complex molecules have limitations. This has led to the use of machine learning (ML) and generative modeling to improve and accelerate the development process. In this review, we explore the impact of ML on de novo PROTAC design $-$ an aspect of molecular design that has not been comprehensively reviewed despite its significance. We delve into the distinct characteristics of PROTAC linker design, underscoring the complexities required to create effective bifunctional molecules capable of TPD. We then examine how ML in the context of fragment-based drug design (FBDD), honed in the realm of small-molecule drug discovery, is paving the way for PROTAC linker design. Our review provides a critical evaluation of the limitations inherent in applying this method to the complex field of PROTAC development. Moreover, we review existing ML works applied to PROTAC design, highlighting pioneering efforts and, importantly, the limitations these studies face. By offering insights into the current state of PROTAC development and the integral role of ML in PROTAC design, we aim to provide valuable perspectives for researchers in their pursuit of better design strategies for this new modality.",
        "subjects": [
            "q-bio.BM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16696",
        "abstract url": "https://arxiv.org/abs/2406.16696",
        "title": "Public Constitutional AI",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "We are increasingly subjected to the power of AI authorities. As AI decisions become inescapable, entering domains such as healthcare, education, and law, we must confront a vital question: how can we ensure AI systems have the legitimacy necessary for effective governance? This essay argues that to secure AI legitimacy, we need methods that engage the public in designing and constraining AI systems, ensuring these technologies reflect the community's shared values. Constitutional AI, proposed by Anthropic, represents a step towards this goal, offering a model for democratic control of AI. However, while Constitutional AI's commitment to hardcoding explicit principles into AI models enhances transparency and accountability, it falls short in two crucial aspects: addressing the opacity of individual AI decisions and fostering genuine democratic legitimacy. To overcome these limitations, this essay proposes \"Public Constitutional AI.\" This approach envisions a participatory process where diverse stakeholders, including ordinary citizens, deliberate on the principles guiding AI development. The resulting \"AI Constitution\" would carry the legitimacy of popular authorship, grounding AI governance in the public will. Furthermore, the essay proposes \"AI Courts\" to develop \"AI case law,\" providing concrete examples for operationalizing constitutional principles in AI training. This evolving combination of constitutional principles and case law aims to make AI governance more responsive to public values. By grounding AI governance in deliberative democratic processes, Public Constitutional AI offers a path to imbue automated authorities with genuine democratic legitimacy, addressing the unique challenges posed by increasingly powerful AI systems while ensuring their alignment with the public interest.",
        "subjects": [
            "cs.CY",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16715",
        "abstract url": "https://arxiv.org/abs/2406.16715",
        "title": "GC-Bench: A Benchmark Framework for Graph Condensation with New Insights",
        "rating": "-1.5",
        "keywords": [
            [
                "architecture search"
            ],
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph condensation (GC) is an emerging technique designed to learn a significantly smaller graph that retains the essential information of the original graph. This condensed graph has shown promise in accelerating graph neural networks while preserving performance comparable to those achieved with the original, larger graphs. Additionally, this technique facilitates downstream applications such as neural architecture search and enhances our understanding of redundancy in large graphs. Despite the rapid development of GC methods, a systematic evaluation framework remains absent, which is necessary to clarify the critical designs for particular evaluative aspects. Furthermore, several meaningful questions have not been investigated, such as whether GC inherently preserves certain graph properties and offers robustness even without targeted design efforts. In this paper, we introduce GC-Bench, a comprehensive framework to evaluate recent GC methods across multiple dimensions and to generate new insights. Our experimental findings provide a deeper insights into the GC process and the characteristics of condensed graphs, guiding future efforts in enhancing performance and exploring new applications. Our code is available at \\url{https://github.com/Emory-Melody/GraphSlim/tree/main/benchmark}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2406.16741",
        "abstract url": "https://arxiv.org/abs/2406.16741",
        "title": "Extracting thin film structures of energy materials using transformers",
        "rating": "-1.5",
        "keywords": [
            [
                "chemical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Neutron-Transformer Reflectometry and Advanced Computation Engine (N-TRACE ), a neural network model using transformer architecture, is introduced for neutron reflectometry data analysis. It offers fast, accurate initial parameter estimations and efficient refinements, improving efficiency and precision for real-time data analysis of lithium-mediated nitrogen reduction for electrochemical ammonia synthesis, with relevance to other chemical transformations and batteries. Despite limitations in generalizing across systems, it shows promises for the use of transformers as the basis for models that could replace trial-and-error approaches to modeling reflectometry data.",
        "subjects": [
            "physics.comp-ph",
            "cs.AI"
        ],
        "comment": "11 pages, 7 figures"
    },
    {
        "paper id": "2406.16756",
        "abstract url": "https://arxiv.org/abs/2406.16756",
        "title": "Addressing Polarization and Unfairness in Performative Prediction",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "When machine learning (ML) models are used in applications that involve humans (e.g., online recommendation, school admission, hiring, lending), the model itself may trigger changes in the distribution of targeted data it aims to predict. Performative prediction (PP) is a framework that explicitly considers such model-dependent distribution shifts when learning ML models. While significant efforts have been devoted to finding performative stable (PS) solutions in PP for system robustness, their societal implications are less explored and it is unclear whether PS solutions are aligned with social norms such as fairness. In this paper, we set out to examine the fairness property of PS solutions in performative prediction. We first show that PS solutions can incur severe polarization effects and group-wise loss disparity. Although existing fairness mechanisms commonly used in literature can help mitigate unfairness, they may fail and disrupt the stability under model-dependent distribution shifts. We thus propose novel fairness intervention mechanisms that can simultaneously achieve both stability and fairness in PP settings. Both theoretical analysis and experiments are provided to validate the proposed method.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16766",
        "abstract url": "https://arxiv.org/abs/2406.16766",
        "title": "Conformal time series decomposition with component-wise exchangeability",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Conformal prediction offers a practical framework for distribution-free uncertainty quantification, providing finite-sample coverage guarantees under relatively mild assumptions on data exchangeability. However, these assumptions cease to hold for time series due to their temporally correlated nature. In this work, we present a novel use of conformal prediction for time series forecasting that incorporates time series decomposition. This approach allows us to model different temporal components individually. By applying specific conformal algorithms to each component and then merging the obtained prediction intervals, we customize our methods to account for the different exchangeability regimes underlying each component. Our decomposition-based approach is thoroughly discussed and empirically evaluated on synthetic and real-world data. We find that the method provides promising results on well-structured time series, but can be limited by factors such as the decomposition step for more complex data.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.AP"
        ],
        "comment": "Accepted at COPA 2024; 34 pages, 14 figures, 8 tables (incl. appendix)"
    },
    {
        "paper id": "2406.16821",
        "abstract url": "https://arxiv.org/abs/2406.16821",
        "title": "General Binding Affinity Guidance for Diffusion Models in Structure-Based Drug Design",
        "rating": "-1.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Structure-Based Drug Design (SBDD) focuses on generating valid ligands that strongly and specifically bind to a designated protein pocket. Several methods use machine learning for SBDD to generate these ligands in 3D space, conditioned on the structure of a desired protein pocket. Recently, diffusion models have shown success here by modeling the underlying distributions of atomic positions and types. While these methods are effective in considering the structural details of the protein pocket, they often fail to explicitly consider the binding affinity. Binding affinity characterizes how tightly the ligand binds to the protein pocket, and is measured by the change in free energy associated with the binding process. It is one of the most crucial metrics for benchmarking the effectiveness of the interaction between a ligand and protein pocket. To address this, we propose BADGER: Binding Affinity Diffusion Guidance with Enhanced Refinement. BADGER is a general guidance method to steer the diffusion sampling process towards improved protein-ligand binding, allowing us to adjust the distribution of the binding affinity between ligands and proteins. Our method is enabled by using a neural network (NN) to model the energy function, which is commonly approximated by AutoDock Vina (ADV). ADV's energy function is non-differentiable, and estimates the affinity based on the interactions between a ligand and target protein receptor. By using a NN as a differentiable energy function proxy, we utilize the gradient of our learned energy function as a guidance method on top of any trained diffusion model. We show that our method improves the binding affinity of generated ligands to their protein receptors by up to 60\\%, significantly surpassing previous machine learning methods. We also show that our guidance method is flexible and can be easily applied to other diffusion-based SBDD frameworks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "physics.bio-ph",
            "physics.chem-ph",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16995",
        "abstract url": "https://arxiv.org/abs/2406.16995",
        "title": "A large language model for predicting T cell receptor-antigen binding specificity",
        "rating": "-1.5",
        "keywords": [
            [
                "clinical",
                "tumor"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The human immune response depends on the binding of T-cell receptors (TCRs) to antigens (pTCR), which elicits the T cells to eliminate viruses, tumor cells, and other pathogens. The ability of human immunity system responding to unknown viruses and bacteria stems from the TCR diversity. However, this vast diversity poses challenges on the TCR-antigen binding prediction methods. In this study, we propose a Masked Language Model (MLM), referred to as tcrLM, to overcome limitations in model generalization. Specifically, we randomly masked sequence segments and train tcrLM to infer the masked segment, thereby extract expressive feature from TCR sequences. Meanwhile, we introduced virtual adversarial training techniques to enhance the model's robustness. We built the largest TCR CDR3 sequence dataset to date (comprising 2,277,773,840 residuals), and pre-trained tcrLM on this dataset. Our extensive experimental results demonstrate that tcrLM achieved AUC values of 0.937 and 0.933 on independent test sets and external validation sets, respectively, which remarkably outperformed four previously published prediction methods. On a large-scale COVID-19 pTCR binding test set, our method outperforms the current state-of-the-art method by at least 8%, highlighting the generalizability of our method. Furthermore, we validated that our approach effectively predicts immunotherapy response and clinical outcomes on a clinical cohorts. These findings clearly indicate that tcrLM exhibits significant potential in predicting antigenic immunogenicity.",
        "subjects": [
            "q-bio.QM",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16997",
        "abstract url": "https://arxiv.org/abs/2406.16997",
        "title": "Wavelet Attention GRU for Efficient Industrial Gas Recognition with Novel Metrics",
        "rating": "-1.5",
        "keywords": [
            [
                "Industrial"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Gas recognition technology has received considerable attention from researchers in recent years. Nevertheless, the gas recognition area has faced obstacles in implementing deep learning-based recognition solutions due to the absence of standardized protocols. To tackle this problem, we suggest using two sets of specialized evaluation measures for gas recognition algorithms. These metrics will make it easier to examine the performance of these algorithms on various datasets. In addition, we provide a new model called the Wavelet Attention GRU (WAG), which is based on the wavelet attention mechanism. This method facilitates the more efficient retrieval of sensor signals. Compared to other models, WAG significantly decreases the number of sensors needed by 75% while obtaining an identification accuracy of 98.33%. This suggests that WAG is a potential approach for advancing gas recognition algorithms.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17001",
        "abstract url": "https://arxiv.org/abs/2406.17001",
        "title": "Deep Learning for Prediction and Classifying the Dynamical behaviour of Piecewise Smooth Maps",
        "rating": "-1.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Support Vector Machine"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper explores the prediction of the dynamics of piecewise smooth maps using various deep learning models. We have shown various novel ways of predicting the dynamics of piecewise smooth maps using deep learning models. Moreover, we have used machine learning models such as Decision Tree Classifier, Logistic Regression, K-Nearest Neighbor, Random Forest, and Support Vector Machine for predicting the border collision bifurcation in the 1D normal form map and the 1D tent map. Further, we classified the regular and chaotic behaviour of the 1D tent map and the 2D Lozi map using deep learning models like Convolutional Neural Network (CNN), ResNet50, and ConvLSTM via cobweb diagram and phase portraits. We also classified the chaotic and hyperchaotic behaviour of the 3D piecewise smooth map using deep learning models such as the Feed Forward Neural Network (FNN), Long Short-Term Memory (LSTM), and Recurrent Neural Network (RNN). Finally, deep learning models such as Long Short-Term Memory (LSTM) and Recurrent Neural Network (RNN) are used for reconstructing the two parametric charts of 2D border collision bifurcation normal form map.",
        "subjects": [
            "cs.LG",
            "nlin.CD"
        ],
        "comment": "32 pages, 22 figures"
    },
    {
        "paper id": "2406.17002",
        "abstract url": "https://arxiv.org/abs/2406.17002",
        "title": "Benchmarking mortality risk prediction from electrocardiograms",
        "rating": "-1.5",
        "keywords": [
            [
                "survival"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Several recent high-impact studies leverage large hospital-owned electrocardiographic (ECG) databases to model and predict patient mortality. MIMIC-IV, released September 2023, is the first comparable public dataset and includes 800,000 ECGs from a U.S. hospital system. Previously, the largest public ECG dataset was Code-15, containing 345,000 ECGs collected during routine care in Brazil. These datasets now provide an excellent resource for a broader audience to explore ECG survival modeling. Here, we benchmark survival model performance on Code-15 and MIMIC-IV with two neural network architectures, compare four deep survival modeling approaches to Cox regressions trained on classifier outputs, and evaluate performance at one to ten years. Our results yield AUROC and concordance scores comparable to past work (circa 0.8) and reasonable AUPRC scores (MIMIC-IV: 0.4-0.5, Code-15: 0.05-0.13) considering the fraction of ECG samples linked to a mortality (MIMIC-IV: 27\\%, Code-15: 4\\%). When evaluating models on the opposite dataset, AUROC and concordance values drop by 0.1-0.15, which may be due to cohort differences. All code and results are made public.",
        "subjects": [
            "eess.SP",
            "cs.LG",
            "cs.NE",
            "stat.AP"
        ],
        "comment": "9 pages plus appendix, 2 figures"
    },
    {
        "paper id": "2406.17008",
        "abstract url": "https://arxiv.org/abs/2406.17008",
        "title": "Meta-learning and Data Augmentation for Stress Testing Forecasting Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The effectiveness of univariate forecasting models is often hampered by conditions that cause them stress. A model is considered to be under stress if it shows a negative behaviour, such as higher-than-usual errors or increased uncertainty. Understanding the factors that cause stress to forecasting models is important to improve their reliability, transparency, and utility. This paper addresses this problem by contributing with a novel framework called MAST (Meta-learning and data Augmentation for Stress Testing). The proposed approach aims to model and characterize stress in univariate time series forecasting models, focusing on conditions where they exhibit large errors. In particular, MAST is a meta-learning approach that predicts the probability that a given model will perform poorly on a given time series based on a set of statistical time series features. MAST also encompasses a novel data augmentation technique based on oversampling to improve the metadata concerning stress. We conducted experiments using three benchmark datasets that contain a total of 49.794 time series to validate the performance of MAST. The results suggest that the proposed approach is able to identify conditions that lead to large errors. The method and experiments are publicly available in a repository.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 5 figures, 3 tables"
    },
    {
        "paper id": "2406.17066",
        "abstract url": "https://arxiv.org/abs/2406.17066",
        "title": "Tolerance of Reinforcement Learning Controllers against Deviations in Cyber Physical Systems",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Cyber-physical systems (CPS) with reinforcement learning (RL)-based controllers are increasingly being deployed in complex physical environments such as autonomous vehicles, the Internet-of-Things(IoT), and smart cities. An important property of a CPS is tolerance; i.e., its ability to function safely under possible disturbances and uncertainties in the actual operation. In this paper, we introduce a new, expressive notion of tolerance that describes how well a controller is capable of satisfying a desired system requirement, specified using Signal Temporal Logic (STL), under possible deviations in the system. Based on this definition, we propose a novel analysis problem, called the tolerance falsification problem, which involves finding small deviations that result in a violation of the given requirement. We present a novel, two-layer simulation-based analysis framework and a novel search heuristic for finding small tolerance violations. To evaluate our approach, we construct a set of benchmark problems where system parameters can be configured to represent different types of uncertainties and disturbancesin the system. Our evaluation shows that our falsification approach and heuristic can effectively find small tolerance violations.",
        "subjects": [
            "eess.SY",
            "cs.AI",
            "cs.LO",
            "cs.RO"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2311.07462"
    },
    {
        "paper id": "2406.17090",
        "abstract url": "https://arxiv.org/abs/2406.17090",
        "title": "Exploring Biomarker Relationships in Both Type 1 and Type 2 Diabetes Mellitus Through a Bayesian Network Analysis Approach",
        "rating": "-1.5",
        "keywords": [
            [
                "Biomarker"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Understanding the complex relationships of biomarkers in diabetes is pivotal for advancing treatment strategies, a pressing need in diabetes research. This study applies Bayesian network structure learning to analyze the Shanghai Type 1 and Type 2 diabetes mellitus datasets, revealing complex relationships among key diabetes-related biomarkers. The constructed Bayesian network presented notable predictive accuracy, particularly for Type 2 diabetes mellitus, with root mean squared error (RMSE) of 18.23 mg/dL, as validated through leave-one-domain experiments and Clarke error grid analysis. This study not only elucidates the intricate dynamics of diabetes through a deeper understanding of biomarker interplay but also underscores the significant potential of integrating data-driven and knowledge-driven methodologies in the realm of personalized diabetes management. Such an approach paves the way for more custom and effective treatment strategies, marking a notable advancement in the field.",
        "subjects": [
            "q-bio.QM",
            "cs.AI",
            "cs.CE",
            "cs.LG"
        ],
        "comment": "Paper is accepted by EMBC 2024"
    },
    {
        "paper id": "2406.17182",
        "abstract url": "https://arxiv.org/abs/2406.17182",
        "title": "Debiased Recommendation with Noisy Feedback",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Ratings of a user to most items in recommender systems are usually missing not at random (MNAR), largely because users are free to choose which items to rate. To achieve unbiased learning of the prediction model under MNAR data, three typical solutions have been proposed, including error-imputation-based (EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods. However, these methods ignore an alternative form of bias caused by the inconsistency between the observed ratings and the users' true preferences, also known as noisy feedback or outcome measurement errors (OME), e.g., due to public opinion or low-quality data collection process. In this work, we study intersectional threats to the unbiased learning of the prediction model from data MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and OME-DR estimators, which largely extend the existing estimators to combat OME in real-world recommendation scenarios. Next, we theoretically prove the unbiasedness and generalization bound of the proposed estimators. We further propose an alternate denoising training approach to achieve unbiased learning of the prediction model under MNAR data with OME. Extensive experiments are conducted on three real-world datasets and one semi-synthetic dataset to show the effectiveness of our proposed approaches. The code is available at https://github.com/haoxuanli-pku/KDD24-OME-DR.",
        "subjects": [
            "cs.IR",
            "cs.LG"
        ],
        "comment": "KDD 24 Research Track Paper"
    },
    {
        "paper id": "2406.17216",
        "abstract url": "https://arxiv.org/abs/2406.17216",
        "title": "Machine Unlearning Fails to Remove Data Poisoning Attacks",
        "rating": "-1.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of training on poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of evaluation settings (e.g., alleviating membership inference attacks), they fail to remove the effects of data poisoning, across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, is required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned datapoints without having to retrain, our work suggests that these methods are not yet \"ready for prime time\", and currently provide limited benefit over retraining.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17810",
        "abstract url": "https://arxiv.org/abs/2406.17810",
        "title": "PIC2O-Sim: A Physics-Inspired Causality-Aware Dynamic Convolutional Neural Operator for Ultra-Fast Photonic Device FDTD Simulation",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The finite-difference time-domain (FDTD) method, which is important in photonic hardware design flow, is widely adopted to solve time-domain Maxwell equations. However, FDTD is known for its prohibitive runtime cost, taking minutes to hours to simulate a single device. Recently, AI has been applied to realize orders-of-magnitude speedup in partial differential equation (PDE) solving. However, AI-based FDTD solvers for photonic devices have not been clearly formulated. Directly applying off-the-shelf models to predict the optical field dynamics shows unsatisfying fidelity and efficiency since the model primitives are agnostic to the unique physical properties of Maxwell equations and lack algorithmic customization. In this work, we thoroughly investigate the synergy between neural operator designs and the physical property of Maxwell equations and introduce a physics-inspired AI-based FDTD prediction framework PIC2O-Sim which features a causality-aware dynamic convolutional neural operator as its backbone model that honors the space-time causality constraints via careful receptive field configuration and explicitly captures the permittivity-dependent light propagation behavior via an efficient dynamic convolution operator. Meanwhile, we explore the trade-offs among prediction scalability, fidelity, and efficiency via a multi-stage partitioned time-bundling technique in autoregressive prediction. Multiple key techniques have been introduced to mitigate iterative error accumulation while maintaining efficiency advantages during autoregressive field prediction. Extensive evaluations on three challenging photonic device simulation tasks have shown the superiority of our PIC2O-Sim method, showing 51.2% lower roll-out prediction error, 23.5 times fewer parameters than state-of-the-art neural operators, providing 300-600x higher simulation speed than an open-source FDTD numerical solver.",
        "subjects": [
            "physics.comp-ph",
            "cs.AI",
            "physics.optics"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16322",
        "abstract url": "https://arxiv.org/abs/2406.16322",
        "title": "Lesion-Aware Cross-Phase Attention Network for Renal Tumor Subtype Classification on Multi-Phase CT Scans",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "diagnosis",
                "CT",
                "cancer",
                "clinical",
                "Tumor",
                "pathological",
                "Lesion"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Multi-phase computed tomography (CT) has been widely used for the preoperative diagnosis of kidney cancer due to its non-invasive nature and ability to characterize renal lesions. However, since enhancement patterns of renal lesions across CT phases are different even for the same lesion type, the visual assessment by radiologists suffers from inter-observer variability in clinical practice. Although deep learning-based approaches have been recently explored for differential diagnosis of kidney cancer, they do not explicitly model the relationships between CT phases in the network design, limiting the diagnostic performance. In this paper, we propose a novel lesion-aware cross-phase attention network (LACPANet) that can effectively capture temporal dependencies of renal lesions across CT phases to accurately classify the lesions into five major pathological subtypes from time-series multi-phase CT images. We introduce a 3D inter-phase lesion-aware attention mechanism to learn effective 3D lesion features that are used to estimate attention weights describing the inter-phase relations of the enhancement patterns. We also present a multi-scale attention scheme to capture and aggregate temporal patterns of lesion features at different spatial scales for further improvement. Extensive experiments on multi-phase CT scans of kidney cancer patients from the collected dataset demonstrate that our LACPANet outperforms state-of-the-art approaches in diagnostic accuracy.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "This article has been accepted for publication in Computers in Biology and Medicine"
    },
    {
        "paper id": "2406.16381",
        "abstract url": "https://arxiv.org/abs/2406.16381",
        "title": "Polar-Coded Tensor-Based Unsourced Random Access with Soft Decoding",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "The unsourced random access (URA) has emerged as a viable scheme for supporting the massive machine-type communications (mMTC) in the sixth generation (6G) wireless networks. Notably, the tensor-based URA (TURA), with its inherent tensor structure, stands out by simultaneously enhancing performance and reducing computational complexity for the multi-user separation, especially in mMTC networks with a large numer of active devices. However, current TURA scheme lacks the soft decoder, thus precluding the incorporation of existing advanced coding techniques. In order to fully explore the potential of the TURA, this paper investigates the Polarcoded TURA (PTURA) scheme and develops the corresponding iterative Bayesian receiver with feedback (IBR-FB). Specifically, in the IBR-FB, we propose the Grassmannian modulation-aided Bayesian tensor decomposition (GM-BTD) algorithm under the variational Bayesian learning (VBL) framework, which leverages the property of the Grassmannian modulation to facilitate the convergence of the VBL process, and has the ability to generate the required soft information without the knowledge of the number of active devices. Furthermore, based on the soft information produced by the GM-BTD, we design the soft Grassmannian demodulator in the IBR-FB. Extensive simulation results demonstrate that the proposed PTURA in conjunction with the IBR-FB surpasses the existing state-of-the-art unsourced random access scheme in terms of accuracy and computational complexity.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2406.16412",
        "abstract url": "https://arxiv.org/abs/2406.16412",
        "title": "Not All RDF is Created Equal: Investigating RDF Load Times on Resource-Constrained Devices",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "As the role of knowledge-based systems in IoT keeps growing, ensuring resource efficiency of RDF stores becomes critical. However, up until now benchmarks of RDF stores were most often conducted with only one dataset, and the differences between the datasets were not explored in detail. In this paper we aim to close this research gap by experimentally evaluating load times of eight diverse RDF datasets from the RiverBench benchmark suite. In the experiments we use five different RDF store implementations and several resource-constrained hardware platforms. To analyze the results, we introduce the notion of relative loading speed (RLS), allowing us to observe that the loading speed can differ between datasets by as much as a factor of 9.01. This serves as clear evidence that \"not all RDF is created equal\" and stresses the importance of using multiple benchmark datasets in evaluations. We outline the possible reasons for this drastic difference, which should be further investigated in future work. To this end, we published the data, code, and the results of our experiments.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16466",
        "abstract url": "https://arxiv.org/abs/2406.16466",
        "title": "SLOctolyzer: Fully automatic analysis toolkit for segmentation and feature extracting in scanning laser ophthalmoscopy images",
        "rating": "-2",
        "keywords": [
            [
                "infrared"
            ],
            [
                "retinal"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Purpose: To describe SLOctolyzer: an open-source analysis toolkit for en face retinal vessels appearing in infrared reflectance scanning laser ophthalmoscopy (SLO) images. Methods: SLOctolyzer includes two main modules: segmentation and measurement. The segmentation module use deep learning methods to delineate retinal anatomy, while the measurement module quantifies key retinal vascular features such as vessel complexity, density, tortuosity, and calibre. We evaluate the segmentation module using unseen data and measure its reproducibility. Results: SLOctolyzer's segmentation module performed well against unseen internal test data (Dice for all-vessels, 0.9097; arteries, 0.8376; veins, 0.8525; optic disc, 0.9430; fovea, 0.8837). External validation against severe retinal pathology showed decreased performance (Dice for arteries, 0.7180; veins, 0.7470; optic disc, 0.9032). SLOctolyzer had good reproducibility (mean difference for fractal dimension, -0.0007; vessel density, -0.0003; vessel calibre, -0.3154 $\u03bc$m; tortuosity density, 0.0013). SLOctolyzer can process a macula-centred SLO image in under 20 seconds and a disc-centred SLO image in under 30 seconds using a standard laptop CPU. Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to convert raw SLO images into reproducible and clinically meaningful retinal vascular parameters. SLO images are captured simultaneous to optical coherence tomography (OCT), and we believe our software will be useful for extracting retinal vascular measurements from large OCT image sets and linking them to ocular or systemic diseases. It requires no specialist knowledge or proprietary software, and allows manual correction of segmentations and re-computing of vascular metrics. SLOctolyzer is freely available at https://github.com/jaburke166/SLOctolyzer.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "10 pages, 5 figures, 6 tables + Supplementary (7 pages, 10 figures, 4 tables). Submitted for peer review at Translational Vision Science and Technology"
    },
    {
        "paper id": "2406.16477",
        "abstract url": "https://arxiv.org/abs/2406.16477",
        "title": "DaLPSR: Leverage Degradation-Aligned Language Prompt for Real-World Image Super-Resolution",
        "rating": "-2",
        "keywords": [
            [
                "diffusion",
                "Super-Resolution"
            ],
            [
                "image restoration"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Image super-resolution pursuits reconstructing high-fidelity high-resolution counterpart for low-resolution image. In recent years, diffusion-based models have garnered significant attention due to their capabilities with rich prior knowledge. The success of diffusion models based on general text prompts has validated the effectiveness of textual control in the field of text2image. However, given the severe degradation commonly presented in low-resolution images, coupled with the randomness characteristics of diffusion models, current models struggle to adequately discern semantic and degradation information within severely degraded images. This often leads to obstacles such as semantic loss, visual artifacts, and visual hallucinations, which pose substantial challenges for practical use. To address these challenges, this paper proposes to leverage degradation-aligned language prompt for accurate, fine-grained, and high-fidelity image restoration. Complementary priors including semantic content descriptions and degradation prompts are explored. Specifically, on one hand, image-restoration prompt alignment decoder is proposed to automatically discern the degradation degree of LR images, thereby generating beneficial degradation priors for image restoration. On the other hand, much richly tailored descriptions from pretrained multimodal large language model elicit high-level semantic priors closely aligned with human perception, ensuring fidelity control for image restoration. Comprehensive comparisons with state-of-the-art methods have been done on several popular synthetic and real-world benchmark datasets. The quantitative and qualitative analysis have demonstrated that the proposed method achieves a new state-of-the-art perceptual quality level, especially in real-world cases based on reference-free metrics.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16564",
        "abstract url": "https://arxiv.org/abs/2406.16564",
        "title": "FASTC: A Fast Attentional Framework for Semantic Traversability Classification Using Point Cloud",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "LIDAR"
            ],
            [
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Producing traversability maps and understanding the surroundings are crucial prerequisites for autonomous navigation. In this paper, we address the problem of traversability assessment using point clouds. We propose a novel pillar feature extraction module that utilizes PointNet to capture features from point clouds organized in vertical volume and a 2D encoder-decoder structure to conduct traversability classification instead of the widely used 3D convolutions. This results in less computational cost while even better performance is achieved at the same time. We then propose a new spatio-temporal attention module to fuse multi-frame information, which can properly handle the varying density problem of LIDAR point clouds, and this makes our module able to assess distant areas more accurately. Comprehensive experimental results on augmented Semantic KITTI and RELLIS-3D datasets show that our method is able to achieve superior performance over existing approaches both quantitatively and quantitatively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to ECAI2023 Our code is publicly available at [this](https://github.com/chenyirui/FASTC)"
    },
    {
        "paper id": "2406.16568",
        "abstract url": "https://arxiv.org/abs/2406.16568",
        "title": "Star+: A New Multi-Domain Model for CTR Prediction",
        "rating": "-2",
        "keywords": [
            [
                "industrial",
                "recommendation"
            ]
        ],
        "abstract": "In this paper, we introduce Star+, a novel multi-domain model for click-through rate (CTR) prediction inspired by the Star model. Traditional single-domain approaches and existing multi-task learning techniques face challenges in multi-domain environments due to their inability to capture domain-specific data distributions and complex inter-domain relationships. Star+ addresses these limitations by enhancing the interaction between shared and domain-specific information through various fusion strategies, such as add, adaptive add, concatenation, and gating fusions, to find the optimal balance between domain-specific and shared information. We also investigate the impact of different normalization techniques, including layer normalization, batch normalization, and partition normalization, on the performance of our model. Our extensive experiments on both industrial and public datasets demonstrate that Star+ significantly improves prediction accuracy and efficiency. This work contributes to the advancement of recommendation systems by providing a robust, scalable, and adaptive solution for multi-domain environments.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16601",
        "abstract url": "https://arxiv.org/abs/2406.16601",
        "title": "Do As I Do: Pose Guided Human Motion Copy",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human motion copy is an intriguing yet challenging task in artificial intelligence and computer vision, which strives to generate a fake video of a target person performing the motion of a source person. The problem is inherently challenging due to the subtle human-body texture details to be generated and the temporal consistency to be considered. Existing approaches typically adopt a conventional GAN with an L1 or L2 loss to produce the target fake video, which intrinsically necessitates a large number of training samples that are challenging to acquire. Meanwhile, current methods still have difficulties in attaining realistic image details and temporal consistency, which unfortunately can be easily perceived by human observers. Motivated by this, we try to tackle the issues from three aspects: (1) We constrain pose-to-appearance generation with a perceptual loss and a theoretically motivated Gromov-Wasserstein loss to bridge the gap between pose and appearance. (2) We present an episodic memory module in the pose-to-appearance generation to propel continuous learning that helps the model learn from its past poor generations. We also utilize geometrical cues of the face to optimize facial details and refine each key body part with a dedicated local GAN. (3) We advocate generating the foreground in a sequence-to-sequence manner rather than a single-frame manner, explicitly enforcing temporal inconsistency. Empirical results on five datasets, iPER, ComplexMotion, SoloDance, Fish, and Mouse datasets, demonstrate that our method is capable of generating realistic target videos while precisely copying motion from a source video. Our method significantly outperforms state-of-the-art approaches and gains 7.2% and 12.4% improvements in PSNR and FID respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16607",
        "abstract url": "https://arxiv.org/abs/2406.16607",
        "title": "An Invitation to Universality in Physics, Computer Science, and Beyond",
        "rating": "-2",
        "keywords": [
            [
                "Physics"
            ]
        ],
        "abstract": "A universal Turing machine is a powerful concept - a single device can compute any function that is computable. A universal spin model, similarly, is a class of physical systems whose low energy behavior simulates that of any spin system. Our categorical framework for universality (arXiv:2307.06851) captures these and other examples of universality as instances. In this article, we present an accessible account thereof with a focus on its basic ingredients and ways to use it. Specifically, we show how to identify necessary conditions for universality, compare types of universality within each instance, and establish that universality and negation give rise to unreachability (such as uncomputability).",
        "subjects": [
            "cs.CC",
            "cs.FL",
            "math-ph"
        ],
        "comment": "10 pages, 1 figure + string diagrams. This article summarizes the framework for universality from arXiv:2307.06851. It is submitted as a contribution to \"Fundamental Structures in Computational and Pure Mathematics, Volume 2\""
    },
    {
        "paper id": "2406.16612",
        "abstract url": "https://arxiv.org/abs/2406.16612",
        "title": "Towards Physically Talented Aerial Robots with Tactically Smart Swarm Behavior thereof: An Efficient Co-design Approach",
        "rating": "-2",
        "keywords": [
            [
                "flight"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "The collective performance or capacity of collaborative autonomous systems such as a swarm of robots is jointly influenced by the morphology and the behavior of individual systems in that collective. In that context, this paper explores how morphology impacts the learned tactical behavior of unmanned aerial/ground robots performing reconnaissance and search & rescue. This is achieved by presenting a computationally efficient framework to solve this otherwise challenging problem of jointly optimizing the morphology and tactical behavior of swarm robots. Key novel developments to this end include the use of physical talent metrics and modification of graph reinforcement learning architectures to allow joint learning of the swarm tactical policy and the talent metrics (search speed, flight range, and cruising speed) that constrain mobility and object/victim search capabilities of the aerial robots executing these tactics. Implementation of this co-design approach is supported by advancements to an open-source Pybullet-based swarm simulator that allows the use of variable aerial asset capabilities. The results of the co-design are observed to outperform those of tactics learning with a fixed Pareto design, when compared in terms of mission performance metrics. Significant differences in morphology and learned behavior are also observed by comparing the baseline design and the co-design outcomes.",
        "subjects": [
            "cs.RO",
            "cs.MA"
        ],
        "comment": "Accepted for presentation in proceedings of ASME IDETC-CIE 2024"
    },
    {
        "paper id": "2406.16624",
        "abstract url": "https://arxiv.org/abs/2406.16624",
        "title": "Decentralized RL-Based Data Transmission Scheme for Energy Efficient Harvesting",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "The evolving landscape of the Internet of Things (IoT) has given rise to a pressing need for an efficient communication scheme. As the IoT user ecosystem continues to expand, traditional communication protocols grapple with substantial challenges in meeting its burgeoning demands, including energy consumption, scalability, data management, and interference. In response to this, the integration of wireless power transfer and data transmission has emerged as a promising solution. This paper considers an energy harvesting (EH)-oriented data transmission scheme, where a set of users are charged by their own multi-antenna power beacon (PB) and subsequently transmits data to a base station (BS) using an irregular slotted aloha (IRSA) channel access protocol. We propose a closed-form expression to model energy consumption for the present scheme, employing average channel state information (A-CSI) beamforming in the wireless power channel. Subsequently, we employ the reinforcement learning (RL) methodology, wherein every user functions as an agent tasked with the goal of uncovering their most effective strategy for replicating transmissions. This strategy is devised while factoring in their energy constraints and the maximum number of packets they need to transmit. Our results underscore the viability of this solution, particularly when the PB can be strategically positioned to ensure a strong line-of-sight connection with the user, highlighting the potential benefits of optimal deployment.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted in EuCNC 2024 conference"
    },
    {
        "paper id": "2406.16692",
        "abstract url": "https://arxiv.org/abs/2406.16692",
        "title": "Stationary and Sparse Denoising Approach for Corticomuscular Causality Estimation",
        "rating": "-2",
        "keywords": [
            [
                "EEG",
                "physiological"
            ]
        ],
        "abstract": "Objective: Cortico-muscular communication patterns are instrumental in understanding movement control. Estimating significant causal relationships between motor cortex electroencephalogram (EEG) and surface electromyogram (sEMG) from concurrently active muscles presents a formidable challenge since the relevant processes underlying muscle control are typically weak in comparison to measurement noise and background activities. Methodology: In this paper, a novel framework is proposed to simultaneously estimate the order of the autoregressive model of cortico-muscular interactions along with the parameters while enforcing stationarity condition in a convex program to ensure global optimality. The proposed method is further extended to a non-convex program to account for the presence of measurement noise in the recorded signals by introducing a wavelet sparsity assumption on the excitation noise in the model. Results: The proposed methodology is validated using both simulated data and neurophysiological signals. In case of simulated data, the performance of the proposed methods has been compared with the benchmark approaches in terms of order identification, computational efficiency, and goodness of fit in relation to various noise levels. In case of physiological signals our proposed methods are compared against the state-of-the-art approaches in terms of the ability to detect Granger causality. Significance: The proposed methods are shown to be effective in handling stationarity and measurement noise assumptions, revealing significant causal interactions from brain to muscles and vice versa.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16724",
        "abstract url": "https://arxiv.org/abs/2406.16724",
        "title": "\u03bc-Net: A Deep Learning-Based Architecture for \u03bc-CT Segmentation",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "biological",
                "medical",
                "CT",
                "X-ray",
                "disease"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "X-ray computed microtomography (\u03bc-CT) is a non-destructive technique that can generate high-resolution 3D images of the internal anatomy of medical and biological samples. These images enable clinicians to examine internal anatomy and gain insights into the disease or anatomical morphology. However, extracting relevant information from 3D images requires semantic segmentation of the regions of interest, which is usually done manually and results time-consuming and tedious. In this work, we propose a novel framework that uses a convolutional neural network (CNN) to automatically segment the full morphology of the heart of Carassius auratus. The framework employs an optimized 2D CNN architecture that can infer a 3D segmentation of the sample, avoiding the high computational cost of a 3D CNN architecture. We tackle the challenges of handling large and high-resoluted image data (over a thousand pixels in each dimension) and a small training database (only three samples) by proposing a standard protocol for data normalization and processing. Moreover, we investigate how the noise, contrast, and spatial resolution of the sample and the training of the architecture are affected by the reconstruction technique, which depends on the number of input images. Experiments show that our framework significantly reduces the time required to segment new samples, allowing a faster microtomography analysis of the Carassius auratus heart shape. Furthermore, our framework can work with any bio-image (biological and medical) from \u03bc-CT with high-resolution and small dataset size",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16734",
        "abstract url": "https://arxiv.org/abs/2406.16734",
        "title": "Scheduling with Obligatory Tests",
        "rating": "-2",
        "keywords": [
            [
                "medical"
            ]
        ],
        "abstract": "Motivated by settings such as medical treatments or aircraft maintenance, we consider a scheduling problem with jobs that consist of two operations, a test and a processing part. The time required to execute the test is known in advance while the time required to execute the processing part becomes known only upon completion of the test. We use competitive analysis to study algorithms for minimizing the sum of completion times for $n$ given jobs on a single machine. As our main result, we prove using a novel analysis technique that the natural $1$-SORT algorithm has competitive ratio at most 1.861. For the special case of uniform test times, we show that a simple threshold-based algorithm has competitive ratio at most 1.585. We also prove a lower bound that shows that no deterministic algorithm can be better than $\\sqrt{2}$-competitive even in the case of uniform test times.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16764",
        "abstract url": "https://arxiv.org/abs/2406.16764",
        "title": "Extensively Not P-Bi-Immune promiseBQP-Complete Languages",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "In this paper, I first establish -- via methods other than the Gottesman-Knill theorem -- the existence of an infinite set of instances of simulating a quantum circuit to decide a decision problem that can be simulated classically. I then examine under what restrictions on quantum circuits the existence of infinitely many classically simulable instances persists. There turns out to be a vast number of such restrictions, and any combination of those found can be applied at the same time without eliminating the infinite set of classically simulable instances. Further analysis of the tools used in this then shows there exists a language that every (promise) BQP language is one-one reducible to. This language is also not P-bi-immune under very many promises.",
        "subjects": [
            "cs.CC",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16780",
        "abstract url": "https://arxiv.org/abs/2406.16780",
        "title": "Koopman Operator-based Detection-Isolation of Cyberattack: A Case Study on Electric Vehicle Charging",
        "rating": "-2",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "One of the key challenges towards the reliable operation of cyber-physical systems (CPS) is the threat of cyberattacks on system actuation signals and measurements. In recent years, system theoretic research has focused on effectively detecting and isolating these cyberattacks to ensure proper restorative measures. Although both model-based and model-free approaches have been used in this context, the latter are increasingly becoming more popular as complexities and model uncertainties in CPS increases. Thus, in this paper we propose a Koopman operator-based model-free cyberattack detection-isolation scheme for CPS. The algorithm uses limited system measurements for its training and generates real-time detection-isolation flags. Furthermore, we present a simulation case study to detect and isolate actuation and sensor attacks in a Lithium-ion battery system of a plug-in electric vehicle during charging.",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": "10 pages, 4 figures, to be published in 2024 American Control Conference"
    },
    {
        "paper id": "2406.16792",
        "abstract url": "https://arxiv.org/abs/2406.16792",
        "title": "Deep Learning and Chaos: A combined Approach To Image Encryption and Decryption",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "anomaly detection"
            ]
        ],
        "abstract": "In this paper, we introduce a novel image encryption and decryption algorithm using hyperchaotic signals from the novel 3D hyperchaotic map, 2D memristor map, Convolutional Neural Network (CNN), and key sensitivity analysis to achieve robust security and high efficiency. The encryption starts with the scrambling of gray images by using a 3D hyperchaotic map to yield complex sequences under disruption of pixel values; the robustness of this original encryption is further reinforced by employing a CNN to learn the intricate patterns and add the safety layer. The robustness of the encryption algorithm is shown by key sensitivity analysis, i.e., the average sensitivity of the algorithm to key elements. The other factors and systems of unauthorized decryption, even with slight variations in the keys, can alter the decryption procedure, resulting in the ineffective recreation of the decrypted image. Statistical analysis includes entropy analysis, correlation analysis, histogram analysis, and other security analyses like anomaly detection, all of which confirm the high security and effectiveness of the proposed encryption method. Testing of the algorithm under various noisy conditions is carried out to test robustness against Gaussian noise. Metrics for differential analysis, such as the NPCR (Number of Pixel Change Rate)and UACI (Unified Average Change Intensity), are also used to determine the strength of encryption. At the same time, the empirical validation was performed on several test images, which showed that the proposed encryption techniques have practical applicability and are robust to noise. Simulation results and comparative analyses illustrate that our encryption scheme possesses excellent visual security, decryption quality, and computational efficiency, and thus, it is efficient for secure image transmission and storage in big data applications.",
        "subjects": [
            "cs.CR",
            "nlin.CD"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16817",
        "abstract url": "https://arxiv.org/abs/2406.16817",
        "title": "GPT-4V Explorations: Mining Autonomous Driving",
        "rating": "-2",
        "keywords": [
            [
                "visual language"
            ],
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "industrial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper explores the application of the GPT-4V(ision) large visual language model to autonomous driving in mining environments, where traditional systems often falter in understanding intentions and making accurate decisions during emergencies. GPT-4V introduces capabilities for visual question answering and complex scene comprehension, addressing challenges in these specialized settings.Our evaluation focuses on its proficiency in scene understanding, reasoning, and driving functions, with specific tests on its ability to recognize and interpret elements such as pedestrians, various vehicles, and traffic devices. While GPT-4V showed robust comprehension and decision-making skills, it faced difficulties in accurately identifying specific vehicle types and managing dynamic interactions. Despite these challenges, its effective navigation and strategic decision-making demonstrate its potential as a reliable agent for autonomous driving in the complex conditions of mining environments, highlighting its adaptability and operational viability in industrial settings.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16835",
        "abstract url": "https://arxiv.org/abs/2406.16835",
        "title": "Preserving Real-World Finger Dexterity Using a Lightweight Fingertip Haptic Device for Virtual Dexterous Manipulation",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "This study presents a lightweight, wearable fingertip haptic device that provides physics-based haptic feedback for dexterous manipulation in virtual environments without hindering real-world interactions. The device's design utilizes thin strings and actuators attached to the fingernails, minimizing the weight (1.76g each finger) while preserving finger flexibility. Multiple types of haptic feedback are simulated by integrating the software with a physics engine. Experiments evaluate the device's performance in pressure perception, slip feedback, and typical dexterous manipulation tasks. and daily operations, while subjective assessments gather user experiences. Results demonstrate that participants can perceive and respond to pressure and vibration feedback. These limited haptic cues are crucial as they significantly enhance efficiency in virtual dexterous manipulation tasks. The device's ability to preserve tactile sensations and minimize hindrance to real-world operations is a key advantage over glove-type haptic devices. This research offers a potential solution for designing haptic interfaces that balance lightweight, haptic feedback for dexterous manipulation and daily wearability.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16850",
        "abstract url": "https://arxiv.org/abs/2406.16850",
        "title": "From Perfect to Noisy World Simulation: Customizable Embodied Multi-modal Perturbations for SLAM Robustness Benchmarking",
        "rating": "-2",
        "keywords": [
            [
                "Gaussian Splatting",
                "RGB-D",
                "NeRF"
            ],
            [
                "SLAM"
            ],
            [
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Embodied agents require robust navigation systems to operate in unstructured environments, making the robustness of Simultaneous Localization and Mapping (SLAM) models critical to embodied agent autonomy. While real-world datasets are invaluable, simulation-based benchmarks offer a scalable approach for robustness evaluations. However, the creation of a challenging and controllable noisy world with diverse perturbations remains under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. The pipeline comprises a comprehensive taxonomy of sensor and motion perturbations for embodied multi-modal (specifically RGB-D) sensing, categorized by their sources and propagation order, allowing for procedural composition. We also provide a toolbox for synthesizing these perturbations, enabling the transformation of clean environments into challenging noisy simulations. Utilizing the pipeline, we instantiate the large-scale Noisy-Replica benchmark, which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced RGB-D SLAM models. Our extensive analysis uncovers the susceptibilities of both neural (NeRF and Gaussian Splatting -based) and non-neural SLAM models to disturbances, despite their demonstrated accuracy in standard benchmarks. Our code is publicly available at https://github.com/Xiaohao-Xu/SLAM-under-Perturbation.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "50 pages. arXiv admin note: substantial text overlap with arXiv:2402.08125"
    },
    {
        "paper id": "2406.16993",
        "abstract url": "https://arxiv.org/abs/2406.16993",
        "title": "Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image Segmentation?",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The advancement of developing efficient medical image segmentation has evolved from initial dependence on Convolutional Neural Networks (CNNs) to the present investigation of hybrid models that combine CNNs with Vision Transformers. Furthermore, there is an increasing focus on creating architectures that are both high-performing in medical image segmentation tasks and computationally efficient to be deployed on systems with limited resources. Although transformers have several advantages like capturing global dependencies in the input data, they face challenges such as high computational and memory complexity. This paper investigates the integration of CNNs and Vision Extended Long Short-Term Memory (Vision-xLSTM) models by introducing a novel approach called UVixLSTM. The Vision-xLSTM blocks captures temporal and global relationships within the patches extracted from the CNN feature maps. The convolutional feature reconstruction path upsamples the output volume from the Vision-xLSTM blocks to produce the segmentation output. Our primary objective is to propose that Vision-xLSTM forms a reliable backbone for medical image segmentation tasks, offering excellent segmentation performance and reduced computational complexity. UVixLSTM exhibits superior performance compared to state-of-the-art networks on the publicly-available Synapse dataset. Code is available at: https://github.com/duttapallabi2907/UVixLSTM",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17070",
        "abstract url": "https://arxiv.org/abs/2406.17070",
        "title": "Collective Bit Flipping-Based Decoding of Quantum LDPC Codes",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum low-density parity-check (QLDPC) codes have been proven to achieve higher minimum distances at higher code rates than surface codes. However, this family of codes imposes stringent latency requirements and poor performance under iterative decoding, especially when the variable degree is low. In this work, we improve both the error correction performance and decoding latency of variable degree-3 (dv-3) QLDPC codes under iterative decoding. Firstly, we perform a detailed analysis of the structure of a well-known family of QLDPC codes, i.e., hypergraph product-based codes. Then, we propose a decoding approach that stems from the knowledge of harmful configurations apparent in these codes. Our decoding scheme is based on applying a modified version of bit flipping (BF) decoding, namely two-bit bit flipping (TBF) decoding, which adds more degrees of freedom to BF decoding. The granularity offered by TBF decoding helps us design sets of decoders that operate in parallel and can collectively decode error patterns appearing in harmful configurations of the code, thus addressing both the latency and performance requirements. Finally, simulation results demonstrate that the proposed decoding scheme surpasses other iterative decoding approaches for various dv-3 QLDPC codes.",
        "subjects": [
            "cs.IT",
            "quant-ph"
        ],
        "comment": "13 pages, 12 figures"
    },
    {
        "paper id": "2406.17080",
        "abstract url": "https://arxiv.org/abs/2406.17080",
        "title": "Multi-Aperture Fusion of Transformer-Convolutional Network (MFTC-Net) for 3D Medical Image Segmentation and Visualization",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Vision Transformers have shown superior performance to the traditional convolutional-based frameworks in many vision applications, including but not limited to the segmentation of 3D medical images. To further advance this area, this study introduces the Multi-Aperture Fusion of Transformer-Convolutional Network (MFTC-Net), which integrates the output of Swin Transformers and their corresponding convolutional blocks using 3D fusion blocks. The Multi-Aperture incorporates each image patch at its original resolutions with its pyramid representation to better preserve minute details. The proposed architecture has demonstrated a score of 89.73 and 7.31 for Dice and HD95, respectively, on the Synapse multi-organs dataset an improvement over the published results. The improved performance also comes with the added benefits of the reduced complexity of approximately 40 million parameters. Our code is available at https://github.com/Siyavashshabani/MFTC-Net",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17083",
        "abstract url": "https://arxiv.org/abs/2406.17083",
        "title": "Boosting Bitcoin Minute Trend Prediction Using the Separation Index",
        "rating": "-2",
        "keywords": [
            [
                "forecasting"
            ]
        ],
        "abstract": "Predicting the trend of Bitcoin, a highly volatile cryptocurrency, remains a challenging task. Accurate forecasting holds immense potential for investors and market participants dealing with High Frequency Trading systems. The purpose of this study is to demonstrate the significance of using a systematic approach toward selecting informative observations for enhancing Bitcoin minute trend prediction. While a multitude of data collection methods exist, a crucial barrier remains: efficiently selecting the most informative data for building powerful prediction models. This study tackles this challenge head-on by introducing the Separation Index, a groundbreaking tool for fast and effective data (feature) subset selection. The Separation Index operates by measuring the improvement in class separability (i.e. upward vs. downward trends) with each added feature set. This innovative metric guides the creation of a highly informative dataset, maximizing the model's ability to differentiate between price movements. Our research demonstrates the effectiveness of this approach, achieving unprecedented accuracy in minute-scale Bitcoin trend prediction, surpassing the performance of previous studies. This significant advancement paves the way for a new era of data-driven decision-making in the dynamic world of cryptocurrency markets.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "10 pages, 4 figures and 4 tables"
    },
    {
        "paper id": "2406.17087",
        "abstract url": "https://arxiv.org/abs/2406.17087",
        "title": "Lomas: A Platform for Confidential Analysis of Private Data",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Public services collect massive volumes of data to fulfill their missions. These data fuel the generation of regional, national, and international statistics across various sectors. However, their immense potential remains largely untapped due to strict and legitimate privacy regulations. In this context, Lomas is a novel open-source platform designed to realize the full potential of the data held by public administrations. It enables authorized users, such as approved researchers and government analysts, to execute algorithms on confidential datasets without directly accessing the data. The Lomas platform is designed to operate within a trusted computing environment, such as governmental IT infrastructure. Authorized users access the platform remotely to submit their algorithms for execution on private datasets. Lomas executes these algorithms without revealing the data to the user and returns the results protected by Differential Privacy, a framework that introduces controlled noise to the results, rendering any attempt to extract identifiable information unreliable. Differential Privacy allows for the mathematical quantification and control of the risk of disclosure while allowing for a complete transparency regarding how data is protected and utilized. The contributions of this project will significantly transform how data held by public services are used, unlocking valuable insights from previously inaccessible data. Lomas empowers research, informing policy development, e.g., public health interventions, and driving innovation across sectors, all while upholding the highest data confidentiality standards.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17133",
        "abstract url": "https://arxiv.org/abs/2406.17133",
        "title": "The Infinite-Dimensional Quantum Entropy: the Unified Entropy Case",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "By a use of the Fredholm determinant theory, the unified quantum entropy notion has been extended to a case of infinite-dimensional systems. Some of the known (in the finite-dimensional case) basic properties of the introduced unified entropies have been extended to the case study. Certain numerical approaches for computing the proposed finite and infinite-dimensional entropies are being outlined as well.",
        "subjects": [
            "quant-ph",
            "cs.IT"
        ],
        "comment": "12 pages, 2 figures"
    },
    {
        "paper id": "2406.17151",
        "abstract url": "https://arxiv.org/abs/2406.17151",
        "title": "Socially Acceptable Bipedal Robot Navigation via Social Zonotope Network Model Predictive Control",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "This study addresses the challenge of social bipedal navigation in a dynamic, human-crowded environment, a research area largely underexplored in legged robot navigation. We present a zonotope-based framework that couples prediction and motion planning for a bipedal ego-agent to account for bidirectional influence with the surrounding pedestrians. This framework incorporates a Social Zonotope Network (SZN), a neural network that predicts future pedestrian reachable sets and plans future socially acceptable reachable set for the ego-agent. SZN generates the reachable sets as zonotopes for efficient reachability-based planning, collision checking, and online uncertainty parameterization. Locomotion-specific losses are added to the SZN training process to adhere to the dynamic limits of the bipedal robot that are not explicitly present in the human crowds data set. These loss functions enable the SZN to generate locomotion paths that are more dynamically feasible for improved tracking. SZN is integrated with a Model Predictive Controller (SZN-MPC) for footstep planning for our bipedal robot Digit. SZN-MPC solves for collision-free trajectory by optimizing through SZN's gradients. and Our results demonstrate the framework's effectiveness in producing a socially acceptable path, with consistent locomotion velocity, and optimality. The SZN-MPC framework is validated with extensive simulations and hardware experiments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "19 pages, 19 figures. arXiv admin note: text overlap with arXiv:2403.16485, arXiv:2310.09969"
    },
    {
        "paper id": "2406.17181",
        "abstract url": "https://arxiv.org/abs/2406.17181",
        "title": "FacePsy: An Open-Source Affective Mobile Sensing System -- Analyzing Facial Behavior and Head Gesture for Depression Detection in Naturalistic Settings",
        "rating": "-2",
        "keywords": [
            [
                "health",
                "healthcare",
                "Facial"
            ]
        ],
        "abstract": "Depression, a prevalent and complex mental health issue affecting millions worldwide, presents significant challenges for detection and monitoring. While facial expressions have shown promise in laboratory settings for identifying depression, their potential in real-world applications remains largely unexplored due to the difficulties in developing efficient mobile systems. In this study, we aim to introduce FacePsy, an open-source mobile sensing system designed to capture affective inferences by analyzing sophisticated features and generating real-time data on facial behavior landmarks, eye movements, and head gestures -- all within the naturalistic context of smartphone usage with 25 participants. Through rigorous development, testing, and optimization, we identified eye-open states, head gestures, smile expressions, and specific Action Units (2, 6, 7, 12, 15, and 17) as significant indicators of depressive episodes (AUROC=81%). Our regression model predicting PHQ-9 scores achieved moderate accuracy, with a Mean Absolute Error of 3.08. Our findings offer valuable insights and implications for enhancing deployable and usable mobile affective sensing systems, ultimately improving mental health monitoring, prediction, and just-in-time adaptive interventions for researchers and developers in healthcare.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Accepted to ACM International Conference on Mobile Human-Computer Interaction (MobileHCI 2024)"
    },
    {
        "paper id": "2406.17235",
        "abstract url": "https://arxiv.org/abs/2406.17235",
        "title": "Task-Agnostic Federated Learning",
        "rating": "-2",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "medical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the realm of medical imaging, leveraging large-scale datasets from various institutions is crucial for developing precise deep learning models, yet privacy concerns frequently impede data sharing. federated learning (FL) emerges as a prominent solution for preserving privacy while facilitating collaborative learning. However, its application in real-world scenarios faces several obstacles, such as task & data heterogeneity, label scarcity, non-identically distributed (non-IID) data, computational vaiation, etc. In real-world, medical institutions may not want to disclose their tasks to FL server and generalization challenge of out-of-network institutions with un-seen task want to join the on-going federated system. This study address task-agnostic and generalization problem on un-seen tasks by adapting self-supervised FL framework. Utilizing Vision Transformer (ViT) as consensus feature encoder for self-supervised pre-training, no initial labels required, the framework enabling effective representation learning across diverse datasets and tasks. Our extensive evaluations, using various real-world non-IID medical imaging datasets, validate our approach's efficacy, retaining 90\\% of F1 accuracy with only 5\\% of the training data typically required for centralized approaches and exhibiting superior adaptability to out-of-distribution task. The result indicate that federated learning architecture can be a potential approach toward multi-task foundation modeling.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17268",
        "abstract url": "https://arxiv.org/abs/2406.17268",
        "title": "Search-based Trace Diagnostic",
        "rating": "-2",
        "keywords": [
            [
                "Diagnosis"
            ]
        ],
        "abstract": "Cyber-physical systems (CPS) development requires verifying whether system behaviors violate their requirements. This analysis often considers system behaviors expressed by execution traces and requirements expressed by signal-based temporal properties. When an execution trace violates a requirement, engineers need to solve the trace diagnostic problem: They need to understand the cause of the breach. Automated trace diagnostic techniques aim to support engineers in the trace diagnostic activity. This paper proposes search-based trace-diagnostic (SBTD), a novel trace-diagnostic technique for CPS requirements. Unlike existing techniques, SBTD relies on evolutionary search. SBTD starts from a set of candidate diagnoses, applies an evolutionary algorithm iteratively to generate new candidate diagnoses (via mutation, recombination, and selection), and uses a fitness function to determine the qualities of these solutions. Then, a diagnostic generator step is performed to explain the cause of the trace violation. We implemented Diagnosis, an SBTD tool for signal-based temporal logic requirements expressed using the Hybrid Logic of Signals (HLS). We evaluated Diagnosis by performing 34 experiments for 17 trace-requirements combinations leading to a property violation and by assessing the effectiveness of SBTD in producing informative diagnoses and its efficiency in generating them on a time basis. Our results confirm that Diagnosis can produce informative diagnoses in practical time for most of our experiments (33 out of 34).",
        "subjects": [
            "cs.SE"
        ],
        "comment": "14 pages plus two for references"
    },
    {
        "paper id": "2406.18610",
        "abstract url": "https://arxiv.org/abs/2406.18610",
        "title": "Vox-UDA: Voxel-wise Unsupervised Domain Adaptation for Cryo-Electron Subtomogram Segmentation with Denoised Pseudo Labeling",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Voxel"
            ],
            [
                "biological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Cryo-Electron Tomography (cryo-ET) is a 3D imaging technology facilitating the study of macromolecular structures at near-atomic resolution. Recent volumetric segmentation approaches on cryo-ET images have drawn widespread interest in biological sector. However, existing methods heavily rely on manually labeled data, which requires highly professional skills, thereby hindering the adoption of fully-supervised approaches for cryo-ET images. Some unsupervised domain adaptation (UDA) approaches have been designed to enhance the segmentation network performance using unlabeled data. However, applying these methods directly to cryo-ET images segmentation tasks remains challenging due to two main issues: 1) the source data, usually obtained through simulation, contain a certain level of noise, while the target data, directly collected from raw-data from real-world scenario, have unpredictable noise levels. 2) the source data used for training typically consists of known macromoleculars, while the target domain data are often unknown, causing the model's segmenter to be biased towards these known macromolecules, leading to a domain shift problem. To address these challenges, in this work, we introduce the first voxel-wise unsupervised domain adaptation approach, termed Vox-UDA, specifically for cryo-ET subtomogram segmentation. Vox-UDA incorporates a noise generation module to simulate target-like noises in the source dataset for cross-noise level adaptation. Additionally, we propose a denoised pseudo-labeling strategy based on improved Bilateral Filter to alleviate the domain shift problem. Experimental results on both simulated and real cryo-ET subtomogram datasets demonstrate the superiority of our proposed approach compared to state-of-the-art UDA methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Under Reviewing"
    },
    {
        "paper id": "2406.16424",
        "abstract url": "https://arxiv.org/abs/2406.16424",
        "title": "Memory-Enhanced Neural Solvers for Efficient Adaptation in Combinatorial Optimization",
        "rating": "-2.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Combinatorial Optimization is crucial to numerous real-world applications, yet still presents challenges due to its (NP-)hard nature. Amongst existing approaches, heuristics often offer the best trade-off between quality and scalability, making them suitable for industrial use. While Reinforcement Learning (RL) offers a flexible framework for designing heuristics, its adoption over handcrafted heuristics remains incomplete within industrial solvers. Existing learned methods still lack the ability to adapt to specific instances and fully leverage the available computational budget. The current best methods either rely on a collection of pre-trained policies, or on data-inefficient fine-tuning; hence failing to fully utilize newly available information within the constraints of the budget. In response, we present MEMENTO, an RL approach that leverages memory to improve the adaptation of neural solvers at inference time. MEMENTO enables updating the action distribution dynamically based on the outcome of previous decisions. We validate its effectiveness on benchmark problems, in particular Traveling Salesman and Capacitated Vehicle Routing, demonstrating it can successfully be combined with standard methods to boost their performance under a given budget, both in and out-of-distribution, improving their performance on all 12 evaluated tasks.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17073",
        "abstract url": "https://arxiv.org/abs/2406.17073",
        "title": "Meta-GCN: A Dynamically Weighted Loss Minimization Method for Dealing with the Data Imbalance in Graph Neural Networks",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Although many real-world applications, such as disease prediction, and fault detection suffer from class imbalance, most existing graph-based classification methods ignore the skewness of the distribution of classes; therefore, tend to be biased towards the majority class(es). Conventional methods typically tackle this problem through the assignment of weights to each one of the class samples based on a function of their loss, which can lead to over-fitting on outliers. In this paper, we propose a meta-learning algorithm, named Meta-GCN, for adaptively learning the example weights by simultaneously minimizing the unbiased meta-data set loss and optimizing the model weights through the use of a small unbiased meta-data set. Through experiments, we have shown that Meta-GCN outperforms state-of-the-art frameworks and other baselines in terms of accuracy, the area under the receiver operating characteristic (AUC-ROC) curve, and macro F1-Score for classification tasks on two different datasets.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17086",
        "abstract url": "https://arxiv.org/abs/2406.17086",
        "title": "BrainMAE: A Region-aware Self-supervised Learning Framework for Brain Signals",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "fMRI"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The human brain is a complex, dynamic network, which is commonly studied using functional magnetic resonance imaging (fMRI) and modeled as network of Regions of interest (ROIs) for understanding various brain functions. Recent studies utilize deep learning approaches to learn the brain network representation based on functional connectivity (FC) profile, broadly falling into two main categories. The Fixed-FC approaches, utilizing the FC profile which represents the linear temporal relation within the brain network, are limited by failing to capture informative brain temporal dynamics. On the other hand, the Dynamic-FC approaches, modeling the evolving FC profile over time, often exhibit less satisfactory performance due to challenges in handling the inherent noisy nature of fMRI data. To address these challenges, we propose Brain Masked Auto-Encoder (BrainMAE) for learning representations directly from fMRI time-series data. Our approach incorporates two essential components: a region-aware graph attention mechanism designed to capture the relationships between different brain ROIs, and a novel self-supervised masked autoencoding framework for effective model pre-training. These components enable the model to capture rich temporal dynamics of brain activity while maintaining resilience to inherent noise in fMRI data. Our experiments demonstrate that BrainMAE consistently outperforms established baseline methods by significant margins in four distinct downstream tasks. Finally, leveraging the model's inherent interpretability, our analysis of model-generated representations reveals findings that resonate with ongoing research in the field of neuroscience.",
        "subjects": [
            "q-bio.QM",
            "cs.LG",
            "q-bio.NC"
        ],
        "comment": "27 pages, 16 figures"
    },
    {
        "paper id": "2406.17102",
        "abstract url": "https://arxiv.org/abs/2406.17102",
        "title": "Achieving Fairness Across Local and Global Models in Federated Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "healthcare"
            ],
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Achieving fairness across diverse clients in Federated Learning (FL) remains a significant challenge due to the heterogeneity of the data and the inaccessibility of sensitive attributes from clients' private datasets. This study addresses this issue by introducing \\texttt{EquiFL}, a novel approach designed to enhance both local and global fairness in federated learning environments. \\texttt{EquiFL} incorporates a fairness term into the local optimization objective, effectively balancing local performance and fairness. The proposed coordination mechanism also prevents bias from propagating across clients during the collaboration phase. Through extensive experiments across multiple benchmarks, we demonstrate that \\texttt{EquiFL} not only strikes a better balance between accuracy and fairness locally at each client but also achieves global fairness. The results also indicate that \\texttt{EquiFL} ensures uniform performance distribution among clients, thus contributing to performance fairness. Furthermore, we showcase the benefits of \\texttt{EquiFL} in a real-world distributed dataset from a healthcare application, specifically in predicting the effects of treatments on patients across various hospital locations.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17112",
        "abstract url": "https://arxiv.org/abs/2406.17112",
        "title": "Integrating Generative AI with Network Digital Twins for Enhanced Network Operations",
        "rating": "-2.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As telecommunications networks become increasingly complex, the integration of advanced technologies such as network digital twins and generative artificial intelligence (AI) emerges as a pivotal solution to enhance network operations and resilience. This paper explores the synergy between network digital twins, which provide a dynamic virtual representation of physical networks, and generative AI, particularly focusing on Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). We propose a novel architectural framework that incorporates these technologies to significantly improve predictive maintenance, network scenario simulation, and real-time data-driven decision-making. Through extensive simulations, we demonstrate how generative AI can enhance the accuracy and operational efficiency of network digital twins, effectively handling real-world complexities such as unpredictable traffic loads and network failures. The findings suggest that this integration not only boosts the capability of digital twins in scenario forecasting and anomaly detection but also facilitates a more adaptive and intelligent network management system.",
        "subjects": [
            "cs.LG",
            "cs.GR",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16369",
        "abstract url": "https://arxiv.org/abs/2406.16369",
        "title": "Machine Learning with Real-time and Small Footprint Anomaly Detection System for In-Vehicle Gateway",
        "rating": "-3",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "Anomaly Detection System (ADS) is an essential part of a modern gateway Electronic Control Unit (ECU) to detect abnormal behaviors and attacks in vehicles. Among the existing attacks, ``one-time`` attack is the most challenging to be detected, together with the strict gateway ECU constraints of both microsecond or even nanosecond level real-time budget and limited footprint of code. To address the challenges, we propose to use the self-information theory to generate values for training and testing models, aiming to achieve real-time detection performance for the ``one-time`` attack that has not been well studied in the past. Second, the generation of self-information is based on logarithm calculation, which leads to the smallest footprint to reduce the cost in Gateway. Finally, our proposed method uses an unsupervised model without the need of training data for anomalies or attacks. We have compared different machine learning methods ranging from typical machine learning models to deep learning models, e.g., Hidden Markov Model (HMM), Support Vector Data Description (SVDD), and Long Short Term Memory (LSTM). Experimental results show that our proposed method achieves 8.7 times lower False Positive Rate (FPR), 1.77 times faster testing time, and 4.88 times smaller footprint.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16370",
        "abstract url": "https://arxiv.org/abs/2406.16370",
        "title": "An Active Search Strategy with Multiple Unmanned Aerial Systems for Multiple Targets",
        "rating": "-3",
        "keywords": [
            [
                "trajectory",
                "flight"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "The challenge of efficient target searching in vast natural environments has driven the need for advanced multi-UAV active search strategies. This paper introduces a novel method in which global and local information is adeptly merged to avoid issues such as myopia and redundant back-and-forth movements. In addition, a trajectory generation method is used to ensure the search pattern within continuous space. To further optimize multi-agent cooperation, the Voronoi partition technique is employed, ensuring a reduction in repetitive flight patterns and making the control of multiple agents in a decentralized way. Through a series of experiments, the evaluation and comparison results demonstrate the efficiency of our approach in various environments. The primary application of this innovative approach is demonstrated in the search for horseshoe crabs within their wild habitats, showcasing its potential to revolutionize ecological survey and conservation efforts.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16380",
        "abstract url": "https://arxiv.org/abs/2406.16380",
        "title": "Testing Topological Data Analysis for Condition Monitoring of Wind Turbines",
        "rating": "-3",
        "keywords": [
            [
                "point cloud"
            ],
            [
                "health",
                "diagnosis"
            ]
        ],
        "abstract": "We present an investigation of how topological data analysis (TDA) can be applied to condition-based monitoring (CBM) of wind turbines for energy generation. TDA is a branch of data analysis focusing on extracting meaningful information from complex datasets by analyzing their structure in state space and computing their underlying topological features. By representing data in a high-dimensional state space, TDA enables the identification of patterns, anomalies, and trends in the data that may not be apparent through traditional signal processing methods. For this study, wind turbine data was acquired from a wind park in Norway via standard vibration sensors at different locations of the turbine's gearbox. Both the vibration acceleration data and its frequency spectra were recorded at infrequent intervals for a few seconds at high frequency and failure events were labelled as either gear-tooth or ball-bearing failures. The data processing and analysis are based on a pipeline where the time series data is first split into intervals and then transformed into multi-dimensional point clouds via a time-delay embedding. The shape of the point cloud is analyzed with topological methods such as persistent homology to generate topology-based key health indicators based on Betti numbers, information entropy and signal persistence. Such indicators are tested for CBM and diagnosis (fault detection) to identify faults in wind turbines and classify them accordingly. Topological indicators are shown to be an interesting alternative for failure identification and diagnosis of operational failures in wind turbines.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "To be published in the proceedings of the Annual Conference of the Prognostics and Health Management Society Style 2024"
    },
    {
        "paper id": "2406.16519",
        "abstract url": "https://arxiv.org/abs/2406.16519",
        "title": "Robust NLoS Localization in 5G mmWave Networks: Data-based Methods and Performance",
        "rating": "-3",
        "keywords": [
            [
                "flight"
            ],
            [
                "5G"
            ]
        ],
        "abstract": "Ensuring smooth mobility management while employing directional beamformed transmissions in 5G millimeter-wave networks calls for robust and accurate user equipment (UE) localization and tracking. In this article, we develop neural network-based positioning models with time- and frequency-domain channel state information (CSI) data in harsh non-line-of-sight (NLoS) conditions. We propose a novel frequency-domain feature extraction, which combines relative phase differences and received powers across resource blocks, and offers robust performance and reliability. Additionally, we exploit the multipath components and propose an aggregate time-domain feature combining time-of-flight, angle-of-arrival and received path-wise powers. Importantly, the temporal correlations are also harnessed in the form of sequence processing neural networks, which prove to be of particular benefit for vehicular UEs. Realistic numerical evaluations in large-scale line-of-sight (LoS)-obstructed urban environment with moving vehicles are provided, building on full ray-tracing based propagation modeling. The results show the robustness of the proposed CSI features in terms of positioning accuracy, and that the proposed models reliably localize UEs even in the absence of a LoS path, clearly outperforming the state-of-the-art with similar or even reduced processing complexity. The proposed sequence-based neural network model is capable of tracking the UE position, speed and heading simultaneously despite the strong uncertainties in the CSI measurements. Finally, it is shown that differences between the training and online inference environments can be efficiently addressed and alleviated through transfer learning.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "16 pages, 13 figures, manuscript currently in review with IEEE"
    },
    {
        "paper id": "2406.16671",
        "abstract url": "https://arxiv.org/abs/2406.16671",
        "title": "STAR: Swarm Technology for Aerial Robotics Research",
        "rating": "-3",
        "keywords": [
            [
                "Robotics"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "In recent years, the field of aerial robotics has witnessed significant progress, finding applications in diverse domains, including post-disaster search and rescue operations. Despite these strides, the prohibitive acquisition costs associated with deploying physical multi-UAV systems have posed challenges, impeding their widespread utilization in research endeavors. To overcome these challenges, we present STAR (Swarm Technology for Aerial Robotics Research), a framework developed explicitly to improve the accessibility of aerial swarm research experiments. Our framework introduces a swarm architecture based on the Crazyflie, a low-cost, open-source, palm-sized aerial platform, well suited for experimental swarm algorithms. To augment cost-effectiveness and mitigate the limitations of employing low-cost robots in experiments, we propose a landmark-based localization module leveraging fiducial markers. This module, also serving as a target detection module, enhances the adaptability and versatility of the framework. Additionally, collision and obstacle avoidance are implemented through velocity obstacles. The presented work strives to bridge the gap between theoretical advances and tangible implementations, thus fostering progress in the field.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16713",
        "abstract url": "https://arxiv.org/abs/2406.16713",
        "title": "ShanghaiTech Mapping Robot is All You Need: Robot System for Collecting Universal Ground Vehicle Datasets",
        "rating": "-3",
        "keywords": [
            [
                "RGB-D"
            ],
            [
                "autonomous driving",
                "Vehicle"
            ],
            [
                "robotics",
                "Robot"
            ]
        ],
        "abstract": "This paper presents the ShanghaiTech Mapping Robot, a state-of-the-art unmanned ground vehicle (UGV) designed for collecting comprehensive multi-sensor datasets to support research in robotics, computer vision, and autonomous driving. The robot is equipped with a wide array of sensors including RGB cameras, RGB-D cameras, event-based cameras, IR cameras, LiDARs, mmWave radars, IMUs, ultrasonic range finders, and a GNSS RTK receiver. The sensor suite is integrated onto a specially designed mechanical structure with a centralized power system and a synchronization mechanism to ensure spatial and temporal alignment of the sensor data. A 16-node on-board computing cluster handles sensor control, data collection, and storage. We describe the hardware and software architecture of the robot in detail and discuss the calibration procedures for the various sensors. The capabilities of the platform are demonstrated through an extensive dataset collected in diverse real-world environments. To facilitate research, we make the dataset publicly available along with the associated robot sensor calibration data. Performance evaluations on a set of standard perception and localization tasks showcase the potential of the dataset to support developments in Robot Autonomy.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Incomplete draft"
    },
    {
        "paper id": "2406.16804",
        "abstract url": "https://arxiv.org/abs/2406.16804",
        "title": "Comment on Chen et al.'s Authentication Protocol for Internet of Health Things",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Medical",
                "Health",
                "healthcare"
            ]
        ],
        "abstract": "The Internet of Medical Things has revolutionized the healthcare industry, enabling the seamless integration of connected medical devices and wearable sensors to enhance patient care and optimize healthcare services. However, the rapid adoption of the Internet of Medical Things also introduces significant security challenges that must be effectively addressed to preserve patient privacy, protect sensitive medical data, and ensure the overall reliability and safety of Internet of Medical Things systems. In this context, a key agreement protocol is used to securely establish shared cryptographic keys between interconnected medical devices and the central system, ensuring confidential and authenticated communication. Recently Chen et al. proposed a lightweight authentication and key agreement protocol for the Internet of health things. In this article, we provide a descriptive analysis of their proposed scheme and prove that Chen et al.'s scheme is vulnerable to Known session-specific temporary information attacks and stolen verifier attacks.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "5 pages, 3 figures, 1 table"
    },
    {
        "paper id": "2406.17076",
        "abstract url": "https://arxiv.org/abs/2406.17076",
        "title": "Avoiding Materialisation for Guarded Aggregate Queries",
        "rating": "-3",
        "keywords": [
            [
                "graph"
            ],
            [
                "SQL"
            ]
        ],
        "abstract": "Database systems are often confronted with queries that join many tables but ultimately only output comparatively small aggregate information. Despite all advances in query optimisation, the explosion of intermediate results as opposed to a much smaller final result challenges modern relational database management systems (DBMSs). In this work, we propose the integration of optimisation techniques into relational DBMSs that aim at minimising, and often entirely eliminating, the need for materialising join results for aggregate queries, provided that they satisfy certain conditions. Apart from novel logical optimisations aimed at practicability, we also provide new, natural, physical operators for combining joins and counting with the aim of reducing the size of intermediate results. We experimentally validate the efficacy of our optimisations through their implementation in Spark SQL, but we note that they are naturally applicable in any RDBMS. Our experiments show consistent significant speed-ups -- often by factor 2 and higher -- for analytical and graph queries. At the same time, we observe no performance degradation, even on queries which, from a theoretical point of view, are least amenable to the proposed optimisations.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17100",
        "abstract url": "https://arxiv.org/abs/2406.17100",
        "title": "Fine-tuning Diffusion Models for Enhancing Face Quality in Text-to-image Generation",
        "rating": "-3",
        "keywords": [
            [
                "Diffusion",
                "inpainting",
                "Text-to-image"
            ],
            [
                "trajectory"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models (DMs) have achieved significant success in generating imaginative images given textual descriptions. However, they are likely to fall short when it comes to real-life scenarios with intricate details.The low-quality, unrealistic human faces in text-to-image generation are one of the most prominent issues, hindering the wide application of DMs in practice. Targeting addressing such an issue, we first assess the face quality of generations from popular pre-trained DMs with the aid of human annotators and then evaluate the alignment between existing metrics such as ImageReward, Human Preference Score, Aesthetic Score Predictor, and Face Quality Assessment, with human judgments. Observing that existing metrics can be unsatisfactory for quantifying face quality, we develop a novel metric named Face Score (FS) by fine-tuning ImageReward on a dataset of (good, bad) face pairs cheaply crafted by an inpainting pipeline of DMs. Extensive studies reveal that FS enjoys a superior alignment with humans. On the other hand, FS opens up the door for refining DMs for better face generation. To achieve this, we incorporate a guidance loss on the denoising trajectories of the aforementioned face pairs for fine-tuning pre-trained DMs such as Stable Diffusion V1.5 and Realistic Vision V5.1. Intuitively, such a loss pushes the trajectory of bad faces toward that of good ones. Comprehensive experiments verify the efficacy of our approach for improving face quality while preserving general capability.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2406.17106",
        "abstract url": "https://arxiv.org/abs/2406.17106",
        "title": "Purely vision-based collective movement of robots",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "bio-inspired"
            ]
        ],
        "abstract": "Collective movement inspired by animal groups promises inherited benefits for robot swarms, such as enhanced sensing and efficiency. However, while animals move in groups using only their local senses, robots often obey central control or use direct communication, introducing systemic weaknesses to the swarm. In the hope of addressing such vulnerabilities, developing bio-inspired decentralized swarms has been a major focus in recent decades. Yet, creating robots that move efficiently together using only local sensory information remains an extraordinary challenge. In this work, we present a decentralized, purely vision-based swarm of terrestrial robots. Within this novel framework robots achieve collisionless, polarized motion exclusively through minimal visual interactions, computing everything on board based on their individual camera streams, making central processing or direct communication obsolete. With agent-based simulations, we further show that using this model, even with a strictly limited field of view and within confined spaces, ordered group motion can emerge, while also highlighting key limitations. Our results offer a multitude of practical applications from hybrid societies coordinating collective movement without any common communication protocol, to advanced, decentralized vision-based robot swarms capable of diverse tasks in ever-changing environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17134",
        "abstract url": "https://arxiv.org/abs/2406.17134",
        "title": "Musculoskeletal AutoEncoder: A Unified Online Acquisition Method of Intersensory Networks for State Estimation, Control, and Simulation of Musculoskeletal Humanoids",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "biomimetic"
            ]
        ],
        "abstract": "While the musculoskeletal humanoid has various biomimetic benefits, the modeling of its complex structure is difficult, and many learning-based systems have been developed so far. There are various methods, such as control methods using acquired relationships between joints and muscles represented by a data table or neural network, and state estimation methods using Extended Kalman Filter or table search. In this study, we construct a Musculoskeletal AutoEncoder representing the relationship among joint angles, muscle tensions, and muscle lengths, and propose a unified method of state estimation, control, and simulation of musculoskeletal humanoids using it. By updating the Musculoskeletal AutoEncoder online using the actual robot sensor information, we can continuously conduct more accurate state estimation, control, and simulation than before the online learning. We conducted several experiments using the musculoskeletal humanoid Musashi, and verified the effectiveness of this study.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at IEEE Robotics and Automation Letters"
    },
    {
        "paper id": "2406.17156",
        "abstract url": "https://arxiv.org/abs/2406.17156",
        "title": "Toward Ubiquitous 3D Object Digitization: A Wearable Computing Framework for Non-Invasive Physical Property Acquisition",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "Accurately digitizing physical objects is central to many applications, including virtual/augmented reality, industrial design, and e-commerce. Prior research has demonstrated efficient and faithful reconstruction of objects' geometric shapes and visual appearances, which suffice for digitally representing rigid objects. In comparison, physical properties, such as elasticity and pressure, are also indispensable to the behavioral fidelity of digitized deformable objects. However, existing approaches to acquiring these quantities either rely on invasive specimen collection or expensive/bulky laboratory setups, making them inapplicable to consumer-level usage. To fill this gap, we propose a wearable and non-invasive computing framework that allows users to conveniently estimate the material elasticity and internal pressure of deformable objects through finger touches. This is achieved by modeling their local surfaces as pressurized elastic shells and analytically deriving the two physical properties from finger-induced wrinkling patterns. Together with photogrammetry-reconstructed geometry and textures, the two estimated physical properties enable us to faithfully replicate the motion and deformation behaviors of several deformable objects. For the pressure estimation, our model achieves a relative error of 3.5%. In the interaction experiments, the virtual-physical deformation discrepancy measures less than 10.1%. Generalization to objects of irregular shape further demonstrates the potential of our approach in practical applications. We envision this work to provide insights for and motivate research toward democratizing the ubiquitous and pervasive digitization of our physical surroundings in daily, industrial, and scientific scenarios.",
        "subjects": [
            "cs.GR",
            "cs.HC"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2406.17173",
        "abstract url": "https://arxiv.org/abs/2406.17173",
        "title": "Diff3Dformer: Leveraging Slice Sequence Diffusion for Enhanced 3D CT Classification with Transformer Networks",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "medical",
                "CT"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The manifestation of symptoms associated with lung diseases can vary in different depths for individual patients, highlighting the significance of 3D information in CT scans for medical image classification. While Vision Transformer has shown superior performance over convolutional neural networks in image classification tasks, their effectiveness is often demonstrated on sufficiently large 2D datasets and they easily encounter overfitting issues on small medical image datasets. To address this limitation, we propose a Diffusion-based 3D Vision Transformer (Diff3Dformer), which utilizes the latent space of the Diffusion model to form the slice sequence for 3D analysis and incorporates clustering attention into ViT to aggregate repetitive information within 3D CT scans, thereby harnessing the power of the advanced transformer in 3D classification tasks on small datasets. Our method exhibits improved performance on two different scales of small datasets of 3D lung CT scans, surpassing the state of the art 3D methods and other transformer-based approaches that emerged during the COVID-19 pandemic, demonstrating its robust and superior performance across different scales of data. Experimental results underscore the superiority of our proposed method, indicating its potential for enhancing medical image classification tasks in real-world scenarios.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "conference"
    },
    {
        "paper id": "2406.17249",
        "abstract url": "https://arxiv.org/abs/2406.17249",
        "title": "SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "voxel"
            ],
            [
                "trajectory",
                "SLAM"
            ],
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "This paper develops a real-time decentralized metric-semantic Simultaneous Localization and Mapping (SLAM) approach that leverages a sparse and lightweight object-based representation to enable a heterogeneous robot team to autonomously explore 3D environments featuring indoor, urban, and forested areas without relying on GPS. We use a hierarchical metric-semantic representation of the environment, including high-level sparse semantic maps of object models and low-level voxel maps. We leverage the informativeness and viewpoint invariance of the high-level semantic map to obtain an effective semantics-driven place-recognition algorithm for inter-robot loop closure detection across aerial and ground robots with different sensing modalities. A communication module is designed to track each robot's observations and those of other robots within the communication range. Such observations are then used to construct a merged map. Our framework enables real-time decentralized operations onboard robots, allowing them to opportunistically leverage communication. We integrate and deploy our proposed framework on three types of aerial and ground robots. Extensive experimental results show an average localization error of 0.22 meters in position and -0.16 degrees in orientation, an object mapping F1 score of 0.92, and a communication packet size of merely 2-3 megabytes per kilometer trajectory with 1,000 landmarks. The project website can be found at https://xurobotics.github.io/slideslam/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Preliminary release"
    },
    {
        "paper id": "2406.17265",
        "abstract url": "https://arxiv.org/abs/2406.17265",
        "title": "Image-Guided Outdoor LiDAR Perception Quality Assessment for Autonomous Driving",
        "rating": "-3",
        "keywords": [
            [
                "point cloud"
            ],
            [
                "Autonomous Driving",
                "LiDAR",
                "vehicle"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "LiDAR is one of the most crucial sensors for autonomous vehicle perception. However, current LiDAR-based point cloud perception algorithms lack comprehensive and rigorous LiDAR quality assessment methods, leading to uncertainty in detection performance. Additionally, existing point cloud quality assessment algorithms are predominantly designed for indoor environments or single-object scenarios. In this paper, we introduce a novel image-guided point cloud quality assessment algorithm for outdoor autonomous driving environments, named the Image-Guided Outdoor Point Cloud Quality Assessment (IGO-PQA) algorithm. Our proposed algorithm comprises two main components. The first component is the IGO-PQA generation algorithm, which leverages point cloud data, corresponding RGB surrounding view images, and agent objects' ground truth annotations to generate an overall quality score for a single-frame LiDAR-based point cloud. The second component is a transformer-based IGO-PQA regression algorithm for no-reference outdoor point cloud quality assessment. This regression algorithm allows for the direct prediction of IGO-PQA scores in an online manner, without requiring image data and object ground truth annotations. We evaluate our proposed algorithm using the nuScenes and Waymo open datasets. The IGO-PQA generation algorithm provides consistent and reasonable perception quality indices. Furthermore, our proposed IGO-PQA regression algorithm achieves a Pearson Linear Correlation Coefficient (PLCC) of 0.86 on the nuScenes dataset and 0.97 on the Waymo dataset.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2406.17277",
        "abstract url": "https://arxiv.org/abs/2406.17277",
        "title": "Securing Voice Authentication Applications Against Targeted Data Poisoning",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "biometric"
            ]
        ],
        "abstract": "Deep neural network-based voice authentication systems are promising biometric verification techniques that uniquely identify biological characteristics to verify a user. However, they are particularly susceptible to targeted data poisoning attacks, where attackers replace legitimate users' utterances with their own. We propose an enhanced framework using realworld datasets considering realistic attack scenarios. The results show that the proposed approach is robust, providing accurate authentications even when only a small fraction (5% of the dataset) is poisoned.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16994",
        "abstract url": "https://arxiv.org/abs/2406.16994",
        "title": "Quantum Multi-Agent Reinforcement Learning for Cooperative Mobile Access in Space-Air-Ground Integrated Networks",
        "rating": "-3.5",
        "keywords": [
            [
                "UAV"
            ],
            [
                "Quantum"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Achieving global space-air-ground integrated network (SAGIN) access only with CubeSats presents significant challenges such as the access sustainability limitations in specific regions (e.g., polar regions) and the energy efficiency limitations in CubeSats. To tackle these problems, high-altitude long-endurance unmanned aerial vehicles (HALE-UAVs) can complement these CubeSat shortcomings for providing cooperatively global access sustainability and energy efficiency. However, as the number of CubeSats and HALE-UAVs, increases, the scheduling dimension of each ground station (GS) increases. As a result, each GS can fall into the curse of dimensionality, and this challenge becomes one major hurdle for efficient global access. Therefore, this paper provides a quantum multi-agent reinforcement Learning (QMARL)-based method for scheduling between GSs and CubeSats/HALE-UAVs in order to improve global access availability and energy efficiency. The main reason why the QMARL-based scheduler can be beneficial is that the algorithm facilitates a logarithmic-scale reduction in scheduling action dimensions, which is one critical feature as the number of CubeSats and HALE-UAVs expands. Additionally, individual GSs have different traffic demands depending on their locations and characteristics, thus it is essential to provide differentiated access services. The superiority of the proposed scheduler is validated through data-intensive experiments in realistic CubeSat/HALE-UAV settings.",
        "subjects": [
            "eess.SP",
            "cs.AI"
        ],
        "comment": "17 pages, 22 figures"
    },
    {
        "paper id": "2406.17147",
        "abstract url": "https://arxiv.org/abs/2406.17147",
        "title": "Quantifying Heterogeneous Ecosystem Services With Multi-Label Soft Classification",
        "rating": "-3.5",
        "keywords": [
            [
                "biodiversity"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Understanding and quantifying ecosystem services are crucial for sustainable environmental management, conservation efforts, and policy-making. The advancement of remote sensing technology and machine learning techniques has greatly facilitated this process. Yet, ground truth labels, such as biodiversity, are very difficult and expensive to measure. In addition, more easily obtainable proxy labels, such as land use, often fail to capture the complex heterogeneity of the ecosystem. In this paper, we demonstrate how land use proxy labels can be implemented with a soft, multi-label classifier to predict ecosystem services with complex heterogeneity.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "q-bio.QM"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2406.17812",
        "abstract url": "https://arxiv.org/abs/2406.17812",
        "title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
        "rating": "-3.5",
        "keywords": [
            [
                "medical"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In a post-ChatGPT world, this paper explores the potential of leveraging scalable artificial intelligence for scientific discovery. We propose that scaling up artificial intelligence on high-performance computing platforms is essential to address such complex problems. This perspective focuses on scientific use cases like cognitive simulations, large language models for scientific inquiry, medical image analysis, and physics-informed approaches. The study outlines the methodologies needed to address such challenges at scale on supercomputers or the cloud and provides exemplars of such approaches applied to solve a variety of scientific problems.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "comment": "17 pages, 5 figures"
    },
    {
        "paper id": "2406.16625",
        "abstract url": "https://arxiv.org/abs/2406.16625",
        "title": "GATSBI: An Online GTSP-Based Algorithm for Targeted Surface Bridge Inspection and Defect Detection",
        "rating": "-4",
        "keywords": [
            [
                "3D",
                "voxel"
            ],
            [
                "Vehicle"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "We study the problem of visual surface inspection of infrastructure for defects using an Unmanned Aerial Vehicle (UAV). We do not assume that the geometric model of the infrastructure is known beforehand. Our planner, termed GATSBI, plans a path in a receding horizon fashion to inspect all points on the surface of the infrastructure. The input to GATSBI consists of a 3D occupancy map created online with 3D pointclouds. Occupied voxels corresponding to the infrastructure in this map are semantically segmented and used to create an infrastructure-only occupancy map. Inspecting an infrastructure voxel requires the UAV to take images from a desired viewing angle and distance. We then create a Generalized Traveling Salesperson Problem (GTSP) instance to cluster candidate viewpoints for inspecting the infrastructure voxels and use an off-the-shelf GTSP solver to find the optimal path for the given instance. As the algorithm sees more parts of the environment over time, it replans the path to inspect uninspected parts of the infrastructure while avoiding obstacles. We evaluate the performance of our algorithm through high-fidelity simulations conducted in AirSim and real-world experiments. We compare the performance of GATSBI with a baseline inspection algorithm where the map is known a priori. Our evaluation reveals that targeting the inspection to only the segmented infrastructure voxels and planning carefully using a GTSP solver leads to a more efficient and thorough inspection than the baseline inspection algorithm.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 12 figures, 2 tables. Submitted to IEEE TAES. arXiv admin note: text overlap with arXiv:2012.04803"
    },
    {
        "paper id": "2406.16795",
        "abstract url": "https://arxiv.org/abs/2406.16795",
        "title": "Fuel-Optimal Formation Reconfiguration by Means of Unidirectional Low-Thrust Propulsion System",
        "rating": "-4",
        "keywords": [
            [
                "chemical"
            ],
            [
                "satellite"
            ]
        ],
        "abstract": "The use of electric low-thrust propulsion systems for orbit maneuvers is becoming a popular choice among satellite manufacturers due to their inherent merits over their chemical counterparts. Many designers choose to incorporate multiple of such thrusters to insure omnidirectional orbit maneuverability, while others choose to equip their satellite with only one thruster nozzle, aiming to reduce the required power, weight, and size of the orbit control system. This paper proposes guidance and control schemes to address the problem of autonomous optimal relative orbit reconfiguration for a formation of two satellites, one of which utilizes a single low-thrust throttleable nozzle. Such under-actuated orbit control system requires the controlled spacecraft to constantly slew to direct the nozzle to the desired thrust direction. These redirection attitude maneuvers are treated within the guidance layer by accommodating recurrent no-thrust periods during which slew maneuvers take place. The control loop is then closed with an MPC-like scheme. The main motivation of this article is to support the future missions of LuxSpace's flagship satellite, Triton-X. Since the proposed guidance and control schemes are meant to answer realistic market needs, they are designed to have some specific qualities that makes them attractive from the practical point of view. Namely, they require minimal computational loads, besides being able to accommodate operational time constraints, e.g. no thrusting during eclipse or during ground contact, within the guidance layer.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16837",
        "abstract url": "https://arxiv.org/abs/2406.16837",
        "title": "A Certifiable Algorithm for Simultaneous Shape Estimation and Object Tracking",
        "rating": "-4",
        "keywords": [
            [
                "3D",
                "RGB-D"
            ],
            [
                "vehicle"
            ],
            [
                "drone"
            ]
        ],
        "abstract": "Applications from manipulation to autonomous vehicles rely on robust and general object tracking to safely perform tasks in dynamic environments. We propose the first certifiably optimal category-level approach for simultaneous shape estimation and pose tracking of an object of known category (e.g. a car). Our approach uses 3D semantic keypoint measurements extracted from an RGB-D image sequence, and phrases the estimation as a fixed-lag smoothing problem. Temporal constraints enforce the object's rigidity (fixed shape) and smooth motion according to a constant-twist motion model. The solutions to this problem are the estimates of the object's state (poses, velocities) and shape (paramaterized according to the active shape model) over the smoothing horizon. Our key contribution is to show that despite the non-convexity of the fixed-lag smoothing problem, we can solve it to certifiable optimality using a small-size semidefinite relaxation. We also present a fast outlier rejection scheme that filters out incorrect keypoint detections with shape and time compatibility tests, and wrap our certifiable solver in a graduated non-convexity scheme. We evaluate the proposed approach on synthetic and real data, showcasing its performance in a table-top manipulation scenario and a drone-based vehicle tracking application.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "11 pages, 6 figures (with appendix). Code released at https://github.com/MIT-SPARK/certifiable_tracking. Video available at https://youtu.be/eTIlVD9pDtc"
    },
    {
        "paper id": "2406.17251",
        "abstract url": "https://arxiv.org/abs/2406.17251",
        "title": "TopoGCL: Topological Graph Contrastive Learning",
        "rating": "-4.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "biological"
            ],
            [
                "chemical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph contrastive learning (GCL) has recently emerged as a new concept which allows for capitalizing on the strengths of graph neural networks (GNNs) to learn rich representations in a wide variety of applications which involve abundant unlabeled information. However, existing GCL approaches largely tend to overlook the important latent information on higher-order graph substructures. We address this limitation by introducing the concepts of topological invariance and extended persistence on graphs to GCL. In particular, we propose a new contrastive mode which targets topological representations of the two augmented views from the same graph, yielded by extracting latent shape properties of the graph at multiple resolutions. Along with the extended topological layer, we introduce a new extended persistence summary, namely, extended persistence landscapes (EPL) and derive its theoretical stability guarantees. Our extensive numerical results on biological, chemical, and social interaction graphs show that the new Topological Graph Contrastive Learning (TopoGCL) model delivers significant performance gains in unsupervised graph classification for 11 out of 12 considered datasets and also exhibits robustness under noisy scenarios.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16772",
        "abstract url": "https://arxiv.org/abs/2406.16772",
        "title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?",
        "rating": "-5",
        "keywords": [
            [
                "Biology"
            ],
            [
                "Chemistry"
            ],
            [
                "Physics"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In this report, we pose the following question: Who is the most intelligent AI model to date, as measured by the OlympicArena (an Olympic-level, multi-discipline, multi-modal benchmark for superintelligent AI)? We specifically focus on the most recently released models: Claude-3.5-Sonnet, Gemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic medal Table approach to rank AI models based on their comprehensive performance across various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet shows highly competitive overall performance over GPT-4o, even surpassing GPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2) Gemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and Claude-3.5-Sonnet, but with a clear performance gap between them. (3) The performance of AI models from the open-source community significantly lags behind these proprietary models. (4) The performance of these models on this benchmark has been less than satisfactory, indicating that we still have a long way to go before achieving superintelligence. We remain committed to continuously tracking and evaluating the performance of the latest powerful models on this benchmark (available at https://github.com/GAIR-NLP/OlympicArena).",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2406.17172",
        "abstract url": "https://arxiv.org/abs/2406.17172",
        "title": "Robust Zero Trust Architecture: Joint Blockchain based Federated learning and Anomaly Detection based Framework",
        "rating": "-6.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "attacks"
            ],
            [
                "IoT"
            ],
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a robust zero-trust architecture (ZTA) tailored for the decentralized system that empowers efficient remote work and collaboration within IoT networks. Using blockchain-based federated learning principles, our proposed framework includes a robust aggregation mechanism designed to counteract malicious updates from compromised clients, enhancing the security of the global learning process. Moreover, secure and reliable trust computation is essential for remote work and collaboration. The robust ZTA framework integrates anomaly detection and trust computation, ensuring secure and reliable device collaboration in a decentralized fashion. We introduce an adaptive algorithm that dynamically adjusts to varying user contexts, using unsupervised clustering to detect novel anomalies, like zero-day attacks. To ensure a reliable and scalable trust computation, we develop an algorithm that dynamically adapts to varying user contexts by employing incremental anomaly detection and clustering techniques to identify and share local and global anomalies between nodes. Future directions include scalability improvements, Dirichlet process for advanced anomaly detection, privacy-preserving techniques, and the integration of post-quantum cryptographic methods to safeguard against emerging quantum threats.",
        "subjects": [
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16323",
        "abstract url": "https://arxiv.org/abs/2406.16323",
        "title": "Low-Complexity CSI Feedback for FDD Massive MIMO Systems via Learning to Optimize",
        "rating": "-10",
        "keywords": [],
        "abstract": "In frequency-division duplex (FDD) massive multiple-input multiple-output (MIMO) systems, the growing number of base station antennas leads to prohibitive feedback overhead for downlink channel state information (CSI). To address this challenge, state-of-the-art (SOTA) fully data-driven deep learning (DL)-based CSI feedback schemes have been proposed. However, the high computational complexity and memory requirements of these methods hinder their practical deployment on resource-constrained devices like mobile phones. To solve the problem, we propose a model-driven DL-based CSI feedback approach by integrating the wisdom of compressive sensing and learning to optimize (L2O). Specifically, only a linear learnable projection is adopted at the encoder side to compress the CSI matrix, thereby significantly cutting down the user-side complexity and memory expenditure. On the other hand, the decoder incorporates two specially designed components, i.e., a learnable sparse transformation and an element-wise L2O reconstruction module. The former is developed to learn a sparse basis for CSI within the angular domain, which explores channel sparsity effectively. The latter shares the same long short term memory (LSTM) network across all elements of the optimization variable, eliminating the retraining cost when problem scale changes. Simulation results show that the proposed method achieves a comparable performance with the SOTA CSI feedback scheme but with much-reduced complexity, and enables multiple-rate feedback.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "submitted to IEEE for publication"
    },
    {
        "paper id": "2406.16328",
        "abstract url": "https://arxiv.org/abs/2406.16328",
        "title": "Convolutional neural network based reduced order modeling for multiscale problems",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we combine convolutional neural networks (CNNs) with reduced order modeling (ROM) for efficient simulations of multiscale problems. These problems are modeled by partial differential equations with high-dimensional random inputs. The proposed method involves two separate CNNs: Basis CNNs and Coefficient CNNs (Coef CNNs), which correspond to two main parts of ROM. The method is called CNN-based ROM. The former one learns input-specific basis functions from the snapshots of fine-scale solutions. An activation function, inspired by Galerkin projection, is utilized at the output layer to reconstruct fine-scale solutions from the basis functions. Numerical results show that the basis functions learned by the Basis CNNs resemble data, which help to significantly reduce the number of the basis functions. Moreover, CNN-based ROM is less sensitive to data fluctuation caused by numerical errors than traditional ROM. Since the tests of Basis CNNs still need fine-scale stiffness matrix and load vector, it can not be directly applied to nonlinear problems. The Coef CNNs can be applied to nonlinear problems and designed to determine the coefficients for linear combination of basis functions. In addition, two applications of CNN-based ROM are presented, including predicting MsFEM basis functions within oversampling regions and building accurate surrogates for inverse problems.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "35 pages, 29 figures"
    },
    {
        "paper id": "2406.16343",
        "abstract url": "https://arxiv.org/abs/2406.16343",
        "title": "Simple Delegated Choice",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper studies delegation in a model of discrete choice. In the delegation problem, an uninformed principal must consult an informed agent to make a decision. Both the agent and principal have preferences over the decided-upon action which vary based on the state of the world, and which may not be aligned. The principal may commit to a mechanism, which maps reports of the agent to actions. When this mechanism is deterministic, it can take the form of a menu of actions, from which the agent simply chooses upon observing the state. In this case, the principal is said to have delegated the choice of action to the agent. We consider a setting where the decision being delegated is a choice of a utility-maximizing action from a set of several options. We assume the shared portion of the agent's and principal's utilities is drawn from a distribution known to the principal, and that utility misalignment takes the form of a known bias for or against each action. We provide tight approximation analyses for simple threshold policies under three increasingly general sets of assumptions. With independently-distributed utilities, we prove a $3$-approximation. When the agent has an outside option the principal cannot rule out, the constant approximation fails, but we prove a $\\log \u03c1/\\log\\log \u03c1$-approximation, where $\u03c1$ is the ratio of the maximum value to the optimal utility. We also give a weaker but tight bound that holds for correlated values, and complement our upper bounds with hardness results. One special case of our model is utility-based assortment optimization, for which our results are new.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16347",
        "abstract url": "https://arxiv.org/abs/2406.16347",
        "title": "VulZoo: A Comprehensive Vulnerability Intelligence Dataset",
        "rating": "-10",
        "keywords": [],
        "abstract": "Software vulnerabilities pose critical security and risk concerns for many software systems. Many techniques have been proposed to effectively assess and prioritize these vulnerabilities before they cause serious consequences. To evaluate their performance, these solutions often craft their own experimental datasets from limited information sources, such as MITRE CVE and NVD, lacking a global overview of broad vulnerability intelligence. The repetitive data preparation process further complicates the verification and comparison of new solutions. To resolve this issue, in this paper, we propose VulZoo, a comprehensive vulnerability intelligence dataset that covers 17 popular vulnerability information sources. We also construct connections among these sources, enabling more straightforward configuration and adaptation for different vulnerability assessment tasks (e.g., vulnerability type prediction). Additionally, VulZoo provides utility scripts for automatic data synchronization and cleaning, relationship mining, and statistics generation. We make VulZoo publicly available and maintain it with incremental updates to facilitate future research. We believe that VulZoo serves as a valuable input to vulnerability assessment and prioritization studies. The dataset with utility scripts is available at https://github.com/NUS-Curiosity/VulZoo.",
        "subjects": [
            "cs.CR",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16348",
        "abstract url": "https://arxiv.org/abs/2406.16348",
        "title": "Acknowledging Good Java Code with Code Perfumes",
        "rating": "-10",
        "keywords": [],
        "abstract": "Java remains one of the most popular programming languages in education. Although Java programming education is well supported by study materials, learners also need more immediate support on the problems they face in their own code. When this support cannot be offered by educators personally, learners can resort to automated program analysis tools such as linters, which provide feedback on potential bugs or code issues. This is constructive feedback, but it may nevertheless feel like criticism. This paper introduces code perfumes for Java, a simple program analysis technique similar to linting, but commending the correct application of good programming practices. We present a catalogue of 20 Java code perfumes related to common Java language constructs for beginner to immediate learners. Our evaluation shows that these code perfumes occur frequently in learners' code, and programs with more code perfume instances tend to have better functionality and readability. Moreover, students who incorporate more code perfumes tend to achieve higher grades. Thus, code perfumes serve as a valuable tool to acknowledge learners' successes, and as a means to inform instructors about their learners' progress.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16350",
        "abstract url": "https://arxiv.org/abs/2406.16350",
        "title": "A Survey on Intent-aware Recommender Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many modern online services feature personalized recommendations. A central challenge when providing such recommendations is that the reason why an individual user accesses the service may change from visit to visit or even during an ongoing usage session. To be effective, a recommender system should therefore aim to take the users' probable intent of using the service at a certain point in time into account. In recent years, researchers have thus started to address this challenge by incorporating intent-awareness into recommender systems. Correspondingly, a number of technical approaches were put forward, including diversification techniques, intent prediction models or latent intent modeling approaches. In this paper, we survey and categorize existing approaches to building the next generation of Intent-Aware Recommender Systems (IARS). Based on an analysis of current evaluation practices, we outline open gaps and possible future directions in this area, which in particular include the consideration of additional interaction signals and contextual information to further improve the effectiveness of such systems.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16367",
        "abstract url": "https://arxiv.org/abs/2406.16367",
        "title": "On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "Retrieval augmented generation (RAG) exhibits outstanding performance in promoting the knowledge capabilities of large language models (LLMs) with retrieved documents related to user queries. However, RAG only focuses on improving the response quality of LLMs via enhancing queries indiscriminately with retrieved information, paying little attention to what type of knowledge LLMs really need to answer original queries more accurately. In this paper, we suggest that long-tail knowledge is crucial for RAG as LLMs have already remembered common world knowledge during large-scale pre-training. Based on our observation, we propose a simple but effective long-tail knowledge detection method for LLMs. Specifically, the novel Generative Expected Calibration Error (GECE) metric is derived to measure the ``long-tailness'' of knowledge based on both statistics and semantics. Hence, we retrieve relevant documents and infuse them into the model for patching knowledge loopholes only when the input query relates to long-tail knowledge. Experiments show that, compared to existing RAG pipelines, our method achieves over 4x speedup in average inference time and consistent performance improvement in downstream tasks.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16383",
        "abstract url": "https://arxiv.org/abs/2406.16383",
        "title": "Context-augmented Retrieval: A Novel Framework for Fast Information Retrieval based Response Generation using Large Language Model",
        "rating": "-10",
        "keywords": [],
        "abstract": "Generating high-quality answers consistently by providing contextual information embedded in the prompt passed to the Large Language Model (LLM) is dependent on the quality of information retrieval. As the corpus of contextual information grows, the answer/inference quality of Retrieval Augmented Generation (RAG) based Question Answering (QA) systems declines. This work solves this problem by combining classical text classification with the Large Language Model (LLM) to enable quick information retrieval from the vector store and ensure the relevancy of retrieved information. For the same, this work proposes a new approach Context Augmented retrieval (CAR), where partitioning of vector database by real-time classification of information flowing into the corpus is done. CAR demonstrates good quality answer generation along with significant reduction in information retrieval and answer generation time.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16391",
        "abstract url": "https://arxiv.org/abs/2406.16391",
        "title": "Self-descriptive Sequences directed by two Periodic Sequences",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the present work, we exhibit a class of self-descriptive sequences that can be explicitly computed and whose frequencies are known. In particular, as a corollary of our main result, we prove that the sequence introduced in \\citeBJM23 has the expected frequencies of occurrences.",
        "subjects": [
            "cs.FL",
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16392",
        "abstract url": "https://arxiv.org/abs/2406.16392",
        "title": "Interval Posets and Polygon Dissections",
        "rating": "-10",
        "keywords": [],
        "abstract": "The Interval poset of a permutation is an effective way of capturing all the intervals of the permutation and the inclusions between them and was introduced recently by Tenner. Thi paper explores the geometric interpretation of interval posets of permutations. We present a bijection between tree interval posets and convex polygons with non-crossing diagonals, offering a novel geometric perspective on this purely combinatorial concept. Additionally, we provide an enumeration of interval posets using this bijection and demonstrate its application to block-wise simple permutations.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16393",
        "abstract url": "https://arxiv.org/abs/2406.16393",
        "title": "Type-B analogue of Bell numbers using Rota's Umbral calculus approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "Rota used the functional L to recover old properties and obtain some new formulas for the Bell numbers. Tanny used Rota's functional L and the celebrated Worpitzky identity to obtain some expression for the ordered Bell numbers, which can be seen as an evident to the fact that the ordered Bell numbers are gamma-positive. In this paper, we extend some of Rota's and Tanny's results to the framework of the set partitions of Coxeter type B.",
        "subjects": [
            "cs.DM",
            "math.CO"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16394",
        "abstract url": "https://arxiv.org/abs/2406.16394",
        "title": "Dyck Paths Enumerated by the Q-bonacci Numbers",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider Dyck paths having height at most two with some constraints on the number of consecutive valleys at height one which must be followed by a suitable number of valleys at height zero. We prove that they are enumerated by so-called Q-bonacci numbers (recently introduced by Kirgizov) which generalize the classical q-bonacci numbers in the case where q is a positive rational.",
        "subjects": [
            "cs.DM",
            "math.CO"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16396",
        "abstract url": "https://arxiv.org/abs/2406.16396",
        "title": "Optimal Generation of Strictly Increasing Binary Trees and Beyond",
        "rating": "-10",
        "keywords": [],
        "abstract": "This article presents two novel algorithms for generating random increasing trees. The first algorithm efficiently generates strictly increasing binary trees using an ad hoc method. The second algorithm improves the recursive method for weighted strictly increasing unary-binary increasing trees, optimizing randomness usage.",
        "subjects": [
            "cs.DS",
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16399",
        "abstract url": "https://arxiv.org/abs/2406.16399",
        "title": "Pop Stacks with a Bypass",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider sorting procedures for permutations making use of pop stacks with a bypass operation, and explore the combinatorial properties of the associated algorithms.",
        "subjects": [
            "cs.DM",
            "cs.DS"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16402",
        "abstract url": "https://arxiv.org/abs/2406.16402",
        "title": "Square-Triangle Tilings: Lift & Flip to Sample?",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce an elementary transformation called flips on tilings by squares and triangles and conjecture that it connects any two tilings of the same region of the Euclidean plane.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16403",
        "abstract url": "https://arxiv.org/abs/2406.16403",
        "title": "Restricted Permutations Enumerated by Inversions",
        "rating": "-10",
        "keywords": [],
        "abstract": "Permutations are usually enumerated by size, but new results can be found by enumerating them by inversions instead, in which case one must restrict one's attention to indecomposable permutations. In the style of the seminal paper by Simion and Schmidt, we investigate all combinations of permutation patterns of length at most 3.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16405",
        "abstract url": "https://arxiv.org/abs/2406.16405",
        "title": "Greedy Gray Codes for some Restricted Classes of Binary Words",
        "rating": "-10",
        "keywords": [],
        "abstract": "We investigate the existence of greedy Gray codes, based on the choice of the first element in the code, for two classes of binary words: generalized Fibonacci words and generalized Dyck words.",
        "subjects": [
            "cs.DM",
            "cs.DS",
            "math.CO"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16406",
        "abstract url": "https://arxiv.org/abs/2406.16406",
        "title": "Morphic Sequences: Complexity and Decidability",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this work we recall Pansiot's result on the complexity of pure morphic sequences and we use the tools developed by Devyatov for morphic sequences to prove the decidability of the complexity class of pure morphic sequences.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16407",
        "abstract url": "https://arxiv.org/abs/2406.16407",
        "title": "Detecting Isohedral Polyforms with a SAT Solver",
        "rating": "-10",
        "keywords": [],
        "abstract": "I show how to express the question of whether a polyform tiles the plane isohedrally as a Boolean formula that can be tested using a SAT solver. This approach is adaptable to a wide range of polyforms, requires no special-case code for different isohedral tiling types, and integrates seamlessly with existing software for computing Heesch numbers of polyforms.",
        "subjects": [
            "cs.DM",
            "cs.CG"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16409",
        "abstract url": "https://arxiv.org/abs/2406.16409",
        "title": "Combinatorics on Social Configurations",
        "rating": "-10",
        "keywords": [],
        "abstract": "In cooperative game theory, the social configurations of players are modeled by balanced collections. The Bondareva-Shapley theorem, perhaps the most fundamental theorem in cooperative game theory, characterizes the existence of solutions to the game that benefit everyone using balanced collections. Roughly speaking, if the trivial set system of all players is one of the most efficient balanced collections for the game, then the set of solutions from which each coalition benefits, the so-called core, is non-empty. In this paper, we discuss some interactions between combinatorics and cooperative game theory that are still relatively unexplored. Indeed, the similarity between balanced collections and uniform hypergraphs seems to be a relevant point of view to obtain new properties on those collections through the theory of combinatorial species.",
        "subjects": [
            "cs.GT",
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16411",
        "abstract url": "https://arxiv.org/abs/2406.16411",
        "title": "On the Orthogonality of Generalized Pattern Sequences",
        "rating": "-10",
        "keywords": [],
        "abstract": "The partial sums of integer sequences that count the occurrences of a specific pattern in the binary expansion of positive integers have been investigated by different authors since the 1950s. In this note, we introduce generalized pattern sequences, which count the occurrences of a finite number of different patterns in the expansion of positive integers in any integer base, and analyze their partial sums.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16413",
        "abstract url": "https://arxiv.org/abs/2406.16413",
        "title": "Counting Polyominoes in a Rectangle b x h",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we provide methods to automatically obtain automata that generate polyominoes inscribed in a rectangle of fixed width and increasing height. We use them to obtain the generating function of those sequences for small widths.",
        "subjects": [
            "cs.DM",
            "cs.FL"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16417",
        "abstract url": "https://arxiv.org/abs/2406.16417",
        "title": "A Bijection between Stacked Directed Polyominoes and Motzkin Paths with Alternative Catastrophes",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a novel bijection between stacked directed polyominoes and Motzkin paths with alternative catastrophes. Further, we show how this new connection can be used in order to obtain a better understanding of certain parameters of stacked directed animals.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16418",
        "abstract url": "https://arxiv.org/abs/2406.16418",
        "title": "Natural Measures on Polyominoes Induced by the Abelian Sandpile Model",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce a natural Boltzmann measure over polyominoes induced by boundary avalanches in the Abelian Sandpile Model. Through the study of a suitable associated process, we give an argument suggesting that the probability distribution of the avalnche sizes has a power-law decay with exponent 3/2, in contrast with the present understanding of bulk avalanches in the model (which has some exponent between 1 and 5/4), and to the ordinary generating function of polyominoes (which is conjectured to have a logarithmic singularity, i.e. exponent 1). We provide some numerical evidence for our claims, and evaluate some other statistical observables on our process, most notably the density of triple points.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16419",
        "abstract url": "https://arxiv.org/abs/2406.16419",
        "title": "Construction of Minkowski Sums by Cellular Automata",
        "rating": "-10",
        "keywords": [],
        "abstract": "We give a construction in a column of a one-dimensional cellular automaton of the Minkowski sum of two sets which can themselves occur in columns of cellular automata. It enables us to obtain another construction of the set of integers that are sums of three squares, answering a question by the same author.",
        "subjects": [
            "cs.DM",
            "cs.FL"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16420",
        "abstract url": "https://arxiv.org/abs/2406.16420",
        "title": "Local Limit Theorems for $q$-Multinomial and Multiple Heine Distributions",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this work we establish local limit theorems for q-multinomial and multiple Heine distributions. Specifically, the pointwise convergence of the q-multinomial distribution of the first kind, as well as for its discrete limit, the multiple Heine distribution, to a multivariate Stieltjes-Wigert type distribution, are provided.",
        "subjects": [
            "cs.DM"
        ],
        "comment": "In Proceedings GASCom 2024, arXiv:2406.14588"
    },
    {
        "paper id": "2406.16444",
        "abstract url": "https://arxiv.org/abs/2406.16444",
        "title": "Enumeration of Row-Column Designs",
        "rating": "-10",
        "keywords": [],
        "abstract": "We computationally completely enumerate a number of types of row-column designs up to isotopism, including double, sesqui and triple arrays as known from the literature, and two newly introduced types that we call mono arrays and AO-arrays. We calculate autotopism group sizes for the designs we generate. For larger parameter values, where complete enumeration is not feasible, we generate examples of some of the designs, and generate exhaustive lists of admissible parameters. For some admissible parameter sets, we prove non-existence results. We also give some explicit constructions of sesqui arrays, mono arrays and AO-arrays, and investigate connections to Youden rectangles and binary pseud Youden designs.",
        "subjects": [
            "math.CO",
            "cs.DM",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16447",
        "abstract url": "https://arxiv.org/abs/2406.16447",
        "title": "Transient Evaluation of Non-Markovian Models by Stochastic State Classes and Simulation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Non-Markovian models have great expressive power, at the cost of complex analysis of the stochastic process. The method of Stochastic State Classes (SSCs) derives closed-form analytical expressions for the joint Probability Density Functions (PDFs) of the active timers with marginal expolynomial PDF, though being hindered by the number of concurrent non-exponential timers and of discrete events between regenerations. Simulation is an alternative capable of handling the large class of PDFs samplable via inverse transform, which however suffers from rare events. We combine these approaches to analyze time-bounded transient properties of non-Markovian models. We enumerate SSCs near the root of the state-space tree and then rely on simulation to reach the target, affording transient evaluation of models for which the method of SSCs is not viable while reducing computational time and variance of the estimator of transient probabilities with respect to simulation. Promising results are observed in the estimation of rare event probabilities.",
        "subjects": [
            "cs.LO",
            "math.NA"
        ],
        "comment": "Accepted at QEST+FORMATS 2024"
    },
    {
        "paper id": "2406.16452",
        "abstract url": "https://arxiv.org/abs/2406.16452",
        "title": "A Queuing Envelope Model for Estimating Latency Guarantees in Deterministic Networking Scenarios",
        "rating": "-10",
        "keywords": [],
        "abstract": "Accurate estimation of queuing delays is crucial for designing and optimizing communication networks, particularly in the context of Deterministic Networking (DetNet) scenarios. This study investigates the approximation of Internet queuing delays using an M/M/1 envelope model, which provides a simple methodology to find tight upper bounds of real delay percentiles. Real traffic statistics collected at large Internet Exchange Points (like Amsterdam and San Francisco) have been used to fit polynomial regression models for transforming packet queuing delays into the M/M/1 envelope models. We finally propose a methodology for providing delay percentiles in DetNet scenarios where tight latency guarantees need to be assured.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16475",
        "abstract url": "https://arxiv.org/abs/2406.16475",
        "title": "Bijective BWT based compression schemes",
        "rating": "-10",
        "keywords": [],
        "abstract": "We show that for any string $w$ of length $n$, $r_B = O(z\\log^2 n)$, where $r_B$ and $z$ are respectively the number of character runs in the bijective Burrows-Wheeler transform (BBWT), and the number of Lempel-Ziv 77 factors of $w$. We can further induce a bidirectional macro scheme of size $O(r_B)$ from the BBWT. Finally, there exists a family of strings with $r_B = \u03a9(\\log n)$ but having only $r=2$ character runs in the standard Burrows--Wheeler transform (BWT). However, a lower bound for $r$ is the minimal run-length of the BBWTs applied to the cyclic shifts of $w$, whose time complexity might be $o(n^2)$ in the light that we show how to compute the Lyndon factorization of all cyclic rotations in $O(n)$ time. Considering also the rotation operation performing cyclic shifts, we conjecture that we can transform two strings having the same Parikh vector to each other by BBWT and rotation operations, and prove this conjecture for the case of binary alphabets and permutations.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16496",
        "abstract url": "https://arxiv.org/abs/2406.16496",
        "title": "Recent advancements on MPC for tracking: periodic and harmonic formulations",
        "rating": "-10",
        "keywords": [],
        "abstract": "The main benefit of model predictive control (MPC) is its ability to steer the system to a given reference without violating the constraints while minimizing some objective. Furthermore, a suitably designed MPC controller guarantees asymptotic stability of the closed-loop system to the given reference as long as its optimization problem is feasible at the initial state of the system. Therefore, one of the limitations of classical MPC is that changing the reference may lead to an unfeasible MPC problem. Furthermore, due to a lack of deep knowledge of the system, it is possible for the user to provide a desired reference that is unfeasible or non-attainable for the MPC controller, leading to the same problem. This chapter summarizes MPC formulations recently proposed that have been designed to address these issues. In particular, thanks to the addition of an artificial reference as decision variable, the formulations achieve asymptotic stability and recursive feasibility guarantees regardless of the reference provided by the user, even if it is changed online or if it violates the system constraints. We show a recent formulation which extends this idea, achieving better performance and larger domains of attraction when working with small prediction horizons. Additional benefits of these formulations, when compared to classical MPC, are also discussed and highlighted with illustrative examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "(21 pages, 8 figures)"
    },
    {
        "paper id": "2406.16500",
        "abstract url": "https://arxiv.org/abs/2406.16500",
        "title": "A Dual-Channel Particle Swarm Optimization Algorithm Based on Adaptive Balance Search",
        "rating": "-10",
        "keywords": [],
        "abstract": "The balance between exploration (Er) and exploitation (Ei) determines the generalization performance of the particle swarm optimization (PSO) algorithm on different problems. Although the insufficient balance caused by global best being located near a local minimum has been widely researched, few scholars have systematically paid attention to two behaviors about personal best position (P) and global best position (G) existing in PSO. 1) P's uncontrollable-exploitation and involuntary-exploration guidance behavior. 2) G's full-time and global guidance behavior, each of which negatively affects the balance of Er and Ei. With regards to this, we firstly discuss the two behaviors, unveiling the mechanisms by which they affect the balance, and further pinpoint three key points for better balancing Er and Ei: eliminating the coupling between P and G, empowering P with controllable-exploitation and voluntary-exploration guidance behavior, controlling G's full-time and global guidance behavior. Then, we present a dual-channel PSO algorithm based on adaptive balance search (DCPSO-ABS). This algorithm entails a dual-channel framework to mitigate the interaction of P and G, aiding in regulating the behaviors of P and G, and meanwhile an adaptive balance search strategy for empowering P with voluntary-exploration and controllable-exploitation guidance behavior as well as adaptively controlling G's full-time and global guidance behavior. Finally, three kinds of experiments on 57 benchmark functions are designed to demonstrate that our proposed algorithm has stronger generalization performance than selected state-of-the-art algorithms.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16506",
        "abstract url": "https://arxiv.org/abs/2406.16506",
        "title": "Natural Gradient Interpretation of Rank-One Update in CMA-ES",
        "rating": "-10",
        "keywords": [],
        "abstract": "The covariance matrix adaptation evolution strategy (CMA-ES) is a stochastic search algorithm using a multivariate normal distribution for continuous black-box optimization. In addition to strong empirical results, part of the CMA-ES can be described by a stochastic natural gradient method and can be derived from information geometric optimization (IGO) framework. However, there are some components of the CMA-ES, such as the rank-one update, for which the theoretical understanding is limited. While the rank-one update makes the covariance matrix to increase the likelihood of generating a solution in the direction of the evolution path, this idea has been difficult to formulate and interpret as a natural gradient method unlike the rank-$\u03bc$ update. In this work, we provide a new interpretation of the rank-one update in the CMA-ES from the perspective of the natural gradient with prior distribution. First, we propose maximum a posteriori IGO (MAP-IGO), which is the IGO framework extended to incorporate a prior distribution. Then, we derive the rank-one update from the MAP-IGO by setting the prior distribution based on the idea that the promising mean vector should exist in the direction of the evolution path. Moreover, the newly derived rank-one update is extensible, where an additional term appears in the update for the mean vector. We empirically investigate the properties of the additional term using various benchmark functions.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "This paper has been accepted for presentation at PPSN2024"
    },
    {
        "paper id": "2406.16515",
        "abstract url": "https://arxiv.org/abs/2406.16515",
        "title": "An FPRAS for #nFBDD",
        "rating": "-10",
        "keywords": [],
        "abstract": "#nFBDD is the problem of counting the number of satisfying assignments, or models, of a non-deterministic free binary decision diagram (nFBDD). The problem is #P-hard. We study the approximate variant of this problem where one seeks an estimate of the model count. It is known that there exists a quasi-polynomial-time randomized approximation scheme (QPRAS) for #nFBDD. We provide the first FPRAS for #nFBDD",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16548",
        "abstract url": "https://arxiv.org/abs/2406.16548",
        "title": "The Derivation of The Probability of Error for BPSK, 16-QAM and 64-QAM in Rayleigh Fading Channel: A Unified Approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "Understanding the probability of error is paramount in the design and analysis of digital communication systems, particularly in Rayleigh fading channels where signal impairments are prevalent. This article presents a unified approach for deriving the probability of error formulations applicable to Binary Phase Shift Keying (BPSK), 16-Quadrature Amplitude Modulation (16-QAM), and 64-QAM in Rayleigh fading channels. This article presents a general approach to derive the probability of error formulations applicable to Binary Phase Shift Keying (BPSK), 16-Quadruple Amplitude Modulation (16-QAM) and 64-QAM in Rayleigh fading channels. The derivation process encompasses the unique characteristics of each modulation scheme and the statistical properties of Rayleigh fading providing a comprehensive framework to analyze error performance. By establishing a unified formulation, this approach simplifies the analysis and facilitates a deeper understanding of error probability behavior across different modulation schemes. The derived formulations offer insights into the impact of fading-induced impairments on system performance, allowing for accurate prediction and optimization of communication systems in real-world fading environments. The insights and techniques presented herein serve as a valuable resource for researchers, engineers, and practitioners engaged in the design and optimization of communication systems operating in challenging fading environments.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16572",
        "abstract url": "https://arxiv.org/abs/2406.16572",
        "title": "ChatGPT's financial discrimination between rich and poor -- misaligned with human behavior and expectations",
        "rating": "-10",
        "keywords": [],
        "abstract": "ChatGPT disrupted the application of machine-learning methods and drastically reduced the usage barrier. Chatbots are now widely used in a lot of different situations. They provide advice, assist in writing source code, or assess and summarize information from various sources. However, their scope is not only limited to aiding humans; they can also be used to take on tasks like negotiating or bargaining. To understand the implications of Chatbot usage on bargaining situations, we conduct a laboratory experiment with the ultimatum game. In the ultimatum game, two human players interact: The receiver decides on accepting or rejecting a monetary offer from the proposer. To shed light on the new bargaining situation, we let ChatGPT provide an offer to a human player. In the novel design, we vary the wealth of the receivers. Our results indicate that humans have the same beliefs about other humans and chatbots. However, our results contradict these beliefs in an important point: Humans favor poor receivers as correctly anticipated by the humans, but ChatGPT favors rich receivers which the humans did not expect to happen. These results imply that ChatGPT's answers are not aligned with those of humans and that humans do not anticipate this difference.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16588",
        "abstract url": "https://arxiv.org/abs/2406.16588",
        "title": "Switching Controller Synthesis for Hybrid Systems Against STL Formulas",
        "rating": "-10",
        "keywords": [],
        "abstract": "Switching controllers play a pivotal role in directing hybrid systems (HSs) towards the desired objective, embodying a ``correct-by-construction'' approach to HS design. Identifying these objectives is thus crucial for the synthesis of effective switching controllers. While most of existing works focus on safety and liveness, few of them consider timing constraints. In this paper, we delves into the synthesis of switching controllers for HSs that meet system objectives given by a fragment of STL, which essentially corresponds to a reach-avoid problem with timing constraints. Our approach involves iteratively computing the state sets that can be driven to satisfy the reach-avoid specification with timing constraints. This technique supports to create switching controllers for both constant and non-constant HSs. We validate our method's soundness, and confirm its relative completeness for a certain subclass of HSs. Experiment results affirms the efficacy of our approach.",
        "subjects": [
            "eess.SY",
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16614",
        "abstract url": "https://arxiv.org/abs/2406.16614",
        "title": "Process Algebra Based Tool Coordination Architectures in Raku and Go",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents ongoing research in our project software engineering with process algebra. In this project we have developed among others a reimplementation of the simulator from the PSF Toolkit, a set of tools for the Process Specification formalism (PSF). This new simulator uses the ToolBus, a tool coordination architecture based on process algebra. We now developed new tool coordination architectures based on this ToolBus. We implement the primitives of the ToolBus in the programming languages Raku and Go. Both these languages have support for concurrency and communication between concurrent entities in the form of channels. We apply these tool coorination architectures on a small example. And we give implementations for the simulator in the PSF Toolkit based on the tool coordination architectures in Raku and Go.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16629",
        "abstract url": "https://arxiv.org/abs/2406.16629",
        "title": "Meta-experiments: Improving experimentation through experimentation",
        "rating": "-10",
        "keywords": [],
        "abstract": "A/B testing is widexly used in the industry to optimize customer facing websites. Many companies employ experimentation specialists to facilitate and improve the process of A/B testing. Here, we present the application of A/B testing to this improvement effort itself, by running experiments on the experimentation process, which we call 'meta-experiments'. We discuss the challenges of this approach using the example of one of our meta-experiments, which helped experimenters to run more sufficiently powered A/B tests. We also point out the benefits of 'dog fooding' for the experimentation specialists when running their own experiments.",
        "subjects": [
            "cs.IR",
            "stat.AP"
        ],
        "comment": "6 pages, 2 figures, 1 table"
    },
    {
        "paper id": "2406.16656",
        "abstract url": "https://arxiv.org/abs/2406.16656",
        "title": "Permutation Codes Correcting Multiple Deletions",
        "rating": "-10",
        "keywords": [],
        "abstract": "Permutation codes in the Ulam metric, which can correct multiple deletions, have been investigated extensively recently owing to their applications. In this work, we are interested in the maximum size of the permutation codes in the Ulam metric and aim to design permutation codes that can correct multiple deletions with efficient decoding algorithms. We first present an improvement on the Gilbert--Varshamov bound of the maximum size of these permutation codes which is the best-known lower bound. Next, we focus on designing permutation codes in the Ulam metric with a decoding algorithm. These constructed codes are the best-known permutation codes that can correct multiple deletions. In particular, the constructed permutation codes can correct $t$ deletions with at most $(3t-1) \\log n+o(\\log n)$ bits of redundancy where $n$ is the length of the code. Finally, we provide an efficient decoding algorithm for our constructed permutation codes.",
        "subjects": [
            "cs.IT",
            "cs.DM",
            "math.CO"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2406.16662",
        "abstract url": "https://arxiv.org/abs/2406.16662",
        "title": "Adaptive Coding for Two-Way Wiretap Channel under Strong Secrecy",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper studies adaptive coding for the two-way wiretap channel. Especially, the strong secrecy metric is of our interest that is defined by the information leakage of transmitted messages to the eavesdropper. First, we consider an adaptive coding, the construction of which is based on running the well studied non-adaptive coding in several rounds and the dependency between the adjacent rounds of transmission is introduced by the key exchange mechanism that is embedded in the non-adaptive coding in each transmission round. As a result, we analyze the reliability and strong secrecy that are measured by the decoding error probability and information leakage, characterize them in terms of the conditional R\u00e9nyi mutual information, and derive inner bounds on the secrecy capacity regions for the TW-WC under strong joint and individual secrecy constraints. Second, we introduce another adaptive coding method that explores the correlation among the outputs at the receivers. With this approach, we show that for the two-way wiretap channel that fulfills the conditionally independent condition, positive transmission rates can be always guaranteed even under the joint secrecy constraint.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2310.13881"
    },
    {
        "paper id": "2406.16675",
        "abstract url": "https://arxiv.org/abs/2406.16675",
        "title": "Decentralized and Centralized IDD Schemes for Cell-Free Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we propose iterative interference cancellation schemes with access points selection (APs-Sel) for cell-free massive multiple-input multiple-output (CF-mMIMO) systems. Closed-form expressions for centralized and decentralized linear minimum mean square error (LMMSE) receive filters with APs-Sel are derived assuming imperfect channel state information (CSI). Furthermore, we develop a list-based detector based on LMMSE receive filters that exploits interference cancellation and the constellation points. A message-passing-based iterative detection and decoding (IDD) scheme that employs low-density parity-check (LDPC) codes is then developed. Moreover, log-likelihood ratio (LLR) refinement strategies based on censoring and a linear combination of local LLRs are proposed to improve the network performance. We compare the cases with centralized and decentralized processing in terms of bit error rate (BER) performance, complexity, and signaling under perfect CSI (PCSI) and imperfect CSI (ICSI) and verify the superiority of the distributed architecture with LLR refinements.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "13 pages, 6 figures"
    },
    {
        "paper id": "2406.16685",
        "abstract url": "https://arxiv.org/abs/2406.16685",
        "title": "A locking-free isogeometric thin shell formulation based on higher order accurate local strain projection via approximate dual splines",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a novel isogeometric discretization approach for the Kirchhoff-Love shell formulation based on the Hellinger-Reissner variational principle. For mitigating membrane locking, we discretize the independent strains with spline basis functions that are one degree lower than those used for the displacements. To enable computationally efficient condensation of the independent strains, we first discretize the variations of the independent strains with approximate dual splines to obtain a projection matrix that is close to a diagonal matrix. We then diagonalize this strain projection matrix via row-sum lumping. The combination of approximate dual test functions with row-sum lumping enables the direct condensation of the independent strain fields at the quadrature point level, while maintaining higher-order accuracy at optimal rates of convergence. We illustrate the numerical properties and the performance of our approach through numerical benchmarks, including a curved Euler-Bernoulli beam and the examples of the shell obstacle course.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16704",
        "abstract url": "https://arxiv.org/abs/2406.16704",
        "title": "Tuning a Cascaded Online Feedback Optimization Controller for Provision of Distributed Flexibility",
        "rating": "-10",
        "keywords": [],
        "abstract": "Coordinating a high number of flexibility providing units (e.g. to provide ancillary services for the transmission system) across various grid layers requires new control concepts. A flexibility request at a point of common coupling can be met by utilizing a cascaded control structure based on online feedback optimization. In this paper the influence of the parameterization of the individual controllers on the performance of the hierarchical flexibility provision is studied on a three-level test system. The results show a high interdependency between the choice of control parameters of one controller and the behavior of other controllers as well as a significant impact on the accuracy and speed of flexibility provision. With a careful tuning, a cascaded structure based on online feedback optimization can achieve efficient vertical coordination of flexibility providing units.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16711",
        "abstract url": "https://arxiv.org/abs/2406.16711",
        "title": "Generalized Modal Analysis in Power System with High CIG Penetration: Concept and Quantitative Assessment",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a Generalized Modal Analysis (GMA) concept for the small-signal stability analysis of power systems with high penetration of Converter-Interfaced Generation (CIG). GMA quantitatively assesses interactions between various elements in the power system, offering intuitive and transparent physical interpretations. The method's versatility in selecting physical quantities at different input and output ports makes it broadly applicable. Based on the concept of GMA, the study further defines interaction quantification indices by selecting voltage ports, examining the impact of grid disturbances on power sources and the support from the power sources to the grid at connection points. Numerical simulations on modified 14-bus and 68-bus systems validate GMA's effectiveness in capturing the coupling of the dynamic characteristics between grid elements. This research provides a theoretical foundation and analytical framework for future analyses of power system stability with diverse power sources.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "submitted to IEEE Transactions on Power Systems for peer-review"
    },
    {
        "paper id": "2406.16719",
        "abstract url": "https://arxiv.org/abs/2406.16719",
        "title": "Backstepping control for the sterile mosquitoes technique: stabilization of extinction equilibrium",
        "rating": "-10",
        "keywords": [],
        "abstract": "The control of a mosquito population using the sterile insect technique is considered. Building on a model-based approach, where the control input is the release rate of sterilized males, we propose a non-negative backstepping control law capable of globally stabilizing the extinction equilibrium of the system. A simulation study supports and validates the theoretical findings, showing the efficacy of the approach both on a reduced model, used for control design, and on a complete model of the mosquito population dynamics.",
        "subjects": [
            "math.OC",
            "eess.SY",
            "math.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16720",
        "abstract url": "https://arxiv.org/abs/2406.16720",
        "title": "Dynamic Probability Logic: Decidability & Computability",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this article, the decidability and computability issues of dynamic probability logic (DPL) are addressed. Firstly, a proof system $\\mathcal{H}_{DPL}$ is introduced for DPL and shown that it is weakly complete. Furthermore, this logic has the finite model property and so is decidable. Secondly, a strongly complete proof system HDPL is presented for DPL and proved that its canonical model is a computable structure.",
        "subjects": [
            "cs.LO",
            "math.LO"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2401.07235"
    },
    {
        "paper id": "2406.16729",
        "abstract url": "https://arxiv.org/abs/2406.16729",
        "title": "Optimizing Search Strategies: A Study of Two-Pointer Linear Search Implementation",
        "rating": "-10",
        "keywords": [],
        "abstract": "This report investigates three fundamental search algorithms: Linear Search, Binary Search, and Two Pointer Search. Linear Search checks each element sequentially, Binary Search divides the search space in half, and Two Pointer Search uses two pointers to scan from both ends of a sorted list. We compare these algorithms in terms of time complexity, space complexity, and practical performance. Our findings demonstrate that while Linear Search is straightforward, it is inefficient for large datasets. Binary Search is efficient for sorted data but requires an initial sorting step. The Two Pointer Search, combining elements of both methods, offers a practical balance of simplicity and efficiency. We propose a novel implementation of the Two Pointer Search algorithm and validate its performance through comprehensive testing on various hardware configurations. The results indicate that our proposed algorithm significantly improves search efficiency, making it suitable for both sorted and unsorted datasets. Future work will extend this algorithm to more complex data structures and real-world applications.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "5 pages, 1 figures, 4 Tables"
    },
    {
        "paper id": "2406.16739",
        "abstract url": "https://arxiv.org/abs/2406.16739",
        "title": "Agent-Driven Automatic Software Improvement",
        "rating": "-10",
        "keywords": [],
        "abstract": "With software maintenance accounting for 50% of the cost of developing software, enhancing code quality and reliability has become more critical than ever. In response to this challenge, this doctoral research proposal aims to explore innovative solutions by focusing on the deployment of agents powered by Large Language Models (LLMs) to perform software maintenance tasks. The iterative nature of agents, which allows for continuous learning and adaptation, can help surpass common challenges in code generation. One distinct challenge is the last-mile problems, errors at the final stage of producing functionally and contextually relevant code. Furthermore, this project aims to surpass the inherent limitations of current LLMs in source code through a collaborative framework where agents can correct and learn from each other's errors. We aim to use the iterative feedback in these systems to further fine-tune the LLMs underlying the agents, becoming better aligned to the task of automated software improvement. Our main goal is to achieve a leap forward in the field of automatic software improvement by developing new tools and frameworks that can enhance the efficiency and reliability of software development.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "The content in this pre-print is the same as in the CRC accepted for publication in the International Conference on Evaluation and Assessment in Software Engineering (EASE 2024)"
    },
    {
        "paper id": "2406.16785",
        "abstract url": "https://arxiv.org/abs/2406.16785",
        "title": "Bisimulation for Impure Simplicial Complexes",
        "rating": "-10",
        "keywords": [],
        "abstract": "As an alternative to Kripke models, simplicial complexes are a versatile semantic primitive on which to interpret epistemic logic. Given a set of vertices, a simplicial complex is a downward closed set of subsets, called simplexes, of the vertex set. A maximal simplex is called a facet. Impure simplicial complexes represent that some agents (processes) are dead. It is known that impure simplicial complexes categorically correspond to so-called partial epistemic (Kripke) models. In this contribution, we define a notion of bisimulation to compare impure simplicial complexes and show that it has the Hennessy-Milner property. These results are for a logical language including atoms that express whether agents are alive or dead. Without these atoms no reasonable standard notion of bisimulation exists, as we amply justify by counterexamples, because such a restricted language is insufficiently expressive.",
        "subjects": [
            "cs.LO",
            "cs.DC"
        ],
        "comment": "Proceedings of Advances in Modal Logic 2024"
    },
    {
        "paper id": "2406.16786",
        "abstract url": "https://arxiv.org/abs/2406.16786",
        "title": "Generalized and high-efficiency arbitrary-positioned buffer for smoothed particle hydrodynamics",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper develops an arbitrary-positioned buffer for the smoothed particle hydrodynamics (SPH) method, whose generality and high efficiency are achieved through two techniques. First, with the local coordinate system established at each arbitrary-positioned in-/outlet, particle positions in the global coordinate system are transformed into those in it via coordinate transformation. Since one local axis is located perpendicular to the in-/outlet boundary, the position comparison between particles and the threshold line or surface can be simplified to just this coordinate dimension. Second, particle candidates subjected to position comparison at one specific in-/outlet are restricted to those within the local cell-linked lists nearby the defined buffer zone, which significantly enhances computational efficiency due to a small portion of particles being checked. With this developed buffer, particle generation and deletion at arbitrary-positioned in- and outlets of complex flows can be handled efficiently and straightforwardly. We validate the effectiveness and versatility of the developed buffer through 2-D and 3-D non-/orthogonal uni-/bidirectional flows with arbitrary-positioned in- and outlets, driven by either pressure or velocity boundary conditions.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "34 pages and 17 figures"
    },
    {
        "paper id": "2406.16798",
        "abstract url": "https://arxiv.org/abs/2406.16798",
        "title": "High-quality activation function for applications in neuromorphic photonic chips realized using low-quality nonlinear optical resonators",
        "rating": "-10",
        "keywords": [],
        "abstract": "Integrated optical devices that can realize a threshold filtration of signals are in demand in photonics. In particular, they play a key role in neuromorphic chips, acting as optical neurons. A list of requirements exist for thresholders to be practically applicable in this context. A value of the threshold, in general, should be independently tunable for each neuron. A sharpness of the corresponding step function should be also dynamically variable, to allow switching between deterministic and stochastic algorithms, for example, in optical Ising machines. Nonlinear ring resonators draw the attention of researchers in this field since they potentially can provide the required type of threshold behavior. Here we suggest the switching mechanism that implements the property of resonators to provide extremely sharp (with respect to the wavelength) $\u03c0$ phase shifts near the critical coupling regime. Adding a variable source of losses into the resonator, a well controlled broadening of the threshold can be achieved. Sharpness of the step in this case is independent on the quality factor of resonators and corresponding width of resonances. The nonlinear switching mechanism allows to use this feature to construct efficient optical neurons, that operate at small intensities. It also allows to use conventional materials like silicon with a relatively weak Kerr nonlinearity. The obtained results potentially lead the way to fast nonlinear Kerr effect based activation functions that can operate in continuous wave regime.",
        "subjects": [
            "physics.optics",
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2406.16843",
        "abstract url": "https://arxiv.org/abs/2406.16843",
        "title": "Constructibility, computational complexity and P versus NP",
        "rating": "-10",
        "keywords": [],
        "abstract": "A class of decision problems related to inconsistency proofs for formal theories is used to show that under a constructive interpretation of computational complexity classes, an assumption referred to as the MW thesis implies that one may explicitly construct decision problems in NP which are not algorithmically solvable. In particular, in this interpretation the MW thesis implies P $\\neq$ NP. It is argued that MW is a naturally valid, yet also naturally unformalizable principle.",
        "subjects": [
            "cs.CC",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17049",
        "abstract url": "https://arxiv.org/abs/2406.17049",
        "title": "Fast Switching Serial and Parallel Paradigms of SNN Inference on Multi-core Heterogeneous Neuromorphic Platform SpiNNaker2",
        "rating": "-10",
        "keywords": [],
        "abstract": "With serial and parallel processors are introduced into Spiking Neural Networks (SNNs) execution, more and more researchers are dedicated to improving the performance of the computing paradigms by taking full advantage of strengths of the available processor. In this paper, we compare and integrate serial and parallel paradigms into one SNN compiling system. For a faster switching between them in the layer granularity, we train the classifier to prejudge a better paradigm before compiling instead of making decision afterwards, saving a great amount of compiling time and RAM space on host PC. The classifier Adaptive Boost with the highest accuracy (91.69 percent) among 12 classifiers is integrated into the switching system, which utilizes less memory and processors on the multi-core neuromorphic hardware backend SpiNNaker2 than two individual paradigms. To the best of our knowledge, it is the first fast switching compiling system for SNN simulation.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17082",
        "abstract url": "https://arxiv.org/abs/2406.17082",
        "title": "A Strongly Normalising System of Dependent Types for Transparent and Opaque Probabilistic Computation",
        "rating": "-10",
        "keywords": [],
        "abstract": "We define an extension of lambda-calculus with dependents types that enables us to encode transparent and opaque probabilistic programs and prove a strong normalisation result for it by a reducibility technique. While transparent non-deterministic programs are formalised in a rather usual way, opaque non-deterministic programs are formalised by introducing in the syntax oracle constants, the behaviour of which is governed by oracular functions. The generality of these functions and the fact that their values are determined by the form of the whole term inside which the relative oracle occurs also enable us to simulate learning-like behaviours.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17085",
        "abstract url": "https://arxiv.org/abs/2406.17085",
        "title": "Perturbed Decision-Focused Learning for Modeling Strategic Energy Storage",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a novel decision-focused framework integrating the physical energy storage model into machine learning pipelines. Motivated by the model predictive control for energy storage, our end-to-end method incorporates the prior knowledge of the storage model and infers the hidden reward that incentivizes energy storage decisions. This is achieved through a dual-layer framework, combining a prediction layer with an optimization layer. We introduce the perturbation idea into the designed decision-focused loss function to ensure the differentiability over linear storage models, supported by a theoretical analysis of the perturbed loss function. We also develop a hybrid loss function for effective model training. We provide two challenging applications for our proposed framework: energy storage arbitrage, and energy storage behavior prediction. The numerical experiments on real price data demonstrate that our arbitrage approach achieves the highest profit against existing methods. The numerical experiments on synthetic and real-world energy storage data show that our approach achieves the best behavior prediction performance against existing benchmark methods, which shows the effectiveness of our method.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17094",
        "abstract url": "https://arxiv.org/abs/2406.17094",
        "title": "ammBoost: State Growth Control for AMMs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Automated market makers (AMMs) are a form of decentralized cryptocurrency exchanges and considered a prime example of Decentralized Finance (DeFi) applications. Their popularity and high trading activity have resulted in millions of on-chain transactions leading to serious scalability issues. In this paper, we address the on-chain storage overhead problem of AMMs by utilizing a new sidechain architecture as a layer 2 solution, building a system called ammBoost. Our system reduces the amount of on-chain transactions, boosts throughput, and supports blockchain pruning. We devise several techniques to enable layer 2 processing for AMMs while preserving correctness and security of the underlying AMM. We also build a proof-of-concept of ammBoost for a Uniswap-inspired use case to empirically evaluate its performance. Our experiments show that ammBoost decreases the gas cost by 94.53% and the chain growth by at least 80%, and that it can support up to 500x of the daily traffic volume observed for Uniswap in practice.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17097",
        "abstract url": "https://arxiv.org/abs/2406.17097",
        "title": "Lower Quantity, Higher Quality: Auditing News Content and User Perceptions on Twitter/X Algorithmic versus Chronological Timelines",
        "rating": "-10",
        "keywords": [],
        "abstract": "Social media personalization algorithms increasingly influence the flow of civic information through society, resulting in concerns about \"filter bubbles\", \"echo chambers\", and other ways they might exacerbate ideological segregation and fan the spread of polarizing content. To address these concerns, we designed and conducted a sociotechnical audit (STA) to investigate how Twitter/X's timeline algorithm affects news curation while also tracking how user perceptions change in response. We deployed a custom-built system that, over the course of three weeks, passively tracked all tweets loaded in users' browsers in the first week, then in the second week enacted an intervention to users' Twitter/X homepage to restrict their view to only the algorithmic or chronological timeline (randomized). We flipped this condition for each user in the third week. We ran our audit in late 2023, collecting user-centered metrics (self-reported survey measures) and platform-centered metrics (views, clicks, likes) for 243 users, along with over 800,000 tweets. Using the STA framework, our results are two-fold: (1) Our algorithm audit finds that Twitter/X's algorithmic timeline resulted in a lower quantity but higher quality of news -- less ideologically congruent, less extreme, and slightly more reliable -- compared to the chronological timeline. (2) Our user audit suggests that although our timeline intervention had significant effects on users' behaviors, it had little impact on their overall perceptions of the platform. Our paper discusses these findings and their broader implications in the context of algorithmic news curation, user-centric audits, and avenues for independent social science research.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "24 pages, 5 figures, Computer-Supported Cooperative Work"
    },
    {
        "paper id": "2406.17120",
        "abstract url": "https://arxiv.org/abs/2406.17120",
        "title": "Cascade Generalization-based Classifiers for Software Defect Prediction",
        "rating": "-10",
        "keywords": [],
        "abstract": "The process of software defect prediction (SDP) involves predicting which software system modules or components pose the highest risk of being defective. The projections and discernments derived from SDP can then assist the software development team in effectively allocating its finite resources toward potentially susceptible defective modules. Because of this, SDP models need to be improved and refined continuously. Hence, this research proposes the deployment of a cascade generalization (CG) function to enhance the predictive performances of machine learning (ML)-based SDP models. The CG function extends the initial sample space by introducing new samples into the neighbourhood of the distribution function generated by the base classification algorithm, subsequently mitigating its bias. Experiments were conducted to investigate the effectiveness of CG-based Na\u00efve Bayes (NB), Decision Tree (DT), and k-Nearest Neighbor (kNN) models on NASA software defect datasets. Based on the experimental results, the CG-based models (CG-NB, CG-DT, CG-kNN) were superior in prediction performance when compared with the baseline NB, DT, and kNN models respectively. Accordingly, the average accuracy value of CG-NB, CG-DT, and CG-kNN models increased by +11.06%, +3.91%, and +5.14%, respectively, over baseline NB, DT, and kNN models. A similar performance was observed for the area under the curve (AUC) value with CG-NB, CG-DT, and CG-kNN recording an average AUC value of +7.98%, +26%, and +24.9% improvement over the baseline NB, DT, and kNN respectively. In addition, the suggested CG-based models outperformed the Bagging and Boosting ensemble variants of the NB, DT, and kNN models as well as existing computationally diverse SDP models.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2406.17121",
        "abstract url": "https://arxiv.org/abs/2406.17121",
        "title": "Competitive Policies for Online Collateral Maintenance",
        "rating": "-10",
        "keywords": [],
        "abstract": "Layer-two blockchain protocols emerged to address scalability issues related to fees, storage cost, and confirmation delay of on-chain transactions. They aggregate off-chain transactions into a fewer on-chain ones, thus offering immediate settlement and reduced transaction fees. To preserve security of the underlying ledger, layer-two protocols often work in a collateralized model; resources are committed on-chain to backup off-chain activities. A fundamental challenge that arises in this setup is determining a policy for establishing, committing, and replenishing the collateral in a way that maximizes the value of settled transactions. In this paper, we study this problem under two settings that model collateralized layer-two protocols. The first is a general model in which a party has an on-chain collateral C with a policy to decide on whether to settle or discard each incoming transaction. The policy also specifies when to replenish C based on the remaining collateral value. The second model considers a discrete setup in which C is divided among k wallets, each of which is of size C/k, such that when a wallet is full, and so cannot settle any incoming transactions, it will be replenished. We devise several online policies for these models, and show how competitive they are compared to optimal (offline) policies that have full knowledge of the incoming transaction stream. To the best of our knowledge, we are the first to study and formulate online competitive policies for collateral and wallet management in the blockchain setting.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17122",
        "abstract url": "https://arxiv.org/abs/2406.17122",
        "title": "A New Algorithm for Whitney Stratification of Varieties",
        "rating": "-10",
        "keywords": [],
        "abstract": "We describe a new algorithm to compute Whitney stratifications of real and complex algebraic varieties. This algorithm is a modification of the algorithm of Helmer and Nanda (HN), but is made more efficient by using techniques for equidimensional decomposition rather than computing the set of associated primes of a polynomial ideal at a key step in the HN algorithm. We note that this modified algorithm may fail to produce a minimal Whitney stratification even when the HN algorithm would produce a minimal stratification. We, additionally, present an algorithm to coarsen any Whitney stratification of a complex variety to a minimal Whitney stratification; the theoretical basis for our approach is a classical result of Teissier.",
        "subjects": [
            "math.AG",
            "cs.SC",
            "math.AC",
            "math.AT"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17132",
        "abstract url": "https://arxiv.org/abs/2406.17132",
        "title": "LLM-Aided Testbench Generation and Bug Detection for Finite-State Machines",
        "rating": "-10",
        "keywords": [],
        "abstract": "This work investigates the potential of tailoring Large Language Models (LLMs), specifically GPT3.5 and GPT4, for the domain of chip testing. A key aspect of chip design is functional testing, which relies on testbenches to evaluate the functionality and coverage of Register-Transfer Level (RTL) designs. We aim to enhance testbench generation by incorporating feedback from commercial-grade Electronic Design Automation (EDA) tools into LLMs. Through iterative feedback from these tools, we refine the testbenches to achieve improved test coverage. Our case studies present promising results, demonstrating that this approach can effectively enhance test coverage. By integrating EDA tool feedback, the generated testbenches become more accurate in identifying potential issues in the RTL design. Furthermore, we extended our study to use this enhanced test coverage framework for detecting bugs in the RTL implementations",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2406.17174",
        "abstract url": "https://arxiv.org/abs/2406.17174",
        "title": "Teaching Theorizing in Software Engineering Research",
        "rating": "-10",
        "keywords": [],
        "abstract": "This chapter seeks to support software engineering (SE) researchers and educators in teaching the importance of theory as well as the theorizing process. Drawing on insights from other fields, the chapter presents 12 intermediate products of theorizing and what they mean in an SE context. These intermediate products serve different roles: some are theory products to frame research studies, some are theory generators, and others are components of theory. Whereas the SE domain doesn't have many theories of its own, these intermediate products of theorizing can be found widely. The chapter aims to help readers to recognize these intermediate products, their role, and how they can help in the theorizing process within SE research. To illustrate their utility, the chapter then applies the set of intermediate theorizing products to the software architecture research field. The chapter ends with a suggested structure for a 12-week course on theorizing in SE which can be readily adapted by educators.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "38 pages, 5 tables, 6 figures"
    },
    {
        "paper id": "2406.17176",
        "abstract url": "https://arxiv.org/abs/2406.17176",
        "title": "Dynamic Provisioning of REST APIs for Model Management",
        "rating": "-10",
        "keywords": [],
        "abstract": "Model-Driven Engineering (MDE) is a software engineering methodology focusing on models as primary artifacts. In the last years, the emergence of Web technologies has led to the development of Web-based modeling tools and model-based approaches for the Web that offer a web-based environment to create and edit models or model-based low-code solutions. A common requirement when developing Web-based modeling tools is to provide a fast and efficient way for model management, and this is particularly a hot topic in model-based system engineering. However, the number of approaches offering RESTful services for model management is still limited. Among the alternatives for developing distributed services, there is a growing interest in the use of RESTful services. In this paper, we present an approach to provide RESTful services for model management that can be used to interact with any kind of model and can be used to build a modeling platform providing modeling-as-a-service. The approach follows the REST principles to provide a stateless and scalable service.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "3 pages, 1 figure, 1 table"
    },
    {
        "paper id": "2406.17180",
        "abstract url": "https://arxiv.org/abs/2406.17180",
        "title": "CogExplore: Contextual Exploration with Language-Encoded Environment Representations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Integrating language models into robotic exploration frameworks improves performance in unmapped environments by providing the ability to reason over semantic groundings, contextual cues, and temporal states. The proposed method employs large language models (GPT-3.5 and Claude Haiku) to reason over these cues and express that reasoning in terms of natural language, which can be used to inform future states. We are motivated by the context of search-and-rescue applications where efficient exploration is critical. We find that by leveraging natural language, semantics, and tracking temporal states, the proposed method greatly reduces exploration path distance and further exposes the need for environment-dependent heuristics. Moreover, the method is highly robust to a variety of environments and noisy vision detections, as shown with a 100% success rate in a series of comprehensive experiments across three different environments conducted in a custom simulation pipeline operating in Unreal Engine.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "9 pages (22 with references and appendix), 12 figures (including 6 in the appendix), 1 table"
    },
    {
        "paper id": "2406.17196",
        "abstract url": "https://arxiv.org/abs/2406.17196",
        "title": "Coded Kalman Filtering over MIMO Gaussian Channels with Feedback",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the problem of remotely stabilizing a linear dynamical system. In this setting, a sensor co-located with the system communicates the system's state to a controller over a noisy communication channel with feedback. The objective of the controller (decoder) is to use the channel outputs to estimate the vector state with finite zero-delay mean squared error (MSE) at the infinite horizon. It has been shown in [1] that for a vector Gauss-Markov source and either a single-input multiple-output (SIMO) or a multiple-input single-output (MISO) channel, linear codes require the minimum capacity to achieve finite MSE. This paper considers the more general problem of linear zero-delay joint-source channel coding (JSCC) of a vector-valued source over a multiple-input multiple-output (MIMO) Gaussian channel with feedback. We study sufficient and necessary conditions for linear codes to achieve finite MSE. For sufficiency, we introduce a coding scheme where each unstable source mode is allocated to a single channel for estimation. Our proof for the necessity of this scheme relies on a matrix-algebraic conjecture that we prove to be true if either the source or channel is scalar. We show that linear codes achieve finite MSE for a scalar source over a MIMO channel if and only if the best scalar sub-channel can achieve finite MSE. Finally, we provide a new counter-example demonstrating that linear codes are generally sub-optimal for coding over MIMO channels.",
        "subjects": [
            "cs.IT",
            "eess.SY"
        ],
        "comment": "Accepted for presentation at the 2024 IEEE International Symposium on Information Theory"
    },
    {
        "paper id": "2406.17206",
        "abstract url": "https://arxiv.org/abs/2406.17206",
        "title": "Model Checking of vGOAL",
        "rating": "-10",
        "keywords": [],
        "abstract": "Developing autonomous decision-making requires safety assurance. Agent programming languages like AgentSpeak and Gwendolen provide tools for programming autonomous decision-making. However, despite numerous efforts to apply model checking to these languages, challenges persist such as a faithful semantic mapping between agent programs and the generated models, efficient model generation, and efficient model checking. As an extension of the agent programming language GOAL, vGOAL has been proposed to formally specify autonomous decisions with an emphasis on safety. This paper tackles the mentioned challenges through two automated model-checking processes for vGOAL: one for Computation Tree Logic and another for Probabilistic Computation Tree Logic. Compared with the existing model-checking approaches of agent programming languages, it has three main advantages. First, it efficiently performs automated model-checking analysis for a given vGOAL specification, including efficiently generating input models for NuSMV and Storm and leveraging these efficient model checkers. Second, the semantic equivalence is established for both nondeterministic models and probabilistic models of vGOAL: from vGOAL to transition systems or DTMCs. Third, an algorithm is proposed for efficiently detecting errors, which is particularly useful for vGOAL specifications that describe complex scenarios. Validation and experiments in a real-world autonomous logistic system with three autonomous mobile robots illustrate both the efficiency and practical usability of the automated CTL and PCTL model-checking process for vGOAL.",
        "subjects": [
            "cs.MA"
        ],
        "comment": "21 pages, 2 figures, it is a draft version of a paper that plans to submit to JAAMAS"
    },
    {
        "paper id": "2406.17240",
        "abstract url": "https://arxiv.org/abs/2406.17240",
        "title": "Pareto Fronts for Compositionally Solving String Diagrams of Parity Games",
        "rating": "-10",
        "keywords": [],
        "abstract": "Open parity games are proposed as a compositional extension of parity games with algebraic operations, forming string diagrams of parity games. A potential application of string diagrams of parity games is to describe a large parity game with a given compositional structure and solve it efficiently as a divide-and-conquer algorithm by exploiting its compositional structure. Building on our recent progress in open Markov decision processes, we introduce Pareto fronts of open parity games, offering a framework for multi-objective solutions. We establish the positional determinacy of open parity games with respect to their Pareto fronts through a novel translation method. Our translation converts an open parity game into a parity game tailored to a given single-objective. Furthermore, we present a simple algorithm for solving open parity games, derived from this translation that allows the application of existing efficient algorithms for parity games. Expanding on this foundation, we develop a compositional algorithm for string diagrams of parity games.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "Preprint"
    }
]